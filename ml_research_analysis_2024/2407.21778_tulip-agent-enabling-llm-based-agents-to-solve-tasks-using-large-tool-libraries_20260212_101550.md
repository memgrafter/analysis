---
ver: rpa2
title: Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries
arxiv_id: '2407.21778'
source_url: https://arxiv.org/abs/2407.21778
tags:
- tool
- tools
- agent
- tulip
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The tulip agent architecture enables large language models to use
  large tool libraries efficiently by replacing the conventional approach of passing
  all tool descriptions to the model with a recursive semantic search in a vector
  store. This reduces context window usage and inference costs significantly, allows
  the agent to work with arbitrarily large tool sets, and supports dynamic tool creation,
  updating, and deletion.
---

# Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries

## Quick Facts
- arXiv ID: 2407.21778
- Source URL: https://arxiv.org/abs/2407.21778
- Authors: Felix Ocker; Daniel Tanneberg; Julian Eggert; Michael Gienger
- Reference count: 40
- Key outcome: Tulip agent achieves 46-95% correct results while cutting costs by 2-3× compared to naive implementations using large tool libraries

## Executive Summary
The tulip agent architecture enables large language models to efficiently use large tool libraries by replacing the conventional approach of passing all tool descriptions to the model with recursive semantic search in a vector store. This approach significantly reduces context window usage and inference costs while maintaining high task performance. The architecture supports dynamic tool creation, updating, and deletion, making it adaptable to evolving task requirements. In experiments with 100 math tools, tulip agent variants demonstrated superior performance compared to baseline approaches.

## Method Summary
The tulip agent implements a semantic search-based approach where tools are stored in a vector store with embeddings, and relevant tools are retrieved recursively for each subtask rather than passing all tool descriptions to the LLM. The architecture includes a function analyzer for introspection, task decomposition module, semantic search module, tool executor, and optional CRUD module for tool management. The system uses Chain of Thought prompting to guide structured task decomposition and tool execution. Experiments compared tulip agent variants against baselines using mathematics tasks, varying language models, embedding models, and top_k parameters.

## Key Results
- Tulip agent variants achieved 46-95% correct results on mathematics tasks
- Costs reduced by 2-3× compared to naive implementations
- Performance remained stable across different embedding models
- AutoTulipAgent successfully created new tools for previously unsupported tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive semantic search in a vector store replaces passing all tool descriptions to the LLM, reducing context window usage and inference costs.
- Mechanism: Instead of embedding all tool descriptions in the system prompt (which counts against the context window), the agent uses semantic search to retrieve only the most relevant tools for each subtask. This search is recursive, allowing decomposition of subtasks that are too abstract to match tools directly.
- Core assumption: Semantic embeddings of tool descriptions and task subtasks are sufficiently similar to enable effective retrieval without degrading task decomposition quality.
- Evidence anchors:
  - [abstract] "Instead, the tulip agent can recursively search for suitable tools in its extensible tool library, implemented exemplarily as a vector store."
  - [section] "This is relevant in case the initial subtasks are not fine-grained enough to find suitable tools. By setting a similarity threshold for the semantic search, we can ensure that only suitable tools are returned."
  - [corpus] Weak - no direct citations found, but the approach aligns with general RAG literature.
- Break condition: If the semantic similarity between subtask embeddings and tool embeddings drops below the threshold, no tools are found and the agent cannot proceed without further decomposition.

### Mechanism 2
- Claim: Task decomposition into subtasks significantly improves tool selection and overall task performance.
- Mechanism: The LLM decomposes the user query into atomic subtasks, then searches for tools matching each subtask description. This structured approach reduces ambiguity and improves precision compared to searching for tools based on the full user query.
- Core assumption: LLMs can reliably decompose tasks into subtasks that map cleanly to individual tools, and the decomposition process doesn't lose essential task context.
- Evidence anchors:
  - [section] "Compared to the granularity of available tools, such tasks are typically posed on a high level of abstraction. Thus, they cannot be matched sensibly to individual tools."
  - [section] "Granularity and clarity are essential to reduce the semantic distance between the task description and the descriptions of suitable tools."
  - [corpus] Weak - no direct citations found, but decomposition is a standard LLM technique.
- Break condition: If the LLM fails to decompose the task properly or creates subtasks that don't align with available tools, the search will return no results and the agent fails.

### Mechanism 3
- Claim: CRUD operations on the tool library enable autonomous tool creation and adaptation, allowing the agent to handle tasks requiring tools not present in the initial library.
- Mechanism: The AutoTulipAgent variant has tools for creating new tools (via code generation), updating existing tools, and deleting tools. When a subtask cannot be matched to existing tools, the agent generates new code, validates it, and adds it to the library.
- Core assumption: The LLM's code generation capabilities are sufficient to create valid, well-documented Python functions that can be processed by the function analyzer.
- Evidence anchors:
  - [section] "Leveraging the code generation abilities of LLMs, the agent triggers the generation of a new tool for a specific task described in natural language."
  - [section] "To ensure validity of the new tool, the agent checks its executability in a loop in the spirit of Joublin et al. [2024] before saving."
  - [corpus] Weak - no direct citations found, but code generation by LLMs is well-established.
- Break condition: If the LLM generates invalid code or code that cannot be properly documented for the function analyzer, the tool creation process fails.

## Foundational Learning

- Concept: Vector embeddings and semantic search
  - Why needed here: The tool library uses semantic search via embeddings to find relevant tools without passing all descriptions to the LLM.
  - Quick check question: What happens if the embedding model fails to capture semantic similarity between task descriptions and tool descriptions?

- Concept: Chain of Thought (CoT) prompting
  - Why needed here: CoT prompting guides the LLM through task decomposition and tool execution in a structured, step-by-step manner.
  - Quick check question: How does CoT prompting improve the LLM's ability to handle complex multi-tool tasks compared to direct prompting?

- Concept: Function introspection and documentation standards
  - Why needed here: The agent automatically extracts tool information from Python functions using introspection, requiring consistent documentation standards.
  - Quick check question: What documentation format is required for the function analyzer to extract tool metadata correctly?

## Architecture Onboarding

- Component map:
  Tool library (vector store with embeddings) -> Function analyzer (introspects Python functions) -> Task decomposition module (splits user queries) -> Search module (semantic search for tools) -> Tool executor (calls identified tools) -> CRUD module (optional, for tool management)

- Critical path:
  1. Initialize tool library with function analyzer and embeddings
  2. Receive user query
  3. Decompose query into subtasks
  4. For each subtask: search for tools, execute tool calls
  5. Return final result or error

- Design tradeoffs:
  - Embedding quality vs. cost: Better embeddings improve search but increase setup costs
  - top_k parameter: Higher values improve recall but increase costs
  - Recursion depth: Deeper recursion handles more complex tasks but increases complexity

- Failure signatures:
  - No tools found for a subtask (similarity threshold too high or poor embeddings)
  - LLM gets stuck in tool execution loop (needs timeout/iteration limit)
  - Tool generation produces invalid code (LLM generation failure)

- First 3 experiments:
  1. Run CotTulipAgent on a simple math task (one tool) to verify basic functionality
  2. Test with a medium-complexity task (2-3 tools) to verify task decomposition and search
  3. Vary top_k parameter to find optimal balance between recall and cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the tulip agent architecture scale when using embedding models beyond the three tested (text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large), particularly more recent or specialized models?
- Basis in paper: [inferred] The paper tested three embedding models but noted that the choice of embedding model had little influence on tool retrieval for variants with advanced planning, suggesting potential for testing with other models.
- Why unresolved: The experiments only included three embedding models, leaving open the question of how other models might perform, especially those designed for specialized tasks or more recent models with improved capabilities.
- What evidence would resolve it: Conducting experiments with a broader range of embedding models, including newer or specialized ones, and comparing their impact on tool retrieval performance and overall agent correctness.

### Open Question 2
- Question: What are the specific limitations of the AutoTulipAgent's autonomous tool creation capabilities, and under what conditions does it fail to generate valid tools?
- Basis in paper: [explicit] The paper mentions that the AutoTulipAgent can create tools autonomously but notes that this depends largely on the LLM's abilities to generate code and that it may be sensible to use a dedicated model for tool creation.
- Why unresolved: While the paper demonstrates the concept of autonomous tool creation, it does not provide a detailed analysis of the limitations or failure conditions of the AutoTulipAgent, leaving questions about its robustness and reliability.
- What evidence would resolve it: A comprehensive evaluation of the AutoTulipAgent's tool creation capabilities, including a systematic analysis of failure modes, the types of tasks it struggles with, and the quality of the generated tools across different scenarios.

### Open Question 3
- Question: How does the tulip agent architecture perform in real-world robotics applications compared to simulated environments, and what are the key challenges in adapting it to physical robots?
- Basis in paper: [explicit] The paper demonstrates the applicability of the tulip agent architecture to a robotics scenario in a simulation but does not discuss its performance in real-world settings.
- Why unresolved: The experiments were conducted in a simulated environment, which may not fully capture the complexities and challenges of real-world robotics applications, such as sensor noise, dynamic environments, and physical constraints.
- What evidence would resolve it: Implementing the tulip agent architecture on physical robots and evaluating its performance in various real-world tasks, including a comparison with the simulated results and an analysis of the challenges encountered during the transition from simulation to reality.

## Limitations

- The tulip agent's performance heavily depends on the quality of semantic embeddings, which is not extensively validated across different embedding models or domains.
- The computational overhead of maintaining and searching embeddings at scale is not fully characterized, despite the context window reduction.
- The success rate and reliability of autonomous tool generation are not quantified, leaving questions about the robustness of the code generation process.

## Confidence

- **High Confidence**: The core claim that recursive semantic search reduces context window usage and inference costs compared to passing all tool descriptions to the LLM. This is directly supported by the experimental results showing 2-3× cost reduction.
- **Medium Confidence**: The claim that task decomposition significantly improves tool selection and overall task performance. While the mechanism is sound and the approach is standard, the paper does not provide ablation studies isolating the contribution of task decomposition from other factors.
- **Medium Confidence**: The assertion that CRUD operations enable autonomous tool creation and adaptation. The mechanism is described and partially demonstrated, but the success rate of autonomous tool generation and its reliability across different domains are not fully characterized.

## Next Checks

1. **Embedding Model Robustness Test**: Validate the tulip agent's performance using different embedding models (e.g., text-embedding-ada-002 vs. text-embedding-3-large) on the same mathematics tasks to quantify the impact of embedding quality on tool retrieval accuracy and overall task performance.

2. **Task Decomposition Ablation**: Create a variant of the tulip agent that skips task decomposition and directly searches for tools using the full user query. Compare its performance against the standard tulip agent on tasks of varying complexity to isolate the contribution of task decomposition to overall success.

3. **Autonomous Tool Creation Reliability**: Systematically test the AutoTulipAgent's tool creation capability by measuring the success rate of generating valid, executable tools for a range of subtasks that cannot be handled by the initial tool library. Document failure modes and quantify the proportion of generated tools that require manual correction.