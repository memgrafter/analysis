---
ver: rpa2
title: Large Language Models show both individual and collective creativity comparable
  to humans
arxiv_id: '2412.03151'
source_url: https://arxiv.org/abs/2412.03151
tags:
- task
- creativity
- responses
- llms
- creative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares Large Language Models (LLMs) and humans across
  13 creativity tasks in three domains: divergent thinking, problem solving, and creative
  writing. Results show that the best LLMs (Claude and GPT-4) rank in the 52nd percentile
  against humans, excelling in divergent thinking and problem solving but lagging
  in creative writing.'
---

# Large Language Models show both individual and collective creativity comparable to humans
## Quick Facts
- arXiv ID: 2412.03151
- Source URL: https://arxiv.org/abs/2412.03151
- Reference count: 40
- One-line primary result: Best LLMs (Claude, GPT-4) rank in the 52nd percentile against humans in creativity, excelling in divergent thinking and problem solving but lagging in creative writing.

## Executive Summary
This study compares the creative performance of five Large Language Models (LLMs) and 467 human participants across 13 creativity tasks spanning divergent thinking, problem solving, and creative writing. Results show that top-performing LLMs achieve median human-equivalent creativity scores, with collective responses from a single LLM matching the creativity of 8-10 humans. While LLMs generate more ideas than humans and excel in divergent thinking and problem solving, they lag behind in creative writing tasks. The study also reveals that LLM responses lack the diversity found in human responses, both within individual responses and across multiple responses.

## Method Summary
The study administered 13 creativity tasks to both humans (467 Chinese participants) and five commercial LLMs (GPT-3.5, GPT-4, Claude, Qwen, SparkDesk). Human responses were evaluated by independent judges using the Consensual Assessment Technique, rating ideas on novelty and usefulness without knowing the source. LLMs were tested with five temperature settings (0.0, 0.5, 1.0, 1.5, 2.0) to assess creativity variations. The collective creativity of LLMs was measured by pooling multiple responses and comparing them to human group performance. Statistical analyses included percentile rankings, collective creativity equivalence calculations, and diversity measurements using text similarity metrics.

## Key Results
- Top LLMs (Claude, GPT-4) achieved 52nd percentile performance against humans in creativity tasks
- LLMs generated 8.85 valid ideas on average versus 3.68 for humans
- One LLM's collective creativity (10 responses) equals 8-10 humans
- Two additional LLM responses provide equivalent creativity gain as one additional human
- LLMs excel in divergent thinking and problem solving but lag in creative writing

## Why This Works (Mechanism)
### Mechanism 1
- Claim: LLMs can match or exceed average human performance in divergent thinking and problem solving tasks.
- Mechanism: LLMs generate a higher volume of ideas than humans, and when novelty and usefulness are combined, many of these ideas are rated comparably to human ideas.
- Core assumption: Judges evaluate ideas on novelty and usefulness independently and fairly, without knowing the source.
- Evidence anchors:
  - [abstract] "overall LLMs excel in divergent thinking and problem solving but lag in creative writing"
  - [section] "On average each human participant generated 3.68 valid ideas. In contrast, LLMs generated a mean of 8.85 valid ideas"
- Break condition: If judges consistently rate human ideas as more novel or useful, or if the trade-off between novelty and usefulness is not captured accurately.

### Mechanism 2
- Claim: Repeated responses from a single LLM can collectively match the creativity of a small group of humans.
- Mechanism: By asking an LLM multiple times, the top-rated responses collectively contribute as many top ideas as a group of humans, with roughly two additional LLM responses equaling one extra human.
- Core assumption: The quality of the best responses from an LLM, when pooled, is comparable to the best from a human group.
- Evidence anchors:
  - [abstract] "When questioned 10 times, an LLM's collective creativity is equivalent to 8-10 humans"
  - [section] "when one LLM was asked 10 times, our findings suggest that one LLM could be considered equivalent on average to a group of 8 to 10 humans"
- Break condition: If human groups consistently outperform the pooled top responses of an LLM, or if the relationship between LLM responses and human equivalence changes significantly.

### Mechanism 3
- Claim: LLMs lack diversity in their outputs compared to humans, both within and between responses.
- Mechanism: LLM responses show higher similarity in ideas within a single response and between different responses than human responses.
- Core assumption: Similarity metrics (cosine similarity, Levenshtein distance) accurately capture the diversity of ideas.
- Evidence anchors:
  - [abstract] "Consistent with previous literature, our results also suggested that the responses generated by LLMs lack diversity in comparison to humans"
  - [section] "responses generated by humans exhibited the lowest text similarity"
- Break condition: If diversity metrics are not sensitive enough to capture meaningful differences, or if humans also show low diversity in certain tasks.

## Foundational Learning
- Concept: Divergent thinking
  - Why needed here: To understand how LLMs and humans generate multiple ideas for open-ended problems.
  - Quick check question: What are the two dimensions used to rate ideas in divergent thinking tasks?
- Concept: Consensual Assessment Technique (CAT)
  - Why needed here: To understand how judges rate creativity without knowing the source of the ideas.
  - Quick check question: What is the key principle of the Consensual Assessment Technique?
- Concept: Collective creativity
  - Why needed here: To understand how the best ideas from multiple responses are compared to a group of humans.
  - Quick check question: How is the collective creativity of an LLM quantified in this study?

## Architecture Onboarding
- Component map: LLM API integration -> Task administration -> Human data collection -> Judge rating system -> Statistical analysis pipeline
- Critical path: Task administration (LLM and human) -> Judge rating -> Statistical analysis -> Collective creativity calculation
- Design tradeoffs: Trade-off between number of LLM responses (cost) and accuracy of collective creativity measure
- Failure signatures: Inconsistent judge ratings, LLM refusal to answer tasks, low inter-rater reliability
- First 3 experiments:
  1. Test if varying temperature settings significantly affects LLM creativity scores.
  2. Compare judge ratings when they know the source (LLM vs. human) vs. when they don't.
  3. Measure the impact of prompt engineering on LLM performance in creative tasks.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the creative performance of LLMs generalize across different cultural contexts beyond China?
- Basis in paper: [inferred] The study was conducted with a Chinese sample and Chinese-language tasks. The authors note that two Chinese-developed LLMs (Qwen and SparkDesk) were included to ensure language appropriateness, but no cross-cultural validation was performed.
- Why unresolved: The study's sample and task design are limited to one cultural context, raising questions about whether the observed patterns of LLM creativity (e.g., strengths in divergent thinking and problem solving, weaknesses in creative writing) hold true in other cultural settings with different educational systems, language structures, and creative norms.
- What evidence would resolve it: Replicating the study with diverse cultural samples (e.g., Western, Middle Eastern, African) using culturally adapted tasks and measuring whether the relative performance of LLMs vs. humans follows the same pattern across cultures.

### Open Question 2
- Question: How do collaborative human-AI teams perform in creative tasks compared to either humans or LLMs alone?
- Basis in paper: [explicit] The authors note that humans are still needed to evaluate and select the most creative ideas from LLM outputs, and they call for further research on the productivity effects of humans working with LLMs. They also mention that their measurement of collective creativity used a nominal group setting rather than simulating interactive group settings.
- Why unresolved: While the study shows that LLMs can generate creative ideas comparable to small groups of humans, it doesn't examine the synergistic potential of human-AI collaboration where humans might build upon, refine, or combine LLM-generated ideas in interactive ways.
- What evidence would resolve it: Experimental studies comparing the creativity outcomes of human-only groups, LLM-only groups, and mixed human-AI teams working together on creative tasks, measuring both the quantity and quality of final creative products.

### Open Question 3
- Question: What is the optimal temperature setting for maximizing creativity across different types of creative tasks?
- Basis in paper: [explicit] The authors found that temperature effects on creativity were inconsistent across tasks, with no simple rule emerging. They note that while temperature impacted diversity of responses, its relationship with creativity varied by task and model.
- Why unresolved: The study tested multiple temperature settings but found task-dependent and model-dependent optimal temperatures, leaving open questions about whether task characteristics (e.g., convergent vs. divergent thinking) or creative domains predict optimal temperature settings.
- What evidence would resolve it: Systematic experiments varying temperature across task types and creative domains, coupled with computational analysis to identify task features that predict optimal temperature settings for creativity maximization.

## Limitations
- Potential training data contamination, as only basic checks were performed to ensure tasks were not included in LLM training corpora
- Reliance on subjective human ratings for creativity assessment, though inter-rater reliability was reported as acceptable (ICC = 0.69)
- Limited generalizability due to primarily Chinese participant pool and cultural context
- Only tested five commercial LLMs with default temperature settings, leaving open questions about how different model architectures might perform

## Confidence
**High Confidence:** LLMs demonstrate superior idea generation volume and achieve median human-equivalent performance in divergent thinking and problem solving tasks. The collective creativity equivalence (8-10 humans per LLM) is well-supported by the data.

**Medium Confidence:** Claims about LLMs lagging in creative writing are supported but may be sensitive to task selection and prompt engineering. The diversity deficit of LLM outputs is observed but the practical implications for real-world applications remain unclear.

**Low Confidence:** The exact mapping between LLM responses and human equivalence (e.g., "two extra LLM responses equaling one extra human") may be sensitive to task type and evaluation methodology. Cross-cultural generalizability of the findings is uncertain.

## Next Checks
1. Conduct cross-cultural validation using diverse participant pools to test the generalizability of the human-LLM performance comparisons.
2. Perform systematic prompt engineering experiments across all 13 tasks to determine if carefully crafted prompts can reduce the observed gap in creative writing tasks.
3. Test additional LLM architectures and fine-tuned models not included in the original study to assess whether the performance patterns hold across a broader range of language models.