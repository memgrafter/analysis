---
ver: rpa2
title: Rationale-guided Prompting for Knowledge-based Visual Question Answering
arxiv_id: '2412.16936'
source_url: https://arxiv.org/abs/2412.16936
tags:
- rationale
- answer
- question
- in-context
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PLRH, a framework that Prompts Large Language
  Models with Rationale Heuristics for knowledge-based Visual Question Answering (VQA).
  The key idea is to generate intermediate thought processes (rationales) to better
  activate the reasoning capacity of LLMs.
---

# Rationale-guided Prompting for Knowledge-based Visual Question Answering

## Quick Facts
- arXiv ID: 2412.16936
- Source URL: https://arxiv.org/abs/2412.16936
- Reference count: 38
- Key outcome: PLRH framework improves knowledge-based VQA performance by 2.2% on OK-VQA and 2.1% on A-OKVQA

## Executive Summary
This paper proposes PLRH (Prompting with Rationale Heuristics), a framework that enhances knowledge-based Visual Question Answering by generating intermediate reasoning steps (rationales) to better activate the reasoning capacity of large language models (LLMs). The framework addresses the challenge that knowledge-based VQA questions require both image understanding and external knowledge, which previous LLM-based approaches struggle with due to insufficient reasoning activation. PLRH operates in three stages: generating rationales for all training samples using Chain of Thought prompting, selecting relevant in-context examples using a vanilla VQA model, and integrating rationale heuristics into prompts to guide LLMs toward correct answers. The approach demonstrates significant improvements over existing baselines on two challenging knowledge-based VQA datasets.

## Method Summary
PLRH is a three-stage framework that Prompts Large Language Models with Rationale Heuristics for knowledge-based Visual Question Answering. First, it generates rationales for all training samples using Chain of Thought prompting with manually crafted in-context examples. Second, during inference, it uses a vanilla VQA model (MCAN-large) to calculate feature similarity between test inputs and training samples, selecting the top-N most similar examples as in-context examples, and generates rationale heuristics for the test input. Third, it constructs a formatted prompt containing context, question, rationale heuristics, and in-context examples to guide the LLM (LLaMA2-Chat 7B) in predicting answers. The framework addresses the limitation of previous approaches that fail to sufficiently activate LLM reasoning capabilities for complex knowledge-based questions.

## Key Results
- PLRH outperforms existing LLM-based approaches by 2.2% on OK-VQA and 2.1% on A-OKVQA datasets
- Performance improves with more in-context examples, plateauing at 8 examples
- Combining rationale heuristics with in-context examples provides better guidance than previous approaches that use only one of these components
- The framework effectively activates LLM reasoning capabilities for knowledge-based VQA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating intermediate thought processes (rationales) improves LLM reasoning for knowledge-based VQA
- Mechanism: The framework first generates rationales for all training samples using Chain of Thought prompting, then uses these rationales as heuristics during inference to guide the LLM toward correct answers
- Core assumption: LLMs benefit from explicit reasoning steps that break down complex knowledge-based questions into logical steps
- Evidence anchors:
  - [abstract]: "We argue that prior methods do not sufficiently activate the capacities of LLMs. We propose a framework called PLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers."
  - [section]: "Instead of directly prompting the LLM to predict the answer, we generate the intermediate thought process, i.e., the rationale and use it to prompt the LLM, which is more beneficial."
- Break condition: If the rationales generated are incorrect or irrelevant to the question, they may mislead the LLM rather than help it

### Mechanism 2
- Claim: Selecting relevant in-context examples using a vanilla VQA model improves prompt effectiveness
- Mechanism: The framework uses a pretrained MCAN-large model to calculate cosine similarity between test input features and training samples, selecting the top-N most similar examples as in-context examples
- Core assumption: Similarity in the fused feature space (combining image and question representations) indicates relevance for the VQA task
- Evidence anchors:
  - [section]: "We calculate the cosine similarity of fused feature between test input and each training sample, then select top-N similar training samples as in-context examples."
  - [section]: "The fused feature f of the imageùë£and questionùëû can be obtained through the backbone ofùïÑ."
- Break condition: If the vanilla VQA model's feature representation is poor or if the similarity metric doesn't correlate with actual relevance, the wrong examples may be selected

### Mechanism 3
- Claim: Combining rationale heuristics with in-context examples in the prompt format provides better guidance than previous approaches
- Mechanism: The framework formats prompts with Context-Question-Rationale-Answer for in-context examples and Context-Question-Rationale for the test input, allowing the LLM to reason through the problem using both examples and rationales
- Core assumption: The prompt format with rationales provides richer context than formats without rationales, enabling better reasoning
- Evidence anchors:
  - [section]: "We construct the prompt to inspire the LLM to predict the answer. The prompt format is as follows: Prompt head Context: ùëêùëñ ‚àñn Question:ùëûùëñ ‚àñn Rationale:ùëüùëñ ‚àñn Answer:ùëéùëñ Context: ùëê ‚àñn Question:ùëû ‚àñn Rationale:ùëü ‚àñn Answer:"
  - [section]: "Our approach outperforms both LLM-based approaches and other baselines... Our approach not only combines PromptCap's captioning method, and Prophet's in-context example selection, but also employs a novel rationale heuristic that further activate the capability of LLM."
- Break condition: If the rationale heuristics are too generic or don't add meaningful reasoning steps, they may not provide additional benefit over simpler prompt formats

## Foundational Learning

- Concept: Chain of Thought (CoT) reasoning
  - Why needed here: The framework relies on generating intermediate reasoning steps to improve LLM performance on complex knowledge-based questions
  - Quick check question: What is the difference between direct prompting and CoT prompting for LLMs?

- Concept: In-context learning with LLMs
  - Why needed here: The framework uses in-context examples to demonstrate the task format and provide relevant context for the LLM
  - Quick check question: How does the number of in-context examples typically affect LLM performance?

- Concept: Feature similarity and retrieval
  - Why needed here: The framework uses a vanilla VQA model to retrieve relevant in-context examples based on feature similarity
  - Quick check question: What distance metric is used to measure similarity between feature representations?

## Architecture Onboarding

- Component map: Image ‚Üí Captioning Model ‚Üí Captions ‚Üí LLM (Stage 1) ‚Üí Rationales ‚Üí Vanilla VQA Model ‚Üí Feature Similarity ‚Üí In-context Examples ‚Üí LLM (Stage 2) ‚Üí Rationale Heuristics ‚Üí Formatted Prompt ‚Üí LLM (Stage 3) ‚Üí Answer

- Critical path:
  1. Image ‚Üí Caption (Captioning model)
  2. Caption + Question + Answer ‚Üí Rationale (LLM, Stage 1)
  3. Test Image + Question ‚Üí Features (Vanilla VQA model)
  4. Feature similarity ‚Üí In-context examples selection
  5. Test Caption + Question ‚Üí Rationale (LLM, Stage 2)
  6. In-context examples + Test Caption + Question + Rationale ‚Üí Answer (LLM, Stage 3)

- Design tradeoffs:
  - Number of in-context examples (8 chosen for context length constraints)
  - Choice of vanilla VQA model for feature extraction
  - Captioning model quality affecting rationale generation
  - LLM size vs. computational resources (7B LLaMA2-Chat)

- Failure signatures:
  - Poor performance on OK-VQA/A-OKVQA datasets
  - Inconsistent rationale generation across similar questions
  - Failure to select relevant in-context examples
  - Over-reliance on image captions rather than external knowledge

- First 3 experiments:
  1. Ablation study: Remove rationale generation and test baseline performance
  2. Sensitivity analysis: Vary number of in-context examples (1, 2, 4, 6, 8)
  3. Case study analysis: Manually examine success and failure cases to identify patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PLRH vary with different sizes of the LLM, and what is the optimal size for this task?
- Basis in paper: [inferred] The paper mentions using LLaMA2-Chat with 7B parameters and notes that LLMs with massive parameters have powerful capabilities. However, it does not explore the impact of varying LLM sizes on performance.
- Why unresolved: The paper does not provide a systematic study on the effect of different LLM sizes on the performance of PLRH.
- What evidence would resolve it: Experimental results comparing PLRH's performance across different LLM sizes (e.g., 7B, 13B, 70B parameters) would clarify the optimal model size for this task.

### Open Question 2
- Question: How does the choice of in-context examples affect the rationale generation and subsequent answer prediction in PLRH?
- Basis in paper: [explicit] The paper discusses the selection of in-context examples using a vanilla VQA model and notes that performance improves with more examples, but it does not explore the impact of different selection strategies or the quality of examples on rationale generation.
- Why unresolved: The paper does not investigate how varying the selection strategy or quality of in-context examples influences the rationale generation and final answer accuracy.
- What evidence would resolve it: Comparative experiments using different in-context example selection strategies or varying the quality of examples would demonstrate their impact on rationale generation and answer prediction.

### Open Question 3
- Question: Can the rationale heuristics generated by PLRH be applied to other multimodal reasoning tasks beyond VQA, such as visual reasoning or image captioning?
- Basis in paper: [inferred] The paper focuses on knowledge-based VQA and demonstrates the effectiveness of rationale heuristics in this domain. However, it does not explore the generalizability of this approach to other multimodal tasks.
- Why unresolved: The paper does not test the applicability of rationale heuristics to other multimodal reasoning tasks, leaving open the question of whether this approach is broadly applicable.
- What evidence would resolve it: Applying PLRH to other multimodal tasks and comparing its performance with existing methods would indicate the generalizability of rationale heuristics.

## Limitations

- Dependence on high-quality image captions as input, as the framework relies on PromptCap to convert images to text before generating rationales
- Substantial computational resources required for both training the vanilla VQA model and running the LLM for rationale generation across all training samples
- Effectiveness depends heavily on the quality of manually crafted in-context examples used in Stage 1, which may not generalize well to all knowledge domains

## Confidence

- High Confidence: The mechanism of using in-context examples for few-shot learning with LLMs is well-established and the implementation details are clearly specified
- Medium Confidence: The rationale generation approach using Chain of Thought prompting is reasonable but may not always produce optimal intermediate reasoning steps for complex knowledge-based questions
- Medium Confidence: The feature similarity-based in-context example selection method is sound but depends on the quality of the vanilla VQA model's representations

## Next Checks

1. **Ablation Study Validation**: Remove the rationale generation component entirely and test whether the framework performs comparably using only in-context examples, to quantify the actual contribution of rationale heuristics

2. **Caption Quality Sensitivity**: Test the framework's performance using different captioning models (e.g., BLIP-2 vs PromptCap) to determine how sensitive the approach is to captioning quality

3. **Knowledge Domain Generalization**: Evaluate the framework on a new knowledge-based VQA dataset outside the OK-VQA/A-OKVQA domain to assess whether the rationale heuristics generalize beyond the training data distribution