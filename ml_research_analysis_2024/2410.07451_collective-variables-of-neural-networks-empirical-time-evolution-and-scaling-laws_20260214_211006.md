---
ver: rpa2
title: 'Collective variables of neural networks: empirical time evolution and scaling
  laws'
arxiv_id: '2410.07451'
source_url: https://arxiv.org/abs/2410.07451
tags:
- network
- neural
- learning
- training
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel method to understand learning dynamics\
  \ and scaling behavior in neural networks by analyzing the empirical Neural Tangent\
  \ Kernel (NTK). The authors propose two collective variables\u2014entropy and trace\
  \ of the NTK\u2014as measures to quantify the diversity of network updates and effective\
  \ learning rates."
---

# Collective variables of neural networks: empirical time evolution and scaling laws

## Quick Facts
- arXiv ID: 2410.07451
- Source URL: https://arxiv.org/abs/2410.07451
- Reference count: 20
- Introduces collective variables (entropy and NTK trace) to analyze neural network learning dynamics and scaling behavior

## Executive Summary
This paper presents a novel framework for understanding neural network learning dynamics through the lens of empirical Neural Tangent Kernel (NTK) analysis. The authors identify two key collective variables - entropy of parameter updates and trace of the NTK - that capture fundamental aspects of how networks learn across different scales. Through extensive experiments spanning multiple architectures including transformers, graph neural networks, auto-encoders, and reinforcement learning models, they demonstrate that small networks primarily compress information while large networks form structured representations. This empirical finding provides new insights into the relationship between network size and learning mechanisms.

## Method Summary
The authors analyze neural network training dynamics by examining the empirical NTK, which measures how output changes with respect to parameter updates. They define two collective variables: the entropy of parameter updates (measuring diversity of learning directions) and the trace of the NTK (measuring effective learning rate). By tracking these variables across training time and different network scales, they identify distinct learning regimes. The methodology involves computing empirical NTK matrices at various training stages, calculating entropy from the distribution of eigenvalues, and monitoring the trace as a proxy for parameter sensitivity. This approach provides a unified framework for comparing learning dynamics across diverse architectures without requiring architecture-specific modifications.

## Key Results
- Small networks exhibit information compression (entropy decrease) while large networks show structure formation (entropy increase)
- NTK trace increases monotonically during training, indicating growing sensitivity to parameter updates
- The transition from compression to structure formation occurs at specific network sizes, suggesting a threshold for "deep learning" emergence
- These patterns hold consistently across transformers, GNNs, auto-encoders, and RL models

## Why This Works (Mechanism)
The proposed mechanism relies on the empirical NTK capturing the effective curvature of the loss landscape in function space. When networks are small, the parameter space is highly constrained, forcing updates to occur in directions that reduce redundancy and compress information. As networks grow larger, they gain representational capacity that allows exploration of structured directions in function space, leading to entropy increase. The NTK trace growth reflects the accumulation of information and increasing sensitivity to parameter changes as the network develops more complex representations.

## Foundational Learning

**Neural Tangent Kernel (NTK)**
- *Why needed*: Provides a framework for analyzing neural network training as a kernel regression problem in function space
- *Quick check*: NTK approximates the behavior of infinitely wide networks trained with gradient descent

**Empirical NTK Computation**
- *Why needed*: Allows analysis of finite-width networks where theoretical NTK approximations may break down
- *Quick check*: Empirical NTK can be computed via finite differences or automatic differentiation

**Eigenvalue Decomposition of NTK**
- *Why needed*: Reveals the spectrum of learning directions and their relative importance
- *Quick check*: Dominant eigenvalues correspond to directions of greatest parameter sensitivity

## Architecture Onboarding

**Component Map**
NTK computation -> Entropy calculation -> Trace monitoring -> Learning regime identification

**Critical Path**
1. Initialize network with random parameters
2. Compute empirical NTK at initialization and throughout training
3. Calculate entropy from NTK eigenvalue distribution
4. Track NTK trace evolution
5. Classify learning regime based on entropy and trace patterns

**Design Tradeoffs**
The empirical approach sacrifices theoretical rigor for practical applicability across architectures. Computing full NTK matrices is computationally expensive but provides rich information about learning dynamics.

**Failure Signatures**
If entropy and trace patterns do not match expected regimes, it may indicate optimization issues, inappropriate initialization, or architectural constraints preventing normal learning dynamics.

**First Experiments**
1. Verify NTK trace growth on a simple MLP trained on MNIST
2. Compare entropy trajectories for small vs. large transformer models
3. Test whether the compression-structure transition persists with different activation functions

## Open Questions the Paper Calls Out
None

## Limitations
The study's empirical nature means the proposed collective variables lack rigorous theoretical justification for why they capture fundamental learning dynamics. The relationship between network size and learning mechanism, while supported by evidence, remains correlative rather than causative. The framework does not account for how optimization hyperparameters or initialization schemes might independently influence the collective variables.

## Confidence

**High confidence**: Empirical observations of NTK trace growth during training; consistent patterns of entropy change across architectures

**Medium confidence**: The dichotomy between information compression and structure formation; the link between large-scale structure formation and "deep learning"

**Low confidence**: Theoretical foundations for why these specific collective variables capture learning dynamics; causal relationship between network size and learning mechanism

## Next Checks
1. Test whether the observed entropy and NTK trace patterns persist under different optimization algorithms (Adam, SGD with momentum, etc.)
2. Validate the collective variable framework on emerging architectures like state-space models (S4, Mamba) and vision transformers
3. Conduct ablation studies to isolate the effects of initialization, batch size, and learning rate schedules on the proposed collective variables