---
ver: rpa2
title: Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Reinforcement
  Learning
arxiv_id: '2411.12155'
source_url: https://arxiv.org/abs/2411.12155
tags:
- action
- learning
- sequence
- tasks
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Coarse-to-fine Q-Network with Action Sequence
  (CQN-AS), a novel value-based reinforcement learning algorithm that learns a critic
  network outputting Q-values over sequences of actions. The key insight is that using
  action sequences in value learning can enhance performance, as demonstrated by improved
  validation loss in return-to-go prediction experiments.
---

# Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2411.12155
- **Source URL**: https://arxiv.org/abs/2411.12155
- **Authors**: Younggyo Seo; Pieter Abbeel
- **Reference count**: 40
- **Primary result**: CQN-AS achieves superior performance on sparse-reward robotic control tasks by learning Q-values over action sequences, particularly excelling in long-horizon tasks where other RL algorithms fail.

## Executive Summary
This paper introduces Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a novel value-based reinforcement learning algorithm that extends the coarse-to-fine Q-network framework by learning Q-values over sequences of actions rather than single actions. The key insight is that using action sequences in value learning can significantly enhance performance by capturing behavioral primitives and reducing validation loss in return-to-go prediction tasks. CQN-AS addresses the value overestimation issues that plague actor-critic methods when trained with action sequences by adopting a critic-only architecture. The algorithm demonstrates state-of-the-art performance on challenging sparse-reward humanoid control and tabletop manipulation tasks from BiGym and RLBench benchmarks, particularly excelling in long-horizon tasks where other RL algorithms fail.

## Method Summary
CQN-AS trains a critic network that outputs Q-values over sequences of actions by aggregating observation features across time steps using a GRU-based recurrent network, then computing Q-values for multiple coarse-to-fine discretization levels and sequence steps in parallel. The algorithm uses N-step returns with target networks for training, temporal ensemble for action execution to smooth movements, and incorporates auxiliary behavioral cloning loss with demonstration data stored in a replay buffer. Unlike actor-critic methods, CQN-AS avoids value overestimation by being a critic-only algorithm that selects discrete actions with highest Q-values without a separate actor network that could exploit value function errors.

## Key Results
- CQN-AS significantly outperforms baselines on sparse-reward humanoid control and tabletop manipulation tasks from BiGym and RLBench benchmarks
- The algorithm excels in challenging long-horizon tasks where other RL algorithms fail due to value overestimation issues
- Return-to-go prediction experiments show that using action sequences (at:t+K={at, ..., at+K-1}) results in lower validation loss compared to single-step actions
- CQN-AS avoids the severe value overestimation that plagues actor-critic methods when trained with action sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using action sequences in value learning reduces validation loss compared to single-step actions
- Mechanism: Action sequences capture behavioral primitives (like "go straight") that make it easier for the model to learn long-term consequences
- Core assumption: The ground-truth return-to-go prediction task benefits from action sequences because they represent more meaningful behavioral units
- Evidence anchors:
  - [abstract]: "we find that using an action sequence at:t+K={at, ..., at+K−1} as input results in lower validation loss"
  - [section]: "we train regression models that predict the ground-truth return-to-go... using an action sequence... results in lower validation losses"
- Break condition: If action sequences don't correspond to meaningful behavioral primitives or if the task requires fine-grained reactive control

### Mechanism 2
- Claim: CQN-AS avoids value overestimation by being a critic-only algorithm
- Mechanism: Without a separate actor network that can exploit value function errors, CQN-AS selects discrete actions with highest Q-values without amplifying estimation errors
- Core assumption: Actor-critic algorithms with action sequences suffer from value overestimation due to wider action spaces making critics more vulnerable to function approximation error
- Evidence anchors:
  - [abstract]: "avoiding value overestimation issues that plague actor-critic methods when trained with action sequences"
  - [section]: "we find these algorithms with action sequences suffer from severe value overestimation... This is because a wider action space makes the critic more vulnerable to function approximation error"
- Break condition: If the discretization becomes too coarse or if the action space is already high-dimensional

### Mechanism 3
- Claim: Coarse-to-fine discretization with action sequences enables effective learning on challenging robotic tasks
- Mechanism: Progressive zooming into action space with sequence-level Q-values allows the agent to learn useful value functions by explicitly learning consequences of action sequences
- Core assumption: The coarse-to-fine discretization framework can be generalized from single actions to action sequences
- Evidence anchors:
  - [abstract]: "CQN-AS extends the coarse-to-fine Q-network framework by training the critic to explicitly learn the consequences of executing action sequences"
  - [section]: "we introduce Coarse-to-fine Q-Network with Action Sequence (CQN-AS), which learns a critic network that outputs Q-values over a sequence of actions"
- Break condition: If the sequence length becomes too long or if the task requires highly reactive control

## Foundational Learning

- Concept: Value-based reinforcement learning and Q-learning
  - Why needed here: The entire algorithm is built on learning Q-values for action sequences
  - Quick check question: What is the difference between a value function and a Q-function?

- Concept: Coarse-to-fine discretization and hierarchical action selection
  - Why needed here: CQN-AS builds upon the coarse-to-fine framework by extending it to action sequences
  - Quick check question: How does the coarse-to-fine approach reduce the computational complexity of continuous control?

- Concept: Actor-critic vs critic-only algorithms
  - Why needed here: Understanding why CQN-AS avoids value overestimation requires knowing the difference between these approaches
  - Quick check question: What vulnerability do actor-critic methods have that critic-only methods avoid?

## Architecture Onboarding

- Component map: Observation encoding → Feature aggregation → Q-value computation → Action selection → Environment interaction → Data storage
- Critical path: Observation encoding → Feature aggregation → Q-value computation → Action selection → Environment interaction → Data storage
- Design tradeoffs:
  - Longer action sequences improve performance but increase computational cost and memory requirements
  - More levels in coarse-to-fine discretization improve precision but slow down inference
  - Using temporal ensemble smooths actions but can reduce reactivity
- Failure signatures:
  - Value overestimation and random near-zero performance (actor-critic with action sequences)
  - Degraded performance with too high N-step return (increased variance)
  - Poor performance on torque control tasks (sequences of joint positions have clearer structure than raw torques)
- First 3 experiments:
  1. Test return-to-go prediction with different action sequence lengths on humanoid demonstrations
  2. Compare CQN-AS vs CQN on a simple sparse-reward task from BiGym
  3. Evaluate the effect of temporal ensemble magnitude on a fine-grained control task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do action sequences perform in torque control tasks compared to position control tasks?
- Basis in paper: [inferred] The paper notes that CQN-AS underperforms CQN on torque control tasks from DeepMind Control Suite, hypothesizing that sequences of joint positions have clearer semantic structure than raw torques.
- Why unresolved: The paper provides a hypothesis but does not empirically test alternative architectures or representations that might improve torque control performance.
- What evidence would resolve it: Systematic experiments comparing different action representations (e.g., torque sequences vs. position sequences) and architectures for torque control tasks.

### Open Question 2
- Question: What is the optimal action sequence length for different types of robotic tasks and control modes?
- Basis in paper: [explicit] The paper shows that sequence length affects performance, with longer sequences generally improving results but potentially degrading performance if too long, and provides ablation studies on sequence length.
- Why unresolved: The paper uses fixed sequence lengths (16 for BiGym, 4 for RLBench) without exploring task-specific optimization, and the relationship between task complexity, control mode, and optimal sequence length remains unclear.
- What evidence would resolve it: Systematic experiments varying sequence lengths across different task types and control modes to identify optimal sequence lengths.

### Open Question 3
- Question: Can CQN-AS be effectively combined with offline RL techniques and large pre-trained vision encoders?
- Basis in paper: [explicit] The paper mentions preliminary experiments combining CQN-AS with Cal-QL for offline RL, showing promising results, and notes that using ResNet-18 encoders requires prohibitive computational resources.
- Why unresolved: The paper only provides preliminary offline RL results and does not explore combinations with other offline RL algorithms or investigate efficient ways to incorporate large vision encoders.
- What evidence would resolve it: Comprehensive experiments combining CQN-AS with various offline RL algorithms and benchmarks using efficient vision encoder integration techniques.

## Limitations
- The evaluation focuses primarily on success rates rather than analyzing the quality of learned policies or sample efficiency metrics beyond the reported numbers
- The paper doesn't provide a clear explanation of why action sequences work better than single actions beyond empirical observations
- The connection between behavioral primitives and return-to-go prediction is asserted but not rigorously established

## Confidence
- Mechanism 1 (validation loss reduction): Medium
- Mechanism 2 (avoiding value overestimation): Medium
- Mechanism 3 (coarse-to-fine effectiveness): Medium

## Next Checks
1. Conduct ablation studies varying sequence length K to determine the optimal trade-off between performance and computational cost
2. Compare CQN-AS against other data-efficient RL methods like Muesli or DreamT on the same benchmarks to establish relative sample efficiency
3. Analyze the learned Q-values to verify they capture meaningful temporal abstractions rather than just memorizing demonstration data