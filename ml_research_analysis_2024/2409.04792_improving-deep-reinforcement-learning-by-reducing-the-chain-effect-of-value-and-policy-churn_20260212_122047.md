---
ver: rpa2
title: Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value
  and Policy Churn
arxiv_id: '2409.04792'
source_url: https://arxiv.org/abs/2409.04792
tags:
- churn
- policy
- value
- chain
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHAIN, a method to reduce the chain effect
  of value and policy churn in deep reinforcement learning (DRL). The authors characterize
  churn in the context of Generalized Policy Iteration and identify how value and
  policy churn compound, leading to learning issues such as greedy action deviation,
  trust region violation, and dual bias of policy value.
---

# Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn

## Quick Facts
- arXiv ID: 2409.04792
- Source URL: https://arxiv.org/abs/2409.04792
- Authors: Hongyao Tang; Glen Berseth
- Reference count: 40
- Key outcome: CHAIN reduces value and policy churn in DRL, improving learning performance across online/offline, value-based/policy-based methods and scaling scenarios.

## Executive Summary
This paper addresses a critical issue in deep reinforcement learning: the compounding effect of value and policy churn during training. The authors characterize churn within the Generalized Policy Iteration framework and demonstrate how it leads to greedy action deviation, trust region violations, and dual bias of policy value. They introduce CHAIN, a simple regularization method that minimizes changes in target values for a separate reference batch during both value and policy updates. CHAIN is easy to implement, plug-and-play compatible with most DRL algorithms, and shows significant improvements in sample efficiency and final performance across diverse settings including scaling experiments with wider and deeper networks.

## Method Summary
CHAIN adds regularization terms to standard DRL objectives that penalize changes in network outputs on a separate reference batch. During each update, the algorithm samples a training batch for the primary loss and a reference batch for churn reduction. For value-based methods, it adds a value churn reduction (VCR) term that minimizes the squared difference between current Q-values and previous Q-values on the reference batch. For policy-based methods, it adds a policy churn reduction (PCR) term that minimizes the squared difference between current policy distributions and previous policy distributions. The coefficients for these regularization terms can be set manually or adjusted automatically based on the relative scales of the primary and regularization losses.

## Key Results
- CHAIN