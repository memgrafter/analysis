---
ver: rpa2
title: Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology
arxiv_id: '2404.10242'
source_url: https://arxiv.org/abs/2404.10242
tags:
- training
- learning
- vit-l
- images
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the scaling properties of self-supervised
  masked autoencoders (MAEs) versus weakly supervised learning (WSL) for learning
  representations from large-scale microscopy datasets. By pretraining increasingly
  larger MAE and WSL models on datasets ranging from 2 million to 93 million images,
  the authors demonstrate that MAEs, especially ViT-based architectures, outperform
  WSL models at inferring known biological relationships from gene perturbation screens.
---

# Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology

## Quick Facts
- arXiv ID: 2404.10242
- Source URL: https://arxiv.org/abs/2404.10242
- Reference count: 40
- Primary result: MAEs, especially ViT-based architectures, outperform WSL models at inferring biological relationships from gene perturbation screens

## Executive Summary
This work investigates the scaling properties of self-supervised masked autoencoders (MAEs) versus weakly supervised learning (WSL) for learning representations from large-scale microscopy datasets. By pretraining increasingly larger MAE and WSL models on datasets ranging from 2 million to 93 million images, the authors demonstrate that MAEs, especially ViT-based architectures, outperform WSL models at inferring known biological relationships from gene perturbation screens. Key findings include: (1) MAE recall of biological relationships scales with model and dataset size, while WSL performance degrades on larger datasets; (2) adding a Fourier domain reconstruction loss stabilizes training of large MAE ViTs; and (3) a novel channel-agnostic MAE architecture (CA-MAE) generalizes to datasets with different channel structures.

## Method Summary
The authors pretrain MAEs and WSL models of increasing sizes on five high-content screening microscopy datasets ranging from 2M to 93M images. MAEs use vision transformer backbones with large patch sizes (8x8) and high mask ratios (75%), combined with a Fourier domain reconstruction loss for large models. The channel-agnostic architecture treats each channel as a separate modality with shared projections. Models are evaluated on recall of known biological relationships from protein interaction databases, perturbation retrieval on out-of-domain datasets, and prediction of morphological features. TVN batch correction is applied to align embeddings across experiments.

## Key Results
- MAE recall of biological relationships scales with model and dataset size, while WSL performance degrades on larger datasets
- Adding Fourier domain reconstruction loss stabilizes training of large MAE ViTs
- CA-MAE architecture generalizes to datasets with different channel structures
- Best model (ViT-L/8+ MAE on 93M images) achieves up to 11.5% relative improvement over WSL in recalling biological relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling up MAE models improves recall of biological relationships in gene perturbation screens.
- Mechanism: Larger MAE models trained on more data learn richer representations of cellular morphology, enabling better identification of biological similarities between perturbations.
- Core assumption: Image reconstruction from masked patches captures sufficient biological signal to differentiate perturbation types.
- Evidence anchors:
  - [abstract] "MAEs, especially ViT-based architectures, outperform WSL models at inferring known biological relationships from gene perturbation screens."
  - [section] "As previously described, we train MU-Nets and MAE ViTs of various sizes on increasingly larger datasets. Table 3 shows that MAEs outperform the pretrained ImageNet and WSL models, especially when we scale up to larger model and training set sizes."
  - [corpus] Weak evidence; no direct citations on MAE scaling in microscopy found.
- Break condition: If larger models overfit to noise or fail to generalize to new datasets with different channel structures.

### Mechanism 2
- Claim: Fourier domain reconstruction loss stabilizes training of large MAE ViTs.
- Mechanism: Adding frequency-domain loss terms helps the model learn to reconstruct high-frequency cellular textures that are biologically meaningful.
- Core assumption: High-frequency image components contain critical biological information about cellular morphology.
- Evidence anchors:
  - [section] "We therefore added an additional reconstruction loss in the Fourier domain [67] to encourage the model to better reconstruct the textures of cellular morphology, which also facilitated more reliable navigation of the loss landscape for reconstruction in general."
  - [section] "We found that setting α = 0.01 worked effectively. As illustrated in Figure 3, we found that training with this loss term consistently resulted in a stable double-descent in loss."
  - [corpus] No direct evidence found in corpus; this appears to be a novel contribution.
- Break condition: If the Fourier loss term becomes computationally prohibitive or degrades reconstruction quality.

### Mechanism 3
- Claim: Channel-agnostic MAE architecture generalizes to datasets with different channel configurations.
- Mechanism: Treating each channel as a separate modality with shared projections allows the model to handle varying numbers and types of channels at inference.
- Core assumption: Cellular information is distributed across channels in a way that can be learned independently and then combined.
- Evidence anchors:
  - [section] "Our implementation treats each channel as a separate modality, creating C × N tokens where C is the number of channels and N is the number of patches... To make the model agnostic to the number and set of channels at test time, we apply a single shared linear projection and the same positional embeddings to all channels."
  - [section] "Table 3 shows results for three channel-agnostic MAEs... CA-MAE ViT-B significantly outperforms the MAE ViT-B/16 when trained on RPI-52M, suggesting that these architectures can offer improved performance over standard MAE ViTs."
  - [corpus] Weak evidence; only one related paper found but not directly addressing channel-agnostic MAEs.
- Break condition: If the model fails to capture cross-channel dependencies or if modality-specific information is lost.

## Foundational Learning

- Concept: Self-supervised learning (SSL)
  - Why needed here: SSL allows learning representations from unlabeled microscopy images, which are abundant but lack manual annotations.
  - Quick check question: How does SSL differ from supervised learning in terms of data requirements?

- Concept: Masked autoencoders (MAEs)
  - Why needed here: MAEs learn to reconstruct masked image patches, capturing spatial relationships and morphological features important for biological analysis.
  - Quick check question: What is the main difference between MAEs and traditional autoencoders?

- Concept: Vision transformers (ViTs)
  - Why needed here: ViTs can handle large input patches and scale well with dataset size, making them suitable for massive HCS datasets.
  - Quick check question: How do ViTs differ from convolutional neural networks in processing image data?

## Architecture Onboarding

- Component map:
  Input -> Encoder (ViT-L/8+ or CA-ViT-L/16+) -> Decoder -> Output embeddings
  Additional Fourier loss computation and combination

- Critical path:
  1. Sample random crops from training images
  2. Apply masking and channel-wise standardization
  3. Pass through encoder to get embeddings
  4. Decode masked patches and apply reconstruction loss
  5. Compute Fourier loss on masked patches
  6. Backpropagate combined loss to update model weights

- Design tradeoffs:
  - Large patch size (8x8) vs. computational cost
  - High mask ratio (75%) vs. reconstruction difficulty
  - Fourier loss weight (α=0.01) vs. stability vs. accuracy

- Failure signatures:
  - Training loss plateaus or diverges early
  - Reconstructions lack cellular texture details
  - Poor transfer to datasets with different channel structures

- First 3 experiments:
  1. Train a small MAE (ViT-S/16) on RxRx3 and evaluate on RxRx3 gene knockouts
  2. Add Fourier domain loss to the small MAE and compare training stability and downstream performance
  3. Implement channel-agnostic architecture and test on a dataset with different channel configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the improved performance of MAEs over WSL models on inferring biological relationships translate to downstream biological discovery tasks like identifying novel drug targets or mechanisms of action?
- Basis in paper: [inferred] The paper shows MAEs outperform WSL in recalling known biological relationships and predicting morphological features, but does not evaluate on novel discovery tasks.
- Why unresolved: The paper focuses on benchmarking against known relationships and morphological feature prediction, leaving the question of real-world biological discovery impact open.
- What evidence would resolve it: Experimental validation showing MAE-based embeddings lead to novel biological insights or discoveries not captured by WSL models.

### Open Question 2
- Question: How do the performance differences between MAE and WSL models change when evaluated on datasets with different cellular perturbation types (e.g., RNAi, CRISPR, small molecules) and experimental conditions?
- Basis in paper: [explicit] The paper trains and evaluates on datasets with varying perturbation types (siRNA, CRISPR, compounds) and experimental conditions, but does not systematically compare model performance across these variations.
- Why unresolved: While the paper includes datasets with different perturbation types, it does not provide a comprehensive analysis of how model performance varies across these types.
- What evidence would resolve it: Systematic evaluation of MAE and WSL models across diverse perturbation types and experimental conditions, quantifying performance differences.

### Open Question 3
- Question: Can the channel-agnostic MAE architecture be further improved to handle datasets with even more diverse channel configurations, such as those with different numbers of channels or channel-specific augmentations?
- Basis in paper: [explicit] The paper introduces a channel-agnostic MAE architecture that generalizes to datasets with different channel structures, but does not explore its limits or potential improvements.
- Why unresolved: The paper demonstrates the effectiveness of the channel-agnostic MAE on a limited set of channel configurations, leaving questions about its scalability and robustness to more diverse datasets.
- What evidence would resolve it: Evaluation of the channel-agnostic MAE on datasets with a wider range of channel configurations, including those with varying numbers of channels and channel-specific augmentations, to assess its performance and identify potential improvements.

## Limitations
- Relies on proprietary datasets (RPI-52M, RPI-93M) that cannot be publicly accessed, limiting reproducibility
- Biological relationship recall metrics depend on completeness and accuracy of reference databases
- Channel-agnostic architecture assumes channels can be treated as independent modalities

## Confidence
- **High confidence**: MAE scaling improvements on recall metrics when compared to fixed-size WSL models
- **Medium confidence**: Effectiveness of Fourier domain reconstruction loss for stabilizing large ViT training
- **Medium confidence**: Channel-agnostic architecture generalization

## Next Checks
1. **Ablation study on Fourier loss**: Systematically vary the Fourier loss weight α (0.001, 0.01, 0.1) and compare training stability and downstream recall performance across different ViT sizes to confirm optimal hyperparameters.

2. **Cross-dataset generalization test**: Evaluate CA-MAE on a third microscopy dataset with significantly different channel composition (e.g., single-channel brightfield or fluorescence-only) to test true channel-agnostic capabilities beyond the tested scenarios.

3. **Computational efficiency analysis**: Measure and compare FLOPs, memory usage, and training time per epoch between MAE and WSL approaches across model sizes to quantify the practical scalability advantages beyond accuracy metrics.