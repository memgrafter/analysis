---
ver: rpa2
title: 'Crocodile: Cross Experts Covariance for Disentangled Learning in Multi-Domain
  Recommendation'
arxiv_id: '2405.12706'
source_url: https://arxiv.org/abs/2405.12706
tags:
- learning
- crocodile
- domain
- experts
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the dilemma in multi-domain recommendation
  learning where models must preserve domain distinctiveness while ensuring sufficient
  parameter learning, especially for domains with limited data. The authors propose
  Crocodile, which employs multiple shared embeddings with a Cross-experts Covariance
  Loss (CovLoss) to disentangle them and preserve diverse user interests across domains.
---

# Crocodile: Cross Experts Covariance for Disentangled Learning in Multi-Domain Recommendation

## Quick Facts
- **arXiv ID**: 2405.12706
- **Source URL**: https://arxiv.org/abs/2405.12706
- **Reference count**: 40
- **Primary result**: Crocodile achieves 0.72% CTR lift and 0.73% GMV lift in online A/B testing on Tencent's advertising platform

## Executive Summary
This paper addresses the fundamental dilemma in multi-domain recommendation learning where models must preserve domain distinctiveness while ensuring sufficient parameter learning, especially for domains with limited data. The authors propose Crocodile, which employs multiple shared embeddings with a Cross-experts Covariance Loss (CovLoss) to disentangle them and preserve diverse user interests across domains. The method also introduces a Prior Informed element-wise Gating (PeG) mechanism to route between embeddings and domain-specific towers. Crocodile outperforms state-of-the-art methods on two public datasets and demonstrates significant improvements in online A/B testing on Tencent's advertising platform.

## Method Summary
Crocodile addresses the multi-domain recommendation challenge by using multiple shared embedding tables instead of domain-specific embeddings to avoid data sparsity issues. It employs a Cross-experts Covariance Loss (CovLoss) that computes covariance between all pairs of expert outputs to explicitly disentangle representations among experts. The Prior Informed element-wise Gating (PeG) mechanism uses prior features (user ID, item ID, domain ID) to control element-wise gating between these shared embeddings and domain-specific towers. The model is trained with a combined BCE loss and CovLoss, where the covariance loss promotes orthogonality among expert representations and prevents dimensional collapse while maintaining domain distinctiveness.

## Key Results
- Achieves 0.09% and 0.19% improvements in AUC and gAUC respectively on the Kuairand1k dataset
- Demonstrates 0.138% and 0.12% improvements in AUC and gAUC on the AliCCP dataset
- Shows 0.72% CTR lift and 0.73% GMV lift in online A/B testing on Tencent's advertising platform
- Maintains advantages even with fewer embedding sets (2) compared to other methods with more (5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-expert Covariance Loss (CovLoss) prevents dimensional collapse by promoting orthogonality among expert representations
- Mechanism: CovLoss computes covariance between all pairs of expert outputs and penalizes high covariance values. This forces each expert to learn distinct, non-overlapping representations of user interests across domains.
- Core assumption: Lower covariance between expert outputs leads to better preservation of domain-specific user interests and prevents all experts from collapsing to similar representations.
- Evidence anchors:
  - [abstract] "a covariance loss upon these embeddings to disentangle them, enabling the model to capture diverse user interests among domains"
  - [section] "a novel cross-expert covariance loss (CovLoss) to explicitly disentangle representations among experts"
  - [corpus] Weak - corpus neighbors focus on different aspects of multi-domain learning, not covariance-based disentanglement
- Break condition: If the covariance calculation becomes numerically unstable or if domains have highly correlated user interests that should be preserved together

### Mechanism 2
- Claim: Multiple shared embeddings with Prior Informed element-wise Gating (PeG) enable sufficient learning while preserving domain distinctiveness
- Mechanism: Instead of domain-specific embeddings that suffer from data scarcity, Crocodile uses shared embeddings optimized across all domains. PeG uses prior features (user ID, item ID, domain ID) to control element-wise gating between these shared embeddings and domain-specific towers, routing domain-relevant information while maintaining parameter efficiency.
- Core assumption: Shared embeddings can learn robust representations when trained on all domain data, and gating mechanisms can route appropriate information to domain-specific towers without requiring separate embeddings per domain.
- Evidence anchors:
  - [abstract] "employs multiple embedding tables to make the model domain-aware at the embeddings which consist most parameters in the model"
  - [section] "a novel Prior Informed element-wise Gating (PeG) mechanism was proposed to route between these embeddings and domain-specific classification towers"
  - [corpus] Missing - corpus doesn't provide direct evidence for this specific multi-embedding with gating approach
- Break condition: If prior features are not informative enough to distinguish domain-specific interests, or if the gating becomes too complex to train effectively

### Mechanism 3
- Claim: The combination of CovLoss and PeG resolves the dilemma between preserving domain distinctiveness and ensuring sufficient parameter learning
- Mechanism: CovLoss ensures experts learn diverse representations that capture different user interests, while PeG routes these diverse representations to appropriate domain towers. This allows the model to maintain distinctiveness without requiring domain-specific embeddings that suffer from data sparsity.
- Core assumption: The interplay between disentangled expert representations and intelligent gating can achieve both goals simultaneously, outperforming approaches that focus on only one aspect.
- Evidence anchors:
  - [abstract] "enabling the model to capture diverse user interests among domains" and "resolves the dilemma of preserving domain distinctiveness v.s. sufficient parameters learning"
  - [section] "Empirical analysis conducted on two public datasets demonstrates that Crocodile achieves state-of-the-art results"
  - [corpus] Weak - corpus neighbors don't address this specific dilemma resolution
- Break condition: If the gating mechanism cannot effectively route the disentangled representations, or if the covariance loss over-regularizes and harms overall performance

## Foundational Learning

- Concept: Dimensional collapse in recommendation systems
  - Why needed here: Understanding why standard embedding approaches fail when scaling up, particularly in multi-domain settings with imbalanced data
  - Quick check question: What happens to the singular value distribution of embeddings when dimensional collapse occurs?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Crocodile builds upon MoE foundations but modifies the gating and introduces covariance loss, so understanding standard MoE behavior is crucial
  - Quick check question: How does standard MoE gating differ from the element-wise gating proposed in Crocodile?

- Concept: Cross-domain transfer learning challenges
  - Why needed here: The paper addresses the fundamental tension between leveraging shared information across domains while preserving domain-specific characteristics
  - Quick check question: What are the main risks of negative transfer in multi-domain recommendation systems?

## Architecture Onboarding

- Component map: Embeddings → Experts → CovLoss (training only) → PeG gating → Domain towers → Prediction

- Critical path: Embeddings → Experts → CovLoss (training only) → PeG gating → Domain towers → Prediction

- Design tradeoffs:
  - Multiple shared embeddings vs. domain-specific embeddings: Trade parameter efficiency and learning stability for potential expressiveness
  - Element-wise vs. vector-level gating: Trade computational complexity for finer control over information routing
  - Covariance loss vs. other disentanglement methods: Trade orthogonality enforcement for potential over-regularization

- Failure signatures:
  - CovLoss causes training instability or numerical issues
  - PeG gating fails to learn meaningful routing patterns
  - Model performance degrades on small domains despite theoretical advantages
  - Increased computational overhead during training becomes prohibitive

- First 3 experiments:
  1. Baseline comparison: Run Crocodile against standard MMoE with same architecture but without CovLoss and PeG to measure individual contribution
  2. Domain ablation: Test performance on smallest domain (S6) to verify sufficient learning claims
  3. Loss sensitivity: Vary CovLoss weight α to find optimal balance between disentanglement and prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Crocodile's Cross-experts Covariance Loss (CovLoss) vary across domains with vastly different data distributions, particularly in extremely imbalanced scenarios?
- Basis in paper: [explicit] The paper mentions that domains like S6 in Kuairand1k have 12-fold less data than S0, and that Crocodile achieves significant improvements in such small domains, but doesn't systematically explore how CovLoss performs across varying degrees of data imbalance.
- Why unresolved: The paper only demonstrates effectiveness in a few extreme cases (like S6) without providing a comprehensive analysis across a spectrum of data imbalances. It's unclear if CovLoss maintains its disentangling properties when the imbalance becomes even more severe or when multiple small domains exist.
- What evidence would resolve it: A systematic study varying the degree of data imbalance across multiple domains and measuring CovLoss's disentangling effectiveness (via DI) and its impact on model performance (AUC/gAUC) for each level of imbalance.

### Open Question 2
- Question: What is the optimal number of embedding sets (M) for Crocodile, and how does this optimal value depend on the number of domains and the characteristics of the dataset?
- Basis in paper: [explicit] The paper mentions that increasing the number of embedding sets improves performance, with Crocodile maintaining advantages even with 2 sets versus other methods with 5, but doesn't provide guidance on how to determine the optimal M.
- Why unresolved: The paper shows that Crocodile works well with different values of M but doesn't explore whether there's a point of diminishing returns or an optimal value that depends on dataset characteristics like the number of domains, feature diversity, or data volume.
- What evidence would resolve it: A comprehensive ablation study varying M across different datasets with varying numbers of domains, feature sets, and data volumes, identifying patterns in how optimal M changes with these factors.

### Open Question 3
- Question: How does Crocodile's performance compare to multi-domain models that use domain-specific embeddings when the data imbalance is reduced or eliminated?
- Basis in paper: [inferred] The paper emphasizes that Crocodile's shared embeddings with CovLoss outperform domain-specific embedding approaches (like SDEM and ME-PLE) primarily because the latter suffer from insufficient learning in small domains. However, it doesn't explore how these approaches compare when data imbalance is less severe.
- Why unresolved: The paper's comparison is conducted on datasets with extreme data imbalance, making it unclear whether Crocodile's advantages would persist in more balanced scenarios where domain-specific embeddings could be adequately learned.
- What evidence would resolve it: Experiments on datasets with progressively more balanced data distributions comparing Crocodile to domain-specific embedding approaches, measuring both disentanglement effectiveness and downstream performance across all domains.

## Limitations

- The exact mathematical formulation of the Cross-experts Covariance Loss (CovLoss) and its implementation details are not fully specified in the paper, which may affect reproducibility
- The paper claims significant improvements over baselines but does not provide comprehensive ablation studies showing the individual contributions of CovLoss and PeG mechanisms
- The impact of the covariance loss on training stability and convergence speed is not discussed, which could be a concern for practical deployment

## Confidence

- **High confidence**: The empirical results showing Crocodile's performance improvements on both public datasets and Tencent's advertising platform
- **Medium confidence**: The theoretical mechanism by which CovLoss prevents dimensional collapse and promotes expert diversity
- **Medium confidence**: The effectiveness of the PeG mechanism in routing information between shared embeddings and domain-specific towers

## Next Checks

1. **CovLoss sensitivity analysis**: Conduct experiments varying the weight of the covariance loss term to determine its optimal contribution and verify that improvements are not solely due to over-regularization
2. **Domain-specific performance verification**: Test Crocodile's performance on the smallest domain (S6) in the Kuairand1k dataset to validate the claim that multiple shared embeddings with PeG enable sufficient learning for data-scarce domains
3. **Comparison with simpler disentanglement methods**: Evaluate Crocodile against simpler approaches like domain-specific batch normalization or adversarial domain adaptation to isolate the contribution of the covariance-based disentanglement approach