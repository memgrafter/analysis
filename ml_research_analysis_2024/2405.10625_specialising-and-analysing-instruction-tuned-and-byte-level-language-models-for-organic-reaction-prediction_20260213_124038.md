---
ver: rpa2
title: Specialising and Analysing Instruction-Tuned and Byte-Level Language Models
  for Organic Reaction Prediction
arxiv_id: '2405.10625'
source_url: https://arxiv.org/abs/2405.10625
tags:
- tasks
- data
- reaction
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlanT5 and ByT5, language models pretrained only on text, can be
  effectively adapted for organic reaction prediction through fine-tuning without
  requiring SMILES-pretraining. The models achieve comparable Top-1 and Top-5 accuracy
  to specialized chemical models despite being trained solely on language tasks.
---

# Specialising and Analysing Instruction-Tuned and Byte-Level Language Models for Organic Reaction Prediction

## Quick Facts
- arXiv ID: 2405.10625
- Source URL: https://arxiv.org/abs/2405.10625
- Authors: Jiayun Pang; Ivan Vulić
- Reference count: 33
- Models pretrained only on text achieve comparable accuracy to specialized chemical models when fine-tuned on chemistry tasks.

## Executive Summary
This paper investigates whether language models pretrained only on natural language (FlanT5 and ByT5) can be effectively adapted for organic reaction prediction tasks without requiring SMILES pretraining. Through extensive experiments on forward reaction prediction, retrosynthesis, and reagent prediction tasks, the authors demonstrate that these models achieve comparable Top-1 and Top-5 accuracy to specialized chemical models like T5Chem, despite being trained solely on language tasks. Key findings include the effectiveness of vocabulary trimming and SMILES-aware preprocessing, the competitive performance of greedy decoding over more sophisticated search algorithms, and the minimal impact of model size beyond 60M parameters.

## Method Summary
The authors fine-tune encoder-decoder transformer models (FlanT5 and ByT5) on three organic chemistry tasks using the USPTO dataset: forward reaction prediction (FWD-S), retrosynthesis (RETRO), and reagent prediction (REAG). They compare different input preprocessing strategies (+none, +simple, +smiles), vocabulary trimming (+trim), and decoding strategies (beam search, greedy search, nucleus sampling, contrastive search). Models are trained with standard hyperparameters (learning rate 0.003, batch size 64, Adafactor optimizer) and evaluated using Top-1 and Top-5 accuracy metrics. The study systematically analyzes the impact of architecture, preprocessing, and decoding choices on performance.

## Key Results
- FlanT5 and ByT5 achieve comparable Top-1 and Top-5 accuracy to specialized chemical models despite being pretrained only on language tasks.
- Vocabulary trimming from 32k to 324 subwords and SMILES-aware preprocessing slightly improve performance while speeding up training.
- Greedy decoding is highly competitive with beam search, with Acc@1 saturating quickly at beam sizes of 2-3.
- Model size has minimal impact beyond 60M parameters, with FlanT5Large showing slight performance degradation suggesting overfitting.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models pretrained only on text can be adapted to predict organic reactions without requiring SMILES pretraining.
- Mechanism: The model's text-based pretraining on general language tasks equips it with generalizable linguistic pattern recognition and contextual understanding that transfers to SMILES representation when fine-tuned on chemistry data.
- Core assumption: SMILES notation, while domain-specific, shares enough structural similarities with natural language sequences that language models can learn to process it effectively.
- Evidence anchors:
  - [abstract] "FlanT5 and ByT5, language models pretrained only on text, can be effectively adapted for organic reaction prediction through fine-tuning without requiring SMILES-pretraining"
  - [section] "our key findings indicate that although being pretrained only on language tasks, FlanT5 and ByT5 provide a solid foundation to fine-tune for reaction prediction"
  - [corpus] Weak - no direct evidence found in corpus about SMILES pretraining transfer
- Break condition: If SMILES sequences have radically different structural properties from natural language that cannot be bridged through fine-tuning.

### Mechanism 2
- Claim: Vocabulary trimming and SMILES-aware preprocessing slightly improve performance while speeding up training.
- Mechanism: By removing vocabulary elements irrelevant to chemistry and optimizing input preprocessing, the model's search space is constrained and computational efficiency is improved without sacrificing expressiveness.
- Core assumption: The original vocabulary contains many natural language-specific tokens that are not useful for SMILES processing and can be safely removed.
- Evidence anchors:
  - [section] "trimming the vocabulary and the corresponding embeddings... does not have any negative impact on the final performance: on the contrary, due to its side-effect of constraining the search space, it even has a slight positive impact"
  - [section] "we trimmed the vocabulary from the original 32k subwords to only 324 subwords"
  - [corpus] Weak - no direct evidence found in corpus about vocabulary trimming for chemistry tasks
- Break condition: If essential chemical concepts are inadvertently removed during vocabulary trimming.

### Mechanism 3
- Claim: Greedy decoding is highly competitive with beam search while being computationally more efficient.
- Mechanism: The model's predictions are sufficiently accurate that simple greedy decoding captures the best or near-best predictions without needing the search breadth of beam search.
- Core assumption: The model's probability distributions for next tokens are peaked enough that the highest probability token is typically the correct choice.
- Evidence anchors:
  - [section] "the most efficient greedy decoding strategy is very competitive while only marginal gains can be achieved from more sophisticated decoding algorithms"
  - [section] "Acc@1 with larger beam sizes saturates quickly and the peak Acc@1 score is typically achieved already with beam size set to 2 or 3"
  - [corpus] Weak - no direct evidence found in corpus about decoding strategy comparison
- Break condition: If the model's probability distributions become flatter or more uncertain, requiring broader search.

## Foundational Learning

- Concept: Transfer learning and fine-tuning
  - Why needed here: The paper relies on adapting pretrained language models to a new domain (chemistry) through fine-tuning rather than training from scratch
  - Quick check question: What is the key difference between pretraining and fine-tuning, and why is fine-tuning used here instead of training a model from scratch?

- Concept: Tokenization and vocabulary management
  - Why needed here: The paper explores different tokenization strategies and vocabulary trimming to optimize performance for SMILES sequences
  - Quick check question: How does subword tokenization differ from byte-level tokenization, and what are the tradeoffs in the context of chemical sequences?

- Concept: Sequence-to-sequence modeling
  - Why needed here: The paper uses encoder-decoder architectures to transform input sequences (reactants) into output sequences (products)
  - Quick check question: What is the fundamental difference between encoder-only models like BERT and encoder-decoder models like T5, and why is the latter more suitable for reaction prediction?

## Architecture Onboarding

- Component map: Pretrained language model (FlanT5/ByT5) -> Vocabulary trimming and SMILES preprocessing -> Fine-tuning on chemistry datasets -> Evaluation with decoding strategies
- Critical path: Load pretrained model → Apply vocabulary trimming and preprocessing → Fine-tune on chemistry data → Evaluate with different decoding strategies
- Design tradeoffs: Model size vs. computational efficiency (Small vs. Base variants), vocabulary size vs. expressiveness (trimming), decoding strategy complexity vs. inference speed
- Failure signatures: Poor performance indicates issues with transfer learning effectiveness, preprocessing mismatches, or insufficient chemistry-specific training data
- First 3 experiments:
  1. Load FlanT5Base with default settings and evaluate on USPTO dataset without any modifications
  2. Apply vocabulary trimming to FlanT5Base and retrain to compare performance
  3. Compare greedy decoding vs. beam search with width 5 on the same fine-tuned model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does continued SMILES-based pretraining on larger datasets (beyond 10M sequences) improve performance compared to language-pretrained models?
- Basis in paper: [explicit] The authors tested continued pretraining on 10M SMILES sequences and found mixed results, noting that longer pretraining with larger datasets might yield better-adapted models but exceeded their computational resources.
- Why unresolved: The authors only tested continued pretraining for 400,000 steps on a subset of 10M SMILES from the full 116M dataset. They acknowledge that longer pretraining and larger datasets, as used in T5Chem (97M SMILES), might provide better initialization points.
- What evidence would resolve it: Running continued pretraining for 1-2 million steps on the full 116M SMILES dataset and comparing performance to language-pretrained models on reaction prediction tasks.

### Open Question 2
- Question: How do different decoding strategies (beyond greedy search and beam search) impact SMILES generation quality and efficiency?
- Basis in paper: [explicit] The authors tested nucleus sampling and contrastive search but found no noteworthy benefits over greedy search. They note that hyper-parameters for these strategies could be further tuned and that a more focused study on decoding strategies for SMILES generation is warranted.
- Why unresolved: Only preliminary experiments were conducted with limited hyper-parameter optimization. The authors suggest that fine-tuning hyper-parameters on development sets could yield improvements but didn't pursue this due to computational constraints.
- What evidence would resolve it: Systematic evaluation of multiple decoding strategies (including recent methods like guided decoding or energy-based decoding) with extensive hyper-parameter optimization on task-specific development sets.

### Open Question 3
- Question: Does model size beyond the Base variant (220M parameters) provide meaningful performance improvements for reaction prediction tasks?
- Basis in paper: [explicit] The authors tested FlanT5Large (780M parameters) on retrosynthesis and found no performance improvement, with scores even decreasing slightly, suggesting potential overfitting.
- Why unresolved: Only one experiment with FlanT5Large was conducted, and the authors suggest that the decrease might be due to overfitting to training data. They don't explore whether even larger models or different architectures might show different scaling behavior.
- What evidence would resolve it: Testing a broader range of model sizes (including frontier models like GPT-4 or Gemini) on diverse reaction prediction tasks with proper regularization techniques to prevent overfitting.

### Open Question 4
- Question: How does the quality and size of training data compare to architectural choices in determining final model performance?
- Basis in paper: [explicit] The authors conducted data efficiency experiments showing that FlanT5, ByT5, and molT5 display very similar "performance trajectories" across different training set sizes, with molT5 lagging slightly. They conclude that training data size and quality are more instrumental than chosen architecture.
- Why unresolved: While the data efficiency experiments provide preliminary evidence, they only tested one task (forward prediction) with three models. The authors acknowledge that creation of larger and higher-quality datasets is needed but don't quantify the relative importance of data vs architecture.
- What evidence would resolve it: Comprehensive ablation studies varying both training data size/quality and architectural choices across multiple reaction prediction tasks, potentially using meta-learning or controlled experiments with synthetic data.

## Limitations

- Transferability Across Chemistry Domains: Success is demonstrated only on USPTO dataset and standard organic reactions; generalization to other chemical representations or more complex tasks remains untested.
- Vocabulary Trimming Risks: The paper doesn't fully explore whether critical chemical concepts might be lost during vocabulary reduction, potentially limiting performance on edge cases.
- Decoding Strategy Saturation: The greedy decoding advantage is based on limited beam size analysis (2-3), and may not hold for more complex chemical spaces or longer sequences.

## Confidence

**High Confidence**: Language models pretrained only on text can be effectively fine-tuned for organic reaction prediction without SMILES pretraining.

**Medium Confidence**: Effectiveness of vocabulary trimming and SMILES-aware preprocessing, as gains are modest and long-term implications aren't fully explored.

**Medium Confidence**: Superiority of greedy decoding, as results are convincing for tested beam sizes but don't extend to larger beam widths or more complex chemical spaces.

## Next Checks

1. **Cross-Domain Transfer Test**: Apply the best-performing FlanT5 and ByT5 models to a different chemistry dataset (e.g., Reaxys or proprietary lab data) to verify whether the transfer learning success generalizes beyond the USPTO dataset.

2. **Vocabulary Safety Analysis**: Systematically analyze which tokens were removed during vocabulary trimming and conduct ablation studies to identify whether any critical chemical concepts were lost. Test the models on edge-case reactions that might require the removed vocabulary.

3. **Extended Decoding Analysis**: Expand the decoding strategy comparison to include beam sizes up to 10 and test on more complex multi-step reaction predictions. This would determine whether the greedy decoding advantage holds as chemical problem complexity increases.