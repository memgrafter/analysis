---
ver: rpa2
title: 'RefuteBench: Evaluating Refuting Instruction-Following for Large Language
  Models'
arxiv_id: '2402.13463'
source_url: https://arxiv.org/abs/2402.13463
tags:
- feedback
- llms
- query
- response
- refuting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes RefuteBench, a comprehensive benchmark designed\
  \ to evaluate how well large language models (LLMs) can follow refuting instructions\u2014\
  user feedback that contradicts the model's initial output. The benchmark covers\
  \ three tasks: question answering, machine translation, and email writing."
---

# RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models

## Quick Facts
- arXiv ID: 2402.13463
- Source URL: https://arxiv.org/abs/2402.13463
- Reference count: 18
- Key outcome: A benchmark reveals LLMs struggle with refuting instructions, with GPT-4 and Claude-2 showing better flexibility, and a recall-and-repeat method significantly improves responsiveness

## Executive Summary
This paper introduces RefuteBench, a comprehensive benchmark designed to evaluate how well large language models can follow refuting instructionsâ€”user feedback that contradicts the model's initial output. The benchmark covers three tasks: question answering, machine translation, and email writing. Experiments on seven representative LLMs reveal that while models generally struggle with feedback acceptance, leading to poor responsiveness, stronger models like GPT-4 and Claude-2 demonstrate higher flexibility. Additionally, LLMs tend to forget user feedback as dialogue progresses. To address these issues, the authors propose a recall-and-repeat prompting method, which significantly improves models' responsiveness by retrieving relevant feedback and encouraging repetition of instructions before task execution.

## Method Summary
The paper constructs RefuteBench by creating datasets for three tasks (QA, MT, Email Writing) where user instructions refute model outputs. The evaluation uses two novel metrics: Feedback Acceptance (FA) for whether models positively accept feedback, and Response Rate (RR) for whether feedback is correctly applied in verification queries. The authors evaluate seven LLMs across single-feedback and multi-feedback settings, finding that most models exhibit "stubbornness" in following refuting instructions. To address this, they propose a recall-and-repeat prompting method that retrieves the most relevant feedback using a finetuned BERT classifier and prompts the model to first repeat the requirement before executing the task.

## Key Results
- LLMs generally struggle with feedback acceptance, leading to poor responsiveness to refuting instructions
- Stronger models like GPT-4 and Claude-2 demonstrate higher flexibility in accepting and applying feedback
- LLMs tend to forget user feedback as dialogue progresses, with performance degrading over conversation length
- The recall-and-repeat method significantly improves both FA and RR metrics across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit strong recency bias when given retrieved feedback immediately before the current query
- Mechanism: The recall-and-repeat method leverages this bias by concatenating the most relevant feedback instruction right before the current query, making it the most recent context the model processes
- Core assumption: LLMs prioritize recent context when generating responses, especially when explicitly prompted to repeat it first
- Evidence anchors:
  - [section]: "Our intuition behind is to utilizes LLMs' recency bias (Holtzman et al., 2019) and self-reinforcement effect (Yan et al., 2023) to make model more flexible"
  - [section]: "We observe that the metrics of RR and FA are positively correlated, where the Pearson correlation coefficient is 0.92, 0.58, and 0.68 in the tasks of QA, MT, and email writing, respectively"
  - [corpus]: Weak evidence - only 5 related papers found, none directly discussing recency bias in feedback following contexts
- Break condition: If the retrieved feedback is not sufficiently relevant or if the model's context window truncates the feedback before query processing

### Mechanism 2
- Claim: Explicit repetition prompts force models to acknowledge and internalize feedback before execution
- Mechanism: The "repeat the requirement and then fulfill the following task" instruction creates a self-reinforcement loop where the model must first vocalize acceptance before proceeding
- Core assumption: LLMs are more likely to follow through on instructions they have explicitly stated or repeated
- Evidence anchors:
  - [section]: "By finetuning the multilingual BERT (Kenton and Toutanova, 2019) to classify which user instruction contains feedback and is viable for the current query, we retrieve the most relevant feedback and design a prompt that asks the model to first confirm and then repeat the feedback"
  - [section]: "Compared with baseline results, CoT (Wei et al., 2022) brings no improvement in MT and brings minor improvements in QA"
  - [corpus]: Weak evidence - no corpus papers specifically discussing repetition as a mechanism for feedback acceptance
- Break condition: If the model generates repetitive content that satisfies the repetition requirement without genuine understanding

### Mechanism 3
- Claim: Classifier-guided retrieval ensures only contextually relevant feedback is presented to the model
- Mechanism: The finetuned multilingual BERT classifier filters feedback instructions to only those that are viable for the current query, reducing noise and improving relevance
- Core assumption: LLMs can better follow feedback when it is contextually appropriate and not competing with irrelevant instructions
- Evidence anchors:
  - [section]: "By finetuning the multilingual BERT (Kenton and Toutanova, 2019) to classify which user instruction contains feedback and is viable for the current query"
  - [section]: "We train for 5 epochs and select the best-performed checkpoint on the validation set"
  - [corpus]: No direct evidence in corpus - this appears to be a novel approach not covered in related papers
- Break condition: If the classifier's precision drops below a threshold where it starts retrieving irrelevant feedback

## Foundational Learning

- Concept: Instruction-following evaluation metrics
  - Why needed here: The paper introduces FA (feedback acceptance) and RR (response rate) as novel metrics for evaluating how well models follow refuting instructions
  - Quick check question: What is the key difference between FA and RR metrics in the context of refuting instruction evaluation?

- Concept: Contextual relevance filtering
  - Why needed here: The classifier must distinguish between general instructions, previous queries, and refuting instructions to identify which feedback is applicable to the current query
  - Quick check question: How does the classifier determine whether a feedback instruction is viable for a given query?

- Concept: Retrieval-augmented prompting
  - Why needed here: The method retrieves relevant feedback from conversation history rather than relying solely on the immediate context, requiring understanding of how retrieval augments model knowledge
  - Quick check question: What are the two main sources of retrieval used in the recall-and-repeat method?

## Architecture Onboarding

- Component map: Data collection pipeline (QA, MT, Email) -> Classification model (BERT-based feedback relevance classifier) -> Retrieval system (selects top-1 relevant feedback) -> Prompt generation system (constructs recall-and-repeat prompts) -> Evaluation framework (FA and RR metrics with GPT-4 evaluation)

- Critical path: 1. Query received -> 2. Classifier predicts feedback relevance -> 3. Top-1 feedback retrieved -> 4. Prompt constructed with "repeat and fulfill" format -> 5. Prompt sent to LLM -> 6. Response evaluated using FA and RR metrics

- Design tradeoffs:
  - Single vs multi-feedback settings: Single is cleaner but less realistic; multi is more challenging but better reflects real-world usage
  - Random vs candidate feedback: Random provides stronger refutation but may be less realistic; candidate feedback is more practical but potentially easier
  - Classifier complexity: More sophisticated classifiers could improve retrieval but add training overhead

- Failure signatures:
  - Low FA scores despite high RR scores indicate the model is applying feedback but not accepting it
  - Classifier retrieving irrelevant feedback leads to prompt degradation
  - Context window truncation causing loss of feedback before query processing
  - Repetition prompt being satisfied with minimal acknowledgment without genuine understanding

- First 3 experiments:
  1. Test classifier precision/recall on held-out data to ensure it's retrieving relevant feedback
  2. Validate prompt format with a simple "repeat and answer" task before adding feedback retrieval
  3. Run ablation study comparing recall-only vs recall-and-repeat to confirm the repetition component adds value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' stubbornness characteristics generalize across different domains and tasks beyond QA, MT, and Email Writing?
- Basis in paper: [explicit] The paper acknowledges that "Other application tasks that related to the refuting instructions such as Code or Reasoning are not considered in this study."
- Why unresolved: The current study only covers three specific tasks (QA, MT, Email Writing), limiting the generalizability of findings to other domains.
- What evidence would resolve it: Conducting similar experiments on additional tasks like code generation, reasoning, or creative writing would reveal whether stubbornness is a universal LLM trait or task-dependent.

### Open Question 2
- Question: What is the precise mechanism behind the observed correlation between immediate feedback acceptance (FA) and long-term feedback application (RR)?
- Basis in paper: [explicit] "We observe that the metrics of RR and FA are positively correlated, where the Pearson correlation coefficient is 0.92, 0.58, and 0.68 in the tasks of QA, MT, and email writing, respectively."
- Why unresolved: While correlation is established, the paper doesn't explain the underlying causal relationship between initial acceptance and subsequent application of feedback.
- What evidence would resolve it: Ablation studies isolating components of the feedback response process (e.g., understanding vs. memory vs. execution) could identify which stages drive this correlation.

### Open Question 3
- Question: How do different RLHF fine-tuning strategies affect LLMs' stubbornness to refuting instructions?
- Basis in paper: [inferred] The paper discusses ChatGPT and Mistral's poor performance, noting they "tend to respond with 'Sorry, but....'" and speculating about RLHF's role in creating conservative responses.
- Why unresolved: The analysis is correlational and doesn't experimentally manipulate RLHF training procedures to isolate their effects on stubbornness.
- What evidence would resolve it: Comparing models trained with different RLHF datasets or strategies (e.g., varying levels of "harmlessness" filtering) would clarify how RLHF contributes to feedback rejection.

## Limitations
- Limited task coverage: Only three tasks (QA, MT, Email Writing) evaluated, limiting generalizability to other domains
- Focus on prompt-based solutions: The paper explicitly avoids fine-tuning approaches due to catastrophic forgetting concerns
- Evaluation methodology limitations: Reliance on GPT-4 for evaluation introduces potential bias and computational overhead

## Confidence
- Overall benchmark effectiveness: Medium - results show consistent patterns but sample sizes and methodology have limitations
- Recall-and-repeat method: High for general mechanism, Medium for generalizability across domains
- Classifier effectiveness: Medium - precision depends on training quality and task relevance
- FA and RR metrics: Medium - novel metrics may not fully capture complexity of refuting instruction-following

## Next Checks
1. Test the recall-and-repeat method on additional tasks beyond the three evaluated (QA, MT, email) to assess domain generalizability
2. Implement an ablation study isolating the effects of the repetition component versus the retrieval component
3. Conduct human evaluation studies to validate the FA and RR metrics against human judgment of feedback acceptance