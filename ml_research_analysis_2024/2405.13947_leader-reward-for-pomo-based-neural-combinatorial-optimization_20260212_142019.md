---
ver: rpa2
title: Leader Reward for POMO-Based Neural Combinatorial Optimization
arxiv_id: '2405.13947'
source_url: https://arxiv.org/abs/2405.13947
tags:
- leader
- problem
- pomo
- reward
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of existing neural combinatorial
  optimization (CO) methods that overlook the importance of focusing on optimal solutions
  rather than overall solution quality. The authors propose Leader Reward, a novel
  advantage function that modifies the REINFORCE algorithm to emphasize the leader
  solution during training.
---

# Leader Reward for POMO-Based Neural Combinatorial Optimization

## Quick Facts
- arXiv ID: 2405.13947
- Source URL: https://arxiv.org/abs/2405.13947
- Reference count: 40
- Key outcome: Leader Reward reduces POMO's gap to optimum by more than 100x on TSP100 with minimal computational overhead

## Executive Summary
This paper addresses the fundamental limitation of existing neural combinatorial optimization methods that overlook the importance of optimal solutions in favor of overall solution quality. The authors propose Leader Reward, a novel advantage function that modifies the REINFORCE algorithm to emphasize the leader solution during training. By applying a multiplier to the best trajectory's advantage while leaving others unchanged, Leader Reward shifts optimization pressure toward replicating and refining the best solutions. The method is compatible with various POMO-based models and inference strategies, and experimental results show significant performance improvements across TSP, CVRP, and FFSP problems.

## Method Summary
The method modifies POMO's advantage function by applying a multiplier α to the leader trajectory's advantage during training. When multiple trajectories are sampled, the one with the best reward receives enhanced gradient updates, focusing the model on replicating optimal solutions. The approach works in two phases: main training with adjustable α values to control exploration-exploitation balance, and final training with α = +∞ and adjusted learning rate to fine-tune the aggressive model. Leader Reward can be combined with inference strategies like SGBS and EAS for further performance gains.

## Key Results
- Leader Reward reduces POMO's gap to optimum by more than 100x on TSP100
- The method works with SGBS and EAS inference strategies, improving their effectiveness
- Performance improvements are observed across TSP, CVRP, and FFSP problems
- Computational overhead is minimal, requiring only one additional operation per training step

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leader Reward modifies the advantage function in REINFORCE to focus training on the leader trajectory rather than average trajectory quality.
- Mechanism: When multiple trajectories are sampled for the same problem, Leader Reward applies a multiplier α to the advantage of the trajectory with the best reward, while leaving others unchanged. This shifts optimization pressure toward replicating and refining the best solutions.
- Core assumption: In combinatorial optimization, the best solution among many trials is the only one that matters, so training should maximize the expected quality of the best trajectory, not the average.
- Evidence anchors: [abstract], [section 4.1], [corpus]

### Mechanism 2
- Claim: Leader Reward increases exploration entropy in the policy distribution.
- Mechanism: By giving the leader trajectory an extra reward, the probability of actions leading to the leader solution increases. Since the leader is usually a low-probability event, this increase in probability reduces entropy less than if it were a high-probability event, thereby increasing overall policy entropy.
- Core assumption: The leader trajectory has a lower probability than the average trajectory, so increasing its probability increases overall entropy.
- Evidence anchors: [section 4.1], [corpus]

### Mechanism 3
- Claim: Leader Reward balances exploration vs exploitation by adjusting α.
- Mechanism: Small α values make the model conservative, focusing on average solution quality. Large α values make the model aggressive, focusing on finding the best solution at the cost of average quality.
- Core assumption: The same α parameter controls both the magnitude of leader reward and the degree of exploration vs exploitation.
- Evidence anchors: [section 4.1], [section 4.2], [corpus]

## Foundational Learning

- Concept: REINFORCE algorithm and advantage functions
  - Why needed here: Leader Reward modifies the standard advantage function used in REINFORCE to emphasize leader solutions.
  - Quick check question: What is the standard advantage function in REINFORCE, and how does Leader Reward modify it?

- Concept: Policy gradient methods and entropy regularization
  - Why needed here: Understanding how entropy affects exploration is crucial for understanding how Leader Reward increases exploration.
  - Quick check question: How does entropy regularization affect policy gradient methods, and how is this related to Leader Reward?

- Concept: Combinatorial optimization problem structure
  - Why needed here: The key insight is that CO problems only care about the best solution, not average quality.
  - Quick check question: What makes combinatorial optimization problems different from other optimization problems in terms of solution evaluation?

## Architecture Onboarding

- Component map: POMO model with transformer architecture -> Leader Reward module -> Adam optimizer -> Training loop with SGBS/EAS inference

- Critical path: 1. Sample multiple trajectories for each problem instance 2. Calculate rewards for each trajectory 3. Apply Leader Reward to modify advantages 4. Compute gradients and update model parameters 5. (Optional) Apply inference strategies during testing

- Design tradeoffs: α hyperparameter controls exploration vs exploitation tradeoff; Leader Reward adds minimal computational overhead; compatible with various inference strategies but requires careful hyperparameter tuning

- Failure signatures: Model converges to poor solutions if α is too large; Model fails to improve if α is too small; Training instability if leader trajectories are not representative

- First 3 experiments: 1. Train POMO with Leader Reward on TSP50 with different α values to observe exploration effects 2. Compare POMO vs Leader Reward on TSP100 to verify performance improvement 3. Combine Leader Reward with SGBS+EAS on TSP100 to test inference phase benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does dynamically adjusting the Leader Reward parameter α during training affect the exploration-exploitation balance compared to fixed α values?
- Basis in paper: [explicit] The authors mention future work will attempt to dynamically adjust α in Leader Reward
- Why unresolved: The paper only tests fixed α values and mentions dynamic adjustment as future work
- What evidence would resolve it: Experiments comparing fixed vs. dynamic α adjustment showing performance differences and exploration patterns

### Open Question 2
- Question: How would Leader Reward perform on combinatorial optimization problems with non-symmetric solution representations?
- Basis in paper: [inferred] The method is explicitly stated to work with POMO-based models which rely on symmetric solution representations
- Why unresolved: All experiments use problems with symmetric solutions (TSP, CVRP, FFSP) where POMO symmetry applies
- What evidence would resolve it: Testing Leader Reward on asymmetric CO problems like ATSP or asymmetric VRPs

### Open Question 3
- Question: What is the impact of Leader Reward on the model's ability to escape local optima during the inference phase?
- Basis in paper: [explicit] The paper states Leader Reward increases exploration and helps discover new strategies
- Why unresolved: While exploration is mentioned, the specific impact on escaping local optima during inference is not quantified
- What evidence would resolve it: Comparative analysis of local optima escape rates with and without Leader Reward during inference

## Limitations
- Performance claims rely on specific problem sizes (TSP100) and may not generalize to all CO problems
- Leader Reward compatibility with inference strategies is demonstrated but not theoretically proven
- The method requires careful hyperparameter tuning of α, which may be problem-dependent

## Confidence

**High Confidence:** The core mechanism of Leader Reward (modifying REINFORCE advantage to emphasize leader solutions) is well-supported by mathematical formulation and experimental results. The performance improvements on TSP, CVRP, and FFSP problems are statistically significant.

**Medium Confidence:** The claim that Leader Reward increases exploration entropy is mathematically proven but relies on assumptions about trajectory probability distributions that may not hold in all scenarios. The balance between aggressive and conservative models through α tuning is demonstrated empirically but lacks comprehensive theoretical analysis.

**Low Confidence:** The claim that Leader Reward works seamlessly with various inference strategies (SGBS, EAS) is based on limited experimental combinations. The assertion that the method reduces POMO's gap to optimum by "more than 100 times" is specific to TSP100 and may not generalize to all problem sizes or types.

## Next Checks

1. **Empirical Validation of Entropy Claims:** Measure and compare policy entropy during training with and without Leader Reward across multiple random seeds to verify the claimed entropy increase.

2. **Generalization Testing:** Evaluate Leader Reward on additional CO problems (e.g., TSP500, larger CVRP instances) to verify the 100x performance improvement claim beyond TSP100.

3. **Stability Analysis:** Conduct ablation studies with different α values to map out the stability landscape and identify conditions under which Leader Reward may fail to converge or produce suboptimal results.