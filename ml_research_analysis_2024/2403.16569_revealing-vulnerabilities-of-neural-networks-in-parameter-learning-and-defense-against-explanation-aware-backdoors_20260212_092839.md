---
ver: rpa2
title: Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense
  Against Explanation-Aware Backdoors
arxiv_id: '2403.16569'
source_url: https://arxiv.org/abs/2403.16569
tags:
- attack
- attacks
- figure
- explanation
- grad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals how Batch Normalization (BN) layers in neural
  networks can be exploited by explanation-aware backdoor attacks, and proposes a
  defense using Channel-Wise Feature Normalization (CFN). Statistical analysis of
  model weights shows that attacks significantly alter BN learning parameters, compromising
  both model predictions and explanations.
---

# Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors

## Quick Facts
- arXiv ID: 2403.16569
- Source URL: https://arxiv.org/abs/2403.16569
- Reference count: 40
- Primary result: CFN defense achieves ~99% ASR reduction and ~91% MSE reduction against explanation-aware backdoors

## Executive Summary
This paper demonstrates how Batch Normalization (BN) layers in neural networks can be exploited by explanation-aware backdoor attacks, where adversaries embed triggers that manipulate both model predictions and their explanations. The authors show that during adversarial fine-tuning, BN learning parameters (γ and β) become corrupted, allowing attacks to persist in evaluation. They propose replacing BN with Channel-Wise Feature Normalization (CFN) as a defense, which achieves significant reduction in attack success without retraining.

## Method Summary
The paper first trains clean CNN models with BN layers, then applies three types of explanation-aware backdoor attacks (Simple Fooling, Red Herring, Full Disguise) through adversarial fine-tuning. For defense, BN layers are replaced with CFN during inference - a normalization method that computes per-channel mean and variance without learnable parameters. The approach is evaluated on CIFAR-10 and GTSRB datasets, measuring Attack Success Rate (ASR) and Mean Square Error (MSE) for explanation preservation.

## Key Results
- BN parameters (γ and β) are significantly altered during attack fine-tuning, facilitating explanation manipulation
- CFN defense reduces ASR by approximately 99% and MSE by approximately 91% across all three attack types
- Models without BN remain vulnerable but show large accuracy drops on clean data, making attacks detectable
- Defense works without retraining, simply by replacing BN with CFN during evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Batch Normalization (BN) learning parameters (γ and β) become corrupted during adversarial fine-tuning, allowing attacks to embed misleading explanations into the model's feature space.
- **Mechanism:** During attack fine-tuning, the adversary adjusts BN parameters to align the feature distribution of the model with the targeted explanation. Since BN parameters are updated during training and used during evaluation, they can absorb attack-specific patterns that persist at inference.
- **Core assumption:** BN parameters are learnable and persist in evaluation, so if they are manipulated during training, they can be exploited during inference.
- **Evidence anchors:** Abstract states attacks "significantly alter BN learning parameters" and that learning parameters inherent to BN function as facilitators. Section explains BN's output uses learned γ and β parameters updated during training and used during evaluation.
- **Break condition:** If BN is replaced with a non-learnable normalization layer like CFN, the attack cannot exploit learned parameters, reducing ASR and MSE.

### Mechanism 2
- **Claim:** Channel-Wise Feature Normalization (CFN) prevents attacks by normalizing each feature channel independently without learnable parameters, thereby removing attack artifacts.
- **Mechanism:** CFN computes per-channel mean and variance over batch and spatial dimensions and normalizes without γ and β. This means any trigger-based pattern spread across channels is normalized out, preventing misleading explanations from persisting.
- **Core assumption:** Attack artifacts manifest across multiple feature channels; CFN's channel-wise normalization will eliminate them.
- **Evidence anchors:** Abstract states CFN protects against ASR and explanation manipulation during prediction. Section explains CFN computes normalization per channel without learned parameters, preventing attacks from embedding in BN parameters.
- **Break condition:** If attack artifacts are concentrated in a single channel or if the batch size is too small to compute reliable statistics, CFN might fail.

### Mechanism 3
- **Claim:** Removing Batch Normalization entirely does not prevent attacks, but it changes the attack surface and model behavior.
- **Mechanism:** Without BN, the model relies solely on learned weights and activations, so attacks must manipulate these directly. This makes the attack less stable and more detectable because it alters core weights significantly, leading to accuracy drops on clean data.
- **Core assumption:** BN acts as a buffer that shields core weights from attack-induced changes; without it, the attack directly corrupts weights.
- **Evidence anchors:** Abstract and section show models without BN are still attackable, but the attack causes large accuracy drops, making it detectable.
- **Break condition:** If the attack is designed to target weights directly and the defender cannot detect accuracy drops, this approach may not be sufficient.

## Foundational Learning

- **Concept: Batch Normalization (BN)**
  - Why needed here: BN is central to how attacks manipulate explanations; understanding its parameters (γ, β) and how they are updated is key to the defense.
  - Quick check question: In BN, which parameters are learned during training and used during evaluation?

- **Concept: Channel-Wise Feature Normalization (CFN)**
  - Why needed here: CFN replaces BN in the defense; knowing its formula and why it lacks learnable parameters explains why it stops attacks.
  - Quick check question: How does CFN compute normalization per channel without using γ and β?

- **Concept: Adversarial Backdoor Attacks on XAI**
  - Why needed here: The paper targets explanation-aware backdoors; understanding how triggers affect both predictions and explanations is essential.
  - Quick check question: What is the difference between Simple Fooling, Red Herring, and Full Disguise attacks?

## Architecture Onboarding

- **Component map:** Original model: CNN backbone (e.g., ResNet or VGG) with Batch Normalization layers -> Defense: Replace BN layers with CFN during inference (no training)

- **Critical path:**
  1. Train clean model with BN
  2. Fine-tune with attack triggers and targeted explanations
  3. For defense: swap BN with CFN during evaluation, keep original weights

- **Design tradeoffs:**
  - Using CFN requires larger batch sizes for stable statistics
  - No need for retraining, but defense is only applied at inference
  - Model accuracy on clean data remains unchanged

- **Failure signatures:**
  - High ASR after defense → CFN not working (small batch size, or attack artifacts concentrated in one channel)
  - Large MSE between original and defended explanations → defense not fully removing attack
  - Accuracy drop on clean data → attack corrupted core weights (not just BN)

- **First 3 experiments:**
  1. Train clean ResNet on CIFAR-10, measure clean accuracy
  2. Apply Simple Fooling attack, measure ASR and MSE
  3. Replace BN with CFN, measure ASR and MSE again to confirm defense

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the vulnerability of Batch Normalization (BN) to adversarial attacks compare to other normalization techniques like Layer Normalization or Instance Normalization in the context of explanation-aware backdoors?
- Basis in paper: [inferred] The paper focuses on BN's role in attacks but does not compare its vulnerability to other normalization methods.
- Why unresolved: The study does not explore alternative normalization techniques, leaving their comparative robustness against explanation-aware attacks unknown.
- What evidence would resolve it: Experimental comparison of BN, Layer Normalization, and Instance Normalization under identical attack scenarios, measuring Attack Success Rate (ASR) and explanation preservation metrics.

### Open Question 2
- Question: Can the Channel-Wise Feature Normalization (CFN) defense be extended to recurrent neural networks (RNNs) or transformers to protect against explanation-aware attacks?
- Basis in paper: [inferred] The paper applies CFN to CNNs but does not discuss its applicability to other architectures.
- Why unresolved: The method's effectiveness and potential modifications for RNNs or transformers are not investigated, limiting understanding of its broader applicability.
- What evidence would resolve it: Implementation and evaluation of CFN in RNNs or transformers, followed by testing against explanation-aware attacks to measure ASR and explanation quality.

### Open Question 3
- Question: What are the theoretical limits of attack detection using explanation methods in the presence of explanation-aware backdoors?
- Basis in paper: [inferred] The paper demonstrates that explanations can be manipulated but does not explore the fundamental limits of detection.
- Why unresolved: The study focuses on defense rather than detection, leaving open questions about the theoretical bounds of identifying attacks through explanations alone.
- What evidence would resolve it: Theoretical analysis and empirical studies determining the conditions under which explanation methods can reliably detect backdoors, possibly involving bounds on attack stealthiness and explanation perturbation.

## Limitations
- The paper does not rigorously establish causality between BN parameter corruption and attack success through ablation studies
- CFN defense effectiveness is not tested against attack variants specifically designed to bypass channel-wise normalization
- Results are limited to CNN architectures on CIFAR-10 and GTSRB datasets, with unknown performance on larger datasets or different architectures

## Confidence

**High Confidence**: The statistical evidence that BN parameters are significantly altered during attacks (ASR ~1%, MSE changes) is robust. The correlation between BN parameter corruption and attack success is well-documented through experimental measurements.

**Medium Confidence**: The CFN defense mechanism is theoretically sound and shows strong empirical results. However, confidence is limited by lack of testing against CFN-specific attack variants and broader architectural coverage.

**Low Confidence**: Claims about BN removal effects are based on limited evidence - the paper states accuracy drops make attacks detectable, but doesn't provide comprehensive analysis of attack stability or detectability metrics without BN.

## Next Checks

1. **Ablation Study on BN Parameters**: Conduct controlled experiments isolating BN parameter changes from weight changes. Fine-tune identical models with and without BN layers, measuring whether ASR and MSE differences persist when only BN parameters are frozen versus when all parameters are frozen.

2. **CFN Bypass Attack Design**: Develop attack variants specifically targeting CFN's channel-wise normalization - for example, creating triggers that concentrate misleading patterns in single channels or use spatial arrangements that survive per-channel normalization. Test whether ASR increases beyond 1% and MSE exceeds acceptable thresholds.

3. **Cross-Architecture and Dataset Validation**: Apply the complete attack-defense pipeline to Transformer-based vision models (e.g., ViT) on ImageNet and to NLP models on text classification tasks. Measure whether BN parameter corruption patterns and CFN defense effectiveness transfer to these architectures and domains.