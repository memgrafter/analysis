---
ver: rpa2
title: Advancing Attribution-Based Neural Network Explainability through Relative
  Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation
arxiv_id: '2412.09311'
source_url: https://arxiv.org/abs/2412.09311
tags:
- attribution
- methods
- relevance
- input
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving neural network
  interpretability by developing a new Layer-Wise Relevance Propagation (LRP) method
  called absLRP, which addresses the problem of incorrect relative attribution between
  neurons with varying activation magnitudes. The method applies this approach to
  Vision Transformers and evaluates its performance against existing methods on ImageNet
  and PascalVOC datasets.
---

# Advancing Attribution-Based Neural Network Explainability through Relative Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation

## Quick Facts
- arXiv ID: 2412.09311
- Source URL: https://arxiv.org/abs/2412.09311
- Reference count: 40
- Primary result: absLRP achieves GAE scores of 0.272 on ImageNet and 0.238 on PascalVOC, outperforming state-of-the-art attribution methods

## Executive Summary
This paper introduces absLRP, a Layer-Wise Relevance Propagation method that addresses incorrect relative attribution between neurons with varying activation magnitudes by incorporating absolute activation values in normalization. The method is applied to Vision Transformers and evaluated using a novel Global Attribution Evaluation (GAE) metric that combines faithfulness, robustness, and localization. absLRP demonstrates superior performance with sparse, contrastive attribution maps that improve interpretability of neural network decisions.

## Method Summary
absLRP extends standard LRP by using absolute neuron activations as normalizing factors, ensuring neurons with larger absolute outputs receive proportionally more relevance. The method employs contrastive initialization (-1/N for non-target classes) to produce sparse attribution maps. It is specifically adapted for Vision Transformers with modifications for self-attention blocks and layer normalization. The GAE metric evaluates attribution quality through local consistency (using gradient-based masking with MoRF/LeRF) and contrastiveness (using mosaic-based scoring), combining these into a single comprehensive score.

## Key Results
- absLRP achieves GAE scores of 0.272 on ImageNet and 0.238 on PascalVOC, outperforming existing methods
- The method provides superior local consistency and contrastiveness compared to state-of-the-art attribution techniques
- absLRP consistently achieves top or near-top performance across VGG, ResNet50, and ViT-Base architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: absLRP improves relative attribution by incorporating absolute activation magnitudes in the normalization denominator
- Mechanism: The LRP rule redistributes relevance based on positive contributions weighted by the absolute value of neuron outputs
- Core assumption: Neurons with larger absolute activation magnitudes have greater impact on model decisions
- Evidence anchors:
  - [abstract] "This rule effectively addresses the issue of incorrect relative attribution between neurons within the same layer that exhibit varying absolute magnitude activations"
  - [section 3.1] "To address incorrect relative attribution due to varying magnitudes within neurons of the same layer, we use the absolute final output of each neuron as the normalizing factor"
- Break Condition: If activation magnitudes don't correlate with feature importance, this normalization could distort rather than improve attributions

### Mechanism 2
- Claim: Contrastive initialization of relevance scores (-1/N for non-target classes) improves interpretability
- Mechanism: Starting with contrastive relevance values helps the propagation algorithm distinguish relevant from irrelevant features
- Core assumption: Contrastive initialization produces more discriminative attribution maps
- Evidence anchors:
  - [section 3.1] "To produce contrastive attribution maps, we utilize the idea from Gu et al. [22] and we set the last layer's starting attribution as1 for the target class, and − 1/N for the other classes"
  - [abstract] "Applying this new rule yields sparse and contrastive attribution maps with no noise"
- Break Condition: Complex decision boundaries may not be captured by simple contrastive initialization

### Mechanism 3
- Claim: GAE metric combines faithfulness, robustness, and localization into a single score
- Mechanism: Uses gradient-based feature selection for perturbation and mosaic-based contrastiveness scoring
- Core assumption: A comprehensive evaluation should assess multiple attribution properties simultaneously
- Evidence anchors:
  - [abstract] "Furthermore, we discuss the insufficiencies of current evaluation metrics for attribution-based explainability and propose a new evaluation metric that combines the notions of faithfulness, robustness and contrastiveness"
  - [section 3.2] "These factors are Local consistency and Contrastiveness" with detailed methodology
  - [section 4] "GAE scores obtained by various attribution methods" showing comprehensive evaluation results
- Break Condition: Imbalanced components could dominate the combined score

## Foundational Learning

- Concept: Layer-Wise Relevance Propagation (LRP)
  - Why needed here: absLRP builds upon LRP as its foundation
  - Quick check question: In LRP, how is the relevance of a neuron in layer l-1 computed from the relevance of neurons in layer l?

- Concept: Transformer architecture components
  - Why needed here: absLRP is applied to Vision Transformers
  - Quick check question: What are the three main inputs to a self-attention mechanism in Transformers?

- Concept: Attribution evaluation metrics
  - Why needed here: Understanding why GAE was developed requires knowledge of existing metrics
  - Quick check question: What is the fundamental difference between faithfulness and robustness in attribution evaluation?

## Architecture Onboarding

- Component map: absLRP consists of core LRP rule with absolute magnitude normalization, contrastive initialization, and application to various neural network layers (residual connections, batch normalization, layer normalization, self-attention blocks). GAE metric includes local consistency evaluation through gradient-based masking and contrastiveness evaluation through mosaic-based scoring.

- Critical path: For absLRP: (1) Initialize relevance at output layer, (2) Propagate relevance backward using new LRP rule, (3) Apply layer-specific modifications. For GAE: (1) Generate attribution maps, (2) Evaluate local consistency through MoRF/LeRF perturbation, (3) Evaluate contrastiveness through mosaic scoring, (4) Combine into final score.

- Design tradeoffs: absLRP trades computational complexity (additional absolute value calculations) for improved attribution quality. GAE trades simplicity for comprehensive evaluation but requires more computation than single-property metrics.

- Failure signatures: absLRP may produce noisy attributions if activation magnitudes are unreliable indicators of importance. GAE may give misleading results if perturbation strategy doesn't match model's decision-making process.

- First 3 experiments:
  1. Implement absLRP on simple CNN and compare attribution maps to standard LRP-α1β0
  2. Apply GAE to compare absLRP with standard LRP methods on small dataset
  3. Test absLRP on Vision Transformer with different patch sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the relative absolute magnitude propagation approach be extended to other types of neural networks beyond CNNs and Transformers?
- Basis in paper: [inferred] The paper mentions potential for extending the approach to diverse data types and tasks
- Why unresolved: The paper focuses on CNNs and Transformers without investigating other architectures
- What evidence would resolve it: Experiments applying the method to RNNs, GNNs, and other architectures

### Open Question 2
- Question: What are the specific factors that make certain models more susceptible to relative magnitude attribution issues?
- Basis in paper: [inferred] The paper notes ResNet50 exhibits similar performance to cLRP, suggesting reduced susceptibility
- Why unresolved: The paper hypothesizes about factors but doesn't provide definitive explanation
- What evidence would resolve it: Controlled experiments isolating effects of architectural components and training configurations

### Open Question 3
- Question: How can the evaluation metric be refined to account for synergistic effects of faithfulness, robustness, and localization?
- Basis in paper: [explicit] The paper acknowledges that evaluating each property in isolation overlooks synergistic value
- Why unresolved: The proposed metric uses simple product combination
- What evidence would resolve it: Developing more sophisticated mathematical framework for combining components

## Limitations

- The absLRP's superiority claims rely heavily on the proposed GAE metric, which lacks independent validation against established benchmarks
- The paper lacks ablation studies showing individual contributions of absolute normalization, contrastive initialization, and pixel-level attribution
- absLRP's performance on datasets beyond ImageNet and PascalVOC remains unverified

## Confidence

- **High Confidence:** Technical formulation of absLRP as LRP extension with absolute magnitude normalization is clearly specified and reproducible
- **Medium Confidence:** Claim that absLRP outperforms state-of-the-art methods is supported by GAE scores, but metric's comprehensiveness needs validation
- **Low Confidence:** Assertion that GAE addresses "insufficiencies of current evaluation metrics" is not substantiated with comparative analysis

## Next Checks

1. **Ablation Study:** Run experiments isolating each absLRP component to quantify individual contributions to performance improvements
2. **Metric Validation:** Compare GAE scores with traditional metric combinations on same datasets to verify whether GAE provides unique insights
3. **Cross-Dataset Generalization:** Evaluate absLRP on additional datasets (CIFAR-10, COCO) to test performance gains generalize to different data distributions