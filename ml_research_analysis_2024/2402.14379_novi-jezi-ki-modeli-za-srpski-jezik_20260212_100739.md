---
ver: rpa2
title: "Novi jezi\u010Dki modeli za srpski jezik"
arxiv_id: '2402.14379'
source_url: https://arxiv.org/abs/2402.14379
tags:
- jerteh
- andrija
- language
- page
- classla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates ten transformer-based language models for
  Serbian text processing, including two new models (jerteh-81 and jerteh-355) trained
  on high-quality Serbian data from the Society for Language Resources and Technologies.
  The models were compared across four tasks: masked language modeling, semantic similarity,
  part-of-speech tagging, and named entity recognition.'
---

# Novi jeziÄki modeli za srpski jezik

## Quick Facts
- arXiv ID: 2402.14379
- Source URL: https://arxiv.org/abs/2402.14379
- Reference count: 0
- This paper evaluates ten transformer-based language models for Serbian text processing

## Executive Summary
This paper evaluates ten transformer-based language models for Serbian text processing, including two new models (jerteh-81 and jerteh-355) trained on high-quality Serbian data from the Society for Language Resources and Technologies. The models were compared across four tasks: masked language modeling, semantic similarity, part-of-speech tagging, and named entity recognition. Results show that model performance varies by task, with jerteh-355 achieving the best overall results in masked language modeling, while SRoBERTa variants excelled in semantic similarity tasks.

## Method Summary
The study evaluates ten transformer-based language models for Serbian using four benchmark tasks. Two new models (jerteh-81 and jerteh-355) were trained specifically on high-quality Serbian corpora from the Society for Language Resources and Technologies. Models were compared using standard evaluation metrics including accuracy, F1 scores, and correlation coefficients. Training data quality and quantity were varied to assess their impact on performance across different model architectures and task types.

## Key Results
- Model performance varies significantly by task type, with different models excelling in different benchmarks
- Larger training datasets generally improve performance, but data quality proved more important than quantity
- jerteh-355 achieved the best overall results in masked language modeling, while SRoBERTa variants excelled in semantic similarity tasks

## Why This Works (Mechanism)
The effectiveness of transformer-based language models for Serbian stems from their ability to capture complex linguistic patterns through self-attention mechanisms. The success of different models on different tasks indicates that language model performance depends heavily on the alignment between model architecture, training data characteristics, and specific task requirements. High-quality training data enables better generalization and more accurate linguistic representations.

## Foundational Learning
- Masked Language Modeling (MLM): Why needed - Pretraining objective that teaches models to understand context; Quick check - Model's ability to predict masked words accurately
- Part-of-Speech Tagging: Why needed - Fundamental syntactic analysis task; Quick check - Accuracy on token classification across different POS categories
- Named Entity Recognition: Why needed - Information extraction capability; Quick check - F1 score on entity boundary detection and classification
- Semantic Similarity: Why needed - Measures model's understanding of meaning and context; Quick check - Correlation with human judgments on sentence pairs

## Architecture Onboarding
- Component Map: Transformer encoder -> Self-attention layers -> Feed-forward networks -> Output layer
- Critical Path: Token embedding -> Positional encoding -> Multi-head attention -> Feed-forward -> Layer normalization -> Output
- Design Tradeoffs: Model size vs. training efficiency, data quality vs. quantity, task-specific vs. general-purpose optimization
- Failure Signatures: Poor generalization to unseen contexts, confusion between similar linguistic structures, bias toward dominant patterns in training data
- Three First Experiments: 1) Compare MLM performance across models with identical training data, 2) Test transfer learning from MLM to downstream tasks, 3) Evaluate model sensitivity to domain shift in test data

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of detailed hyperparameter tuning information across different models
- Limited error analysis for each task preventing understanding of systematic failure patterns
- Evaluation only covers four specific tasks, potentially missing broader language processing needs

## Confidence
- Overall model performance ranking: Medium
- Data quality importance: Medium
- Task-specific model selection recommendation: High

## Next Checks
1. Conduct ablation studies comparing model performance when trained on high-quality versus larger but lower-quality Serbian datasets with controlled parameters
2. Perform comprehensive error analysis across all four tasks to identify systematic failure patterns and linguistic phenomena that challenge current models
3. Extend evaluation to additional Serbian NLP tasks (such as dependency parsing or machine translation) to assess model generalization beyond the four tested tasks