---
ver: rpa2
title: 'Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from
  Averaging to Automation'
arxiv_id: '2410.08371'
source_url: https://arxiv.org/abs/2410.08371
tags:
- merging
- merged
- methods
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares model merging techniques, from simple averaging
  (Model Soups) to advanced automated approaches like evolutionary merging. It introduces
  Differentiable Adaptive Merging (DAM) as an efficient, gradient-based alternative
  that optimizes scaling coefficients to integrate model strengths with lower computational
  overhead.
---

# Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation

## Quick Facts
- arXiv ID: 2410.08371
- Source URL: https://arxiv.org/abs/2410.08371
- Authors: Thomas Gauthier-Caron; Shamane Siriwardhana; Elliot Stein; Malikeh Ehghaghi; Charles Goddard; Mark McQuade; Jacob Solawetz; Maxime Labonne
- Reference count: 7
- Primary result: DAM achieves competitive performance (~0.62 average accuracy) while matching or exceeding more complex methods like evolutionary merging

## Executive Summary
This paper introduces Differentiable Adaptive Merging (DAM), a gradient-based approach for merging specialized language models that balances performance with computational efficiency. DAM learns column-wise scaling coefficients to integrate model strengths while preserving individual capabilities through KL divergence loss and cosine similarity regularization. Experiments across multilingual, mathematical, and SQL domains demonstrate DAM's effectiveness compared to simpler methods like Model Soups and more complex approaches like evolutionary merging, with DAM achieving similar or superior results using significantly fewer resources.

## Method Summary
DAM optimizes scaling coefficients for linear layers, embeddings, and normalization layers using gradient-based optimization rather than evolutionary strategies. The method applies KL divergence loss to ensure the merged model's outputs match individual models on their respective datasets, with additional cosine similarity regularization to promote diversity in scaling patterns. The approach requires aligned models (same base architecture) and representative datasets for each model's specialization. DAM was evaluated on Mistral and Llama 3 variants fine-tuned for Japanese, math, German, Korean, and SQL tasks, with performance measured on language and task-specific benchmarks.

## Key Results
- DAM achieves ~0.62 average accuracy on language tasks, matching or exceeding evolutionary merging and Model Soups
- Simple averaging (Model Soups) performs competitively when model similarity is high, challenging assumptions about complexity
- DAM demonstrates significantly lower computational overhead compared to evolutionary methods while maintaining strong performance
- Column-wise scaling (rather than layer-wise) provides optimal balance between control and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Column-wise scaling allows DAM to selectively amplify or suppress feature contributions from each model, improving integration without catastrophic forgetting.
- Mechanism: DAM learns a scaling coefficient for each column of each model's weight matrix. This means each feature's contribution from each model can be adjusted independently, allowing the merged model to balance strengths from different models without erasing any one model's capabilities.
- Core assumption: The alignment between models is preserved, so functionally equivalent features occupy the same relative positions in weight matrices.
- Evidence anchors:
  - [abstract] states DAM "optimizes model integration through scaling coefficients, minimizing computational demands."
  - [section] describes the column-wise scaling: "The goal is to find optimal scaling coefficients cl_ij for each column j in the weight matrix of each model..."
  - [corpus] is weak here—no direct neighbor papers discuss this specific column-wise approach.
- Break condition: If models are not properly aligned (e.g., trained from different initializations or on entirely different tasks), the column-wise scaling could combine mismatched features, causing performance degradation.

### Mechanism 2
- Claim: KL divergence loss ensures the merged model's output distribution matches the individual models on their respective datasets, preserving each model's task-specific strengths.
- Mechanism: The objective function minimizes the KL divergence between the logits of the merged model and each individual model on their corresponding datasets. This forces the merged model to produce outputs consistent with what each input model would have produced on its own data.
- Core assumption: The representative datasets used for DAM training accurately reflect the capabilities and distributions the original models were trained on.
- Evidence anchors:
  - [abstract] mentions DAM is "data-informed," suggesting use of representative data.
  - [section] explicitly states: "The KL divergence loss for the merged model is given by: LKL = Σ KL(logitsmerged(Di)∥logitsi(Di))"
  - [corpus] shows related work (AdaMerging) uses similar entropy minimization, supporting this approach.
- Break condition: If the training datasets are not representative of the original training distributions, the KL divergence loss may push the merged model away from preserving true capabilities.

### Mechanism 3
- Claim: Cosine similarity regularization prevents scaling coefficients from different models from becoming too similar, promoting diversity in feature space utilization.
- Mechanism: The objective function includes a term that minimizes the cosine similarity between scaling coefficients of different models for each layer. This encourages each model to scale features in unique ways rather than converging to similar scaling patterns.
- Core assumption: Diversity in scaling patterns leads to better overall performance by allowing each model to contribute distinct capabilities.
- Evidence anchors:
  - [section] describes this component: "We add a constraint to reduce the cosine similarity between the scaling coefficients of different models for each layer."
  - [corpus] is weak—no direct neighbor papers discuss this specific regularization approach.
- Break condition: If the cosine similarity term is too strong, it might force unnecessary diversity even when models should share similar scaling patterns, potentially harming performance.

## Foundational Learning

- Concept: KL divergence and its role in measuring distribution similarity
  - Why needed here: Understanding KL divergence is crucial because DAM uses it as a primary loss component to ensure the merged model's outputs match individual models' outputs on their respective datasets.
  - Quick check question: What does it mean when KL divergence between two distributions is zero? (Answer: The distributions are identical)

- Concept: Gradient-based optimization for parameter learning
  - Why needed here: DAM uses gradient-based optimization to learn the scaling coefficients, which is more efficient than evolutionary strategies that require evaluating many combinations.
  - Quick check question: How does gradient-based optimization differ from evolutionary strategies in terms of computational efficiency? (Answer: Gradient-based methods use derivatives to directly optimize parameters, while evolutionary strategies require evaluating many candidate solutions)

- Concept: Catastrophic forgetting in model merging
  - Why needed here: Understanding catastrophic forgetting helps explain why DAM needs careful objective function design to preserve each model's capabilities while merging.
  - Quick check question: What happens during catastrophic forgetting when merging models? (Answer: The merged model loses capabilities that were present in the individual models)

## Architecture Onboarding

- Component map:
  Base model (parent model weights) -> Input models (specialized models to merge) -> DAM scaling layer (learns column-wise scaling coefficients) -> Loss computation module (KL divergence, cosine similarity, regularization) -> Training loop (optimizes scaling coefficients using representative datasets)

- Critical path:
  1. Load aligned models (same base model, preserved alignment)
  2. Prepare representative datasets for each model's specialization
  3. Initialize DAM scaling coefficients
  4. Forward pass through models with scaling applied
  5. Compute losses (KL divergence, cosine similarity, regularization)
  6. Backward pass to update scaling coefficients
  7. Repeat until convergence

- Design tradeoffs:
  - DAM vs Evolutionary Merging: DAM is much faster and more computationally efficient but may not explore the parameter space as thoroughly
  - Column-wise vs Layer-wise scaling: Column-wise allows finer control but increases the number of parameters to optimize
  - KL divergence vs Entropy loss: KL divergence requires individual model logits but may provide better preservation of model-specific capabilities

- Failure signatures:
  - Performance worse than individual models: Indicates poor scaling coefficient learning or misaligned models
  - One model's capabilities dominate: Suggests scaling coefficients are not properly balancing contributions
  - High variance across runs: May indicate instability in the optimization process or insufficient regularization

- First 3 experiments:
  1. DAM with KL divergence only (no cosine similarity or regularization) - tests the core mechanism
  2. DAM with cosine similarity regularization only - tests the diversity-promoting component
  3. DAM with both KL and cosine similarity - tests the full objective function as described in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does simple averaging (Model Soups) outperform more complex methods like DAM or evolutionary merging?
- Basis in paper: [explicit] "Our findings reveal that even simple averaging methods, like Model Soups, perform competitively when model similarity is high, underscoring each technique's unique strengths and limitations."
- Why unresolved: The paper identifies that Model Soups can be competitive when models are similar, but does not specify the exact thresholds or characteristics of model similarity that make this true. Factors like training method overlap, task alignment, and parameter distribution differences could all play a role.
- What evidence would resolve it: Controlled experiments varying degrees of model similarity (e.g., different base models, fine-tuning tasks, parameter distributions) while measuring performance of Model Soups vs. DAM could establish clear conditions for when averaging suffices.

### Open Question 2
- Question: Can DAM's column-wise scaling approach be effectively extended to non-linear layers or attention mechanisms beyond standard MLP and normalization layers?
- Basis in paper: [explicit] "DAM can be applied to all linear layers, embedding layers, and layer normalization layers within the models" but experiments show "merging only the linear layers, while retaining the embeddings and normalization weights from the base model, performs best."
- Why unresolved: The paper demonstrates DAM works on linear layers but limits itself to this subset, leaving questions about potential benefits or drawbacks of scaling other architectural components like attention weights, key/value projections, or non-linear activations.
- What evidence would resolve it: Systematic experiments applying DAM to different layer types (attention heads, non-linear activations, etc.) and measuring performance impacts would clarify whether extending beyond linear layers is beneficial.

### Open Question 3
- Question: How does the computational efficiency of DAM compare to evolutionary merging when scaling to larger model sizes (e.g., 70B+ parameters) or more complex merging scenarios?
- Basis in paper: [explicit] "DAM achieves similar or superior results with significantly fewer resources" compared to evolutionary methods, but the experiments use 7B-8B parameter models.
- Why unresolved: The paper establishes DAM's efficiency advantage on moderate-sized models, but doesn't explore whether this advantage persists or diminishes when scaling to frontier model sizes where evolutionary methods might benefit from parallelization or distributed computing.
- What evidence would resolve it: Direct runtime and resource consumption comparisons of DAM vs. evolutionary merging across multiple model scales (7B, 30B, 70B+) would reveal whether DAM's efficiency advantage is scale-invariant or diminishes at larger sizes.

## Limitations
- DAM requires aligned models (same base architecture), limiting its applicability to models with different initializations or architectures
- The computational efficiency claims lack direct empirical comparisons with evolutionary methods in terms of training time and resource usage
- Performance depends heavily on the quality and representativeness of the training datasets used for each model's specialization

## Confidence

**High Confidence**: DAM's core mechanism (column-wise scaling with KL divergence loss) is technically sound and well-described

**Medium Confidence**: DAM's performance claims are supported by experimental results, but the comparison methodology could be more rigorous

**Low Confidence**: Claims about DAM's efficiency advantages relative to evolutionary methods lack direct empirical support

## Next Checks
1. **Efficiency Benchmark**: Measure and compare wall-clock training time and GPU memory usage for DAM versus evolutionary merging on identical hardware
2. **Alignment Robustness**: Test DAM's performance when merging models with slight architectural differences or different initializations
3. **Scaling Coefficient Analysis**: Examine the learned scaling coefficients to verify they are indeed diverse (low cosine similarity) and that this diversity correlates with performance improvements