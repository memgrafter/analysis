---
ver: rpa2
title: What is the Visual Cognition Gap between Humans and Multimodal LLMs?
arxiv_id: '2406.10424'
source_url: https://arxiv.org/abs/2406.10424
tags:
- reasoning
- visual
- matrix
- mars-vqa
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MaRs-VQA, a visual question answering dataset
  designed to evaluate the visual cognition capabilities of multimodal large language
  models (MLLMs) through matrix reasoning tasks inspired by human intelligence tests
  like RPM and WISC. The dataset contains 1,440 image instances with VQA annotations
  and human performance baselines.
---

# What is the Visual Cognition Gap between Humans and Multimodal LLMs?

## Quick Facts
- arXiv ID: 2406.10424
- Source URL: https://arxiv.org/abs/2406.10424
- Reference count: 29
- Primary result: Current MLLMs perform significantly worse than humans on visual matrix reasoning tasks, with visual pattern extraction identified as the primary bottleneck

## Executive Summary
This paper introduces MaRs-VQA, a benchmark dataset designed to evaluate visual cognition capabilities of multimodal large language models through matrix reasoning tasks. The dataset contains 1,440 image instances with VQA annotations and human performance baselines. Experiments show that current MLLMs (including GPT-4o, Claude, and Qwen2-VL) perform significantly worse than humans on these tasks, even with chain-of-thought prompting. The authors fine-tuned a baseline model Qwen2-VCog using step-by-step reasoning annotations, achieving performance close to human adults on in-domain tasks but showing limited out-of-domain generalization. The results reveal that visual pattern extraction and working memory, rather than language reasoning, are the primary bottlenecks for MLLMs in abstract visual reasoning.

## Method Summary
The authors created MaRs-VQA, a visual question answering dataset with 1,440 matrix reasoning problems inspired by human intelligence tests. They evaluated multiple MLLMs using zero-shot inference with chain-of-thought prompting, then fine-tuned Qwen2-VL with step-by-step reasoning annotations to create Qwen2-VCog. The evaluation included both in-domain testing on MaRs-VQA and out-of-domain testing on RAVEN dataset. The chain-of-thought format consisted of two sections: Reasoning (describing visual pattern extraction and rule identification) and Answer (providing final predictions). Fine-tuning used LoRA adapters with structured reasoning annotations as supervision.

## Key Results
- MLLMs achieved only ~25% accuracy on MaRs-VQA compared to 90% human accuracy
- Qwen2-VCog fine-tuned with reasoning annotations achieved near-human performance on in-domain tasks but showed limited generalization to RAVEN dataset
- When deprived of option images, MLLM performance dropped to chance level, indicating failure in visual pattern extraction
- Scaling model size alone showed diminishing returns without architectural improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current MLLMs fail at visual pattern extraction rather than language reasoning
- Mechanism: When visual features are available through option images, MLLMs can match patterns effectively, but without visual input they perform at chance level
- Core assumption: Visual working memory and pattern extraction capabilities are the primary bottlenecks
- Evidence anchors:
  - [abstract] "visual pattern extraction and working memory, rather than language reasoning, are the primary bottlenecks"
  - [section] "when deprived of option images, indicating a failure to extract the underlying visual rules"

### Mechanism 2
- Claim: Fine-tuning with step-by-step reasoning annotations improves in-domain performance but shows limited generalization
- Mechanism: SFT with structured reasoning chains teaches models the expected reasoning format and specific patterns from training data
- Core assumption: The reasoning annotations provide crucial supervision signals that models lack during zero-shot inference
- Evidence anchors:
  - [abstract] "fine-tuned a baseline model Qwen2-VCog using step-by-step reasoning annotations, achieving performance close to human adults on in-domain tasks but showing limited out-of-domain generalization"

### Mechanism 3
- Claim: Scaling model size alone is insufficient to bridge the visual cognition gap
- Mechanism: Larger models perform better within limits but still fall short of human performance, suggesting architectural limitations
- Core assumption: Visual cognition requires capabilities beyond what current transformer architectures provide
- Evidence anchors:
  - [abstract] "scaling model size alone is insufficient to bridge the gap between MLLM and human visual cognition"

## Foundational Learning

- Concept: Visual working memory
  - Why needed here: The paper identifies visual working memory as a key bottleneck for MLLMs
  - Quick check question: What happens to MLLM performance when option images are removed from the input?

- Concept: Zero-shot inference
  - Why needed here: The benchmark evaluates models without task-specific training, similar to how humans approach these tests
  - Quick check question: How does the performance gap change when comparing zero-shot to fine-tuned models?

- Concept: Chain-of-thought reasoning
  - Why needed here: The paper uses structured CoT prompting to guide MLLM reasoning through multi-step visual analysis
  - Quick check question: What are the two distinct sections of the proposed CoT format?

## Architecture Onboarding

- Component map: Visual encoder (CLIP-based) -> Adapter layer -> Autoregressive decoder -> LoRA adapters -> Structured reasoning output formatter

- Critical path: Visual input → Feature extraction → Pattern matching across images → Rule identification → Option verification → Final answer generation

- Design tradeoffs:
  - Visual encoder capacity vs. model size constraints
  - Language reasoning strength vs. visual pattern extraction capability
  - Fine-tuning efficiency vs. generalization ability

- Failure signatures:
  - Performance at chance level without option images
  - Large gap between in-domain and out-of-domain performance after fine-tuning
  - Inability to handle multiple simultaneous attribute changes

- First 3 experiments:
  1. Test MLLM performance with only question image vs. with option images included
  2. Compare fine-tuned vs. zero-shot performance on in-domain and out-of-domain datasets
  3. Evaluate impact of adding textual descriptions of options to visual input

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to visual encoders would most effectively improve MLLMs' performance on matrix reasoning tasks?
- Basis in paper: Explicit - The paper identifies visual pattern extraction and working memory as primary bottlenecks
- Why unresolved: The paper identifies the problem but does not propose specific architectural solutions or test them experimentally

### Open Question 2
- Question: How does the developmental trajectory of matrix reasoning ability in humans compare to the learning patterns of MLLMs when trained on similar visual reasoning tasks?
- Basis in paper: Explicit - The paper notes that "Matrix reasoning ability develops early in human neurodevelopment" and contrasts this with MLLMs that "rely on training data"
- Why unresolved: The paper identifies the developmental difference but does not explore the underlying mechanisms

### Open Question 3
- Question: What is the minimum amount and type of training data required for MLLMs to achieve human-level zero-shot matrix reasoning performance?
- Basis in paper: Explicit - The paper states that "scaling laws have some applicability to visual cognition tasks, but merely increasing model size and training data is insufficient"
- Why unresolved: The paper demonstrates current limitations but does not systematically explore data scaling approaches

## Limitations

- The evaluation relies on synthetic benchmark datasets that may not fully capture real-world visual cognition complexity
- Fine-tuning shows limited out-of-domain generalization, raising questions about whether models learn general principles or memorize patterns
- Human baseline comparisons may not account for time pressure, fatigue, or expertise level variations

## Confidence

- **High Confidence**: Experimental results showing MLLMs performing significantly worse than humans on MaRs-VQA tasks are well-supported by data
- **Medium Confidence**: Visual pattern extraction and working memory identified as primary bottlenecks have supporting evidence but could benefit from more direct measurement
- **Low Confidence**: Assertion that current MLLM architectures fundamentally cannot bridge the visual cognition gap may be premature given rapid architectural innovations

## Next Checks

1. **Cross-dataset Generalization Test**: Evaluate Qwen2-VCog on additional visual reasoning datasets beyond RAVEN to assess true generalization capabilities

2. **Visual Working Memory Ablation**: Design controlled experiments varying the number of visual elements MLLMs must track simultaneously to quantify the relationship between visual working memory and reasoning performance

3. **Architectural Innovation Comparison**: Implement and test alternative visual architectures on MaRs-VQA benchmark to determine whether performance gap is due to architectural limitations or other factors