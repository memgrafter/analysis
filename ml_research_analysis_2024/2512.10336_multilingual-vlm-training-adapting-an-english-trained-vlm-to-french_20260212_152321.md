---
ver: rpa2
title: 'Multilingual VLM Training: Adapting an English-Trained VLM to French'
arxiv_id: '2512.10336'
source_url: https://arxiv.org/abs/2512.10336
tags:
- french
- translation
- finetuning
- english
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines how to adapt an English-trained Vision-Language
  Model (VLM) to French, using both a translation-only pipeline and finetuning approaches.
  Experiments employ a Llama-based architecture and translate existing English datasets
  to French for training and evaluation.
---

# Multilingual VLM Training: Adapting an English-Trained VLM to French

## Quick Facts
- **arXiv ID**: 2512.10336
- **Source URL**: https://arxiv.org/abs/2512.10336
- **Reference count**: 7
- **One-line primary result**: Translation-only pipeline outperforms finetuning for French VLM adaptation due to data quality issues.

## Executive Summary
This work examines how to adapt an English-trained Vision-Language Model (VLM) to French, using both a translation-only pipeline and finetuning approaches. Experiments employ a Llama-based architecture and translate existing English datasets to French for training and evaluation. Manual translation audits reveal that 60% of question-answer pairs are high quality, but only 40% are suitable for precise evaluation or training. Benchmarks show that the translation-only approach consistently outperforms finetuned models across ScienceQA, TextVQA, and POPE, suggesting that noisy or scarce French data can degrade finetuning performance. Loss curves indicate stable learning during both pretraining and finetuning stages. The results point to translation-induced noise as the main bottleneck, highlighting the need for native-language multimodal datasets and improved translation quality to advance multilingual VLM performance.

## Method Summary
The study adapts an English-trained VLM to French using four approaches: an I/O translation pipeline with no training, English pretraining followed by French LoRA finetuning, French pretraining followed by French finetuning, and a two-stage double finetuning approach (vision-first, then LLM). The VLM combines OpenHermes 2.5 Mistral 7B (LLM) with ViT SO400M 14 SigLIP 384 (vision encoder). Pretraining uses Train Monkey 2 while finetuning uses translated blip_laion_cc_sbu_558k. Translation is performed using Llama-3.1-8B-Instruct. Models are evaluated on translated ScienceQA, TextVQA, and POPE benchmarks, with manual translation quality audits categorizing outputs as high, moderate, or low quality.

## Key Results
- Translation-only pipeline consistently outperforms finetuned models across all three benchmarks (ScienceQA, TextVQA, POPE)
- Manual audit shows 60% of translated QA pairs are high quality, but only 40% suitable for training/evaluation
- Finetuned models exhibit pathological behavior, such as always outputting "yes" on POPE
- Loss curves demonstrate stable learning during both pretraining and finetuning stages

## Why This Works (Mechanism)
The translation-only pipeline succeeds because it preserves the original English VLM's strong performance without introducing finetuning-induced artifacts. The finetuned models' pathological behavior (e.g., always outputting "yes" on POPE) suggests that translation noise combined with limited negative examples in the training data creates severe class imbalance, causing the model to learn degenerate decision boundaries.

## Foundational Learning
- **Vision-Language Model Architecture**: Combines image encoder with language model through cross-attention; needed to understand multimodal input processing; quick check: verify projection layer integration between vision and language components.
- **LoRA Finetuning**: Parameter-efficient adaptation using low-rank adapters; needed to reduce computational cost of multilingual adaptation; quick check: confirm LoRA ranks (r=128) and scaling factors (α=256) are correctly applied.
- **Translation Pipeline**: Uses English-to-French translation for input and French-to-English for output; needed to leverage existing English-trained models; quick check: validate translation quality scores meet minimum thresholds for each dataset.
- **Manual Quality Auditing**: Categorizes translated QA pairs as high/moderate/low quality; needed to quantify data reliability for training/evaluation; quick check: ensure inter-rater agreement for quality assessments.
- **Double Finetuning Strategy**: Two-stage approach (vision then LLM) with near-zero encoder learning in second stage; needed to prevent catastrophic forgetting; quick check: verify stage 2 encoder LR is approximately zero as specified.
- **Benchmark Translation**: Translates evaluation datasets while preserving semantic integrity; needed for fair multilingual comparison; quick check: compare human and machine translation outputs for sample questions.

## Architecture Onboarding

**Component Map**: Input Text+Image -> Vision Encoder -> Projection -> LLM -> Output Text

**Critical Path**: Input -> Vision Encoder -> Cross-Attention -> LLM -> Output

**Design Tradeoffs**: Translation-only vs. finetuning balances computational cost against model adaptation quality; manual auditing trades scalability for data quality assessment.

**Failure Signatures**: Finetuned models output "yes" consistently on POPE; translation errors propagate through evaluation pipeline; inadequate negative examples in training data cause class imbalance.

**First Experiments**:
1. Build I/O translation pipeline using Llama-3.1-8B-Instruct with provided system prompt, translate French inputs to English, run English VLM, translate outputs back.
2. Prepare translated datasets by converting Train Monkey 2 and blip_laion_cc_sbu_558k to French using translation model.
3. Train three model variants: English pretrain→French LoRA finetune, French pretrain→French finetune, and two-stage double finetuning with specified hyperparameters.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out specific open questions, but the limitations section implicitly raises several important directions: the impact of translation quality on downstream VLM performance, the generalizability of these findings to other language pairs with different linguistic structures, and the effectiveness of finetuning approaches when applied to native-language multimodal datasets rather than translated ones.

## Limitations
- Translation model quality (Llama-3.1-8B-Instruct) fundamentally constrains pipeline effectiveness, with only 60% of QA pairs rated high quality
- Evaluation methodology relies on translated datasets that may not preserve semantic and contextual integrity of original English benchmarks
- Poor finetuning performance attributed to "noisy data" without detailed analysis of specific failure modes or training data balance issues
- Limited exploration of alternative translation models or human translation alternatives that could improve pipeline quality
- Single language pair focus limits generalizability to other languages with different grammatical and semantic structures

## Confidence
- **High Confidence**: Experimental setup clearly described with specified model architectures, training procedures, and evaluation benchmarks; loss curves provide evidence of stable training
- **Medium Confidence**: Conclusion that translation-only outperforms finetuning is well-supported, but underlying reasons (data quality vs. training methodology) are not definitively established
- **Low Confidence**: Generalizability to other language pairs or VLM architectures remains uncertain; paper doesn't explore whether results hold for languages with different linguistic structures

## Next Checks
1. Conduct systematic error analysis comparing translation model outputs against professional human translations for dataset subset to measure how translation errors impact VLM performance on different task types.
2. Analyze distribution of yes/no responses and categorical outputs in translated training data for each finetuning experiment to verify correlation between training data imbalance and pathological "always yes" behavior on POPE.
3. Test finetuning approach on small native French multimodal dataset (if available) to isolate whether poor performance stems from translation quality or finetuning methodology itself.