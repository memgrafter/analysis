---
ver: rpa2
title: Can Large Language Models Identify Authorship?
arxiv_id: '2403.08213'
source_url: https://arxiv.org/abs/2403.08213
tags:
- authorship
- text
- llms
- texts
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether Large Language Models (LLMs) can perform
  authorship verification and attribution in a zero-shot setting without domain-specific
  fine-tuning. The authors propose Linguistically Informed Prompting (LIP), which
  guides LLMs to analyze linguistic features like phrasal verbs, punctuation, and
  humor for authorship analysis.
---

# Can Large Language Models Identify Authorship?

## Quick Facts
- arXiv ID: 2403.08213
- Source URL: https://arxiv.org/abs/2403.08213
- Authors: Baixiang Huang; Canyu Chen; Kai Shu
- Reference count: 29
- Primary result: GPT-4 with Linguistically Informed Prompting achieves 86.67% accuracy for authorship verification and 90% weighted F1 for attribution among 10 candidates

## Executive Summary
This paper evaluates whether Large Language Models can perform authorship verification and attribution tasks in a zero-shot setting without fine-tuning. The authors propose Linguistically Informed Prompting (LIP), which guides LLMs to analyze linguistic features like phrasal verbs, punctuation, and humor for authorship analysis. Experiments on Blog and Email datasets demonstrate that GPT-4 with LIP significantly outperforms traditional models like BERT, achieving high accuracy while providing explainable results through natural language analysis of linguistic features.

## Method Summary
The authors propose a zero-shot approach using Large Language Models for authorship verification (determining if two texts share an author) and attribution (identifying the author among candidates). They develop Linguistically Informed Prompting (LIP) that guides LLMs to analyze specific linguistic features such as phrasal verbs, punctuation, rare words, humor, and sarcasm. The method is evaluated on two datasets - Blog Authorship Attribution corpus and Enron Email dataset - using various prompt types ranging from no guidance to LIP. The approach leverages the inherent linguistic knowledge within pre-trained LLMs without requiring domain-specific fine-tuning.

## Key Results
- GPT-4 with LIP achieves 86.67% accuracy for authorship verification and 90% weighted F1 for attribution among 10 candidates
- LIP outperforms traditional models like BERT and TF-IDF in both authorship verification and attribution tasks
- The approach provides explainable results by generating natural language analysis of linguistic features from the text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linguistically Informed Prompting (LIP) leverages LLMs' inherent linguistic knowledge to guide authorship analysis.
- Mechanism: LIP provides explicit linguistic features as reasoning context to direct the LLM's attention toward relevant stylistic cues.
- Core assumption: LLMs possess embedded linguistic knowledge that can be activated through targeted prompting.
- Evidence anchors: The paper states "we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes" and "This approach exploits the inherent linguistic knowledge embedded within LLMs."
- Break condition: If the LLM lacks sufficient training data on linguistic features, or if the features provided don't align with the author's actual writing patterns.

### Mechanism 2
- Claim: Zero-shot LLMs can perform authorship verification without fine-tuning.
- Mechanism: LLMs utilize their pre-trained understanding of language patterns and stylistic variations to compare two texts and determine if they share the same author.
- Core assumption: Pre-trained LLMs have learned sufficient stylistic information across diverse text corpora to generalize to authorship tasks.
- Evidence anchors: "Our assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning" and "GPT-4 consistently outperforms the other models in both datasets."
- Break condition: When texts are too short, from underrepresented domains, or when stylistic differences are subtle.

### Mechanism 3
- Claim: LIP improves explainability by generating natural language analysis of linguistic features.
- Mechanism: By explicitly prompting for linguistic feature analysis, the LLM provides interpretable reasoning that references specific text elements.
- Core assumption: LLMs can generate coherent, relevant explanations when properly prompted to focus on specific features.
- Evidence anchors: "Our end-to-end approach improves the explainability of authorship analysis" and "LLMs can provide explanations in natural language regarding their decision-making processes."
- Break condition: If the LLM generates generic or irrelevant explanations, or if the linguistic features don't actually correlate with authorship.

## Foundational Learning

- Concept: Stylometry and linguistic feature extraction
  - Why needed here: Authorship analysis fundamentally relies on identifying distinctive writing patterns, which requires understanding what features are meaningful for authorship attribution
  - Quick check question: What are the key linguistic features that forensic linguists use to distinguish authors, and why are features like humor and sarcasm particularly valuable?

- Concept: Zero-shot learning and prompt engineering
  - Why needed here: The approach depends on leveraging pre-trained models without fine-tuning, requiring knowledge of how to craft effective prompts that guide reasoning
  - Quick check question: How does prompt specificity affect LLM performance on specialized tasks like authorship analysis, and what's the difference between few-shot and zero-shot prompting?

- Concept: Evaluation metrics for classification tasks
  - Why needed here: Understanding Weighted F1, Macro F1, and Micro F1 is essential for interpreting model performance, especially with imbalanced datasets
  - Quick check question: When would Macro F1 be more informative than Weighted F1 for authorship attribution, and what does each metric reveal about model behavior?

## Architecture Onboarding

- Component map: Data preprocessing pipeline → Dataset sampler → Prompt generator (LIP/no_guidance/little_guidance/grammar_guidance) → LLM API (GPT-3.5 Turbo, GPT-4 Turbo) → JSON parser → Evaluation metrics calculator → Supporting components: Corpus neighbor finder, experiment runner, result visualizer

- Critical path: 1. Sample balanced dataset pairs/texts 2. Generate appropriate prompts with linguistic guidance 3. Send to LLM and parse JSON response 4. Calculate evaluation metrics 5. Compare against baseline models (BERT, TF-IDF)

- Design tradeoffs: Zero-shot vs. fine-tuned (avoids training overhead but may sacrifice accuracy on specialized domains), Prompt specificity vs. flexibility (more specific prompts improve accuracy but may miss unexpected patterns), Token limits vs. analysis depth (longer texts provide more features but may exceed context windows)

- Failure signatures: Consistently low accuracy across all prompt types suggests the LLM lacks sufficient stylistic understanding, High variance between prompt types indicates sensitivity to prompt formulation, Performance degradation with more candidate authors suggests scalability issues

- First 3 experiments: 1. Run authorship verification with no_guidance vs. LIP on Blog dataset to quantify prompt impact 2. Test attribution with 10 vs. 20 candidate authors to assess scalability 3. Perform ablation study removing individual linguistic features to identify most impactful ones

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on authorship verification when the number of candidate authors increases to 50 or 100, particularly in real-world scenarios like social media platforms?
- Basis in paper: The paper mentions scalability as a limitation when the number of candidate authors increases, noting that performance may degrade in real-world scenarios with many potential authors.
- Why unresolved: The paper only tests up to 20 candidate authors, leaving uncertainty about performance at larger scales common in real-world applications.
- What evidence would resolve it: Conducting experiments with 50-100 candidate authors across different datasets would provide concrete performance metrics and identify any performance degradation thresholds.

### Open Question 2
- Question: Can the LIP method effectively distinguish between human-authored and LLM-generated texts, given the increasing prevalence of machine-generated content?
- Basis in paper: The paper identifies this as a limitation, stating that their method may not effectively distinguish between human-authored and machine-generated texts.
- Why unresolved: The experiments only evaluate performance on human-authored texts, without testing the model's ability to detect machine-generated content.
- What evidence would resolve it: Testing the model on datasets containing both human and LLM-generated texts would reveal whether LIP can identify authorship patterns unique to machine-generated content.

### Open Question 3
- Question: What specific neural mechanisms within LLMs contribute to their authorship attribution decisions, beyond the linguistic features they identify?
- Basis in paper: The paper acknowledges that while LLMs provide explainable outputs through linguistic features, the mechanistic interpretability of how these decisions are made at the neuronal level remains unexplored.
- Why unresolved: Current analysis only examines the linguistic features highlighted by LLMs, not the internal neural processes that lead to these decisions.
- What evidence would resolve it: Conducting mechanistic interpretability studies, such as feature attribution or neural network analysis, would reveal how specific neurons or attention patterns contribute to authorship attribution decisions.

## Limitations
- Evaluation relies on two specific datasets (Blog and Email corpora) which may not represent the full diversity of authorship scenarios
- Performance metrics don't account for adversarial scenarios where authors deliberately attempt to mask their writing style
- The zero-shot approach may struggle with domain-specific writing styles or languages other than English

## Confidence
- **High Confidence**: GPT-4 with LIP outperforming other approaches on the tested datasets (supported by specific accuracy and F1 scores)
- **Medium Confidence**: The general mechanism of using linguistic features to guide LLMs (based on reasonable assumptions about LLM capabilities but limited direct evidence)
- **Low Confidence**: The scalability claims for authorship attribution with many candidates (only tested up to 10 candidates, with performance degradation noted)

## Next Checks
1. **Cross-domain validation**: Test the LIP approach on additional text types (academic papers, social media posts, legal documents) to assess generalizability beyond blogs and emails.

2. **Adversarial testing**: Evaluate whether deliberately obfuscated texts or texts with mixed authorship styles can fool the LLM, establishing robustness boundaries.

3. **Explainability audit**: Systematically verify whether the linguistic features cited in LLM explanations actually correlate with correct predictions, using human annotation to validate the explanations' accuracy.