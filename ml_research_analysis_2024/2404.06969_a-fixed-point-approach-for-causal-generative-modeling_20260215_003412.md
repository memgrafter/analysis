---
ver: rpa2
title: A Fixed-Point Approach for Causal Generative Modeling
arxiv_id: '2404.06969'
source_url: https://arxiv.org/abs/2404.06969
tags:
- causal
- fixed-point
- scms
- then
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new framework for Structural Causal Models
  (SCMs) based on fixed-point problems on causally ordered variables, eliminating
  the need for Directed Acyclic Graphs (DAGs). The authors establish conditions under
  which SCMs can be uniquely recovered given the topological ordering (TO) and propose
  a two-stage causal generative model: (1) inferring the causal order from observations
  using a zero-shot method that predicts leaves of graphs during training, and (2)
  learning the fixed-point SCM using a transformer-based architecture with a new attention
  mechanism.'
---

# A Fixed-Point Approach for Causal Generative Modeling

## Quick Facts
- arXiv ID: 2404.06969
- Source URL: https://arxiv.org/abs/2404.06969
- Reference count: 40
- One-line primary result: Introduces a fixed-point SCM framework that eliminates DAGs, achieving high accuracy in causal discovery and counterfactual prediction on generated out-of-distribution datasets

## Executive Summary
This paper proposes a novel framework for Structural Causal Models (SCMs) based on fixed-point problems on causally ordered variables, eliminating the need for Directed Acyclic Graphs (DAGs). The authors establish conditions under which SCMs can be uniquely recovered given the topological ordering (TO) and propose a two-stage causal generative model: (1) inferring the causal order from observations using a zero-shot method that predicts leaves of graphs during training, and (2) learning the fixed-point SCM using a transformer-based architecture with a new attention mechanism. The model is evaluated on generated out-of-distribution datasets, outperforming various baselines on causal discovery and inference tasks. The proposed method consistently achieves high accuracy in recovering the true causal graphs and counterfactual samples.

## Method Summary
The method consists of a two-stage approach for causal generative modeling. First, a zero-shot topological ordering (TO) inference model is trained to predict causal orderings from observational data by predicting leaves of synthetic DAGs during training. Second, a transformer-based fixed-point SCM (TANM) is learned using the predicted TOs, employing a novel causal attention mechanism that relaxes the row-sum constraint of standard attention to enable flexible DAG modeling while preserving the triangular Jacobian property. The model is trained using MSE loss on fixed-point equations and generates new samples by estimating quantile functions for noise sampling.

## Key Results
- Outperforms baselines on causal discovery tasks with high F1 scores for graph recovery
- Achieves accurate counterfactual predictions with low re-scaled ℓ2 distance to ground truth
- Demonstrates robustness to out-of-distribution data with consistent performance across varying graph densities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fixed-point reformulation of SCMs eliminates the need for DAGs by leveraging causal ordering directly in the function parameterization.
- Mechanism: By representing SCMs as fixed-point problems on causally ordered variables, the model only requires the topological order instead of the full graph structure. This allows learning causal relationships without explicitly searching the combinatorial DAG space.
- Core assumption: The causal ordering is known or can be inferred from data, and the structural equations satisfy lower-triangular Jacobian constraints.
- Evidence anchors:
  - [abstract] "propose a novel formalism for describing Structural Causal Models (SCMs) as fixed-point problems on causally ordered variables, eliminating the need for Directed Acyclic Graphs (DAGs)"
  - [section 2.5] "We are now ready to present our new definition of SCMs"
  - [corpus] Weak evidence - neighbor papers focus on causal discovery but don't address DAG elimination directly
- Break condition: If the causal ordering cannot be uniquely determined from data, or if the structural equations violate the lower-triangular constraint.

### Mechanism 2
- Claim: The two-stage approach (infer TO, then learn SCM) bypasses the NP-hard DAG search problem.
- Mechanism: First, a zero-shot TO inference model predicts leaves sequentially from observations, avoiding combinatorial search. Second, the fixed-point SCM is learned using a transformer architecture that exploits causal attention.
- Core assumption: TOs can be inferred from observational data without ground truth labels, and the causal structure is preserved through attention mechanisms.
- Evidence anchors:
  - [abstract] "first infers in a zero-shot manner a valid TO from observations, and then learns the generative SCM on the ordered variables"
  - [section 3] "we propose to amortize the learning of a zero-shot TO inference method from observations on synthetically generated datasets"
  - [corpus] Moderate evidence - neighbor papers discuss amortized inference but not sequential TO prediction
- Break condition: If the TO inference model fails to predict valid orderings, or if the causal attention cannot properly model the structural constraints.

### Mechanism 3
- Claim: The causal attention mechanism enables modeling of causal structures while preserving the triangular Jacobian property.
- Mechanism: The proposed attention matrix allows rows to sum to any value between [0,1], unlike standard attention which forces rows to sum to 1. This enables modeling of root nodes and arbitrary DAG structures while maintaining the lower-triangular constraint.
- Core assumption: The causal attention can be optimized to satisfy the structural constraints while learning meaningful relationships.
- Evidence anchors:
  - [section 4.1] "we propose to relax this constraint in the following" and "by relaxing the constraint of the optimal transport problem"
  - [section 4.1] "Therefore the proposed causal encoder layer can parameterize a whole family of fixed-point SCM in a latent space"
  - [corpus] Weak evidence - neighbor papers don't discuss attention mechanisms for causal modeling
- Break condition: If the attention mechanism cannot maintain the lower-triangular constraint during optimization, or if the learned relationships don't correspond to valid causal structures.

## Foundational Learning

- Concept: Structural Causal Models and DAGs
  - Why needed here: Understanding the relationship between SCMs and DAGs is crucial for grasping why the fixed-point reformulation is significant
  - Quick check question: What is the main computational challenge in learning SCMs from observational data?

- Concept: Topological Ordering and Causal Ordering
  - Why needed here: The paper relies on topological ordering as a key concept for the fixed-point reformulation
  - Quick check question: How does a topological ordering relate to the causal structure of a DAG?

- Concept: Triangular Maps and Jacobian Constraints
  - Why needed here: The fixed-point formulation requires functions with specific Jacobian properties (lower-triangular)
  - Quick check question: What property must a function's Jacobian have to preserve the causal ordering in the fixed-point formulation?

## Architecture Onboarding

- Component map:
  - Data → Encoder → Causal Embedding → Causal Attention → Causal Encoder → Causal Decoder → Output
  - TO Inference Model (M) → Fixed-Point SCM Model (TANM)
  - Training: MSE loss on fixed-point equations
  - Generation: Quantile function estimation for noise sampling

- Critical path:
  1. Train TO inference model M on synthetic datasets
  2. Use M to predict TO from new observations
  3. Train TANM using predicted TO and MSE loss
  4. Generate new samples using estimated quantile functions

- Design tradeoffs:
  - Fixed-point vs autoregressive approaches: Fixed-point allows direct modeling of causal structure but requires careful Jacobian constraints
  - Attention mechanism: Causal attention enables flexible DAG modeling but adds complexity compared to standard attention
  - TO inference: Zero-shot prediction avoids search but requires synthetic data for training

- Failure signatures:
  - Poor TO prediction: TO inference model fails to generalize to new data distributions
  - Jacobian constraint violation: Learned functions don't maintain lower-triangular structure
  - Convergence issues: Fixed-point equations don't converge during training

- First 3 experiments:
  1. Verify TO inference model predicts valid orderings on synthetic test data
  2. Check Jacobian constraints are maintained during TANM training
  3. Test counterfactual prediction accuracy on simple synthetic SCMs

## Open Questions the Paper Calls Out

- Question: Can the proposed framework handle cyclic causal structures?
  - Basis in paper: The authors explicitly state their model is designed for DAGs and do not address cyclic graphs.
  - Why unresolved: The paper focuses on acyclic structures and does not provide any theoretical or empirical evidence for handling cycles.
  - What evidence would resolve it: Demonstrating successful recovery of causal relationships in synthetic or real-world datasets with cyclic structures.

- Question: How does the model perform on high-dimensional data (e.g., >1000 variables)?
  - Basis in paper: The authors mention scalability limitations and discuss potential solutions but do not provide experimental results.
  - Why unresolved: The paper focuses on problems with up to 100 variables and does not evaluate the model's performance on high-dimensional data.
  - What evidence would resolve it: Empirical results showing the model's performance on datasets with >1000 variables, including computational time and memory usage.

- Question: Can the model handle discrete and mixed-type variables?
  - Basis in paper: The authors focus on continuous variables and do not address discrete or mixed-type data.
  - Why unresolved: The paper does not provide any theoretical framework or experimental results for handling discrete or mixed-type variables.
  - What evidence would resolve it: Demonstrating successful recovery of causal relationships in datasets containing discrete and/or mixed-type variables.

## Limitations

- Relies entirely on synthetic datasets, limiting generalizability to real-world applications
- Performance depends heavily on accurate topological ordering inference from observational data
- Causal attention mechanism adds complexity that may affect optimization stability

## Confidence

- High confidence in the mathematical framework and fixed-point formulation
- Medium confidence in the TO inference approach and its zero-shot capability
- Medium confidence in the causal attention mechanism's ability to preserve structural constraints
- Low confidence in real-world applicability without validation on actual causal systems

## Next Checks

1. **Cross-distribution robustness test**: Evaluate TO inference accuracy when training and test distributions differ significantly, particularly for real-world datasets where synthetic data characteristics may not transfer

2. **Jacobian constraint verification**: Implement systematic checks during training to ensure the lower-triangular Jacobian property is maintained, including monitoring constraint violations and their impact on convergence

3. **Ablation of attention mechanism**: Test whether standard attention with explicit masking can achieve comparable results to the proposed causal attention, isolating the contribution of the softmax relaxation approach