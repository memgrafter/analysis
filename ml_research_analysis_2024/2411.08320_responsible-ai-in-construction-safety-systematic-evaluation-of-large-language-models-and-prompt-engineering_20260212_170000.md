---
ver: rpa2
title: 'Responsible AI in Construction Safety: Systematic Evaluation of Large Language
  Models and Prompt Engineering'
arxiv_id: '2411.08320'
source_url: https://arxiv.org/abs/2411.08320
tags:
- llms
- safety
- construction
- knowledge
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically evaluates two large language models (GPT-3.5
  and GPT-4o) for construction safety applications using standardized BCSP certification
  exams. Across 385 multiple-choice questions in seven safety knowledge areas, both
  models significantly exceeded the 60% passing threshold, with GPT-4o achieving 84.6%
  accuracy and GPT-3.5 reaching 73.8%.
---

# Responsible AI in Construction Safety: Systematic Evaluation of Large Language Models and Prompt Engineering

## Quick Facts
- arXiv ID: 2411.08320
- Source URL: https://arxiv.org/abs/2411.08320
- Authors: Farouq Sammour; Jia Xu; Xi Wang; Mo Hu; Zhenyu Zhang
- Reference count: 0
- Primary result: GPT-4o achieved 84.6% accuracy on BCSP safety exams, significantly exceeding the 60% passing threshold

## Executive Summary
This study systematically evaluates two large language models (GPT-3.5 and GPT-4o) for construction safety applications using standardized BCSP certification exams. Across 385 multiple-choice questions spanning seven safety knowledge areas, both models significantly exceeded the 60% passing threshold, with GPT-4o achieving 84.6% accuracy and GPT-3.5 reaching 73.8%. The research examines model performance, reliability, consistency, and the impact of prompt engineering strategies, providing evidence-based insights for responsible AI integration in construction safety management.

## Method Summary
The study used 385 multiple-choice questions from BCSP ASP, CSP, and CHST exams covering seven safety knowledge areas. Questions were processed through a Python test environment using OpenAI API with GPT-3.5 and GPT-4o models. Five prompt configurations (Direct, Chain-of-Thought, Few-Shot) with three output format variations were tested across five repeated trials per configuration. Temperature was set to 0 and seed to 42 for reproducibility. Performance was evaluated using accuracy, entropy-based consistency, and repeated-measures ANOVA for reliability analysis.

## Key Results
- GPT-4o achieved 84.6% accuracy while GPT-3.5 reached 73.8%, both exceeding the 60% BCSP passing threshold
- GPT-4o demonstrated consistent performance with a narrow 7.9% accuracy range across knowledge areas, while GPT-3.5 showed greater variability (64.1%-77.6%)
- Both models excelled in safety management systems and hazard identification but struggled with science, mathematics, and emergency response topics
- Prompt engineering strategies showed variable effectiveness, with accuracy differences up to 13.5% for GPT-3.5 and 7.9% for GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve high accuracy on standardized safety exams due to pre-trained knowledge on domain-specific corpora.
- Mechanism: Large-scale pretraining on web and academic data includes safety standards, occupational health texts, and regulatory guidelines, enabling models to recognize correct patterns and terminology.
- Core assumption: The BCSP exams contain questions whose answers are derivable from information available in the pretraining corpus.
- Evidence anchors:
  - [abstract]: "Both models consistently exceed the BCSP benchmark, with GPT-4o achieving an accuracy rate of 84.6% and GPT-3.5 reaching 73.8%."
  - [section]: "LLMs are pre-trained on vast text datasets using natural language processing and deep learning techniques."
  - [corpus]: Weak evidence; no direct citations of safety exam content in corpus abstracts.
- Break condition: If questions require real-time interpretation of site-specific scenarios or novel combinations not represented in pretraining data.

### Mechanism 2
- Claim: Prompt engineering can influence LLM performance, but effects are model- and knowledge-area-specific.
- Mechanism: Different prompting strategies (CoT, FS, DP) activate distinct reasoning paths and context interpretation heuristics, with optimal choice varying by subfield and model.
- Core assumption: The internal reasoning mechanisms of LLMs respond differently to structured input depending on task complexity and model architecture.
- Evidence anchors:
  - [abstract]: "Our study also highlights the impact of prompt engineering strategies, with variations in accuracy reaching 13.5% for GPT-3.5 and 7.9% for GPT-4o."
  - [section]: "Separate ANOVA analyses were conducted for each KA-model combination... no single prompt configuration is universally optimal across all KAs."
  - [corpus]: Weak evidence; no prompt engineering studies specific to safety domains in corpus.
- Break condition: When prompt effects are neutralized by dominant pre-trained knowledge patterns or when no prompts significantly affect accuracy.

### Mechanism 3
- Claim: LLMs exhibit reliable and consistent performance across multiple exam attempts when parameters are fixed.
- Mechanism: Deterministic settings (temperature=0, fixed seed) suppress stochastic output variation, ensuring reproducibility for fact-based questions.
- Core assumption: The LLM's internal state and output generation are fully controlled by hyperparameters, eliminating randomness in responses to identical inputs.
- Evidence anchors:
  - [abstract]: "Reliability was evaluated through repeated measures ANOVA... no significant differences in average accuracy across repetitions for any configuration tested."
  - [section]: "Given the fact-based and non-creative nature of MCQs, the temperature for both GPT models was set to 0, minimizing output randomness."
  - [corpus]: No corpus evidence directly supporting reliability claims; missing from abstracts.
- Break condition: When model updates, context window limits, or unstated stochastic behaviors alter outputs between trials.

## Foundational Learning

- Concept: BCSP certification exam structure and content domains.
  - Why needed here: Understanding the scope of questions ensures correct interpretation of LLM accuracy results and their practical applicability.
  - Quick check question: What are the seven knowledge areas covered in BCSP exams and why do they matter for safety competency evaluation?

- Concept: ANOVA and statistical testing for comparing model configurations.
  - Why needed here: Proper experimental design and significance testing validate whether observed accuracy differences are meaningful or due to chance.
  - Quick check question: How does a repeated-measures ANOVA differ from a standard one-way ANOVA in evaluating LLM reliability?

- Concept: Entropy as a consistency metric for LLM responses.
  - Why needed here: Quantifies variability in LLM answers to the same question, indicating stability and trustworthiness of outputs.
  - Quick check question: What entropy value range indicates maximum consistency, and how is it calculated for multiple-choice answers?

## Architecture Onboarding

- Component map:
  - Dataset layer: 385 MCQs from BCSP ASP, CSP, and CHST exams, evenly distributed across seven KAs
  - Test environment layer: Python-based API interface to GPT-3.5 and GPT-4o with controlled hyperparameters
  - Prompt layer: Direct, Chain-of-Thought, and Few-Shot techniques with structured JSON output formats
  - Evaluation layer: Accuracy, reliability (repeated trials), and consistency (entropy) metrics
  - Analysis layer: Descriptive stats, ANOVA, and error classification (knowledge gaps, reasoning flaws, memory issues, calculation errors)

- Critical path:
  1. Load MCQ dataset
  2. Apply prompt configuration
  3. Send question to LLM via API
  4. Parse JSON output for answer and reasoning
  5. Compare to ground truth
  6. Store results for statistical analysis

- Design tradeoffs:
  - Fixed temperature (0) vs. stochastic exploration: favors reproducibility but may suppress beneficial variability
  - Single question per API call vs. batch: ensures memory reset but increases latency
  - Entropy calculation with 5 possible responses vs. 4: includes error state for robustness

- Failure signatures:
  - Low accuracy on science/math KAs: indicates insufficient pretraining coverage or weak calculation ability
  - High entropy in low accuracy bins: reveals instability when model encounters unfamiliar topics
  - Model-specific prompt effectiveness variance: suggests architectural or training data differences impact reasoning

- First 3 experiments:
  1. Run baseline DP prompts across all KAs for both models to establish control accuracy
  2. Test CoT prompts only on KA1 (Science/Math) to measure reasoning impact on calculation-heavy questions
  3. Compare FS vs. DP on KA2 (Management Systems) to assess benefit of example-based guidance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary across different construction safety tasks and scenarios compared to their performance on standardized multiple-choice exams?
- Basis in paper: [inferred] The study acknowledges that MCQs limit assessment of LLMs' ability to creatively and meaningfully apply knowledge in real-world scenarios, and suggests future research should incorporate long-form, open-ended questions.
- Why unresolved: The study primarily used standardized multiple-choice questions, which do not fully capture the complexity and variability of real-world construction safety situations.
- What evidence would resolve it: Comparative studies evaluating LLM performance on diverse real-world construction safety tasks and scenarios, alongside standardized exam performance.

### Open Question 2
- Question: What are the most effective strategies for integrating LLMs with external knowledge sources, such as retrieval-augmented generation (RAG), to improve their performance in construction safety applications?
- Basis in paper: [explicit] The study mentions that RAG presents a long-term solution to eliminate knowledge gaps but also highlights its limitations, including inconsistent prioritization of retrieved information and the "lost-in-the-middle" problem.
- Why unresolved: While RAG is identified as a potential solution, the study notes that its application in construction safety contexts requires further investigation to address these limitations.
- What evidence would resolve it: Research demonstrating the effectiveness of various RAG strategies and their impact on LLM performance in construction safety tasks.

### Open Question 3
- Question: How can the interpretability and explainability of LLM outputs be improved to increase trust and adoption in construction safety applications?
- Basis in paper: [explicit] The study highlights the challenge of interpretability in LLMs, noting the variability in prompting techniques and output requirements across different models and knowledge areas, as well as the inherent probabilistic nature of LLMs.
- Why unresolved: The study acknowledges that many LLM behaviors remain unknown and that efforts to trace the reasons behind specific outputs are hindered by the models' variability.
- What evidence would resolve it: Development and validation of methods to enhance the interpretability and explainability of LLM outputs, such as attention visualization techniques or post-hoc explanation methods.

## Limitations

- Access to proprietary BCSP exam questions prevents exact reproduction of evaluation conditions
- Error classification framework lacks independent validation against human expert annotations
- Corpus evidence supporting foundational mechanisms is notably weak, with no direct citations linking pretraining data to safety domain coverage

## Confidence

**High Confidence**: The overall accuracy results for both GPT-3.5 (73.8%) and GPT-4o (84.6%) exceed the 60% passing threshold consistently across multiple knowledge areas. The statistical methodology using repeated-measures ANOVA is appropriate for the experimental design, and the reliability findings (no significant differences across repeated trials) are well-supported by the controlled temperature and seed settings.

**Medium Confidence**: The prompt engineering effectiveness conclusions show variable accuracy differences up to 13.5% for GPT-3.5, but the optimal prompting strategy varies significantly by knowledge area and model. This suggests the findings are context-dependent rather than universally applicable. The error type analysis provides useful insights but lacks independent verification of the classification scheme.

**Low Confidence**: The corpus evidence supporting foundational mechanisms is notably weak. No direct citations link pretraining data to safety domain coverage, and prompt engineering literature specific to safety applications is absent from the corpus. This limits confidence in the proposed mechanisms explaining why certain prompting strategies work better for specific knowledge areas.

## Next Checks

1. Cross-validation with public safety question sets: Test the same LLM configurations on publicly available construction safety questions (e.g., OSHA materials, NCCER exams) to verify whether accuracy patterns hold across different question sources and difficulty levels.

2. Expert error classification audit: Have construction safety professionals independently classify a random sample of model errors to validate the knowledge gaps, reasoning flaws, memory issues, and calculation errors framework.

3. Alternative consistency metrics: Calculate Cohen's kappa and test-retest correlation coefficients alongside entropy to provide multiple perspectives on LLM response stability and compare their diagnostic power for identifying problematic knowledge areas.