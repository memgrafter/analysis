---
ver: rpa2
title: Probing the limitations of multimodal language models for chemistry and materials
  research
arxiv_id: '2411.16955'
source_url: https://arxiv.org/abs/2411.16955
tags:
- arxiv
- performance
- tasks
- plots
- adsorption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces MaCBench, a benchmark for evaluating vision-language
  models in chemistry and materials science across three core scientific workflows:
  data extraction, experiment execution, and results interpretation. While models
  perform well on basic perception tasks like equipment identification, they struggle
  with spatial reasoning, cross-modal synthesis, and multi-step inference.'
---

# Probing the limitations of multimodal language models for chemistry and materials research

## Quick Facts
- arXiv ID: 2411.16955
- Source URL: https://arxiv.org/abs/2411.16955
- Reference count: 40
- Primary result: MaCBench benchmark reveals VLLMs struggle with spatial reasoning, cross-modal synthesis, and multi-step inference despite strong performance on basic perception tasks

## Executive Summary
This paper introduces MaCBench, a comprehensive benchmark for evaluating vision-language models in chemistry and materials science across three core scientific workflows: data extraction, experiment execution, and results interpretation. While models perform well on basic perception tasks like equipment identification, they exhibit fundamental limitations in spatial reasoning, cross-modal information synthesis, and multi-step logical inference. Performance correlates strongly with the online prevalence of training examples, suggesting reliance on pattern matching rather than genuine scientific reasoning. These findings highlight the need for advances in training data curation and model architectures to develop reliable multimodal scientific assistants.

## Method Summary
The study systematically evaluates leading VLLMs (Claude 3.5 Sonnet, GPT-4o, Gemini Pro, Llama 3.2 90B Vision) using the ChemBench framework with MaCBench, a benchmark containing 1,155 questions across nine task categories. The benchmark includes both mined patent images and generated images to ensure comprehensive coverage. Model performance is measured as the fraction of correctly answered questions with tolerance-based scoring (1% or 5% thresholds for numeric answers). Five-run averaging with standard deviation provides error bars, and performance is compared against random baselines. The evaluation pipeline uses prompt templates and regex-based answer parsing with LLM extractor fallback.

## Key Results
- VLLMs achieve strong performance on basic perception tasks (equipment identification, peak recognition) but struggle with spatial reasoning tasks (stereochemistry, crystal system assignment)
- Performance correlates strongly with online prevalence of training examples, suggesting pattern matching rather than scientific reasoning
- Models perform better with text representations than visual representations of identical information
- Performance degrades significantly as tasks require more reasoning steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLLMs struggle with spatial reasoning tasks despite good performance on perception tasks.
- Mechanism: Models fail to integrate visual and conceptual understanding required for spatial tasks like stereochemistry assignment and crystal structure interpretation, relying instead on pattern matching from training data.
- Core assumption: The architecture and training data do not adequately capture 3D spatial relationships and transformations needed for scientific reasoning.
- Evidence anchors:
  - [abstract] "they exhibit fundamental limitations in spatial reasoning, cross-modal information synthesis, and multi-step logical inference"
  - [section] "Our analysis reveals consistent degradation in performance as tasks require more reasoning steps" and "Models shows nearly a 35% increase in the performance when presented with the same peak positions in text against visually showing the peaks"
  - [corpus] Weak evidence - corpus doesn't directly address spatial reasoning mechanisms
- Break condition: If models are augmented with explicit 3D spatial reasoning modules or trained with synthetic spatial data that emphasizes 3D relationships.

### Mechanism 2
- Claim: Performance correlates with online prevalence of training examples, suggesting pattern matching rather than scientific reasoning.
- Mechanism: Models rely on memorization of common patterns from internet data rather than developing genuine understanding of underlying scientific principles, leading to poor generalization on less common structures.
- Core assumption: The training corpus contains biased representation of scientific structures, with common structures overrepresented.
- Evidence anchors:
  - [abstract] "Performance correlates strongly with the online prevalence of training examples, suggesting reliance on pattern matching rather than scientific reasoning"
  - [section] "Our analysis reveals a striking correlation between the prominence of crystal structures on the Internet and task performance"
  - [corpus] Weak evidence - corpus neighbors don't directly address correlation between online prevalence and performance
- Break condition: If models are trained on curated datasets with balanced representation of common and rare structures, or if evaluation includes generalization tests on structures outside training distribution.

### Mechanism 3
- Claim: Models perform better with text representations than visual representations of the same information.
- Mechanism: Current VLLMs have not developed robust strategies for cross-modal information synthesis, leading to superior performance when information is presented in text form.
- Core assumption: The model architecture processes text and images through separate pathways that are not effectively integrated during reasoning.
- Evidence anchors:
  - [abstract] "they exhibit fundamental limitations in spatial reasoning, cross-modal information synthesis, and multi-step logical inference"
  - [section] "we find that for all tasks where we show the same information as images and text, the performance in the text modality is better than when the information is provided as an image"
  - [corpus] Weak evidence - corpus doesn't directly address modality performance differences
- Break condition: If cross-modal training objectives are implemented that explicitly require integration of text and image information during reasoning.

## Foundational Learning

- Concept: Cross-modal information synthesis
  - Why needed here: The paper shows that VLLMs perform significantly better with text than with images for the same information, indicating a fundamental limitation in integrating different modalities.
  - Quick check question: Why do models perform better when the same information is presented as text rather than as images?

- Concept: Multi-step logical inference
  - Why needed here: The paper demonstrates that model performance degrades as tasks require more reasoning steps, suggesting limitations in chaining logical steps.
  - Quick check question: What happens to model performance when tasks require multiple reasoning steps versus single-step tasks?

- Concept: Pattern matching vs. scientific reasoning
  - Why needed here: The correlation between performance and online prevalence suggests models may be relying on memorization rather than genuine understanding of scientific principles.
  - Quick check question: How does the performance on crystal structure tasks correlate with the online presence of those structures?

## Architecture Onboarding

- Component map: MaCBench benchmark -> ChemBench framework -> VLLMs (Claude 3.5 Sonnet, GPT-4o, Gemini Pro, Llama 3.2 90B Vision) -> Prompt templates -> Answer parsing -> Performance evaluation
- Critical path: Data extraction (crystal structures, spectra) -> Experiment execution (XRD patterns, AFM images) -> Results interpretation (relative intensity ordering, stereochemistry)
- Design tradeoffs: Benchmark uses both mined patent images and generated images for comprehensive coverage, but this introduces potential distribution shifts; tolerance-based scoring accounts for natural uncertainties but may mask true limitations
- Failure signatures: Performance drops on spatial reasoning tasks, multi-step reasoning tasks, and cross-modal synthesis; sensitivity to minor prompt variations causing significant performance differences
- First 3 experiments:
  1. Test the same task with identical information presented as text versus image to quantify cross-modal performance differences
  2. Compare performance on single-step versus multi-step versions of the same task to isolate reasoning complexity effects
  3. Evaluate models on crystal structures with varying online prevalence to test correlation between internet presence and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications could enable VLLMs to better handle spatial reasoning tasks in chemistry and materials science?
- Basis in paper: [explicit] The paper identifies spatial reasoning as a fundamental limitation of current VLLMs, noting their struggles with tasks like stereochemistry assignment and crystal structure interpretation.
- Why unresolved: The paper identifies this as a core limitation but doesn't propose specific architectural solutions beyond general suggestions about synthetic training data.
- What evidence would resolve it: Development and evaluation of VLLMs with specialized spatial reasoning modules, tested against MaCBench tasks, demonstrating improved performance on spatial tasks.

### Open Question 2
- Question: How does the performance of VLLMs correlate with the specificity of scientific terminology in prompts across different domains of chemistry and materials science?
- Basis in paper: [explicit] The paper demonstrates that removing scientific terminology improves performance in some tasks and notes model sensitivity to specific technical vocabularies.
- Why unresolved: The paper only examines this in limited cases and doesn't provide a comprehensive analysis across different scientific domains or terminology types.
- What evidence would resolve it: Systematic experiments varying terminology specificity across multiple scientific domains, measuring performance changes and identifying optimal terminology levels for different task types.

### Open Question 3
- Question: What is the relationship between model performance on scientific tasks and the prevalence of similar examples in training data versus genuine scientific reasoning capabilities?
- Basis in paper: [explicit] The paper finds a strong correlation between model performance and the online prominence of crystal structures, suggesting pattern matching rather than reasoning.
- Why unresolved: The paper identifies this correlation but doesn't establish causation or explore whether this pattern holds across other scientific domains.
- What evidence would resolve it: Controlled experiments comparing performance on commonly available vs. rare scientific examples, coupled with tests designed to require reasoning beyond pattern matching, to distinguish between memorization and understanding.

## Limitations

- The correlation between performance and online prevalence suggests pattern matching but cannot definitively prove causation versus other confounding factors
- Tolerance-based scoring (1-5% thresholds) may mask true model limitations by allowing near-misses to count as correct answers
- The benchmark may not fully represent the diversity and complexity of real-world scientific workflows, limiting generalizability of findings

## Confidence

- High Confidence: Basic perception tasks (equipment identification, peak recognition) show strong model performance with consistent results across multiple models and clear task definitions
- Medium Confidence: The correlation between performance and online prevalence is well-documented, but the interpretation that this indicates pattern matching rather than scientific reasoning involves inferential leaps
- Low Confidence: The broad claim that VLLMs cannot serve as reliable scientific assistants due to fundamental architectural limitations is overstated given the evidence

## Next Checks

1. **Controlled Generalization Test**: Evaluate models on crystal structures systematically varied by online prevalence (high vs. low internet presence) while controlling for structural complexity and task difficulty to isolate the effect of training data prevalence from other factors.

2. **Architecture-Agnostic Spatial Reasoning**: Implement a controlled comparison using the same benchmark tasks with models employing different spatial reasoning approaches (e.g., explicit 3D coordinate processing vs. standard image processing) to determine whether spatial reasoning limitations are truly architectural or task-specific.

3. **Cross-Modal Training Ablation**: Train a baseline VLLM with and without explicit cross-modal training objectives that require integration of text and image information, then compare performance on the text vs. image versions of identical tasks to quantify the impact of targeted architectural modifications.