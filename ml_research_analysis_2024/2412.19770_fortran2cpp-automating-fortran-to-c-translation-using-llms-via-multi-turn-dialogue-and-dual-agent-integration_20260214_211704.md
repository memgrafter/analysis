---
ver: rpa2
title: 'Fortran2CPP: Automating Fortran-to-C++ Translation using LLMs via Multi-Turn
  Dialogue and Dual-Agent Integration'
arxiv_id: '2412.19770'
source_url: https://arxiv.org/abs/2412.19770
tags:
- code
- fortran
- test
- translation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating legacy Fortran
  code to C++ for high-performance computing applications. The authors propose Fortran2CPP,
  a novel multi-turn dialogue dataset generated using a dual-agent LLM system with
  Questioner-Solver modules that iteratively refine translations through compilation,
  execution, and testing.
---

# Fortran2CPP: Automating Fortran-to-C++ Translation using LLMs via Multi-Turn Dialogue and Dual-Agent Integration

## Quick Facts
- arXiv ID: 2412.19770
- Source URL: https://arxiv.org/abs/2412.19770
- Reference count: 21
- Automates Fortran-to-C++ translation using dual-agent LLM system with Questioner-Solver modules, achieving up to 3.31x CodeBLEU improvement and 92% increase in compilation success rates

## Executive Summary
This paper addresses the challenge of translating legacy Fortran code to C++ for high-performance computing applications. The authors propose Fortran2CPP, a novel multi-turn dialogue dataset generated using a dual-agent LLM system with Questioner-Solver modules that iteratively refine translations through compilation, execution, and testing. Fine-tuning on their 11.7k dialogue dataset yielded up to 3.31x improvement in CodeBLEU scores and 92% increase in compilation success rates compared to baseline models, demonstrating enhanced syntactic accuracy and functional reliability in Fortran-to-C++ translation. The dataset and models are open-sourced on GitHub.

## Method Summary
The Fortran2CPP approach uses a dual-agent Questioner-Solver LLM system to generate a multi-turn dialogue dataset through iterative refinement. The Questioner LLM generates domain-specific questions based on current memory state and environmental feedback, while the Solver LLM focuses on planning and execution tasks. The pipeline includes initial translation, unit test generation, compilation fixing, execution fixing, and consistency improvement phases. The resulting 11.7k dialogue dataset captures nuanced translation processes with compilation and runtime feedback, particularly valuable for low-resource languages like Fortran. Selected open-weight LLMs (DeepSeek-Coder 6.7B, CodeLlama 13B, StarCoder 15.5B) are fine-tuned on this dataset using specified hyperparameters.

## Key Results
- Up to 3.31x improvement in CodeBLEU scores compared to baseline models
- 92% increase in compilation success rates
- 11.7k multi-turn dialogue dataset capturing iterative Fortran-to-C++ translation workflows

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Questioner-Solver module design enhances translation accuracy by separating environment assessment from decision-making, enabling more nuanced problem-solving.
- Mechanism: By dividing the agent core into two specialized LLMs, the Questioner dynamically generates domain-specific inquiries based on current memory state and environmental feedback, while the Solver focuses on planning and execution tasks.
- Core assumption: Complex translation tasks require specialized roles rather than a single general-purpose LLM.

### Mechanism 2
- Claim: Iterative refinement through compilation, execution, and testing loops ensures high-quality translations.
- Mechanism: The multi-turn dialogue dataset captures iterative feedback-decision workflows including code translation, compilation, execution, unit testing, and error-fixing, allowing models to learn from previous mistakes.
- Core assumption: Compilation and execution feedback provides rich information for improving translations.

### Mechanism 3
- Claim: Dialogue-based training extends LLMs' knowledge in low-resource languages like Fortran.
- Mechanism: The multi-turn dialogue dataset captures the nuanced translation process with compilation and runtime feedback, creating a knowledge-rich resource that supplements limited parallel corpora.
- Core assumption: Low-resource languages benefit from process-oriented data rather than just code pairs.

## Foundational Learning

- Concept: Compiler feedback analysis
  - Why needed here: Compilation errors provide detailed information about syntax and semantic issues that simple execution feedback cannot capture.
  - Quick check question: What type of information do compilers provide that execution testing alone cannot?

- Concept: Multi-agent coordination
  - Why needed here: Separating question generation from solution execution allows for more focused and effective problem-solving in complex translation tasks.
  - Quick check question: How does the division of labor between Questioner and Solver improve translation accuracy?

- Concept: Iterative refinement loops
  - Why needed here: Multiple passes through translation, testing, and fixing ensure that errors are systematically addressed rather than superficially masked.
  - Quick check question: Why is an iterative approach more effective than a single-pass translation method?

## Architecture Onboarding

- Component map: Questioner LLM -> Solver LLM -> Memory -> Tools (compilers, execution runners) -> Environment
- Critical path: Initial translation → Unit test generation → Compilation fixing → Execution fixing → Consistency verification
- Design tradeoffs:
  - Dual-agent vs single-agent: Better specialization vs increased complexity
  - Compilation-first vs execution-first: More detailed error info vs end-to-end validation
  - Large dataset vs quality: More training data vs potential noise
- Failure signatures:
  - Stuck in infinite loops: Questions become repetitive, no progress in fixing errors
  - Compilation success but execution failure: Syntax correct but logic flawed
  - Inconsistent naming: Translation introduces naming conflicts across code pairs
- First 3 experiments:
  1. Translate a simple Fortran function (e.g., array manipulation) and verify compilation and execution success
  2. Test a Fortran code with external dependencies to validate the filtering criteria
  3. Run a complex Fortran program through the full pipeline to assess scalability and identify bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the Questioner-Solver module's performance change if more than two LLMs were used in the core architecture?
- Basis in paper: [explicit] The paper discusses the Questioner-Solver module design using two LLMs as the core component, suggesting it advances beyond single LLM agents.
- Why unresolved: The paper does not explore architectures with more than two LLMs in the core, leaving the impact of additional LLMs on performance unknown.
- What evidence would resolve it: Comparative experiments testing the Questioner-Solver module with 2, 3, and 4 LLM configurations on the same Fortran-to-C++ translation tasks.

### Open Question 2
- Question: Would the dataset generation pipeline achieve similar success rates with other low-resource programming languages beyond Fortran?
- Basis in paper: [explicit] The paper highlights the approach's value for low-resource languages like Fortran, achieving a 29.6% successful data conversation rate.
- Why unresolved: The experiments were conducted only with Fortran code, so generalizability to other low-resource languages remains untested.
- What evidence would resolve it: Application of the same pipeline to generate translation datasets for other low-resource languages (e.g., COBOL, Lisp) with comparative success rate metrics.

### Open Question 3
- Question: How does the quality of unit tests generated by the LLM-based approach compare to those written by human experts for the same code?
- Basis in paper: [inferred] The paper relies heavily on LLM-generated unit tests for verification but acknowledges the need for stricter validation processes.
- Why unresolved: The paper does not include a direct comparison between LLM-generated unit tests and human-written equivalents for the same Fortran-to-C++ translations.
- What evidence would resolve it: Side-by-side comparison of LLM-generated unit tests versus expert-written unit tests for identical code samples, evaluated on coverage, accuracy, and edge case detection.

## Limitations
- The dual-agent design may not transfer effectively to other programming language pairs with different syntactic and semantic characteristics
- CodeBLEU metric may not fully capture functional equivalence for complex numerical algorithms in HPC applications
- Reliance on compilation feedback may limit effectiveness for languages with less strict compile-time checking

## Confidence

**High Confidence** (Mechanistic plausibility supported by evidence):
- Iterative refinement approach effectively improves translation quality
- Dual-agent Questioner-Solver design provides meaningful specialization
- Multi-turn dialogue dataset successfully extends LLM knowledge in low-resource languages

**Medium Confidence** (Some evidence but with limitations):
- 3.31x CodeBLEU improvement translates to meaningful real-world performance gains
- 92% increase in compilation success rates indicates robust syntactic translation capabilities
- Approach scales effectively to complex Fortran programs

**Low Confidence** (Limited evidence or high uncertainty):
- Long-term maintenance of translated code matches original Fortran quality
- Method performs equally well on Fortran dialects not represented in training corpus
- Dual-agent approach outperforms single-agent alternatives in all scenarios

## Next Checks

1. **Cross-language generalization test**: Apply the Questioner-Solver framework to translate between other low-resource language pairs (e.g., COBOL to Java, or Ada to Rust) to validate the approach's generalizability beyond Fortran-C++.

2. **Functional equivalence benchmark**: Develop a comprehensive test suite that goes beyond compilation success to measure actual functional equivalence between Fortran and C++ implementations, particularly for numerical accuracy and performance characteristics critical in HPC applications.

3. **Ablation study on agent design**: Conduct systematic experiments comparing the dual-agent Questioner-Solver approach against single-agent models with varying prompt engineering strategies to quantify the specific contribution of the specialized agent roles to overall performance improvements.