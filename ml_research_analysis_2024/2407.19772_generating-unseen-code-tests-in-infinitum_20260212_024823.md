---
ver: rpa2
title: Generating Unseen Code Tests In Infinitum
arxiv_id: '2407.19772'
source_url: https://arxiv.org/abs/2407.19772
tags:
- code
- instructions
- assign
- benchmark
- tests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses benchmark leakage in LLM evaluation by proposing
  an AST-based method to generate unseen test data for code-related tasks. The core
  idea is to automatically translate ASTs from source code into low-level English
  instructions, which are then used to prompt models to generate code, enabling regression
  testing and debugging.
---

# Generating Unseen Code Tests In Infinitum

## Quick Facts
- arXiv ID: 2407.19772
- Source URL: https://arxiv.org/abs/2407.19772
- Reference count: 26
- One-line primary result: AST-based method generates unseen test data for LLM code evaluation, revealing significant performance gaps across models.

## Executive Summary
This paper addresses benchmark leakage in LLM evaluation by proposing an AST-based method to generate unseen test data for code-related tasks. The core idea is to automatically translate ASTs from source code into low-level English instructions, which are then used to prompt models to generate code, enabling regression testing and debugging. A debugging dictionary tracks common model errors across programming constructs. The approach was implemented for Python text-to-code generation using the NAPS dataset, producing the "auto-regression" benchmark.

## Method Summary
The method involves parsing source code into ASTs, translating these into English instructions, and using them to prompt LLMs to generate code. This creates fresh benchmarks that mitigate leakage and enable debugging by identifying specific coding construct failures. The approach uses the NAPS dataset and focuses on Python text-to-code generation, with a debugging dictionary to track model errors.

## Key Results
- GPT-4o achieved a 93% whole-test pass rate on 135 problems
- Llama-3-8b reached only 41% pass rate on the same benchmark
- Significant performance differences across models highlight the method's ability to differentiate model capabilities

## Why This Works (Mechanism)
The method works by translating ASTs into low-level English instructions, which are then used to prompt models to generate code. This approach ensures that the generated test cases are unseen and mitigates benchmark leakage. The debugging dictionary tracks common model errors, enabling targeted debugging and regression testing.

## Foundational Learning
- **AST (Abstract Syntax Tree)**: A tree representation of code structure; needed to capture code semantics for translation.
  - Quick check: Can the AST accurately represent the intended code logic?
- **Benchmark Leakage**: When test data is inadvertently included in training data; needed to ensure fair evaluation.
  - Quick check: Are the generated test cases truly unseen by the model?
- **Debugging Dictionary**: A tool to track common model errors; needed to identify and address model weaknesses.
  - Quick check: Does the dictionary cover a comprehensive range of coding constructs?

## Architecture Onboarding

**Component Map**
- Source Code -> AST Parser -> AST-to-English Translator -> English Instructions -> LLM Prompt -> Generated Code -> Test Execution -> Performance Evaluation

**Critical Path**
The critical path is Source Code -> AST Parser -> AST-to-English Translator -> LLM Prompt -> Generated Code -> Test Execution, as errors in any step can compromise the entire pipeline.

**Design Tradeoffs**
- AST complexity vs. translation accuracy: More complex ASTs may improve translation fidelity but increase computational overhead.
- Dataset specificity vs. generalizability: Using NAPS limits generalizability to other languages or tasks.

**Failure Signatures**
- Poor AST parsing leading to incorrect translations.
- Ambiguity in English instructions causing inconsistent code generation.
- Debugging dictionary missing key coding constructs, leading to undetected errors.

**First 3 Experiments**
1. Test AST-to-English translation accuracy on a small, manually verified code sample.
2. Validate the debugging dictionary by running it on a known buggy codebase.
3. Compare model performance on auto-regression vs. traditional benchmarks to quantify leakage mitigation.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on the NAPS dataset limits generalizability to other programming languages or code generation tasks.
- AST-to-English translation may introduce semantic gaps or ambiguity, affecting test case fidelity.
- Scalability to complex languages or real-world codebases remains untested.

## Confidence
- **High Confidence**: The method's core approach (AST-based test generation) and its application to Python text-to-code generation are well-documented and reproducible.
- **Medium Confidence**: The performance differences between models (e.g., GPT-4o vs. Llama-3-8b) are statistically significant but may not generalize to other datasets or tasks.
- **Low Confidence**: The scalability of the method to other programming languages and real-world codebases remains untested and speculative.

## Next Checks
1. Test the AST-to-English translation approach on a broader range of programming languages (e.g., Java, C++) to assess generalizability.
2. Validate the debugging dictionary by applying it to a different code generation task (e.g., function completion or bug fixing) to ensure robustness.
3. Conduct a human evaluation of the generated test cases to confirm their semantic accuracy and relevance to the intended code generation tasks.