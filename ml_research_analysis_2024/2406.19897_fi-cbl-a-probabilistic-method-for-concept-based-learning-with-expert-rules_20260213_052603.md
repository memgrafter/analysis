---
ver: rpa2
title: 'FI-CBL: A Probabilistic Method for Concept-Based Learning with Expert Rules'
arxiv_id: '2406.19897'
source_url: https://arxiv.org/abs/2406.19897
tags:
- concepts
- concept
- probabilities
- learning
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FI-CBL, a probabilistic method for concept-based
  learning with expert rules that divides images into patches, transforms them into
  embeddings using an autoencoder, and clusters the embeddings to learn concept values.
  The method implements frequentist inference by computing prior and posterior probabilities
  of concepts based on patch rates from images with certain concept values.
---

# FI-CBL: A Probabilistic Method for Concept-Based Learning with Expert Rules

## Quick Facts
- arXiv ID: 2406.19897
- Source URL: https://arxiv.org/abs/2406.19897
- Reference count: 40
- Primary result: FI-CBL outperforms concept bottleneck models when training data is limited, achieving F1 measures up to 0.968 on concept classification tasks

## Executive Summary
This paper introduces FI-CBL, a probabilistic method for concept-based learning that incorporates expert rules. The approach divides images into patches, transforms them into embeddings using an autoencoder, and clusters these embeddings to learn concept values. By computing prior and posterior probabilities based on patch rates from concept-annotated images, FI-CBL enables concept inference for new images. The method stands out for its transparency through explicit probabilistic calculations and its ability to integrate expert rules as logical functions that update concept probabilities.

## Method Summary
FI-CBL processes concept-annotated images by dividing them into patches and transforming these patches into embeddings via an autoencoder. The embeddings are clustered, and probabilities that patches in each cluster belong to images with specific concept values are computed from image-level concept labels. This frequency-based approach estimates concept probabilities without requiring patch-level annotations. Expert rules, expressed as logical functions over concept values, are incorporated through Bayesian updating formulas that modify prior and conditional probabilities. The method is particularly effective when training data is limited, as it relies on frequency estimation rather than learning complex representations.

## Key Results
- FI-CBL achieves F1 measures up to 0.968 for concept classification on modified MNIST and CelebA datasets
- The method outperforms concept bottleneck models when training data is small (fewer than 5000 instances for MNIST, fewer than 4000 for CelebA)
- FI-CBL provides explicit probabilistic calculations and transparent inference, unlike black-box neural network approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FI-CBL leverages weak supervision by learning concept probabilities from image-level labels without patch-level annotations.
- Mechanism: Images are divided into patches, embeddings are extracted via autoencoder, and patches are clustered. Probabilities that patches in each cluster belong to images with specific concept values are computed from image-level concept labels, enabling concept inference for new images.
- Core assumption: Patches from the same image share consistent concept values, and clustering groups patches with similar concepts together.
- Evidence anchors: [abstract] states "divide each concept-annotated image into patches... and to cluster the embeddings assuming that each cluster will mainly contain embeddings of patches with certain concepts." [section 4] describes computing "probabilities of labels for patches by separating embeddings... into groups (clusters) with different contents and by counting up how many whole images having a certain concept value contain patches from each group (cluster)."

### Mechanism 2
- Claim: FI-CBL updates prior and conditional probabilities using expert rules to improve concept inference.
- Mechanism: Expert rules in the form of logical functions modify prior probabilities of concepts and conditional probabilities that embeddings belong to certain clusters given concept values, using Bayesian updating formulas.
- Core assumption: Expert rules accurately capture domain knowledge that can be expressed as logical functions over concept values, and these rules improve concept probability estimates.
- Evidence anchors: [abstract] states "FI-CBL allows us to incorporate the expert rules in the form of logic functions into the inference procedure... to update prior and conditional probabilities of concepts to satisfy the rules." [section 6] provides mathematical derivations showing how prior probabilities are updated using Bayes' rule with expert rules, and how conditional probabilities are similarly updated.

### Mechanism 3
- Claim: FI-CBL achieves better performance than concept bottleneck models when training data is limited.
- Mechanism: By using patch clustering and frequency-based probability estimation, FI-CBL requires less training data than neural network-based CBM, which need large datasets to learn concept representations.
- Core assumption: The clustering-based approach captures sufficient concept information with fewer training samples than end-to-end neural networks.
- Evidence anchors: [abstract] states "Numerical experiments show that FI-CBL outperforms the concept bottleneck model in cases when the number of training data is small." [section 8] shows F1 measures comparing FI-CBL and CBM across varying training set sizes, demonstrating FI-CBL's superiority for small datasets.

## Foundational Learning

- Concept: Bayes' Rule and Bayesian Inference
  - Why needed here: FI-CBL uses Bayes' rule to compute posterior probabilities of concepts given patch embeddings and to update probabilities using expert rules.
  - Quick check question: Can you write Bayes' rule for computing P(Concept | Evidence) from P(Evidence | Concept) and P(Concept)?

- Concept: Frequentist Probability Estimation
  - Why needed here: FI-CBL estimates concept probabilities by computing relative frequencies of patches from images with specific concept values in each cluster.
  - Quick check question: How would you estimate P(Cluster | Concept) if you know which clusters contain patches from images with that concept value?

- Concept: Clustering and Unsupervised Learning
  - Why needed here: FI-CBL clusters patch embeddings to group patches with similar content, enabling probability estimation based on cluster membership.
  - Quick check question: What clustering algorithm would you choose for high-dimensional patch embeddings, and why?

## Architecture Onboarding

- Component map: Image preprocessing -> Patch extraction -> Autoencoder embedding -> Clustering -> Probability computation -> Expert rule integration -> Inference engine

- Critical path:
  1. Patch extraction from training images
  2. Autoencoder training for patch embeddings
  3. Clustering of all patch embeddings
  4. Probability computation from training data
  5. Expert rule integration (if applicable)
  6. Inference for new images using multinomial distribution

- Design tradeoffs:
  - Number of patches per image vs. computational cost and concept granularity
  - Number of clusters vs. concept discrimination and probability estimation accuracy
  - Embedding size vs. representation quality and memory requirements
  - Threshold values for concept assignment vs. precision and recall

- Failure signatures:
  - Poor clustering results in mixed concept patches within clusters
  - Autoencoder fails to capture discriminative features in patch embeddings
  - Expert rules conflict with data patterns, degrading performance
  - Insufficient training data leads to unreliable probability estimates

- First 3 experiments:
  1. Verify clustering quality by visualizing cluster centers and checking if they correspond to distinct concepts
  2. Test probability computation accuracy by comparing estimated concept probabilities with ground truth on a validation set
  3. Evaluate expert rule integration by measuring performance changes when rules are added/removed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the number of clusters impact the performance and interpretability of the FI-CBL model?
- Basis in paper: [explicit] The authors mention that a large difference in the size of concepts in an image can lead to significant deterioration of the model and suggest using sliding windows for producing patches, but also note that this requires additional studies.
- Why unresolved: The paper does not provide a detailed analysis of how different numbers of clusters affect the model's performance or how to determine the optimal number of clusters.
- What evidence would resolve it: Empirical studies comparing the performance of FI-CBL with different numbers of clusters on various datasets, along with guidelines for selecting the optimal number of clusters.

### Open Question 2
- Question: How can the FI-CBL model be extended to handle continuous concept values instead of binary or categorical values?
- Basis in paper: [inferred] The paper focuses on binary and categorical concept values, but does not discuss how to handle continuous values.
- Why unresolved: The current formulation of FI-CBL relies on discrete probability distributions, which may not be suitable for continuous values.
- What evidence would resolve it: Development of a probabilistic framework for FI-CBL that can handle continuous concept values, along with experimental validation on datasets with continuous concepts.

### Open Question 3
- Question: How does the FI-CBL model compare to other concept-based learning models in terms of interpretability and performance when dealing with complex, high-dimensional data?
- Basis in paper: [explicit] The authors compare FI-CBL to the concept bottleneck model (CBM) and show that FI-CBL outperforms CBM when the number of training instances is small, but note that CBM can provide better results when the number of training data is large.
- Why unresolved: The comparison is limited to the CBM, and there may be other concept-based learning models that perform better or worse than FI-CBL in different scenarios.
- What evidence would resolve it: Comprehensive benchmarking of FI-CBL against various concept-based learning models on diverse datasets, evaluating both interpretability and performance metrics.

## Limitations
- Performance heavily depends on clustering quality and autoencoder's ability to generate meaningful patch embeddings
- Expert rules must be carefully formulated to avoid conflicts with data patterns
- The approach may struggle with concepts that are distributed across multiple patches or require complex spatial relationships

## Confidence
- Mechanism 1 (weak supervision via patch clustering): High
- Mechanism 2 (expert rule integration): Medium
- Mechanism 3 (data efficiency advantage): Medium

## Next Checks
1. Test FI-CBL's robustness to noisy or incorrect expert rules by systematically varying rule quality
2. Evaluate the method's performance on datasets with more complex concept distributions and spatial relationships
3. Compare FI-CBL's clustering quality and probability estimation accuracy against alternative approaches using the same datasets