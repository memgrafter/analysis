---
ver: rpa2
title: Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation
arxiv_id: '2403.18804'
source_url: https://arxiv.org/abs/2403.18804
tags:
- teacher
- student
- language
- modules
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether parameter-efficient fine-tuning
  (PEFT) modules can be transferred between different pre-trained language models
  (PLMs), a property termed "transferable modularity." The authors propose using knowledge
  distillation to initialize PEFT modules of a smaller "student" model with those
  from a larger "teacher" model, followed by fine-tuning. For incompatible models
  with mismatched internal dimensionality, they introduce a pruning and alignment
  method based on correlation and linear sum assignment.
---

# Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation

## Quick Facts
- arXiv ID: 2403.18804
- Source URL: https://arxiv.org/abs/2403.18804
- Authors: Mateusz Klimaszewski; Piotr Andruszkiewicz; Alexandra Birch
- Reference count: 0
- Primary result: Transferred PEFT modules improve student model performance in matching setups, with less consistent gains for incompatible models

## Executive Summary
This paper investigates whether parameter-efficient fine-tuning (PEFT) modules can be transferred between different pre-trained language models (PLMs), a property termed "transferable modularity." The authors propose using knowledge distillation to initialize PEFT modules of a smaller "student" model with those from a larger "teacher" model, followed by fine-tuning. For incompatible models with mismatched internal dimensionality, they introduce a pruning and alignment method based on correlation and linear sum assignment. Experiments on NER, PI, and NLI tasks across 20+ languages show that transferred modules significantly improve student performance in matching model setups, though improvements are less consistent for incompatible models.

## Method Summary
The method involves fine-tuning a teacher PLM with PEFT modules on a target task, then transferring these modules to initialize a student PLM's PEFT modules before its own fine-tuning. For compatible models (same family), direct module copying suffices. For incompatible models with different latent dimensions, a correlation-based alignment method samples embeddings from both models, computes Pearson correlation matrices, solves a linear sum assignment problem to find optimal dimension mappings, then prunes and reorders teacher module weights accordingly. The student model is then fine-tuned with these initialized modules and evaluated against baselines and the teacher.

## Key Results
- Matching PLMs (e.g., mBERT → DistilBERT) show consistent performance gains from transferred modules across most tasks and languages (88.7% positive cases)
- Incompatible PLM transfers yield weaker, more inconsistent improvements due to alignment challenges
- The proposed correlation-based pruning and alignment method enables module transfer between models with different latent space dimensions without increasing inference complexity
- Transferred modules provide significant F1 score improvements in NER tasks across 20+ languages when models are compatible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained task-specific PEFT modules contain knowledge that can bootstrap student fine-tuning when transferred from teacher models.
- Mechanism: Teacher model is fine-tuned with PEFT on a task, then the trained modules are copied to initialize student's PEFT modules before its own fine-tuning. This provides a non-random starting point, reducing training steps needed to reach similar performance.
- Core assumption: The student and teacher PLMs share compatible representations so that teacher's module weights are meaningful in student's context.
- Evidence anchors:
  - [abstract] "propose using knowledge distillation to initialize PEFT modules of a smaller 'student' model with those from a larger 'teacher' model"
  - [section 2] "we aim to initialise the research objective of transferable modularity... extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs"
  - [corpus] Weak; neighboring papers focus on transferability of PEFT modules generally, but not on KD-based initialization specifically.
- Break condition: If teacher and student use incompatible architectures or if task-specific knowledge is too divergent, initialization may not help or could hurt.

### Mechanism 2
- Claim: For incompatible PLMs, a correlation-based pruning and alignment method can map teacher module dimensions to student module dimensions without changing inference complexity.
- Mechanism: Sample embeddings from both models, compute Pearson correlation matrix across dimensions, solve a linear sum assignment to find optimal one-to-one dimension mapping, then prune and reorder teacher module weights accordingly.
- Core assumption: Correlated dimensions between teacher and student embeddings correspond to the same or similar features despite different latent space indexing.
- Evidence anchors:
  - [section 2.1] "establish a correlation matrix between latent spaces... find the best possible alignment... using the calculated assignment indices, we remove not-mapped weights"
  - [corpus] Weak; no corpus evidence of this exact pruning/alignment approach, only general KD or transfer learning methods.
- Break condition: If embeddings are not well-aligned or correlation is low due to fundamentally different feature representations, mapping will be noisy and performance gain minimal.

### Mechanism 3
- Claim: Matching PLMs (e.g., mBERT → DistilBERT) yield consistent performance gains from transferred modules; incompatible PLMs yield weaker, inconsistent gains.
- Mechanism: Matching PLMs share architecture and dimensionality, so direct weight transfer works well. Incompatible PLMs require alignment, which introduces noise, leading to less reliable gains.
- Core assumption: Task-agnostic distillation preserves alignment between teacher and student latent spaces.
- Evidence anchors:
  - [section 4.1] "SKIP method presents consistency; the difference compared to the baseline was positive across most tasks and languages (88.7% cases)"
  - [section 4.2] "The disparity between matching and incompatible experiments can be attributed to alignment challenges... correlation-based method encounters difficulties when dealing with models of greater depth"
  - [corpus] Weak; no corpus evidence comparing these two specific setups.
- Break condition: If distillation or alignment process fails to preserve useful feature correspondence, transferred modules provide little or no benefit.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: The entire study hinges on transferring small, task-specific modules rather than full model weights to enable efficient adaptation.
  - Quick check question: What is the key difference between full fine-tuning and PEFT in terms of parameter count and update strategy?

- Concept: Knowledge distillation (KD)
  - Why needed here: The approach repurposes KD methodology not to compress the model but to transfer fine-tuned modules between models.
  - Quick check question: In standard KD, what role does the teacher's output distribution play, and how does that differ from transferring PEFT modules?

- Concept: Linear sum assignment (LSA) problem
  - Why needed here: LSA is used to optimally align teacher and student dimensions when their latent spaces are mismatched.
  - Quick check question: What optimization objective does LSA solve when given a cost matrix, and why is negative correlation used as the cost here?

## Architecture Onboarding

- Component map: Teacher PLM → Task-specific PEFT training → Saved PEFT modules → (Optional) Alignment/Pruning step → Module initialization → Student PLM → Student PEFT training → Evaluation
- Critical path:
  1. Train teacher with PEFT on target task
  2. If matching models, copy modules directly
  3. If incompatible, run correlation+LSA alignment and prune
  4. Initialize student PEFT modules with transferred weights
  5. Fine-tune student on same task
  6. Evaluate performance vs baseline and teacher
- Design tradeoffs:
  - Matching vs incompatible models: simplicity and reliability vs broader applicability but alignment complexity
  - Correlation sampling granularity: more samples improve alignment accuracy but increase offline computation
  - Skipping vs averaging modules across layers: skipping preserves layer-specific specialization, averaging may blur distinct features
- Failure signatures:
  - No improvement over baseline: likely misalignment or incompatible representations
  - Worse performance than baseline: module transfer may introduce noise or conflicting knowledge
  - Inconsistent gains across languages/tasks: alignment method may not generalize well to all feature spaces
- First 3 experiments:
  1. Train teacher mBERT with Adapter on NER, copy modules to DistilBERT, fine-tune and evaluate.
  2. Repeat experiment with LoRA modules instead of Adapters.
  3. Train teacher XLM-R-Large on NLI, align and transfer modules to XLM-R-Base, fine-tune and evaluate.

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The correlation-based alignment method's effectiveness decreases with deeper models, potentially limiting scalability
- No ablation studies isolating the contribution of knowledge distillation versus direct weight transfer
- Limited exploration of alternative alignment strategies beyond Pearson correlation

## Confidence
- Matching PLMs: High confidence - consistent performance gains across most tasks and languages
- Incompatible PLMs: Medium confidence - inconsistent gains and alignment challenges noted in experiments

## Next Checks
1. **Alignment Robustness Test**: Systematically vary the number of embedding samples used for correlation calculation to quantify its impact on alignment quality and downstream performance.

2. **Layer-wise Transfer Analysis**: Measure individual adapter/LoRA module contributions by transferring only specific layers and evaluating performance degradation patterns.

3. **Cross-task Transferability**: Evaluate whether modules trained on one task (e.g., NER) can provide initialization benefits when transferred for a different task (e.g., NLI) within the same model family.