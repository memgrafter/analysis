---
ver: rpa2
title: Language-guided Skill Learning with Temporal Variational Inference
arxiv_id: '2402.16354'
source_url: https://arxiv.org/abs/2402.16354
tags:
- skill
- skills
- learning
- variational
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LAST leverages large language models (LLMs) to propose an initial
  segmentation of expert trajectories, then refines this via a hierarchical variational
  inference framework to discover reusable skills. The approach merges short subsequences
  into longer, semantically meaningful skills and introduces an auxiliary Minimum
  Description Length (MDL) objective to balance compression and reusability.
---

# Language-guided Skill Learning with Temporal Variational Inference

## Quick Facts
- arXiv ID: 2402.16354
- Source URL: https://arxiv.org/abs/2402.16354
- Reference count: 40
- Key outcome: LAST uses LLM-generated segmentation with temporal variational inference to discover reusable skills, significantly improving zero-shot transfer and online learning efficiency on BabyAI and ALFRED

## Executive Summary
LAST (Language-guided skill discovery with Auxiliary Skill segmentation and Temporal variational inference) addresses the challenge of learning reusable skills from expert demonstrations in long-horizon tasks. The method leverages large language models to propose an initial segmentation of expert trajectories, then refines this via a hierarchical variational inference framework to discover semantically meaningful skills. Experiments demonstrate that LAST significantly outperforms state-of-the-art skill learning baselines in both zero-shot transfer and online adaptation settings.

## Method Summary
LAST combines LLM-based initial segmentation with temporal variational inference to discover reusable skills from expert demonstrations. The process begins with an LLM (GPT-4) segmenting trajectories into short subsequences (1-5 actions) with language annotations. A hierarchical variational inference framework then merges these segments into longer, semantically coherent skills while incorporating an auxiliary Minimum Description Length (MDL) objective to balance compression and reusability. Finally, the learned skills are used in an online hierarchical RL setting where a high-level policy selects skills while the low-level skill policy remains frozen.

## Key Results
- Outperforms state-of-the-art skill learning baselines on BabyAI and ALFRED in zero-shot transfer scenarios
- Demonstrates significant improvement in sample efficiency during online learning of new long-horizon tasks
- Discovers interpretable skills that balance compression (fewer skills) with reconstruction accuracy

## Why This Works (Mechanism)

### Mechanism 1
Using an LLM to propose initial segmentation dramatically reduces the search space for skill discovery by constraining the downstream variational inference to only merging pre-segmented subsequences rather than exploring all possible segmentations.

### Mechanism 2
The temporal variational inference framework with language supervision enables merging short segments into longer, reusable skills while maintaining semantic coherence through multi-modal variational encoders that condition on both visual observations and LLM-generated language annotations.

### Mechanism 3
The Minimum Description Length (MDL) auxiliary objective encourages compression of skills while preserving their ability to reconstruct trajectories, leading to more reusable skills by finding the optimal balance between using fewer general skills and maintaining reconstruction accuracy.

## Foundational Learning

- Concept: Variational Inference
  - Why needed here: LAST uses variational inference to approximate the posterior distribution over skills given trajectories, which is intractable to compute directly.
  - Quick check question: What is the key difference between the approximate posterior q(β:T, k:T | ·) and the true posterior p(β:T, k:T | ·) in LAST's framework?

- Concept: Mutual Information Maximization
  - Why needed here: The paper references that many skill discovery methods maximize mutual information between skills and states to encourage diversity, though LAST takes a different approach with MDL.
  - Quick check question: How does maximizing mutual information between skills and states differ from LAST's approach of using MDL for skill discovery?

- Concept: Hierarchical Reinforcement Learning
  - Why needed here: LAST's final stage involves freezing the low-level skill policy and learning a high-level policy that selects skills, which is a classic hierarchical RL setup.
  - Quick check question: In LAST's online hierarchical RL phase, why is it beneficial to freeze the low-level skill policy rather than fine-tuning it?

## Architecture Onboarding

- Component map: Data → LLM segmentation → Variational inference (with MDL) → Skill library → Online RL fine-tuning
- Critical path: Expert demonstrations with goals → LLM-generated segments → Variational inference → Learned skill library → High-level policy for new tasks
- Design tradeoffs: Using LLM-generated segmentation trades computational cost and potential hallucination risk for drastically reduced search space and semantic guidance. The MDL objective trades some reconstruction accuracy for skill reusability.
- Failure signatures: 
  - Skills are too fine-grained (many short skills): Likely the MDL weight is too low or LLM segmentation is too detailed
  - Skills are too coarse (few long skills): Likely the MDL weight is too high or LLM segmentation is too coarse
  - Poor zero-shot transfer: Likely the variational inference is not properly merging segments or the skills are not semantically meaningful
  - Online RL fails to learn: Likely the skill library is not expressive enough or the high-level policy cannot effectively compose skills
- First 3 experiments:
  1. Verify LLM segmentation produces reasonable 1-5 action segments with meaningful language annotations on a small dataset
  2. Test the variational inference alone (without MDL) to see if it can merge segments into longer skills on a validation set
  3. Evaluate zero-shot transfer on a held-out task with the learned skills to assess reusability before online RL fine-tuning

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section implies several areas for future work including scaling to more complex environments and extending to continuous action spaces.

## Limitations
- Performance heavily depends on the quality of LLM-generated initial segmentation, with no robustness analysis for different LLM qualities
- Limited evaluation scope to relatively constrained environments (BabyAI and ALFRED) without testing on more complex, open-world scenarios
- Hyperparameter sensitivity, particularly to the MDL weight, is not thoroughly explored across different environments or task complexities

## Confidence
- High Confidence: The hierarchical structure of LAST and the mathematical framework are well-defined and rigorous
- Medium Confidence: The claim about LLM-generated segmentation reducing search space is logically sound but lacks quantitative evidence
- Low Confidence: The claim about MDL achieving optimal balance between compression and reusability is primarily theoretical with limited empirical validation

## Next Checks
1. **LLM Robustness Test**: Run LAST with different segmentation granularities (varying segment lengths from 1-3 vs 1-5 actions) and compare downstream skill quality and learning efficiency
2. **Ablation on MDL Weight**: Systematically vary the MDL weight parameter across multiple orders of magnitude and measure its impact on skill reusability, compression ratio, and transfer performance
3. **Scalability Assessment**: Test LAST on a more complex environment with longer trajectories and more diverse tasks (e.g., Meta-World or RoboTHOR) to evaluate scalability beyond BabyAI and ALFRED