---
ver: rpa2
title: Illustrating Classic Brazilian Books using a Text-To-Image Diffusion Model
arxiv_id: '2408.00544'
source_url: https://arxiv.org/abs/2408.00544
tags:
- images
- image
- diffusion
- https
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the feasibility of using text-to-image (TTI)
  diffusion models, specifically Stable Diffusion, to generate illustrations for classic
  Brazilian literature. The authors selected seven classic books and employed a two-phase
  methodology involving initial image generation followed by refinement.
---

# Illustrating Classic Brazilian Books using a Text-To-Image Diffusion Model

## Quick Facts
- arXiv ID: 2408.00544
- Source URL: https://arxiv.org/abs/2408.00544
- Reference count: 37
- One-line primary result: Text-to-image diffusion models can generate relevant illustrations for classic Brazilian literature, but exhibit racial bias and require careful prompt engineering

## Executive Summary
This study explores the feasibility of using text-to-image diffusion models, specifically Stable Diffusion, to generate illustrations for classic Brazilian literature. The authors selected seven classic books and employed a two-phase methodology involving initial image generation followed by refinement. They generated 1500 images per book and evaluated them using CLIP and Inception Scores. The results indicate that carefully crafted prompts significantly improve image quality and relevance, with high CLIP scores for some books. However, the study also highlights model biases, such as a tendency to produce predominantly white figures. The quantitative evaluation revealed variations in image quality across different books, with O Triste Fim de Policarpo Quaresma achieving the highest Inception Score. Overall, the study demonstrates the potential of using generative models for literary illustration while acknowledging the need for improved prompt engineering and addressing inherent biases.

## Method Summary
The authors selected seven classic Brazilian books and generated 1500 images per book using Stable Diffusion XL. The methodology involved two phases: initial image generation using Stable Diffusion XL Base 1.0, followed by refinement with Stable Diffusion XL Refiner 1.0. The process included denoising techniques and a large-scale generation of images, evaluated quantitatively using CLIP and Inception Scores (IS) metrics. Human-crafted descriptive prompts based on literary scenes guided the generation process, with five prompts per book producing 300 images each.

## Key Results
- CLIP scores ranged from 0.2972 to 0.5819, indicating varying semantic relevance across books
- O Triste Fim de Policarpo Quaresma achieved the highest Inception Score (4.5406), indicating superior image quality and diversity
- Model consistently generated predominantly white characters despite prompts describing diverse racial backgrounds
- Carefully crafted prompts significantly improved image quality and relevance compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Carefully crafted prompts significantly improve image quality and relevance.
- Mechanism: The TTI model relies on text embeddings from CLIP to guide image generation. Precise prompts that capture essential scene elements allow the model to align visual output more closely with the intended literary description.
- Core assumption: The model's text encoder effectively maps descriptive language to coherent visual concepts.
- Evidence anchors:
  - [abstract] states that "carefully crafted prompts significantly improve image quality and relevance."
  - [section II-C] explains that "descriptive prompts, each crafted to depict specific scenes from the selected books" were used.
  - [corpus] shows related work on prompt engineering for accurate object counts in generated images, supporting the importance of prompt specificity.
- Break condition: If prompts are too generic or overly complex, the model fails to generate accurate or relevant illustrations.

### Mechanism 2
- Claim: The two-stage diffusion process (base generation + refinement) enhances image fidelity.
- Mechanism: Initial denoising in Stable Diffusion XL Base 1.0 creates a rough image, which the refiner model then completes, allowing for higher resolution and detail.
- Core assumption: The refiner model can effectively correct imperfections and add fine details to the latent images from the base model.
- Evidence anchors:
  - [section II-C] describes the initial generation with Stable Diffusion XL Base 1.0.
  - [section II-D] explains the refinement stage using Stable Diffusion XL Refiner 1.0.
  - [corpus] lacks direct evidence of this specific two-stage approach, indicating this is a novel contribution.
- Break condition: If the refiner model is not properly aligned with the base model's latent space, image quality may degrade.

### Mechanism 3
- Claim: Quantitative metrics (CLIP and IS) provide objective evaluation of generated image quality.
- Mechanism: CLIP score measures semantic alignment between images and text prompts, while IS assesses image diversity and recognizability.
- Core assumption: These metrics reliably reflect human judgment of image quality and relevance to the source material.
- Evidence anchors:
  - [section II-F] details the use of CLIP and IS scores for evaluation.
  - [section III-B] presents quantitative results using these metrics for different book concepts.
  - [corpus] includes papers discussing CLIP-based evaluation for image captioning and GAN image quality assessment, supporting the validity of these metrics.
- Break condition: If the metrics are not well-calibrated to the specific task of literary illustration, they may not accurately reflect image quality.

## Foundational Learning

- Concept: Text-to-Image (TTI) diffusion models
  - Why needed here: Understanding how TTI models like Stable Diffusion generate images from text is crucial for designing effective prompts and interpreting results.
  - Quick check question: How do diffusion models progressively transform noise into coherent images based on text prompts?

- Concept: Latent Diffusion Models (LDMs)
  - Why needed here: LDMs, which Stable Diffusion is based on, operate in a compressed latent space, affecting the generation process and potential biases.
  - Quick check question: What advantages do LDMs have over other generative models in terms of image quality and computational efficiency?

- Concept: CLIP (Contrastive Language-Image Pre-training)
  - Why needed here: CLIP is used both as the text encoder for TTI models and as an evaluation metric, making it essential to understand its role and limitations.
  - Quick check question: How does CLIP's joint text-image embedding space enable semantic evaluation of generated images?

## Architecture Onboarding

- Component map:
  Input: Literary text from seven classic Brazilian books -> Prompt generator: Human-crafted descriptive prompts -> TTI model: Stable Diffusion XL (Base + Refiner) -> Output: Generated images -> Evaluation: CLIP and IS scores

- Critical path:
  1. Select descriptive passages from books
  2. Create detailed prompts capturing scene essence
  3. Generate initial images using Stable Diffusion XL Base 1.0
  4. Refine images using Stable Diffusion XL Refiner 1.0
  5. Evaluate results using CLIP and IS scores

- Design tradeoffs:
  - Prompt complexity vs. image relevance: More detailed prompts may yield better results but are harder to craft.
  - Model bias vs. prompt specificity: Careful prompt engineering can mitigate some model biases, but may not fully eliminate them.
  - Evaluation metrics vs. human judgment: Quantitative scores provide objective measures but may not fully capture aesthetic quality.

- Failure signatures:
  - Low CLIP scores: Images poorly aligned with text prompts
  - Low IS scores: Images lack diversity or recognizability
  - Consistent racial bias in generated characters: Model trained on biased data

- First 3 experiments:
  1. Generate images using only the book text as prompts to establish baseline quality.
  2. Create detailed, scene-specific prompts and compare CLIP and IS scores to baseline.
  3. Analyze image sets for racial and cultural representation, comparing to literary descriptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can text-to-image diffusion models be optimized to reduce biases in generated illustrations, particularly in representing diverse skin tones and ethnicities?
- Basis in paper: [explicit] The authors note that the Stable Diffusion model tended to produce predominantly white figures, even when prompts did not specify skin color, indicating a bias in the training data.
- Why unresolved: The paper identifies the bias but does not explore methods to mitigate it, such as using more diverse training datasets or implementing bias correction algorithms.
- What evidence would resolve it: Comparative studies showing improved representation of diverse skin tones and ethnicities in generated images after implementing specific bias mitigation techniques.

### Open Question 2
- Question: What are the most effective prompt engineering techniques for improving the semantic relevance and quality of text-to-image generated illustrations in literary contexts?
- Basis in paper: [explicit] The authors emphasize the importance of carefully crafted prompts for generating high-quality and contextually relevant images, but do not provide detailed guidelines on prompt engineering techniques.
- Why unresolved: The paper highlights the impact of prompts but lacks a comprehensive framework or set of best practices for prompt engineering in literary illustration.
- What evidence would resolve it: Empirical studies comparing different prompt engineering strategies and their impact on the quality and relevance of generated illustrations.

### Open Question 3
- Question: How can the environmental impact of text-to-image diffusion models be quantified and minimized, particularly in comparison to human-generated illustrations?
- Basis in paper: [inferred] The authors do not discuss the environmental implications of using text-to-image models, although the broader literature on generative AI mentions this concern.
- Why unresolved: The paper focuses on the technical and qualitative aspects of image generation without addressing the environmental costs associated with model training and inference.
- What evidence would resolve it: Lifecycle assessments comparing the carbon footprint of AI-generated illustrations versus traditional human-created illustrations.

## Limitations
- Model exhibits significant racial bias, consistently generating predominantly white characters regardless of prompt descriptions
- Evaluation methodology relies heavily on quantitative metrics that may not capture nuanced aesthetic and cultural aspects of literary illustration
- Prompt engineering process is labor-intensive and may not be easily generalizable to other literary works without significant manual effort

## Confidence
**High Confidence:** The core finding that carefully crafted prompts significantly improve image quality is well-supported by both the quantitative metrics (CLIP scores ranging from 0.2972 to 0.5819) and qualitative observations across multiple book examples.

**Medium Confidence:** The effectiveness of the two-stage diffusion process is demonstrated but lacks direct comparison with single-stage generation methods, making it difficult to definitively attribute quality improvements to the refinement step specifically.

**Low Confidence:** The study's claims about the practical applicability of this approach for actual book illustration are limited by the lack of human evaluation studies and the absence of testing on books with more complex narrative structures or culturally specific content.

## Next Checks
1. Conduct human evaluation studies with literary experts and target audience members to validate whether the generated images effectively capture the essence of the source material and meet aesthetic standards for book illustration.

2. Test the methodology on a more diverse corpus of Brazilian literature, including works by Afro-Brazilian authors and contemporary literature, to assess how well the approach handles varied cultural contexts and modern narrative styles.

3. Implement bias mitigation techniques during both the generation process (through adversarial prompting) and evaluation phase to systematically measure and reduce racial and cultural representation issues in the generated illustrations.