---
ver: rpa2
title: Bootstrapping Referring Multi-Object Tracking
arxiv_id: '2406.05039'
source_url: https://arxiv.org/abs/2406.05039
tags:
- tracking
- temporal
- language
- object
- refer-kitti-v2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new and large-scale dataset, Refer-KITTI-V2,
  for referring multi-object tracking (RMOT), which aims to detect and track multiple
  objects following natural language expressions. To address the limited semantic
  diversity of existing RMOT datasets, the authors propose a three-step semi-automatic
  labeling pipeline that generates 9,758 expressions with 617 unique words.
---

# Bootstrapping Referring Multi-Object Tracking

## Quick Facts
- arXiv ID: 2406.05039
- Source URL: https://arxiv.org/abs/2406.05039
- Authors: Yani Zhang; Dongming Wu; Wencheng Han; Xingping Dong
- Reference count: 40
- Introduces Refer-KITTI-V2 dataset with 9,758 expressions covering 617 unique words

## Executive Summary
This paper addresses the limited semantic diversity in existing referring multi-object tracking (RMOT) datasets by introducing a large-scale, diverse dataset called Refer-KITTI-V2. The authors propose a three-step semi-automatic labeling pipeline that generates high-quality annotations covering object dynamics with minimal manual effort. They also introduce TempRMOT, an end-to-end Transformer-based framework that incorporates a temporal enhancement module to improve long-term spatial-temporal interactions, achieving state-of-the-art performance on both Refer-KITTI and Refer-KITTI-V2 datasets.

## Method Summary
The authors propose a semi-automatic labeling pipeline that generates diverse language expressions by combining language items with rules and expanding them via LLM. They introduce TempRMOT, an end-to-end Transformer-based framework that incorporates a temporal enhancement module to improve long-term spatial-temporal interactions. The framework uses a query-driven approach with a memory mechanism to store historical query features, enabling efficient refinement of object representations across frames. Cross-modal fusion aligns visual and linguistic features for better object tracking.

## Key Results
- Achieves state-of-the-art performance on Refer-KITTI with +3.16% improvement in HOTA
- Achieves state-of-the-art performance on Refer-KITTI-V2 with +4.04% improvement in HOTA
- Introduces Refer-KITTI-V2 dataset with 9,758 expressions covering 617 unique words

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The semi-automatic labeling pipeline generates diverse language expressions by combining language items with rules and expanding them via LLM.
- Mechanism: The pipeline first identifies basic attributes ("language items") such as class, color, position, and action. These items are then combined using predefined rules to create initial prompts. Finally, a large language model (LLM) expands these prompts into more diverse expressions while preserving meaning.
- Core assumption: The LLM can generate semantically equivalent expressions that maintain the original meaning and the annotation system can automatically propagate labels between frames.
- Evidence anchors:
  - [abstract] "To efficiently generate high-quality annotations covering object dynamics with minimal manual effort, we propose a semi-automatic labeling pipeline that formulates a total of 9,758 language prompts."
  - [section] "we employ a large language model (LLM) to create a wider range of semantic expressions. For each expression generated in the second step and those in the Refer-KITTI dataset, we require GPT-3.5 [23] to generate four alternative representations that preserve the original meaning."

### Mechanism 2
- Claim: The query-based temporal enhancement module improves tracking performance by modeling long-term spatial-temporal interactions.
- Mechanism: The module stores historical query features in a memory mechanism and uses them to update current queries through a temporal decoder. This allows the model to capture temporal dynamics and refine object representations across frames.
- Core assumption: The temporal decoder can effectively aggregate historical information and the FIFO memory mechanism can maintain a constant memory consumption while providing sufficient temporal context.
- Evidence anchors:
  - [abstract] "At its core is a query-driven Temporal Enhancement Module that represents each object as a Transformer query, enabling long-term spatial-temporal interactions with other objects and past frames to efficiently refine these queries."
  - [section] "We propose an N ×K query memory to store the query sets from the previous moments, where N represents the number of stored frames, and K is the number of objects stored per frame. This memory operates on a first-in, first-out (FIFO) principle."

### Mechanism 3
- Claim: The end-to-end Transformer-based framework with cross-modal fusion aligns visual and linguistic features for better object tracking.
- Mechanism: The framework uses a fusion encoder with cross-attention to align visual features from video frames with linguistic features from expressions. This aligned representation is then used by the object decoder to generate detections and track objects.
- Core assumption: The cross-attention mechanism can effectively align features from different modalities and the object decoder can generate accurate detections based on these aligned features.
- Evidence anchors:
  - [abstract] "The core idea is to employ a language expression as a semantic cue to guide the prediction of multi-object tracking, comprehensively accounting for variations in object quantity and temporal semantics."
  - [section] "The two vanilla features are then mapped into the same dimension and fed into a fusion encoder (ξ) to perform cross-modal fusion using a cross attention module, which helps align features of different modalities."

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The entire framework is built on Transformer architecture, using self-attention and cross-attention for both temporal modeling and cross-modal fusion.
  - Quick check question: How does self-attention differ from cross-attention in the context of this framework?

- Concept: Object detection and tracking fundamentals
  - Why needed here: The framework builds upon multi-object tracking (MOT) concepts, requiring understanding of bounding box regression, object association, and tracking metrics like HOTA.
  - Quick check question: What are the key differences between detection accuracy (DetA) and association accuracy (AssA) in tracking evaluation?

- Concept: Natural language processing and semantic understanding
  - Why needed here: The task involves understanding natural language expressions to guide object tracking, requiring knowledge of language encoding, semantic representation, and cross-modal alignment.
  - Quick check question: How does the RoBERTa text encoder contribute to the cross-modal fusion process in this framework?

## Architecture Onboarding

- Component map: Visual Backbone (ResNet50) -> Feature Pyramid -> Cross-Modal Fusion Encoder -> Temporal Enhancement Module -> Object Decoder -> Bounding Box Refinement -> Loss Computation; Text Encoder (RoBERTa) -> Cross-Modal Fusion Encoder; Memory Mechanism -> Temporal Decoder -> Object Decoder
- Critical path: Visual features -> Cross-modal fusion -> Temporal enhancement -> Object detection and tracking
- Design tradeoffs: The use of temporal enhancement adds computational overhead but improves tracking accuracy, especially for objects with complex motion states.
- Failure signatures: Loss of temporal context in the memory mechanism, misalignment in cross-modal fusion, or failure to distinguish between objects with similar appearances but different motion states.
- First 3 experiments:
  1. Test the cross-modal fusion encoder with fixed text features to ensure proper alignment between visual and linguistic modalities.
  2. Evaluate the temporal enhancement module with a fixed object decoder to verify its contribution to tracking performance.
  3. Assess the end-to-end model on a small subset of the dataset to identify any integration issues between components.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions arise from the limitations discussed:

### Open Question 1
- Question: How does the performance of TempRMOT compare when using different language models for prompt expansion in the three-step labeling pipeline?
- Basis in paper: [explicit] The paper mentions using GPT-3.5 for prompt expansion but doesn't explore alternatives
- Why unresolved: The authors only use GPT-3.5 for prompt expansion without comparing it to other language models or evaluating the impact of different language models on the final model performance
- What evidence would resolve it: Experimental results comparing TempRMOT's performance when using different language models (e.g., GPT-4, LLaMA) for prompt expansion in the labeling pipeline

### Open Question 2
- Question: What is the impact of the temporal enhancement module on small object detection and tracking accuracy?
- Basis in paper: [explicit] The paper mentions that TempRMOT still faces limitations in accurately detecting small objects
- Why unresolved: The authors acknowledge the limitation but do not provide detailed analysis or experimental results specifically focusing on small object performance
- What evidence would resolve it: Detailed ablation studies and performance metrics comparing small object detection and tracking accuracy with and without the temporal enhancement module

### Open Question 3
- Question: How does the performance of TempRMOT scale with increasing video length and frame rate?
- Basis in paper: [inferred] The paper introduces a memory mechanism with a fixed length (N=4 for Refer-KITTI, N=5 for Refer-KITTI-V2) without exploring its scalability
- Why unresolved: The authors do not provide experiments or analysis on how the model performs with varying video lengths or frame rates, which is crucial for real-world applications
- What evidence would resolve it: Experimental results showing TempRMOT's performance across different video lengths and frame rates, along with analysis of the memory mechanism's scalability

### Open Question 4
- Question: What is the computational overhead of the temporal enhancement module in real-time applications?
- Basis in paper: [explicit] The paper mentions an additional 0.006 seconds per frame for the temporal enhancement module but doesn't discuss real-time applicability
- Why unresolved: While the authors provide the computational overhead, they don't discuss the implications for real-time applications or compare it to real-time requirements
- What evidence would resolve it: Analysis of TempRMOT's performance in real-time scenarios, including frame rate comparisons with and without the temporal enhancement module, and discussion of its applicability to real-time systems

## Limitations
- The semi-automatic labeling pipeline relies heavily on LLM performance and may introduce subtle biases
- Performance gains are primarily demonstrated on two KITTI-based datasets, raising questions about generalization
- The temporal enhancement module's exact contribution is difficult to isolate due to the end-to-end training approach

## Confidence
- High Confidence: The effectiveness of the TempRMOT framework on benchmark datasets (Refer-KITTI and Refer-KITTI-V2), supported by clear HOTA improvements of +3.16% and +4.04% respectively.
- Medium Confidence: The scalability and quality of the semi-automatic labeling pipeline, as the diversity metrics are impressive but the long-term stability of generated expressions hasn't been extensively validated.
- Medium Confidence: The architectural contributions of the temporal enhancement module, though improvements are significant, the exact contribution of each component within the module isn't fully isolated.

## Next Checks
1. Test the framework's performance on a third-party dataset with different characteristics (e.g., urban vs. rural settings) to assess generalization beyond the KITTI-based datasets.
2. Conduct ablation studies isolating the temporal enhancement module's components (memory mechanism vs. temporal decoder) to quantify individual contributions to performance gains.
3. Evaluate the framework's robustness to noisy or ambiguous language expressions by introducing controlled perturbations to the input expressions and measuring tracking performance degradation.