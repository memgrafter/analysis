---
ver: rpa2
title: 'Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification
  in Alignment'
arxiv_id: '2408.06266'
source_url: https://arxiv.org/abs/2408.06266
tags:
- preference
- alignment
- arxiv
- clair
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how preference datasets and alignment objectives
  interact during contrastive alignment of LLMs. It introduces Contrastive Learning
  from AI Revisions (CLAIR), a method that creates minimally contrastive preference
  pairs by revising a weaker model's output with a stronger one, and Anchored Preference
  Optimization (APO), a family of alignment objectives that explicitly control reward
  dynamics based on the model-data relationship.
---

# Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment

## Quick Facts
- arXiv ID: 2408.06266
- Source URL: https://arxiv.org/abs/2408.06266
- Reference count: 21
- Key outcome: CLAIR+APO-zero improves MixEval-Hard by 7.65%, closing 45% of gap to GPT4-turbo

## Executive Summary
This paper addresses underspecification in LLM alignment by introducing Contrastive Learning from AI Revisions (CLAIR) and Anchored Preference Optimization (APO). CLAIR creates minimally contrastive preference pairs by revising weaker model outputs with a stronger model, while APO provides explicit control over reward dynamics during training. The authors align Llama-3-8B-Instruct on 32K UltraFeedback prompts using APO variants and show that contrastiveness is the primary driver of performance improvements across all tested datasets.

## Method Summary
The authors create preference datasets using four methods: CLAIR (revision-based), on-policy judge, off-policy judge, and Stronger Preferred baselines. They implement APO variants (APO-zero, APO-down) alongside conventional objectives (DPO, KTO, SFT) and train Llama-3-8B-Instruct for 18 epochs on each dataset-objective pair using RMSProp optimizer (lr=2×10⁻⁷), batch size 16, and 512 token truncation. Models are evaluated on MixEval-Hard and LC-AlpacaEval2.0 to measure alignment performance and correlation with human rankings.

## Key Results
- CLAIR+APO-zero achieves 7.65% improvement on MixEval-Hard, closing 45% of gap to GPT4-turbo
- APO variants consistently outperform conventional objectives across all datasets
- Contrastiveness from CLAIR is identified as the key driver of performance gains
- APO-down outperforms APO-zero on off-policy Judge dataset, validating the model-data relationship hypothesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimally contrastive preference pairs improve the learning signal.
- Mechanism: When winning and losing outputs differ only along the relevant axis, the model can more easily attribute preference to correct features.
- Core assumption: Spurious differences between outputs obscure true preference signal and increase credit assignment difficulty.
- Evidence anchors:
  - CLAIR creates minimally contrastive preference pairs by revising weaker model's output with stronger one
  - Minimally contrastive outputs result in less spurious differences and clearer minimal contrast
  - FMR score of 0.625 for APO: Alpha-Divergence Preference Optimization suggests divergence regime control improves alignment
- Break condition: If revision process introduces new spurious differences or fails to minimally modify output, benefit disappears.

### Mechanism 2
- Claim: APO provides explicit control over model likelihood changes during training.
- Mechanism: APO variants specify whether winning/losing outputs should increase or decrease in likelihood, preventing ambiguous training dynamics seen in DPO.
- Core assumption: Relationship between model quality and preference data quality determines whether increasing or decreasing likelihood of winning outputs is beneficial.
- Evidence anchors:
  - APO is family of alignment objectives that explicitly control reward dynamics based on model-data relationship
  - APO-zero increases likelihood of winning outputs and decreases likelihood of losing outputs
  - FMR score of 0.647 for ADPO: Anchored Direct Preference Optimization indicates anchoring optimization process improves outcomes
- Break condition: If dataset contains mixed quality pairs where some winning outputs are better and others worse than target model, single APO variant may not be optimal.

### Mechanism 3
- Claim: Contrastiveness is primary driver of performance gains in alignment.
- Mechanism: CLAIR revision process creates outputs that differ minimally but meaningfully, allowing model to learn precise features that make one output preferable.
- Core assumption: Effectiveness of alignment is more dependent on quality of preference signal than specific alignment objective used.
- Evidence anchors:
  - Experiments with CLAIR preferences and APO achieve 7.65% improvement on MixEval-Hard
  - Analysis indicates contrastiveness of CLAIR preferences is major driver of performance
  - FMR score of 0.604 for Accelerated Preference Optimization suggests optimizing preference signal improves alignment speed and effectiveness
- Break condition: If revision process fails to create meaningful contrasts or differences introduced are not aligned with human preferences, benefit disappears.

## Foundational Learning

- Concept: Contrastive learning objectives
  - Why needed here: Understanding how DPO and APO work is crucial for implementing alignment methods described
  - Quick check question: What is key difference between DPO and APO in terms of how they handle likelihood of winning and losing outputs?

- Concept: Preference dataset creation
  - Why needed here: Creating effective preference datasets is essential for training aligned models
  - Quick check question: How does CLAIR method differ from traditional judge-based preference dataset creation?

- Concept: Reward modeling and reinforcement learning
  - Why needed here: Understanding connection between preference data and reward models helps grasp alignment objectives
  - Quick check question: How does DPO avoid need for explicit reward model compared to traditional RLHF methods?

## Architecture Onboarding

- Component map:
  Data creation pipeline: CLAIR (revision-based) -> Judge-based methods
  Alignment objectives: DPO -> KTO -> SFT -> APO-zero -> APO-down
  Evaluation framework: MixEval-Hard -> LC-AlpacaEval2.0
  Training infrastructure: TRL library -> 8 NVIDIA H100 GPUs

- Critical path:
  1. Generate preference pairs using CLAIR or Judge methods
  2. Filter and prepare preference data
  3. Select appropriate alignment objective based on model-data relationship
  4. Train model for 18 epochs with checkpointing
  5. Evaluate on MixEval-Hard and LC-AlpacaEval2.0

- Design tradeoffs:
  - Contrastiveness vs dataset size: CLAIR creates more contrastive pairs but may be more expensive to generate
  - Explicit control vs simplicity: APO provides more control but is more complex than DPO
  - On-policy vs off-policy data: On-policy data is more relevant but may be harder to collect at scale

- Failure signatures:
  - Degradation in performance when using Stronger Preferred dataset with contrastive objectives
  - Inconsistent results between MixEval-Hard and LC-AlpacaEval2.0
  - Training instability when using mismatched APO variant and dataset

- First 3 experiments:
  1. Implement CLAIR data creation and compare Jaccard similarity and Levenshtein distance with Judge-based methods
  2. Train Llama-3-8B-Instruct on CLAIR data using APO-zero and evaluate on MixEval-Hard
  3. Compare performance of APO-zero vs APO-down on off-policy Judge dataset to validate model-data relationship assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is precise mechanism by which contrastiveness in preference pairs drives performance improvements?
- Basis in paper: The authors show CLAIR's contrastiveness is major driver of performance but don't explain underlying mechanism
- Why unresolved: While paper demonstrates contrastive data improves alignment, it doesn't explain why - whether due to clearer credit assignment, reduced spurious correlations, or other factors
- What evidence would resolve it: Controlled experiments isolating different aspects of contrastiveness and their impact on alignment performance

### Open Question 2
- Question: How does relationship between model capability and preference data quality affect choice of APO variant?
- Basis in paper: Authors note different APO variants work better depending on whether winning outputs are better or worse than target model
- Why unresolved: While paper identifies this relationship, it doesn't provide clear guidelines for determining which APO variant to use in practice when relative quality of model vs preference data is unknown
- What evidence would resolve it: Systematic study of APO performance across different model-data capability relationships

### Open Question 3
- Question: Can APO variants be selected at preference pair level rather than dataset level?
- Basis in paper: Authors mention this as potential extension in future work section
- Why unresolved: Current implementation uses same APO variant for all pairs in dataset, but real-world datasets may contain mixed-quality pairs requiring different optimization strategies
- What evidence would resolve it: Experiments comparing pair-level vs dataset-level APO variant selection

### Open Question 4
- Question: How does choice of reference model in APO affect alignment outcomes?
- Basis in paper: Paper uses πref in reward calculation but doesn't explore different reference models
- Why unresolved: Paper fixes reference model but doesn't investigate how different choices impact alignment performance
- What evidence would resolve it: Systematic comparison of APO performance using different reference models

## Limitations

- The claim that contrastiveness is primary driver of performance gains has medium confidence due to lack of isolation from other factors
- The 45% gap closure to GPT4-turbo has low confidence due to potential evaluation variance and lack of statistical significance testing
- Theoretical justification for when each APO variant should be preferred could be more rigorous

## Confidence

- **High confidence**: APO variants consistently outperform conventional objectives across all tested datasets
- **Medium confidence**: Contrastiveness is primary driver of performance gains; APO provides explicit control over model likelihood changes
- **Low confidence**: The 45% gap closure to GPT4-turbo is robust across different evaluation conditions

## Next Checks

1. **Isolate contrastiveness effect**: Create synthetic preference pairs with controlled levels of contrastiveness and test whether APO performance scales with contrastiveness regardless of dataset origin.

2. **Validate APO variant selection**: Conduct systematic study varying model-data relationship across multiple domains to test hypothesis that APO-zero should be used when winning outputs are better than target, and APO-down when they are worse.

3. **Test statistical significance**: Run multiple training seeds for best-performing APO + CLAIR combination and perform significance testing on MixEval-Hard improvements to establish robustness.