---
ver: rpa2
title: 'OMENN: One Matrix to Explain Neural Networks'
arxiv_id: '2412.02399'
source_url: https://arxiv.org/abs/2412.02399
tags:
- omenn
- neural
- input
- methods
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OMENN addresses the lack of transparency in deep neural networks
  by representing a neural network as a single, interpretable matrix for each input,
  derived through a series of linear transformations that represent the processing
  of the input by each successive layer. The method introduces weight and bias representation
  matrices that, when combined with the input, yield a single matrix whose sum corresponds
  directly to the model's logit value, enabling locally precise attribution-based
  explanations.
---

# OMENN: One Matrix to Explain Neural Networks

## Quick Facts
- arXiv ID: 2412.02399
- Source URL: https://arxiv.org/abs/2412.02399
- Authors: Adam Wróbel; Mikołaj Janusz; Bartosz Zieliński; Dawid Rymarczyk
- Reference count: 40
- One-line result: OMENN represents neural networks as single interpretable matrices that outperform state-of-the-art XAI methods on faithfulness, contrastivity, and distractability metrics.

## Executive Summary
OMENN (One Matrix to Explain Neural Networks) addresses the lack of transparency in deep neural networks by representing any input as a single, interpretable matrix. The method introduces weight and bias representation matrices that, when combined with the input, yield a single matrix whose sum corresponds directly to the model's logit value. This enables locally precise attribution-based explanations that can be applied to both Vision Transformers and Convolutional Neural Networks.

Experiments on the FunnyBirds benchmark and ImageNet demonstrate that OMENN achieves state-of-the-art performance, with faithfulness scores nearly double the second-best method on ViT-B/16. The method excels in identifying image regions that drive distinctions between data classes and provides interpretable explanations across modern neural network architectures.

## Method Summary
OMENN reformulates neural network layers (fully-connected, convolution, attention, normalization, activation functions) as affine or input-dependent affine transformations. It creates weight representation matrix Cw and bias representation matrix Cb that, when combined with the input X, yield a single explanation matrix C. The sum of all elements in C matches the logit value of the class being explained, ensuring completeness. The method requires modifying the model's backward propagation to compute raw weights and biases, followed by post-processing steps including outlier removal and smoothing. Implementation supports both ViT and CNN architectures and has been evaluated on FunnyBirds and ImageNet datasets.

## Key Results
- Achieved faithfulness score of 0.053 on ViT-B/16, nearly double the second-best method
- Outperformed or matched state-of-the-art methods on FunnyBirds benchmark across five evaluation dimensions
- Highest score in Contrastivity, excelling at identifying image regions that drive distinctions between data classes
- Successfully applied to both Vision Transformers and Convolutional Neural Networks

## Why This Works (Mechanism)

### Mechanism 1
OMENN provides locally exact explanations by representing the neural network as a single input-dependent affine transformation. It reformulates each layer into an affine transformation and composes these into a unified transformation. Core assumption: each layer can be expressed as affine or input-dependent affine transformation. Break condition: if any layer cannot be expressed affinely, OMENN cannot be applied.

### Mechanism 2
The OMENN explanation matrix C is directly derived from the model's weights and biases, ensuring faithfulness to the network's output. It creates weight representation matrix Cw and bias representation matrix Cb that when combined with input X yield explanation matrix C, whose sum matches the logit value. Core assumption: dynamic linearity property holds for the neural network. Break condition: if network contains operations violating dynamic linearity, explanation matrix won't accurately represent output.

### Mechanism 3
OMENN outperforms other XAI methods on benchmarks like FunnyBirds and ImageNet, demonstrating its effectiveness. Its locally precise explanations, derived directly from model parameters, provide more accurate attribution than methods relying on approximations like gradients or perturbations. Core assumption: benchmarks used are appropriate for evaluating XAI methods. Break condition: if benchmarks don't adequately capture OMENN's strengths, claimed performance may not hold.

## Foundational Learning

- **Concept: Affine transformations and their composition**
  - Why needed here: OMENN relies on representing each layer as an affine transformation and composing these into a single transformation
  - Quick check question: Can you express a fully-connected layer as an affine transformation? (Answer: Yes, y = Wx + b)

- **Concept: Kronecker product and its properties**
  - Why needed here: The Kronecker product is used in vectorization of certain layers (e.g., attention) to express them as affine transformations
  - Quick check question: What is the result of A ⊗ (B + C) using Kronecker product properties? (Answer: (A ⊗ B) + (A ⊗ C))

- **Concept: Dynamic linearity**
  - Why needed here: OMENN's approach relies on the dynamic linearity property, which allows the entire neural network to be represented as a single input-dependent affine transformation
  - Quick check question: What is the key difference between a static affine transformation and an input-dependent affine transformation? (Answer: In an input-dependent affine transformation, the weights and biases are functions of the input.)

## Architecture Onboarding

- **Component map**: Layer reformulation -> Parameter combination -> Augmented input creation -> Explanation matrix construction (Cw and Cb) -> Explanation generation
- **Critical path**: 1) Identify and reformulate each layer in neural network, 2) Combine weights and biases for each layer, 3) Create augmented input, 4) Compute explanation matrix C, 5) Generate final explanation
- **Design tradeoffs**: OMENN trades computational complexity for locally exact explanations. Requires modifying backward pass and may not apply to architectures with non-affine operations
- **Failure signatures**: 1) Layer cannot be expressed as affine transformation, 2) Incorrect implementation of layer reformulation or parameter combination, 3) Errors in augmented input or explanation matrix construction, 4) Issues with explanation generation process
- **First 3 experiments**:
  1. Implement OMENN on simple fully-connected network and verify explanation matrix C sums to correct logit value
  2. Apply OMENN to convolutional network and compare explanations with gradient-based methods
  3. Evaluate OMENN on small benchmark dataset (e.g., FunnyBirds) and compare performance with other XAI methods

## Open Questions the Paper Calls Out

- **Open Question 1**: How does OMENN's performance vary with different neural network architectures beyond ViTs and CNNs, such as RNNs or GNNs? The paper focuses on ViTs and CNNs but doesn't explore other architectures. Testing OMENN on RNNs and GNNs would provide insights into its versatility.

- **Open Question 2**: Can OMENN be adapted to provide explanations for multi-modal inputs, such as combining text and image data? The methodology described is tailored to image data and doesn't address handling other data types like text. Developing an extension for multi-modal inputs would be valuable.

- **Open Question 3**: What is the computational overhead of OMENN compared to other XAI methods, especially for large-scale models or datasets? The paper mentions efficient implementation but doesn't provide detailed analysis of computational overhead compared to other methods.

## Limitations

- Theoretical claims about dynamic linearity and exact affine reformulations are not fully validated in experimental section
- FunnyBirds benchmark results cannot be independently verified due to missing framework details and code
- Comparison with other XAI methods relies on assumptions about implementation parity and evaluation protocols

## Confidence

- **High confidence**: Mechanism 1 (affine transformation formulation) - mathematically well-specified with clear implementation path
- **Medium confidence**: Mechanism 2 (faithfulness of C matrix) - theoretically sound but dependent on correct layer reformulations
- **Medium confidence**: Mechanism 3 (benchmark performance) - claims are specific but verification requires external resources

## Next Checks

1. Implement OMENN on a simple two-layer fully-connected network with synthetic data where ground truth explanations are known, verifying that C sums to the correct logit value
2. Test OMENN's affine transformation formulations on each layer type individually (fully-connected, convolution, attention) using small models with controlled inputs
3. Reproduce the faithfulness metric calculation on a single ImageNet class using Quantus, comparing Pearson correlation scores with baseline methods under controlled conditions