---
ver: rpa2
title: Text Clustering with Large Language Model Embeddings
arxiv_id: '2403.15112'
source_url: https://arxiv.org/abs/2403.15112
tags:
- clustering
- text
- embeddings
- dataset
- k-means
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the effectiveness of embeddings from large
  language models (LLMs) in enhancing text clustering performance. Five datasets of
  varying size and domain were used, including the CSTR abstracts, SyskillWebert,
  20Newsgroups, MN-DS, and Reuters datasets.
---

# Text Clustering with Large Language Model Embeddings

## Quick Facts
- arXiv ID: 2403.15112
- Source URL: https://arxiv.org/abs/2403.15112
- Reference count: 40
- Primary result: OpenAI embeddings combined with k-means algorithm yield the highest clustering performance across multiple metrics and datasets

## Executive Summary
This study systematically evaluates the effectiveness of large language model embeddings for text clustering tasks. Using five diverse datasets and comparing five different embedding methods (TF-IDF, BERT, OpenAI, LLaMA-2, and Falcon) with four clustering algorithms, the research demonstrates that OpenAI embeddings consistently outperform alternatives in clustering quality. The combination of k-means clustering with OpenAI's embeddings achieves superior results across Adjusted Rand Index, F1-score, and Homogeneity metrics. The study also investigates whether increasing model dimensionality and using summarisation techniques improve clustering efficiency, finding that these strategies do not consistently enhance performance.

## Method Summary
The study compares five embedding methods (TF-IDF, BERT, OpenAI, LLaMA-2, and Falcon) with four clustering algorithms (k-means, k-means++, agglomerative hierarchical clustering, fuzzy c-means) across five datasets of varying size and domain. Text preprocessing involves removing HTML tags, emails, and non-Latin characters. Clustering performance is evaluated using five metrics: Adjusted Rand Index, F1-score, Homogeneity score, Silhouette Score, and Calinski-Harabasz Index. The minimum viable reproduction plan requires preprocessing datasets, generating embeddings using the specified models, applying clustering algorithms, and evaluating results with the defined metrics.

## Key Results
- OpenAI embeddings combined with k-means algorithm achieved highest values of Adjusted Rand Index, F1-score, and Homogeneity score across most experiments
- BERT embeddings performed best among open-source alternatives
- Increasing model dimensionality and using summarisation techniques did not consistently enhance clustering efficiency
- K-means algorithm paired with OpenAI embeddings yielded the most consistent high performance across datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenAI embeddings capture structured language better than other embeddings
- Mechanism: Pre-training on diverse internet text enables OpenAI models to encode broader semantic relationships and contextual understanding
- Core assumption: Training data diversity directly translates to better clustering performance
- Evidence anchors: Abstract states "OpenAI's GPT-3.5 Turbo model yields better results in three out of five clustering metrics across most tested datasets" and section confirms "OpenAI embeddings generally yield superior clustering performance on structured, formal texts"

### Mechanism 2
- Claim: K-means algorithm combined with OpenAI embeddings maximizes clustering quality metrics
- Mechanism: K-means efficiently partitions high-dimensional embedding space when embeddings are well-separated and meaningful
- Core assumption: Good embeddings produce clear geometric separation that K-means can exploit
- Evidence anchors: Abstract and section both state "The combination of k-means algorithm and OpenAI's embeddings yielded the highest values of Adjusted Rand Index, F 1-score, and Homogeneity score in most experiments"

### Mechanism 3
- Claim: Increasing model dimensionality and using summarisation don't consistently improve clustering efficiency
- Mechanism: Higher dimensionality can introduce noise and summarisation may lose critical clustering-relevant information
- Core assumption: More parameters and compressed representations always improve performance
- Evidence anchors: Abstract states "increasing model dimensionality and employing summarisation techniques do not consistently enhance clustering efficiency" and section confirms this finding

## Foundational Learning

- Concept: TF-IDF vs neural embeddings
  - Why needed here: Understanding baseline vs modern approaches for text representation
  - Quick check question: What key limitation does TF-IDF have that neural embeddings address?

- Concept: Clustering algorithm selection
  - Why needed here: Different algorithms have different strengths for various data structures
  - Quick check question: Why might K-means work better with well-separated embeddings compared to hierarchical clustering?

- Concept: Evaluation metrics in clustering
  - Why needed here: Need to understand what different metrics measure and their limitations
  - Quick check question: Why might Silhouette Score and Adjusted Rand Index give different results on the same clustering?

## Architecture Onboarding

- Component map: Dataset preprocessing → Embedding computation → Clustering algorithm → Evaluation metrics → Visualization/analysis
- Critical path: Embedding quality → Clustering algorithm performance → Metric evaluation → Interpretation
- Design tradeoffs: Model size vs computational cost, embedding quality vs interpretability, algorithm simplicity vs sophistication
- Failure signatures: Low F1 scores with high Silhouette scores (good internal structure but poor ground truth alignment), summarisation reducing performance, larger models underperforming
- First 3 experiments:
  1. Run K-means with TF-IDF vs OpenAI embeddings on CSTR dataset to establish baseline improvement
  2. Test same embeddings with FuzzyCM on Reuters dataset to verify algorithm dependence
  3. Compare Falcon-7b vs Falcon-40b embeddings on SyskillWebert to observe model size effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM embeddings perform on datasets with highly informal or noisy text compared to structured formal texts?
- Basis in paper: The paper found OpenAI embeddings perform best on structured formal texts but didn't test on informal/noisy data
- Why unresolved: The study focused on formal academic and news datasets, leaving informal text domains unexplored
- What evidence would resolve it: Comparative clustering experiments using social media, forum posts, or chat data with various LLM embeddings

### Open Question 2
- Question: What is the optimal balance between model size and computational efficiency for text clustering tasks?
- Basis in paper: The study observed that larger models don't consistently improve clustering and have significant computational demands
- Why unresolved: The experiments were limited by computational resources, preventing comprehensive testing of very large models
- What evidence would resolve it: Systematic experiments across different model sizes and dataset scales measuring clustering performance vs computational cost

### Open Question 3
- Question: How do different text preprocessing techniques (beyond basic cleaning) affect clustering performance with LLM embeddings?
- Basis in paper: The study used basic preprocessing but acknowledged that more sophisticated approaches might be needed
- Why unresolved: The preprocessing was kept minimal to isolate embedding effects, leaving the impact of advanced techniques unexplored
- What evidence would resolve it: Comparative experiments testing various preprocessing pipelines (lemmatization, stopword removal, etc.) across different embeddings

## Limitations

- Dataset selection, while diverse, may not fully represent all text clustering scenarios, particularly specialized domains
- Computational constraints limited exploration of additional model variants and hyperparameter tuning
- Comparison with traditional methods may not capture the full potential of modern embedding techniques with more sophisticated clustering approaches

## Confidence

- **High Confidence**: OpenAI embeddings consistently outperform other embeddings in clustering tasks
- **Medium Confidence**: Larger model sizes and dimensionality reduction through summarisation don't consistently improve clustering efficiency
- **Medium Confidence**: K-means combined with OpenAI embeddings shows superior performance but may be algorithm-specific

## Next Checks

1. Test the same embedding and clustering combinations on specialized domain datasets (e.g., medical literature or legal documents) to verify generalizability beyond current datasets
2. Compare the performance of the identified best combination (k-means + OpenAI embeddings) against state-of-the-art clustering methods specifically designed for LLM embeddings
3. Investigate the impact of different preprocessing strategies on clustering performance, particularly for multilingual datasets where non-Latin character handling could significantly affect results