---
ver: rpa2
title: Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation
arxiv_id: '2409.09135'
source_url: https://arxiv.org/abs/2409.09135
tags:
- conversation
- engagement
- partner
- https
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a dataset of dyadic conversations recorded
  using smart glasses and proposes a novel fusion approach using Large Language Models
  (LLMs) to predict engagement levels. The method involves creating a multimodal transcript
  that combines speech, gaze, and facial expression data, which is then processed
  by an LLM to predict self-reported engagement scores.
---

# Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation

## Quick Facts
- arXiv ID: 2409.09135
- Source URL: https://arxiv.org/abs/2409.09135
- Authors: Cheng Charles Ma; Kevin Hyekang Joo; Alexandria K. Vail; Sunreeta Bhattacharya; Álvaro Fernández García; Kailana Baker-Matsuoka; Sheryl Mathew; Lori L. Holt; Fernando De la Torre
- Reference count: 40
- Primary result: Introduces a novel LLM-based fusion approach for engagement prediction in dyadic conversations, achieving performance comparable to classical fusion techniques

## Executive Summary
This paper presents a novel approach to predicting engagement levels in dyadic conversations by leveraging Large Language Models (LLMs) for multimodal fusion. The method involves creating a multimodal transcript that combines speech, gaze, and facial expression data, which is then processed by an LLM to predict self-reported engagement scores. The approach demonstrates that LLMs can effectively reason about real-world human behavior through structured prompts, offering a promising direction for computational behavior analysis.

## Method Summary
The method involves collecting smart glasses recordings of dyadic conversations, extracting behavioral features (speech transcription, gaze tracking, facial expressions), and generating multimodal transcripts that combine these features with dialogue text. The multimodal transcript is then processed by an LLM (using OpenAI's API) to predict engagement levels, with self-reported engagement scores serving as ground truth. The approach is evaluated using RMSE for exact score prediction and Krippendorff's alpha for valence/arousal classification.

## Key Results
- The LLM-based fusion approach achieves performance comparable to classical fusion techniques (KNN, SVM, RF, Bi-LSTM, MLP) in predicting engagement levels.
- The multimodal transcript effectively serves as a structured prompt, enabling the LLM to reason about conversational behavior.
- Inclusion of personality and belief questionnaire data in the system prompt improves engagement prediction accuracy.

## Why This Works (Mechanism)

### Mechanism 1
The multimodal transcript serves as a structured prompt that aligns the LLM's reasoning capabilities with human conversational behavior. By translating behavioral signals (gaze, facial expressions, speech) into natural language descriptions and sequencing them alongside dialogue, the model receives temporally aligned, interpretable cues that map directly to the questionnaire items.

### Mechanism 2
Inclusion of personality and belief questionnaire data in the system prompt improves engagement prediction accuracy. Providing the model with self-reported personality traits and socio-political beliefs allows it to contextualize behavioral cues within individual predispositions, enhancing inference of subjective engagement.

### Mechanism 3
The Socratic Models framework enables multimodal models to "discuss" behavioral cues via language, avoiding the need for abstract embedding spaces. By framing the task as a language-driven exchange, specialized pre-trained models translate their interpretations of inputs into natural language, which is then processed by an LLM for behavioral reasoning.

## Foundational Learning

- **Concept: Multimodal fusion in human behavior analysis**
  - Why needed here: The task requires combining audio, visual, and linguistic signals to predict engagement, which is inherently multimodal.
  - Quick check question: What are the three main modalities used in this engagement prediction task?

- **Concept: Large language model prompting strategies**
  - Why needed here: The LLM must be prompted effectively with structured behavioral data to simulate human responses.
  - Quick check question: How is the multimodal transcript structured to guide the LLM's reasoning?

- **Concept: Evaluation metrics for behavioral prediction**
  - Why needed here: Performance is assessed using RMSE and Krippendorff's alpha, which are standard for regression and inter-rater agreement tasks.
  - Quick check question: What do RMSE and Krippendorff's alpha measure in this context?

## Architecture Onboarding

- **Component map:**
  Data collection (Pupil Invisible smart glasses) -> Preprocessing (video undistortion, synchronization, feature extraction) -> Multimodal transcript generation -> LLM inference -> Evaluation

- **Critical path:**
  1. Capture raw behavioral data with smart glasses
  2. Preprocess and extract features (facial expressions, gaze, speech)
  3. Generate multimodal transcript with system prompt
  4. Send to LLM and collect predictions
  5. Evaluate against self-reported engagement scores

- **Design tradeoffs:**
  - Using GPT-4 vs. smaller models: higher accuracy but higher cost and latency
  - Including personality/belief data: improved personalization but potential privacy concerns
  - Five-minute transcript limit: reduces cost but may lose long-term engagement signals

- **Failure signatures:**
  - Low Krippendorff's alpha: model fails to capture valence/arousal distinctions
  - High RMSE: poor exact score prediction
  - Non-numeric LLM responses: model unable to interpret prompt or modality mismatch

- **First 3 experiments:**
  1. Ablation study: test performance with only speech, only gaze, only facial expressions
  2. Truncation study: vary transcript length to find optimal input size
  3. Model comparison: test GPT-4, GPT-3.5, and open-source alternatives on same task

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the LLM fusion approach scale with increasing conversation length beyond the 5-minute truncation limit? The paper mentions truncating multimodal transcripts to five minutes and suggests future studies could explore the impact of different truncation lengths.

### Open Question 2
What is the impact of different pre-trained model choices (e.g., alternative facial expression analyzers or speech recognition systems) on the accuracy of the multimodal transcript and subsequent LLM predictions? The paper acknowledges that the multimodal transcript relies on pre-trained models like OpenFace, MediaPipe, and Whisper, and notes that bias and robustness issues in these models should be considered.

### Open Question 3
How does the LLM fusion approach generalize to conversations with different participant demographics (e.g., age, cultural background, personality types) not represented in the current dataset? The paper acknowledges the limited demographic variance in its participant sample and raises the question of how well LLMs can simulate engagement questionnaire responses for different populations.

## Limitations

- The study is based on a small dataset of 34 participants, limiting generalizability.
- The specific demographic focus (young adults in Israel) may limit applicability to other populations or cultural contexts.
- The evidence supporting the proposed mechanisms is largely theoretical or based on related work rather than direct empirical validation.

## Confidence

- **Multimodal Transcript as Effective Prompt**: Medium
- **Inclusion of Personality and Belief Data**: Low
- **Socratic Models Framework**: Low

## Next Checks

1. Conduct an ablation study to test the contribution of each modality (speech, gaze, facial expressions) to the LLM's performance.
2. Validate the approach on a larger and more diverse dataset to assess generalizability.
3. Design experiments to directly test the proposed mechanisms, such as evaluating the impact of personality/belief data on predictions or analyzing the LLM's reasoning process using interpretability tools.