---
ver: rpa2
title: Extending Translate-Train for ColBERT-X to African Language CLIR
arxiv_id: '2404.08134'
source_url: https://arxiv.org/abs/2404.08134
tags:
- training
- language
- document
- documents
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The HLTCOE team adapted the Translate-Train approach to train ColBERT-X
  models for African language CLIR, using machine-translated MS MARCO passages and
  document collections. They explored masked language model fine-tuning for Yoruba,
  English-ColBERT indexing of translated documents, and JH POLO in-domain query generation.
---

# Extending Translate-Train for ColBERT-X to African Language CLIR

## Quick Facts
- arXiv ID: 2404.08134
- Source URL: https://arxiv.org/abs/2404.08134
- Reference count: 17
- Primary result: English-ColBERT models using translated documents achieved nDCG@20 up to 0.574 (Hausa) and R@100 up to 0.644 (Somali)

## Executive Summary
The HLTCOE team adapted the Translate-Train approach to train ColBERT-X models for African language CLIR, using machine-translated MS MARCO passages and document collections. They explored masked language model fine-tuning for Yoruba, English-ColBERT indexing of translated documents, and JH POLO in-domain query generation. The best results came from English-ColBERT models using translated documents, with nDCG@20 scores up to 0.574 (Hausa) and R@100 up to 0.644 (Somali). ColBERT-X models trained with Translate-Train outperformed English-trained variants, but were less effective than English-ColBERT. MLM fine-tuning helped when followed by Translate-Train, but combining both with JH POLO did not consistently improve performance. Unofficial runs using a different ColBERT-X implementation showed improved stability and effectiveness.

## Method Summary
The team used machine translation to translate MS MARCO passages into Hausa, Somali, Swahili, and Yoruba, then trained ColBERT-X models using the Translate-Train approach where English queries are paired with translated passages. They also explored masked language model fine-tuning for Yoruba using the Afriberta corpus before Translate-Train training, and generated in-domain training queries using the JH POLO approach with GPT-4. The retrieval models were then evaluated on CIRAL test collections using nDCG@20 and R@100 metrics.

## Key Results
- English-ColBERT models using translated documents outperformed ColBERT-X models trained with Translate-Train
- ColBERT-X with Translate-Train outperformed English-trained variants for most languages
- MLM fine-tuning improved performance when followed by Translate-Train, but not when combined with JH POLO
- Unofficial ColBERT-X implementation runs showed improved stability and effectiveness compared to official runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translate-Train adapts ColBERT-X to African languages by translating MS MARCO passages into target languages while keeping English queries.
- Mechanism: Training the model on English query-translated passage pairs allows the model to learn cross-language retrieval patterns without requiring translated queries.
- Core assumption: Machine translation quality is sufficient for the translated passages to retain meaningful semantic information for training.
- Evidence anchors:
  - [abstract] "Our submissions use machine translation models to translate the documents and the training passages, and ColBERT-X as the retrieval model."
  - [section 2.2] Describes machine translation setup for MS MARCO passages into Hausa, Somali, Swahili, and Yoruba.
  - [corpus] Weak - corpus shows related work but no direct evidence of MT quality impact.
- Break condition: If MT quality is too poor, translated passages lose semantic coherence, breaking the training signal.

### Mechanism 2
- Claim: Masked Language Model (MLM) fine-tuning with Afriberta corpus improves ColBERT-X performance for low-resource languages like Yoruba.
- Mechanism: Fine-tuning XLM-RoBERTa on African language text preserves and enhances multilingual representations before retrieval training.
- Core assumption: Exposure to target language text during MLM fine-tuning prevents catastrophic forgetting of other language knowledge.
- Evidence anchors:
  - [section 3.1] "Since XLM-RoBERTa pretraining does not include Yoruba, we designed a fine-tuning step to accommodate this absence."
  - [section 4] "We observe that MLM fine-tuning is generally helpful even when followed by Translate-Train."
  - [corpus] Weak - corpus lacks direct evidence of Afriberta corpus impact on model performance.
- Break condition: If Afriberta corpus is too small or unrepresentative, MLM fine-tuning may not provide meaningful language-specific improvements.

### Mechanism 3
- Claim: JH POLO generates in-domain training queries that better match the CIRAL collection than translated MS MARCO queries.
- Mechanism: GPT-4 creates English queries based on document pairs from the target collection, providing relevant training examples without translation issues.
- Core assumption: GPT-4 can generate queries that accurately reflect document content without assuming prior knowledge of the articles.
- Evidence anchors:
  - [section 3.3] Describes JH POLO methodology and prompt design for generating training queries.
  - [section 4] "However, the in-domain JH POLO fine-tuning does not seem to be helpful" - indicates mixed results.
  - [corpus] Weak - corpus lacks direct evidence of JH POLO's effectiveness in African language CLIR.
- Break condition: If GPT-4 consistently generates queries that reference articles or assume prior knowledge, the training examples become invalid.

## Foundational Learning

- Concept: Cross-Language Information Retrieval (CLIR)
  - Why needed here: Understanding CLIR is essential for grasping why translation-based approaches are necessary when query and document languages differ.
  - Quick check question: What is the main challenge in CLIR compared to monolingual retrieval?

- Concept: Dense Retrieval Models (e.g., ColBERT-X)
  - Why needed here: The paper uses ColBERT-X as the core retrieval model, requiring understanding of how dense retrieval differs from traditional sparse retrieval.
  - Quick check question: How does dense retrieval represent documents differently from traditional BM25?

- Concept: Knowledge Distillation and Transfer Learning
  - Why needed here: Translate-Train is a form of transfer learning that adapts English-trained models to African languages using translated data.
  - Quick check question: What is the key difference between Translate-Train and standard fine-tuning?

## Architecture Onboarding

- Component map:
  English queries -> Machine translation (documents + MS MARCO passages) -> MLM fine-tuning (Yoruba) -> Translate-Train -> ColBERT-X indexing -> Retrieval

- Critical path:
  1. Machine translate documents and MS MARCO passages
  2. MLM fine-tune XLM-RoBERTa with Afriberta corpus (for Yoruba)
  3. Translate-Train with English queries and translated passages
  4. Index translated documents with ColBERT-X
  5. Retrieve using English queries

- Design tradeoffs:
  - Translation quality vs. coverage: Better MT improves training but may be unavailable for low-resource languages
  - Training time vs. performance: Multiple fine-tuning steps improve results but increase computational cost
  - In-domain vs. out-of-domain training: JH POLO uses target collection but requires LLM access

- Failure signatures:
  - Poor retrieval results: Likely MT quality issues or insufficient training data
  - Training instability: May indicate implementation issues (e.g., PLAID vs. ColBERT-X codebase differences)
  - Mismatched document-query pairs: Could result from poor JH POLO query generation

- First 3 experiments:
  1. Train English-ColBERT on translated documents only (baseline)
  2. Train ColBERT-X with Translate-Train (evaluate translation impact)
  3. Add MLM fine-tuning with Afriberta corpus (evaluate language model enhancement)

## Open Questions the Paper Calls Out
None

## Limitations
- Translation quality significantly impacts training effectiveness, but no explicit evaluation of translation fidelity is provided
- Mixed results from JH POLO in-domain training suggest potential issues with synthetic query generation quality
- Implementation instability differences between official and unofficial ColBERT-X runs indicate possible codebase issues

## Confidence
- High: ColBERT-X with Translate-Train outperforms English-trained variants for most languages
- Medium: MLM fine-tuning benefits are language-dependent and context-sensitive
- Low: JH POLO effectiveness conclusions due to inconsistent results

## Next Checks
1. Evaluate translation quality metrics (BLEU scores) for all African language document and passage translations to quantify MT impact
2. Compare official vs. unofficial implementation results across multiple training runs to isolate implementation stability effects
3. Conduct ablation studies on JH POLO query generation to determine if synthetic queries consistently reference document content vs. generating irrelevant queries