---
ver: rpa2
title: 'BIS: NL2SQL Service Evaluation Benchmark for Business Intelligence Scenarios'
arxiv_id: '2410.22925'
source_url: https://arxiv.org/abs/2410.22925
tags:
- query
- queries
- similarity
- nl2sql
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BIS, a novel benchmark designed to evaluate
  NL2SQL models in realistic business intelligence scenarios. Unlike existing benchmarks
  that focus on factual questions over Wikipedia, BIS addresses common BI challenges
  such as temporal queries, complex joins, and metadata mapping.
---

# BIS: NL2SQL Service Evaluation Benchmark for Business Intelligence Scenarios

## Quick Facts
- arXiv ID: 2410.22925
- Source URL: https://arxiv.org/abs/2410.22925
- Authors: Bora Caglayan; Mingxue Wang; John D. Kelleher; Shen Fei; Gui Tong; Jiandong Ding; Puchao Zhang
- Reference count: 16
- Primary result: Introduces BIS benchmark with 239 questions across nine BI-specific categories and novel evaluation metrics for NL2SQL models

## Executive Summary
This paper addresses the gap between existing NL2SQL benchmarks and real-world business intelligence requirements. While current benchmarks focus on factual questions over Wikipedia, BIS tackles BI-specific challenges including temporal queries, complex joins, and metadata mapping. The benchmark uses a realistic database schema with time series data and technical terminology, making it more representative of enterprise BI scenarios.

## Method Summary
The BIS benchmark introduces 239 questions across nine BI-specific categories designed to evaluate NL2SQL models in realistic business intelligence scenarios. The benchmark employs a realistic database schema incorporating time series data and technical terminology. Two novel evaluation metrics are proposed: semantic query similarity and result similarity, which provide nuanced assessment by measuring partial matches and structural similarities between predicted and ground truth SQL queries. These metrics offer alternatives to traditional exact match evaluation strategies.

## Key Results
- Introduces BIS benchmark specifically designed for business intelligence scenarios
- Contains 239 questions across nine BI-specific categories
- Proposes two novel evaluation metrics: semantic query similarity and result similarity
- Benchmark available at https://github.com/boracaglayan/bis-nl2sql

## Why This Works (Mechanism)
The benchmark works by addressing the fundamental mismatch between existing Wikipedia-based NL2SQL benchmarks and real business intelligence requirements. By incorporating temporal queries, complex joins, and metadata mapping challenges, BIS creates a more realistic evaluation environment. The novel evaluation metrics allow for partial credit scoring and semantic equivalence detection, which better reflect real-world BI scenarios where exact SQL matches may not be necessary if results are equivalent.

## Foundational Learning
- Temporal query handling: Needed for analyzing time series data common in BI; Quick check: Can the model correctly parse "last quarter" vs "Q3 2023"?
- Complex join operations: Required for multi-table BI analysis; Quick check: Does the model handle self-joins and outer joins correctly?
- Metadata mapping: Essential for translating business terminology to technical schema; Quick check: Can the model map "revenue" to correct table/column?
- Time series data processing: Critical for BI trend analysis; Quick check: Does the model aggregate data correctly over different time periods?
- Technical terminology: Necessary for enterprise database understanding; Quick check: Can the model interpret industry-specific terms?
- Result equivalence: Important for practical BI applications; Quick check: Do semantically equivalent queries receive similar scores?

## Architecture Onboarding
Component map: User Question -> Natural Language Processing -> SQL Generation -> Database Query -> Result Evaluation -> Semantic Similarity Scoring

Critical path: The evaluation pipeline from question to result similarity scoring is critical, as it determines how well models handle BI-specific challenges and whether the novel metrics provide meaningful differentiation.

Design tradeoffs: The benchmark trades breadth (239 questions) for depth in BI-specific scenarios, focusing on quality and realism over quantity. The novel metrics trade computational simplicity for nuanced evaluation.

Failure signatures: Models may fail on temporal reasoning, complex join operations, or metadata mapping. The semantic similarity metric may not capture all meaningful SQL variations.

First experiments:
1. Test baseline models on the full benchmark to establish performance baselines
2. Evaluate semantic query similarity metric by comparing semantically equivalent but syntactically different queries
3. Validate result similarity metric by comparing queries with identical results but different structures

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset contains only 239 questions across nine categories, potentially insufficient for comprehensive evaluation
- Database schema represents a single domain rather than multi-domain enterprise complexity
- Novel evaluation metrics require further validation across different model architectures and database schemas

## Confidence
- High confidence: The need for BI-specific NL2SQL benchmarks and inadequacy of Wikipedia-based benchmarks for enterprise scenarios
- Medium confidence: Effectiveness of proposed evaluation metrics in providing nuanced assessment
- Medium confidence: Representativeness of database schema for real-world BI scenarios

## Next Checks
1. Test the benchmark with additional NL2SQL models beyond those mentioned to establish broader performance patterns
2. Validate semantic query similarity metric by having SQL experts manually verify high-scoring query pairs are semantically equivalent
3. Expand database schema to include multiple domains and test benchmark's discriminative power across different industry contexts