---
ver: rpa2
title: Achieving Instance-dependent Sample Complexity for Constrained Markov Decision
  Process
arxiv_id: '2402.16324'
source_url: https://arxiv.org/abs/2402.16324
tags:
- optimal
- bound
- algorithm
- each
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies constrained Markov Decision Processes (CMDPs)\
  \ and proposes an algorithm that achieves logarithmic regret or O(1/\u03B5) sample\
  \ complexity for the first time. The key idea is to use a linear programming formulation\
  \ of CMDPs and solve it in an online manner, adaptively updating constraints based\
  \ on historical samples."
---

# Achieving Instance-dependent Sample Complexity for Constrained Markov Decision Process

## Quick Facts
- arXiv ID: 2402.16324
- Source URL: https://arxiv.org/abs/2402.16324
- Reference count: 40
- Primary result: First algorithm achieving logarithmic regret or O(1/ε) sample complexity for constrained MDPs

## Executive Summary
This paper studies constrained Markov Decision Processes (CMDPs) and proposes an algorithm that achieves logarithmic regret or O(1/ε) sample complexity for the first time. The key idea is to use a linear programming formulation of CMDPs and solve it in an online manner, adaptively updating constraints based on historical samples. The algorithm identifies an optimal basis of the LP and then resolves the LP with adaptive constraints, avoiding the non-degeneracy assumption common in prior work. The resulting sample complexity bound is O(1/ε) up to logarithmic factors, improving upon the O(1/ε^2) bound from previous literature.

## Method Summary
The method uses an online primal LP formulation with adaptive constraints to solve CMDPs. It first identifies an optimal LP basis by eliminating sub-optimal actions and redundant constraints using empirical estimates from samples. Then it adaptively resolves the primal LP with remaining resource capacities, updating per-step budgets based on historical information. The policy is constructed from the output occupancy measures, achieving ε-accuracy by checking both reward and constraint violation bounds.

## Key Results
- Achieves logarithmic regret O(log N) without requiring a unique optimal solution
- First algorithm to achieve O(1/ε) sample complexity for CMDPs (up to logarithmic factors)
- Improves upon prior O(1/ε^2) bounds by using adaptive constraint resolution
- Removes non-degeneracy assumption common in previous literature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive constraint updating (αn, µn) self-corrects constraint violations and reduces regret to O(log N)
- Mechanism: The algorithm dynamically adjusts remaining resource capacities after each decision epoch based on observed consumption. When realized consumption is below target, the per-step budget increases, encouraging more consumption in future steps; if over-consumption occurs, future per-step budgets shrink, creating a feedback loop that drives realized consumption toward targets.
- Core assumption: The empirical estimates of C(J*, I*) and B(:, I*) converge sufficiently fast so that the remaining bias is O(1/√n), but the adaptive feedback dominates and corrects this drift over time.
- Evidence anchors: [abstract] "adaptive remaining resource capacities", [section] "the key is to show that the stochastic process α̃k(n) and µ̃s behave as a sub-martingale"
- Break condition: If the estimation error is too large (n too small), the adaptive feedback cannot correct, leading to persistent constraint violations and O(√N) regret.

### Mechanism 2
- Claim: Identifying a fixed optimal basis (I*, J*) and always resolving within that basis avoids non-degeneracy assumptions and guarantees O(log N) regret
- Mechanism: By