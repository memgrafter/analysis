---
ver: rpa2
title: A review on the use of large language models as virtual tutors
arxiv_id: '2405.11983'
source_url: https://arxiv.org/abs/2405.11983
tags:
- https
- education
- learning
- review
- virtual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews applications of Large Language Models (LLMs)
  in education, focusing on virtual tutors for question generation and assessment.
  It identifies 29 relevant works, with BERT and GPT-3 being the most popular models.
---

# A review on the use of large language models as virtual tutors

## Quick Facts
- arXiv ID: 2405.11983
- Source URL: https://arxiv.org/abs/2405.11983
- Reference count: 31
- Review identifies 29 relevant works on LLM applications in education, highlighting challenges in reproducibility and transparency

## Executive Summary
This paper reviews applications of Large Language Models (LLMs) in education, focusing on virtual tutors for question generation and assessment. The authors systematically analyzed 29 papers published before March 2023, finding that BERT and GPT-3 are the most popular architectures used. Question generation and virtual assistants are the most common applications identified. While some studies report promising results, such as 75% useful questions generated and 96.5% code correction rates, the review highlights significant concerns about reproducibility due to limited code and data availability.

## Method Summary
The authors conducted a systematic literature review searching databases including ACM Digital Library, IEEE Xplore, and arXiv for papers published before March 2023. They identified 29 relevant papers that specifically addressed LLMs in educational contexts, particularly for virtual tutoring applications. The review categorized papers by architecture type, application area, and reported outcomes. Data extraction focused on model types, tasks performed, evaluation metrics, and reproducibility factors such as code and dataset availability.

## Key Results
- BERT and GPT-3 are the most commonly used LLM architectures in educational applications
- Question generation and virtual assistant applications dominate the field
- Only 3 out of 29 papers made their code and data publicly available, limiting reproducibility
- Reported performance includes 75% useful questions generated and 96.5% code correction rate, though these cannot be independently verified

## Why This Works (Mechanism)
LLMs work effectively as virtual tutors because they can process and generate natural language at scale, enabling them to understand student queries, generate educational content, and provide personalized feedback. Their ability to capture contextual relationships in text allows them to assess student work, identify misconceptions, and adapt explanations to individual learning needs. The large parameter counts in these models enable them to handle diverse educational domains and maintain coherent, context-aware conversations.

## Foundational Learning
- Transformer architecture: why needed - enables parallel processing and long-range dependencies; quick check - verify attention mechanism implementation
- Fine-tuning techniques: why needed - adapts pre-trained models to specific educational domains; quick check - confirm learning rate scheduling
- Prompt engineering: why needed - guides model behavior for educational tasks; quick check - test different prompt formats for consistency
- Evaluation metrics for educational AI: why needed - measures pedagogical effectiveness beyond standard NLP metrics; quick check - validate metrics against human expert ratings
- Educational domain adaptation: why needed - ensures models understand subject-specific terminology and concepts; quick check - test on domain-specific benchmarks

## Architecture Onboarding
Component map: User Interface -> LLM API/Instance -> Evaluation Module -> Database/Storage
Critical path: Student query → Model inference → Response generation → Assessment/feedback → Learning analytics storage
Design tradeoffs: Performance vs. interpretability (complex models harder to debug), accuracy vs. fairness (bias in educational outcomes), customization vs. generalizability (domain-specific vs. universal models)
Failure signatures: Model hallucinations producing incorrect information, bias amplification affecting certain student groups, overfitting to training data patterns
3 first experiments:
1. Test question generation accuracy on sample educational texts with expert evaluation
2. Evaluate code correction capabilities on programming exercises with ground truth solutions
3. Assess fairness across demographic groups using standardized bias detection tools

## Open Questions the Paper Calls Out
None

## Limitations
- Major concerns about reproducibility due to limited code and data availability (only 3/29 papers shared resources)
- Findings based on literature pre-March 2023, missing more recent developments and newer models
- Focus on English-language papers limits generalizability to other linguistic and cultural contexts
- Performance metrics reported cannot be independently verified

## Confidence
- High confidence: LLMs are being applied to education for question generation and virtual assistants
- Medium confidence: BERT and GPT-3 are the most commonly used architectures in educational applications
- Low confidence: Specific performance metrics and success rates due to reproducibility issues

## Next Checks
1. Replicate key experiments from the most promising studies with publicly available code and data
2. Conduct systematic searches for more recent literature (post-March 2023) to update the review
3. Test the generalizability of reported findings across different educational domains and language contexts