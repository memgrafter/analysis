---
ver: rpa2
title: Investigating Context Effects in Similarity Judgements in Large Language Models
arxiv_id: '2408.10711'
source_url: https://arxiv.org/abs/2408.10711
tags:
- similarity
- llms
- judgements
- country
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates context effects in similarity judgments
  by large language models (LLMs), specifically focusing on order bias. The authors
  replicate Tversky and Gati's study on human similarity judgments, which found evidence
  of order effects when rating the similarity between pairs of countries.
---

# Investigating Context Effects in Similarity Judgements in Large Language Models

## Quick Facts
- arXiv ID: 2408.10711
- Source URL: https://arxiv.org/abs/2408.10711
- Reference count: 29
- Only three model-temperature-prompt combinations demonstrate statistically significant order effects that align with human judgments

## Executive Summary
This paper investigates context effects in similarity judgments by large language models (LLMs), specifically focusing on order bias. The authors replicate Tversky and Gati's study on human similarity judgments, which found evidence of order effects when rating the similarity between pairs of countries. They test eight different LLMs across various temperature settings and prompt styles to determine if LLMs exhibit similar order effects. The study finds that while all models show some asymmetry in similarity judgments, only three specific combinations (Mistral 7B with temperature 1.0 and 1.5, and Llama3 8B with temperature 0.001 and 0.5) demonstrate statistically significant order effects that align with human judgments. Notably, GPT-4 and Llama3 8B show perfectly symmetric judgments in dual prompt settings, suggesting they can be aligned with human judgments through appropriate tuning.

## Method Summary
The study replicates Tversky and Gati's experiment on human similarity judgments using eight different LLMs (Mistral 7B, Llama2 variants, Llama3 variants, GPT-3.5, GPT-4) with various temperature settings (0.001, 0.5, 1.0, 1.5). Three prompt styles are used for both single pairs (SSD, SST, SSA) and dual pairs (DSD, DST, DSA). For each model-temperature-prompt combination, similarity scores are generated for 21 country pairs in both orders, and one-sided paired t-tests determine statistical significance of order effects. The dual prompt setting tests symmetry by including both orders in the same prompt.

## Key Results
- Only three model-temperature-prompt combinations show statistically significant order effects that align with human judgments
- GPT-4 and Llama3 8B demonstrate perfectly symmetric judgments in dual prompt settings
- All models exhibit some asymmetry in similarity judgments, but most do not align with human patterns
- Temperature settings significantly influence whether order effects emerge in LLM outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can exhibit order bias in similarity judgments when the less prominent country is placed first in the pair.
- Mechanism: When the less prominent country is placed first, LLMs focus on overlapping features (political system, culture, proximity), increasing similarity scores. When the more prominent country is placed first, LLMs consider additional features (size, economy), reducing overlap and lowering similarity scores.
- Core assumption: LLMs process prompts sequentially and weigh features based on contextual prominence.
- Evidence anchors:
  - [abstract] "Only three model-temperature-prompt combinations (Mistral 7B with temperature 1.0 and 1.5, and Llama3 8B with temperature 0.001 and 0.5) demonstrate statistically significant order effects that align with human judgments."
  - [section] "Tversky and Gati hypothesise that if ð‘  (ð‘Ž, ð‘) and ð‘  (ð‘, ð‘Ž) are the similarity of a pair of subject and referent in the two orders, then ð‘  (ð‘Ž, ð‘) > ð‘  (ð‘, ð‘Ž)."
- Break condition: If LLMs use attention mechanisms that normalize feature weights regardless of order, or if prompts explicitly separate entities to eliminate context effects.

### Mechanism 2
- Claim: Temperature settings influence whether LLMs show order bias in similarity judgments.
- Mechanism: Higher temperature values introduce more variability in responses, potentially revealing context-sensitive patterns similar to human judgments. Lower temperatures produce more deterministic outputs that may suppress order effects.
- Evidence anchors:
  - [abstract] "Only three model-temperature-prompt combinations (Mistral 7B with temperature 1.0 and 1.5, and Llama3 8B with temperature 0.001 and 0.5) demonstrate statistically significant order effects."
  - [section] "We utilise different temperature settings and different variants of prompts in terms of the text."
- Break condition: If temperature only affects randomness but not underlying feature weighting mechanisms, or if models have internal normalization that overrides temperature effects.

### Mechanism 3
- Claim: Prompt wording significantly affects whether LLMs show symmetric or asymmetric similarity judgments.
- Mechanism: Different phrasings ("assess the degree to which A is similar to B" vs "how similar are A and B") create different contextual frameworks that influence how LLMs weigh entity features and determine similarity.
- Evidence anchors:
  - [section] "It is well-known that LLM outputs, even at zero temperature, are sensitive to choice of words in a prompt."
  - [section] "We also conduct a novel variant of the experiment by prompting the LLMs with both orders of countries in the same prompt."
- Break condition: If LLMs have learned that similarity is inherently symmetric regardless of prompt phrasing, or if the model's training data normalized against prompt sensitivity.

## Foundational Learning

- Concept: Order effects in human similarity judgments
  - Why needed here: Understanding how humans show context-dependent similarity judgments provides the baseline for comparing LLM behavior.
  - Quick check question: Why do humans give different similarity ratings when the order of countries in a pair is reversed?

- Concept: Temperature in LLM sampling
  - Why needed here: Temperature controls output variability and affects whether context-sensitive patterns emerge in LLM responses.
  - Quick check question: What happens to LLM output consistency when temperature is set to 0 vs 1.0?

- Concept: Prompt sensitivity in LLMs
  - Why needed here: Different phrasings can significantly alter LLM outputs even with identical semantic content.
  - Quick check question: How might changing "assess the degree to which A is similar to B" to "how similar are A and B" affect the LLM's feature weighting?

## Architecture Onboarding

- Component map: Input layer (country pair ordering, prompt phrasing) -> Processing layer (attention mechanisms, feature extraction) -> Output layer (similarity score generation, 0-20 scale) -> Temperature control (sampling strategy)

- Critical path: Prompt formatting and country pair ordering -> Context-sensitive feature extraction -> Similarity score calculation -> Temperature-controlled sampling -> Output formatting

- Design tradeoffs:
  - Deterministic vs. stochastic outputs: Lower temperature gives consistent but potentially less human-like responses
  - Prompt sensitivity vs. robustness: More sensitive prompts may better capture context effects but are less reliable
  - Feature weighting vs. generalization: Context-specific feature weighting may improve alignment but reduce cross-domain applicability

- Failure signatures:
  - Symmetric outputs regardless of order (missing human-like bias)
  - Temperature-independent behavior (context effects not properly modulated)
  - Prompt-insensitive responses (not capturing context-dependent reasoning)

- First 3 experiments:
  1. Test Mistral 7B with temperature 1.0 using both single and dual prompt styles to confirm order effect presence/absence
  2. Compare GPT-4 at temperature 0.5 with Llama3 8B at temperature 0.001 to understand temperature-prompt interaction effects
  3. Create a mixed-order prompt with country pairs in both orders and analyze if symmetry emerges across all models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors do LLMs use to evaluate similarity between entities when exhibiting order effects, and how do these differ from human reasoning processes?
- Basis in paper: [explicit] The paper mentions that in the Tversky and Gati study, different orders of countries led to different factors being considered (e.g., political system, culture, proximity vs. size, economy). It suggests comparing this with LLM reasoning through Chain of Thought prompting.
- Why unresolved: The paper acknowledges this as future work but doesn't investigate the actual reasoning processes LLMs use when showing order effects.
- What evidence would resolve it: Analysis of LLM Chain of Thought outputs showing the specific factors considered for each ordering, compared with human participant explanations from the original study.

### Open Question 2
- Question: How do different temperature settings interact with prompt styles to produce statistically significant order effects in similarity judgments?
- Basis in paper: [explicit] The paper finds that only certain model-temperature combinations show significant order effects, but doesn't deeply explore why these specific combinations work while others don't.
- Why unresolved: The paper identifies which settings work but doesn't explain the underlying mechanisms of why temperature and prompt style interact in this way.
- What evidence would resolve it: Systematic analysis of how temperature affects the probability distribution of generated similarity scores across different prompt styles, possibly through ablation studies.

### Open Question 3
- Question: Are order effects in LLM similarity judgments consistent across different domains beyond country pairs, such as abstract concepts or everyday objects?
- Basis in paper: [inferred] The paper focuses exclusively on country pairs from the Tversky and Gati study, but doesn't test whether similar effects appear in other domains.
- Why unresolved: The study is limited to one specific dataset (country pairs) and doesn't explore generalizability to other types of similarity judgments.
- What evidence would resolve it: Replication of the experiments with different entity types (e.g., animals, fruits, abstract concepts) to test whether order effects persist across domains.

### Open Question 4
- Question: Can the alignment of LLM similarity judgments with human biases be tuned for specific applications where order effects are either beneficial or detrimental?
- Basis in paper: [explicit] The paper discusses implications for applications where order bias may be beneficial (e.g., dating, mental health support) or detrimental (e.g., e-commerce), but doesn't explore how to tune this behavior.
- Why unresolved: While the paper identifies which models can be aligned with human judgments, it doesn't investigate methods for deliberately controlling this alignment.
- What evidence would resolve it: Development and validation of techniques to deliberately induce or remove order effects in LLMs for specific use cases.

## Limitations

- Only three out of twenty-four model-temperature-prompt combinations showed statistically significant order effects that align with human judgments
- The specific country pairs used in the experiment are not detailed, making replication difficult
- The relationship between temperature settings and order effect emergence is not clearly explained
- The study does not investigate whether context effects persist across different domains beyond country similarity judgments

## Confidence

**High Confidence:** The finding that most model-temperature-prompt combinations do not show statistically significant order effects aligns with the data and represents a robust negative result. The methodology for detecting order effects using paired t-tests is standard and well-executed.

**Medium Confidence:** The claim that only three specific combinations demonstrate significant order effects that align with human judgments is supported by the statistical analysis, though the small number of successful cases limits generalizability. The observation about perfect symmetry in GPT-4 and Llama3 8B dual prompt settings is clearly stated but requires further investigation to understand its implications.

**Low Confidence:** The proposed mechanism explaining why order effects occur (less prominent country first leads to feature overlap focus) remains speculative without direct evidence about how LLMs process and weigh features. The relationship between temperature settings and order effect emergence is not clearly explained.

## Next Checks

1. **Cross-Domain Replication:** Test the same model-temperature-prompt combinations using similarity judgments in non-geopolitical domains (e.g., animal species, product categories, or abstract concepts) to determine if order effects are domain-specific or represent a general LLM behavior pattern.

2. **Attention Pattern Analysis:** Use attention visualization tools to examine whether LLMs process country pairs differently based on order, specifically looking for patterns in how features are weighted when the less prominent country appears first versus second in the prompt.

3. **Controlled Temperature Sweep:** Conduct a finer-grained temperature sweep (e.g., 0.1 increments from 0.1 to 1.5) for the Mistral 7B and Llama3 8B models to identify the precise temperature thresholds where order effects emerge or disappear, helping to clarify the relationship between stochasticity and context sensitivity.