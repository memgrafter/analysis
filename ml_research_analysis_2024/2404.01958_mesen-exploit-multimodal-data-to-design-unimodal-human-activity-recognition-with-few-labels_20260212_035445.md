---
ver: rpa2
title: 'MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition
  with Few Labels'
arxiv_id: '2404.01958'
source_url: https://arxiv.org/abs/2404.01958
tags:
- data
- multimodal
- mesen
- unimodal
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of human activity recognition
  (HAR) with limited labeled data and modality constraints. The core method, MESEN,
  is a multimodal-empowered unimodal sensing framework that exploits unlabeled multimodal
  data to enhance unimodal HAR performance.
---

# MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels

## Quick Facts
- arXiv ID: 2404.01958
- Source URL: https://arxiv.org/abs/2404.01958
- Reference count: 40
- This paper presents MESEN, a multimodal-empowered unimodal sensing framework that significantly outperforms state-of-the-art baselines, achieving an average accuracy of 65.7% and F1-score of 63.0% on unimodal HAR tasks.

## Executive Summary
This paper addresses the challenge of human activity recognition (HAR) with limited labeled data and modality constraints. MESEN is a multimodal-empowered unimodal sensing framework that exploits unlabeled multimodal data to enhance unimodal HAR performance. The framework employs a multi-task mechanism during multimodal-aided pre-training, integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning. Extensive experiments on eight multimodal datasets demonstrate that MESEN significantly outperforms state-of-the-art baselines in unimodal HAR scenarios.

## Method Summary
MESEN is a multimodal-empowered unimodal sensing framework that leverages unlabeled multimodal data to improve unimodal HAR performance with few labeled samples. The method consists of two main stages: multimodal-aided pre-training and unimodal fine-tuning. During pre-training, MESEN uses cross-modal feature contrastive learning to capture inter-modality correlations while maintaining distinct intra-modality spaces, combined with multimodal pseudo-classification aligning to ensure paired samples are classified into the same pseudo-class. The fine-tuning stage employs layer-aware fine-tuning with regularization to preserve knowledge from pre-training while adapting to few labeled samples, using L2 regularization on encoder layers and classifier parameters.

## Key Results
- MESEN achieves an average accuracy of 65.7% and F1-score of 63.0% on unimodal HAR tasks
- Significantly outperforms state-of-the-art baselines on eight multimodal datasets
- Effectively handles scenarios with limited labeled data and modality constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal feature contrastive learning captures inter-modality correlations while maintaining distinct intra-modality spaces.
- Mechanism: The method computes one positive pair and separate negative sets for inter- and intra-modality features, ignoring intra-modality negatives to preserve modality-specific spaces.
- Core assumption: Paired multimodal samples contain correlated information that benefits unimodal feature extraction when exploited during pre-training.
- Evidence anchors:
  - [abstract] The proposed mechanism integrates cross-modal feature contrastive learning and multimodal pseudo-classification aligning.
  - [section] "To highlight significant regions in the input sensor data that contribute to the final prediction in multimodal fusion, we apply Gradient-weighted Class Activation Mapping (Grad-CAM) [32] to the last convolutional layer of each modality encoder individually after supervised multimodal fusion training."
  - [corpus] Weak/absent: Corpus neighbors focus on cross-modal transfer and sensor fusion but don't directly address contrastive learning with intra-/inter-modality separation.
- Break condition: If modality differences are not preserved during contrastive learning, unimodal features may lose modality-specific discriminative properties.

### Mechanism 2
- Claim: Multimodal pseudo-classification aligning ensures that paired samples are classified into the same pseudo-class while non-paired samples are separated.
- Mechanism: The method applies pseudo-classification to modality features and maximizes similarity between pseudo-classes of paired samples while maximizing dissimilarity for non-paired samples.
- Core assumption: Pseudo-classification alignment improves the correspondence between unimodal predictions, leading to better fusion results and unimodal feature extraction.
- Evidence anchors:
  - [abstract] The proposed mechanism integrating cross-modal feature contrastive learning and multimodal pseudo-classification aligning.
  - [section] "The alignment measures the degree of similarity between the unimodal prediction probabilities of paired samples. As discussed in ¬ß3, the fusion of unimodal predicted probabilities can guide unimodal feature extraction."
  - [corpus] Weak/absent: Corpus neighbors discuss cross-modal transfer and fusion but don't specifically address pseudo-classification alignment for unimodal enhancement.
- Break condition: If pseudo-classification fails to align paired samples or incorrectly aligns non-paired samples, unimodal features may not benefit from multimodal guidance.

### Mechanism 3
- Claim: Layer-aware fine-tuning with regularization preserves knowledge from pre-training while adapting to few labeled samples.
- Mechanism: The method applies L2 regularization to encoder layers and classifier parameters during fine-tuning, with adjustable weights to control knowledge retention.
- Core assumption: Regularization prevents overfitting when fine-tuning with limited labeled data while retaining effective features learned during pre-training.
- Evidence anchors:
  - [abstract] Subsequently, MESEN can adapt to downstream unimodal HAR with only a few labeled samples.
  - [section] "To mitigate these issues, we employ a layer-aware fine-tuning mechanism with the regularization loss ùêøFR. During fine-tuning, the model consists of two parts: the pre-trained encoder which is to be fine-tuned, and the classifier head which needs to be trained from scratch."
  - [corpus] Weak/absent: Corpus neighbors don't discuss layer-aware fine-tuning or regularization for few-shot adaptation.
- Break condition: If regularization is too strong, fine-tuning may not adapt sufficiently to the specific unimodal task; if too weak, overfitting may occur.

## Foundational Learning

- Concept: Contrastive learning and its variants
  - Why needed here: MESEN uses cross-modal feature contrastive learning to capture correlations between paired multimodal samples while preserving modality differences.
  - Quick check question: What is the difference between standard contrastive learning and the cross-modal approach used in MESEN?
- Concept: Pseudo-classification and its role in self-supervised learning
  - Why needed here: MESEN employs multimodal pseudo-classification aligning to ensure paired samples are classified similarly, providing a prompt for downstream recognition.
  - Quick check question: How does pseudo-classification alignment improve unimodal feature extraction compared to standard contrastive learning?
- Concept: Layer-aware fine-tuning with regularization
  - Why needed here: MESEN uses this mechanism to prevent overfitting when adapting pre-trained models to few labeled samples while preserving learned knowledge.
  - Quick check question: Why is L2 regularization applied to both encoder layers and classifier parameters during fine-tuning?

## Architecture Onboarding

- Component map: Unlabeled multimodal data ‚Üí cross-modal feature contrastive learning + multimodal pseudo-classification aligning ‚Üí pre-trained unimodal encoder ‚Üí few labeled unimodal data ‚Üí layer-aware fine-tuning with regularization ‚Üí unimodal HAR
- Critical path:
  1. Pre-train modality encoders using unlabeled multimodal data with cross-modal contrastive loss and pseudo-classification loss
  2. Extract pre-trained unimodal encoder for the target modality
  3. Fine-tune with few labeled samples using classification loss + regularization loss
- Design tradeoffs:
  - More modalities during pre-training improve performance but increase computational cost
  - Stronger regularization preserves knowledge but may limit adaptation to specific tasks
  - Pseudo-classification alignment requires accessible activity category information
- Failure signatures:
  - Poor unimodal feature clustering after pre-training indicates ineffective contrastive learning
  - Overfitting during fine-tuning suggests insufficient regularization or too few labeled samples
  - Performance degradation with more modalities may indicate modality-specific noise or interference
- First 3 experiments:
  1. Verify cross-modal contrastive learning: Compare unimodal features after pre-training with and without contrastive learning on a simple dataset
  2. Test pseudo-classification alignment: Measure alignment between pseudo-classes of paired samples before and after applying the alignment loss
  3. Evaluate layer-aware fine-tuning: Compare fine-tuning performance with different regularization strengths on a small labeled dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MESEN's performance scale with increasing numbers of modalities beyond the datasets tested?
- Basis in paper: [explicit] The paper discusses scalability concerns, noting that increasing modality pairs introduce extra training costs, but does not empirically test performance with more than the available modalities in their datasets.
- Why unresolved: The evaluation is limited to existing multimodal datasets, and the theoretical complexity analysis doesn't address practical performance degradation or improvements with additional modalities.
- What evidence would resolve it: Empirical testing of MESEN on synthetic or real datasets with 4+ modalities, measuring both accuracy and computational efficiency.

### Open Question 2
- Question: Can MESEN's pre-training stage be effectively adapted for multimodal inference scenarios where multiple modalities are available during deployment?
- Basis in paper: [explicit] The discussion section mentions that MESEN can be adapted for multimodal inference through concatenation of features, but notes this wasn't the primary design focus and might not capture full potential compared to Cosmo.
- Why unresolved: The paper only mentions this possibility theoretically without implementing or evaluating the adapted approach for multimodal inference.
- What evidence would resolve it: Implementation and evaluation of MESEN with multimodal inference on datasets where multiple modalities are available during deployment, comparing performance to Cosmo.

### Open Question 3
- Question: How would incorporating physics-informed data augmentation (like UniHAR's approach) affect MESEN's cross-dataset performance?
- Basis in paper: [explicit] The discussion mentions that cross-dataset performance is affected by domain discrepancies and that UniHAR's physics-informed data augmentation could address this, but doesn't implement or test this combination.
- Why unresolved: The paper only tests cross-dataset performance without attempting to mitigate domain shift issues through data augmentation techniques.
- What evidence would resolve it: Implementation of MESEN with physics-informed data augmentation applied during pre-training, followed by cross-dataset evaluation comparing performance to baseline MESEN.

## Limitations

- The framework's scalability with increasing numbers of modalities is not empirically tested, raising concerns about performance degradation or computational inefficiency with more than the available modalities.
- The paper lacks detailed implementation specifications for key mechanisms, particularly the cross-modal contrastive learning parameters and regularization strategy, making exact reproduction challenging.
- Limited analysis of failure modes under extreme conditions (very few labels, highly imbalanced datasets, or highly asymmetric modality distributions) leaves uncertainty about robustness.

## Confidence

**High confidence**: The overall framework design and experimental methodology are well-established. The use of standard metrics (accuracy, F1-score) and the comparison with multiple baselines on eight diverse datasets provides strong empirical support for the general approach.

**Medium confidence**: The core mechanisms (contrastive learning, pseudo-classification alignment, and layer-aware fine-tuning) are theoretically justified but lack detailed implementation specifications that would be necessary for complete reproduction. The paper provides sufficient detail for understanding the concepts but not for exact replication.

**Low confidence**: The scalability analysis and failure mode characterization are minimal. The paper doesn't adequately address how the framework performs under extreme conditions (very few labels, many modalities, or highly imbalanced datasets).

## Next Checks

1. **Contrastive Learning Sensitivity Analysis**: Systematically vary the temperature parameters and negative sample ratios in the cross-modal contrastive learning component to determine their impact on unimodal feature quality and final HAR performance.

2. **Pseudo-classification Alignment Robustness**: Test the framework's performance when modality pairs have varying levels of correlation (highly correlated vs. weakly correlated) to assess whether the pseudo-classification alignment remains effective across different modality relationships.

3. **Fine-tuning Regularization Trade-off**: Conduct experiments varying ùúÜPR and ùúÜFR across multiple orders of magnitude to identify optimal regularization levels and characterize the trade-off between overfitting prevention and adaptation capability.