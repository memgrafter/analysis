---
ver: rpa2
title: 'MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline'
arxiv_id: '2401.08190'
source_url: https://arxiv.org/abs/2401.08190
tags:
- math
- code
- solution
- dataset
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MARIO, a math reasoning pipeline integrating
  text analysis with Python code execution. The approach addresses limitations of
  pure code-centric solutions by incorporating common-sense reasoning through textual
  explanations alongside precise numerical computations.
---

# MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline

## Quick Facts
- arXiv ID: 2401.08190
- Source URL: https://arxiv.org/abs/2401.08190
- Authors: Minpeng Liao; Wei Luo; Chengxi Li; Jing Wu; Kai Fan
- Reference count: 40
- One-line primary result: MARIO achieves 47.0% accuracy on MATH and 70.1% on GSM8K, setting new state-of-the-art results for 7B models

## Executive Summary
This paper presents MARIO, a math reasoning pipeline integrating text analysis with Python code execution. The approach addresses limitations of pure code-centric solutions by incorporating common-sense reasoning through textual explanations alongside precise numerical computations. The dataset combines GSM8K and MATH problems, enhanced with GPT-4 annotations, human review, and self-training, resulting in 82K high-quality training examples. A 7B model fine-tuned on this dataset achieves 47.0% accuracy on MATH and 70.1% on GSM8K, setting new state-of-the-art results for models of this size. The method also introduces an outcome value model trained with LoRA for solution verification, improving performance through outlier-free selection.

## Method Summary
MARIO uses a three-stage fine-tuning pipeline on Llemma models: Continual Pre-training on math/coding corpora, Supervised Fine-tuning with HTML-formatted solutions combining text and code, and Multi-Task LoRA fine-tuning for outcome value modeling. The approach integrates text analysis with Python code execution, where textual reasoning handles common-sense constraints while code handles precise computation. During inference, solutions are generated with up to 5 code snippets for GSM8K and 8 for MATH problems, then evaluated by an outcome value model that enables outlier-free selection for improved accuracy.

## Key Results
- 7B model achieves 47.0% accuracy on MATH and 70.1% on GSM8K, setting new SOTA for model size
- HTML-like formatting reduces initial training loss by 20% compared to REACT format
- Outcome Value Model consistently improves performance across multiple datasets through outlier detection
- Self-verification mechanism detects and filters solutions with negative or fractional quantities in intermediate steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HTML-like formatting reduces initial training loss by 20% compared to REACT string format
- Mechanism: HTML-like formatting aligns better with pre-training corpus distribution, reducing log-likelihood from the start
- Core assumption: Pre-trained LLMs have seen HTML-like structures more frequently than REACT-style key-value strings
- Evidence anchors:
  - [section]: "we noted a substantial decrease in log-likelihood. In our data pipeline of Figure 2, when training the Llemma-34B on the 26.9K dataset with different formats, this reduction was evident from the initial loss observed in the first iteration"
  - [corpus]: Weak evidence - no explicit comparison of corpus statistics provided
- Break condition: If pre-training corpus changes significantly or HTML-like formats become less common

### Mechanism 2
- Claim: Outcome Value Model (OVM) improves solution quality by outlier detection and re-ranking
- Mechanism: OVM predicts likelihood of final answer being correct, enabling outlier-free selection algorithm that filters inconsistent solutions
- Core assumption: Multiple sampled solutions will have distinct quality levels, and OVM can accurately distinguish correct from incorrect answers
- Evidence anchors:
  - [abstract]: "Our experiments demonstrate that our approach can significantly enhance performance on math reasoning tasks"
  - [section]: "Our findings indicate that the outlier-free OVM algorithm can significantly enhance the results compared to majority voting"
  - [corpus]: Strong evidence - Table 3 shows OVM consistently improves performance across multiple datasets
- Break condition: When OVM accuracy drops below threshold needed for meaningful selection, or when all sampled solutions are incorrect

### Mechanism 3
- Claim: Text analysis + code snippets integration addresses limitations of pure code-centric approaches
- Mechanism: Textual reasoning handles common-sense constraints while code handles precise computation, creating complementary solution paths
- Core assumption: Math word problems require both logical reasoning about real-world constraints and exact numerical computation
- Evidence anchors:
  - [abstract]: "The approach addresses limitations of pure code-centric solutions by incorporating common-sense reasoning through textual explanations alongside precise numerical computations"
  - [section]: "Our approach incorporates a textual analysis that ensures the results derived from code execution are consistent with the constraints of the physical world"
  - [corpus]: Strong evidence - Multiple examples in case study show text analysis catching unreasonable intermediate results
- Break condition: When problems are purely symbolic with no real-world constraints, or when code can handle all reasoning independently

## Foundational Learning

- Concept: Chain-of-Thought reasoning
  - Why needed here: GSM8K and MATH problems require multi-step reasoning where intermediate steps build toward final answer
  - Quick check question: Can you trace through a simple problem and identify each reasoning step before coding?

- Concept: Python code execution for mathematical operations
  - Why needed here: LLMs struggle with exact calculations; code interpreter provides precise symbolic and numerical computation
  - Quick check question: What Python libraries would you use for symbolic differentiation vs numerical equation solving?

- Concept: Data format alignment with pre-training corpus
  - Why needed here: Training efficiency depends on how well fine-tuning data matches distribution seen during pre-training
  - Quick check question: How would you analyze token distribution differences between REACT and HTML formats?

## Architecture Onboarding

- Component map:
  GSM8K/MATH datasets → GPT-4 annotation pipeline → Human review → HTML formatting → Llemma fine-tuning
  Code interpreter plugin → Solution generation → OVM evaluation → Outlier detection → Final answer selection
  MetaMath augmentation → Teacher model sampling → Self-training → Value model fine-tuning

- Critical path:
  1. Data generation (GPT-4 + human review)
  2. HTML re-formatting
  3. Llemma supervised fine-tuning (SFT)
  4. Value model fine-tuning (OVM)
  5. Inference with outlier detection

- Design tradeoffs:
  - Pure code vs hybrid text+code: Code-only misses common sense, text-only lacks precision
  - Manual vs automated review: Manual ensures quality but scales poorly; automated scales but may miss edge cases
  - HTML vs REACT format: HTML reduces training loss but may limit flexibility in solution representation

- Failure signatures:
  - Training divergence: Likely format mismatch or learning rate too high
  - Low OVM accuracy: May indicate insufficient training data diversity or model capacity issues
  - Negative quantities in solutions: Text analysis failing to catch physical constraints

- First 3 experiments:
  1. Run single problem through pipeline with verbose output to trace all intermediate steps
  2. Compare training loss curves for HTML vs REACT format on small subset
  3. Test OVM accuracy on held-out validation set with known correct/incorrect solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HTML-like data formatting compare to REACT format in terms of long-term training stability and convergence for mathematical reasoning tasks?
- Basis in paper: [explicit] The paper mentions HTML-like formatting results in 20% lower initial loss compared to REACT format, but only marginal performance improvement (1%) after 3 epochs on MATH test set.
- Why unresolved: The authors only conducted a preliminary comparison and hypothesize that careful hyperparameter tuning could improve HTML format performance, but didn't explore this systematically.
- What evidence would resolve it: A comprehensive ablation study comparing different data formats across multiple model sizes, training durations, and hyperparameter configurations to determine optimal formatting for mathematical reasoning tasks.

### Open Question 2
- Question: What is the optimal balance between text analysis and code snippets for mathematical reasoning problems of varying complexity?
- Basis in paper: [inferred] The paper demonstrates success with integrated text+code solutions but doesn't systematically investigate the relative importance or optimal ratio of textual vs computational components across different problem types.
- Why unresolved: While the paper shows that their hybrid approach works well, they don't analyze which types of problems benefit more from text versus code, or how to optimally allocate reasoning steps between the two modalities.
- What evidence would resolve it: Controlled experiments varying the proportion of text vs code in solutions across different mathematical domains (algebra, geometry, word problems) to identify optimal decomposition strategies.

### Open Question 3
- Question: How does the self-verification mechanism perform when extended to more complex mathematical domains beyond GSM8K and MATH?
- Basis in paper: [explicit] The paper describes a self-verification process that detects inconsistent or impossible intermediate results and can return "None" for invalid solutions, but only demonstrates this on relatively simple problems.
- Why unresolved: The paper only shows verification on grade-school level problems where inconsistencies are obvious (negative quantities, fractional people). The scalability and effectiveness of this approach for higher mathematics remains unknown.
- What evidence would resolve it: Evaluation of the verification system on advanced mathematical domains (calculus, abstract algebra, real analysis) where errors might be more subtle and require sophisticated validation criteria.

## Limitations
- Format comparison limited to single model and dataset subset, making generalization uncertain
- OVM effectiveness depends heavily on having diverse solution samples, not thoroughly explored when all samples are incorrect
- Three-stage fine-tuning pipeline lacks detailed hyperparameter specifications for exact replication

## Confidence
- High Confidence: The hybrid text+code approach addressing limitations of pure code-centric solutions
- Medium Confidence: The OVM improvement claims - Table 3 shows consistent improvements but magnitude depends on sample diversity
- Medium Confidence: HTML format reducing training loss - mechanism is plausible but lacks comprehensive ablation studies

## Next Checks
1. **Format Ablation Study**: Replicate the training loss comparison between HTML and REACT formats using the same 26.9K dataset subset, measuring initial loss values across 3-5 different random seeds to establish statistical significance of the 20% reduction claim.

2. **OVM Robustness Testing**: Create synthetic test cases where all sampled solutions are incorrect (e.g., by systematically introducing logical errors) and measure OVM's behavior - does it still select a "best" answer, or can it recognize when no solution is reliable?

3. **Out-of-Domain Generalization**: Evaluate the trained MARIO model on GaoKao2023-Math-En and other out-of-domain datasets using both majority voting and OVM selection methods, comparing performance degradation patterns to identify whether the OVM provides consistent benefits across domain shifts.