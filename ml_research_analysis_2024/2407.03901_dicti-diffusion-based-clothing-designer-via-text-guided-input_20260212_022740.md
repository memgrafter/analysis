---
ver: rpa2
title: 'DiCTI: Diffusion-based Clothing Designer via Text-guided Input'
arxiv_id: '2407.03901'
source_url: https://arxiv.org/abs/2407.03901
tags:
- image
- dicti
- text
- garment
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DiCTI, a text-guided diffusion-based clothing
  designer that generates high-resolution, photorealistic images of people wearing
  garments described by text prompts. The method formulates garment synthesis as an
  inpainting task, leveraging a pre-trained latent diffusion model within a text-conditioned
  inpainting framework.
---

# DiCTI: Diffusion-based Clothing Designer via Text-guided Input

## Quick Facts
- **arXiv ID:** 2407.03901
- **Source URL:** https://arxiv.org/abs/2407.03901
- **Reference count:** 34
- **Primary result:** DiCTI achieves 27.48 CLIP-S score for text prompt adherence, outperforming FICE (22.04) while maintaining photorealistic quality (KID: 0.066 vs 0.082)

## Executive Summary
DiCTI presents a novel diffusion-based approach for text-guided clothing synthesis that generates photorealistic images of people wearing garments described by text prompts. The method formulates garment synthesis as an inpainting task, leveraging pre-trained latent diffusion models within a text-conditioned framework. By first generating masks covering body areas to be inpainted, then synthesizing new garments matching text descriptions, and finally applying identity preservation to restore facial features, DiCTI addresses the challenge of creating realistic virtual try-on experiences. The system demonstrates significant improvements over state-of-the-art methods across multiple evaluation metrics.

## Method Summary
DiCTI leverages pre-trained latent diffusion models for text-guided clothing synthesis by treating the task as an inpainting problem. Given an input image and text description, the system first generates a mask covering the body area to be modified using manual segmentation techniques. This mask guides the diffusion model to synthesize new garments matching the text prompt while preserving the original background and non-clothing regions. The approach incorporates identity preservation to maintain facial features during the generation process, ensuring the final image retains the person's identity. The method operates on high-resolution images and demonstrates strong performance across diverse clothing categories and poses.

## Key Results
- Outperforms FICE baseline with CLIP-S score of 27.48 vs 22.04 for text prompt adherence
- Achieves lower FrÃ©chet Inception Distance (KID: 0.066 vs 0.082), indicating better image realism
- Shows superior performance across human-rated quality metrics including identity preservation, pose preservation, prompt adherence, and realism

## Why This Works (Mechanism)
DiCTI's effectiveness stems from formulating clothing synthesis as an inpainting task, which allows leveraging the powerful representational capabilities of pre-trained diffusion models. By using masks to constrain the generation to specific body regions, the method maintains control over where new garments appear while preserving the original image context. The text conditioning enables precise control over garment characteristics, while the identity preservation module ensures that facial features remain consistent despite the image modifications. This combination of spatial control, text guidance, and identity preservation creates a robust framework for generating realistic virtual try-on results.

## Foundational Learning
- **Latent Diffusion Models**: Why needed - to generate high-quality images efficiently by operating in compressed latent space rather than pixel space. Quick check - verify model can generate diverse, high-resolution images from random noise.
- **Inpainting Framework**: Why needed - to selectively modify specific regions of an image while preserving the rest. Quick check - confirm mask generation correctly identifies target regions for modification.
- **Text-to-Image Conditioning**: Why needed - to translate textual garment descriptions into visual representations. Quick check - test if different text prompts produce visibly distinct garment styles.
- **Identity Preservation**: Why needed - to maintain person recognition despite clothing changes. Quick check - verify facial features remain recognizable after garment synthesis.
- **Mask Generation**: Why needed - to provide spatial guidance for the inpainting process. Quick check - ensure masks accurately cover target body regions without affecting non-clothing areas.
- **Pre-trained Model Fine-tuning**: Why needed - to adapt general image generation capabilities to the specific task of clothing synthesis. Quick check - confirm fine-tuning improves performance on clothing-specific tasks.

## Architecture Onboarding

**Component Map:** Input Image -> Mask Generation -> Text Conditioning -> Latent Diffusion Model -> Identity Preservation -> Output Image

**Critical Path:** The essential processing sequence involves generating segmentation masks, applying text conditioning to the diffusion model, performing latent image synthesis, and finally applying identity preservation to restore facial features.

**Design Tradeoffs:** The approach trades computational efficiency for quality by using pre-trained models and fine-tuning rather than training from scratch. It prioritizes photorealism and text adherence over real-time performance, making it suitable for applications where quality matters more than speed.

**Failure Signatures:** Common failure modes include inaccurate mask generation leading to incomplete garment synthesis, text conditioning misalignment resulting in garments that don't match descriptions, and identity preservation failures where facial features become distorted or unrecognizable.

**First Experiments:**
1. Test basic inpainting capability on simple images with clear masks to verify diffusion model functionality
2. Evaluate text conditioning by generating multiple garments from different text prompts using the same base image
3. Assess identity preservation by comparing facial feature similarity before and after garment synthesis

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation primarily compares against a single baseline (FICE), limiting generalizability claims
- Mask generation approach based on manual segmentation may not scale well to diverse body types and occlusions
- Identity preservation focuses primarily on facial features, potentially overlooking full-body consistency requirements

## Confidence
- **High**: Performance improvements over FICE on VITON-HD and Fashionpedia datasets; technical soundness of diffusion-based inpainting formulation; validity of identity preservation approach for facial features
- **Medium**: Claims about generating "high-resolution, photorealistic images" beyond evaluated datasets; generalization to clothing categories not represented in training data; robustness to diverse poses and body shapes
- **Low**: Claims about real-world applicability without user testing; scalability to production-level virtual try-on systems; performance in cross-dataset transfer scenarios

## Next Checks
1. Test DiCTI on additional virtual try-on datasets with varied clothing categories and body types to assess generalization beyond VITON-HD and Fashionpedia
2. Conduct user studies with diverse demographic groups to evaluate perceptual quality and identity preservation across different ethnicities, ages, and body shapes
3. Evaluate computational efficiency and inference time to determine practical deployment feasibility for real-time virtual try-on applications