---
ver: rpa2
title: 'Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge
  Graph'
arxiv_id: '2404.03623'
source_url: https://arxiv.org/abs/2404.03623
tags:
- bojack
- horseman
- joker
- knowledge
- batman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a patching-based framework to decode factual
  knowledge embedded in the latent representations of Large Language Models (LLMs).
  The framework leverages activation patching to extract semantics from token embeddings,
  without relying on training or external models.
---

# Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph

## Quick Facts
- **arXiv ID**: 2404.03623
- **Source URL**: https://arxiv.org/abs/2404.03623
- **Reference count**: 40
- **Key outcome**: Framework decodes factual knowledge from LLM latent representations and represents it as a dynamic knowledge graph showing layer-wise evolution

## Executive Summary
This work introduces a patching-based framework to decode factual knowledge embedded in the latent representations of Large Language Models (LLMs). The framework leverages activation patching to extract semantics from token embeddings without training or external models. The extracted factual information is represented as a dynamic knowledge graph, tracing its evolution across model layers. Experiments on claim verification datasets reveal insights into LLM reasoning, including entity centrality, multi-hop reasoning, and representation errors causing evaluation mistakes.

## Method Summary
The framework uses activation patching to decode factual knowledge from LLM representations by replacing placeholder token embeddings with weighted summaries of input token embeddings. The method constructs dynamic knowledge graphs to represent the evolution of factual information across model layers. The approach employs POS-tagging to assign weights to tokens, focusing on end tokens of nouns and verbs to preserve semantic content. The decoded information is evaluated on claim verification tasks using FEVER and CLIMATE-FEVER datasets with LLaMA 2 7B model.

## Key Results
- Framework achieves 78% ROC AUC on FEVER dataset without fine-tuning
- Reveals layer-wise evolution patterns: early layers focus on syntax, later layers shift to claim-related facts
- Identifies specific representation errors causing model evaluation mistakes in factual reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation patching enables semantic probing without training or external models
- Mechanism: By replacing the latent representation of a placeholder token with a weighted summary of input token embeddings, the model's output text reveals the factual knowledge encoded in those layers
- Core assumption: The language model's generation is sensitive to changes in input embeddings in a way that preserves and reveals semantic content
- Evidence anchors:
  - [abstract] "Our framework employs activation patching... to extract encoded knowledge. Accordingly, we neither rely on training nor external models"
  - [section 3.2] "This model inference, patched with the summary input representation from l-th layer, generates a structured text ol"
  - [corpus] Weak evidence - patching is established in mechanistic interpretability but this specific weighted-sum multi-token application is novel
- Break condition: If the patched inference generates unstructured or nonsensical text consistently across layers, indicating the model doesn't reliably decode semantic content from altered embeddings

### Mechanism 2
- Claim: The weighted sum of token embeddings preserves entity and predicate information
- Mechanism: POS-tagging assigns higher weights to end tokens of nouns and verbs, creating a summary vector that emphasizes sentence entities and predicates
- Core assumption: End tokens of nouns and verbs carry the most semantic weight for entity and predicate representation
- Evidence anchors:
  - [section 3.2] "Nouns and verbs are assigned a weight equal to zero to all but their end token which receives a weight of one"
  - [section 3.2] "We focus on end tokens based on Meng et al.'s (2022) study, which found that the model forms a subject representation at the final token of an entity name"
  - [corpus] Moderate evidence - POS-based weighting is common in NLP, but specific validation of this weighting scheme for multi-token patching is limited
- Break condition: If the generated factual information consistently focuses on incorrect entities or predicates, suggesting the weighting scheme fails to capture semantic relevance

### Mechanism 3
- Claim: Dynamic knowledge graphs reveal layer-wise evolution of factual knowledge
- Mechanism: By representing each layer's decoded facts as a graph and concatenating them temporally, the framework visualizes how knowledge transforms across layers
- Core assumption: Graph similarity metrics can capture meaningful semantic changes in encoded knowledge between layers
- Evidence anchors:
  - [abstract] "representing its dynamics using a graph representation" and "combining a multi-relational graph of entities and relations... with a dynamic graph"
  - [section 4.4] "We use a knowledge graph to represent the list of ground literals... We eventually concatenate all the graphs generated for the different values of l, creating a dynamic graph"
  - [corpus] Moderate evidence - graph-based representation of model behavior is established, but this specific application to layer-wise evolution via patching is novel
- Break condition: If graph similarity metrics show no meaningful differentiation between layers or fail to correlate with known model behavior patterns

## Foundational Learning

- Concept: Activation patching in mechanistic interpretability
  - Why needed here: Understanding how vector-level interventions reveal model behavior
  - Quick check question: What does activation patching replace in a model's computation, and what does this reveal?

- Concept: Knowledge graph construction and analysis
  - Why needed here: The framework converts decoded facts into graph structures for visualization and analysis
  - Quick check question: How do subject-predicate-object triples become nodes and edges in a knowledge graph?

- Concept: POS-tagging and token weighting schemes
  - Why needed here: The framework uses POS information to weight token embeddings for semantic preservation
  - Quick check question: Why might end tokens of nouns and verbs be weighted more heavily than other tokens?

## Architecture Onboarding

- Component map: Source prompt → LLM inference (store hidden states) → Target prompt with placeholder → Patching operation → Generate structured text → Convert to SPO triples → Build knowledge graph → Concatenate across layers
- Critical path: Patching operation and subsequent text generation - this is where the semantic decoding happens
- Design tradeoffs: Multi-token vs single-token patching (richer semantics vs simpler implementation), weighted sum vs other aggregation methods (focus on entities vs uniform treatment)
- Failure signatures: Unstructured output text from patched inference, graph similarity metrics showing no layer differentiation, factual information not matching input claim requirements
- First 3 experiments:
  1. Run framework on a simple claim ("Paris is the capital of France") and verify structured output contains expected facts
  2. Compare outputs from early vs late layers to confirm known pattern of early layers focusing on syntax
  3. Test different weighting schemes (uniform vs POS-based) to measure impact on factual information quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of tokens for patching (e.g., focusing on nouns and verbs vs. all tokens) affect the quality and relevance of the decoded factual information?
- Basis in paper: [explicit] The paper discusses using part-of-speech tagging to assign weights to tokens, focusing on end tokens of nouns and verbs.
- Why unresolved: The paper presents a specific approach but does not explore alternative token selection strategies or their impact on the decoded information.
- What evidence would resolve it: Experiments comparing the framework's performance using different token selection strategies (e.g., all tokens, different POS combinations) and analyzing the resulting factual information for relevance and accuracy.

### Open Question 2
- Question: What are the underlying mechanisms causing the shift in attention towards in-context examples in the final layers of the LLM?
- Basis in paper: [inferred] The paper speculates that limited factual knowledge in the final layers and a "store-and-seek" pattern might influence this attention shift.
- Why unresolved: The paper presents speculative hypotheses but does not provide a detailed analysis of the attention mechanisms or conduct experiments to validate these hypotheses.
- What evidence would resolve it: Detailed analysis of attention matrices across layers, experiments manipulating the amount of factual knowledge in the input, and ablation studies on the "store-and-seek" pattern.

### Open Question 3
- Question: How can the framework be adapted to handle longer and more complex claims that require multi-hop reasoning over multiple entities?
- Basis in paper: [inferred] The paper mentions the challenge of multi-hop reasoning and presents examples where the model struggles to connect information across multiple entities.
- Why unresolved: The paper focuses on sentence-level claim verification and does not explore strategies for scaling the framework to handle more complex reasoning tasks.
- What evidence would resolve it: Experiments testing the framework's performance on claims requiring multi-hop reasoning, analysis of the decoded factual information for completeness and accuracy, and development of techniques to improve the model's ability to connect information across multiple entities.

## Limitations

- The framework's reliance on activation patching introduces uncertainties about the validity of semantic content extraction across diverse claim types
- The specific POS-based weighting scheme for token aggregation lacks comprehensive validation against alternative methods
- The interpretation of global knowledge evolution patterns remains speculative without quantitative validation across diverse datasets

## Confidence

- **High Confidence**: The core framework of using activation patching to decode factual knowledge from LLM representations is well-established in mechanistic interpretability literature. The basic claim verification evaluation methodology is sound.
- **Medium Confidence**: The specific implementation details (POS-based weighting, weighted-sum aggregation, graph similarity metrics) are plausible but require empirical validation. The interpretation of layer-wise knowledge evolution patterns needs further substantiation.
- **Low Confidence**: The global interpretability claims about word-based knowledge evolving into claim-related facts are speculative and lack quantitative validation across diverse claim types.

## Next Checks

1. **Alternative Aggregation Method Comparison**: Implement and compare the framework using alternative token representation aggregation methods (mean pooling, max pooling, attention-based weighted sum) against the current POS-based weighted sum to measure impact on factual information quality and graph similarity metrics.

2. **Layer-wise Graph Similarity Validation**: Design experiments to verify that graph similarity metrics (e.g., Graph Edit Distance, Jaccard similarity) meaningfully differentiate between layers with known behavioral differences (early layers focusing on syntax vs. later layers on semantics) and correlate with established model behavior patterns.

3. **Cross-dataset Generalization Test**: Apply the framework to a diverse set of claim verification datasets beyond FEVER and CLIMATE-FEVER (e.g., SciFact for scientific claims, SocialIQA for common sense) to assess the robustness of the claimed global trends and identify potential dataset-specific biases in the knowledge evolution patterns.