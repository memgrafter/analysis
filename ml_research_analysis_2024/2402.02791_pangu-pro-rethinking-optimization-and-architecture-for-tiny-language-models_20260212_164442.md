---
ver: rpa2
title: "PanGu-$\u03C0$ Pro:Rethinking Optimization and Architecture for Tiny Language\
  \ Models"
arxiv_id: '2402.02791'
source_url: https://arxiv.org/abs/2402.02791
tags:
- language
- pangu
- training
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically explores optimization and architectural
  strategies for tiny language models under 2B parameters. The authors analyze three
  key aspects: neural architecture (including tokenizer compression and depth/width
  tuning), parameter initialization (comparing random vs.'
---

# PanGu-$π$ Pro:Rethinking Optimization and Architecture for Tiny Language Models

## Quick Facts
- arXiv ID: 2402.02791
- Source URL: https://arxiv.org/abs/2402.02791
- Reference count: 40
- Primary result: PanGu-π-1.5B Pro outperforms Qwen-1.8B with 16.67% fewer parameters

## Executive Summary
This study systematically explores optimization and architectural strategies for tiny language models under 2B parameters. The authors analyze three key aspects: neural architecture (including tokenizer compression and depth/width tuning), parameter initialization (comparing random vs. inherited weights from larger models), and optimization strategies (batch size, learning rate scaling, and multiple-round training). Their methodology includes empirical studies on a 1B-parameter base model, identifying that compact tokenizers covering 90%+ of corpus, deeper architectures (around 20 layers), and parameter inheritance via learnable pruning masks are particularly effective. The resulting PanGu-π-1B Pro achieves 8.87 average improvement on benchmarks over the baseline, while PanGu-π-1.5B Pro outperforms larger models like Qwen-1.8B with 16.67% fewer parameters, achieving a 60.64 average score versus 55.04.

## Method Summary
The authors conducted a systematic exploration of tiny language model optimization through three perspectives: neural architecture, parameter initialization, and optimization strategy. They started with a 1B-parameter PanGu-π base model and systematically varied tokenizer size (8k, 32k, 48k, 100k), depth (15, 20, 25, 30 layers), and tested parameter inheritance from a pre-trained PanGu-π-7B model using learnable pruning masks. The optimization strategy included multiple-round training with 50% sampling rate, AdamW optimizer with cosine learning rate decay, and batch size scaling. They validated their findings through extensive experiments across multiple benchmarks including C-Eval, CMMLU, MMLU, AGI-Eval, BoolQ, AX-b, PIQA, EPRSTM, XSum, and C3.

## Key Results
- PanGu-π-1B Pro achieves 8.87 average improvement over baseline on examination, knowledge, reasoning, and understanding benchmarks
- PanGu-π-1.5B Pro outperforms Qwen-1.8B with 16.67% fewer parameters, achieving 60.64 vs 55.04 average score
- Tokenizer compression to 48k vocabulary covering 97.86% of corpus reduces parameter overhead while maintaining performance
- Deeper architectures (around 20 layers) show better performance for tiny models despite inference speed costs
- Parameter inheritance from larger models via learnable pruning masks accelerates convergence and boosts performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compact tokenizers reduce parameter overhead while preserving most corpus coverage.
- Mechanism: Removing low-frequency tokens from the vocabulary shrinks embedding and head layer parameters, which dominate in tiny models.
- Core assumption: Token frequency follows a long-tail distribution, so removing infrequent tokens has minimal impact on most corpus.
- Evidence anchors:
  - [abstract] "compressing the tokenizer by removing low-frequency vocabularies enhances the model's representational efficiency"
  - [section] "tokens exhibit a long-tail effect, where the top 48k vocabularies accounting for 97.86% of all the training corpus"
  - [corpus] Weak: no direct corpus frequency stats provided
- Break condition: If corpus has uniform or short-tail token distribution, removal would hurt coverage and performance.

### Mechanism 2
- Claim: Deeper architectures yield higher performance for tiny models at the cost of inference speed.
- Mechanism: Adding layers increases model capacity, enabling better representation learning, but slows computation due to sequential dependencies.
- Core assumption: For fixed parameter budget, allocating more to depth rather than width is more effective in tiny models.
- Evidence anchors:
  - [section] "deeper tiny language models exhibit better performance, however, at the cost of inference speed"
  - [section] "depth is the primary factor for tiny language models, and deeper models usually achieve high performance"
  - [corpus] Weak: no direct corpus evidence
- Break condition: If training data is insufficient or noise-heavy, deeper models may overfit without generalization gain.

### Mechanism 3
- Claim: Parameter inheritance from larger models improves convergence and performance.
- Mechanism: Critical parameters from a pre-trained large model initialize the tiny model, transferring learned representations.
- Core assumption: Key layers and neurons from large models are transferable and beneficial to smaller architectures.
- Evidence anchors:
  - [abstract] "Inheriting parameters from the large model proves effective in boosting performance and expediting convergence"
  - [section] "adopting data-driven learnable criteria has demonstrated greater efficacy compared to heuristic methods"
  - [corpus] Weak: no direct corpus evidence
- Break condition: If large model is overfit or domain-mismatched, inherited parameters may degrade tiny model performance.

## Foundational Learning

- Concept: Tokenizer compression via frequency-based pruning
  - Why needed here: Large vocabularies waste parameters in tiny models; pruning low-frequency tokens reduces overhead without losing coverage.
  - Quick check question: If a tokenizer covers 97.86% of corpus with 48k tokens, how many tokens are needed to cover 90%?

- Concept: Depth vs. width allocation in parameter-constrained models
  - Why needed here: For fixed parameter budget, depth has stronger correlation with performance than width in tiny models.
  - Quick check question: In a 1B parameter model, if you increase depth by 2 layers, how must you adjust width to maintain parameter count?

- Concept: Parameter inheritance via layer and neuron importance
  - Why needed here: Transferring important weights from large models accelerates training and boosts final performance.
  - Quick check question: Which layers are most critical to preserve when inheriting from a large model?

## Architecture Onboarding

- Component map: Tokenizer compression (48k vocab) -> Architecture tuning (21 layers, width 1792) -> Parameter inheritance (learnable masks from PanGu-π-7B) -> Multi-round training (50% sampling)
- Critical path: Tokenizer compression → Architecture tuning → Parameter inheritance → Multi-round training
- Design tradeoffs:
  - Depth vs. speed: Deeper models perform better but slower
  - Tokenizer size vs. coverage: Smaller vocab reduces params but may hurt rare token handling
  - Batch size vs. convergence: Larger batches reduce iterations but may slow convergence
- Failure signatures:
  - Performance stalls despite more training: Likely data forgetting; need more rounds or better sampling
  - Inference too slow: Consider reducing depth or switching to GQA
  - Model underfits: Check if tokenizer coverage is too low or initialization is poor
- First 3 experiments:
  1. Vary tokenizer size (8k, 32k, 48k, 100k) to find optimal coverage-parameter trade-off
  2. Adjust depth (15, 20, 25, 30) while fixing width to measure performance vs. speed
  3. Compare random vs. learned pruning masks for parameter inheritance to assess convergence benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal depth configuration for tiny language models of different parameter sizes (e.g., 500M, 2B) beyond the 1B model studied?
- Basis in paper: [inferred] The paper shows depth is the primary factor for tiny language models and recommends around 20 layers for 1B parameters, but acknowledges this needs to be validated for other sizes.
- Why unresolved: The study focused specifically on 1B parameter models and used parameter constraints to control for width/expansion rate, making it unclear if the 20-layer recommendation generalizes to other model sizes.
- What evidence would resolve it: Systematic experiments varying depth while controlling for total parameters across multiple model sizes (500M, 2B, etc.) to establish depth-performance relationships for each size category.

### Open Question 2
- Question: How does the effectiveness of parameter inheritance strategies vary across different base model architectures and training datasets?
- Basis in paper: [explicit] The paper demonstrates parameter inheritance from PanGu-π-7B using learnable pruning masks is effective, but notes this needs to be tested across different architectures and datasets.
- Why unresolved: The study only tested inheritance from one specific 7B model (PanGu-π-7B) and doesn't explore whether these findings hold when inheriting from other architectures or when using different training corpora.
- What evidence would resolve it: Comparative experiments testing parameter inheritance from multiple source models (LLaMA, Qwen, etc.) trained on different datasets, measuring performance across the same target tasks.

### Open Question 3
- Question: What are the fundamental limitations of multiple-round training for tiny models, and at what point does additional training become counterproductive?
- Basis in paper: [inferred] The paper shows performance improvements plateau after two rounds of training with 50% sampling rate, but doesn't investigate the theoretical or practical limits of this approach.
- Why unresolved: The study only tested up to three rounds with fixed sampling rates, leaving open questions about whether more rounds could help with different sampling strategies, data orders, or model capacities.
- What evidence would resolve it: Extensive experiments varying the number of training rounds (up to 10+), testing different sampling strategies (adaptive, curriculum-based), and measuring both performance and catastrophic forgetting metrics.

## Limitations

- Results are corpus-specific and may not generalize to different domains or languages
- Parameter inheritance mechanism lacks detailed explanation of initialization and convergence criteria
- Study focuses only on PanGu-π models, limiting generalizability to other model families
- Inference speed tradeoffs of deeper architectures are acknowledged but not quantitatively measured

## Confidence

- Tokenizer compression effectiveness: Medium confidence - well-supported by frequency analysis but limited to single corpus validation
- Deeper architectures for tiny models: Medium confidence - supported by ablation studies but with clear inference speed tradeoffs not quantified
- Parameter inheritance benefits: Medium confidence - empirical improvements shown but mechanism and initialization details unclear
- Multiple-round training efficacy: Low-Medium confidence - theoretical justification present but practical implementation details sparse

## Next Checks

1. **Cross-corpus tokenizer validation**: Test the 48k tokenizer configuration on at least three diverse corpora (different domains/languages) to verify that the 97.86% coverage assumption holds and that parameter savings translate consistently across domains.

2. **Speed-performance tradeoff quantification**: Measure wall-clock inference time for models with varying depth (15-30 layers) on representative hardware to create a comprehensive performance-per-second curve, validating the claimed speed-cost of deeper architectures.

3. **Parameter inheritance ablation**: Implement and compare three initialization strategies: random weights, static pruning masks, and learnable pruning masks to isolate the specific contribution of the learnable component to convergence speed and final performance.