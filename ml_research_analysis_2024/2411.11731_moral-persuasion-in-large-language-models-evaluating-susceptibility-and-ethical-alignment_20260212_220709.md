---
ver: rpa2
title: 'Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical
  Alignment'
arxiv_id: '2411.11731'
source_url: https://arxiv.org/abs/2411.11731
tags:
- moral
- llms
- persuasion
- agent
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigated how large language models (LLMs) can be
  influenced through persuasion in morally ambiguous scenarios and aligned with ethical
  frameworks. Two experiments were conducted: one examining LLM susceptibility to
  persuasion using morally ambiguous decision-making scenarios, and another exploring
  how different ethical prompts affect moral foundation scores.'
---

# Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment

## Quick Facts
- arXiv ID: 2411.11731
- Source URL: https://arxiv.org/abs/2411.11731
- Authors: Allison Huang; Yulu Niki Pi; Carlos Mougan
- Reference count: 8
- Key outcome: LLMs can be influenced through persuasion in morally ambiguous scenarios and aligned with ethical frameworks through targeted prompting

## Executive Summary
This study investigates how large language models can be influenced by persuasion in morally ambiguous scenarios and aligned with ethical frameworks. Two experiments were conducted: one examining LLM susceptibility to persuasion using morally ambiguous decision-making scenarios, and another exploring how different ethical prompts affect moral foundation scores. The research demonstrates that LLMs are indeed persuadable, with effectiveness varying based on model, scenario complexity, and conversation length. Additionally, LLMs can be aligned with specific ethical frameworks (utilitarianism, deontology, and virtue ethics) through targeted prompting, though the degree of alignment varies across models.

## Method Summary
The study employed a two-stage experimental approach using 100 high-ambiguity scenarios from the moralchoice dataset. In the first experiment, a Persuader Agent attempted to change a Base Agent's initial decisions through multi-turn conversations, measuring Change in Action Likelihood (CAL) and Decision Change Rate (DCR). The second experiment involved prompting LLMs to align with specific ethical frameworks and measuring their moral foundation scores using the Moral Foundations Questionnaire (MFQ-30). Multiple LLM models were tested including claude-3-haiku, llama-3.1-8b, mistral-7b-instruct, and gpt-4o variants.

## Key Results
- LLMs are persuadable in morally ambiguous scenarios, with claude-3-haiku and llama-3.1-8b showing highest susceptibility
- Persuasion effectiveness varies significantly based on model architecture, scenario complexity, and conversation length
- Different ethical frameworks produce varying moral foundation scores across models, with gpt-4o and mistral-7b-instruct showing significant variation while claude-3-haiku remains more consistent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are susceptible to persuasion in morally ambiguous scenarios because they can be influenced by conversational agents through multi-turn exchanges.
- Mechanism: The persuader agent engages in a dialogue with the base agent, presenting arguments and attempting to shift the base agent's decision. The base agent's action likelihood changes based on the persuasiveness of the conversation.
- Core assumption: The LLM's decision-making process is not fixed and can be influenced by external input, particularly in morally ambiguous scenarios where there is no clear right or wrong answer.
- Evidence anchors:
  - [abstract] "The results demonstrate that LLMs can indeed be persuaded in morally charged scenarios, with the success of persuasion depending on factors such as the model used, the complexity of the scenario, and the conversation length."
  - [section] "The findings showed that LLMs are indeed persuadable, with models like claude-3-haiku and llama-3.1-8b being the most susceptible."
- Break condition: The persuasion fails if the base agent's decision remains unchanged despite the persuader's efforts, or if the conversation devolves into non-productive exchanges.

### Mechanism 2
- Claim: Different ethical frameworks can be used to align LLMs by prompting them to adopt specific moral perspectives.
- Mechanism: The LLM is provided with a prompt that instructs it to adopt a particular ethical framework (e.g., utilitarianism, deontology, virtue ethics). The LLM then responds to a moral foundations questionnaire based on that framework.
- Core assumption: LLMs can internalize and apply ethical frameworks through targeted prompting, and their responses will reflect the chosen framework.
- Evidence anchors:
  - [abstract] "The second experiment involves prompting LLMs to align with specific ethical frameworks (utilitarianism, deontology, and virtue ethics) and measuring their moral foundation scores."
  - [section] "We guide LLMs to align with specific moral frameworks and then use a questionnaire based on Moral Foundations Theory to examine how these different moral philosophies are encoded in the LLMs."
- Break condition: The alignment fails if the LLM's responses do not reflect the chosen ethical framework, or if the LLM refuses to engage with the prompt.

### Mechanism 3
- Claim: LLMs exhibit varying degrees of susceptibility to persuasion and alignment, with some models being more responsive to ethical prompts than others.
- Mechanism: The susceptibility to persuasion and alignment is influenced by factors such as the model's architecture, training data, and inherent biases. Some models may be more rigid in their moral reasoning, while others may be more flexible.
- Core assumption: The susceptibility to persuasion and alignment is not uniform across all LLMs and depends on individual model characteristics.
- Evidence anchors:
  - [abstract] "Notably, LLMs of distinct sizes but from the same company produced markedly different outcomes, highlighting the variability in their susceptibility to ethical persuasion."
  - [section] "Models like claude-3-haiku and llama-3.1-8b were the most susceptible, while others demonstrated greater resistance to persuasion."
- Break condition: The variability breaks down if all models exhibit the same level of susceptibility to persuasion and alignment, regardless of their architecture or training data.

## Foundational Learning

- Concept: Moral Foundations Theory (MFT)
  - Why needed here: MFT provides a framework for understanding the diverse moral reasoning that underlies human behavior across cultures and political orientations. It is used to evaluate how well LLMs represent different moral values.
  - Quick check question: What are the five moral foundations identified by MFT, and how do they differ in their emphasis across cultures and political orientations?

- Concept: Ethical frameworks (utilitarianism, deontology, virtue ethics)
  - Why needed here: These ethical frameworks provide structured approaches for decision-making in moral dilemmas. They are used to align LLMs with specific moral perspectives.
  - Quick check question: How do utilitarianism, deontology, and virtue ethics differ in their approach to moral decision-making, and what are some key examples of their application?

- Concept: Persuasion in LLMs
  - Why needed here: Persuasion is a key mechanism for influencing LLM behavior in morally ambiguous scenarios. Understanding how persuasion works in LLMs is crucial for evaluating their susceptibility to ethical alignment.
  - Quick check question: What are some common persuasion strategies used in LLMs, and how do they differ from human persuasion techniques?

## Architecture Onboarding

- Component map: Persuader Agent -> Base Agent -> Ethical Framework Prompts -> Moral Foundations Questionnaire (MFQ)
- Critical path:
  1. Generate morally ambiguous scenarios
  2. Assign roles to LLMs (Persuader Agent and Base Agent)
  3. Engage in multi-turn conversations between the Persuader Agent and Base Agent
  4. Evaluate the Base Agent's decision change rate and rule violation rate
  5. Align LLMs with ethical frameworks using targeted prompts
  6. Evaluate the LLM's moral foundation scores using the MFQ
- Design tradeoffs:
  - Model size vs. susceptibility to persuasion: Larger models may be less susceptible to persuasion, but smaller models may be more flexible in their moral reasoning
  - Conversation length vs. persuasion effectiveness: Longer conversations may increase persuasion effectiveness, but they may also lead to non-productive exchanges
  - Ethical framework specificity vs. LLM alignment: More specific ethical frameworks may lead to better alignment, but they may also limit the LLM's flexibility in moral reasoning
- Failure signatures:
  - Persuasion failure: The Base Agent's decision remains unchanged despite the Persuader Agent's efforts
  - Alignment failure: The LLM's responses do not reflect the chosen ethical framework, or the LLM refuses to engage with the prompt
  - Evaluation failure: The MFQ scores do not accurately reflect the LLM's moral foundation alignment
- First 3 experiments:
  1. Evaluate the Base Agent's susceptibility to persuasion using a set of morally ambiguous scenarios and a Persuader Agent
  2. Align the LLM with a specific ethical framework using targeted prompts and evaluate its moral foundation scores using the MFQ
  3. Compare the susceptibility to persuasion and alignment across different LLM models and ethical frameworks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of LLM architecture or training influence susceptibility to moral persuasion?
- Basis in paper: [explicit] The paper notes that no clear predictors for susceptibility to persuasion were found, and no model family is significantly stronger at persuasion or less susceptible to persuasion.
- Why unresolved: The study tested various models but did not identify specific architectural or training factors that predict persuasion susceptibility.
- What evidence would resolve it: Systematic comparison of model architectures, training data, and fine-tuning approaches to identify correlations with persuasion susceptibility.

### Open Question 2
- Question: How does persuasion susceptibility generalize across different types of moral scenarios beyond those tested?
- Basis in paper: [explicit] The study tested high-ambiguity scenarios and found persuasion effectiveness varied significantly based on scenario complexity.
- Why unresolved: The paper only tested a specific set of high-ambiguity scenarios from the moralchoice dataset, limiting generalizability to other moral situations.
- What evidence would resolve it: Testing persuasion effectiveness across a broader range of moral scenarios with varying levels of complexity and different ethical frameworks.

### Open Question 3
- Question: What are the long-term effects of repeated moral persuasion attempts on LLM behavior?
- Basis in paper: [inferred] The study only examined single persuasion attempts within four-turn conversations, not cumulative effects.
- Why unresolved: The experimental design focused on immediate persuasion outcomes rather than examining how repeated exposure to persuasion might alter LLM decision-making patterns over time.
- What evidence would resolve it: Longitudinal studies tracking LLM responses to repeated persuasion attempts across multiple sessions and extended time periods.

## Limitations

- Limited diversity of ethical frameworks tested (only utilitarianism, deontology, and virtue ethics)
- Use of high-ambiguity scenarios may not generalize to more clear-cut ethical situations
- Reliance on single-turn moral foundation evaluations may not capture dynamic ethical reasoning over extended conversations

## Confidence

- **High Confidence**: LLMs can be persuaded in morally ambiguous scenarios, with persuasion effectiveness varying by model and scenario complexity
- **Medium Confidence**: Different ethical frameworks can meaningfully influence LLM moral foundation scores, though the magnitude and consistency of this effect varies across models
- **Medium Confidence**: Model size and architecture impact susceptibility to persuasion, but the relationship is not linear and depends on multiple factors

## Next Checks

1. **Cross-Cultural Validation**: Test the persuasion and alignment mechanisms across multiple languages and cultural contexts to assess whether the observed effects are universal or culturally specific.

2. **Dynamic Ethical Reasoning**: Extend the evaluation to multi-turn conversations where ethical frameworks are applied consistently over longer interactions, measuring how moral foundation scores evolve during extended dialogue.

3. **Real-World Scenario Testing**: Apply the persuasion and alignment mechanisms to practical ethical decision-making scenarios in fields like healthcare, law, or autonomous systems to validate ecological validity.