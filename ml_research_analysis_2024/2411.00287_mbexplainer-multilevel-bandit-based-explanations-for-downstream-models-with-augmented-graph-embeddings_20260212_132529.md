---
ver: rpa2
title: 'MBExplainer: Multilevel bandit-based explanations for downstream models with
  augmented graph embeddings'
arxiv_id: '2411.00287'
source_url: https://arxiv.org/abs/2411.00287
tags:
- features
- graph
- downstream
- node
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of explaining predictions made
  by ensemble models that combine neural graph embeddings from Graph Neural Networks
  (GNNs) with additional tabular features in downstream machine learning tasks. The
  authors propose MBExplainer, a model-agnostic explanation approach that generates
  human-comprehensible explanations consisting of three components: the most important
  subgraph, the topmost important nodal features, and the topmost important augmented
  downstream features.'
---

# MBExplainer: Multilevel bandit-based explanations for downstream models with augmented graph embeddings

## Quick Facts
- arXiv ID: 2411.00287
- Source URL: https://arxiv.org/abs/2411.00287
- Reference count: 12
- Accuracy on MUTAG dataset: 0.8421

## Executive Summary
MBExplainer addresses the challenge of explaining predictions from ensemble models that combine neural graph embeddings with tabular features. The method generates human-comprehensible explanations consisting of three components: the most important subgraph, the topmost important nodal features, and the topmost important augmented downstream features. By using a game-theoretic formulation with three Shapley values and a novel multilevel search algorithm, MBExplainer achieves both interpretability and computational tractability for complex graph-based machine learning models.

## Method Summary
MBExplainer is a model-agnostic explanation approach that works with ensemble models combining graph embeddings from GNNs with tabular features. It uses a game-theoretic formulation with three Shapley values corresponding to subgraphs, node features, and augmented downstream features. To make computation tractable, it employs a multilevel search algorithm using three interweaved Monte Carlo Tree Searches to simultaneously prune local search spaces, while a contextual bandit efficiently allocates the pruning budget among these spaces. The method generates explanations as a triple (S', G', M) representing the most important subgraph, node features, and augmented features respectively.

## Key Results
- Achieved 0.8421 accuracy on MUTAG dataset for the downstream model
- Demonstrated effectiveness on multiple public graph datasets for both node and graph classification tasks
- Showed promise in providing interpretable explanations for complex ensemble models combining graph embeddings with tabular features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MBExplainer uses a game-theoretic formulation to attribute importance to three distinct components: subgraph, node features, and augmented downstream features.
- Mechanism: Three separate Shapley value games are defined, each corresponding to one component. The total explanation score is a weighted sum of the three Shapley values, allowing MBExplainer to capture both individual and interaction effects.
- Core assumption: The downstream prediction can be modeled as a cooperative game among the three components.
- Evidence anchors:
  - [abstract] "A game-theoretic formulation is used to take the contributions of each component and their interactions into account by assigning three Shapley values corresponding to their own specific games."
  - [section 4.1] Formal definitions of the three Shapley values and their corresponding games.
- Break condition: If the downstream model does not satisfy the assumptions of cooperative game theory (e.g., superadditivity), the Shapley value attribution may not be meaningful.

### Mechanism 2
- Claim: MBExplainer achieves computational tractability by simultaneously pruning local search spaces using three interweaved Monte Carlo Tree Searches (MCTS).
- Mechanism: Each MCTS explores one of the three search spaces (subgraphs, node features, downstream features). Pruning actions remove nodes or features, and MCTS parameters are updated based on estimated Shapley values of the resulting components.
- Core assumption: The most important explanation can be found by iteratively pruning less important elements.
- Evidence anchors:
  - [abstract] "MBExplainer applies a novel multilevel search algorithm that enables simultaneous pruning of local search spaces in a computationally tractable way."
  - [section 4.2.1] Detailed description of the MCTS setup and pruning actions.
- Break condition: If the search space is too large or the pruning budget is insufficient, the MCTS may not converge to the optimal explanation.

### Mechanism 3
- Claim: MBExplainer efficiently allocates pruning budget among the three local search spaces using a contextual bandit algorithm.
- Mechanism: The contextual bandit receives the current context (S', G', M) and selects the next search space to prune based on estimated rewards (Shapley values). This balances exploration and exploitation to maximize the quality of the explanation within the budget.
- Core assumption: The contextual bandit can learn which search space is most promising at each step based on past experience.
- Evidence anchors:
  - [abstract] "MBExplainer also includes a global search algorithm that uses contextual bandits to efficiently allocate pruning budget among the local search spaces."
  - [section 4.2.2] Description of the contextual bandit algorithm and its integration with the MCTS.
- Break condition: If the reward function (Shapley value) is not a good proxy for explanation quality, the bandit may make suboptimal pruning decisions.

## Foundational Learning

- Concept: Shapley values and cooperative game theory
  - Why needed here: To formally define and quantify the importance of each component in the explanation.
  - Quick check question: What is the difference between a Shapley value and a simple feature importance score?

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: To efficiently explore and prune the large search spaces of subgraphs and features.
  - Quick check question: How does MCTS balance exploration and exploitation during the search process?

- Concept: Contextual bandits
  - Why needed here: To dynamically allocate the pruning budget among the three search spaces based on their current importance.
  - Quick check question: What is the difference between a contextual bandit and a regular multi-armed bandit?

## Architecture Onboarding

- Component map: Upstream GNN -> Graph embeddings -> Downstream model -> MBExplainer -> MCTS for subgraph pruning -> MCTS for node feature pruning -> MCTS for downstream feature pruning -> Contextual bandit

- Critical path:
  1. Train upstream GNN on graph data
  2. Train downstream model on graph embeddings + tabular features
  3. MBExplainer initializes explanation triple and MCTSs
  4. MCTSs iteratively prune their respective search spaces
  5. Contextual bandit allocates pruning budget among MCTSs
  6. MBExplainer outputs the final explanation triple

- Design tradeoffs:
  - Explanation quality vs. computational cost: More pruning iterations lead to better explanations but higher cost
  - Shapley value estimation vs. accuracy: Monte Carlo sampling is faster but less accurate than exact computation
  - Exploration vs. exploitation: Balancing between trying new pruning actions and exploiting known good ones

- Failure signatures:
  - Explanation does not improve with more pruning iterations: MCTS may be stuck in a local optimum
  - Contextual bandit consistently selects the same search space: Reward function may not be informative
  - Explanation includes irrelevant features: Shapley value estimation may be inaccurate

- First 3 experiments:
  1. Run MBExplainer on a small synthetic dataset with known ground truth to verify explanation quality
  2. Vary the pruning budget and measure the tradeoff between explanation quality and computational cost
  3. Compare MBExplainer explanations with those from other GNN explanation methods on a real-world dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MBExplainer's performance scale with increasing graph size and complexity compared to existing methods?
- Basis in paper: [inferred] The paper mentions computational challenges and acceleration strategies but does not provide systematic scaling analysis across different graph sizes.
- Why unresolved: The experimental results focus on specific datasets without varying graph sizes to demonstrate scaling behavior.
- What evidence would resolve it: Systematic experiments showing MBExplainer's runtime and accuracy across graphs of varying sizes and densities compared to baseline methods.

### Open Question 2
- Question: How sensitive is MBExplainer to hyperparameter choices like exploration-exploitation trade-off (c) and bandit algorithm parameters?
- Basis in paper: [explicit] The paper mentions hyperparameters like c and bandit settings but does not systematically analyze their impact on explanation quality.
- Why unresolved: The experiments use fixed hyperparameter values without sensitivity analysis or guidance on optimal parameter selection.
- What evidence would resolve it: Ablation studies showing how different hyperparameter values affect explanation quality and computational efficiency.

### Open Question 3
- Question: Can MBExplainer be extended to handle dynamic/temporal graphs where node/edge features change over time?
- Basis in paper: [explicit] The paper mentions temporal features in Remark 3.3 but does not explore this extension or its implications.
- Why unresolved: The current formulation assumes static graphs, and extending it to temporal scenarios requires additional theoretical and practical considerations.
- What evidence would resolve it: A modified MBExplainer framework that incorporates temporal dependencies and experiments demonstrating its effectiveness on dynamic graph datasets.

## Limitations
- Computational complexity in high-dimensional feature spaces may limit scalability
- Assumes cooperative game theory appropriately models explanation attribution
- Contextual bandit approach may not always identify optimal pruning decisions if Shapley values poorly correlate with explanation quality

## Confidence
- **High confidence**: The core algorithmic framework combining MCTS with contextual bandits is sound and well-defined
- **Medium confidence**: The Shapley value formulation for attribution is theoretically correct but may have approximation errors in practice
- **Medium confidence**: Empirical results show competitive performance but could benefit from larger-scale validation

## Next Checks
1. **Benchmark Comparison**: Systematically compare MBExplainer against state-of-the-art GNN explanation methods (GNNExplainer, PGExplainer) on multiple datasets to establish relative performance

2. **Ablation Study**: Remove the contextual bandit component and use uniform budget allocation to quantify the contribution of the global search strategy

3. **Scalability Analysis**: Test MBExplainer on graphs with 10K+ nodes and high-dimensional tabular features to identify computational bottlenecks and potential optimizations