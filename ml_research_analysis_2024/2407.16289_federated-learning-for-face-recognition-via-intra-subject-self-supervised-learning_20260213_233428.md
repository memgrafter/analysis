---
ver: rpa2
title: Federated Learning for Face Recognition via Intra-subject Self-supervised Learning
arxiv_id: '2407.16289'
source_url: https://arxiv.org/abs/2407.16289
tags:
- learning
- data
- recognition
- face
- fedfs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedFS, a federated learning framework for personalized
  face recognition using intra-subject self-supervised learning. The key idea is to
  leverage aggregated features of local and global models to cooperate with representations
  of an off-the-shelf model, employing adaptive soft label construction and intra-subject
  self-supervised learning.
---

# Federated Learning for Face Recognition via Intra-subject Self-supervised Learning

## Quick Facts
- **arXiv ID**: 2407.16289
- **Source URL**: https://arxiv.org/abs/2407.16289
- **Reference count**: 33
- **Primary result**: AUROC improvements of 16.7% and 9.7% on DigiFace-1M and VGGFace respectively using MobileFaceNet pre-trained model

## Executive Summary
This paper proposes FedFS, a federated learning framework for personalized face recognition that leverages intra-subject self-supervised learning. The method addresses the challenge of training face recognition models in federated settings where each client only has data for a single identity. By combining adaptive soft label construction, intra-subject self-supervised learning, and regularization losses, FedFS achieves superior performance compared to previous methods while protecting user data privacy.

## Method Summary
FedFS uses a three-model architecture consisting of a fixed pre-trained model, a global model aggregated across clients, and a personalized client model. The method employs intra-subject self-supervised learning with adaptive soft labels constructed using dot product operations, combined with a regularization loss that maintains similarity between global and personalized model outputs. The framework uses FedAvg for parameter aggregation with SGD optimizer, 2 local epochs, and 5 communication rounds per client. Experiments use datasets including MS-Celeb-1M for pre-training, and DigiFace-1M and VGGFace for federated learning evaluation.

## Key Results
- AUROC improvements of 16.7% on DigiFace-1M and 9.7% on VGGFace datasets
- Outperforms other federated learning approaches across various participation rate environments
- Superior TPIR@FPIR=0.001 performance compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
FedFS reduces intra-class variation by combining dot product and cosine similarity operations. The dot product captures magnitude and angular differences while cosine similarity normalizes and emphasizes angular similarity, allowing more robust training on positive-only data.

### Mechanism 2
Adaptive soft labels increase intra-subject representation quality by weighting similarity scores. Instead of hard labels, the model uses a weighted distribution of dot products that emphasizes self-similarity and reduces variance within identities.

### Mechanism 3
Regularization loss prevents overfitting and bias when training with only local data. A cosine similarity-based loss between global and personalized model outputs encourages shared structure while allowing personalization.

## Foundational Learning

- **Concept**: Contrastive learning with positive-only data
  - Why needed here: Clients only have data for one identity, so traditional contrastive methods with negative pairs cannot be applied directly
  - Quick check question: How does the model learn identity invariance without explicit negative examples?

- **Concept**: Federated averaging with heterogeneous data
  - Why needed here: Clients have unique identities and data distributions, so model personalization must be balanced with global generalization
  - Quick check question: What happens if some clients have very limited or low-quality data?

- **Concept**: Soft label adaptation in metric learning
  - Why needed here: Standard cross-entropy treats all positives equally, but adaptive weighting can emphasize harder or more reliable pairs
  - Quick check question: How does the choice of the hyperparameter K affect the distribution of adaptive soft labels?

## Architecture Onboarding

- **Component map**: Pre-trained model (fixed) -> Global model (aggregated) -> Personalized model (client-specific) -> Adaptive soft label module (dot product + weighting) -> Intra-subject self-supervised module (cosine similarity + regularization)

- **Critical path**:
  1. Local forward pass: compute embeddings from global, personalized, and pre-trained models
  2. Intra-subject loss: cosine similarity between global/personalized and pre-trained embeddings
  3. Adaptive soft label: dot product + top-K weighting
  4. Regularization: cosine similarity between global and personalized embeddings
  5. Parameter update and aggregation

- **Design tradeoffs**:
  - Personalization vs. generalization: too much personalization may hurt global model quality
  - Complexity vs. privacy: more sophisticated methods may require more client-side computation
  - Label weighting vs. stability: aggressive weighting may cause unstable training

- **Failure signatures**:
  - Degraded performance on unseen identities: overfitting to local data
  - High variance across clients: poor global model aggregation
  - Slow convergence: inappropriate learning rates or weightings

- **First 3 experiments**:
  1. Compare AUROC with and without adaptive soft labels on a small subset
  2. Vary the participation rate (0.1, 0.5, 0.9) and measure TPIR@FPIR=0.001
  3. Test sensitivity to the regularization weight Î» by sweeping 0.3, 0.5, 0.7, 0.9

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the hyperparameter K (ratio of batch size for adaptive soft label) affect the performance of FedFS?
- Basis in paper: The paper mentions K is set to 4 but does not explore other values or their impact on performance
- Why unresolved: The paper only provides results for a single K value, leaving the sensitivity of FedFS to this hyperparameter unexplored
- What evidence would resolve it: A systematic study varying K across a range of values and measuring the resulting performance on DigiFace-1M and VGGFace datasets would clarify the optimal K and its impact on FedFS

### Open Question 2
- Question: How does FedFS perform when clients have multiple identities instead of a single identity as assumed in the experiments?
- Basis in paper: The experiments assume each client has only one identity, which may not reflect real-world scenarios where clients often have multiple identities
- Why unresolved: The paper does not address the scenario of multiple identities per client, leaving the applicability of FedFS in more complex scenarios unclear
- What evidence would resolve it: Conducting experiments with clients having multiple identities and comparing the performance to the single-identity case would demonstrate FedFS's robustness to this more realistic scenario

### Open Question 3
- Question: How does the performance of FedFS scale with the number of participating clients?
- Basis in paper: The experiments use 10,000 clients, but it's unclear how FedFS would perform with significantly more or fewer clients
- Why unresolved: The paper does not explore the scalability of FedFS to different numbers of clients, which is crucial for understanding its applicability to large-scale federated learning scenarios
- What evidence would resolve it: Conducting experiments with varying numbers of clients, from small-scale to large-scale, and measuring the performance would reveal the scalability limits and optimal client count for FedFS

## Limitations
- The adaptive soft label mechanism relies on a K ratio hyperparameter without systematic sensitivity analysis across different K values
- The method's scalability to extremely large numbers of clients (beyond 10,000) or very low participation rates (<0.1) remains untested
- Performance may degrade if the global model provides a poor regularization signal due to highly diverse client identities

## Confidence
- **High confidence**: Claims about AUROC improvements (16.7% and 9.7%) on DigiFace-1M and VGGFace datasets with MobileFaceNet
- **Medium confidence**: Claims about FedFS outperforming other federated learning approaches in various participation rate environments
- **Medium confidence**: Claims about reduced computational complexity compared to previous methods

## Next Checks
1. Conduct a systematic ablation study varying the K ratio in adaptive soft labels (e.g., K=0.1, 0.3, 0.5, 0.7, 0.9) to quantify its impact on performance across different identity distributions
2. Test the method with participation rates below 0.1 and client counts exceeding 10,000 to evaluate scalability limits and identify performance degradation points
3. Implement and compare against alternative regularization strategies (e.g., knowledge distillation, contrastive regularization) to isolate the contribution of the specific cosine similarity-based regularization loss