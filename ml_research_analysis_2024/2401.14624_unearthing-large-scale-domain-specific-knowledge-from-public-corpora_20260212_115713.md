---
ver: rpa2
title: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora
arxiv_id: '2401.14624'
source_url: https://arxiv.org/abs/2401.14624
tags:
- data
- retrieve-pile
- language
- performance
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Retrieve-from-CC, a method for automatically
  collecting domain-specific data by leveraging LLMs to expand seed keywords into
  comprehensive queries, which are then used to retrieve relevant documents from Common
  Crawl. The authors construct a knowledge-rich dataset called Retrieve-Pile (735GB,
  188B tokens) covering STEM, humanities, social sciences, and miscellaneous domains.
---

# Unearthing Large Scale Domain-Specific Knowledge from Public Corpora

## Quick Facts
- arXiv ID: 2401.14624
- Source URL: https://arxiv.org/abs/2401.14624
- Authors: Zhaoye Fei; Yunfan Shao; Linyang Li; Zhiyuan Zeng; Conghui He; Qipeng Guo; Hang Yan; Dahua Lin; Xipeng Qiu
- Reference count: 40
- One-line primary result: Retrieve-Pile (735GB, 188B tokens) improves LLM performance on math reasoning (+6-11 points) and knowledge tasks (+4.3-13 points)

## Executive Summary
This paper introduces Retrieve-from-CC, a method for automatically collecting domain-specific knowledge from public corpora using LLM-guided query expansion and BM25 retrieval. The approach expands seed keywords into comprehensive queries through LLM question generation, then retrieves relevant documents from Common Crawl. The resulting dataset, Retrieve-Pile, covers STEM, humanities, social sciences, and miscellaneous domains with high educational value. When used to train Llama2 and Mistral models, it achieves significant improvements on mathematical reasoning and knowledge-related benchmarks.

## Method Summary
The method employs a two-stage pipeline: first, LLM-generated queries expand seed keywords into domain-relevant questions and reasoning procedures; second, BM25 retrieves top-1000 documents per query from Common Crawl. Retrieved data undergoes post-processing including quality filtering and deduplication before training Llama2-QoC and Mistral-QoC models for 50k steps. The approach balances efficiency (BM25 over dense retrieval) with automation (LLM query expansion) to scale to billions of documents while maintaining domain specificity.

## Key Results
- Retrieve-Pile achieves MATH score improvements of +6.1 to +11.0 points and GSM8K improvements of +8.3 to +11.0 points
- Knowledge-related tasks show gains of +4.3 points on MMLU and +13.0 points on AGIEval
- Dataset demonstrates high educational value with QuRating scores exceeding other open-source knowledge datasets
- Minimal data contamination (0.17-0.49 n-gram overlap) enables effective zero-shot performance

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Query expansion broadens semantic scope through LLM-generated questions
- Mechanism: LLMs generalize from seed keywords to generate diverse questions covering related concepts
- Core assumption: LLMs possess sufficient domain knowledge for meaningful query generation
- Evidence anchors: Abstract and section 3.2 mention generalization capability, but corpus lacks direct evidence of query quality

### Mechanism 2
- Claim: Thought generation retrieves reasoning procedures, not just facts
- Mechanism: LLM-generated reasoning paths serve as queries for documents containing cognitive processes
- Core assumption: Generated reasoning paths effectively retrieve documents with actual reasoning procedures
- Evidence anchors: Abstract mentions mining reasoning procedures, but corpus lacks empirical evidence of reasoning content proportion

### Mechanism 3
- Claim: Combined breadth and depth produce high educational value
- Mechanism: Dual-stage expansion (breadth via questions, depth via reasoning) from diverse sources yields comprehensive educational data
- Core assumption: Educational value maximizes when data covers both concepts and reasoning processes
- Evidence anchors: Corpus shows QuRating scores are highest, but correlation with learning outcomes is unestablished

## Foundational Learning
- Concept: BM25 ranking algorithm
  - Why needed here: Fast, effective relevance scoring for billions of documents without heavy computational resources
  - Quick check question: How does BM25 differ from TF-IDF, and why is it more suitable for document retrieval than simple keyword matching?
- Concept: Query expansion and relevance feedback
  - Why needed here: Expanding queries captures broader semantic content; LLM-generated queries provide implicit relevance feedback
  - Quick check question: What are the key differences between explicit and implicit relevance feedback, and which type does this method employ?
- Concept: Data contamination and n-gram overlap
  - Why needed here: Contamination analysis using n-gram overlap is crucial for understanding generalization capabilities
  - Quick check question: What are the limitations of using n-gram overlap as a contamination metric, and what false positives might it produce?

## Architecture Onboarding
- Component map: Seed keyword input → Query Expansion (LLM) → Query Pool → BM25 Retrieval → Post-processing (deduplication, quality filtering) → Retrieve-Pile dataset
- Critical path: Query expansion and retrieval stages are most critical; poor expansion leads to irrelevant retrieval, inefficient retrieval prevents scaling
- Design tradeoffs: BM25 chosen over dense retrieval for efficiency (speed vs accuracy tradeoff); LLM-generated queries accepted despite potential inaccuracies to enable automation
- Failure signatures: Low retrieval relevance, poor dataset diversity, excessive data contamination
- First 3 experiments:
  1. Test query expansion quality by manually evaluating a sample of LLM-generated queries against seed keywords for relevance and diversity
  2. Benchmark BM25 retrieval against a small dense retrieval baseline on a subset of documents to quantify the accuracy tradeoff
  3. Run contamination analysis on a small subset of downstream tasks to verify the n-gram overlap methodology and identify potential false positives

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the choice of retriever (BM25 vs. dense retrievers like DPR) impact the quality and diversity of the collected data in Retrieve-from-CC?
- Basis in paper: Authors acknowledge BM25 efficiency but note potential accuracy loss compared to dense retrievers as future research
- Why unresolved: No empirical comparisons between BM25 and dense retrievers on data quality and diversity
- What evidence would resolve it: Comparative study using both BM25 and DPR on same dataset with quality/diversity evaluation

### Open Question 2
- Question: What are the long-term effects of using LLM-generated queries with potential hallucinations on the model's performance and behavior?
- Basis in paper: Authors acknowledge potential hallucinations but argue they don't cause incorrect retrieval since data comes from public corpora
- Why unresolved: No investigation of whether hallucinatory queries influence reasoning patterns or introduce biases over time
- What evidence would resolve it: Longitudinal studies tracking performance and behavior changes with hallucinatory vs. verified queries

### Open Question 3
- Question: How does the performance improvement from Retrieve-Pile vary across different model architectures and sizes beyond Llama2 and Mistral?
- Basis in paper: Authors observe architecture-dependent improvement potential, with Llama2 showing greater gains than Mistral
- Why unresolved: Only two model families tested, leaving uncertainty about other architectures' responses
- What evidence would resolve it: Systematic testing on diverse model architectures and sizes measuring performance gains

## Limitations
- Lack of direct empirical evidence for quality and diversity of LLM-generated queries
- No ablation studies isolating contribution of query expansion versus retrieved data quality
- Educational value assessment doesn't establish correlation with actual learning outcomes
- Data contamination analysis using n-gram overlap may have false positives and misses semantic similarity

## Confidence
- **High Confidence**: Dataset size claims (735GB, 188B tokens) and downstream performance improvements on MATH (+6-11), GSM8K (+8-11), MMLU (+4.3), and AGIEval (+13)
- **Medium Confidence**: Effectiveness of two-stage query expansion mechanism and its contribution to educational value
- **Low Confidence**: Quality of LLM-generated reasoning procedures and their contribution to pedagogical value

## Next Checks
1. Conduct ablation study comparing models trained on Retrieve-Pile with and without reasoning-procedure component to quantify specific contribution
2. Perform human evaluation of random sample of LLM-generated queries and retrieved documents to assess quality, diversity, and relevance directly
3. Run controlled experiments using alternative query expansion methods (e.g., simpler keyword expansion or different LLM prompts) to establish optimal pipeline configuration