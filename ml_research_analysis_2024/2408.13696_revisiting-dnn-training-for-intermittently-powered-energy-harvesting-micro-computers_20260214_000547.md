---
ver: rpa2
title: Revisiting DNN Training for Intermittently-Powered Energy-Harvesting Micro-Computers
arxiv_id: '2408.13696'
source_url: https://arxiv.org/abs/2408.13696
tags:
- energy
- dropout
- power
- inference
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NExUME introduces a novel training and inference framework for
  DNNs in energy-harvesting wireless sensor networks. It uses DynFit, which dynamically
  adjusts dropout rates and quantization levels based on real-time energy availability
  during training, and DynInfer, an intermittency-aware task scheduler for efficient
  inference under power constraints.
---

# Revisiting DNN Training for Intermittently-Powered Energy-Harvesting Micro-Computers

## Quick Facts
- arXiv ID: 2408.13696
- Source URL: https://arxiv.org/abs/2408.13696
- Reference count: 40
- Key outcome: NExUME introduces a novel training and inference framework for DNNs in energy-harvesting wireless sensor networks, achieving 6-22% accuracy improvements over state-of-the-art methods with less than 5% additional compute overhead.

## Executive Summary
This paper introduces NExUME, a comprehensive framework for training and deploying Deep Neural Networks (DNNs) in intermittently-powered energy-harvesting wireless sensor networks. The framework addresses the challenge of unreliable power supply by integrating energy variability awareness into both training (DynFit) and inference (DynInfer) phases. DynFit dynamically adjusts dropout rates and quantization levels based on real-time energy availability during training, while DynInfer performs intermittency-aware task scheduling to optimize inference under power constraints. The approach was evaluated across multiple public datasets and a new machine status monitoring dataset, demonstrating superior energy efficiency and accuracy improvements on various hardware platforms.

## Method Summary
NExUME consists of three main components: DynNAS for neural architecture search, DynFit for energy-aware training optimization, and DynInfer for intermittency-aware inference scheduling. The core innovation lies in DynFit's dynamic adjustment of dropout rates and quantization levels proportional to real-time energy availability, and DynInfer's task fusion mechanism that combines smaller tasks into larger atomic units when energy constraints require it. The framework was implemented on multiple hardware platforms including TI MSP430FR5994, Arduino Nano 33 BLE Sense, ESP32 S3 Eye, STM32H7, and Raspberry Pi Pico, using both public datasets and a new machine status monitoring dataset for industrial predictive maintenance.

## Key Results
- Achieves 6-22% accuracy improvements over state-of-the-art methods for DNNs in energy-harvesting environments
- Maintains less than 5% additional compute overhead during training
- Demonstrates superior energy efficiency (MOps/Joule) across different hardware platforms and energy sources
- Successfully evaluated on multiple datasets including a new machine status monitoring dataset for industrial predictive maintenance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DynFit enables energy-aware training by dynamically adjusting dropout rates and quantization levels in response to real-time energy availability.
- Mechanism: Dropout rate di for each layer i is calculated as di = dmax * (1 - Eb/Emax), where Eb is the current available energy and Emax is the maximum energy observed. Similarly, quantization levels qj are adjusted proportionally to energy availability.
- Core assumption: The DNN can learn effectively with variable dropout rates and quantization levels during training without catastrophic performance degradation.
- Evidence anchors:
  - [abstract] "We propose a dynamic dropout technique that adapts to both the architecture of the device and the variability in energy availability"
  - [section] "Our proposed approach leverages a device model that incorporates specific parameters of the network architecture and the energy harvesting profile to optimize dropout rates dynamically during the training phase"
  - [corpus] Weak - corpus papers focus on federated learning and scheduling but don't address dynamic training adjustments based on energy availability
- Break condition: If the model fails to converge or accuracy drops significantly when dropout rates and quantization levels vary too widely during training.

### Mechanism 2
- Claim: DynInfer enables efficient inference under intermittent power by fusing tasks to minimize checkpointing overhead.
- Mechanism: When the energy required for executing multiple QuantaTasks exceeds the available energy budget, DynInfer fuses smaller tasks into larger atomic units that can be executed within the energy constraints. This is formalized as finding a partition of QuantaTasks into subsets where the sum of energy requirements is ≤ Eb, and minimizing the number of subsets.
- Core assumption: Task fusion reduces checkpointing overhead without introducing significant computational delays or memory constraints.
- Evidence anchors:
  - [section] "When the energy required for executing multiple QuantaTasks exceeds the available energy budget, DynInfer employs task fusion to combine smaller tasks into larger atomic units"
  - [section] "This reduces the number of checkpoints and the overhead associated with task switching"
  - [corpus] Weak - corpus papers discuss scheduling but don't specifically address task fusion to minimize checkpointing overhead in intermittent systems
- Break condition: If task fusion creates tasks too large to fit in available memory or if the overhead of managing fused tasks exceeds the benefits of reduced checkpointing.

### Mechanism 3
- Claim: The adaptive regularization strategy in DynFit prevents overfitting/undertraining caused by uneven weight updates due to dynamic dropout and quantization.
- Mechanism: DynFit monitors the update frequency Fp of each weight wp over a window of T iterations. Weights with Fp < θlow are considered under-trained, and those with Fp > θhigh are considered overfitting. Dropout rates and L2 regularization are adjusted accordingly.
- Core assumption: Monitoring weight update frequencies can effectively identify and correct training imbalances caused by dynamic adjustments.
- Evidence anchors:
  - [section] "We monitor the update frequency Fp of each weight wp over a window of T iterations: Fp = 1/T * sum(Up(t))"
  - [section] "Weights with Fp < θlow are considered under-trained, and those with Fp > θhigh are considered overfitting"
  - [corpus] Weak - corpus papers don't discuss adaptive regularization strategies for handling uneven weight updates in intermittent training scenarios
- Break condition: If the regularization adjustments fail to balance training across all weights or if the monitoring overhead becomes prohibitive.

## Foundational Learning

- Concept: Intermittent computing and energy harvesting systems
  - Why needed here: NExUME is specifically designed for DNNs operating in environments with intermittent power supply from energy harvesting sources. Understanding how these systems work is fundamental to grasping the problem space and solution approach.
  - Quick check question: What are the key components of a typical energy harvesting setup and how do they interact to provide power to a compute unit?

- Concept: Deep Neural Network training fundamentals
  - Why needed here: DynFit modifies standard DNN training by introducing dynamic dropout and quantization. Understanding standard training procedures, loss functions, and optimization techniques is essential to comprehend how DynFit's modifications work.
  - Quick check question: How do dropout and quantization typically function in DNN training, and what changes when they're made dynamic based on energy availability?

- Concept: Task scheduling and checkpointing in embedded systems
  - Why needed here: DynInfer's task scheduling and task fusion mechanisms are built on concepts from embedded systems programming. Understanding how tasks are scheduled, how checkpointing works, and how to manage state in resource-constrained environments is crucial.
  - Quick check question: What is the purpose of checkpointing in intermittent computing systems, and how does task fusion help reduce checkpointing overhead?

## Architecture Onboarding

- Component map: NExUME consists of DynNAS -> DynFit -> DynInfer, where DynNAS finds the best architecture for the intermittent environment, DynFit trains the network considering energy intermittency, and DynInfer performs inference under intermittent power conditions.
- Critical path: The critical path for implementing NExUME involves: (1) Implementing the energy-aware training loop in DynFit with dynamic dropout and quantization adjustments, (2) Implementing the task scheduling and fusion logic in DynInfer, and (3) Integrating these components with the chosen hardware platform's energy monitoring and checkpointing mechanisms.
- Design tradeoffs: The main tradeoffs involve balancing accuracy improvements against computational overhead (DynFit adds ≤5% overhead), managing the complexity of dynamic adjustments versus the simplicity of static configurations, and choosing between more frequent but smaller checkpoints versus fewer but larger ones through task fusion.
- Failure signatures: Key failure modes include: (1) Training failure due to dropout rates or quantization levels becoming too extreme, (2) Inference failure due to task fusion creating unmanageably large tasks, (3) Energy monitoring inaccuracies leading to incorrect adjustments, and (4) Checkpoint/restoration failures causing data corruption.
- First 3 experiments:
  1. Implement DynFit's dynamic dropout mechanism on a simple DNN (e.g., MNIST classification) with simulated energy traces, comparing accuracy and convergence to static dropout.
  2. Implement task fusion in DynInfer on a benchmark DNN inference workload, measuring checkpointing overhead reduction versus task management overhead.
  3. Integrate DynFit and DynInfer on actual energy harvesting hardware (e.g., TI MSP430FR5994), running a complete training-inference cycle and measuring end-to-end accuracy and energy efficiency improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for determining when to fuse multiple QuantaTasks to minimize checkpointing overhead without exceeding the available energy budget?
- Basis in paper: [inferred] The paper mentions task fusion as a strategy when energy required for multiple QuantaTasks exceeds the available energy budget, but does not provide a specific algorithm or decision criteria for when to fuse tasks.
- Why unresolved: The paper states that task fusion is used when the sum of energy requirements exceeds the budget, but does not specify how to decide which tasks to fuse or how many tasks to fuse together to optimize performance.
- What evidence would resolve it: Empirical studies comparing different task fusion strategies (e.g., greedy, dynamic programming) showing their impact on checkpointing overhead, task completion rate, and energy efficiency would provide evidence for the optimal strategy.

### Open Question 2
- Question: How does the adaptive regularization strategy (monitoring update frequencies and adjusting dropout rates) perform under different types of energy intermittency patterns (e.g., periodic vs. random)?
- Basis in paper: [explicit] The paper introduces an adaptive regularization strategy that monitors weight update frequencies and adjusts dropout rates to prevent overfitting and under-training, but does not evaluate its performance under different energy intermittency patterns.
- Why unresolved: The paper describes the strategy but does not provide results on how it performs under varying energy patterns, which is crucial for understanding its robustness in real-world scenarios.
- What evidence would resolve it: Experiments comparing the adaptive regularization strategy's performance under different energy intermittency patterns (e.g., periodic, random, bursty) would show its effectiveness and limitations in handling various types of energy fluctuations.

### Open Question 3
- Question: What are the trade-offs between using different dropout strategies (L2 Dynamic Dropout, Optimal Brain Damage Dropout, Feature Map Reconstruction Error Dropout, etc.) in terms of accuracy, energy efficiency, and computational overhead?
- Basis in paper: [explicit] The paper introduces multiple dropout strategies but does not provide a comprehensive comparison of their trade-offs.
- Why unresolved: While the paper presents different dropout strategies, it does not evaluate their relative performance in terms of accuracy, energy efficiency, and computational overhead, which are critical factors for choosing the best strategy.
- What evidence would resolve it: A comparative study of all introduced dropout strategies under the same experimental conditions, measuring their accuracy, energy efficiency, and computational overhead, would provide insights into their trade-offs and help identify the most suitable strategy for different scenarios.

## Limitations

- The adaptive regularization strategy lacks specific parameter tuning guidance and empirical validation of its effectiveness across different energy patterns
- Energy monitoring integration between hardware and software layers is only briefly mentioned without discussing potential accuracy limitations or calibration requirements
- Task fusion algorithm's complexity analysis and memory overhead implications are not provided, making it difficult to assess scalability on resource-constrained devices

## Confidence

- DynFit mechanism claims (6-22% accuracy improvements): **Medium** - The theoretical framework is sound, but validation on diverse hardware platforms and energy conditions is limited to the authors' own experiments
- DynInfer efficiency claims (<5% compute overhead): **Medium** - While the concept is valid, the overhead measurement methodology and worst-case scenarios are not fully explored
- End-to-end energy efficiency claims: **Low** - The paper presents promising results but lacks comprehensive comparison across different energy harvesting scenarios and device types

## Next Checks

1. Implement the weight update frequency monitoring system and validate that it correctly identifies under-trained and overfit weights across different energy profiles and training phases
2. Conduct a memory overhead analysis of the task fusion algorithm on resource-constrained devices, measuring actual memory usage versus the theoretical energy savings
3. Test the framework's robustness to energy monitoring inaccuracies by introducing controlled errors in energy availability measurements and measuring impact on training convergence and inference accuracy