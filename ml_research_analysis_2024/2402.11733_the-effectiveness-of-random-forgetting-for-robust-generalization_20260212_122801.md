---
ver: rpa2
title: The Effectiveness of Random Forgetting for Robust Generalization
arxiv_id: '2402.11733'
source_url: https://arxiv.org/abs/2402.11733
tags:
- robust
- forgetting
- fomo
- training
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FOMO introduces a novel adversarial training paradigm inspired
  by active forgetting in the brain to address robust overfitting. It alternates between
  forgetting and relearning phases: a subset of later-layer weights are randomly reinitialized
  to regulate information flow, followed by relearning with consistency regularization
  from an exponentially averaged stable model.'
---

# The Effectiveness of Random Forgetting for Robust Generalization

## Quick Facts
- arXiv ID: 2402.11733
- Source URL: https://arxiv.org/abs/2402.11733
- Authors: Vijaya Raghavan T Ramkumar; Bahram Zonooz; Elahe Arani
- Reference count: 40
- One-line primary result: FOMO significantly reduces the gap between best and last robust test accuracy while improving final robust performance across multiple datasets.

## Executive Summary
FOMO introduces a novel adversarial training paradigm inspired by active forgetting in the brain to address robust overfitting. It alternates between forgetting and relearning phases: a subset of later-layer weights are randomly reinitialized to regulate information flow, followed by relearning with consistency regularization from an exponentially averaged stable model. This cyclic process mitigates overfitting and improves generalization. FOMO significantly reduces the gap between best and last robust test accuracy (e.g., from -7.88% to -0.22% on CIFAR-10) while improving final robust performance over baselines like PGD-AT, TRADES, KD+SWA, AWP, and TE. It is robust to AutoAttacks, natural corruptions, and perturbations across ℓ∞ and ℓ2 norms, converging to flatter minima and maintaining computational efficiency. FOMO scales well across CIFAR-10/100, SVHN, and Tiny-ImageNet, demonstrating strong and consistent robustness.

## Method Summary
FOMO is a three-phase training paradigm designed to address robust overfitting in adversarial training. During the forgetting phase, a random subset of later-layer weights are reinitialized to prevent memorization. The consolidation phase updates an exponential moving average of the network weights to maintain stable knowledge. The relearning phase trains with adversarial examples and consistency regularization, guided by the stable model. This cycle repeats, with the stable model serving as long-term memory that consolidates generalizable features. FOMO is trained for 200 epochs with SGD, learning rate decay, and standard data augmentation.

## Key Results
- FOMO reduces the gap between best and last robust test accuracy from -7.88% to -0.22% on CIFAR-10.
- FOMO achieves 54.52% robust accuracy on CIFAR-10 compared to 50.45% for PGD-AT, improving the trade-off between standard and robust accuracy.
- FOMO demonstrates improved robustness to AutoAttacks, natural corruptions, and ℓ2 perturbations while converging to flatter minima.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FOMO reduces robust overfitting by periodically forgetting a subset of later-layer weights and relearning with consistency regularization.
- **Mechanism**: The forgetting phase reinitializes a random subset of parameters in later layers, preventing them from overfitting to training data. The relearning phase uses consistency regularization with a stable model (exponential moving average) to consolidate generalizable features and guide the network toward flatter minima.
- **Core assumption**: Later layers are more prone to memorization and overfitting, while earlier layers capture more generalizable features. Randomly reinitializing later-layer weights disrupts memorization patterns without destroying generalizable representations.
- **Evidence anchors**:
  - [abstract]: "FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features."
  - [section 3.1]: "we limit the forgetting process to the later layers of the network... these layers have more capacity to memorize and tend to overfit the training data"
  - [corpus]: Weak evidence - no direct corpus references to layer-wise forgetting in adversarial training
- **Break condition**: If forgetting occurs in earlier layers instead of later layers, or if the percentage of forgotten weights is too high (>50%), the network may lose critical generalizable features and performance will degrade.

### Mechanism 2
- **Claim**: FOMO achieves robust generalization by maintaining a stable model that consolidates knowledge across relearning phases.
- **Mechanism**: After each relearning phase, an exponential moving average of the network weights is stored in a stable model. This stable model serves as long-term memory that captures generalized representations, while the main network undergoes forgetting and relearning cycles.
- **Core assumption**: The stable model can effectively consolidate generalizable knowledge across multiple relearning cycles without being affected by the forgetting operations applied to the main network.
- **Evidence anchors**:
  - [abstract]: "alternates between the forgetting phase... and the relearning phase... followed by relearning with consistency regularization from an exponentially averaged stable model"
  - [section 3.2]: "we intend to consolidate this information after each relearning step by employing another network fϕ called stable model similar to fθ. The knowledge learned by fθ is consolidated in the stable model"
  - [corpus]: No direct corpus evidence found for this specific stable model approach in adversarial training
- **Break condition**: If the decay parameter αc is too high (>0.999), the stable model may not update quickly enough to capture new generalizable features. If too low (<0.99), it may not provide stable guidance.

### Mechanism 3
- **Claim**: FOMO improves robustness against natural corruptions and various perturbation norms by learning flatter minima through the forgetting-relearning-consolidation cycle.
- **Mechanism**: The alternating cycles of forgetting and relearning prevent the network from converging to sharp minima that are sensitive to perturbations. The consistency regularization with the stable model further encourages flatter minima by providing stable supervision.
- **Core assumption**: Flatter minima in the loss landscape correlate with better generalization and robustness to both adversarial and natural perturbations.
- **Evidence anchors**:
  - [section 9]: "Our method demonstrates significantly reduced sensitivity to perturbations... suggesting that the FOMO solution may reside within flatter local minima"
  - [section 7]: "Models are trained on clean images and tested on CIFAR-10-C... The results reveal that the mCA consistently improves with FOMO"
  - [corpus]: No direct corpus evidence for forgetting-relearning cycles leading to flatter minima in adversarial training
- **Break condition**: If the forgetting frequency is too high or the relearning duration too short, the network may not have sufficient time to consolidate generalizable features before the next forgetting phase.

## Foundational Learning

- **Concept**: Adversarial training and robust overfitting
  - Why needed here: FOMO is specifically designed to address robust overfitting in adversarial training, so understanding this phenomenon is essential for grasping the problem FOMO solves
  - Quick check question: What is the key difference between standard overfitting and robust overfitting in adversarial training?

- **Concept**: Exponential moving average and model ensembling
  - Why needed here: The stable model in FOMO uses exponential moving average to consolidate knowledge, similar to techniques like SWA (Stochastic Weight Averaging)
  - Quick check question: How does exponential moving average differ from simple averaging of model checkpoints?

- **Concept**: Kullback-Leibler divergence and consistency regularization
  - Why needed here: FOMO uses KL divergence between the current model and stable model as a consistency regularization term during relearning
  - Quick check question: What does KL divergence measure between two probability distributions, and why is it useful for consistency regularization?

## Architecture Onboarding

- **Component map**: Main network (fθ) -> Forgetting phase -> Relearning phase -> Stable model (fϕ) -> Consistency regularization -> Improved generalization
- **Critical path**: Training loop → Warm-up phase → [Forgetting phase → Relearning phase → Consolidation] repeated → Inference using stable model
- **Design tradeoffs**:
  - Forgetting percentage vs. relearning duration: Higher forgetting requires longer relearning
  - Stable model decay rate: Balances between stability and adaptability
  - Layer selection for forgetting: Later layers more prone to overfitting but also more expressive
- **Failure signatures**:
  - If robust test accuracy drops significantly during forgetting phase: Forgetting percentage may be too high
  - If robust test accuracy plateaus early: Stable model decay rate may be too low
  - If computational cost becomes prohibitive: Forgetting frequency may be too high
- **First 3 experiments**:
  1. Implement FOMO with 3.5% forgetting on PreAct-ResNet18 on CIFAR-10, compare best/last robust accuracy against PGD-AT
  2. Vary forgetting percentage (1%, 3.5%, 10%) while keeping relearning epochs fixed at 5, measure impact on robust overfitting
  3. Test FOMO's robustness to ℓ2 perturbations by modifying the perturbation norm in adversarial training and evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal percentage of parameters to forget and the duration of the relearning phase for different network architectures and datasets?
- Basis in paper: [inferred] The paper investigates the impact of forgetting and relearning on overfitting, but does not provide a definitive answer on the optimal parameters for different architectures and datasets.
- Why unresolved: The optimal parameters for forgetting and relearning are likely to vary depending on the specific architecture and dataset, and the paper does not provide a comprehensive study on this.
- What evidence would resolve it: A systematic study across various architectures and datasets, testing different percentages of forgotten parameters and relearning phase durations, would provide insights into the optimal parameters.

### Open Question 2
- Question: How does the forgetting process affect the interpretability and explainability of deep neural networks?
- Basis in paper: [explicit] The paper introduces a novel learning paradigm called "Forget to Mitigate Overfitting (FOMO)" which involves forgetting and relearning phases.
- Why unresolved: The paper focuses on the effectiveness of FOMO in mitigating overfitting and improving robustness, but does not delve into how the forgetting process impacts the interpretability and explainability of the networks.
- What evidence would resolve it: An analysis of the feature representations learned by the network before and after the forgetting process, and how these changes affect the network's decision-making process, would provide insights into the interpretability and explainability.

### Open Question 3
- Question: Can the FOMO approach be extended to other types of neural networks beyond convolutional neural networks, such as recurrent neural networks or transformers?
- Basis in paper: [explicit] The paper primarily focuses on the application of FOMO to convolutional neural networks (CNNs) for image classification tasks.
- Why unresolved: The paper does not explore the applicability of FOMO to other types of neural networks, such as recurrent neural networks (RNNs) or transformers, which are widely used in other domains like natural language processing and time series analysis.
- What evidence would resolve it: Experiments applying FOMO to different types of neural networks and evaluating its effectiveness in mitigating overfitting and improving robustness in these domains would provide insights into its generalizability.

## Limitations

- The layer selection criterion for forgetting (using 3.5% of later-layer parameters) is empirically chosen rather than theoretically justified, with no ablation studies on different layer selection strategies.
- The mechanism by which forgetting later layers specifically addresses robust overfitting, while earlier layers maintain generalizable features, remains unproven with no direct empirical evidence supporting this architectural hypothesis.
- The interaction between forgetting percentage, relearning duration, and stable model decay rate is not thoroughly explored, leaving hyperparameter tuning as a potential performance bottleneck.

## Confidence

- **High**: FOMO significantly reduces the gap between best and last robust test accuracy across multiple datasets and architectures. The empirical improvements over strong baselines are consistent and substantial.
- **Medium**: FOMO's effectiveness against natural corruptions and ℓ2 perturbations is demonstrated, but the underlying mechanism (flatter minima) is inferred rather than directly measured.
- **Low**: The claim that later layers are more prone to memorization than earlier layers lacks direct empirical support within the paper.

## Next Checks

1. Conduct ablation studies varying the forgetting percentage (1%, 3.5%, 10%) and measure its impact on robust overfitting across different architectures to establish optimal hyperparameter ranges.
2. Perform layer-wise analysis to empirically verify whether forgetting earlier layers degrades performance while later-layer forgetting improves it, directly testing the paper's architectural hypothesis.
3. Measure and compare loss landscape curvature (e.g., using PAC-Bayes bounds or eigenvalue analysis) between FOMO and baseline models to provide direct evidence for the flatter minima claim.