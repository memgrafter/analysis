---
ver: rpa2
title: Variational Search Distributions
arxiv_id: '2409.06142'
source_url: https://arxiv.org/abs/2409.06142
tags:
- variational
- optimization
- distribution
- fitness
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Variational Search Distributions (VSD) is a method for efficiently
  finding rare, desirable designs in high-dimensional, discrete combinatorial spaces
  through sequential batch experiments. The method casts the search problem as variational
  inference, optimizing a lower bound on the expected log-probability of improvement
  while matching a learned generative distribution to a prior.
---

# Variational Search Distributions

## Quick Facts
- arXiv ID: 2409.06142
- Source URL: https://arxiv.org/abs/2409.06142
- Authors: Daniel M. Steinberg; Rafael Oliveira; Cheng Soon Ong; Edwin V. Bonilla
- Reference count: 14
- Primary result: Variational Search Distributions (VSD) efficiently finds rare, desirable designs in high-dimensional, discrete combinatorial spaces through sequential batch experiments

## Executive Summary
Variational Search Distributions (VSD) addresses the challenge of finding rare, desirable designs in high-dimensional discrete spaces through sequential batch experiments. The method casts the search problem as variational inference, optimizing a lower bound on the expected log-probability of improvement while matching a learned generative distribution to a prior. VSD uses gradient-based optimization and can leverage scalable predictive models like CNNs, making it suitable for high-throughput experimental settings.

The method demonstrates competitive performance across multiple protein and DNA/RNA engineering problems, achieving precision scores of 0.2-0.4 and recall scores of 0.1-0.3 on fitness landscape tasks. For black-box optimization of high-dimensional AA V and GFP datasets, VSD achieved simple regret values of 0.1-0.2 in final experimental rounds. The approach consistently outperforms baseline methods including random sampling, AdaLead, and PEX, particularly on higher-dimensional problems.

## Method Summary
VSD implements sequential design optimization by treating the search for desirable designs as a variational inference problem. The method optimizes a lower bound (ELBO) on the expected log-probability of improvement while maintaining a connection to the prior distribution through KL regularization. A class probability estimator (CPE) predicts the likelihood of designs being desirable, enabling the use of scalable models like CNNs instead of traditional Gaussian process surrogates. The variational distribution is optimized using score function gradient estimators, and batch sequential sampling allows efficient exploration while maintaining diversity in recommendations.

## Key Results
- Achieved precision scores of 0.2-0.4 and recall scores of 0.1-0.3 across multiple protein and DNA/RNA engineering problems
- Simple regret values of 0.1-0.2 in final experimental rounds for black-box optimization of high-dimensional AA V and GFP datasets
- Consistently outperformed baseline approaches including random sampling, AdaLead, and PEX, particularly on higher-dimensional problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variational inference is effective for sequential design optimization because it directly optimizes the lower bound on the expected log-probability of improvement while maintaining a connection to the prior.
- Mechanism: The ELBO objective `Eq(x|ϕ)[log αPI(x, DN, τ)] - DKL[q(x|ϕ)‖p(x|D0)]` balances exploration (KL term) and exploitation (expected log-PI term), and gradient-based optimization allows iterative refinement of the generative model.
- Core assumption: The variational family q(x|ϕ) is expressive enough to approximate the true posterior over desirable designs.
- Evidence anchors:
  - [abstract] "VSD uses off-the-shelf gradient based optimization routines, can learn powerful generative models for desirable designs"
  - [section 2.2] "The difficulty is that we cannot directly evaluate or empirically sample from p(x|y > τ). However, if we consider the reverse Kullback-Leibler (KL) divergence... we recognize the right hand side of Equation 3 as the well known (negative) variational evidence lower bound (ELBO)"
  - [corpus] Weak evidence - related papers focus on variational autoencoders and diffusion models but not directly on this active search formulation.
- Break condition: If the variational family is too restrictive, the method cannot recover the true posterior, leading to poor performance.

### Mechanism 2
- Claim: Batch sequential sampling with VSD allows efficient exploration of high-dimensional discrete spaces while maintaining diversity in recommendations.
- Mechanism: By drawing B samples from q(x|ϕ*) each round and updating with new experimental data, VSD can cover the space more effectively than single-point methods, and the KL regularization prevents collapse to a single mode.
- Core assumption: The batch size B is large enough to capture meaningful diversity but small enough to be experimentally feasible.
- Evidence anchors:
  - [abstract] "VSD can outperform existing baseline methods on a set of real sequence-design problems in various biological systems"
  - [section 2.2] "we find that we only need O(1000) samples to estimate the required expectations for ELBO optimization on problems with m = O(100)"
  - [corpus] Missing - related papers don't discuss batch sequential sampling in this specific context.
- Break condition: If B is too small, diversity suffers; if too large, experimental budget constraints are violated.

### Mechanism 3
- Claim: Using class probability estimation (CPE) instead of surrogate models provides scalability and flexibility for high-throughput experiments.
- Mechanism: CPE directly estimates Pr(y > τ |x, DN) using a proper scoring rule, enabling use of scalable models like CNNs instead of GPs, which is crucial for high-dimensional problems.
- Core assumption: CPE models can accurately estimate the probability of improvement without requiring full predictive distributions.
- Evidence anchors:
  - [section 2.3] "We do this with class probability estimation (CPE) on the labels z := 1[y > τ] ∈ {0, 1} so Pr(y > τ |x, DN) = p(z = 1|x, DN) ≈ πθ(x)"
  - [section 2.3] "Using a CPE also opens up the choice of estimators that are more scalable than a GP surrogate, satisfying our last desiderata (D4)"
  - [corpus] Weak evidence - related papers focus on variational autoencoders and diffusion models but don't discuss CPE in this specific optimization context.
- Break condition: If the CPE cannot accurately estimate class probabilities, the VSD objective becomes unreliable.

## Foundational Learning

- Concept: Variational Inference and Evidence Lower Bound (ELBO)
  - Why needed here: VSD is built on variational inference principles, using ELBO as the optimization objective to approximate the posterior over desirable designs.
  - Quick check question: Why do we use the reverse KL divergence in VSD instead of the forward KL divergence?

- Concept: Black-Box Optimization and Acquisition Functions
  - Why needed here: VSD uses probability of improvement (PI) as an acquisition function, which requires understanding of Bayesian optimization concepts and their relationship to the VSD objective.
  - Quick check question: How does the expected log-PI term in the ELBO relate to traditional acquisition functions in Bayesian optimization?

- Concept: Score Function Gradient Estimation
  - Why needed here: Since the variational distribution is over discrete spaces, reparameterization tricks cannot be used, requiring score function estimators for gradient computation.
- Why needed here: The score function estimator allows gradient-based optimization of the ELBO even with discrete random variables.
  - Quick check question: What are the advantages and disadvantages of using score function estimators compared to reparameterization tricks?

## Architecture Onboarding

- Component map: Prior distribution p(x|D0) -> Class probability estimator πθ(x) -> Variational distribution q(x|ϕ) -> Experimental black-box function f0(x) -> ELBO objective

- Critical path: 1. Initialize CPE with training data 2. Optimize variational parameters using score function gradients 3. Sample batch of designs from optimized variational distribution 4. Evaluate designs experimentally 5. Update training data and repeat

- Design tradeoffs:
  - Independent vs auto-regressive variational distributions - independence is simpler but may miss dependencies
  - CPE vs GP surrogate - CPE scales better but may be less accurate for small datasets
  - Fixed vs adaptive thresholds - fixed is simpler but adaptive can focus search more effectively

- Failure signatures:
  - Variational distribution collapses to single mode - check KL regularization strength
  - Poor exploration of design space - verify batch size and prior specification
  - Slow convergence - examine gradient estimator variance and learning rate

- First 3 experiments:
  1. Small combinatorial problem (e.g., TFBIND8) with fixed threshold to verify basic functionality
  2. Medium-dimensional problem (e.g., AA V) with adaptive threshold to test scaling
  3. High-dimensional problem (e.g., GFP) with auto-regressive variational distribution to test expressiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the asymptotic convergence rates for learning the true conditional generative distribution of designs under different configurations of VSD?
- Basis in paper: [explicit] The paper states "We derive asymptotic convergence rates for learning the true conditional generative distribution of designs with certain configurations of our method."
- Why unresolved: The paper mentions deriving these rates but does not provide the actual mathematical results or proofs.
- What evidence would resolve it: The paper should include the formal theorems and proofs showing the convergence rates under various prior, variational, and surrogate model configurations.

### Open Question 2
- Question: How does VSD's performance scale with sequence length and alphabet size beyond the tested ranges?
- Basis in paper: [inferred] The experiments tested sequences with lengths up to 237 and alphabet sizes of 4 or 20, but these may not represent the full scalability limits.
- Why unresolved: The paper only tested specific ranges and did not explore the method's behavior on extremely long sequences or very large alphabets.
- What evidence would resolve it: Systematic experiments varying sequence length and alphabet size across orders of magnitude, measuring precision, recall, and computational efficiency.

### Open Question 3
- Question: What is the impact of using different divergence measures (beyond KL divergence) in the VSD objective?
- Basis in paper: [inferred] The paper focuses on KL divergence but mentions "For a divergence D : P(X) × P(X) → [0, ∞)" suggesting other divergences could be used.
- Why unresolved: The paper only empirically evaluates KL divergence and does not explore alternative divergence measures.
- What evidence would resolve it: Comparative experiments using different divergences (e.g., Rényi, Wasserstein) in the VSD objective, measuring their effects on convergence speed and solution quality.

## Limitations
- Requires complete or near-complete combinatorial data for fitness landscape tasks, limiting applicability to scenarios with sparse data
- Performance degrades on very high-dimensional problems where sampling O(1000) candidates becomes computationally expensive
- Method requires careful tuning of CPE architecture and threshold scheduling, with optimal settings being problem-dependent

## Confidence
- Claim: VSD can outperform existing baseline methods
  - Confidence: Medium (based on precision/recall scores from 8 fitness landscape tasks and 2 black-box optimization problems)
- Claim: Batch sequential sampling with VSD allows efficient exploration
  - Confidence: High (consistent improvement over single-point methods demonstrated experimentally)
- Claim: CPE-based approach provides scalability for high-throughput experiments
  - Confidence: Medium (theoretical justification provided but limited empirical validation on extremely large-scale problems)

## Next Checks
1. Test VSD on fitness landscape tasks with sparse data (10-20% coverage) to verify if the CPE-based approach maintains performance when the ground truth is incomplete.

2. Implement ablation studies removing the KL regularization term to quantify its contribution to preventing mode collapse and maintaining diversity in generated candidates.

3. Scale VSD to ultra-high-dimensional design spaces (m > 500) to identify practical limits of the current implementation and determine if alternative variational families are needed.