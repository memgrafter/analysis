---
ver: rpa2
title: Minor DPO reject penalty to increase training robustness
arxiv_id: '2408.09834'
source_url: https://arxiv.org/abs/2408.09834
tags:
- reject
- preference
- rewards
- learning
- margin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a stability issue in Direct Preference Optimization
  (DPO) for aligning large language models to human preferences. The authors identify
  that DPO's symmetric treatment of chosen and rejected samples leads to over-penalizing
  reject samples, potentially causing model degeneration.
---

# Minor DPO reject penalty to increase training robustness

## Quick Facts
- arXiv ID: 2408.09834
- Source URL: https://arxiv.org/abs/2408.09834
- Reference count: 2
- Key outcome: MinorDPO improves DPO stability by reducing excessive penalty on reject samples, achieving higher GSM8K scores particularly at higher learning rates

## Executive Summary
This paper addresses a stability issue in Direct Preference Optimization (DPO) where symmetric treatment of chosen and rejected samples leads to over-penalizing reject samples, potentially causing model degeneration. The authors propose MinorDPO, which modifies the loss function by adding a constraint that prevents excessive penalty on reject samples when their log probabilities are already below the reference model. Experiments on MetaMath dataset using Qwen1.5-7B-Chat base model show MinorDPO outperforms standard DPO in most settings and achieves higher scores on GSM8K test set, particularly when using higher learning rates.

## Method Summary
MinorDPO addresses DPO's stability issue by modifying the reject sample penalty in the loss function. Instead of applying the full penalty term to all reject samples, MinorDPO uses a maximum function that only applies penalty when necessary. Specifically, the reject sample penalty is replaced with max(0, log πθ(yl|x)/πref(yl|x)), which ensures that no penalty is applied when the reject sample's log probability is already lower than the reference model. This modification reduces the risk of over-penalizing reject samples while maintaining the alignment objective. The method is implemented using LLaMa-Factory framework with Qwen1.5-7B-Chat base model, trained on MetaMath dataset for 1 epoch with batch size 128 and learning rates of 1e-6 or 1e-5.

## Key Results
- MinorDPO outperforms standard DPO on GSM8K test set in most settings
- Higher learning rates (1e-5) show particularly strong performance gains with MinorDPO
- The method improves training robustness without introducing additional hyperparameters
- Better alignment with original RLHF approach while maintaining simplicity

## Why This Works (Mechanism)
MinorDPO works by addressing a fundamental asymmetry in how DPO treats chosen and rejected samples. In standard DPO, both chosen and rejected samples are treated symmetrically with the same margin constraint β, but this can lead to over-penalizing reject samples when their log probabilities are already sufficiently low. By applying a max(0, log πθ(yl|x)/πref(yl|x)) constraint, MinorDPO ensures that reject samples only receive penalty when they exceed the reference model's probability, preventing excessive punishment that could cause model degeneration.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A method for aligning language models to human preferences without reinforcement learning; needed to understand the baseline method being modified; quick check: verify understanding of the standard DPO loss formulation with chosen and rejected sample treatment
- **KL divergence constraints in RLHF**: Used to maintain proximity to reference models during fine-tuning; needed to understand why β parameter matters for stability; quick check: confirm that β affects margin constraints rather than direct KL constraints in DPO
- **Margin-based optimization**: The approach of using margin constraints instead of direct probability matching; needed to understand how MinorDPO modifies the reject penalty; quick check: verify that max(0, log πθ(yl|x)/πref(yl|x)) creates asymmetric treatment of samples
- **Model degeneration symptoms**: Issues like repeated tokens or mode collapse during training; needed to understand what robustness improvements MinorDPO provides; quick check: monitor generated text quality during inference for degeneration signs
- **Preference dataset characteristics**: How close vs. distant preference pairs affect optimization stability; needed to understand when MinorDPO's approach is most beneficial; quick check: analyze MetaMath dataset preference pair distances
- **Learning rate interleaving with constraints**: How learning rate interacts with margin constraints β; needed to understand why higher learning rates show better results with MinorDPO; quick check: compare training stability across different learning rate settings

## Architecture Onboarding
- **Component map**: DPO loss function -> Modified reject penalty with max constraint -> Improved training stability
- **Critical path**: Base model → MetaMath preference pairs → DPO/MinorDPO training → GSM8K evaluation
- **Design tradeoffs**: Standard DPO uses symmetric treatment (simpler, more prone to over-penalization) vs. MinorDPO uses asymmetric treatment (slightly more complex, more robust)
- **Failure signatures**: Model degeneration with repeated tokens, underfitting due to reduced reject penalty, training instability at high learning rates
- **3 first experiments**: 1) Train standard DPO and MinorDPO with β=0.02 at learning rate 1e-6, compare GSM8K scores; 2) Train both methods at learning rate 1e-5 to test robustness at higher rates; 3) Monitor generated text quality during training to detect degeneration symptoms

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the precise relationship between DPO's β parameter and the KL constraint strength, and how does this relationship affect model stability across different learning rates?
- Basis in paper: [explicit] The paper explicitly states that "Rafailov et al. (2023) claim β accounting for the strength of the KL constraints" but the authors argue that "β is the constraints strength for margin and affect the training process by interleaving with the learning rate, instead of affecting πθ and πref directly as in Eq. 1"
- Why unresolved: The paper identifies a discrepancy between the theoretical claim about β and its actual behavior in DPO, but doesn't provide a complete mathematical explanation of how β's role differs between RL and DPO
- What evidence would resolve it: A formal mathematical derivation showing how β in DPO relates to margin constraints rather than direct KL constraints, along with empirical validation across a wider range of learning rates and model sizes

### Open Question 2
- Question: Under what specific data distribution conditions does MinorDPO outperform standard DPO, and what are the theoretical limits of this improvement?
- Basis in paper: [explicit] The paper shows MinorDPO performs better on MetaMath datasets with close preference pairs, but doesn't comprehensively characterize when this advantage holds
- Why unresolved: The paper only tests on MetaMath datasets and doesn't provide theoretical analysis of the conditions under which MinorDPO's approach of reducing reject penalty is beneficial
- What evidence would resolve it: Systematic testing across diverse datasets with varying preference pair distances, combined with theoretical analysis of the margin distribution effects on DPO vs MinorDPO performance

### Open Question 3
- Question: How can training metrics be developed to quantify sufficiency of training and prevent overfitting in preference optimization methods?
- Basis in paper: [explicit] The paper suggests "there may exist some training metrics that can be used to quantify whether training is sufficient or not, and thus those metrics can be used inside DPO/MinorDPO in some way to solve the over-fit problem"
- Why unresolved: This is proposed as future work without concrete methodology or experimental validation
- What evidence would resolve it: Development and validation of specific metrics that correlate with overfitting risk, along with integration into the optimization objective and demonstration of improved generalization performance

## Limitations
- Limited experimental scope to single base model (Qwen1.5-7B-Chat) and dataset (MetaMath)
- Evaluation only on GSM8K test set, not comprehensive across multiple downstream tasks
- Theoretical justification for why the max constraint prevents degeneration is not rigorously proven

## Confidence
- **High confidence**: The identification of the specific issue with DPO's symmetric treatment of chosen and rejected samples leading to potential over-penalization is well-supported by the mathematical formulation of the standard DPO loss. The proposed mathematical modification to address this issue is clearly specified and implementable.
- **Medium confidence**: The experimental results showing MinorDPO outperforming standard DPO on GSM8K with higher learning rates are supported by the reported data, but the limited scope (single model, single dataset, single evaluation task) reduces confidence in generalizability. The claim that MinorDPO improves robustness without introducing additional hyperparameters is technically accurate but the practical significance of this benefit is not thoroughly demonstrated.
- **Low confidence**: The paper's assertion that MinorDPO is "more aligned with the original RLHF approach" is not well-supported with concrete evidence or analysis comparing the behavior of the methods. The robustness improvements are claimed but not directly measured through systematic analysis of degeneration symptoms or stability metrics during training.

## Next Checks
1. **Cross-model validation**: Implement and test MinorDPO on multiple base models of different scales (e.g., LLaMA, Mistral, different parameter sizes) and architectures to verify if the improvements generalize beyond Qwen1.5-7B-Chat.

2. **Direct robustness measurement**: Design experiments that specifically measure model degeneration symptoms (e.g., repeated token generation, mode collapse) during training by monitoring generated text quality and diversity metrics, rather than inferring robustness from GSM8K performance alone.

3. **Broader dataset evaluation**: Test MinorDPO on multiple preference datasets beyond MetaMath (e.g., Anthropic's HH dataset, ShareGPT) and evaluate on multiple downstream tasks to assess whether the robustness improvements transfer across different domains and preference distributions.