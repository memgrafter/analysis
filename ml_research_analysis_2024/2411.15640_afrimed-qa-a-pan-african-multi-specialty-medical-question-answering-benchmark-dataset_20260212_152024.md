---
ver: rpa2
title: 'AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark
  Dataset'
arxiv_id: '2411.15640'
source_url: https://arxiv.org/abs/2411.15640
tags:
- medical
- llms
- answer
- questions
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AfriMed-QA, the first large-scale Pan-African
  multispecialty medical Question-Answering dataset comprising 15,275 questions (MCQs,
  SAQs, and CQs) sourced from over 60 medical schools across 16 African countries.
  The dataset covers 32 medical specialties and includes both expert-contributed and
  crowdsourced questions with human answers.
---

# AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset

## Quick Facts
- **arXiv ID:** 2411.15640
- **Source URL:** https://arxiv.org/abs/2411.15640
- **Reference count:** 16
- **Key outcome:** First large-scale Pan-African multispecialty medical QA dataset with 15,275 questions across 32 specialties from 16 countries, showing significant performance variation across models and geographies.

## Executive Summary
This work introduces AfriMed-QA, the first large-scale Pan-African multispecialty medical Question-Answering dataset comprising 15,275 questions (MCQs, SAQs, and CQs) sourced from over 60 medical schools across 16 African countries. The dataset covers 32 medical specialties and includes both expert-contributed and crowdsourced questions with human answers. The authors evaluate 30 large language models across correctness, bias, and demographic dimensions, finding significant performance variation across specialties and geographies. While general models outperform biomedical models and smaller edge-friendly LLMs struggle to achieve passing scores, human evaluations show consistent consumer preference for LLM answers over clinician responses. The dataset is released under CC-BY-NC-SA 4.0 license to support development of culturally-attuned medical AI for African healthcare.

## Method Summary
The AfriMed-QA dataset was constructed through a multi-phase approach involving medical educators and institutions across 16 African countries. Questions were collected from medical school curricula and expert clinicians, covering 32 specialties including Internal Medicine, Surgery, Pediatrics, Obstetrics/Gynecology, and others. The dataset includes three question types: Multiple Choice Questions (MCQs), Short Answer Questions (SAQs), and Clinical Questions (CQs). Questions were curated to reflect African medical contexts and patient demographics. The dataset was then used to evaluate 30 different large language models across multiple dimensions including correctness, bias, and demographic representation. Automated scoring methods were employed alongside human evaluations to assess model performance.

## Key Results
- AfriMed-QA contains 15,275 medical questions across 32 specialties from 16 African countries
- General LLMs outperformed biomedical-specific models on the benchmark
- Human evaluators consistently preferred LLM answers over clinician responses in head-to-head comparisons

## Why This Works (Mechanism)
The AfriMed-QA dataset addresses a critical gap in medical AI development by providing culturally and geographically relevant training and evaluation data for African healthcare contexts. By incorporating questions that reflect local disease patterns, treatment protocols, and patient demographics specific to African populations, the dataset enables the development of medical AI systems that are better attuned to the realities of African healthcare delivery. The inclusion of multiple question types (MCQs, SAQs, CQs) allows for comprehensive evaluation of both factual knowledge and clinical reasoning capabilities across different medical specialties.

## Foundational Learning

1. **Medical Question-Answering Datasets**
   - Why needed: Essential for training and evaluating AI systems in medical domains
   - Quick check: Dataset contains diverse question types (MCQs, SAQs, CQs) across multiple specialties

2. **Cross-Cultural Medical AI**
   - Why needed: Medical knowledge and practice vary significantly across geographic regions
   - Quick check: Questions sourced from 16 African countries with local disease patterns and protocols

3. **Benchmark Evaluation Methods**
   - Why needed: Standardized assessment of AI model performance across multiple dimensions
   - Quick check: 30 LLMs evaluated on correctness, bias, and demographic factors

## Architecture Onboarding

**Component Map:**
Dataset Creation -> Question Curation -> Model Evaluation -> Performance Analysis

**Critical Path:**
1. Question collection from 60+ medical schools
2. Expert review and categorization into 32 specialties
3. Automated and human evaluation of 30 LLMs
4. Analysis of performance across specialties and demographics

**Design Tradeoffs:**
- Balance between expert-contributed questions and crowdsourced content
- Geographic representation vs. depth of specialty coverage
- Automated scoring efficiency vs. human evaluation accuracy

**Failure Signatures:**
- Poor performance on edge-friendly LLMs indicates computational resource limitations
- Geographic performance gaps suggest regional knowledge gaps in models
- Specialty-specific weaknesses reveal domain expertise limitations

**First 3 Experiments:**
1. Evaluate baseline performance of general LLMs vs biomedical-specific models
2. Test geographic variation by country of question origin
3. Assess consumer preference for LLM vs clinician responses

## Open Questions the Paper Calls Out

None

## Limitations

- Strong specialty bias toward Internal Medicine, Surgery, Pediatrics, and Obstetrics/Gynecology (~75% of questions)
- Geographic concentration in specific countries (Nigeria, South Africa, Kenya) with limited representation from other African nations
- Reliance on automated scoring methods that may miss clinical nuances and safety-critical reasoning failures

## Confidence

- **High:** Dataset novelty as first Pan-African multispecialty medical QA benchmark
- **Medium:** Performance evaluation results due to automated scoring approach
- **Medium:** Consumer preference findings based on specific evaluation conditions
- **Low:** Generalizability across all African healthcare contexts due to geographic and specialty concentration

## Next Checks

1. Conduct comprehensive human expert review of LLM outputs across all 32 specialties to validate automated scoring accuracy and identify potential clinical reasoning failures.

2. Expand geographic and specialty diversity testing by collecting additional questions from underrepresented African countries and medical specialties to assess model performance gaps.

3. Perform cross-cultural validation studies comparing LLM responses against diverse clinician groups from different African regions to evaluate consistency and identify systematic biases in model outputs.