---
ver: rpa2
title: LLM-assisted Vector Similarity Search
arxiv_id: '2412.18819'
source_url: https://arxiv.org/abs/2412.18819
tags:
- search
- similarity
- vector
- queries
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the limitation of vector similarity search
  in handling complex, nuanced queries that involve constraints, negations, or conceptual
  requirements. To overcome this, it proposes a hybrid approach combining vector similarity
  search with LLM ranking: first using FAISS to shortlist the top 15 results, then
  employing GPT-4o to re-rank these based on contextual relevance to the original
  query.'
---

# LLM-assisted Vector Similarity Search

## Quick Facts
- arXiv ID: 2412.18819
- Source URL: https://arxiv.org/abs/2412.18819
- Authors: Md Riyadh; Muqi Li; Felix Haryanto Lie; Jia Long Loh; Haotian Mi; Sayam Bohra
- Reference count: 0
- Primary result: LLM-assisted approach significantly outperforms raw vector search on complex queries involving negations and conceptual requirements while maintaining efficiency.

## Executive Summary
This paper addresses the limitation of vector similarity search in handling complex, nuanced queries that involve constraints, negations, or conceptual requirements. To overcome this, it proposes a hybrid approach combining vector similarity search with LLM ranking: first using FAISS to shortlist the top 15 results, then employing GPT-4o to re-rank these based on contextual relevance to the original query. Experiments on two small structured datasets (100 food dishes and 100 tourist spots) show that while both methods perform similarly on simple queries, the LLM-assisted approach significantly outperforms raw vector search on complex queries—correctly handling negations and identifying conceptually relevant matches.

## Method Summary
The paper proposes a two-step hybrid approach: (1) FAISS vector similarity search to shortlist top 15 results using OpenAI's text-embedding-ada-002 embeddings, and (2) GPT-4o LLM to re-rank these results based on contextual relevance to the original query. This combines the efficiency of vector search for initial candidate retrieval with the contextual understanding capabilities of LLMs for refining results on complex queries involving negations or conceptual requirements.

## Key Results
- Raw vector similarity search fails on queries with negations, as "no fish" and "fish" may be close in embedding space
- LLM re-ranking correctly identifies conceptually relevant matches (e.g., panda research base for "exposure to wildlife") that vector search misses
- The hybrid approach improves accuracy for complex queries without sacrificing efficiency, with benefits scaling to larger datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vector similarity search alone fails on queries with negations or conceptual requirements.
- Mechanism: FAISS-based vector search uses semantic embeddings that do not inherently distinguish between presence and absence of terms; thus "no fish" and "fish" may be close in embedding space.
- Core assumption: The embedding model (text-embedding-ada-002) does not encode negation as a strong opposing signal.
- Evidence anchors:
  - [abstract] "However, its effectiveness diminishes when handling intricate queries with contextual nuances."
  - [section] Food dataset example: "The LLM correctly filtered out dishes containing fish or shrimp, while the raw similarity search failed to do so, presumably due to the presence of negation in the query."
- Break condition: If embedding model is trained to explicitly handle negation or if the dataset is small enough that negation is captured by exact term matching.

### Mechanism 2
- Claim: LLMs can understand contextual relevance beyond semantic similarity.
- Mechanism: GPT-4o re-ranks the FAISS short-list by reasoning over the query and each candidate's full description, identifying conceptual matches (e.g., panda research base for "exposure to wildlife").
- Core assumption: GPT-4o has sufficient world knowledge to link related concepts not explicitly mentioned in the short-list.
- Evidence anchors:
  - [abstract] "By leveraging the natural language understanding capabilities of LLMs, this method improves the accuracy of search results for complex tasks."
  - [section] Tourist dataset example: "The LLM identified the relevance of a research base for giant panda breeding to the 'exposure to wildlife', which the raw similarity search ignored in its ranking."
- Break condition: If short-list is too large, GPT-4o may miss relevant items; if short-list is too small, relevant items may be excluded before LLM ranking.

### Mechanism 3
- Claim: Two-stage approach retains efficiency while improving accuracy.
- Mechanism: FAISS provides fast initial filtering to a manageable short-list (e.g., top 15), then LLM ranking incurs cost only on this subset.
- Core assumption: FAISS ranking correlates enough with relevance to make short-listing worthwhile; LLM cost on 15 items is acceptable.
- Evidence anchors:
  - [abstract] "This paper explores a hybrid approach combining vector similarity search with Large Language Models (LLMs) to enhance search accuracy and relevance."
  - [section] "By combining these two steps, we aim to achieve the best of both worlds: the efficiency of vector similarity search for initial shortlisting, and the contextual understanding and ranking capabilities of LLMs for refining the final results."
- Break condition: If short-list size grows (e.g., >50), LLM cost dominates; if FAISS quality drops, short-list may miss relevant items.

## Foundational Learning

- Concept: Vector similarity search fundamentals (FAISS, embeddings, approximate nearest neighbor).
  - Why needed here: The paper builds on FAISS for initial candidate retrieval; understanding its mechanics is essential to tuning short-list size.
  - Quick check question: What is the trade-off between recall and speed in FAISS's approximate nearest neighbor search?

- Concept: Embedding model limitations (e.g., negation handling, contextual nuance).
  - Why needed here: Explains why raw vector search fails on complex queries and motivates LLM re-ranking.
  - Quick check question: Why might "I like fishing" and "I do not like fishing" be close in embedding space?

- Concept: LLM prompt engineering and ranking.
  - Why needed here: GPT-4o must be prompted effectively to rank short-listed items by contextual relevance.
  - Quick check question: What information should be included in the LLM prompt to ensure accurate re-ranking?

## Architecture Onboarding

- Component map: Data source (CSV/relational DB) → Embedding model (text-embedding-ada-002) → FAISS index → Query embedding → FAISS short-list (top 15) → GPT-4o re-ranking → Final top 3 results.

- Critical path:
  1. Embed query
  2. FAISS search → short-list
  3. Construct LLM prompt (query + short-list + instructions)
  4. LLM re-rank → return top N

- Design tradeoffs:
  - Short-list size: Larger → better recall but higher LLM cost; smaller → faster but risk missing relevant items
  - Embedding model choice: More advanced embeddings may reduce LLM reliance but cost more
  - LLM model: GPT-4o balances quality and speed; smaller models may be faster but less accurate on complex queries

- Failure signatures:
  - Short-list too small → LLM can't find good matches
  - Short-list too large → latency spikes, LLM may miss relevant items in long list
  - Embedding drift → FAISS ranking degrades

- First 3 experiments:
  1. Vary short-list size (5, 15, 30) on food dataset and measure LLM accuracy vs latency
  2. Swap embedding model (e.g., text-embedding-3-small) and compare FAISS+LLM vs raw vector results
  3. Test LLM re-ranking on a short-list generated from a larger dataset (e.g., 4500+ rows) to assess scalability

## Open Questions the Paper Calls Out

- How does varying the shortlist size from the initial FAISS search affect the final relevance of LLM-assisted search results? The paper suggests this as future work but didn't experimentally test different shortlist sizes.

- Does the LLM-assisted approach maintain its advantage over raw vector search when applied to larger, more diverse datasets beyond CSV files and relational databases? The paper mentions internal deployment benefits but lacks detailed experimental results on larger datasets.

- What is the optimal balance between the computational efficiency of vector similarity search and the accuracy gains from LLM re-ranking in terms of overall system latency? The paper notes performance implications but doesn't provide quantitative latency analysis.

## Limitations

- Evaluation based on only two small datasets (100 items each), limiting generalizability to larger, more diverse collections
- Performance gains come at additional LLM inference cost, but detailed latency or cost analysis is not provided
- The effectiveness of FAISS short-listing in preserving relevant items for LLM re-ranking is not empirically validated

## Confidence

- **High confidence**: The core mechanism of combining FAISS short-listing with GPT-4o re-ranking is technically sound and the food dataset negation example is clearly demonstrated
- **Medium confidence**: The claimed benefits for complex queries and scalability to larger datasets are supported by presented results but would benefit from more rigorous testing
- **Low confidence**: The exact prompt format for GPT-4o, complete evaluation queries, and latency/cost trade-offs are not specified, making full replication difficult

## Next Checks

1. Test the hybrid approach on a larger dataset (500+ items) with more diverse content to validate scalability claims and measure how short-list size affects both recall and latency

2. Conduct ablation studies varying the short-list size (5, 15, 30, 50) to quantify the trade-off between FAISS recall and LLM re-ranking effectiveness, measuring both accuracy and inference time

3. Compare the hybrid approach against alternative solutions like using a more advanced embedding model or adding explicit negation handling in the embedding layer, to determine if the LLM re-ranking step is truly necessary for the performance gains observed