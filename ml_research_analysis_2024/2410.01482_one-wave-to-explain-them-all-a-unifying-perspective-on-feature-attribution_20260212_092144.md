---
ver: rpa2
title: 'One Wave To Explain Them All: A Unifying Perspective On Feature Attribution'
arxiv_id: '2410.01482'
source_url: https://arxiv.org/abs/2410.01482
tags:
- wavelet
- audio
- attribution
- feature
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Wavelet Attribution Method (WAM), a feature
  attribution approach that operates in the wavelet domain rather than the pixel domain.
  WAM provides a unified framework for explaining model decisions across audio, images,
  and volumes by leveraging wavelet coefficients, which preserve structural information
  across scales and spatial locations.
---

# One Wave To Explain Them All: A Unifying Perspective On Feature Attribution

## Quick Facts
- arXiv ID: 2410.01482
- Source URL: https://arxiv.org/abs/2410.01482
- Reference count: 40
- This work introduces Wavelet Attribution Method (WAM), a feature attribution approach that operates in the wavelet domain rather than the pixel domain

## Executive Summary
This paper introduces Wavelet Attribution Method (WAM), a unified framework for explaining model decisions across audio, images, and volumes by computing feature attributions in the wavelet domain. WAM adapts gradient-based attribution techniques like SmoothGrad and Integrated Gradients to wavelet coefficients, providing explanations that capture both spatial and frequency information. The method demonstrates strong quantitative performance, achieving state-of-the-art results in insertion and deletion metrics across multiple modalities and model architectures.

## Method Summary
WAM computes feature attributions by first transforming inputs into the wavelet domain using the Discrete Wavelet Transform (DWT), then computing gradients with respect to wavelet coefficients rather than pixel values. The method supports both SmoothGrad and Integrated Gradients variants in the wavelet domain, enabling explanations that reveal multi-scale structures and frequency-dependent model behavior. The wavelet coefficients are then transformed back to the input domain for visualization, providing interpretable attributions that capture both spatial locations and frequency content.

## Key Results
- WAM achieves state-of-the-art performance in insertion and deletion metrics across audio, images, and volumes
- The method successfully reveals multi-scale structures in 3D volumes and key audio components in noisy signals
- WAM provides insights into model behavior that are not accessible through pixel-based attribution methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wavelet coefficients preserve spatial and frequency information while providing interpretable low-level features
- Mechanism: The Discrete Wavelet Transform (DWT) decomposes signals into components localized in both space and scale, capturing structural elements like edges, textures, and transient patterns
- Core assumption: The wavelet transform is invertible and differentiable almost everywhere, allowing gradients to be computed with respect to wavelet coefficients
- Evidence anchors:
  - [abstract]: "WAM leverages the spatial and scale-localized properties of wavelet coefficients to provide explanations that capture both the where and what of a model's decision-making process"
  - [section 3.1]: "Under appropriate admissibility and regularity conditions on ψ, the DWT is an invertible linear transform"
  - [corpus]: Weak - no direct corpus evidence for wavelet properties, but this is well-established signal processing theory
- Break condition: If the wavelet transform is not differentiable or invertible, or if the chosen mother wavelet doesn't capture relevant structural features

### Mechanism 2
- Claim: Computing gradients in the wavelet domain provides more meaningful attributions than pixel-based methods
- Mechanism: The wavelet domain separates features by scale and orientation, allowing the model's sensitivity to different frequency components to be quantified
- Core assumption: Model decisions depend on specific scales/frequencies rather than individual pixels
- Evidence anchors:
  - [abstract]: "WAM provides a unified framework for explaining model decisions across audio, images, and volumes by leveraging wavelet coefficients"
  - [section 3.2]: "Computing the gradient of f with respect to the wavelet transform of x will enable us to understand the model's reliance on features such as textures, edges, or shapes"
  - [section 4.2]: "WAM quantifies the importance of each scale in the final prediction. Scales in the wavelet domain correspond to dyadic frequency ranges in the Fourier domain"
- Break condition: If the model doesn't actually rely on scale/frequency information, or if the wavelet decomposition fails to capture relevant features

### Mechanism 3
- Claim: WAMIG inherits theoretical properties from Integrated Gradients while extending them to the wavelet domain
- Mechanism: By composing the integrated gradients path method with the wavelet transform, WAMIG satisfies completeness and sensitivity axioms
- Core assumption: The composition of differentiable functions remains differentiable, preserving the mathematical properties of the original method
- Evidence anchors:
  - [section 3.2]: "We adapt the Integrated Gradient method from the image domain to the wavelet domain"
  - [section B.1]: "If W −1 is differentiable almost everywhere, then WAMIG satisfies completeness"
  - [section B]: "WAMIG inherits from the theoretical properties of the vanilla integrated gradients method, and especially sensitivity"
- Break condition: If the inverse wavelet transform is not differentiable, or if the path integration fails to capture inter-scale dependencies

## Foundational Learning

- Concept: Wavelet transforms and multiresolution analysis
  - Why needed here: Understanding how DWT decomposes signals into scale-localized components is fundamental to grasping why WAM works
  - Quick check question: What is the difference between continuous and discrete wavelet transforms, and why is the discrete version used in WAM?

- Concept: Gradient-based attribution methods (SmoothGrad, Integrated Gradients)
  - Why needed here: WAM extends these methods to the wavelet domain, so understanding their original formulations is crucial
  - Quick check question: How do SmoothGrad and Integrated Gradients differ in their approach to smoothing gradient-based explanations?

- Concept: Signal processing and frequency domain analysis
  - Why needed here: The wavelet domain is essentially a multi-scale frequency representation, so familiarity with frequency analysis is important
  - Quick check question: How do wavelet scales correspond to frequency ranges in the Fourier domain?

## Architecture Onboarding

- Component map: Input signal preprocessing -> Wavelet transform -> Gradient computation -> Inverse transform -> Attribution map -> Smoothing/Integration module for WAMSG/WAMIG variants -> Mother wavelet selection module -> Evaluation metrics (Insertion, Deletion, Faithfulness)

- Critical path:
  1. Apply DWT to input signal
  2. Compute model gradients with respect to wavelet coefficients
  3. Apply smoothing or integration if using WAMSG/WAMIG
  4. Transform back to input domain for visualization

- Design tradeoffs:
  - Mother wavelet choice: Haar (computational efficiency, sharp transitions) vs Daubechies (smoothness, texture details)
  - Smoothing parameter σ: Higher values for smoother explanations but potential loss of detail
  - Number of integration steps: More steps for accurate path integration but higher computational cost

- Failure signatures:
  - Poor reconstruction quality indicates inappropriate wavelet choice or insufficient decomposition levels
  - Noisy attributions suggest inadequate smoothing or inappropriate gradient computation
  - Inconsistent results across modalities may indicate improper baseline selection or implementation errors

- First 3 experiments:
  1. Verify wavelet transform invertibility by reconstructing a simple signal and measuring reconstruction error
  2. Compare WAM attributions with pixel-based attributions on a simple binary classification task to validate qualitative improvements
  3. Test WAMIG completeness property by verifying that sum of attributions equals prediction difference from baseline on a known differentiable function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different wavelet types (e.g., Haar vs. Daubechies vs. biorthogonal) affect the quantitative performance of WAM across modalities?
- Basis in paper: [explicit] The paper demonstrates that WAM is theoretically invariant to the choice of mother wavelet for images and volumes, with minor variations for audio due to signal reconstruction differences.
- Why unresolved: While the paper shows theoretical invariance and provides empirical checks up to 8 decimal places, it does not systematically compare performance across different wavelet types or analyze how wavelet choice affects specific downstream tasks.
- What evidence would resolve it: Comprehensive ablation studies across multiple wavelet families showing performance differences (or lack thereof) on insertion/deletion scores, faithfulness metrics, and specific application domains like medical imaging or audio classification.

### Open Question 2
- Question: Can WAM be extended to handle point cloud data or unstructured modalities through graph wavelet transforms?
- Basis in paper: [inferred] The paper explicitly states that WAM cannot currently handle point cloud data and suggests graph wavelet transforms as a potential solution, noting that the wavelet decomposition cannot be applied to text data.
- Why unresolved: The mathematical framework for WAM relies on standard wavelet transforms that require structured, grid-like data, and no implementation or evaluation of graph-based extensions is provided.
- What evidence would resolve it: Successful implementation of WAM using graph wavelet transforms on point cloud datasets with demonstrated performance improvements over existing point cloud explanation methods.

### Open Question 3
- Question: How does WAM perform when explaining multimodal models that combine different data types (e.g., vision-language models)?
- Basis in paper: [explicit] The paper mentions that WAM has potential applications for explaining multimodal models but notes that wavelet decomposition cannot be applied to text data, limiting current applicability.
- Why unresolved: While the paper establishes WAM's effectiveness for single-modality inputs, it does not explore how to handle the heterogeneous nature of multimodal inputs where some modalities lack wavelet representations.
- What evidence would resolve it: Experimental results showing WAM successfully explaining vision-language model decisions, either through hybrid approaches or by finding alternative representations for non-wavelet-compatible modalities.

## Limitations
- Reliance on the invertibility and differentiability of the wavelet transform, which is assumed but not empirically validated across all use cases
- Choice of mother wavelet significantly impacts attribution quality, yet the paper only demonstrates results with Haar wavelets
- Cross-modal generalization claims remain untested beyond the three presented modalities

## Confidence

- **High Confidence**: The mathematical foundation of wavelet transforms and their invertibility (well-established signal processing theory)
- **Medium Confidence**: The extension of gradient-based attribution methods to the wavelet domain (theoretically sound but implementation-dependent)
- **Medium Confidence**: Quantitative performance claims on benchmark datasets (strong results reported but dependent on specific implementation details)
- **Low Confidence**: Claims about connecting feature attribution with model robustness (the relationship is suggested but not rigorously established)

## Next Checks
1. **Wavelet Choice Ablation**: Systematically test WAM performance across different mother wavelets (Haar, Daubechies, Symlets) to identify optimal choices for different modalities and establish the sensitivity of results to wavelet selection.

2. **Computational Complexity Analysis**: Benchmark WAM against standard pixel-based attribution methods (SmoothGrad, Integrated Gradients) measuring wall-clock time, memory usage, and GPU utilization across varying input sizes and model architectures.

3. **Robustness Connection Validation**: Design experiments that directly test whether WAM's frequency-based attributions predict model robustness to adversarial attacks or input corruptions, establishing the claimed connection between attribution and robustness.