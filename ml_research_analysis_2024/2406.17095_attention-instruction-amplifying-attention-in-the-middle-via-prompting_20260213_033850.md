---
ver: rpa2
title: 'Attention Instruction: Amplifying Attention in the Middle via Prompting'
arxiv_id: '2406.17095'
source_url: https://arxiv.org/abs/2406.17095
tags:
- attention
- document
- position
- instruction
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) suffer
  from position bias in multi-document question answering and whether prompting can
  mitigate this bias. The authors design relative and absolute attention instructions
  to guide models to focus on specific segments of retrieved documents.
---

# Attention Instruction: Amplifying Attention in the Middle via Prompting

## Quick Facts
- arXiv ID: 2406.17095
- Source URL: https://arxiv.org/abs/2406.17095
- Reference count: 33
- Primary result: LLMs can follow absolute attention instructions using document IDs to mitigate position bias, but cannot understand relative positions.

## Executive Summary
This study investigates position bias in large language models when answering questions over multi-document contexts. The authors find that LLMs exhibit a preference for beginning and end documents, struggling with middle segments. Through systematic testing of relative and absolute attention instructions, they demonstrate that while LLMs lack relative position awareness, they can effectively follow absolute attention instructions using document indexes. This capability allows for significant accuracy improvements when the gold document matches the instructed segment, providing a practical method to mitigate position bias through prompting.

## Method Summary
The study tests five open-sourced LLMs (Llama-2-chat, Llama-3, Tulu-2, Mistral-instruct-v0.1, and Mistral-instruct-v0.2) on a multi-document question answering task. Documents are chunked to 100 tokens maximum and presented with task instructions, attention instructions, and questions. Three document indexing types are tested: no-index, ID-index (document IDs), and position-index (relative position words). Performance is measured using accuracy, where answers are considered correct if they contain the gold answer.

## Key Results
- LLMs cannot follow relative attention instructions (no diagonal patterns in accuracy heatmaps)
- LLMs demonstrate capacity to adapt attention to specific segments using matching document IDs
- Position words can serve as indexes for regional control when used in absolute attention instructions
- Accuracy improves significantly when gold document matches instructed segment using absolute indexing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can follow absolute attention instructions using document IDs to guide attention toward specific segments
- Mechanism: The model interprets the instruction's mention of a specific document ID as a semantic cue to allocate more attention to tokens in that document, increasing the attention scores of that segment in the final token's attention weights
- Core assumption: LLMs can map explicit document IDs to semantic segments and adjust attention accordingly without positional awareness
- Evidence anchors:
  - [abstract] "they demonstrate the capacity to adapt attention to a specific segment using matching indexes"
  - [section 4] "when the document ID is used as a reference, the models' reasoning is significantly affected, with boosted performance on the diagonals across all models"
  - [corpus] Weak evidence; corpus mentions position bias but not document ID-based attention instructions

### Mechanism 2
- Claim: Position words (e.g., "beginning", "midsection", "tail") can serve as indexes for regional control when used in absolute attention instructions
- Mechanism: Position words act as categorical labels that LLMs associate with specific document groups, allowing regional attention control even without understanding relative positions
- Core assumption: LLMs have latent correlations between position words and document IDs that can be leveraged for regional attention control
- Evidence anchors:
  - [abstract] "Our investigation on absolute attention instruction shows evidence that the attention of LLMs to a segment within the context can be enhanced semantically"
  - [section 5] "we replace the document ID with relative position words and refer to the position indexes in the attention instructions"
  - [corpus] Weak evidence; corpus discusses position effects but not position word-based indexing

### Mechanism 3
- Claim: LLMs exhibit primary and recency bias, favoring beginning and end documents, which can be mitigated through attention instructions
- Mechanism: By explicitly instructing the model to focus on middle segments using absolute indexes, attention instructions can counteract the inherent position bias in the attention mechanism
- Core assumption: The position bias is a result of attention mechanism design rather than semantic understanding, making it susceptible to instruction-based correction
- Evidence anchors:
  - [abstract] "This bias causes models to favor the beginning or end text within the context (Liu et al., 2024a)"
  - [section 4] "when prompted to focus on document 1, its performance improves slightly regardless of the gold document position"
  - [corpus] Moderate evidence; corpus mentions U-shaped attention bias and position effects in LLMs

## Foundational Learning

- Concept: Attention mechanism in transformers
  - Why needed here: Understanding how self-attention works is crucial to grasp how attention instructions can modify attention distributions
  - Quick check question: How does the attention score between two tokens get computed in a transformer?

- Concept: In-context learning and instruction following
  - Why needed here: The effectiveness of attention instructions relies on the model's ability to follow natural language instructions
  - Quick check question: What is the difference between fine-tuning and in-context learning in LLMs?

- Concept: Position embeddings and position bias
  - Why needed here: Position bias is the core problem being addressed, and understanding how position embeddings work is key to understanding why the bias exists
  - Quick check question: How do position embeddings typically encode token positions in transformers?

## Architecture Onboarding

- Component map:
  Input prompt -> LLMs -> Dataset -> Evaluation
  (task instruction + attention instruction + search results + question) -> (5 models) -> (MDQA with 3/9 documents) -> (accuracy heatmap)

- Critical path:
  1. Generate search results (documents)
  2. Construct prompt with attention instruction
  3. Run inference on LLM
  4. Calculate accuracy based on answer presence
  5. Analyze attention scores if needed

- Design tradeoffs:
  - Using absolute vs. relative attention instructions: Absolute instructions work better but require document IDs
  - Index types: ID-index provides precise control, position-index allows regional control but is less precise
  - Context length: Longer contexts may reduce the effectiveness of attention instructions

- Failure signatures:
  - No diagonal effect in accuracy heatmaps (attention instructions not followed)
  - Performance drops when gold document is in middle positions
  - Attention scores not changing according to instructions

- First 3 experiments:
  1. Test relative attention instructions with no index to confirm lack of position awareness
  2. Test absolute attention instructions with ascending ID index to verify attention following
  3. Test absolute attention instructions with position index to check regional control capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of attention instructions vary across different sizes and architectures of large language models?
- Basis in paper: [explicit] The paper notes that Llama-3 exhibits better instruction-following ability than Mistral-Instruct-v0.2, despite having similar absolute accuracy. It also mentions testing models ranging from 7B to 8B.
- Why unresolved: The paper only tests a limited range of model sizes and does not systematically investigate the impact of model size and architecture on the effectiveness of attention instructions.
- What evidence would resolve it: A comprehensive study testing attention instructions across a wider range of model sizes and architectures, analyzing the correlation between model capacity and instruction-following ability.

### Open Question 2
- Question: Can attention instructions be used to mitigate position bias in more complex retrieval scenarios, such as when multiple documents contain correct or partially correct answers?
- Basis in paper: [inferred] The paper acknowledges this limitation, stating that real-world scenarios may involve multiple documents with correct or partially correct answers and conflicting information.
- Why unresolved: The study is limited to scenarios where only one document contains the gold answer. The effectiveness of attention instructions in more complex scenarios remains unexplored.
- What evidence would resolve it: Experiments testing attention instructions in scenarios with multiple relevant documents, analyzing the impact on answer accuracy and the ability to handle conflicting information.

### Open Question 3
- Question: How do attention instructions interact with other techniques for mitigating position bias, such as context reordering and position embedding modification?
- Basis in paper: [explicit] The paper discusses these techniques in the related work section but does not investigate their interaction with attention instructions.
- Why unresolved: The study focuses solely on attention instructions and does not explore potential synergies or conflicts with other position bias mitigation techniques.
- What evidence would resolve it: Experiments combining attention instructions with context reordering or position embedding modification, analyzing the combined effect on position bias and answer accuracy.

## Limitations

- The effectiveness of attention instructions may degrade with longer contexts as the number of documents increases
- The study only tests attention instructions at a fixed granularity (3-document and 9-document settings)
- The mechanism by which LLMs map explicit document IDs to semantic segments remains unclear

## Confidence

**High Confidence:** The claim that LLMs cannot follow relative attention instructions is well-supported by the experimental results showing no diagonal patterns in accuracy heatmaps when using relative instructions.

**Medium Confidence:** The claim that absolute attention instructions improve performance when gold document matches the instructed segment is supported by the data, but the generalizability to longer contexts and different document configurations remains uncertain.

**Low Confidence:** The assumption that position words can serve as effective categorical labels for regional attention control is speculative, as the paper only briefly mentions this approach without comprehensive experimental validation.

## Next Checks

1. **Cross-context validation:** Test the absolute attention instruction method with contexts containing 15-30 documents to verify whether the diagonal pattern in accuracy heatmaps persists and whether performance degradation occurs as predicted by the authors.

2. **Ablation study on document ID semantics:** Create experiments where document IDs are randomized or replaced with meaningless tokens to determine whether models are truly responding to the semantic meaning of IDs versus simply detecting the presence of specific token patterns.

3. **Attention visualization across positions:** Generate and analyze attention weight heatmaps for multiple model layers to verify that attention scores actually concentrate on the instructed segments, not just that final answer accuracy improves, providing direct evidence of the attention instruction mechanism.