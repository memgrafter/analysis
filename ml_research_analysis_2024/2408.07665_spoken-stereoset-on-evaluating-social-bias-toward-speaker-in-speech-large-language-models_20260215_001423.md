---
ver: rpa2
title: 'Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large
  Language Models'
arxiv_id: '2408.07665'
source_url: https://arxiv.org/abs/2408.07665
tags:
- speech
- bias
- language
- biases
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Spoken Stereoset, the first dataset designed
  to evaluate social biases in Speech Large Language Models (SLLMs) based on speaker
  demographic attributes (gender and age). The dataset contains 3,640 test instances
  across 17 speakers, featuring speech contexts with stereotypical, anti-stereotypical,
  and irrelevant continuations.
---

# Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models

## Quick Facts
- arXiv ID: 2408.07665
- Source URL: https://arxiv.org/abs/2408.07665
- Authors: Yi-Cheng Lin; Wei-Chih Chen; Hung-yi Lee
- Reference count: 0
- Primary result: First dataset to evaluate social bias in Speech Large Language Models (SLLMs) based on speaker demographic attributes

## Executive Summary
This paper introduces Spoken Stereoset, the first dataset designed to evaluate social biases in Speech Large Language Models (SLLMs) based on speaker demographic attributes (gender and age). The dataset contains 3,640 test instances across 17 speakers, featuring speech contexts with stereotypical, anti-stereotypical, and irrelevant continuations. The authors evaluate three SOTA SLLMs (Qwen-Audio-Chat, LTU-AS, and SALMONN variants) using three metrics: Speech Language Instruction Following Score (slifs), Speech Language Modeling Score (slms), and Speech Language Bias Score (slbs). Results show most models exhibit minimal bias with slbs scores close to 50, indicating near-unbiased performance. SALMONN models demonstrate superior performance across metrics, while SALMONN 13B shows slight anti-stereotypical bias in the age domain (slbs=44.43).

## Method Summary
The study constructs Spoken Stereoset by synthesizing speech contexts from 17 speakers (8 male, 9 female; young and elderly) using TTS systems, paired with three text continuations (stereotypical, anti-stereotypical, irrelevant). SLLMs are evaluated on their ability to select appropriate continuations while controlling for demographic information embedded in the speech signal. The evaluation uses three metrics: slifs measures instruction-following (selecting valid continuations), slms measures language modeling quality, and slbs measures bias by comparing preference for stereotypical vs. anti-stereotypical continuations. The study includes text-only control experiments to isolate bias contributions from speech encoders versus LLM backbones.

## Key Results
- Most SLLMs show minimal bias with slbs scores close to 50 (near-unbiased)
- SALMONN models achieve superior performance across all metrics
- SALMONN 13B exhibits slight anti-stereotypical bias in age domain (slbs=44.43)
- Text-only LLMs show fair performance, suggesting speech encoders introduce minimal additional bias
- All models demonstrate high instruction-following (slifs) and language modeling (slms) scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset design exposes speaker-based bias by embedding demographic attributes into the speech signal while keeping text continuations free of explicit speaker cues.
- Mechanism: By synthesizing identical text contexts with speech from different speakers (male/female, young/elderly), the only varying factor is the speaker's voice, allowing SLLMs to reveal bias based on perceived demographics.
- Core assumption: Speech encoders extract and propagate paralinguistic features (e.g., pitch, prosody) that correlate with demographic attributes, and LLMs respond differently based on these features.
- Evidence anchors:
  - [abstract] "By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases."
  - [section 3.1] "Speech contains rich speaker information, including age, gender, accent, and emotional state, which provides context beyond the words themselves."
  - [corpus] Weak evidence - related papers focus on gender bias but not the specific design of controlling text while varying speech.
- Break condition: If the speech encoder does not reliably extract demographic cues or the LLM ignores them, bias scores will not differ meaningfully across speaker groups.

### Mechanism 2
- Claim: The three-metric evaluation framework (slifs, slms, slbs) isolates instruction-following, language modeling, and bias separately, enabling precise measurement of model behavior.
- Mechanism: slifs ensures the model selects a valid answer option, slms verifies meaningful response quality, and slbs compares stereotypical vs. anti-stereotypical preference, so bias can be measured without conflating it with other failures.
- Core assumption: Models can be scored independently on each dimension, and a model that fails slifs or slms will not falsely appear unbiased due to chance.
- Evidence anchors:
  - [section 3.3] "We introduce three metrics similar to those proposed by StereoSet... to comprehensively evaluate these aspects."
  - [section 3.3] "A slbs closer to 50 indicates a more unbiased model."
  - [corpus] Moderate evidence - related works use bias metrics but rarely separate instruction-following from bias itself.
- Break condition: If a model consistently outputs the same category regardless of input, slifs will fail and slbs will be undefined, masking true bias patterns.

### Mechanism 3
- Claim: Text-only control experiments demonstrate that bias originates from speech encoders rather than the LLM backbone, validating the necessity of speech-specific bias evaluation.
- Mechanism: By prompting LLMs with transcriptions only (no audio), any remaining bias would be due to text-based stereotypes; minimal bias in this setting implies speech encoders introduce the demographic bias.
- Evidence anchors:
  - [section 5.4] "Our purpose is to investigate the bias level introduced by the speech encoder... we designed an experiment to assess the bias in the backbone large language model by prompting it with text-only inputs."
  - [section 5.4] "Overall, these models exhibit less bias because they focus primarily on semantic tasks... limited focus on paralinguistic attributes."
  - [corpus] Weak evidence - no corpus paper directly replicates this ablation approach.
- Break condition: If text-only inputs still produce demographic bias, the design fails to isolate speech-encoder contribution.

## Foundational Learning

- Concept: Speech encoder extraction of paralinguistic features (pitch, prosody, spectral characteristics)
  - Why needed here: Bias detection relies on whether demographic cues are encoded from raw audio before LLM reasoning.
  - Quick check question: Can you explain how Mel-filterbanks or MFCCs can encode gender and age information?

- Concept: Bias benchmarking via contrastive continuations (stereotypical vs. anti-stereotypical)
  - Why needed here: The dataset construction depends on generating balanced contrastive pairs to measure preference strength.
  - Quick check question: How would you ensure that stereotypical and anti-stereotypical continuations are equally plausible given the context?

- Concept: Instruction-following evaluation for multimodal prompts
  - Why needed here: slifs requires detecting when a model outputs "others" instead of a valid choice, critical for fair bias scoring.
  - Quick check question: What regular expression or classifier would you use to detect non-conforming outputs?

## Architecture Onboarding

- Component map:
  - Text-to-Speech synthesis module → speaker-controlled audio generation
  - Speech encoder (pretrained wav2vec, HuBERT, etc.) → feature extraction
  - LLM backbone (Qwen, Vicuna) → continuation generation
  - Bias evaluation pipeline → slifs/slm/slms computation
  - GPT-4o evaluator → mapping raw outputs to categories

- Critical path:
  1. Load context and three continuations
  2. Synthesize speech per speaker demographic
  3. Run SLLM with audio + text prompt
  4. Parse model output via GPT-4o evaluator
  5. Compute slifs/slm/slbs metrics
  6. Aggregate by demographic group

- Design tradeoffs:
  - TTS choice vs. naturalness: Azure TTS offers speaker metadata but may sound less natural than Topmediai.
  - Sampling strategy: temperature=1.0, top-p=0.9, top-k=100 trades diversity for reliability; lower temp reduces spurious "others" outputs.
  - Evaluator selection: GPT-4o gives high agreement but introduces its own bias; human annotation would be slower but more transparent.

- Failure signatures:
  - High "others" rate → model confusion or instruction-following failure
  - slms near 66.67 → random guessing, indicating poor language modeling
  - slbs far from 50 with balanced slifs/slm → genuine bias
  - Disparate slbs across text-only vs. audio → speech encoder bias

- First 3 experiments:
  1. Run all SLLMs on Spoken Stereoset and record raw output categories.
  2. Compute baseline slifs/slm/slbs for each model; identify models with high "others" rate.
  3. Repeat with text-only inputs; compare slbs changes to isolate speech-encoder contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SLLMs perform on bias evaluation when tested with non-US cultural contexts and speaker demographics?
- Basis in paper: [explicit] The paper acknowledges that Spoken Stereoset measurements are limited to US cultural and social norms, stating "When a model is applied in a different social context, Spoken Stereoset may not accurately reflect the presence of biases."
- Why unresolved: The dataset was constructed using US-based annotators and stereotypes specific to US cultural norms. The authors explicitly note that performance may differ in other cultural contexts.
- What evidence would resolve it: Testing the same SLLMs on equivalent datasets constructed with different cultural contexts and speaker demographics would reveal whether performance changes across regions.

### Open Question 2
- Question: What specific components of speech encoders introduce bias, and how can these be isolated and mitigated?
- Basis in paper: [explicit] The paper states it's "challenging to measure" the bias introduced by speech encoders directly, and the text-only experiment setup only provides indirect evidence about this.
- Why unresolved: The experimental design cannot directly isolate bias contributions from the speech encoder versus the LLM backbone, as noted by the authors' need to use text-only prompts as a proxy.
- What evidence would resolve it: Systematic ablation studies where different speech encoder architectures are tested with identical LLM backbones, or experiments that manipulate specific encoder parameters while measuring bias changes.

### Open Question 3
- Question: Why does SALMONN 13B show anti-stereotypical bias in the age domain but not in gender, while other models maintain near-unbiased performance across both domains?
- Basis in paper: [explicit] The results show SALMONN 13B has slbs=44.43 in the age domain but near-50 scores in gender, with the authors noting this as "a different tendency, favoring anti-stereotypical associations."
- Why unresolved: The authors identify this as an outlier finding but do not investigate the underlying reasons for this domain-specific bias pattern or whether it relates to model architecture, training data, or instruction-tuning.
- What evidence would resolve it: Comparative analysis of SALMONN 13B's training data composition across age versus gender contexts, or experiments varying model size and architecture to determine what factors contribute to this domain-specific bias.

## Limitations
- Reliance on TTS-generated speech may not capture natural speech variations and could introduce artifacts
- GPT-4o evaluator introduces a second layer of potential bias and may not perfectly align with human judgment
- Relatively small dataset (3,640 instances across 17 speakers) may lack statistical power for detecting subtle bias patterns
- Focus limited to gender and age domains, potentially missing other important demographic attributes

## Confidence
- Bias measurement methodology (High confidence): The three-metric framework (slifs, slms, slbs) is well-defined and mathematically sound
- Speech encoder bias isolation (Medium confidence): Text-only control experiments provide suggestive but indirect evidence
- Model bias findings (Medium confidence): Minimal bias findings are consistent but based on small effect sizes and limited speaker diversity
- Dataset generalizability (Low confidence): TTS synthesis and specific context choices may limit real-world applicability

## Next Checks
1. **Human evaluation validation**: Conduct human annotation of a subset of model responses to verify the accuracy and consistency of GPT-4o evaluator judgments, particularly for ambiguous cases near category boundaries.

2. **Natural speech extension**: Test the same evaluation pipeline on naturally recorded speech from diverse speakers to assess whether TTS artifacts influence bias measurements and to validate findings across more realistic speech conditions.

3. **Expanded demographic dimensions**: Extend the dataset and evaluation to include additional demographic attributes such as accent, race, and socioeconomic indicators to provide a more comprehensive assessment of speaker-based bias in SLLMs.