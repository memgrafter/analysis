---
ver: rpa2
title: 'MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device
  Use Cases'
arxiv_id: '2402.14905'
source_url: https://arxiv.org/abs/2402.14905
tags:
- arxiv
- mobilellm
- your
- sharing
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates efficient large language models (LLMs)
  for on-device deployment, motivated by rising cloud costs and latency issues. It
  challenges the prevailing belief that data and parameter quantity solely determine
  model quality, instead emphasizing architecture's significance for sub-billion parameter
  LLMs.
---

# MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

## Quick Facts
- arXiv ID: 2402.14905
- Source URL: https://arxiv.org/abs/2402.14905
- Reference count: 40
- Primary result: Deep-and-thin architecture with embedding sharing and grouped-query attention achieves 2.7%/4.3% accuracy boost over previous 125M/350M state-of-the-art models

## Executive Summary
MobileLLM challenges the conventional wisdom that larger models and more data always yield better performance, demonstrating that architectural innovations can significantly improve sub-billion parameter language models for on-device deployment. The paper introduces a deep-and-thin architecture with embedding sharing and grouped-query attention, establishing new state-of-the-art results for small models while maintaining low latency and computational requirements. The authors show that small models can achieve comparable performance to much larger models (like LLaMA-v2 7B) on practical on-device tasks such as API calling, making them viable alternatives for edge computing scenarios.

## Method Summary
The authors propose a deep-and-thin architecture design for sub-billion parameter models, featuring more layers with smaller hidden dimensions to increase depth while reducing width. They introduce embedding sharing across layers to reduce parameters without sacrificing performance, and implement grouped-query attention to improve inference efficiency. The baseline MobileLLM model achieves significant accuracy improvements over previous state-of-the-art small models. Additionally, they develop an immediate block-wise weight-sharing approach (MobileLLM-LS) that further improves accuracy with no increase in model size and minimal latency overhead, demonstrating that architectural optimizations can outperform simply scaling parameters or data.

## Key Results
- 2.7%/4.3% accuracy improvement over previous 125M/350M state-of-the-art models on zero-shot common sense reasoning tasks
- MobileLLM significantly outperforms previous sub-billion models on chat benchmarks
- Achieves comparable correctness to LLaMA-v2 7B in API calling tasks while being 56x smaller

## Why This Works (Mechanism)
The deep-and-thin architecture works by leveraging depth over width, allowing the model to capture more complex hierarchical representations despite having fewer parameters per layer. Embedding sharing reduces redundancy across layers while maintaining representational capacity, and grouped-query attention improves inference efficiency by reducing the number of unique key-value heads that need to be computed. The block-wise weight-sharing approach (MobileLLM-LS) exploits the observation that certain architectural patterns can be shared across blocks without significant performance degradation, enabling further parameter reduction while maintaining accuracy.

## Foundational Learning
- **Deep-and-thin architecture**: More layers with smaller hidden dimensions instead of fewer wide layers - needed to maintain representational power while reducing per-layer computation; quick check: verify depth-to-width ratio maintains expressivity
- **Embedding sharing**: Reusing embedding parameters across layers - needed to reduce parameter count without losing semantic richness; quick check: ensure shared embeddings capture diverse semantic spaces
- **Grouped-query attention**: Reducing unique key-value heads - needed to speed up inference while preserving attention quality; quick check: confirm attention patterns remain diverse across groups
- **Block-wise weight-sharing**: Sharing parameters across architectural blocks - needed to minimize model size without accuracy loss; quick check: validate that shared blocks don't create performance bottlenecks
- **On-device optimization**: Balancing accuracy, latency, and memory - needed for practical deployment on resource-constrained devices; quick check: measure actual latency on target hardware
- **Sub-billion parameter regime**: Focused optimization for small models - needed because architectural principles differ from large-scale models; quick check: verify improvements scale appropriately within this parameter range

## Architecture Onboarding

**Component map:** Input -> Embedding Layer -> Deep-and-Thin Transformer Blocks (with Embedding Sharing) -> Grouped-Query Attention -> Output Layer

**Critical path:** Token embedding → Deep layers with shared embeddings → Grouped-query attention computation → Output projection

**Design tradeoffs:** Depth vs width (favoring depth), parameter efficiency vs representational capacity, inference speed vs attention quality, model size vs accuracy

**Failure signatures:** Degradation in attention diversity with aggressive grouping, loss of semantic richness with excessive embedding sharing, vanishing gradients in very deep thin architectures, accuracy plateaus despite increasing depth

**3 first experiments:** 1) Compare deep-and-thin vs wide-and-shallow architectures with matched parameters, 2) Test embedding sharing across different layer granularities, 3) Evaluate grouped-query attention with varying group sizes

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation primarily focused on zero-shot common sense reasoning and chat benchmarks, with limited testing on specialized domains or complex reasoning tasks
- Results are demonstrated within the sub-billion parameter regime, leaving uncertainty about scalability to larger or smaller models
- Claims about challenging parameter-centric approaches are supported within tested ranges but may not generalize to all model scales or tasks

## Confidence
- High confidence in the technical implementation of deep-and-thin architecture and weight-sharing mechanisms
- Medium confidence in generalization of architectural findings beyond tested model sizes and tasks
- Medium confidence in practical significance of latency improvements given minimal overhead reported

## Next Checks
1. Test MobileLLM architecture across a broader parameter range (1M-10B) to verify if architectural optimizations maintain effectiveness at different scales
2. Evaluate on domain-specific tasks (medical, legal, technical) to assess whether architecture's benefits transfer to specialized use cases
3. Conduct ablation studies comparing architectural changes against alternative optimizations like quantization, pruning, or different attention mechanisms to isolate specific contributions of proposed design choices