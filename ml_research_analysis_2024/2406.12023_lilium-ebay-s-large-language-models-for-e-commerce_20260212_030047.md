---
ver: rpa2
title: 'LiLiuM: eBay''s Large Language Models for e-commerce'
arxiv_id: '2406.12023'
source_url: https://arxiv.org/abs/2406.12023
tags:
- data
- training
- e-commerce
- language
- checkpoint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LiLiuM series of large language models (LLMs) was developed
  to address eBay's need for domain-specific language models, eliminating reliance
  on external models. The models, with 1B, 7B, and 13B parameters, were trained on
  3 trillion tokens of multilingual text, including general web data and eBay-specific
  e-commerce data.
---

# LiLiuM: eBay's Large Language Models for e-commerce

## Quick Facts
- arXiv ID: 2406.12023
- Source URL: https://arxiv.org/abs/2406.12023
- Reference count: 40
- Primary result: Domain-specific tokenizer achieves up to 34% faster text generation on e-commerce tasks compared to LLaMA-2

## Executive Summary
LiLiuM is a series of large language models (1B, 7B, 13B parameters) developed by eBay to eliminate reliance on external models for e-commerce applications. Trained on 3 trillion tokens of multilingual text including general web data and eBay-specific e-commerce data, the models incorporate advanced techniques like custom tokenizer development, data filtering, and checkpoint averaging. LiLiuM outperforms LLaMA-2 on non-English NLU tasks, machine translation, and e-commerce-specific tasks while achieving significant speed improvements through domain-specific vocabulary optimization.

## Method Summary
The LiLiuM models were trained using a Megatron-LM based framework with custom tokenizer support on 800 NVIDIA A100 80GB GPUs. The training incorporated 3D parallelism (data, tensor, pipeline) and distributed optimizer states, using mixed-precision training with FlashAttention-2. Models were trained on 3 trillion tokens from datasets including RefinedWeb, RedPajama-V2, e-commerce listings, products, code data, and machine translation data. Final models were selected through checkpoint averaging using the last ~20k iterations. The training used 65k vocabulary size for the tokenizer, with models configured as transformer decoder-only architectures with rotary position embeddings and SwiGLU activation.

## Key Results
- Achieved up to 34% faster text generation on eBay-specific downstream tasks compared to LLaMA-2
- Outperformed LLaMA-2 on non-English NLU tasks by 3.5 points on average
- Improved machine translation performance through inclusion of parallel data
- Demonstrated strong performance on e-commerce-specific tasks including perplexity, item selection, and aspect prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific tokenizer yields up to 34% faster decoding on e-commerce tasks
- Mechanism: Vocabulary trained on e-commerce data preserves frequent domain tokens (e.g., "Yugioh") as single tokens rather than splitting into subwords, reducing token sequence length during generation
- Core assumption: Frequent e-commerce terms benefit more from being single tokens than from being split into subwords
- Evidence anchors:
  - [abstract] "We develop our own tokenizer and model vocabulary, customized towards e-commerce. This way, we can achieve up to 34% speed-up in text generation on eBay-specific downstream tasks compared to LLaMA-2."
  - [section] "Our tokenizer that was trained with e-commerce data, keeps the word as a single token, since it appears frequently in the e-commerce domain. This means, our LLMs are roughly 4 times faster when generating this word."
  - [corpus] Weak: No direct corpus evidence provided for frequency analysis of "Yugioh" or similar tokens

### Mechanism 2
- Claim: Checkpoint averaging improves final model performance across tasks
- Mechanism: Averaging parameters from multiple checkpoints creates a model that better represents the training trajectory and avoids overfitting to the last checkpoint
- Core assumption: Later checkpoints capture different aspects of the solution space that averaging can combine beneficially
- Evidence anchors:
  - [section] "We find that averaging checkpoints from the last ≈20k iterations results in the best performance"
  - [section] "We perform checkpoint averaging for all three LiLiuM models and find that using checkpoints from the last ≈20k iterations works best for us"
  - [corpus] Weak: No direct corpus evidence; based on training methodology observations

### Mechanism 3
- Claim: Including multilingual and parallel data improves translation and non-English NLU performance
- Mechanism: Training on diverse language data teaches the model cross-lingual representations and translation patterns
- Core assumption: The model can learn transfer learning from multilingual data even when not explicitly fine-tuned for translation
- Evidence anchors:
  - [section] "We include some parallel machine translation data in the training to boost machine translation performance"
  - [section] "Adding non-English monolingual data already boosts the translation performance significantly. Adding parallel data further improves translation quality by a significant margin"
  - [section] "On non-English NLU tasks, we outperform the respective LLaMA-2 model by 3.5 points on average"
  - [corpus] Weak: Performance improvements observed but no detailed analysis of which languages or data types contributed most

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: LiLiuM models are trained to predict the next token given previous context, requiring understanding of sequential dependencies
  - Quick check question: How does the model handle long-range dependencies given a context size of 4096 tokens?

- Concept: Tokenization and vocabulary design
  - Why needed here: The custom tokenizer directly impacts model efficiency and performance on e-commerce tasks
  - Quick check question: What trade-offs exist between vocabulary size (65k vs 30k) and model performance?

- Concept: Mixed-precision training
  - Why needed here: Enables training with larger batch sizes and models on available GPU memory constraints
  - Quick check question: Why might bf16 mixed precision be preferred over full fp32 for this training setup?

## Architecture Onboarding

- Component map: Data loading -> preprocessing -> tokenization -> model forward pass -> loss calculation -> backpropagation -> parameter update
- Critical path: Data loading → preprocessing → tokenization → model forward pass → loss calculation → backpropagation → parameter update
- Design tradeoffs: Larger vocabulary improves domain-specific performance but increases embedding matrix size; more layers improve quality but increase computational cost
- Failure signatures: Training slowdowns may indicate hardware issues; loss spikes suggest gradient problems; poor downstream performance may indicate data quality issues
- First 3 experiments:
  1. Train a small model (1B) with different vocabulary sizes to measure impact on decoding speed and quality
  2. Test checkpoint averaging on a trained model to verify performance improvements
  3. Evaluate translation performance with and without parallel data inclusion to measure impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of structured e-commerce data in autoregressive language modeling impact the model's ability to generalize to other domains or tasks outside of e-commerce?
- Basis in paper: [explicit] The paper discusses the use of structured e-commerce data and its serialization for autoregressive language modeling.
- Why unresolved: The paper focuses on the impact of structured data on e-commerce-specific tasks but does not explore how this impacts generalization to other domains or tasks.
- What evidence would resolve it: Experiments comparing the model's performance on non-e-commerce tasks before and after incorporating structured e-commerce data would provide insights into its generalization capabilities.

### Open Question 2
- Question: What is the optimal balance between multilingual and monolingual data in training large language models to achieve the best performance across multiple languages?
- Basis in paper: [explicit] The paper discusses the use of multilingual data and its impact on non-English NLU tasks and machine translation.
- Why unresolved: The paper does not provide a detailed analysis of the optimal balance between multilingual and monolingual data for achieving the best performance across multiple languages.
- What evidence would resolve it: A systematic study varying the ratio of multilingual to monolingual data and measuring the performance across different languages and tasks would help determine the optimal balance.

### Open Question 3
- Question: How does the size of the model vocabulary affect the trade-off between model performance and computational efficiency in large language models?
- Basis in paper: [explicit] The paper discusses the development of a larger vocabulary tokenizer and its impact on text generation speed and model performance.
- Why unresolved: The paper provides insights into the benefits of a larger vocabulary but does not explore the trade-off between model performance and computational efficiency in detail.
- What evidence would resolve it: Experiments comparing the performance and computational efficiency of models with different vocabulary sizes would provide insights into the optimal vocabulary size for balancing performance and efficiency.

## Limitations

- The claimed 34% speed-up from domain-specific tokenization lacks detailed corpus analysis to verify frequency distributions of e-commerce terms
- The optimal checkpoint averaging window of 20k iterations appears heuristic without theoretical grounding
- Performance improvements on multilingual tasks lack detailed breakdown by language family or analysis of which data types drove gains

## Confidence

- High Confidence: Basic training methodology (3T tokens, 800 A100 GPUs, Megatron-LM framework) and downstream evaluation framework are well-documented and reproducible
- Medium Confidence: Checkpoint averaging methodology and its general effectiveness, though optimal iteration count appears heuristic
- Low Confidence: Domain-specific tokenizer speed claims, translation performance attributions, and whether gains on e-commerce tasks generalize to other domains

## Next Checks

1. **Tokenizer Efficiency Validation**: Analyze the frequency distribution of e-commerce-specific tokens in the training corpus and measure actual decoding speed improvements on a controlled set of domain-specific text samples, comparing token sequences with and without the custom tokenizer

2. **Averaging Hyperparameter Sensitivity**: Systematically vary the checkpoint averaging window (5k, 10k, 20k, 30k iterations) on a trained model to identify whether 20k is truly optimal or if performance is relatively stable across a range

3. **Cross-Domain Generalization Test**: Evaluate LiLiuM models on non-e-commerce domain benchmarks (general web, technical documentation, creative writing) to determine if the domain-specific optimizations create performance bottlenecks on general language tasks