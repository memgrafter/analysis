---
ver: rpa2
title: 'J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language
  Modeling'
arxiv_id: '2407.15828'
source_url: https://arxiv.org/abs/2407.15828
tags:
- speech
- dialogue
- data
- corpus
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces J-CHAT, a large-scale Japanese spoken dialogue
  corpus designed to support spoken dialogue language modeling. The corpus was constructed
  using a language-independent, fully automated method that collects spontaneous,
  acoustically clean dialogue data from YouTube and podcasts.
---

# J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling

## Quick Facts
- arXiv ID: 2407.15828
- Source URL: https://arxiv.org/abs/2407.15828
- Authors: Wataru Nakata; Kentaro Seki; Hitomi Yanaka; Yuki Saito; Shinnosuke Takamichi; Hiroshi Saruwatari
- Reference count: 22
- Primary result: Large-scale Japanese spoken dialogue corpus enabling improved generative spoken dialogue language models

## Executive Summary
This paper introduces J-CHAT, a large-scale Japanese spoken dialogue corpus constructed through a fully automated pipeline that collects spontaneous, acoustically clean dialogue data from YouTube and podcasts. The corpus contains 69k hours of Japanese dialogue data across 4.9 million dialogues. Using generative spoken dialogue language models (dGSLMs) trained on J-CHAT, the authors demonstrate improved naturalness and meaningfulness in dialogue generation compared to models trained on smaller or less diverse datasets. The work validates J-CHAT's utility for advancing spoken dialogue system research.

## Method Summary
J-CHAT was constructed using an automated pipeline: YouTube and podcast data collection → Whisper language identification → PyAnnote speaker diarization → Demucs noise removal → HuBERT feature extraction → k-means clustering (1000 units) → dGSLM training (speech-to-unit, unit LM, unit-to-speech) → HiFi-GAN vocoder. The resulting corpus contains 69k hours of Japanese dialogue data across 4.9 million dialogues. dGSLM models were trained using fairseq with 32 NVIDIA V100 GPUs, learning rate of 2 × 10⁻⁴, for 100,000 steps. Model performance was evaluated through MOS tests for naturalness and meaningfulness.

## Key Results
- J-CHAT contains 69k hours of Japanese dialogue data across 4.9 million dialogues collected automatically from YouTube and podcasts
- dGSLM models trained on J-CHAT show improved naturalness and meaningfulness compared to models trained on smaller or less diverse datasets
- Domain diversity (YouTube + podcasts) improves dialogue generation quality compared to single-domain training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fully automated data collection from YouTube and podcasts enables large-scale, spontaneous, acoustically clean Japanese dialogue speech corpus construction.
- Mechanism: YouTube/Podcast data collection → Whisper language ID → Speaker diarization → Noise removal → Clean dialogue segments
- Core assumption: Automated pipeline preserves high-quality dialogue characteristics and can scale without manual curation
- Evidence anchors:
  - [abstract] "fully automated method that collects spontaneous, acoustically clean dialogue data from YouTube and podcasts"
  - [section 3] Details of data collection, language identification, diarization, and noise removal steps
  - [corpus] 69k hours of data, 4.9M dialogues collected automatically
- Break condition: Automated filtering misclassifies non-dialogue or introduces artifacts; language ID accuracy < 80% or noise removal degrades speech quality

### Mechanism 2
- Claim: Domain diversity (YouTube + podcasts) improves dialogue generation naturalness and meaningfulness compared to single-domain training
- Mechanism: Multiple domains → Broader dialogue variety → Richer conversational patterns → Better model generalization
- Core assumption: Conversational styles and topics vary meaningfully across domains, and models benefit from exposure to this diversity
- Evidence anchors:
  - [abstract] "collected data from multiple domains by our method improve the naturalness and meaningfulness"
  - [section 5.3] Experimental comparison shows dGSLM-J-CHAT outperforms dGSLM trained on only YouTube or only podcasts
  - [corpus] Subset statistics show YouTube dialogues shorter/more numerous, podcast dialogues longer, both with similar speaker counts
- Break condition: Domain-specific artifacts dominate; diversity does not translate into model performance gains

### Mechanism 3
- Claim: HuBERT-based discretization + k-means clustering enables effective speech-to-unit conversion for dialogue modeling
- Mechanism: HuBERT feature extraction → k-means clustering (1000 units) → Discrete token sequence → Language model input
- Core assumption: HuBERT captures phonetic/linguistic features suitable for dialogue, and 1000 clusters provide sufficient granularity
- Evidence anchors:
  - [section 4.2] t-SNE visualization shows J-CHAT covers broader HuBERT feature space than STUDIES/JNV
  - [section 5.1] Experimental setup describes HuBERT-based discretization and 1000 clusters
  - [corpus] J-CHAT phonetic diversity supports effective clustering
- Break condition: Discretization loses critical dialogue structure; too few/too many clusters degrade model performance

## Foundational Learning

- Concept: Speaker diarization
  - Why needed here: To identify individual speakers and turn boundaries in multi-party dialogue audio for corpus segmentation
  - Quick check question: Can you explain how speaker diarization differs from speech recognition and why it's critical for dialogue corpus construction?

- Concept: Self-supervised speech representation (HuBERT)
  - Why needed here: To extract meaningful phonetic and linguistic features from raw audio for downstream dialogue modeling and unit discretization
  - Quick check question: What advantages does HuBERT offer over traditional MFCC or filterbank features for spoken dialogue modeling?

- Concept: Speech enhancement/source separation (Demucs)
  - Why needed here: To remove background music and noise from podcast audio while preserving dialogue quality
  - Quick check question: How does Demucs differ from traditional noise suppression techniques, and why is this important for maintaining dialogue naturalness?

## Architecture Onboarding

- Component map: YouTube/Podcast data collection → Whisper language ID → PyAnnote speaker diarization → Demucs noise removal → HuBERT feature extraction → k-means clustering (1000 units) → dGSLM (speech-to-unit + unit LM + unit-to-speech) → HiFi-GAN vocoder
- Critical path: Data collection → Language filtering → Dialogue segmentation → Noise removal → Feature extraction → Model training → Evaluation
- Design tradeoffs: Automated collection maximizes scale but may include lower-quality segments; denoising preserves quality but may alter natural speech characteristics; 1000 clusters balance granularity and model efficiency
- Failure signatures: Low language ID accuracy (p < 0.8) → poor Japanese filtering; high single-speaker dialogues (>80%) → non-dialogue content; denoising artifacts → degraded naturalness; insufficient phonetic diversity → poor model generalization
- First 3 experiments:
  1. Validate language ID accuracy on YouTube/podcast samples with manual verification
  2. Test speaker diarization on known dialogue datasets to ensure accurate turn segmentation
  3. Compare denoising output quality using objective metrics (PESQ, STOI) on podcast samples with/without background music

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does J-CHAT's corpus construction method generalize effectively to other languages beyond Japanese?
- Basis in paper: [explicit] The paper claims the method is "language-independent" but only demonstrates it for Japanese
- Why unresolved: The methodology was validated only on Japanese data sources (YouTube and podcasts with Japanese metadata). No experiments or evidence show the method works for other languages or scripts.
- What evidence would resolve it: Successful application and validation of the corpus construction pipeline on at least two other languages with different linguistic properties (e.g., Chinese, Arabic, or a language with non-Latin script).

### Open Question 2
- Question: What is the optimal dataset size and domain diversity for training effective generative spoken dialogue language models?
- Basis in paper: [inferred] The paper notes that dGSLM-podcast (trained on 58k hours) performed no better than dGSLM-YouTube (11k hours), suggesting size alone isn't sufficient, but doesn't identify what combination of size and diversity is optimal.
- Why unresolved: The experiments compared different domains but didn't systematically vary dataset sizes or measure the point of diminishing returns for either size or diversity.
- What evidence would resolve it: Controlled experiments varying dataset size while holding domain diversity constant, and vice versa, to identify the optimal balance for model performance.

### Open Question 3
- Question: How does the quality of dialogues in J-CHAT compare to manually curated dialogue corpora in terms of linguistic richness and conversational coherence?
- Basis in paper: [inferred] The paper emphasizes J-CHAT's large scale and spontaneous nature but acknowledges generated dialogues "often lack coherence" and compares against smaller corpora (STUDIES) only on phonetic diversity, not dialogue quality.
- Why unresolved: No direct comparison of dialogue quality metrics (coherence, turn-taking, topic consistency) between J-CHAT and smaller, manually curated corpora.
- What evidence would resolve it: Human evaluation studies comparing dialogue samples from J-CHAT against curated corpora across multiple quality dimensions beyond naturalness and meaningfulness.

### Open Question 4
- Question: What is the impact of different noise removal techniques on the downstream performance of spoken dialogue language models?
- Basis in paper: [explicit] The paper uses Demucs for noise removal but doesn't compare against other techniques or measure how noise removal quality affects model performance.
- Why unresolved: Only one noise removal method was applied, with no ablation studies or comparison to alternatives, making it unclear if this is the optimal approach.
- What evidence would resolve it: Systematic comparison of different noise removal techniques (including no removal) measuring their impact on both the acoustic quality of the corpus and the performance of trained dialogue models.

## Limitations
- Automated pipeline may include lower-quality segments or misclassify non-dialogue content
- Evaluation relies on subjective MOS ratings from limited native Japanese speakers
- No comparison of dialogue quality metrics between J-CHAT and manually curated corpora
- HuBERT discretization with 1000 clusters appears arbitrary without ablation studies

## Confidence

**High Confidence:** The corpus contains 69k hours of Japanese dialogue data across 4.9 million dialogues collected using the described automated pipeline. The methodology for data collection (YouTube/Podcast sourcing, Whisper language ID, speaker diarization, Demucs denoising) is clearly specified and technically sound.

**Medium Confidence:** The domain diversity (YouTube + podcasts) meaningfully improves dialogue generation naturalness and meaningfulness compared to single-domain training. While the experimental comparison shows dGSLM-J-CHAT outperforming single-domain variants, the absolute performance gains and their practical significance require further validation.

**Low Confidence:** The HuBERT-based discretization with 1000 clusters is optimal for Japanese spoken dialogue modeling. The choice of 1000 clusters appears somewhat arbitrary without ablation studies showing how different cluster counts affect model performance. The t-SNE visualization demonstrates coverage but doesn't prove that 1000 clusters capture the necessary granularity for dialogue generation.

## Next Checks

1. **Language ID Validation:** Conduct a manual verification study where native Japanese speakers review a stratified sample of 1,000 audio segments identified as Japanese by Whisper, classifying them as true positives, false positives, or ambiguous cases. This will establish the actual language identification accuracy and identify systematic errors in the filtering process.

2. **Denoising Impact Assessment:** Perform a controlled experiment comparing dialogue model performance using audio processed with different denoising configurations: (a) full Demucs processing, (b) mild denoising, (c) no denoising. Evaluate using both objective metrics (PESQ, STOI) and subjective MOS ratings to determine if aggressive denoising removes natural conversational elements that affect dialogue authenticity.

3. **Cluster Count Ablation:** Systematically vary the number of k-means clusters (500, 1000, 2000, 4000) in the HuBERT discretization step while keeping all other model parameters constant. Train dGSLM variants on each configuration and evaluate dialogue naturalness and meaningfulness to identify the optimal discretization granularity for Japanese spoken dialogue modeling.