---
ver: rpa2
title: 'Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference
  for Problem-Solving with Language Models'
arxiv_id: '2408.00724'
source_url: https://arxiv.org/abs/2408.00724
tags:
- inference
- sampling
- rebase
- voting
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies inference scaling laws and compute-optimal inference
  for large language models (LLMs) in problem-solving tasks. The authors investigate
  the trade-offs between model sizes and inference strategies, including greedy search,
  majority voting, best-of-n, weighted voting, and tree search algorithms.
---

# Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models

## Quick Facts
- arXiv ID: 2408.00724
- Source URL: https://arxiv.org/abs/2408.00724
- Authors: Yangzhen Wu; Zhiqing Sun; Shanda Li; Sean Welleck; Yiming Yang
- Reference count: 40
- Key outcome: Smaller models combined with advanced inference strategies like REBASE tree search can outperform larger models under the same compute budget on mathematical reasoning tasks.

## Executive Summary
This paper studies inference scaling laws and compute-optimal inference for large language models in problem-solving tasks. The authors investigate trade-offs between model sizes and inference strategies, finding that scaling inference compute with advanced strategies can be more computationally efficient than scaling model parameters. They propose REBASE, a reward model-guided tree search algorithm that achieves better cost-performance trade-offs than sampling-based methods. Their experiments show that Llemma-7B with REBASE consistently outperforms Llemma-34B across all tested inference strategies on the MATH benchmark.

## Method Summary
The authors fine-tune Llemma-7B and Llemma-34B models on MetaMath dataset using supervised fine-tuning. They train a reward model on Math-Shepherd dataset to score partial solutions. The paper evaluates multiple inference strategies including greedy search, sampling-based voting (majority and weighted), MCTS, and the proposed REBASE algorithm. REBASE uses a reward model to guide node expansion in tree search without requiring expensive rollouts. The experiments measure problem-solving accuracy as a function of inference compute (FLOPs) on MATH and GSM8K benchmarks.

## Key Results
- Smaller models with advanced inference strategies (like REBASE) achieve Pareto-optimal trade-offs in cost and performance
- Llemma-7B with REBASE consistently outperforms Llemma-34B across all tested inference strategies on MATH benchmark
- Sampling-based voting methods have inherent performance limits with diminishing returns as sample count increases
- REBASE is empirically shown to be compute-optimal compared to sampling and MCTS methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models combined with advanced inference strategies can outperform larger models under the same compute budget.
- Mechanism: By allocating more compute to inference-time strategies like REBASE tree search, smaller models can generate and evaluate more solution candidates, leading to better accuracy-cost trade-offs than larger models using simpler inference.
- Core assumption: The compute budget is fixed and the same for both small and large models, and advanced inference strategies can effectively utilize this compute to generate high-quality solutions.
- Evidence anchors:
  - [abstract]: "smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark."
  - [section 4.2]: "Fig. 4 and Fig. 5 shows the relationship between error rate and inference FLOPs for Llemma 7B and Llemma 34B using different inference strategies. Llemma-7B requires around 2× less total FLOPs than Llemma-34B to achieve comparable accuracy."
  - [corpus]: Found related work on compute-optimal problem solving and scaling laws, but direct evidence for this specific mechanism is not yet available in the corpus.

### Mechanism 2
- Claim: Sampling-based voting methods have inherent performance limits due to their convergence behavior.
- Mechanism: As the number of samples increases, the accuracy of sampling-based voting methods (like majority voting) converges to a limit determined by the model's output distribution, showing diminishing returns with more samples.
- Core assumption: The underlying model's output distribution does not change with the number of samples, and the convergence rate is exponential.
- Evidence anchors:
  - [section 3.1]: "Theorems 1 & 2 state the convergence of the accuracy with increasing number of samples, indicating that the performance gains of using more samples will saturate for any fixed models. The limit is determined by the likelihood of generating the correct answers through all possible reasoning paths."
  - [abstract]: "Our analysis shows performance limits and diminishing returns from sampling, pointing to the need for more sophisticated inference algorithms."
  - [corpus]: Found related work on scaling laws and inference strategies, but specific theoretical analysis of voting methods' convergence behavior is not yet available in the corpus.

### Mechanism 3
- Claim: REBASE tree search is compute-optimal compared to sampling and MCTS methods.
- Mechanism: REBASE uses a reward model to guide node expansion in the search tree, eliminating the need for expensive rollouts while ensuring enough candidate solutions for voting, leading to better cost-performance trade-offs.
- Core assumption: The reward model can accurately estimate the quality of intermediate nodes, and the node expansion budget is appropriately allocated based on these estimates.
- Evidence anchors:
  - [section 3.1.2]: "The REBASE tree search method...uses a process reward model to determine how much each node should be expanded at each depth...This saves inference compute compared to methods such as MCTS, since it does not involve estimate node quality with explicit rollouts."
  - [section 4.3]: "REBASE consistently achieves the best cost-performance tradeoffs, outperforming the sampling-based methods in all settings when fixing the model and the evaluation task...REBASE is the compute-optimal strategy at all inference compute budgets, with 7B typically the optimal model size."
  - [corpus]: Found related work on tree search and inference strategies, but specific evidence for REBASE's compute-optimality is not yet available in the corpus.

## Foundational Learning

- Concept: Scaling laws for neural networks
  - Why needed here: Understanding how model performance scales with model size, dataset size, and compute is crucial for interpreting the inference scaling laws studied in this paper.
  - Quick check question: According to the Chinchilla scaling laws, what is the optimal ratio of model parameters to training tokens for compute-optimal training?

- Concept: Inference strategies for LLMs
  - Why needed here: The paper compares various inference strategies (greedy search, majority voting, best-of-n, weighted voting, and tree search algorithms) and their impact on performance under different compute budgets.
  - Quick check question: How does weighted majority voting differ from standard majority voting in terms of candidate solution selection?

- Concept: Tree search algorithms (e.g., MCTS)
  - Why needed here: The paper introduces REBASE, a novel tree search algorithm, and compares it to MCTS in terms of compute-efficiency and performance.
  - Quick check question: What is the main difference between REBASE and MCTS in terms of node quality estimation and expansion?

## Architecture Onboarding

- Component map: Policy model (LLM) -> Inference strategy -> Reward model (if applicable) -> Final answer selection
- Critical path: Policy model → Inference strategy → Reward model (if applicable) → Final answer selection
- Design tradeoffs:
  - Model size vs. inference compute: Larger models may require more compute for inference, but can potentially achieve better performance with simpler inference strategies.
  - Inference strategy complexity vs. compute efficiency: More complex inference strategies (e.g., REBASE) may achieve better performance under the same compute budget, but may also require more sophisticated reward models and implementation.
  - Candidate solution quality vs. quantity: Generating more candidate solutions can potentially lead to better performance, but may also increase computational cost and require more sophisticated selection mechanisms.
- Failure signatures:
  - Suboptimal performance: If the chosen model size and inference strategy combination does not achieve the best possible performance under the given compute budget.
  - Inefficient use of compute: If the inference strategy does not effectively utilize the available compute budget, leading to suboptimal performance or unnecessary computational cost.
  - Poor generalization: If the model and inference strategy combination does not generalize well to new, unseen problems or datasets.
- First 3 experiments:
  1. Compare the performance of different model sizes (e.g., Llemma-7B vs. Llemma-34B) under a fixed compute budget using a simple inference strategy (e.g., greedy search).
  2. Evaluate the impact of increasing the number of candidate solutions on the performance of sampling-based voting methods (e.g., majority voting, weighted majority voting) using a fixed model size.
  3. Compare the performance and compute efficiency of REBASE to sampling and MCTS methods using a fixed model size and compute budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the REBASE algorithm's advantage on hard problems persist across different mathematical domains (e.g., algebra, geometry, calculus) beyond just MATH difficulty levels?
- Basis in paper: The paper shows REBASE outperforms sampling on MATH-hard problems (levels 3-5) compared to MATH-easy problems (levels 1-2), but doesn't test different mathematical domains.
- Why unresolved: The experiments only use MATH dataset which has problems categorized by difficulty levels, not by mathematical domains. Testing across domains would require new datasets and experimental setup.
- What evidence would resolve it: Running REBASE vs sampling comparisons on domain-specific math datasets (e.g., geometry-only, algebra-only) while controlling for difficulty levels would show if REBASE's advantage generalizes across mathematical domains.

### Open Question 2
- Question: What is the theoretical relationship between the reward model's quality and the convergence rate of REBASE compared to sampling-based methods?
- Basis in paper: The paper provides convergence bounds for sampling-based voting methods but doesn't theoretically analyze REBASE's convergence behavior or how reward model quality affects it.
- Why unresolved: The paper empirically shows REBASE outperforms sampling but doesn't provide theoretical analysis of why this happens or how reward model quality impacts performance.
- What evidence would resolve it: A theoretical analysis showing how the reward model's accuracy/error rate affects REBASE's node expansion decisions and final accuracy, potentially proving faster convergence than sampling methods.

### Open Question 3
- Question: How do inference scaling laws change when using chain-of-thought prompting versus direct answer generation across different model sizes?
- Basis in paper: The experiments use stepwise solution generation but don't compare chain-of-thought prompting with direct answer generation approaches.
- Why unresolved: The paper assumes stepwise solution generation but doesn't test whether inference scaling laws differ when models are prompted to generate reasoning steps versus directly answering questions.
- What evidence would resolve it: Experiments comparing inference scaling laws for chain-of-thought prompting versus direct answer generation across various model sizes and inference strategies would reveal if the observed scaling relationships depend on prompting approach.

### Open Question 4
- Question: Is there a universal optimal balance temperature (Tb) for REBASE across different model families and task domains, or does it need to be tuned per configuration?
- Basis in paper: The paper sets Tb=0.1 for all experiments but doesn't explore sensitivity to this hyperparameter or test if a single optimal value exists across configurations.
- Why unresolved: The paper fixes Tb=0.1 without exploring its impact on performance or whether this value is optimal across different models, tasks, and compute budgets.
- What evidence would resolve it: A systematic hyperparameter sweep of Tb values across multiple model families (e.g., Pythia, Llemma, Mistral) and task domains would reveal if there's a universal optimal value or if Tb needs task-specific tuning.

## Limitations

- The exact implementation details of REBASE, particularly the node expansion mechanism and reward assignment, are not fully specified, which could impact reproducibility.
- The theoretical analysis of voting methods' convergence behavior may not fully capture practical implementation constraints or the impact of model fine-tuning on convergence behavior.
- The compute-optimality of REBASE is demonstrated empirically but requires further validation across different model sizes, tasks, and implementation details.

## Confidence

- **High Confidence**: The finding that smaller models with advanced inference strategies can outperform larger models under the same compute budget is well-supported by experimental results on MATH and GSM8K benchmarks.
- **Medium Confidence**: The theoretical analysis of voting methods' convergence behavior is sound, but the practical implications may vary depending on implementation details and model characteristics.
- **Low Confidence**: The compute-optimality of REBASE compared to sampling and MCTS methods is demonstrated empirically, but the exact implementation details and hyperparameter choices could significantly impact the results.

## Next Checks

1. Implement and evaluate REBASE with varying reward model quality to determine the sensitivity of its compute-optimal advantage to reward model performance.
2. Conduct ablation studies on REBASE's node expansion mechanism to quantify the impact of different expansion strategies on compute efficiency and solution quality.
3. Test the performance limits of sampling-based voting methods with models of varying fine-tuning quality to assess the impact of model capacity on convergence behavior and performance ceilings.