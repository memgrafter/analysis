---
ver: rpa2
title: A Step-by-step Introduction to the Implementation of Automatic Differentiation
arxiv_id: '2402.16020'
source_url: https://arxiv.org/abs/2402.16020
tags:
- node
- function
- nodes
- graph
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a step-by-step tutorial for implementing automatic
  differentiation, a key component in deep learning. It addresses the challenge of
  teaching students the implementation of automatic differentiation systems, which
  is often too complex to directly demonstrate due to the sophistication of existing
  frameworks.
---

# A Step-by-step Introduction to the Implementation of Automatic Differentiation

## Quick Facts
- arXiv ID: 2402.16020
- Source URL: https://arxiv.org/abs/2402.16020
- Authors: Yu-Hsueh Fang; He-Zhe Lin; Jie-Jyun Liu; Chih-Jen Lin
- Reference count: 3
- Primary result: Complete tutorial for implementing forward-mode automatic differentiation with document, slides, and source code suitable for beginners

## Executive Summary
This paper presents a step-by-step tutorial for implementing automatic differentiation (AD), a fundamental component in deep learning frameworks. The authors address the pedagogical challenge of teaching AD implementation, which is often too complex to directly demonstrate due to the sophistication of existing frameworks like TensorFlow and PyTorch. The tutorial provides a streamlined approach to implementing a simple AD system, focusing on forward mode while offering motivation for each implementation detail to make the process natural and accessible.

## Method Summary
The tutorial introduces basic concepts of automatic differentiation, including computational graphs and the chain rule, then guides readers through implementation steps. It covers creating the computational graph, finding topological order, and computing partial derivatives. The approach extends the example from Baydin et al. (2018) into implementation details, providing motivation behind each step. The complete tutorial package includes documentation, slides, and source code designed for beginners to understand and implement their own automatic differentiation system.

## Key Results
- Complete tutorial package with document, slides, and source code for forward-mode AD implementation
- Step-by-step guidance covering computational graph creation, topological ordering, and derivative computation
- Natural implementation flow with motivation provided for each design decision
- Extension of Baydin et al. (2018) example into practical implementation details
- Educational material suitable for beginners learning automatic differentiation concepts

## Why This Works (Mechanism)
The tutorial works by breaking down the complex topic of automatic differentiation into manageable, sequential steps. By focusing on forward-mode AD first, it establishes foundational concepts before introducing more complex reverse-mode approaches. The inclusion of motivation for each implementation detail helps learners understand not just what to implement, but why each component is necessary, creating a natural flow that builds understanding incrementally.

## Foundational Learning
- Computational graphs: Represent mathematical expressions as directed acyclic graphs; needed to track operations and dependencies; quick check: can you draw the graph for f(x) = (x+2)*3?
- Chain rule: Fundamental calculus principle for differentiating composite functions; needed to compute derivatives through multiple operations; quick check: can you compute df/dx for f(x) = (x² + 1)³?
- Topological ordering: Linear ordering of nodes where each node comes before nodes it points to; needed to ensure correct evaluation order; quick check: can you find a valid order for graph A->B, A->C, B->D, C->D?
- Forward accumulation: Computes derivatives by traversing the computational graph forward; needed as the basic AD approach; quick check: can you compute derivatives by propagating values forward?
- Partial derivatives: Derivatives with respect to individual variables; needed for gradient computation; quick check: can you compute ∂f/∂x and ∂f/∂y for f(x,y) = x² + 2xy?

## Architecture Onboarding

**Component Map:**
- Input variables -> Computation nodes -> Computational graph -> Topological sort -> Derivative computation -> Output gradients

**Critical Path:**
1. Variable creation and operation recording
2. Graph construction from operations
3. Topological sorting of the graph
4. Forward derivative propagation through sorted nodes
5. Gradient extraction from final node

**Design Tradeoffs:**
- Forward vs reverse mode: Forward mode simpler to implement but less efficient for functions with many inputs
- Explicit vs implicit graph: Explicit graph easier to debug but uses more memory
- Static vs dynamic graph: Static graphs enable optimization but reduce flexibility

**Failure Signatures:**
- Cycle detection errors in graph construction
- Incorrect topological ordering leading to wrong derivative computation
- Missing operation implementations in the derivative propagation
- Memory leaks from improper graph cleanup

**3 First Experiments:**
1. Implement and test f(x) = x² + 2x + 1 to verify basic forward mode works
2. Test f(x,y) = x² + y² to verify multivariate derivative computation
3. Implement and test f(x) = sin(x) + cos(x) to verify trigonometric operation derivatives

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitation section suggests that extending the tutorial to reverse-mode automatic differentiation would be a valuable next step, as reverse-mode is more commonly used in deep learning applications.

## Limitations
- Focuses exclusively on forward-mode AD, which is less efficient than reverse-mode for deep learning applications
- Implementation details are simplified and may not capture all edge cases present in production systems
- Educational effectiveness is assumed rather than empirically validated with student outcomes
- Code examples are presented without extensive testing or verification against established AD frameworks

## Confidence
- High confidence in the correctness of basic forward-mode AD implementation steps
- Medium confidence in the pedagogical value for beginners
- Medium confidence in the completeness of the implementation details
- Low confidence in the tutorial's effectiveness without empirical validation

## Next Checks
1. Test the provided implementation against standard test cases from established AD frameworks to verify correctness
2. Conduct a controlled study with students learning AD to measure the tutorial's educational effectiveness
3. Implement and test the reverse-mode extension suggested by the authors to validate the tutorial's extensibility