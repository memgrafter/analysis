---
ver: rpa2
title: 'CaLMQA: Exploring culturally specific long-form question answering across
  23 languages'
arxiv_id: '2406.17761'
source_url: https://arxiv.org/abs/2406.17761
tags:
- questions
- question
- answer
- culturally
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CALMQA, the first textual multilingual long-form
  question answering dataset containing 51.7K culturally specific questions across
  23 languages. The authors define culturally specific questions as those referring
  to unique cultural concepts or having context-dependent answers, collected through
  web forum scraping and native speaker contributions.
---

# CaLMQA: Exploring culturally specific long-form question answering across 23 languages

## Quick Facts
- arXiv ID: 2406.17761
- Source URL: https://arxiv.org/abs/2406.17761
- Reference count: 40
- Primary result: 51.7K culturally specific questions across 23 languages reveal that LLMs produce significantly more factual errors (45%-52%) on culturally specific questions compared to culturally agnostic ones (64%-71%)

## Executive Summary
This paper introduces CALMQA, the first multilingual long-form question answering dataset containing 51.7K culturally specific questions across 23 languages. The authors define culturally specific questions as those referring to unique cultural concepts or having context-dependent answers, collected through web forum scraping and native speaker contributions. Evaluating seven state-of-the-art LLMs on 3,644 questions, they find that answers to culturally specific questions contain significantly more factual errors (45%-52%) compared to culturally agnostic questions (64%-71%). Surface-level issues like incorrect language generation and repetitions are particularly problematic for low-resource languages, with open-weight models performing substantially worse than closed models. Human evaluation across five languages confirms these findings, showing that culturally specific questions receive lower ratings and that factuality issues strongly predict answer quality.

## Method Summary
The paper presents a comprehensive evaluation framework for multilingual long-form question answering. The CALMQA dataset contains 51.7K culturally specific questions across 23 languages, collected through web forum scraping and native speaker contributions. The evaluation pipeline uses automatic metrics including FACTSCORE for factuality, LLM-as-a-Judge for relevance, and surface-level quality detection (language identification, repetition detection). Seven LLMs are evaluated: GPT-4, Claude-3-Opus, Gemini-1.5-Pro, LLaMA-3-70B, Aya-Expanse-32B, and Mixtral-8x22B. The authors employ translation-free data collection for low-resource languages and use GPT-4-O for translating answers to English for factuality evaluation.

## Key Results
- Culturally specific questions receive lower quality answers (45%-52% factual errors) compared to culturally agnostic questions (64%-71% factual errors)
- Surface-level issues like wrong language generation and repetitions are particularly problematic for low-resource languages
- Open-weight models perform substantially worse than closed models on low-resource languages
- Human evaluation confirms that culturally specific questions receive lower ratings and factuality issues strongly predict answer quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Culturally specific questions have more factual errors because models lack training data for those cultural concepts
- Mechanism: The model's factual precision drops on culturally specific questions because these questions require knowledge about concepts, entities, or events unique to specific cultures that are underrepresented in training data
- Core assumption: Training data distribution correlates with factual precision on downstream tasks
- Evidence anchors:
  - [abstract] "answers to culturally specific questions contain more factual errors (45%-52%) compared to culturally agnostic questions (64%-71%)"
  - [section 3.2] "Generated answers to culturally agnostic questions tend to be more factual (64%–71%) than answers to culturally specific questions (45%–52%)"
  - [corpus] Weak - corpus contains general LFQA papers but not specific analysis of cultural knowledge gaps
- Break condition: If training data becomes culturally balanced or models develop better cultural reasoning capabilities

### Mechanism 2
- Claim: Low-resource languages suffer more surface-level issues because models lack language-specific fine-tuning
- Mechanism: Models produce more wrong-language outputs and repetitions for low-resource languages due to insufficient exposure during training and lack of language-specific adaptation
- Core assumption: Model performance degrades proportionally to training data scarcity for a given language
- Evidence anchors:
  - [abstract] "Surface-level issues like incorrect language generation and repetitions are particularly problematic for low-resource languages"
  - [section 3.2] "Open-weight models are comparable to their closed counterparts on high- and mid-resource languages... The closed models significantly outperform the open models on the low-resource languages"
  - [corpus] Weak - corpus has general multilingual QA papers but lacks specific analysis of surface-level issues in low-resource settings
- Break condition: If models receive targeted fine-tuning on low-resource language data or develop better cross-lingual transfer

### Mechanism 3
- Claim: Translation-based evaluation pipelines introduce errors that affect factuality and relevance scores
- Mechanism: Automatic evaluation metrics rely on translating non-English answers to English, which introduces translation errors that propagate to claim extraction and verification steps
- Core assumption: Translation quality directly impacts downstream evaluation accuracy
- Evidence anchors:
  - [section 3.1] "we translate our questions and answers into English using GPT-4 O. Then, we apply the claim extraction and verification pipeline"
  - [section 5] "Our automatic evaluation relies on surface-level measures such as language detection and token repetitions... it is possible then these factors influenced the results"
  - [corpus] Weak - corpus has evaluation papers but not specific analysis of translation pipeline impacts
- Break condition: If evaluation pipelines incorporate language-specific claim extraction or develop multilingual factuality metrics

## Foundational Learning

- Concept: Cultural specificity in NLP
  - Why needed here: The paper defines culturally specific questions as those referring to unique cultural concepts or having context-dependent answers, which is fundamental to understanding the dataset and evaluation
  - Quick check question: What distinguishes culturally specific questions from culturally agnostic questions in the CALMQA dataset?

- Concept: Multilingual evaluation challenges
  - Why needed here: The paper highlights issues with lexical metrics not correlating well across languages and the need for translation-free data collection
  - Quick check question: Why did the authors choose translation-free data collection methods for low-resource languages?

- Concept: Long-form question answering evaluation
  - Why needed here: The paper uses multiple evaluation dimensions (surface quality, factuality, relevance) rather than standard lexical metrics for LFQA
  - Quick check question: What are the three main evaluation dimensions used in the paper, and why are standard metrics insufficient?

## Architecture Onboarding

- Component map: Data collection → Automatic filtering → Model evaluation pipeline → Human evaluation → Analysis
- Critical path: Question collection → Model generation → Surface-level issue detection → Factuality/relevance evaluation → Human validation
- Design tradeoffs: Translation-free vs translation-based data collection, automatic vs human evaluation, cost vs coverage
- Failure signatures: High surface-level errors indicate language model limitations; low factuality scores indicate knowledge gaps; poor relevance scores suggest misunderstanding
- First 3 experiments:
  1. Replicate surface-level issue detection pipeline on a small sample to verify language identification accuracy
  2. Run factuality evaluation on a subset of answers to validate translation pipeline quality
  3. Compare automatic evaluation metrics against human ratings on a small validation set to assess correlation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more comprehensive evaluation metrics for multilingual long-form QA that assess fluency and completeness beyond just surface-level issues?
- Basis in paper: [inferred] The paper acknowledges that automatic evaluation relies on surface-level measures like language detection and token repetitions, which don't assess fluency or completeness of outputs that lack these issues
- Why unresolved: Current automatic evaluation methods are limited to identifying basic errors (wrong language, repetitions) but cannot evaluate the overall quality of answers that pass these surface-level checks
- What evidence would resolve it: Development and validation of new metrics that can assess fluency, coherence, and completeness of multilingual long-form answers, potentially through human evaluation studies or correlation with human ratings

### Open Question 2
- Question: What is the impact of translation quality on the evaluation of factuality and relevance for non-English LLM-generated answers?
- Basis in paper: [explicit] The paper notes that factuality evaluation depends on the quality of translation when translating answers into English for claim extraction and verification
- Why unresolved: The paper uses GPT-4-O for translation but doesn't systematically evaluate how translation errors might affect factuality scores or relevance judgments
- What evidence would resolve it: Controlled studies comparing factuality/relevance scores when using different translation methods (human vs machine translation) or evaluating the correlation between translation quality and evaluation metrics

### Open Question 3
- Question: How can we improve cross-lingual transfer to address data scarcity for underrepresented languages like Afar?
- Basis in paper: [explicit] The paper concludes that improving cross-lingual transfer to address data scarcity may help underrepresented languages like Afar
- Why unresolved: The paper identifies this as a need but doesn't explore specific methods or architectures for improving cross-lingual transfer for low-resource languages
- What evidence would resolve it: Experiments testing different cross-lingual transfer techniques (multilingual pretraining, few-shot learning, knowledge distillation) on low-resource languages and measuring their impact on answer quality

## Limitations

- Translation-based evaluation pipeline may introduce errors that affect factuality and relevance scores for non-English answers
- Human evaluation was limited to only five languages and 200 questions, limiting generalizability across the full 23-language spectrum
- The paper doesn't explore whether observed cultural knowledge gaps stem from training data distribution or from models' inherent inability to reason about cultural contexts

## Confidence

**High Confidence**: The finding that culturally specific questions receive lower quality answers (45%-52% factual errors) compared to culturally agnostic questions (64%-71% factual errors) is well-supported by multiple evaluation methods and human validation. The superiority of closed models over open-weight models for low-resource languages is also consistently demonstrated across different metrics.

**Medium Confidence**: The interpretation that surface-level issues in low-resource languages stem from training data scarcity is plausible but could also reflect evaluation pipeline limitations, particularly language identification failures. The claim that FACTSCORE's lower scores on low-resource languages indicate factuality issues rather than evaluation pipeline problems requires further validation.

**Low Confidence**: The paper's assertion that the FACTSCORE pipeline accurately captures factuality across 23 languages is difficult to verify without ground-truth factuality annotations in all languages. The human evaluation results may not fully represent the broader low-resource language landscape due to the limited language coverage.

## Next Checks

1. **Translation Pipeline Validation**: Manually validate a random sample of 100 translated answers from low-resource languages to assess translation quality and its impact on factuality scores. Compare FACTSCORE results with and without translation to isolate pipeline effects.

2. **Language-Specific Factuality Annotation**: Conduct human factuality annotation on a subset of answers in 3-4 languages (including at least one low-resource language) to validate whether automatic FACTSCORE results accurately reflect ground-truth factuality.

3. **Cross-Lingual Transfer Analysis**: Compare model performance on culturally specific questions across language families to determine whether performance correlates with typological similarity or training data overlap, helping distinguish between knowledge gaps and evaluation pipeline issues.