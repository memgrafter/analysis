---
ver: rpa2
title: A Diagonal Structured State Space Model on Loihi 2 for Efficient Streaming
  Sequence Processing
arxiv_id: '2409.15022'
source_url: https://arxiv.org/abs/2409.15022
tags:
- loihi
- processing
- jetson
- arxiv
- recurrent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first neuromorphic hardware implementation
  of a structured state-space model (S4D) on Intel's Loihi 2 processor, demonstrating
  efficient token-by-token processing for streaming sequence applications. The key
  insight is that S4D's diagonal structure and recurrent formulation align well with
  neuromorphic architectures where compute and memory are co-located.
---

# A Diagonal Structured State Space Model on Loihi 2 for Efficient Streaming Sequence Processing

## Quick Facts
- arXiv ID: 2409.15022
- Source URL: https://arxiv.org/abs/2409.15022
- Reference count: 28
- Primary result: First neuromorphic implementation of S4D achieving 1000× energy savings over GPU for streaming sequence processing

## Executive Summary
This paper presents the first neuromorphic hardware implementation of a structured state-space model (S4D) on Intel's Loihi 2 processor, demonstrating efficient token-by-token processing for streaming sequence applications. The key insight is that S4D's diagonal structure and recurrent formulation align well with neuromorphic architectures where compute and memory are co-located. The authors implemented S4D with quantization-aware fine-tuning, achieving competitive accuracy (99.20% on sMNIST, 96.16% on psMNIST, 84.13% on sCIFAR) with 67k-265k parameters. On token-by-token processing tasks, Loihi 2 consumed 1000× less energy with 75× lower latency and 75× higher throughput compared to a recurrent S4D implementation on Jetson Orin Nano. While GPUs excel at offline batch processing with convolutional implementations, this work demonstrates neuromorphic processors as a promising alternative for real-time streaming applications requiring sequential token processing.

## Method Summary
The authors implemented a diagonal structured state-space model (S4D) on Loihi 2 by leveraging its diagonal A matrix to enable independent neuron dynamics. The implementation used post-training quantization with 8-bit weights and quantization-aware fine-tuning to recover accuracy lost during fixed-point conversion. The model architecture consisted of input expansion, four S4D blocks with ReLU activations and mixing projections, and output reduction. Two model sizes were evaluated (67k and 265k parameters) on sequential MNIST, permuted sequential MNIST, and sequential CIFAR10 datasets. The implementation used both fall-through and pipelined processing modes to optimize the latency-throughput tradeoff.

## Key Results
- Loihi 2 achieved 99.20% accuracy on sMNIST, 96.16% on psMNIST, and 84.13% on sCIFAR with 67k-265k parameters
- For token-by-token processing, Loihi 2 consumed 1000× less energy with 75× lower latency and 75× higher throughput compared to Jetson Orin Nano
- On large batch processing, Loihi 2 showed competitive energy efficiency (1.25-3× better than Jetson) but lower throughput (2-4× worse)
- The implementation demonstrated neuromorphic hardware's advantage for real-time streaming applications requiring sequential token processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diagonal S4D structure enables independent neuron dynamics on Loihi 2
- Mechanism: S4D's diagonal A matrix means each hidden state evolves independently without inter-state connections, allowing each state to be computed entirely within a single programmable neuron on Loihi 2
- Core assumption: Loihi 2's programmable neurons can implement the continuous-time dynamics Ax(t) + Bu(t) directly
- Evidence anchors:
  - [section 3.1]: "Since the matrices A, B, and C of the S4D dynamics are fully diagonal, all hidden states in the S4D layers compute their dynamics fully independently without interactions with other states. Therefore, the state-space dynamics can be computed within the programmable neurons"
  - [abstract]: "The key insight is that S4D's diagonal structure and recurrent formulation align well with neuromorphic architectures where compute and memory are co-located"
- Break condition: If A matrix cannot be made diagonal while maintaining accuracy, or if Loihi 2 neurons cannot implement the required continuous-time dynamics

### Mechanism 2
- Claim: Quantization-aware fine-tuning recovers accuracy lost during fixed-point conversion
- Mechanism: Fake quantization during training simulates Loihi 2's fixed precision arithmetic, allowing the model to adapt its parameters to maintain accuracy under quantization constraints
- Core assumption: Straight-through estimator can properly propagate gradients through quantization operations
- Evidence anchors:
  - [section 3.2]: "Subsequently, we perform quantization aware fine tuning (QAFT) in recurrent mode to recover any loss in accuracy due to PTQ"
  - [section 3.2]: "To allow gradient flow in the backward computation, we use a straight-through estimator [22]"
- Break condition: If the straight-through estimator fails to provide meaningful gradients, or if the quantization granularity is too coarse for the model to recover accuracy

### Mechanism 3
- Claim: Neuromorphic architecture excels at token-by-token processing due to lack of memory movement overhead
- Mechanism: Loihi 2's compute-in-memory architecture eliminates the need to transfer weights between memory and compute units for each token, while GPUs incur significant overhead when processing recurrent SSMs token-by-token
- Core assumption: The memory-compute separation overhead in GPUs is the dominant factor in recurrent processing inefficiency
- Evidence anchors:
  - [abstract]: "This is in contrast to GPUs, where the separation of compute and memory makes them efficient only when processing highly structured compute and memory reads [15] (e.g., batching, convolution) and therefore inefficient when computing the recurrent formulation of SSMs"
  - [section 4.2]: "For token-by-token processing, Loihi 2 outperforms Jetson in all metrics on all datasets. Latency and energy are two to three orders of magnitude lower for Loihi 2"
- Break condition: If GPU memory bandwidth improves significantly, or if the recurrent computation itself becomes the bottleneck rather than memory movement

## Foundational Learning

- Concept: State-Space Models (SSMs)
  - Why needed here: Understanding how SSMs convert continuous-time dynamics to discrete-time implementations is crucial for implementing S4D on neuromorphic hardware
  - Quick check question: What is the relationship between the continuous-time formulation (Equation 1) and the discrete-time recurrent formulation (Equation 2) in SSMs?

- Concept: Diagonalization of state-space matrices
  - Why needed here: S4D relies on diagonalizing the A matrix to enable independent neuron dynamics, which is the key architectural fit with neuromorphic hardware
  - Quick check question: How does diagonalizing the A matrix affect the computational complexity and parallelism of SSM implementations?

- Concept: Quantization and fixed-point arithmetic
  - Why needed here: Loihi 2 only supports fixed precision computation, requiring understanding of quantization-aware training techniques
  - Quick check question: What is the purpose of the straight-through estimator in quantization-aware fine-tuning?

## Architecture Onboarding

- Component map:
  Input -> expansion layer (programmable neurons) -> S4D block 1 -> mixing -> S4D block 2 -> mixing -> S4D block 3 -> mixing -> S4D block 4 -> mixing -> reduction layer (programmable neurons) -> output
  Linear synapses for all projections between layers
  31-111 neurocores depending on model size

- Critical path:
  Input → expansion → S4D block 1 → mixing → S4D block 2 → mixing → S4D block 3 → mixing → S4D block 4 → mixing → reduction → output
  Each S4D block processes tokens sequentially in recurrent mode

- Design tradeoffs:
  - ReLU vs GLU/GeLU: Simpler activation chosen for increased sparsity and reduced complexity
  - No normalization layers: Removed to simplify implementation and reduce parameter count
  - No residual connections: Omitted to reduce complexity, may impact training stability

- Failure signatures:
  - Accuracy drops significantly after quantization: Indicates insufficient QAFT or overly aggressive quantization
  - Poor energy efficiency compared to expectations: May indicate suboptimal neurocore allocation or inefficient data movement
  - High latency despite efficient architecture: Could signal bottlenecks in inter-neurocore communication

- First 3 experiments:
  1. Verify single-neuron S4D dynamics by implementing a minimal 1-state S4D layer on a single neurocore and checking output against CPU reference
  2. Test quantization impact by running full model with and without QAFT on a small dataset to quantify accuracy recovery
  3. Compare fall-through vs pipelined processing modes on a streaming dataset to characterize the latency-throughput tradeoff

## Open Questions the Paper Calls Out
- How does the accuracy of S4D on Loihi 2 scale with model size and what is the optimal parameter count for different sequence lengths and datasets?
- What is the minimum batch size at which Jetson's convolutional implementation becomes more energy-efficient than Loihi 2's recurrent implementation for streaming applications?
- How would implementing the convolutional mode of S4D on Loihi 2 affect the latency-throughput tradeoff for streaming applications?
- How does the quantization-aware fine-tuning performance change when using different numbers of fine-tuning epochs or different quantization strategies?

## Limitations
- Limited to relatively small model sizes (67k-265k parameters) due to neuromorphic hardware constraints
- Only evaluated on standard sequence classification tasks, not tested on more complex sequential decision-making problems
- Fixed 8-bit weight quantization may limit model capacity and accuracy for more challenging tasks

## Confidence
- High confidence: The neuromorphic efficiency claims for token-by-token processing are well-supported by empirical measurements comparing Loihi 2 to Jetson Orin Nano, showing 1000× energy savings and 75× lower latency
- Medium confidence: The architectural claims about S4D's diagonal structure enabling independent neuron dynamics are theoretically sound but rely on the assumption that Loihi 2 neurons can implement the required continuous-time dynamics without approximation
- Medium confidence: The quantization-aware fine-tuning approach is standard practice, but the specific effectiveness depends on the straight-through estimator's ability to provide meaningful gradients through 8-bit quantization

## Next Checks
1. Test the S4D implementation on a more complex sequential task (e.g., language modeling or reinforcement learning) to evaluate scalability and real-world applicability
2. Characterize the accuracy-latency tradeoff by varying quantization precision (4-bit, 6-bit, 8-bit) and measuring the impact on both accuracy and energy efficiency
3. Implement a comparison with alternative neuromorphic SSM architectures (e.g., HiPPO-based models) to establish whether the diagonal structure provides unique advantages for neuromorphic deployment