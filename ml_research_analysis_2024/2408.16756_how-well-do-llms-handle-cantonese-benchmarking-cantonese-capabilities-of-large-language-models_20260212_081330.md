---
ver: rpa2
title: How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities of Large
  Language Models
arxiv_id: '2408.16756'
source_url: https://arxiv.org/abs/2408.16756
tags:
- cantonese
- shot
- llms
- internlm-2
- huggingface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks Cantonese language capabilities of large
  language models (LLMs), addressing the underrepresentation of Cantonese in NLP research
  despite its 85+ million speakers. The authors introduce four new Cantonese benchmarks
  (Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU) and a translation dataset (Yue-TRANS),
  evaluating 35 mainstream LLMs across factual generation, mathematical logic, complex
  reasoning, general knowledge, and translation tasks.
---

# How Well Do LLMs Handle Cantonese? Benchmarking Cantonese Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2408.16756
- Source URL: https://arxiv.org/abs/2408.16756
- Reference count: 40
- Primary result: This paper benchmarks Cantonese language capabilities of large language models (LLMs), addressing the underrepresentation of Cantonese in NLP research despite its 85+ million speakers

## Executive Summary
This paper introduces four new Cantonese benchmarks (Yue-TruthfulQA, Yue-GSM8K, Yue-ARC-C, Yue-MMLU) and a translation dataset (Yue-TRANS) to evaluate 35 mainstream LLMs across factual generation, mathematical logic, complex reasoning, general knowledge, and translation tasks. The study addresses the significant gap in Cantonese language processing research, despite Cantonese having over 85 million speakers worldwide. Results show that while LLMs perform better in English and Mandarin, newer Qwen, Llama, Mixtral, and Yi series models demonstrate superior Cantonese capabilities, with GPT-4o excelling in translation quality.

## Method Summary
The authors constructed four new Cantonese benchmarks and a translation dataset to comprehensively evaluate LLM performance across five key tasks: factual generation, mathematical logic, complex reasoning, general knowledge, and translation. They tested 35 mainstream LLMs using standardized evaluation metrics and compared performance across English, Mandarin, and Cantonese. The benchmarks were designed to cover diverse linguistic phenomena in Cantonese, including colloquial expressions and domain-specific terminology. Model rankings were determined based on average performance across all benchmark tasks, with additional analysis of translation quality using both automated metrics and human evaluation where available.

## Key Results
- Newer Qwen, Llama, Mixtral, and Yi series models demonstrate superior Cantonese capabilities compared to older models
- GPT-4o excels in translation quality between Cantonese and other languages
- Significant performance gap exists between English/Mandarin and Cantonese across all tested models
- Code-switching handling remains a challenge for most models, indicating research opportunities in data augmentation

## Why This Works (Mechanism)
The effectiveness of newer models in Cantonese processing stems from their training on more diverse and representative multilingual datasets that include Cantonese-specific linguistic features. Models like Qwen, Llama, Mixtral, and Yi series likely benefited from improved tokenization strategies and attention mechanisms that better capture Cantonese's tonal variations and unique syntactic structures. The superior translation performance of GPT-4o suggests that enhanced cross-lingual transfer learning capabilities enable better handling of language pairs involving Cantonese.

## Foundational Learning
- Cantonese language characteristics: Understanding tonal variations and unique syntactic structures is essential for accurate model evaluation
  - Why needed: Cantonese has distinct linguistic features that differ significantly from Mandarin and English
  - Quick check: Verify that benchmarks include tonal and colloquial expression tests
- Benchmark construction methodology: Standardized evaluation metrics ensure fair comparison across diverse models
  - Why needed: Consistent evaluation framework enables meaningful performance comparisons
  - Quick check: Confirm benchmark tasks align with established NLP evaluation standards
- Multilingual model training: Models must handle multiple language families with varying linguistic structures
  - Why needed: Effective Cantonese processing requires understanding of language family differences
  - Quick check: Review model training data composition for language family representation

## Architecture Onboarding

Component Map:
Benchmark Construction -> Model Evaluation -> Performance Analysis -> Model Ranking

Critical Path:
The critical path involves creating comprehensive Cantonese benchmarks that accurately represent linguistic phenomena, evaluating 35 diverse models systematically, analyzing performance gaps between languages, and deriving actionable recommendations for model selection and future research directions.

Design Tradeoffs:
- Benchmark specificity vs. generalizability: Highly specific Cantonese benchmarks may not capture broader language model capabilities
- Automated vs. human evaluation: Automated metrics provide scalability but may miss nuanced linguistic quality
- Model diversity vs. practical applicability: Testing 35 models provides comprehensive analysis but may overwhelm practical implementation

Failure Signatures:
- Poor performance on colloquial expressions indicates insufficient exposure to natural Cantonese usage
- Inconsistent translation quality suggests inadequate parallel corpus training
- Code-switching failures reveal limitations in handling mixed-language contexts

First Experiments:
1. Run initial model evaluations using simplified Cantonese test sets to establish baseline performance
2. Compare automated benchmark scores with human evaluation for translation tasks
3. Test code-switching scenarios to identify specific language boundary handling issues

## Open Questions the Paper Calls Out
The paper raises questions about the optimal approaches for data augmentation to improve Cantonese language model performance, particularly for code-switching scenarios. It also questions how benchmark performance translates to real-world Cantonese language processing tasks and what specific linguistic phenomena remain challenging for current models.

## Limitations
- Major uncertainties remain regarding the representativeness of the four newly created Cantonese benchmarks for the full range of Cantonese linguistic phenomena
- The translation dataset's quality depends on the expertise of annotators, though no validation methodology is described
- Limited cross-validation with existing Cantonese datasets prevents assessment of potential bias in the new benchmarks
- The study focuses primarily on mainstream models, potentially missing specialized Cantonese-focused architectures

## Confidence
- The claim that newer Qwen, Llama, Mixtral, and Yi series models demonstrate superior Cantonese capabilities: Medium confidence due to potential evaluation bias from the benchmark design
- The assertion that GPT-4o excels in translation quality: Medium confidence given the absence of human evaluation metrics beyond automated measures
- The paper's findings about research opportunities in data augmentation and code-switching handling: Medium confidence based on observed model limitations rather than systematic investigation

## Next Checks
1. Conduct human evaluation studies comparing benchmark results with real-world Cantonese task performance across different domains
2. Test benchmark generalizability by evaluating models on additional Cantonese datasets not used in benchmark creation
3. Perform ablation studies removing specific benchmark components to identify potential bias sources and their impact on overall model rankings