---
ver: rpa2
title: Is Less More? Exploring Token Condensation as Training-free Test-time Adaptation
arxiv_id: '2410.14729'
source_url: https://arxiv.org/abs/2410.14729
tags:
- token
- tokens
- adaptation
- clip
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving zero-shot inference
  performance of vision-language models (VLMs) like CLIP on unseen downstream datasets.
  It proposes a training-free test-time adaptation method called Token Condensation
  as Adaptation (TCA) that leverages token condensation techniques to refine token
  usage during inference.
---

# Is Less More? Exploring Token Condensation as Training-free Test-time Adaptation

## Quick Facts
- arXiv ID: 2410.14729
- Source URL: https://arxiv.org/abs/2410.14729
- Authors: Zixin Wang; Dong Gong; Sen Wang; Zi Huang; Yadan Luo
- Reference count: 40
- Primary result: Achieves up to 21.4% performance improvement over strongest baseline on cross-dataset benchmarks while reducing GFLOPs by 12.2% to 48.9%

## Executive Summary
This paper addresses the challenge of improving zero-shot inference performance of vision-language models (VLMs) like CLIP on unseen downstream datasets. It proposes Token Condensation as Adaptation (TCA), a training-free test-time adaptation method that leverages token condensation techniques to refine token usage during inference. TCA uses a domain-aware token reservoir to store representative anchor tokens, which guide cross-head token pruning and merging to remove class-irrelevant and class-ambiguous tokens. The method achieves significant performance improvements while reducing computational costs, with minimal hyperparameter dependency on both CLIP and SigLIP series.

## Method Summary
TCA is a training-free test-time adaptation method that improves visual-text alignment in VLMs by selectively condensing tokens during inference. The method maintains a domain-aware token reservoir (DTR) that stores anchor tokens from low-entropy samples. These anchor tokens guide cross-head token pruning and merging operations to remove class-irrelevant and class-ambiguous tokens. TCA also employs logits self-correction using stored anchor tokens to improve visual-text alignment without modifying model parameters. The approach transforms token condensation from an efficiency technique into a test-time adaptation strategy, achieving performance improvements of up to 21.4% over baselines while reducing GFLOPs by 12.2% to 48.9%.

## Key Results
- Achieves up to 21.4% performance improvement over strongest baseline on cross-dataset benchmarks
- Reduces GFLOPs by 12.2% to 48.9% while maintaining or improving performance
- Demonstrates minimal hyperparameter dependency on both CLIP and SigLIP series
- Shows effectiveness on CIFAR-100-Corrupted and cross-dataset benchmark with 10 datasets

## Why This Works (Mechanism)

### Mechanism 1
TCA improves visual-text alignment by selectively removing class-irrelevant and class-ambiguous tokens. It maintains a domain-aware token reservoir (DTR) that stores anchor tokens from low-entropy samples, which guide cross-head token pruning and merging. The core assumption is that class-irrelevant background tokens and class-ambiguous object tokens degrade visual-text alignment in VLMs.

### Mechanism 2
TCA achieves training-free test-time adaptation by leveraging stored domain anchor tokens for logits self-correction. The cosine similarity between current sample <cls> tokens and stored domain anchor tokens serves as a token-level classifier, providing auxiliary information to adjust predicted probabilities and improve visual-text alignment without modifying model parameters.

### Mechanism 3
TCA reduces computational costs while maintaining or improving performance through efficient token condensation. By pruning and merging tokens based on cross-head attention scores, TCA reduces the number of tokens processed while preserving essential information, resulting in lower GFLOPs (12.2% to 48.9% reduction) while achieving performance improvements (up to 21.4% over baselines).

## Foundational Learning

- **Vision-Language Models (VLMs)**: Models like CLIP learn generalizable image representations but struggle with zero-shot inference on unseen downstream datasets due to distribution shifts. This limitation is crucial because TCA addresses this specific challenge by adapting VLMs during test-time without additional training.

- **Token Condensation Techniques**: Originally designed to enhance vision transformer efficiency by refining token usage during inference. TCA builds upon these techniques, extending them beyond efficiency improvements to achieve training-free test-time adaptation.

- **Test-time Adaptation (TTA)**: Dynamically fine-tunes pre-trained models on unlabeled data batches to enhance generalization. TCA is positioned as a training-free alternative to traditional TTA methods, which require large batch sizes and extensive augmentations leading to high computational costs.

## Architecture Onboarding

- **Component map**: Input image -> Visual Encoder -> Token Condensation Module -> Condensed tokens -> Next layer -> Final <cls> token -> Logits Correction Module -> Visual-text alignment -> Classification

- **Critical path**: 1) Input image processed by visual encoder 2) Token condensation applied using DTR guidance 3) Condensed tokens passed to next layer 4) Final <cls> token compared with DTR for logits correction 5) Visual-text alignment used for classification

- **Design tradeoffs**: Token retention vs. computational efficiency (controlled by R parameter), reservoir size vs. memory usage (controlled by M parameter), pruning aggressiveness vs. information preservation (controlled by α parameter), logits correction weight vs. potential noise introduction (controlled by λ parameter)

- **Failure signatures**: Performance degradation when R < 0.7 (too aggressive pruning), poor adaptation on OOD benchmarks (concept shifts not captured by DTR), inconsistent results across different datasets (DTR not representative of diverse domains), computational overhead when reservoir size M is too large

- **First 3 experiments**: 1) Run TCA with default parameters (R=0.9, M=2) on cross-dataset benchmark to verify baseline performance improvements 2) Test TCA with varying R values (0.6, 0.7, 0.8, 0.9) to identify optimal computational-efficiency tradeoff 3) Evaluate TCA on OOD benchmark to assess limitations with concept shifts

## Open Questions the Paper Calls Out

### Open Question 1
How can we quantify the capacity of TCA to mitigate domain shift, especially when the test domain diverges significantly from the training domain? The paper acknowledges TCA's limitations in handling out-of-distribution data but doesn't provide a quantitative measure for this limitation. Developing a metric to measure TCA's effectiveness across different levels of domain divergence would resolve this question.

### Open Question 2
What lightweight solutions can be developed for backpropagation and network updates to facilitate test-time adaptation in TCA? The paper suggests this is a potential area for future research but does not explore specific solutions or their impact on TCA's performance. Implementing and testing various lightweight backpropagation techniques would address this question.

### Open Question 3
How does the choice of merging center number K in TCA affect the model's performance, and what is the optimal strategy for selecting K? While the paper identifies K=2 as optimal in tested scenarios, it does not explore why this is the case or how to adapt this choice to different types of data or domain shifts. A comprehensive study on the relationship between K, data characteristics, and domain shift would resolve this question.

## Limitations
- TCA struggles with out-of-distribution benchmarks due to conceptual shifts not captured by the domain anchor token reservoir
- The method's reliance on CLIP-style visual-text alignment may not transfer effectively to other VLM architectures
- Computational efficiency gains come at the cost of potential information loss through aggressive token pruning

## Confidence

**High Confidence**: The core observation that VLMs like CLIP suffer from distribution shifts in zero-shot inference is well-established. The empirical demonstration that token condensation can reduce computational costs while maintaining reasonable performance is well-supported.

**Medium Confidence**: The claim of "up to 21.4% performance improvement over the strongest baseline" is supported by experiments but may be dataset-specific. The effectiveness of the logits self-correction mechanism shows promise but lacks extensive validation across diverse scenarios.

**Low Confidence**: The assertion that TCA has "minimal hyperparameter dependency" appears overstated. The paper shows sensitivity to R values below 0.7, and optimal performance requires careful tuning of multiple parameters that likely vary by dataset and model architecture.

## Next Checks

1. **Cross-architecture generalization test**: Implement TCA on non-CLIP VLM architectures (e.g., ALIGN, BASIC) to verify the claim of minimal hyperparameter dependency and effectiveness beyond the CLIP family. Measure performance variance when transferring optimal parameters from CLIP to other architectures.

2. **Anchor token quality analysis**: Systematically evaluate how the quality and diversity of domain anchor tokens affect TCA performance. Test scenarios where anchor tokens are deliberately sampled from incorrect or ambiguous classes to quantify the robustness of the reservoir mechanism.

3. **Long-tail distribution adaptation**: Evaluate TCA on datasets with long-tail class distributions where class-irrelevant tokens are more prevalent. Compare performance against traditional test-time adaptation methods that explicitly address class imbalance to determine if TCA's token-level approach can effectively handle skewed distributions.