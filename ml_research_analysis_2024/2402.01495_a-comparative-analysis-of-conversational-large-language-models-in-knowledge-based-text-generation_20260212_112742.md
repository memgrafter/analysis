---
ver: rpa2
title: A Comparative Analysis of Conversational Large Language Models in Knowledge-Based
  Text Generation
arxiv_id: '2402.01495'
source_url: https://arxiv.org/abs/2402.01495
tags:
- language
- triples
- text
- llms
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares the ability of four large language models to
  generate natural language text from semantic triples derived from knowledge graphs.
  The models are evaluated on the WebNLG dataset using BLEU, METEOR, TER, and BERTScore
  metrics.
---

# A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation

## Quick Facts
- arXiv ID: 2402.01495
- Source URL: https://arxiv.org/abs/2402.01495
- Reference count: 7
- The fine-tuned LLaMA-FT-7B model achieves the best performance in generating natural language text from semantic triples, outperforming GPT-3.5-Turbo and Vicuna-7B.

## Executive Summary
This paper evaluates four large language models on the task of generating natural language text from semantic triples derived from knowledge graphs. The models are compared using BLEU, METEOR, TER, and BERTScore metrics on the WebNLG+ 2020 dataset. The study demonstrates that fine-tuned LLaMA-FT-7B achieves the best performance, followed by GPT-3.5-Turbo and Vicuna-7B. Few-shot prompting and post-processing significantly improve the performance of smaller models, particularly LLaMA-7B. The research also identifies common issues in generated predictions, including inaccurate information, mistranslations, off-prompt responses, redundancy, and unlexicalized entities.

## Method Summary
The study compares four models: LLaMA-7B (base), LLaMA-FT-7B (fine-tuned on conversational triple-to-text data), Vicuna-7B (dialogue-tuned), and GPT-3.5-Turbo (zero-shot). Models are evaluated on the WebNLG+ 2020 dataset using both zero-shot and few-shot prompting approaches. Few-shot examples follow a system/assistant/user message structure. Post-processing removes "Output text" prefixes and repeated prompt fragments. LLaMA-FT-7B was fine-tuned on 26,422 conversations using LoRA with each example appearing approximately five times in different contexts.

## Key Results
- LLaMA-FT-7B achieves the highest performance across all metrics, with BERTScore of 0.96 matching state-of-the-art models
- Few-shot prompting improves LLaMA-7B's BLEU score from 0.06 to 0.11, while Vicuna-7B requires minimal post-processing
- GPT-3.5-Turbo performs well in zero-shot but lacks the fine-tuning advantage of LLaMA-FT-7B
- All models struggle with unlexicalized triples, off-prompt responses, and hallucinated content

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned LLaMA-FT-7B achieves the best performance because it was explicitly trained on conversational triple-to-text data in chat completion format. Fine-tuning adapts the model's weights to the target task, enabling better alignment with the structured input-output format required for triple verbalization. Training data diversity and repetition (26,422 conversations, each example ~5 times) allows the model to learn multiple phrasing patterns and edge cases.

### Mechanism 2
Few-shot prompting significantly improves smaller models (LLaMA-7B) by providing in-context examples that clarify the input-output format. In-context learning allows the model to infer task patterns without weight updates, compensating for lack of task-specific training. The few-shot examples must be representative and correctly formatted to guide the model.

### Mechanism 3
Post-processing removes unintended output artifacts (e.g., "Output text" prefixes) and improves lexical overlap metrics. Simple string filtering aligns generated text with reference format, boosting automatic metric scores without changing model weights. Post-processing rules are conservative enough not to strip meaningful content.

## Foundational Learning

- **Semantic triple structure (subject-property-object)**: Understanding the input format is essential for interpreting model performance and error types. Quick check: What is the difference between a triple and a sentence in the context of knowledge graphs?
- **BLEU/METEOR/TER/BERTScore evaluation metrics**: These metrics determine how well models match human reference outputs; knowing their focus (lexical vs. semantic) is critical for interpreting results. Quick check: Which metric would you use to measure semantic similarity between generated and reference texts?
- **Fine-tuning vs. in-context learning (few-shot)**: Knowing when to use parameter updates vs. prompt engineering guides resource allocation and model selection. Quick check: When would you prefer few-shot prompting over fine-tuning for a new task?

## Architecture Onboarding

- **Component map**: WebNLG triples → JSON chat completion format → Model inference → Post-processing → Metric calculation
- **Critical path**: 1. Load WebNLG triples → 2. Format as chat completion messages → 3. Generate text → 4. Post-process output → 5. Compute metrics → 6. Error analysis
- **Design tradeoffs**: Larger models (GPT-3.5-Turbo) require API access and are costlier vs. smaller fine-tuned models (LLaMA-FT-7B) that run locally. Few-shot prompting is cheaper than fine-tuning but may underperform on complex formats. Post-processing boosts metrics but does not fix semantic errors.
- **Failure signatures**: Off-prompt outputs (misinterpreting format), Unlexicalized triples (raw URIs remain), Redundancy loops (model stuck in generation loop), Hallucinations (adding info not in triples)
- **First 3 experiments**: 1. Run zero-shot on LLaMA-7B with WebNLG test set; verify BLEU ≈ 0.06. 2. Add three in-context examples to prompt; verify BLEU increases to ≈ 0.11. 3. Apply post-processing to both outputs; verify metric improvements without content loss.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of large language models on triple verbalization tasks generalize to more complex graph data structures beyond semantic triples? The study focused solely on semantic triples and did not explore more complex graph data structures like subgraphs or entire knowledge graphs.

### Open Question 2
How do the limitations of the employed test dataset (limited to English triples) affect the generalizability of the findings to multilingual benchmarks with morphologically rich languages? The study did not evaluate the models on multilingual benchmarks or languages with different morphological structures.

### Open Question 3
How do the findings of this study compare to the performance of other state-of-the-art models evaluated on the WebNLG dataset, such as Control Prefixes or T5-Large+Wiki+Position? The paper provides a limited comparison to a few specific models and does not offer a comprehensive analysis of how the LLMs' performance stacks up against the broader landscape of state-of-the-art models.

## Limitations

- Fine-tuning details for LLaMA-FT-7B lack complete methodological specifications (learning rates, optimization specifics)
- Study focuses exclusively on DBpedia-based triples, limiting generalizability to other knowledge graph domains
- Manual error analysis covers only 150 predictions, which may not fully represent the broader error landscape across the 1,779 test examples

## Confidence

- **High confidence**: The comparative ranking of models (LLaMA-FT-7B > GPT-3.5-Turbo > Vicuna-7B > LLaMA-7B) is well-supported by consistent metric improvements across all evaluation metrics
- **Medium confidence**: The claim that conversational fine-tuning specifically drives LLaMA-FT-7B's superior performance, while plausible, lacks direct ablation studies comparing conversational vs. non-conversational fine-tuning approaches
- **Low confidence**: The assertion that LLaMA-FT-7B would outperform GPT-3.5-Turbo on longer or more complex texts is speculative, as the study only evaluates on the WebNLG dataset with fixed-length outputs

## Next Checks

1. **Replicate fine-tuning methodology**: Implement the exact LoRA fine-tuning procedure with controlled hyperparameters to verify that conversational format and repetition frequency (5× per example) are the key drivers of performance gains.

2. **Cross-domain generalization test**: Evaluate the same models on a non-DBpedia knowledge graph dataset (such as Wikidata-based triples) to assess whether the performance hierarchy holds across different knowledge sources.

3. **Ablation study on prompt components**: Systematically remove or modify components of the few-shot prompt (examples, system/assistant/user structure, post-processing rules) to quantify their individual contributions to performance improvements.