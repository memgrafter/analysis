---
ver: rpa2
title: 'STRICTA: Structured Reasoning in Critical Text Assessment for Peer Review
  and Beyond'
arxiv_id: '2409.05367'
source_url: https://arxiv.org/abs/2409.05367
tags:
- reasoning
- assessment
- steps
- text
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces STRICTA, a framework for structured reasoning
  in critical text assessment tasks such as peer review. It formalizes assessment
  as an explicit, step-wise reasoning process modeled as a causal graph populated
  with expert interaction data.
---

# STRICTA: Structured Reasoning in Critical Text Assessment for Peer Review and Beyond

## Quick Facts
- arXiv ID: 2409.05367
- Source URL: https://arxiv.org/abs/2409.05367
- Authors: Nils Dycke; Matej Zečević; Ilia Kuznetsov; Beatrix Suess; Kristian Kersting; Iryna Gurevych
- Reference count: 40
- Primary result: Introduces STRICTA framework for structured reasoning in critical text assessment, demonstrating that while LLMs generate factually consistent content, they underperform in decision-making compared to humans

## Executive Summary
STRICTA formalizes critical text assessment tasks as explicit, step-wise reasoning processes modeled as causal graphs populated with expert interaction data. The framework breaks down assessment into interconnected reasoning steps (READ, EXTRACT, INFER) that form a directed acyclic graph, enabling interpretable decomposition and analysis of expert reasoning patterns. Applied to biomedical paper assessment, the framework collects over 4000 reasoning steps from 40+ experts across 22 papers and evaluates LLM assistance, finding that human oversight significantly improves LLM alignment with expert reasoning while revealing fundamental differences in how humans and LLMs approach decision-making tasks.

## Method Summary
STRICTA models expert assessment workflows as causal graphs with 45 interconnected reasoning steps, using Structural Causal Models (SCMs) to enable quantitative analysis of decision factors and counterfactual reasoning. The framework collects expert annotations following a fixed workflow structure, then estimates structural equations from this data to construct the causal model. LLMs are integrated through a graph-based program architecture that calls them step-by-step following the workflow graph, with answers from parent steps provided as inputs and a self-refinement loop incorporating the final verdict to ensure consistency. Evaluation uses metrics including BERT-F1, SummaC, TRUE, and F1 scores to compare LLM performance against human baselines across different reasoning steps.

## Key Results
- LLMs generate factually consistent content across reasoning steps but underperform in final decision-making compared to human experts
- Human oversight significantly improves LLM alignment with expert reasoning patterns and mitigates error propagation
- Transfer experiments show promising results when applying the workflow to NLP paper assessment with minimal adaptation
- Analysis reveals fundamental differences between human and LLM reasoning, with humans weighing assessment factors differently than LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal graph structure enables interpretable decomposition of expert reasoning into traceable steps
- Mechanism: The workflow graph breaks down assessment into interconnected reasoning steps (READ, EXTRACT, INFER) that form a directed acyclic graph, allowing step-by-step analysis and error localization
- Core assumption: Expert reasoning follows consistent patterns that can be captured in a fixed graph structure across different documents
- Evidence anchors: [abstract] "STRICTA breaks down the assessment into a graph of interconnected reasoning steps drawing on causality theory"; [section 3.2] "Solving a STRICTA problem means finding the most likely values for all unobserved Ehidden ⊂ C ∪T given I and partially observed values"
- Break condition: If expert reasoning varies significantly between documents or domains, fixed graph structure fails to capture necessary variability

### Mechanism 2
- Claim: SCM modeling enables quantitative analysis of decision factors and counterfactual reasoning
- Mechanism: By estimating structural equations from expert data, the SCM allows computation of average causal effects (ACE) and counterfactual interventions to understand how individual steps influence final verdicts
- Core assumption: The relationships between reasoning steps can be modeled as structural equations that capture causal rather than merely associational relationships
- Evidence anchors: [abstract] "This graph is populated based on expert interaction data and used to study the assessment process"; [section 4.3] "Using average causal effect estimation (ACE) (Section 3.1), we compare the effect of interventions on different steps on the final verdict"
- Break condition: If structural equations cannot be accurately estimated from limited expert data, causal analysis becomes unreliable

### Mechanism 3
- Claim: LLM integration through graph-based program architecture enables structured reasoning assistance
- Mechanism: LLMs are called step-by-step following the workflow graph, with answers from parent steps provided as inputs, and self-refinement loop incorporating the final verdict to ensure consistency
- Core assumption: LLMs can follow structured reasoning paths when provided with explicit graph-based prompts and can adapt to individual reasoning patterns
- Evidence anchors: [abstract] "Experiments with four state-of-the-art LLMs demonstrate that LLMs are prone to error propagation, but human oversight effectively mitigates this issue"; [section 5] "The LLM is called step-by-step, with the answers from parent steps provided as inputs, thereby enforcing the workflow's reasoning structure"
- Break condition: If LLMs cannot maintain consistency across the reasoning chain or fail to adapt to individual patterns, structured assistance breaks down

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: Provides formal framework for modeling cause-and-effect relationships in expert reasoning, enabling causal queries and counterfactual analysis
  - Quick check question: What distinguishes SCMs from Bayesian networks in terms of causal reasoning capabilities?

- Concept: Directed Acyclic Graphs (DAGs) in reasoning
  - Why needed here: Represents the workflow structure where each node depends only on its parents, ensuring consistent reasoning flow
  - Quick check question: How does topological ordering of DAG nodes relate to the sequential nature of the assessment workflow?

- Concept: Abductive reasoning vs deductive reasoning
  - Why needed here: The task of explaining a given verdict based on paper text is abductive (finding most likely explanation) rather than deductive (deriving conclusion from premises)
  - Quick check question: Why is paper assessment considered abductive reasoning rather than deductive reasoning?

## Architecture Onboarding

- Component map: Expert annotators → Workflow graph (READ/EXTRACT/INFER steps) → SCM with structural equations → LLM program architecture (step-by-step calls with refinement) → Evaluation metrics (BERT-F1, SummaC, TRUE, F1)
- Critical path: Data collection (expert annotations following workflow) → SCM construction (estimate structural equations) → Analysis (ACE, counterfactuals) → LLM assistance experiments (graph-based program with refinement)
- Design tradeoffs: Fixed graph structure provides consistency but may miss domain-specific nuances; text-based variables allow expressiveness but complicate causal modeling; human oversight improves LLM performance but reduces automation
- Failure signatures: Low inter-annotator agreement suggests workflow complexity issues; LLM performance degradation across steps indicates error propagation; high variability in specific steps suggests need for targeted assistance
- First 3 experiments:
  1. Replicate analysis on a small subset of papers to verify workflow structure and estimate structural equations
  2. Test LLM program architecture on simple reasoning steps before full workflow integration
  3. Compare LLM performance with and without human oversight on a few sample papers to establish baseline effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the STRICTA framework be effectively transferred to other critical text assessment domains beyond biomedical paper review?
- Basis in paper: [explicit] The authors mention transfer experiments to NLP paper assessment and outline steps for applying STRICTA to automatic essay scoring
- Why unresolved: The paper demonstrates transfer to one additional domain (NLP) with preliminary results, but systematic validation across multiple domains is needed
- What evidence would resolve it: Successful application and evaluation of STRICTA in at least 3-4 different text assessment domains (e.g., fact-checking, Wikipedia quality assessment, legal document review) with comparable dataset sizes and performance metrics

### Open Question 2
- Question: What specific architectural improvements could better address error propagation in LLM-assisted STRICTA workflows?
- Basis in paper: [explicit] The authors identify error propagation as a key limitation and mention Graph-of-Thought and backtracking architectures as promising directions
- Why unresolved: While the paper identifies the problem and suggests potential solutions, it doesn't implement or evaluate these architectural improvements
- What evidence would resolve it: Implementation and evaluation of LLM architectures with built-in backtracking capabilities (like Graph-of-Thought) showing improved performance compared to current sequential approaches

### Open Question 3
- Question: How can evaluation metrics be developed to better capture the quality of individual reasoning steps in STRICTA tasks?
- Basis in paper: [inferred] The authors note that current automatic metrics favor more extensive answers and may not properly evaluate efficient but concise responses
- Why unresolved: The paper relies on existing text generation metrics (BERT-F1, SummaC, TRUE) which the authors acknowledge have limitations for this specific task
- What evidence would resolve it: Development and validation of context-aware evaluation metrics specifically designed for step-by-step reasoning tasks that correlate well with human judgment across different step types

### Open Question 4
- Question: What are the optimal human-AI collaboration patterns for different types of reasoning steps in STRICTA workflows?
- Basis in paper: [explicit] The authors simulate human oversight scenarios but note that real-world interaction dynamics need further investigation
- Why unresolved: The paper only simulates human-AI collaboration without real user studies to understand actual interaction patterns
- What evidence would resolve it: Large-scale controlled user studies comparing different human-AI collaboration patterns (full oversight, partial assistance, autonomous) across various step types and measuring both performance and user satisfaction

## Limitations

- The fixed graph structure of 45 reasoning steps may not capture all domain-specific nuances across different assessment contexts
- Causal analysis depends on accurately estimating structural equations from expert data, which becomes challenging with limited annotations
- The LLM evaluation focuses on a specific biomedical domain with English papers, raising questions about generalizability to other domains or languages

## Confidence

**High Confidence:** The workflow graph structure and causal modeling approach (SCM framework) are well-established and theoretically sound. The LLM program architecture following graph-based reasoning is supported by prior work in structured reasoning.

**Medium Confidence:** The effectiveness of human oversight in improving LLM performance is demonstrated but may depend on specific implementation details and prompt engineering that are not fully specified. The transferability of reasoning patterns across papers and experts is promising but needs further validation.

**Low Confidence:** The generalizability of findings to domains beyond biomedical paper assessment and to non-English languages. The long-term sustainability of the fixed graph structure for evolving assessment practices.

## Next Checks

1. **Cross-domain transfer validation:** Test the STRICTA framework on peer review tasks in computer science or social sciences to evaluate domain transfer of the workflow graph and structural equations.

2. **Language generalization test:** Apply the framework to papers in languages other than English to assess whether the reasoning patterns and LLM performance transfer across languages.

3. **Expert sample size sensitivity:** Conduct ablation studies with varying numbers of expert annotators per paper (from 3 to 10) to determine the minimum sample size needed for reliable SCM estimation and analysis.