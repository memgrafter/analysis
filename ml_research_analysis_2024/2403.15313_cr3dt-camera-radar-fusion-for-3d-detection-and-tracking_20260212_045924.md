---
ver: rpa2
title: 'CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking'
arxiv_id: '2403.15313'
source_url: https://arxiv.org/abs/2403.15313
tags:
- detection
- tracking
- radar
- performance
- cr3dt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CR3DT, a camera-RADAR fusion model for 3D
  object detection and multi-object tracking in autonomous driving. The method addresses
  the performance gap between LiDAR-based and camera-only perception systems by leveraging
  the spatial and velocity information from RADAR sensors in addition to camera data.
---

# CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking

## Quick Facts
- arXiv ID: 2403.15313
- Source URL: https://arxiv.org/abs/2403.15313
- Authors: Nicolas Baumann; Michael Baumgartner; Edoardo Ghignone; Jonas Kühne; Tobias Fischer; Yung-Hsu Yang; Marc Pollefeys; Michele Magno
- Reference count: 38
- Primary result: CR3DT achieves 35.1% mAP and 45.6% NDS on nuScenes 3D detection, with 14.9% AMOTA improvement for tracking over camera-only methods

## Executive Summary
CR3DT introduces a camera-RADAR fusion architecture for 3D object detection and multi-object tracking in autonomous driving. The method addresses the performance gap between LiDAR-based and camera-only perception systems by leveraging RADAR's spatial and velocity information. By fusing RADAR and camera data in Bird's-Eye View space and incorporating velocity estimates into the tracking pipeline, CR3DT achieves substantial improvements over state-of-the-art camera-only models while maintaining computational efficiency.

## Method Summary
CR3DT builds upon the BEVDet architecture, fusing RADAR and camera data in Bird's-Eye View space using a pillar-based encoding approach. The model projects six camera views into BEV using Lift Splat Shoot, encodes RADAR data in pillars, and concatenates these features for BEV encoding. A residual connection preserves RADAR information flow to the detection head. For tracking, CR3DT modifies data association by incorporating velocity similarity terms that directly compare predicted and observed velocities, improving object association across frames.

## Key Results
- Achieves 35.1% mAP and 45.6% NDS on nuScenes 3D detection validation set
- Reduces mean Average Velocity Error (mAVE) by 45.3% versus camera-only detector
- Achieves 38.1% AMOTA on nuScenes tracking validation set, a 14.9% improvement over camera-only tracking models
- Reduces ID switches by approximately 43% compared to camera-only tracking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CR3DT improves detection and tracking by fusing RADAR's velocity and spatial data with camera features in Bird's-Eye View (BEV) space.
- Mechanism: The model projects both camera and RADAR data into the BEV space, where camera features are lifted using Lift Splat Shoot (LSS) and RADAR data is encoded in pillars. These are concatenated and fed into a BEV encoder, allowing the network to leverage both the rich appearance information from cameras and the precise spatial and velocity information from RADAR sensors.
- Core assumption: BEV space is an effective intermediate representation that preserves spatial and velocity information from both sensor modalities.
- Evidence anchors:
  - [abstract]: "CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor."
  - [section]: "We opted to fuse these two types of sensor data within the Bird’s-Eye View (BEV) space, based on promising results demonstrated by using RADAR in this domain."
  - [corpus]: Weak evidence; no direct citation in corpus papers supporting BEV fusion for RADAR-camera fusion specifically.

### Mechanism 2
- Claim: The improved velocity estimates from RADAR enhance tracking accuracy by refining motion correlation in the data association step.
- Mechanism: CR3DT modifies the data association in the tracking pipeline by incorporating a velocity similarity term that directly compares the predicted and observed velocities of objects, rather than relying on pseudo-velocities derived from centroid positions. This allows the tracker to make more accurate associations between frames, reducing ID switches.
- Core assumption: RADAR provides more reliable velocity measurements than can be derived from camera-only methods, and this velocity information is useful for data association.
- Evidence anchors:
  - [abstract]: "With the rich velocity information contained in the RADAR data, the detector furthermore reduces the mean Average Velocity Error (mA VE) by 45.3 % versus the previously mentioned SotA camera-only detector."
  - [section]: "The data association step, referred to as DA in Fig. 1, was modified to leverage the improved positional detections and velocity estimates of CR3DT."
  - [corpus]: Weak evidence; corpus papers focus on sensor fusion but do not specifically address velocity-based tracking improvements.

### Mechanism 3
- Claim: Residual connections in the fusion architecture improve the flow of RADAR information to the detection head.
- Mechanism: After the BEV encoding layer, a residual connection is added that concatenates the RADAR data with the encoded BEV features. This allows the detection head to directly access RADAR information, which may contain complementary spatial and velocity cues not fully captured in the BEV encoding.
- Core assumption: RADAR data contains unique information that benefits from being passed directly to the detection head, and the residual connection does not disrupt the BEV encoding.
- Evidence anchors:
  - [section]: "Furthermore, the ablation study discussed in Section V-B showed, that it is beneficial to add a residual connection to the output of the feature encoding layer of dimension (256x128x128), leading to a final input size to the CenterPoint [8] detection head of ((256+18)x128x128)."
  - [corpus]: No direct evidence in corpus papers for the use of residual connections in RADAR-camera fusion.

## Foundational Learning

- Concept: Bird's-Eye View (BEV) representation
  - Why needed here: CR3DT operates in BEV space to fuse camera and RADAR data and perform detection and tracking.
  - Quick check question: What is the purpose of projecting image and RADAR data into BEV space in CR3DT?

- Concept: Sensor fusion in autonomous driving
  - Why needed here: CR3DT fuses RADAR and camera data to improve perception performance beyond what either sensor can achieve alone.
  - Quick check question: Why is sensor fusion beneficial for 3D object detection and tracking in autonomous driving?

- Concept: Data association in multi-object tracking
  - Why needed here: CR3DT uses an improved data association step that leverages velocity estimates to associate detections across frames.
  - Quick check question: How does CR3DT's data association step differ from traditional methods in multi-object tracking?

## Architecture Onboarding

- Component map:
  Input: 6 camera views (RGB images, 704x256), 5 RADAR sensors (BEV grid, 128x128, 18 features per cell) -> Backbone: ResNet-50 for image feature extraction -> View Transformer: Lift Splat Shoot (LSS) for projecting image features to BEV -> RADAR Encoder: Pillar-based encoding of RADAR data -> Fusion: Concatenation of image and RADAR features in BEV space, with residual connection -> BEV Encoder: ResNet-based encoding of fused features -> Detection Head: CenterPoint for 3D object detection -> Tracking Head: CC-3DT++ for multi-object tracking, using velocity estimates for data association

- Critical path: Image and RADAR data → BEV projection and encoding → Fusion → Detection and Tracking

- Design tradeoffs:
  - Sensor choice: RADAR vs. LiDAR - CR3DT uses RADAR for cost-effectiveness and velocity information, but RADAR data is sparser and noisier than LiDAR.
  - Fusion strategy: Intermediate fusion in BEV space vs. early or late fusion - CR3DT uses intermediate fusion to leverage the benefits of BEV representation.
  - Velocity utilization: Explicit use of velocity in tracking vs. implicit motion cues - CR3DT explicitly uses RADAR velocity for data association, improving tracking accuracy.

- Failure signatures:
  - Detection performance degrades if RADAR data is too sparse or noisy, or if the BEV projection loses critical information.
  - Tracking performance degrades if velocity estimates are unreliable, or if the data association step is not properly tuned.

- First 3 experiments:
  1. Ablation study on the fusion strategy: Compare CR3DT with and without RADAR data, and with different fusion approaches (e.g., voxelization vs. pillar encoding).
  2. Ablation study on the velocity similarity term: Compare CR3DT's tracking performance with and without the explicit velocity similarity term in the data association step.
  3. Ablation study on the residual connection: Compare CR3DT's performance with and without the residual connection that passes RADAR data directly to the detection head.

## Open Questions the Paper Calls Out
None

## Limitations
- Data Sparsity and Noise: CR3DT's effectiveness may be limited by RADAR's inherent sparsity and noise characteristics, particularly in complex scenarios with many small or distant objects.
- Generalization to Other Datasets: The evaluation is primarily conducted on the nuScenes dataset, with limited evidence for performance on other datasets or real-world deployments with different RADAR configurations.
- Computational Overhead: While using RADAR instead of LiDAR for cost-effectiveness, the fusion architecture and additional tracking head may introduce significant computational overhead not thoroughly analyzed.

## Confidence
- Detection Performance Improvement: Medium
- Tracking Performance Improvement: Medium
- Velocity Utilization Effectiveness: Low

## Next Checks
1. **Ablation Study on RADAR Data Quality**: Evaluate CR3DT's performance across different levels of RADAR data quality (e.g., varying RADAR sensor counts, different noise levels, or simulated RADAR dropout scenarios) to understand the method's robustness to RADAR sensor limitations.

2. **Cross-Dataset Generalization Test**: Train and evaluate CR3DT on a different autonomous driving dataset (e.g., Waymo Open Dataset or Argoverse) to assess the model's generalizability beyond nuScenes and validate the fusion approach in diverse environments and sensor configurations.

3. **Computational Efficiency Analysis**: Conduct a detailed analysis of inference time, memory usage, and power consumption for CR3DT compared to camera-only and LiDAR-based methods, including profiling on embedded hardware representative of real autonomous driving systems.