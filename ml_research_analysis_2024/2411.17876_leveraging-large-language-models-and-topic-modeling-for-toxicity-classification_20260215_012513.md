---
ver: rpa2
title: Leveraging Large Language Models and Topic Modeling for Toxicity Classification
arxiv_id: '2411.17876'
source_url: https://arxiv.org/abs/2411.17876
tags:
- topic
- toxicity
- full
- classification
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of bias in toxicity classification
  models, particularly the influence of annotator positionality on model performance.
  The authors propose a topic modeling-enhanced fine-tuning approach, using Latent
  Dirichlet Allocation (LDA) to cluster toxic tweets into topics and then fine-tuning
  BERTweet and HateBERT models on these topic-specific subsets.
---

# Leveraging Large Language Models and Topic Modeling for Toxicity Classification

## Quick Facts
- arXiv ID: 2411.17876
- Source URL: https://arxiv.org/abs/2411.17876
- Reference count: 29
- Key outcome: Topic modeling-enhanced fine-tuning improves toxicity classification F1 scores by mitigating annotator positionality bias

## Executive Summary
This paper addresses bias in toxicity classification models caused by annotator positionality—the influence of annotators' identities and experiences on toxicity labels. The authors propose using Latent Dirichlet Allocation (LDA) to cluster toxic tweets into topics, then fine-tuning BERTweet and HateBERT models on these topic-specific subsets. Results show that fine-tuning on individual topics improves F1 scores compared to using the full dataset or prominent baselines like GPT-4, PerspectiveAPI, and RewireAPI, suggesting topic modeling can effectively mitigate bias and enhance classification accuracy.

## Method Summary
The approach involves three main steps: (1) preprocessing the NLPositionality dataset of labeled toxic tweets, (2) applying LDA topic modeling to cluster tweets into 3, 6, or 10 topics, and (3) fine-tuning pre-trained BERTweet and HateBERT models on each topic-specific subset while freezing all layers except the classification head. The fine-tuned models are evaluated using F1 score and compared against baseline models and full-dataset fine-tuning results.

## Key Results
- Fine-tuning BERTweet and HateBERT on topic-specific subsets improved F1 scores compared to full-dataset fine-tuning
- Topic modeling reduced the influence of annotator positionality on model performance
- The approach outperformed prominent classification models including GPT-4, PerspectiveAPI, and RewireAPI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topic modeling reduces bias by isolating annotator positionality effects
- Mechanism: LDA clusters tweets into topics where annotator biases are more consistent within each cluster, reducing cross-topic confusion
- Core assumption: Annotator positionality correlates with topical content
- Evidence anchors: Topic modeling can help mitigate bias and improve accuracy; clustering toxicity dataset into 3, 6, and 10 topics
- Break condition: If annotator positionality is not correlated with topical content

### Mechanism 2
- Claim: Fine-tuning on topic-specific subsets improves F1 score over full dataset
- Mechanism: Models learn more discriminative features when trained on homogeneous data with consistent toxicity patterns
- Core assumption: Models benefit from training on homogeneous data subsets that reduce noise
- Evidence anchors: Fine-tuning on individual topics improved F1 score compared to full dataset; leveraging BERTweet's ability to perform tasks on limited context
- Break condition: If topic subsets are too small or contain mixed patterns

### Mechanism 3
- Claim: Transfer learning preserves generalization while adapting to task-specific context
- Mechanism: Pre-trained models retain broad linguistic knowledge while fine-tuning adapts them to toxicity classification
- Core assumption: Pre-trained models contain useful general features that transfer without catastrophic forgetting
- Evidence anchors: Transfer learning reduces risk of overfitting by preserving generalization; HateBERT fine-tuned on ToxiGen data
- Break condition: If fine-tuning overwrites too much pre-trained knowledge

## Foundational Learning

- Concept: Latent Dirichlet Allocation (LDA) topic modeling
  - Why needed here: To cluster toxic tweets into topics so fine-tuning can occur on homogeneous subsets
  - Quick check question: What statistical method is used to cluster the toxicity dataset into topics before fine-tuning?

- Concept: Transfer learning in transformer models
  - Why needed here: To adapt pre-trained BERTweet and HateBERT models to toxicity classification without losing general linguistic knowledge
  - Quick check question: Which layers are frozen during fine-tuning to preserve pre-trained knowledge while adapting to toxicity classification?

- Concept: Bias in human annotations
  - Why needed here: To understand why annotator positionality affects model performance and how topic modeling can mitigate this
  - Quick check question: What term describes the influence of annotators' identities and lived experiences on toxicity labels?

## Architecture Onboarding

- Component map: Dataset → Preprocessing → LDA Topic Modeling → Model Selection (BERTweet/HateBERT) → Fine-tuning → Evaluation
- Critical path: LDA clustering → fine-tuning on topic subsets → evaluation against baselines
- Design tradeoffs: Number of topics (3 vs 6 vs 10) balances expressiveness vs. dataset size constraints
- Failure signatures: Low F1 scores on fine-tuned models, inconsistent topic distributions, high variance across seeds
- First 3 experiments:
  1. Run LDA with k=3, 6, 10 topics and compare topic coherence scores
  2. Fine-tune BERTweet on each topic subset and compare F1 scores
  3. Compare topic-specific fine-tuning results against full-dataset fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the fine-tuned BERTweet and HateBERT models generalize to toxicity classification tasks on platforms beyond Twitter, such as Facebook or Reddit?
- Basis in paper: The authors mention leveraging BERTweet's ability to perform tasks on limited context and HateBERT's learned ability to understand implicit toxicity, suggesting potential application to other platforms
- Why unresolved: The paper only evaluates performance on the NLPositionality dataset derived from Twitter
- What evidence would resolve it: Testing the fine-tuned models on toxicity datasets from other platforms and comparing their performance to existing models

### Open Question 2
- Question: What is the impact of different topic modeling approaches (e.g., Non-negative Matrix Factorization, Hierarchical Dirichlet Process) on the performance of fine-tuned BERTweet and HateBERT models for toxicity classification?
- Basis in paper: The authors use LDA but acknowledge topic modeling was not as expressive as necessary due to variety in tweets
- Why unresolved: The paper only explores one topic modeling approach and does not investigate the impact of alternative methods
- What evidence would resolve it: Applying different topic modeling approaches and evaluating fine-tuned model performance on resulting topic-specific subsets

### Open Question 3
- Question: How do the fine-tuned BERTweet and HateBERT models perform on datasets with different levels of annotator positionality bias, and can they effectively mitigate the impact of such bias on toxicity classification?
- Basis in paper: The paper's motivation is to address biases introduced by human annotations in toxicity classification
- Why unresolved: The paper only evaluates the models on the NLPositionality dataset
- What evidence would resolve it: Testing the fine-tuned models on datasets with different levels of annotator positionality bias

## Limitations

- The study only uses a single dataset (NLPositionality) with tweets, limiting generalizability to other domains
- Critical implementation details like exact preprocessing steps and LDA parameter settings are omitted
- The evaluation focuses on F1 scores rather than explicit bias metrics, making it difficult to confirm positionality bias reduction

## Confidence

- Confidence Level: Medium for the core claim that topic modeling mitigates annotator positionality bias
- Confidence Level: Low regarding the generalizability of the approach to other datasets or platforms
- Confidence Level: Medium for the technical implementation despite missing critical details

## Next Checks

1. **Bias Measurement Validation**: Replicate the study while explicitly measuring annotator positionality bias to confirm that improved F1 scores correspond to reduced positionality bias

2. **Dataset Generalization Test**: Apply the topic modeling approach to at least two additional toxicity datasets to assess whether performance improvements generalize beyond the NLPositionality dataset

3. **Topic Number Sensitivity Analysis**: Systematically compare the performance impact of different topic numbers (k=2 through k=15) on both topic coherence and classification performance to determine whether k=3 is genuinely optimal