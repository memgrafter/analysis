---
ver: rpa2
title: On the Inductive Bias of Stacking Towards Improving Reasoning
arxiv_id: '2409.19044'
source_url: https://arxiv.org/abs/2409.19044
tags:
- midas
- training
- stacking
- reasoning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the inductive bias of stacking-based training
  strategies for language models, focusing on efficiency and reasoning performance.
  The authors propose MIDAS, a novel variant of gradual stacking that copies the middle
  block of layers from a smaller network to initialize a larger one, achieving up
  to 40% faster training compared to standard methods.
---

# On the Inductive Bias of Stacking Towards Improving Reasoning

## Quick Facts
- arXiv ID: 2409.19044
- Source URL: https://arxiv.org/abs/2409.19044
- Authors: Nikunj Saunshi; Stefani Karp; Shankar Krishnan; Sobhan Miryoosefi; Sashank J. Reddi; Sanjiv Kumar
- Reference count: 23
- Primary result: MIDAS variant achieves up to 40% faster training and strong inductive bias towards reasoning tasks

## Executive Summary
This paper investigates the inductive bias of stacking-based training strategies for language models, focusing on efficiency and reasoning performance. The authors propose MIDAS, a novel variant of gradual stacking that copies the middle block of layers from a smaller network to initialize a larger one, achieving up to 40% faster training compared to standard methods. Surprisingly, MIDAS not only improves training efficiency but also exhibits a strong inductive bias towards better downstream performance, particularly on reasoning tasks like reading comprehension and math problems, despite similar or slightly worse pretraining perplexity.

The authors construct reasoning primitives—simple synthetic tasks that isolate core reasoning capabilities—and demonstrate that MIDAS outperforms standard training on these tasks, both with and without fine-tuning. They hypothesize that this inductive bias arises from MIDAS's connection to looped models, which are known to simulate iterative solutions. Empirical results validate these findings across 1B, 2B, and 8B parameter models, highlighting the potential of stacking-based methods to enhance reasoning abilities in language models.

## Method Summary
The study uses UL2 decoder-only transformer architectures with 24/48/72 layers, training 1B, 2B, and 8B parameter models on a mixture of C4 (57%), Wikipedia (17%), GitHub (17%), and Arxiv (9%) datasets for 500B tokens. Three training strategies are compared: standard pretraining, gradual stacking (duplicating last L/k layers), and MIDAS (duplicating middle L/k layers) with PROP-α schedules (α=1,2,3). Models are evaluated on downstream reasoning tasks including TriviaQA, TyDiQA, NaturalQuestions, WebQuestions, SQuADv2, DROP, QuAC, CoQA, and math problems like ASDiv, MAWPS, SVAMP, using both 5-shot prompting and fine-tuning. Reasoning primitives include copying, variable assignment, and pre-school math tasks to isolate specific capabilities.

## Key Results
- MIDAS achieves up to 40% faster training compared to standard methods while maintaining or improving downstream reasoning performance
- MIDAS outperforms baseline and gradual stacking on reasoning primitives (0.6 vs 0.48 accuracy) and downstream reasoning tasks
- Models show better performance on closed-book reasoning tasks versus open-book tasks, indicating a reasoning rather than memorization bias
- Similarity analysis reveals MIDAS has a closer structure to looped models than gradual stacking, suggesting a connection between architecture and reasoning capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MIDAS copies the middle block of layers from a smaller model to initialize a larger one, which preserves the natural role of layers while inducing a looped-model-like similarity structure
- Mechanism: By duplicating the middle block rather than the last block, MIDAS maintains the distinct functional roles of different layers (as observed in ALBERT-style looped models) while still benefiting from parameter sharing across stages
- Core assumption: The middle layers of a deep network contain the most functionally similar and computationally important representations for reasoning tasks
- Evidence anchors: [abstract] "We propose a novel variant of gradual stacking called MIDAS that can speed up language model training by up to 40%"; [section] "MIDAS has a closer similarity structure to ALBERT style looped models than GRAD STACK" (Figure 2)

### Mechanism 2
- Claim: The inductive bias of MIDAS toward reasoning emerges from its connection to looped models, which are known to simulate iterative solutions
- Mechanism: By creating models with high cosine similarity between layer blocks (like looped models), MIDAS inherits the inductive bias to find iterative, algorithmic solutions rather than one-shot transformations
- Core assumption: Looped models' ability to simulate iterative solutions directly translates to better reasoning performance in language models
- Evidence anchors: [abstract] "Finally, we conjecture the underlying reason for this inductive bias by exploring the connection of stacking to looped models"; [section] "looped models have been conjectured to solve algorithmic problems by finding iterative solutions"

### Mechanism 3
- Claim: MIDAS achieves better downstream performance despite similar perplexity because it extracts more skills from the same data through its specific initialization strategy
- Mechanism: The middle-layer copying strategy creates a better optimization landscape that allows the model to learn reasoning primitives more effectively, even when the pretraining loss is similar to baseline
- Core assumption: Different training strategies can extract different levels of skills from the same pretraining data, and MIDAS is particularly effective at extracting reasoning skills
- Evidence anchors: [abstract] "MIDAS is not only training-efficient but surprisingly also has an inductive bias towards improving downstream tasks, especially tasks that require reasoning abilities"; [section] "MIDAS achieves better downstream evaluations despite performing similarly in terms of pretraining validation perplexity"

## Foundational Learning

- Concept: Inductive bias in machine learning
  - Why needed here: Understanding how MIDAS's initialization strategy creates specific biases that improve reasoning performance
  - Quick check question: Can you explain why two models with identical pretraining perplexity might perform differently on downstream reasoning tasks?

- Concept: Layer-wise functional specialization in transformers
  - Why needed here: Understanding why copying middle layers (rather than last layers) is beneficial for reasoning tasks
  - Quick check question: What evidence suggests that transformer layers have distinct functional roles, and how might this affect stacking strategies?

- Concept: Looped models and iterative computation
  - Why needed here: Understanding the theoretical connection between MIDAS's architecture and its reasoning capabilities
  - Quick check question: How do looped models with shared parameters across layers enable iterative solution finding, and why would this help with reasoning?

## Architecture Onboarding

- Component map: Small model → Gradual stacking (last L/k layers duplicated) → MIDAS (middle L/k layers duplicated) → Evaluation on reasoning tasks
- Critical path: Initialize small model → run stagewise training with middle-layer copying → evaluate pretraining perplexity → test on downstream reasoning tasks
- Design tradeoffs: Middle-layer copying vs end-layer copying (reasoning vs memorization bias), speed vs accuracy tradeoffs in different PROP-α schedules
- Failure signatures: Similar perplexity but worse downstream performance indicates wrong layer copying strategy; poor arithmetic performance suggests insufficient memorization capability
- First 3 experiments:
  1. Compare MIDAS with PROP-2 schedule vs baseline on closed-book QA vs open-book QA to confirm reasoning bias
  2. Test arithmetic vs reasoning performance on GSM8K with and without calculator to isolate capabilities
  3. Evaluate copying primitive performance to verify basic in-context learning ability improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inductive bias of MIDAS compare to other variants of stacking-based training strategies, such as progressive stacking?
- Basis in paper: [inferred] The paper discusses the inductive bias of MIDAS and its performance on reasoning tasks, but does not compare it directly to other stacking methods
- Why unresolved: The paper focuses on the inductive bias of MIDAS but does not provide a direct comparison with other stacking methods like progressive stacking
- What evidence would resolve it: A comparative study of different stacking methods, including progressive stacking, on reasoning tasks would provide insights into the unique inductive bias of MIDAS

### Open Question 2
- Question: What is the theoretical explanation for the improved reasoning capabilities observed in models trained with MIDAS?
- Basis in paper: [explicit] The paper conjectures that the improved reasoning capabilities of MIDAS are due to its connection to looped models, but does not provide a theoretical explanation
- Why unresolved: While the paper suggests a connection to looped models, it does not delve into the theoretical underpinnings of why this connection leads to improved reasoning
- What evidence would resolve it: A theoretical analysis or mathematical proof demonstrating how the structure of MIDAS leads to enhanced reasoning capabilities would clarify this aspect

### Open Question 3
- Question: What is the relationship between the memorization and reasoning capabilities of language models, and how does MIDAS influence this balance?
- Basis in paper: [inferred] The paper discusses the distinction between memorization and reasoning tasks, noting that MIDAS improves reasoning capabilities, but does not explore the underlying relationship
- Why unresolved: While the paper highlights improvements in reasoning, it does not investigate how MIDAS affects the balance between memorization and reasoning in language models
- What evidence would resolve it: A detailed study examining how MIDAS influences the trade-off between memorization and reasoning, possibly through ablation studies or controlled experiments, would clarify this relationship

## Limitations

- Limited Generalization Beyond Decoder-Only Models: The study focuses exclusively on decoder-only transformer architectures, leaving open questions about whether MIDAS's inductive bias toward reasoning generalizes to other model types
- Synthetic Task Representativeness: While reasoning primitives provide clean signals about model capabilities, they may not fully capture the complexity of real-world reasoning tasks
- Single Dataset and Objective Setup: The experiments use a specific dataset mixture and UL2 objective, which may not reflect performance on other pretraining corpora or objectives

## Confidence

- High Confidence: The empirical finding that MIDAS achieves faster training (up to 40% speedup) and better downstream reasoning performance compared to baseline and gradual stacking
- Medium Confidence: The mechanism connecting MIDAS's layer duplication strategy to improved reasoning through looped-model-like similarity structures
- Low Confidence: The claim that MIDAS extracts more skills from the same data compared to baseline training

## Next Checks

1. **Ablation on Layer Selection**: Systematically test copying different layer blocks (first, middle, last) and combinations thereof to determine which specific layer positions contribute most to reasoning performance versus other capabilities

2. **Cross-Architecture Validation**: Apply the MIDAS strategy to encoder-decoder models and evaluate whether the reasoning inductive bias transfers, or if it's specific to the decoder-only architecture used in this study

3. **Curriculum Learning Comparison**: Compare MIDAS with curriculum learning approaches that explicitly order training data by reasoning complexity to isolate whether the benefit comes from architectural initialization versus data ordering effects