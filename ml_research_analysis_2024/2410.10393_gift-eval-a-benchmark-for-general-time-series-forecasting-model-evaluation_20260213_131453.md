---
ver: rpa2
title: 'GIFT-Eval: A Benchmark For General Time Series Forecasting Model Evaluation'
arxiv_id: '2410.10393'
source_url: https://arxiv.org/abs/2410.10393
tags:
- short
- time
- series
- data
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GIFT-Eval is a comprehensive benchmark for evaluating time series
  foundation models, addressing the lack of diverse, large-scale datasets for this
  task. It includes 23 datasets spanning 144,000 time series and 177 million data
  points across seven domains, 10 frequencies, and prediction lengths from short to
  long-term.
---

# GIFT-Eval: A Benchmark For General Time Series Forecasting Model Evaluation

## Quick Facts
- arXiv ID: 2410.10393
- Source URL: https://arxiv.org/abs/2410.10393
- Authors: Taha Aksu; Gerald Woo; Juncheng Liu; Xu Liu; Chenghao Liu; Silvio Savarese; Caiming Xiong; Doyen Sahoo
- Reference count: 40
- Primary result: Comprehensive benchmark with 23 datasets, 144,000 time series, and 177 million data points spanning 7 domains and 10 frequencies for evaluating time series foundation models

## Executive Summary
GIFT-Eval addresses the critical need for diverse, large-scale benchmarks to evaluate time series foundation models. The benchmark includes 23 datasets spanning 144,000 time series and 177 million data points across seven domains, ten frequencies, and prediction lengths ranging from short to long-term. To ensure fair evaluation, the authors provide a non-leaking pretraining dataset containing approximately 230 billion data points. Experiments with 17 baselines, including statistical models, deep learning approaches, and foundation models, reveal that foundation models like Moirai and Chronos generally outperform others, especially in long-term forecasting, while deep learning models like PatchTST excel in higher-frequency or multivariate scenarios.

## Method Summary
GIFT-Eval provides a comprehensive evaluation framework for time series foundation models using 23 diverse datasets with 144,000 time series and 177 million data points. The benchmark includes 17 baselines spanning statistical models (Naive, Seasonal Naive, Auto_Arima, Auto_Theta, Auto_ETS), deep learning models (DeepAR, TFT, TiDE, N-BEATS, PatchTST, iTransformer, DLinear, Crossformer), and foundation models (TimesFM, Chronos, Moirai, VisionTS). Models are evaluated using median Mean Absolute Percentage Error (MAPE) for point forecasts and Continuous Ranked Probability Score (CRPS) for probabilistic forecasts, with results normalized against a Seasonal Naive baseline. Hyperparameter search ranges are provided for deep learning models, while foundation models are evaluated in zero-shot manner. The framework ensures no data leakage between pretraining and evaluation sets through careful dataset curation.

## Key Results
- Foundation models (Moirai, Chronos) generally outperform other approaches, particularly in long-term forecasting scenarios
- Deep learning models like PatchTST excel in higher-frequency or multivariate forecasting tasks
- Moirai and Chronos demonstrate superior performance over TimesFM and VisionTS across most evaluation metrics
- Foundation models show consistent advantages in low-frequency data but struggle with high-frequency, noisy datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GIFT-Eval improves generalization by providing diverse data spanning 7 domains, 10 frequencies, multivariate inputs, and long-term predictions.
- Mechanism: Diversity in benchmark design exposes models to varied real-world scenarios, reducing overfitting to narrow dataset distributions.
- Core assumption: Generalization improves with exposure to heterogeneous time series characteristics.
- Evidence anchors:
  - [abstract]: "GIFT-Eval encompasses 23 datasets over 144,000 time series and 177 million data points, spanning seven domains, 10 frequencies, multivariate inputs, and prediction lengths ranging from short to long-term forecasts."
  - [section 3.2]: Detailed analysis of time series features (trend, seasonal strength, entropy, Hurst, stability, lumpiness) confirms diverse data distribution.
- Break Condition: If models fail on certain domain/frequency combinations, suggests insufficient pretraining data or model capacity limits.

### Mechanism 2
- Claim: Providing non-leaking pretraining data prevents unfair advantage and enables fair evaluation of foundation models.
- Mechanism: Ensures pretraining and evaluation sets are mutually exclusive, avoiding memorization of evaluation data.
- Core assumption: Data leakage inflates performance metrics and masks true generalization.
- Evidence anchors:
  - [abstract]: "To facilitate the effective pretraining and evaluation of foundation models, we also provide a non-leaking pretraining dataset containing approximately 230 billion data points."
  - [section F.2]: Empirical analysis shows performance boost when leakage is present, confirming its effect.
- Break Condition: If leakage exists despite precautions, benchmarks become unreliable for model comparison.

### Mechanism 3
- Claim: Comprehensive benchmarking across 17 baselines (statistical, deep learning, foundation) reveals relative strengths and weaknesses.
- Mechanism: Multiple model families tested under identical conditions isolate architectural contributions from data effects.
- Core assumption: Controlled experiments enable valid comparisons across different model types.
- Evidence anchors:
  - [section 4]: Experiments cover Naive, Seasonal Naive, Auto_Arima, Auto_Theta, Auto_ETS, DeepAR, TFT, TiDE, N-BEATS, PatchTST, iTransformer, DLinear, Crossformer, TimesFM, VisionTS, Chronos, Moirai.
  - [section 4.1]: Domain, frequency, and prediction length analyses highlight when each model family excels.
- Break Condition: If experimental setup varies across models (e.g., different hyperparameters), results may not be directly comparable.

## Foundational Learning

- Concept: Time series decomposition (trend, seasonality, residuals).
  - Why needed here: Understanding decomposition is essential for interpreting time series features and forecasting performance.
  - Quick check question: How does STL decomposition separate trend and seasonal components in a time series?

- Concept: Probabilistic forecasting metrics (CRPS).
  - Why needed here: CRPS evaluates uncertainty estimates, critical for foundation models that produce full predictive distributions.
  - Quick check question: What does CRPS measure, and how does it differ from MAPE?

- Concept: Transformer architecture for time series.
  - Why needed here: Many baseline models use transformers; understanding attention and positional encoding is key to interpreting results.
  - Quick check question: How do transformer-based models handle variable-length time series inputs?

## Architecture Onboarding

- Component map: Datasets (23) -> Preprocessing (Arrow format) -> Models (17 baselines) -> Evaluation (MAPE/CRPS metrics) -> Aggregation (domain/frequency/length analysis)
- Critical path: Load datasets → preprocess into Arrow format → run models with consistent hyperparameters → compute metrics → aggregate by domain/frequency/length
- Design tradeoffs: Large diversity vs. computational cost; non-leaking data vs. limited pretraining coverage; comprehensive baselines vs. evaluation runtime
- Failure signatures: OOM errors on high-frequency datasets; poor performance on specific domains suggests data scarcity; inconsistent rankings indicate leakage or hyperparameter mismatch
- First 3 experiments:
  1. Run all statistical baselines on a small subset to verify data loading and metric computation.
  2. Evaluate one deep learning model (e.g., PatchTST) across all frequencies to test scalability.
  3. Run a foundation model (e.g., Moirai) on a single domain to validate pretraining data isolation.

## Open Questions the Paper Calls Out

- Question: How does the performance gap between deep learning models and foundation models vary across different time series characteristics, such as trend strength, seasonal strength, and lumpiness?
  - Basis in paper: [explicit] The paper discusses that foundation models generally outperform deep learning models in low-frequency data but struggle with high-frequency, noisy data, suggesting a performance gap related to time series characteristics.
  - Why unresolved: The paper provides qualitative analysis but lacks quantitative metrics to precisely measure the performance gap across specific time series features.
  - What evidence would resolve it: A detailed quantitative analysis showing model performance metrics (e.g., MAPE, CRPS) across various time series features like trend, seasonality, and lumpiness for each model type.

- Question: What is the impact of data leakage from pretraining datasets on the performance of foundation models in time series forecasting?
  - Basis in paper: [explicit] The paper mentions that foundation models like Moirai, Chronos, and TimesFM have partial data leakage issues when evaluated against GIFT-Eval, and it suggests that this leakage can boost performance.
  - Why unresolved: While the paper acknowledges the leakage issue, it does not provide a comprehensive analysis of how much this affects model rankings and generalizability.
  - What evidence would resolve it: A controlled experiment comparing model performance with and without data leakage, along with an analysis of how this affects model rankings and predictions on unseen data.

- Question: Why do foundation models like Chronos show performance degradation over longer prediction horizons, and how can this be mitigated?
  - Basis in paper: [explicit] The paper notes that Chronos exhibits performance degradation as prediction length increases, likely due to the recursive multi-step forecasting strategy leading to error accumulation.
  - Why unresolved: The paper does not explore potential solutions or architectural changes to address this issue.
  - What evidence would resolve it: Experiments testing alternative forecasting strategies (e.g., direct multi-step forecasting) or architectural modifications to reduce error accumulation in foundation models over long horizons.

## Limitations

- The benchmark's primary limitation is the potential for domain-specific performance variations that may not generalize to real-world applications outside the seven included domains
- Computational cost of running all 17 baselines across 144,000 time series may limit accessibility for researchers with constrained resources
- Zero-shot evaluation approach for foundation models may not fully capture their potential when fine-tuned on domain-specific data

## Confidence

- High Confidence: The benchmark's comprehensive dataset coverage and methodology for preventing data leakage are well-documented and empirically validated. The performance trends showing foundation models excelling in long-term forecasting and deep learning models performing better in high-frequency scenarios are consistently observed across multiple experiments.
- Medium Confidence: The relative rankings of foundation models (Moirai and Chronos outperforming TimesFM and VisionTS) are supported by the data, but could shift with different evaluation protocols or additional datasets. The claim that GIFT-Eval addresses the lack of diverse benchmarks is supported but could benefit from comparison with emerging alternatives.
- Low Confidence: The assertion that the pretraining dataset of 230 billion data points is sufficient for all downstream tasks is difficult to verify without extensive ablation studies across different pretraining sizes.

## Next Checks

1. Conduct a sensitivity analysis by systematically removing datasets from specific domains to quantify the impact on foundation model performance and identify which domains contribute most to overall benchmark results.
2. Implement and evaluate an additional foundation model not included in the original benchmark to test the generalizability of the observed performance trends and ensure the benchmark isn't inadvertently favoring certain architectural approaches.
3. Perform a computational efficiency analysis comparing the training/inference times of all 17 models across different dataset sizes and frequencies to provide a more complete picture of practical usability beyond pure forecasting accuracy.