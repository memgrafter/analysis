---
ver: rpa2
title: Calibrating Expressions of Certainty
arxiv_id: '2410.04315'
source_url: https://arxiv.org/abs/2410.04315
tags:
- calibration
- confidence
- certainty
- phrases
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to calibrating linguistic
  expressions of certainty (e.g., "Maybe", "Likely") by modeling them as distributions
  over the probability simplex rather than fixed scores. The authors generalize miscalibration
  measures like expected calibration error (ECE) and introduce an optimal transport-based
  post-hoc calibration method that adjusts the use of certainty phrases to improve
  calibration.
---

# Calibrating Expressions of Certainty

## Quick Facts
- arXiv ID: 2410.04315
- Source URL: https://arxiv.org/abs/2410.04315
- Reference count: 40
- Key outcome: Introduces optimal transport-based post-hoc calibration for linguistic certainty phrases, achieving ECE reduction from 0.22 to 0.07 for GPT-4o on TruthfulQA

## Executive Summary
This paper addresses the challenge of calibrating linguistic expressions of certainty (e.g., "Maybe", "Likely") by modeling them as distributions over the probability simplex rather than fixed scores. The authors introduce a novel post-hoc calibration method using optimal transport that adjusts the use of certainty phrases to improve calibration while maintaining natural language output. They demonstrate effectiveness through analysis of radiologists' clinical reports and calibration of language models on question-answering tasks, showing improved reliability diagrams and comparable performance to traditional calibration techniques.

## Method Summary
The method treats certainty phrases as Beta distributions over probability values, fitting these distributions from survey data. It generalizes the expected calibration error (ECE) to handle distribution-valued confidence through continuous contributions across bins. The core innovation is using entropy-regularized unbalanced optimal transport to learn a calibration map that adjusts phrase usage with minimal changes. This post-hoc calibration is applied to both clinical reports (2,662 paired chest X-ray and CT reports) and language models (GPT-4o, Claude-3.5-Sonnet on SciQ and TruthfulQA datasets), with evaluation using ECE, Brier Score, accuracy, and reliability diagrams.

## Key Results
- Post-hoc calibration reduces ECE from 0.22 to 0.07 for GPT-4o on TruthfulQA dataset
- Calibration reduces ECE from 0.25 to 0.04 for Claude-3.5-Sonnet on same dataset
- Method produces smoother, more interpretable reliability diagrams compared to traditional binning approaches
- Analysis of radiologists' reports reveals systematic miscalibration patterns in clinical certainty expressions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating certainty phrases as distributions over the probability simplex allows for more accurate modeling of human uncertainty compared to fixed-point scores.
- Mechanism: Instead of assigning a single probability value to a phrase like "Likely", the model represents it as a Beta distribution (e.g., Beta(5,2)) that captures the range and density of how people actually interpret the phrase. This distribution-based representation enables the calibration method to account for the inherent ambiguity in linguistic expressions of certainty.
- Core assumption: Certainty phrases naturally represent ranges of probability values rather than exact points, and these ranges vary across individuals and contexts.
- Evidence anchors:
  - [abstract]: "we model uncertainty as distributions over the simplex to capture their semantics more accurately"
  - [section]: "Rather than focusing solely on the mean, our method offers a more realistic quantification of calibration error by treating each certainty phrase as a distribution"
- Break condition: If human interpretation of certainty phrases is actually more precise than assumed, or if the distributional representation doesn't capture meaningful variation.

### Mechanism 2
- Claim: Optimal transport-based post-hoc calibration adjusts the use of certainty phrases to improve calibration by finding the minimal transformation needed.
- Mechanism: The method formulates calibration as a discrete optimal transport problem where the "cost" of changing one phrase to another is based on the improvement in calibration error. The transport plan T represents how much to adjust usage of each phrase, minimizing the net change required while improving overall calibration.
- Core assumption: The relationship between phrases and their calibration error can be captured in a cost matrix, and the optimal transport framework can find an efficient transformation.
- Evidence anchors:
  - [abstract]: "we introduce a novel post-hoc calibration method"
  - [section]: "This mapping is derived as the solution to an optimal transport problem that minimizes the net change in calibration error"
- Break condition: If the cost matrix doesn't accurately capture the calibration relationships, or if the optimal transport solution produces unintuitive or ineffective phrase adjustments.

### Mechanism 3
- Claim: The generalized ECE estimator provides more robust and interpretable calibration measurements than traditional binning methods.
- Mechanism: By modeling confidence distributions, the method computes ECE using weighted contributions from each sample across all bins, rather than forcing each sample into a single bin. This produces smoother calibration curves that are less sensitive to binning choices and provide better visualization for comparing models.
- Core assumption: The continuous nature of confidence distributions allows for more nuanced estimation of calibration error than discrete binning approaches.
- Evidence anchors:
  - [abstract]: "we generalize existing estimators for miscalibration metrics"
  - [section]: "This formulation is robust to binning strategies and alleviates estimation noise under small sample sizes"
- Break condition: If the computational complexity of the continuous estimator outweighs its benefits, or if the smoothing obscures important calibration patterns.

## Foundational Learning

- Concept: Beta distribution fitting and parameterization
  - Why needed here: The method uses Beta distributions to represent confidence distributions for certainty phrases, requiring understanding of how to fit these distributions to survey data and interpret their parameters.
  - Quick check question: How do you fit a Beta distribution to empirical data using the method of moments?

- Concept: Optimal transport theory and implementation
  - Why needed here: The calibration method relies on solving an entropy-regularized unbalanced optimal transport problem, requiring knowledge of the Sinkhorn algorithm and its variants.
  - Quick check question: What is the role of the entropy regularization parameter in the Sinkhorn algorithm for optimal transport?

- Concept: Kernel density estimation and Nadaraya-Watson estimators
  - Why needed here: The generalized ECE estimator can be viewed as a kernel density estimator with input-dependent kernels, requiring understanding of kernel regression concepts.
  - Quick check question: How does the Nadaraya-Watson estimator relate to the generalized ECE calculation in this method?

## Architecture Onboarding

- Component map: Clinical reports or question-answer pairs -> Beta distribution fitting from surveys -> Generalized ECE estimation with continuous contributions -> Optimal transport calibration solving -> Applied calibration map -> Evaluation metrics
- Critical path: The main workflow is: extract (pathology, confidence) pairs from clinical reports → fit Beta distributions to survey data → estimate ECE using continuous contributions → solve optimal transport problem for calibration → apply calibration map and evaluate
- Design tradeoffs: The method trades computational complexity (continuous ECE estimation, optimal transport solving) for improved calibration accuracy and interpretability. The choice of K certainty phrases involves balancing granularity against practical usability.
- Failure signatures: Poor calibration performance despite optimization could indicate: (1) inadequate survey data for fitting distributions, (2) incorrect cost matrix formulation, (3) suboptimal hyperparameter choices for optimal transport, or (4) fundamental limitations in the distributional representation of certainty.
- First 3 experiments:
  1. Validate Beta distribution fitting by comparing fitted parameters to survey data histograms for a few certainty phrases
  2. Test ECE estimation on synthetic data with known calibration properties to verify the continuous contribution mechanism works as expected
  3. Run optimal transport calibration on a small dataset and manually inspect the resulting calibration map for intuitive adjustments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different confidence distribution shapes (e.g., normal, uniform, or skewed distributions) on calibration performance compared to beta distributions?
- Basis in paper: [inferred] The paper uses beta distributions to model confidence phrases and shows good calibration performance, but does not explore other distribution types.
- Why unresolved: The paper only explores beta distributions for modeling confidence phrases, leaving open the question of whether other distribution shapes might be more effective for certain applications or types of certainty phrases.
- What evidence would resolve it: Experiments comparing calibration performance using different distribution types (e.g., normal, uniform, skewed) for modeling confidence phrases across various datasets and applications would provide evidence to determine if beta distributions are optimal or if other distributions yield better results in specific scenarios.

### Open Question 2
- Question: How does the optimal transport calibration method perform when applied to multi-class classification problems beyond binary classification?
- Basis in paper: [explicit] The paper focuses on binary classification and mentions that alternative definitions of calibration are equivalent for this case, but does not explore multi-class scenarios.
- Why unresolved: The current formulation and experiments are limited to binary classification, leaving open the question of how the method extends to multi-class problems where the relationship between calibration error and the number of classes may be more complex.
- What evidence would resolve it: Applying the optimal transport calibration method to multi-class classification datasets and comparing its performance to existing multi-class calibration techniques would provide insights into its effectiveness and potential limitations in more complex classification scenarios.

### Open Question 3
- Question: What are the effects of varying the number of certainty phrases (K) on calibration performance, and is there an optimal range for different types of applications?
- Basis in paper: [explicit] The paper mentions that calibration performance varies with K but remains consistent across models and datasets, and chooses K=12 for experiments, but does not provide a detailed analysis of the optimal range for different applications.
- Why unresolved: While the paper shows that calibration performance is relatively stable across different values of K, it does not explore in depth how the choice of K affects performance in various contexts or identify if there is an optimal range for specific applications.
- What evidence would resolve it: Conducting a comprehensive study varying K across different datasets, applications (e.g., medical diagnosis, general question answering), and language models would reveal how the number of certainty phrases impacts calibration performance and help determine if there are application-specific optimal ranges for K.

## Limitations
- Method's effectiveness depends heavily on quality and representativeness of survey data used to fit Beta distributions
- Computational complexity of continuous ECE estimation and optimal transport solving may limit scalability to very large datasets
- Clinical report analysis based on single dataset (chest X-ray and CT reports) may not generalize to other medical domains

## Confidence
- **High confidence**: The core mathematical framework (optimal transport for calibration, generalized ECE estimation) is sound and well-established. The improved reliability diagrams and reduced ECE on the TruthfulQA dataset are concrete, measurable improvements.
- **Medium confidence**: The clinical report analysis showing radiologists' miscalibration is compelling but based on a single dataset. The claim that this method produces more interpretable calibration curves than traditional approaches needs broader validation across different types of uncertainty expressions and domains.
- **Low confidence**: The claim that this approach is superior for all types of linguistic uncertainty calibration. The method's performance on other datasets, domains, or types of uncertainty expressions (beyond the tested "Maybe", "Likely", etc.) remains to be seen.

## Next Checks
1. **Cross-domain validation**: Apply the calibration method to a different medical imaging dataset (e.g., mammography reports) and a non-medical domain (e.g., financial forecasts or weather predictions) to assess generalizability. Compare calibration performance and reliability diagram smoothness across domains.

2. **Distribution fitting validation**: Conduct a controlled experiment where human subjects provide probability estimates for the same certainty phrases across multiple contexts. Compare the fitted Beta distributions to these empirical distributions to validate whether the distributional assumption holds in practice.

3. **Ablation study on hyperparameters**: Systematically vary the optimal transport hyperparameters (ϵ, τ2) and binning choices in ECE estimation across multiple datasets. Quantify the impact on calibration performance and identify robust default settings that work across different scenarios.