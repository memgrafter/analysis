---
ver: rpa2
title: Neutralizing Backdoors through Information Conflicts for Large Language Models
arxiv_id: '2411.18280'
source_url: https://arxiv.org/abs/2411.18280
tags:
- backdoor
- attacks
- arxiv
- language
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of backdoor attacks on large
  language models (LLMs), where malicious actors embed hidden triggers that cause
  unintended behavior. The proposed method neutralizes these backdoors by introducing
  internal and external information conflicts: internally through model merging with
  a conflict model trained on clean data, and externally by incorporating contradictory
  evidence into prompts.'
---

# Neutralizing Backdoors through Information Conflicts for Large Language Models

## Quick Facts
- arXiv ID: 2411.18280
- Source URL: https://arxiv.org/abs/2411.18280
- Reference count: 40
- Reduces attack success rates by up to 98% while maintaining over 90% accuracy on clean data

## Executive Summary
This paper addresses the critical problem of backdoor attacks on large language models (LLMs), where malicious actors embed hidden triggers that cause unintended behavior. The proposed method neutralizes these backdoors by introducing internal and external information conflicts: internally through model merging with a conflict model trained on clean data, and externally by incorporating contradictory evidence into prompts. Experiments on four LLMs show the method significantly outperforms eight state-of-the-art defense baselines.

## Method Summary
The method employs a two-phase approach to neutralize backdoor attacks. First, it trains a conflict model using LoRA on a small percentage of clean data, then merges it with the backdoored model to create internal parametric conflicts. Second, when external evidence generation is needed, it uses TextRank to extract keywords from the input, generates contradictory evidence with an external LLM, and appends it to the original prompt. This creates both internal conflicts at the parameter level and external conflicts at the prompt level, effectively neutralizing malicious behaviors while preserving clean data accuracy.

## Key Results
- Reduces attack success rates by up to 98% across multiple backdoor attack types
- Maintains over 90% clean data accuracy on four different LLMs (GPT2-XL, GPT-J, LLaMA, LLaMA-2)
- Outperforms eight state-of-the-art defense baselines in both effectiveness and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Internal Parametric Conflicts
- Claim: Internal information conflict neutralizes backdoor behaviors by introducing contradictory knowledge at the parameter level
- Mechanism: Trains a conflict model using LoRA on clean data, then merges it with the backdoored model using linear combination to create parametric memory conflicts
- Core assumption: Backdoored model knowledge can be disrupted by introducing contradictory information at the parameter level without complete retraining
- Evidence anchors: [abstract] "Internally, we leverage a lightweight dataset to train a conflict model, which is then merged with the backdoored model to neutralize malicious behaviors by embedding contradictory information within the model's parametric memory."

### Mechanism 2: External Contradictory Evidence
- Claim: External information conflict neutralizes backdoors by providing contradictory evidence that challenges the model's internal knowledge
- Mechanism: Generates evidence supporting the model's backdoor behavior, then modifies it to introduce contradictions using an external LLM, concatenating with the original input
- Core assumption: LLMs can be influenced by external contradictory evidence that conflicts with their internal knowledge representations
- Evidence anchors: [abstract] "Externally, we incorporate convincing contradictory evidence into the prompt to challenge the model's internal backdoor knowledge."

### Mechanism 3: Selective Capability Preservation
- Claim: Model merging preserves the backdoored model's clean data capabilities while neutralizing backdoor behaviors
- Mechanism: Merges conflict model (trained on clean data) with backdoored model, inheriting clean data processing capabilities while disrupting backdoor triggers
- Core assumption: Model merging can selectively preserve beneficial capabilities while disrupting harmful behaviors through parameter-level conflicts
- Evidence anchors: [section] "Model merging [65], [77] involves integrating multiple trained models into a single model, often leveraging the strengths of models trained on different tasks to enhance performance and robustness."

## Foundational Learning

- **Concept**: Parameter-efficient fine-tuning (PEFT) and LoRA
  - Why needed here: Enables efficient training of conflict model on clean data without full model retraining
  - Quick check question: What is the key advantage of using LoRA for training the conflict model compared to full-model fine-tuning?

- **Concept**: Model merging techniques (linear combination, SLERP, TIES, Passthrough)
  - Why needed here: Required for merging conflict model with backdoored model to create parametric conflicts
  - Quick check question: How does linear combination differ from TIES merging in terms of parameter handling and computational complexity?

- **Concept**: TextRank algorithm for keyword extraction
  - Why needed here: Identifies relevant keywords from input query to guide external LLM in creating contradictory evidence
  - Quick check question: What is the key principle behind how TextRank identifies important keywords in a text?

## Architecture Onboarding

- **Component map**: Input processing → Conflict model training (LoRA on clean data) → Model merging → External evidence generation (when needed) → Final output
- **Critical path**: Clean data → LoRA training → Model merging → Backdoor neutralization
- **Design tradeoffs**: Internal vs external conflicts (effectiveness vs computational resources), merging methods (performance vs computational cost), clean data percentage (defense performance vs computational cost)
- **Failure signatures**: High ASR despite CDA maintenance (insufficient clean data), significant CDA degradation (over-aggressive conflict model), high computational costs (optimization needed)
- **First 3 experiments**:
  1. Train conflict model using LoRA on 10% clean data, merge with backdoored model using linear combination; measure ASR and CDA
  2. Test external evidence generation by prompting backdoored model for supporting evidence, then modifying with external LLM; measure effectiveness
  3. Compare different model merging methods (linear, SLERP, TIES, Passthrough) on same conflict model to find optimal balance of performance and cost

## Open Questions the Paper Calls Out
- How does the method perform against more sophisticated adaptive backdoor attacks that specifically target the information conflict mechanisms?
- What is the minimum percentage of clean data required for conflict model training to maintain effectiveness while minimizing computational overhead?
- How does the proposed method generalize to non-textual domains such as computer vision or speech recognition?

## Limitations
- Mechanism of parametric memory conflicts lacks comprehensive empirical validation across diverse backdoor types
- External evidence generation effectiveness depends heavily on quality of external LLM used
- Computational cost analysis is incomplete with no detailed comparison across model sizes and merging methods
- Effectiveness against adaptive attacks remains largely unexplored

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Basic feasibility of using LoRA for conflict model training | High |
| Theoretical mechanism of internal parametric conflicts | Medium |
| Robustness of external evidence generation approach | Low |
| Effectiveness against adaptive attacks | Low |

## Next Checks
1. **Cross-Attack Type Validation**: Test method against diverse backdoor attack types to verify 98% ASR reduction holds across different attack patterns
2. **Adaptive Attack Resistance**: Evaluate effectiveness when attackers modify triggers to evade internal conflicts (e.g., using more complex trigger patterns or multiple triggers)
3. **Resource Efficiency Analysis**: Conduct detailed computational cost analysis comparing proposed method with baselines across different model sizes and datasets