---
ver: rpa2
title: 'WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal
  language models evaluation'
arxiv_id: '2410.12722'
source_url: https://arxiv.org/abs/2410.12722
tags:
- medical
- language
- dataset
- data
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WorldMedQA-V introduces a clinically validated, multilingual, and
  multimodal dataset for evaluating vision-language models in healthcare. It contains
  568 medical multiple-choice questions with associated images from four countries
  (Brazil, Israel, Japan, and Spain) in both original languages and English translations.
---

# WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation

## Quick Facts
- arXiv ID: 2410.12722
- Source URL: https://arxiv.org/abs/2410.12722
- Reference count: 33
- A clinically validated, multilingual, multimodal medical examination dataset containing 568 questions with images from Brazil, Israel, Japan, and Spain

## Executive Summary
WorldMedQA-V introduces a novel dataset for evaluating vision-language models in healthcare contexts, addressing critical gaps in existing medical QA benchmarks. The dataset combines multilingual medical questions from national licensing exams across four countries with associated clinical images, all clinically validated by native-speaking clinicians. Through systematic evaluation of ten VLMs, the study demonstrates that multimodal input generally improves model performance across languages, though significant disparities exist with Hebrew showing lower accuracy likely due to pretraining data bias. GPT-4o emerged as the top performer, showing particularly strong cross-linguistic consistency when provided with image information.

## Method Summary
The dataset was constructed by collecting medical multiple-choice questions from national licensing exams in Brazil, Israel, Japan, and Spain, paired with associated clinical images. A clinical validation process involving native-speaking clinicians ensured accuracy and cultural appropriateness across all original languages and English translations. Ten VLMs were evaluated using the VLMEvalKit framework, comparing performance with and without image input across all four countries in both original languages and English translations. Models were configured with 512 tokens, temperature 0, and CUDA > 12.0 requirements.

## Key Results
- VLMs performed better with image input than without, demonstrating the value of multimodal reasoning in medical contexts
- Accuracy varied significantly across languages, with Hebrew showing the lowest performance likely due to limited representation in pretraining data
- GPT-4o achieved the highest overall accuracy and demonstrated improved cross-linguistic consistency with multimodal input
- Models generally performed better on English-translated datasets compared to original languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language models perform better with image input due to enhanced multimodal reasoning.
- Mechanism: Visual information supplements textual context, enabling models to resolve ambiguities and improve diagnostic accuracy.
- Core assumption: Images provide critical clinical details not captured in text alone.
- Evidence anchors:
  - [abstract] "Most models performed better with image input"
  - [section] "Models generally performed better on English-translated datasets, particularly for the Spain and Israel datasets"
- Break condition: If images are redundant or misleading, performance may degrade.

### Mechanism 2
- Claim: Multilingual performance disparities arise from training data bias.
- Mechanism: Models pretrained predominantly on English data struggle with underrepresented languages like Hebrew.
- Core assumption: Pretraining corpus distribution affects downstream multilingual performance.
- Evidence anchors:
  - [abstract] "accuracy varied across languages, with Hebrew showing lower performance likely due to limited representation in pretraining data"
  - [section] "The underperformance in Hebrew, in contrast, could reflect Hebrewâ€™s lower representation in pretraining data"
- Break condition: If pretraining data becomes more balanced across languages, disparities may diminish.

### Mechanism 3
- Claim: Clinical validation by native-speaking clinicians ensures dataset quality and relevance.
- Mechanism: Domain experts review questions and translations to maintain accuracy and cultural appropriateness.
- Core assumption: Clinical expertise is necessary to validate medical content across languages.
- Evidence anchors:
  - [abstract] "clinically validated, multilingual, and multimodal dataset"
  - [section] "A clinical validation process was carried out for all collected and translated data"
- Break condition: If validation process is bypassed or conducted by non-experts, dataset reliability may suffer.

## Foundational Learning

- Concept: Multimodal reasoning
  - Why needed here: Models must integrate visual and textual data to answer medical questions accurately.
  - Quick check question: Can a model correctly diagnose a condition from both an X-ray and patient history?

- Concept: Language model bias
  - Why needed here: Understanding how pretraining data distribution affects multilingual performance.
  - Quick check question: Why might a model perform worse on Hebrew than on Japanese medical questions?

- Concept: Clinical validation
  - Why needed here: Ensures dataset questions are accurate and culturally appropriate across languages.
  - Quick check question: What role do native-speaking clinicians play in validating translated medical questions?

## Architecture Onboarding

- Component map: Dataset -> Model -> Evaluation Framework -> Results
- Critical path: Data collection -> Clinical validation -> Translation -> Model evaluation
- Design tradeoffs: Balancing dataset size with clinical accuracy vs. broader coverage
- Failure signatures: Inconsistent performance across languages, degraded accuracy without images
- First 3 experiments:
  1. Evaluate model performance with and without image input on a subset of questions.
  2. Compare model accuracy across different languages within the dataset.
  3. Test model consistency between original language and English translations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would VLMs perform on medical exams from countries not included in WorldMedQA-V, particularly from underrepresented regions like Africa, North and Central America, Oceania, and other parts of Asia?
- Basis in paper: [explicit] The authors acknowledge that their dataset only includes data from four countries (Brazil, Israel, Japan, and Spain) and that this geographic limitation results in underrepresentation of certain regions.
- Why unresolved: The current dataset is limited to four countries across three continents, leaving a significant gap in understanding how VLMs would perform in other linguistic and cultural contexts. The authors note this as a limitation but do not explore performance in other regions.
- What evidence would resolve it: Expanding the dataset to include medical exams from additional countries across all continents and evaluating VLMs on this expanded dataset would provide evidence for how well current models generalize to different healthcare environments.

### Open Question 2
- Question: How does the performance of VLMs vary when presented with multiple images from different time points or modalities (e.g., X-rays, CT scans, pathology slides) compared to the single image per question format used in WorldMedQA-V?
- Basis in paper: [explicit] The authors note that "Real-world clinical scenarios often involve multiple images from different time points or modalities, such as a sequence of X-rays, CT scans, and pathology slides."
- Why unresolved: The current benchmark pairs only one image per question, which may not reflect the complexity of real-world medical scenarios where multiple images are typically analyzed together. The authors suggest this as a limitation but do not test performance with multi-image scenarios.
- What evidence would resolve it: Creating a dataset with questions that require analysis of multiple images from different modalities and time points, then evaluating VLMs on this dataset, would reveal how performance changes with more complex multimodal inputs.

### Open Question 3
- Question: What is the impact of text within medical images on VLM performance, particularly when this text is not translated or adapted as was the case in WorldMedQA-V?
- Basis in paper: [explicit] The authors state "Another limitation is that text that is within images were not translated or adapted."
- Why unresolved: The current evaluation does not account for how well VLMs can process and understand text embedded within medical images, which is a common feature in clinical settings. This limitation could affect performance, especially for non-English text within images.
- What evidence would resolve it: Creating a version of the dataset where all text within images is translated and adapted, then comparing VLM performance on the original versus translated versions, would quantify the impact of untranslated image text on model accuracy.

## Limitations
- Geographic coverage limited to four countries across three continents, underrepresenting Africa, North and Central America, Oceania, and other parts of Asia
- Single image per question format doesn't reflect real-world clinical scenarios involving multiple images from different modalities and time points
- Text within images was not translated or adapted, potentially affecting performance on non-English image content

## Confidence
- Medium: The clinical validation process and dataset construction methodology are well-documented, but several limitations affect generalizability. The Hebrew performance gap, while attributed to pretraining bias, could also reflect other factors such as question difficulty or translation quality that weren't fully isolated. The dataset size (568 questions) is relatively small for comprehensive VLM evaluation, potentially limiting statistical power for detecting subtle performance differences.

## Next Checks
1. Conduct ablation studies to isolate the specific contribution of images versus textual context by systematically varying the quality and relevance of visual inputs while holding question content constant.

2. Perform cross-validation by randomly splitting the dataset into training/validation subsets to assess model performance consistency and ensure the observed language disparities aren't artifacts of dataset partitioning.

3. Evaluate model performance on a separate, independently collected medical question set in Hebrew to verify whether the observed performance gap persists, helping distinguish between language representation bias and dataset-specific factors.