---
ver: rpa2
title: Equivariant Neural Tangent Kernels
arxiv_id: '2406.06504'
source_url: https://arxiv.org/abs/2406.06504
tags:
- neural
- group
- networks
- kernel
- equivariant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work derives the neural tangent kernels (NTKs) for group convolutional
  neural networks, which are equivariant architectures used in medical imaging and
  quantum chemistry. The authors provide recursive formulas for NTKs and neural network
  Gaussian process kernels across lifting, group convolutional, group pooling, and
  pointwise nonlinearity layers for arbitrary symmetry groups.
---

# Equivariant Neural Tangent Kernels

## Quick Facts
- arXiv ID: 2406.06504
- Source URL: https://arxiv.org/abs/2406.06504
- Reference count: 40
- Equivariant NTKs outperform non-equivariant ones on medical image classification tasks in the infinite-width limit

## Executive Summary
This work derives neural tangent kernels (NTKs) for group convolutional neural networks (GCNNs), providing a theoretical framework to study training dynamics and generalization of equivariant architectures. The authors develop recursive formulas for NTKs and neural network Gaussian process kernels across lifting, group convolutional, group pooling, and pointwise nonlinearity layers for arbitrary symmetry groups. Specializing to roto-translations in 2D (G=C_n â‹‰ R^2), they implement efficient kernel computations using ordinary convolutions and demonstrate convergence of empirical NTKs to analytical ones as width increases. Medical image classification experiments confirm that equivariant NTKs maintain performance benefits in the infinite-width limit.

## Method Summary
The method involves deriving recursive formulas for NTKs and NNGPs for GCNNs with arbitrary symmetry groups. The authors implement these recursions for roto-translations in 2D, using ordinary convolutions to compute group convolutions efficiently. Validation involves comparing empirical NTK estimates (from finite-width networks) with analytical kernels across increasing widths. The framework is then applied to medical image classification tasks, comparing equivariant and non-equivariant NTK performance using the kernel method.

## Key Results
- Recursive NTK formulas derived for lifting, group convolution, nonlinearity, and pooling layers for arbitrary symmetry groups
- Empirical NTKs converge to analytical NTKs as network width increases for roto-translation equivariant networks
- Equivariant NTKs outperform non-equivariant NTKs on medical image classification tasks
- Efficient implementation using ordinary convolutions for C_n â‹‰ R^2 group convolutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Equivariant NTKs outperform non-equivariant NTKs as kernel predictors for classification tasks.
- Mechanism: The equivariant NTK incorporates the symmetry constraints of the data (e.g., rotations, translations) into the kernel function, leading to better generalization on tasks with known symmetries. The NTK in the infinite width limit becomes deterministic and independent of initialization, allowing for analytical study of the training dynamics.
- Core assumption: The data exhibits the symmetries that the equivariant architecture is designed for.
- Evidence anchors:
  - [abstract]: "In medical image classification experiments, equivariant NTKs outperform non-equivariant ones, confirming equivariance benefits persist in the infinite-width limit."
  - [section]: "In order to achieve a meaningful comparison, we constructed two models... As an equivariant counterpart we followed the same structure but replaced the convolutional layers with ð¶4 â‹‰ R2-convolutional ones and changed the SumPool to a group-pooling layer."

### Mechanism 2
- Claim: The recursive formulas for NTKs allow for efficient computation of the equivariant NTK for arbitrary symmetry groups.
- Mechanism: The authors derive recursive relations for the NNGP and NTK of group convolutional layers, lifting layers, group pooling layers, and pointwise nonlinearities. These recursions start from the input features and proceed layer by layer, allowing for the analytical computation of the NTK without resorting to expensive Monte-Carlo estimation.
- Core assumption: The symmetry group is known and the group convolution operation is correctly implemented.
- Evidence anchors:
  - [abstract]: "The authors provide recursive formulas for NTKs and neural network Gaussian process kernels across lifting, group convolutional, group pooling, and pointwise nonlinearity layers for arbitrary symmetry groups."
  - [section]: "In this section, we will derive the recursive relations for the NTK and the NNGP for group convolutional layers... That is, we will compute the expression (7) for group convolutions (13), lifting layers (15) and group pooling layers (19)."

### Mechanism 3
- Claim: The convergence of empirical NTKs to analytical NTKs as width increases validates the theoretical framework.
- Mechanism: As the width of the network increases, the empirical NTK (computed from finite-width networks) converges to the analytical NTK (computed using the recursive formulas). This convergence is demonstrated experimentally by comparing the two for networks with increasing width.
- Core assumption: The network is trained with the correct initialization and the width is sufficiently large.
- Evidence anchors:
  - [abstract]: "they implement efficient kernel computations using ordinary convolutions and demonstrate convergence of empirical NTKs to analytical ones as width increases."
  - [section]: "As a basic test of the validity of our analytically computed kernels, we compare them to Monte-Carlo estimates in Figure 1... Figure 1 confirms that the estimate converges to the analyitcal kernel for both NNGP and NTK as the width increases, as expected."

## Foundational Learning

- Concept: Group theory and representation theory
  - Why needed here: The equivariant neural networks are built on symmetry groups, and understanding their structure and representations is crucial for deriving the NTKs.
  - Quick check question: Can you explain the difference between the regular representation and the fundamental representation of a group?

- Concept: Neural tangent kernels (NTKs) and their recursive computation
  - Why needed here: The entire framework relies on the ability to compute the NTK of a network analytically, which requires understanding the recursive formulas for different layer types.
  - Quick check question: How does the NTK of a linear layer relate to the NTK of the previous layer?

- Concept: Convolutional neural networks (CNNs) and their kernel methods
  - Why needed here: The equivariant NTKs are a generalization of the NTKs for CNNs, so understanding the latter is a prerequisite for understanding the former.
  - Quick check question: What is the relationship between the NNGP and the NTK for a convolutional layer?

## Architecture Onboarding

- Component map:
  - Input features -> Lifting layer -> Group convolution -> Nonlinearity -> Group pooling -> Output

- Critical path:
  1. Define the symmetry group and its representations
  2. Implement the group convolution operation
  3. Implement the lifting layer
  4. Implement the group pooling layer
  5. Implement the recursive formulas for the NNGP and NTK
  6. Validate the implementation by comparing empirical and analytical NTKs

- Design tradeoffs:
  - Group choice: Different symmetry groups capture different invariances (e.g., roto-translations vs. rotations)
  - Kernel support: The size of the kernel support affects the computational cost and the expressiveness of the model
  - NTK parametrization: Different parametrizations (e.g., NTK vs. standard) affect the initialization and the scaling of the gradients

- Failure signatures:
  - Poor performance: The equivariant constraints are too restrictive or the data does not exhibit the expected symmetries
  - Slow convergence: The recursive formulas are not correctly implemented or the width is not large enough
  - Numerical instability: The implementation has issues with the Haar measure or the group operations

- First 3 experiments:
  1. Implement the equivariant NTK for roto-translations in 2D and validate it on a simple classification task with known rotational symmetry (e.g., rotated MNIST)
  2. Compare the performance of the equivariant NTK to a non-equivariant NTK on a medical image classification task (e.g., histological images)
  3. Study the effect of kernel support size on the performance and computational cost of the equivariant NTK

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the equivariant NTK recursions change for group convolutions with respect to the fundamental representation instead of the regular representation?
- Basis in paper: [explicit] The paper mentions that many important cases like graph neural networks for quantum chemistry involve equivariance with respect to the fundamental representation and suggests extending results to this case.
- Why unresolved: The current work only derives NTK recursions for group convolutions equivariant under the regular representation, leaving the fundamental representation case unexplored.
- What evidence would resolve it: Derivation and implementation of NTK recursions for group convolutions equivariant under the fundamental representation, along with numerical validation.

### Open Question 2
- Question: Would performing the NTK recursions in the Fourier domain simplify the expressions and improve computational efficiency?
- Basis in paper: [inferred] The paper suggests that computing the kernel recursions in the Fourier domain could lead to drastic simplifications similar to how convolutions become pointwise products in the Fourier domain for group convolutions.
- Why unresolved: The current implementation performs recursions in the spatial domain, not exploring potential simplifications from Fourier analysis.
- What evidence would resolve it: Implementation of the NTK recursions in the Fourier domain and comparison of computational efficiency and numerical accuracy with the spatial domain implementation.

### Open Question 3
- Question: How does the stability and generalization of equivariant neural networks in the large-depth limit compare to their non-equivariant counterparts?
- Basis in paper: [explicit] The paper mentions that the stability and generalization of neural networks in the large-depth limit was investigated in [59], and suggests repeating this analysis for equivariant neural networks using NTK tools.
- Why unresolved: While the paper provides NTK recursions for equivariant networks, it does not analyze their stability and generalization properties in the large-depth limit.
- What evidence would resolve it: Analysis of the eigenvalues and trace of the equivariant NTK throughout training for varying depths, and comparison with non-equivariant networks to quantify generalization benefits.

## Limitations
- Assumes exact symmetry in data and infinite-width networks, with finite-width networks showing empirical variance
- Computational complexity scales poorly with group size, limiting applicability to large symmetry groups
- Current implementation focuses on discrete roto-translations in 2D, requiring extensions for continuous groups or higher dimensions

## Confidence
- Mechanism 1 (Equivariant NTK performance): High confidence
- Mechanism 2 (Recursive computation): High confidence
- Mechanism 3 (Width convergence): Medium confidence

## Next Checks
1. Test equivariant NTK performance on datasets with varying degrees of rotational symmetry to quantify robustness to symmetry mismatch
2. Implement and validate recursive formulas for continuous symmetry groups (SO(2)) to assess scalability beyond discrete groups
3. Compare computational complexity and memory requirements between equivariant NTK computation and standard NTK across different group sizes and network depths