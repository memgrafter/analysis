---
ver: rpa2
title: Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural
  Networks
arxiv_id: '2403.04504'
source_url: https://arxiv.org/abs/2403.04504
tags:
- rating
- ratings
- ordinality
- rogmc
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ROGMC, a method to exploit rating ordinality
  in graph neural networks (GNNs) for matrix completion in recommender systems. ROGMC
  uses cumulative preference propagation to directly incorporate rating ordinality
  in GNN's message passing, allowing for users' stronger preferences to be more emphasized
  based on inherent orders of rating types.
---

# Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks

## Quick Facts
- arXiv ID: 2403.04504
- Source URL: https://arxiv.org/abs/2403.04504
- Authors: Jaehyun Lee; SeongKu Kang; Hwanjo Yu
- Reference count: 20
- Primary result: ROGMC consistently outperforms existing GNN strategies on three real-world datasets, with larger improvements when fewer ratings are available

## Executive Summary
This paper introduces ROGMC, a novel method that leverages rating ordinality in graph neural networks for matrix completion in recommender systems. The approach uses cumulative preference propagation to directly incorporate rating orders during message passing, allowing stronger preferences to be more emphasized. The method transforms the bipartite user-item graph into multiple subgraphs based on rating thresholds and employs interest regularization to enhance preference learning. ROGMC demonstrates consistent performance improvements over existing GNN-based recommendation methods across three real-world datasets.

## Method Summary
ROGMC operates by converting the original bipartite graph into a set of subgraphs, where each subgraph contains edges with ratings greater than or equal to a specific value. The method employs cumulative preference propagation in GNN's message passing, which directly incorporates rating ordinality by allowing users' stronger preferences to be more emphasized based on the inherent order of rating types. This is complemented by interest regularization that facilitates preference learning using underlying interest information. The approach effectively captures the ordinal nature of ratings while maintaining the graph structure necessary for GNN operations.

## Key Results
- ROGMC consistently outperforms existing GNN strategies that use rating types on three real-world datasets
- The method shows larger improvements when fewer ratings are available, making it particularly effective for sparse datasets
- Performance gains are attributed to the direct incorporation of rating ordinality through cumulative preference propagation

## Why This Works (Mechanism)
The method works by recognizing that traditional GNN approaches for recommendation often treat ratings as categorical features, losing the ordinal information inherent in rating scales. By transforming the bipartite graph into multiple subgraphs and using cumulative preference propagation, ROGMC preserves and leverages the ordinal relationships between different rating values. The interest regularization component further enhances the model's ability to learn meaningful preference patterns by incorporating additional structural information about user interests.

## Foundational Learning

**Graph Neural Networks (GNNs)**
- *Why needed*: GNNs provide the fundamental framework for processing graph-structured data in recommendation systems
- *Quick check*: Verify that node representations are properly aggregated through message passing

**Matrix Completion**
- *Why needed*: The core task of predicting missing user-item interactions in recommender systems
- *Quick check*: Confirm that the model can handle both observed and missing entries effectively

**Rating Ordinality**
- *Why needed*: Captures the inherent ordering in rating scales (e.g., 5-star ratings)
- *Quick check*: Ensure that the model preserves ordinal relationships during transformations

**Bipartite Graphs**
- *Why needed*: Standard representation for user-item interactions in recommendation
- *Quick check*: Validate that the graph structure is maintained during subgraph transformations

**Cumulative Propagation**
- *Why needed*: Allows information from higher ratings to influence lower rating subgraphs
- *Quick check*: Verify that preference information flows correctly across subgraphs

## Architecture Onboarding

**Component Map**
Original Bipartite Graph -> Subgraph Transformation -> Cumulative Message Passing -> Interest Regularization -> Prediction Layer

**Critical Path**
1. Subgraph creation based on rating thresholds
2. Cumulative message passing across subgraphs
3. Interest regularization application
4. Final prediction generation

**Design Tradeoffs**
- Subgraph transformation increases computational complexity but preserves ordinal information
- Cumulative propagation requires careful handling of information flow between subgraphs
- Interest regularization adds regularization strength but may require hyperparameter tuning

**Failure Signatures**
- Performance degradation when rating distributions are extremely skewed
- Computational bottlenecks during subgraph creation for very large graphs
- Overfitting when interest regularization is too strong

**3 First Experiments**
1. Test basic subgraph transformation on a small dataset to verify correct structure creation
2. Evaluate cumulative message passing with synthetic rating data to confirm ordinal preservation
3. Assess interest regularization impact by comparing with and without regularization on a validation set

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements most pronounced with sparse datasets, but untested on extremely sparse scenarios (<1% density)
- Computational overhead of subgraph transformation not fully addressed for scalability
- Limited generalizability due to testing on only three real-world datasets

## Confidence

**High Confidence**
- The core methodology of cumulative preference propagation is technically sound
- Theoretical foundation for graph transformation and interest regularization is robust

**Medium Confidence**
- Empirical performance improvements are demonstrated but limited by small sample size of datasets
- Claims about larger improvements with fewer ratings need more extensive validation

**Low Confidence**
- Scalability to very large-scale recommendation systems remains unverified
- Real-time production performance not evaluated

## Next Checks

1. Test ROGMC on additional datasets with varying sparsity levels, particularly extremely sparse datasets (less than 1% density), to verify the claimed advantage in low-data scenarios.

2. Conduct computational complexity analysis and runtime benchmarking to evaluate scalability compared to baseline methods, especially regarding the subgraph transformation process.

3. Perform ablation studies to isolate the contribution of cumulative preference propagation versus interest regularization, and test alternative ways of incorporating rating ordinality to validate the uniqueness of the proposed approach.