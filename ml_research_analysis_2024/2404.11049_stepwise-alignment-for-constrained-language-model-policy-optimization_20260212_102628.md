---
ver: rpa2
title: Stepwise Alignment for Constrained Language Model Policy Optimization
arxiv_id: '2404.11049'
source_url: https://arxiv.org/abs/2404.11049
tags:
- safety
- policy
- should
- sacpo
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  (LLMs) with human values while ensuring safety and trustworthiness. The authors
  propose a novel algorithm called Stepwise Alignment for Constrained Policy Optimization
  (SACPO) that formulates alignment as an optimization problem to maximize reward
  under safety constraints.
---

# Stepwise Alignment for Constrained Language Model Policy Optimization

## Quick Facts
- **arXiv ID**: 2404.11049
- **Source URL**: https://arxiv.org/abs/2404.11049
- **Reference count**: 40
- **Primary result**: SACPO fine-tunes Alpaca-7B better than SFT baselines in both helpfulness and harmlessness

## Executive Summary
This paper addresses the challenge of aligning large language models (LLMs) with human values while ensuring safety and trustworthiness. The authors propose a novel algorithm called Stepwise Alignment for Constrained Policy Optimization (SACPO) that formulates alignment as an optimization problem to maximize reward under safety constraints. The key idea is that the optimal policy incorporating both reward and safety can be directly obtained from a reward-aligned policy. SACPO aligns LLMs step-wise with each metric using simple yet powerful alignment algorithms like direct preference optimization (DPO). The proposed method offers advantages such as simplicity, stability, computational efficiency, and flexibility in algorithms and datasets. Under mild assumptions, theoretical analysis provides upper bounds on optimality and safety constraint violation. Experimental results show that SACPO can fine-tune Alpaca-7B better than state-of-the-art methods in terms of both helpfulness and harmlessness.

## Method Summary
SACPO formulates LLM alignment as an optimization problem that maximizes reward while satisfying safety constraints. The algorithm operates in two sequential steps: first aligning the model for one metric (either helpfulness or harmlessness) using preference-based alignment methods like DPO or KTO, then realigning for the second metric. This stepwise approach leverages the insight that the optimal policy incorporating both objectives can be derived from a reward-aligned policy. The method also includes P-SACPO, which uses linear model merging to balance the trade-off between competing objectives. The approach is theoretically grounded, with upper bounds on optimality and constraint violation provided under mild assumptions.

## Key Results
- SACPO outperforms SFT baselines on both helpfulness and harmlessness metrics
- Stepwise alignment (helpfulness first, then harmlessness) shows superior performance compared to other alignment orders
- The method demonstrates flexibility by working with different alignment algorithms (DPO, KTO) and datasets
- P-SACPO successfully balances the trade-off between competing objectives through linear model merging

## Why This Works (Mechanism)
SACPO works by decomposing the complex multi-objective alignment problem into sequential, manageable steps. By first aligning for one objective and then realigning for the second, the algorithm avoids the computational complexity of joint optimization while maintaining theoretical guarantees. The stepwise approach exploits the structure of the alignment problem where the optimal policy can be constructed from reward-aligned policies. This decomposition enables the use of simpler, more stable alignment algorithms like DPO for each step, avoiding the instability issues common in multi-objective RLHF approaches.

## Foundational Learning

**Constrained Policy Optimization**: A framework for optimizing policies under safety constraints, needed to formalize the trade-off between helpfulness and harmlessness. Quick check: Verify that constraint violation is bounded by theoretical guarantees.

**Direct Preference Optimization (DPO)**: A preference-based alignment method that learns from pairwise comparisons, needed for the stepwise alignment steps. Quick check: Confirm that DPO converges faster than other preference learning methods.

**Model Merging**: The technique of combining trained models through weighted averaging, needed for P-SACPO's trade-off balancing. Quick check: Test different mixing ratios to verify the Pareto front behavior.

**Red-teaming**: The process of generating adversarial prompts to test model safety, needed to evaluate harmlessness. Quick check: Ensure red-teaming prompts cover diverse harmful scenarios.

**GPT-4 Evaluation**: Using a powerful LLM as a judge for alignment quality, needed for automated metric assessment. Quick check: Validate GPT-4 scoring consistency across multiple runs.

## Architecture Onboarding

**Component map**: Alpaca-7B -> SACPO (Step 1: DPO/KTO on Metric A) -> SACPO (Step 2: DPO/KTO on Metric B) -> P-SACPO (Model Merging) -> Evaluation

**Critical path**: Data preparation → Base model initialization → First alignment step → Second alignment step → Model merging (if applicable) → GPT-4 evaluation

**Design tradeoffs**: Simplicity vs. joint optimization performance (SACPO trades some optimality for stability), computational efficiency vs. flexibility (stepwise approach is faster but order-dependent), theoretical guarantees vs. practical assumptions (bounds hold under mild assumptions that may not always be met)

**Failure signatures**: "Exaggerated safety behaviors" where models refuse benign requests, poor performance on one metric when alignment order is suboptimal, failure of model merging to achieve desired trade-off balance

**Three first experiments**:
1. Reproduce the baseline SFT model performance on the PKU-SafeRLHF dataset
2. Run SACPO with DPO first on helpfulness, then on harmlessness, and measure win rates
3. Test P-SACPO with different mixing ratios (q values) to verify trade-off behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on GPT-4 judging, introducing potential bias and subjectivity
- Experiments only evaluate on Alpaca-7B, limiting generalizability to other model architectures
- PKU-SafeRLHF dataset is not publicly available for independent verification
- Theoretical analysis assumes "mild conditions" without exploring what happens when assumptions are violated

## Confidence
- **High confidence**: The core algorithmic contribution and stepwise alignment approach
- **Medium confidence**: Experimental results showing improved alignment over SFT baselines
- **Low confidence**: Claims about computational efficiency and stability compared to other methods

## Next Checks
1. Attempt to obtain or reconstruct the PKU-SafeRLHF dataset to verify the claimed 30,000+ expert evaluations
2. Systematically evaluate SACPO performance when aligning first for harmlessness vs. helpfulness to confirm order sensitivity
3. Apply SACPO to a different base model (e.g., LLaMA-7B or Vicuna) using the same alignment procedure to verify generalization