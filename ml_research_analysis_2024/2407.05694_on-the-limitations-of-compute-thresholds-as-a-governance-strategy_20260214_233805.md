---
ver: rpa2
title: On the Limitations of Compute Thresholds as a Governance Strategy
arxiv_id: '2407.05694'
source_url: https://arxiv.org/abs/2407.05694
tags:
- compute
- https
- flop
- risk
- thresholds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper critically examines the use of compute thresholds as
  a governance strategy for AI models, particularly in the context of the EU AI Act
  and US Executive Orders. The core argument is that relying on static compute thresholds
  (measured in FLOPs) is shortsighted and ineffective for several reasons: The relationship
  between compute and risk is highly uncertain and rapidly changing.'
---

# On the Limitations of Compute Thresholds as a Governance Strategy

## Quick Facts
- arXiv ID: 2407.05694
- Source URL: https://arxiv.org/abs/2407.05694
- Authors: Sara Hooker
- Reference count: 40
- One-line primary result: Static compute thresholds are brittle governance tools that fail to capture rapidly shifting relationships between compute and risk.

## Executive Summary
This paper critically examines the use of compute thresholds (measured in FLOPs) as a governance strategy for AI models, particularly in the context of the EU AI Act and US Executive Orders. The core argument is that relying on static compute thresholds is shortsighted and ineffective because the relationship between compute and risk is highly uncertain and rapidly changing. Smaller models are becoming increasingly performant due to optimization techniques and better data quality, undermining the assumption that larger models always pose greater risks.

The paper recommends moving away from hard-coded thresholds to dynamic, percentile-based approaches that adapt to the distribution of model properties each year. It also suggests using a risk index composed of multiple metrics rather than relying solely on compute. The key outcome is that compute thresholds as currently implemented are unlikely to effectively mitigate AI risks and require significant improvements in specification and approach to be meaningful governance tools.

## Method Summary
The paper analyzes compute thresholds as a governance strategy by examining the relationship between compute and risk, evaluating scaling laws, and assessing measurement challenges. It collects evidence on model performance across different compute levels, investigates how optimization techniques affect this relationship, and identifies ambiguities in FLOP measurement standards. The methodology involves theoretical analysis of scaling laws, empirical examination of capability emergence, and policy analysis of current threshold implementations.

## Key Results
- The relationship between compute and risk is highly uncertain and rapidly changing due to optimization techniques that improve performance without increasing training FLOPs.
- Current scaling laws have limited predictive power for downstream capabilities, making it difficult to set appropriate thresholds based on compute alone.
- FLOP as a metric is under-specified and can be easily manipulated, creating loopholes in governance frameworks.
- The paper recommends dynamic, percentile-based thresholds and multi-metric risk indices instead of static compute thresholds.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Static compute thresholds are brittle because the compute-performance relationship is rapidly shifting.
- **Mechanism**: As optimization techniques (like distillation, chain-of-thought, context-length increases) improve performance without increasing training FLOPs, a static FLOP cutoff will misclassify models: some below the threshold will be as capable as those above, and some above may not be more risky.
- **Core assumption**: Post-training compute improvements can close capability gaps that FLOP thresholds are meant to measure.
- **Evidence anchors**:
  - [abstract] "The relationship between compute and risk is highly uncertain and rapidly changing... smaller models are becoming increasingly performant due to optimization techniques."
  - [section 2.1] "Optimization breakthroughs compensate for compute... These improvements dramatically improve performance but are currently completely ignored by compute thresholds."
  - [corpus] weak; neighboring papers focus on governance framing, not performance drift.
- **Break condition**: If future compute gains come almost entirely from pre-training scaling rather than inference-time or post-training techniques, the FLOP threshold may regain discriminative power.

### Mechanism 2
- **Claim**: FLOP as a metric is under-specified and easy to manipulate.
- **Mechanism**: The definition of FLOP in current policy does not cover quantized weights, MoE routing, ensembling, or system-level aggregation, creating loopholes where developers can avoid scrutiny without reducing risk.
- **Core assumption**: Without precise measurement guidance, developers can legally under-report FLOP or exploit edge cases in the threshold logic.
- **Evidence anchors**:
  - [section 3] "Existing policies do not specify key details around FLOP measurement that are necessary to ensure fair reporting."
  - [appendix A] "How to handle quantized models? Models are often quantized during training... the EU AI Act fails to specify how to handle integer operations."
  - [corpus] weak; corpus does not address measurement loopholes.
- **Break condition**: If policy mandates explicit technical standards (e.g., counts all training FLOPs, includes MoE, and defines system-level aggregation), the loophole risk diminishes.

### Mechanism 3
- **Claim**: We are poor at predicting which capabilities emerge at which compute scales, making static thresholds ineffective.
- **Mechanism**: Scaling laws reliably predict pre-training loss but fail to predict downstream task performance or risk-relevant abilities; emergent properties are unpredictable, so any chosen FLOP cutoff is arbitrary.
- **Core assumption**: The lack of reliable capability prediction means the threshold is not grounded in evidence of risk inflection.
- **Evidence anchors**:
  - [section 4] "Evidence to date suggests we are not good at predicting what abilities emerge at different scales... despite considerable effort... our ability to predict the emergence of specific downstream capabilities with scale remains elusive."
  - [abstract] "We are not good at predicting what abilities emerge at different scales, making it difficult to set appropriate thresholds."
  - [corpus] weak; corpus neighbors discuss governance but not capability prediction limitations.
- **Break condition**: If new predictive methods or empirical evidence demonstrate reliable scaling of specific risk-relevant capabilities, thresholds could become evidence-based.

## Foundational Learning

- **Concept**: Compute vs. performance scaling laws
  - **Why needed here**: The paper's critique hinges on the assumption that the relationship between compute and risk-relevant performance is unstable; understanding scaling laws clarifies why static thresholds fail.
  - **Quick check question**: Do scaling laws predict downstream task performance as accurately as they predict pre-training loss? (Answer: No—they are much less reliable for downstream capabilities.)

- **Concept**: FLOP measurement in deep learning training
  - **Why needed here**: Governance thresholds rely on FLOP counts, but the metric is ambiguous in practice; knowing what counts as FLOP and what doesn't is critical to evaluating the policy's feasibility.
  - **Quick check question**: Does FLOP include inference-time compute like chain-of-thought or MoE routing? (Answer: No—current policy definitions exclude them.)

- **Concept**: Mixture-of-Experts (MoE) architectures
  - **Why needed here**: MoEs are increasingly common but current FLOP thresholds do not specify how to count them, creating a loophole.
  - **Quick check question**: Should FLOP count all expert parameters or only active ones at inference? (Answer: The paper argues all should count, but policy is silent.)

## Architecture Onboarding

- **Component map**: Policy context -> Compute thresholds (static FLOP cutoff) -> Technical reality (FLOP metric, optimization techniques, model architectures) -> Risk assessment (capabilities, emergent properties, system-level risk) -> Governance outcome (models above threshold require reporting/scrutiny)

- **Critical path**: Define FLOP -> Measure training compute -> Compare to threshold -> Assign risk tier -> Apply regulatory actions

- **Design tradeoffs**:
  - Precision vs. simplicity: Hard-coded FLOP is simple but imprecise; dynamic percentiles are more accurate but complex to implement.
  - Coverage vs. burden: Lower thresholds catch more models but increase regulatory load; higher thresholds reduce burden but miss risk.
  - Uniformity vs. modality-specific: Single threshold easy to enforce but unfair across domains; multiple thresholds more fair but complex.

- **Failure signatures**:
  - Many low-risk models just below threshold escape scrutiny.
  - High-risk models just above threshold get unnecessary scrutiny.
  - Models exploit unmeasured post-training compute to bypass the threshold.
  - Threshold becomes obsolete quickly as optimization techniques advance.

- **First 3 experiments**:
  1. Take a set of models spanning below/above a chosen threshold; measure their actual downstream risk-relevant performance; test false positive/negative rates.
  2. Simulate a policy that counts all training FLOPs plus inference-time compute (e.g., chain-of-thought passes); recompute which models cross the threshold.
  3. Implement a dynamic percentile-based threshold on historical data; compare coverage and precision to static threshold over time.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal percentile threshold for dynamic compute-based risk assessment, and how should this threshold be adjusted over time as the distribution of model capabilities evolves?
- **Basis in paper**: [explicit] The paper recommends using a dynamic threshold based on a percentile of the distribution of model properties released each year rather than a hard-coded threshold.
- **Why unresolved**: The paper suggests using the top 5-10 percentile but does not provide empirical justification for this specific range or guidance on how to adjust it as the distribution shifts over time.
- **What evidence would resolve it**: Empirical studies tracking model capabilities over time, analysis of false positive/negative rates at different percentile thresholds, and modeling of how distributions of model properties are expected to change.

### Open Question 2
- **Question**: How can we effectively measure and account for "inference-time compute" enhancements (like chain-of-thought reasoning, retrieval-augmented generation, and model distillation) in compute-based risk assessments?
- **Basis in paper**: [explicit] The paper identifies that post-training optimizations can impart 5-20x performance gains that dramatically increase risk profiles but are not reflected in training FLOP counts.
- **Why unresolved**: Current policies focus on training FLOP, but inference-time optimizations are increasingly important for model capabilities. There is no standardized method for quantifying these enhancements in a way that can be incorporated into risk assessments.
- **What evidence would resolve it**: Development of standardized benchmarks for measuring inference-time compute overhead, empirical studies correlating inference-time optimizations with specific risk factors, and creation of a unified metric that captures both training and inference compute contributions to risk.

### Open Question 3
- **Question**: What is the empirical relationship between training compute and specific risk factors (e.g., bio-risk, cybersecurity threats, toxicity) across different modalities and downstream tasks?
- **Basis in paper**: [explicit] The paper argues that while larger models tend to produce more toxic text and harmful associations, these relationships hold at compute levels far below current thresholds, and the relationship is not straightforward across modalities.
- **Why unresolved**: There is limited scientific evidence supporting the specific thresholds chosen (1026 for general models, 1023 for biological models) and the relationship between compute and risk varies significantly across different domains and tasks.
- **What evidence would resolve it**: Comprehensive empirical studies mapping training compute to specific risk factors across multiple modalities, analysis of risk emergence at different scale points, and development of domain-specific risk models that account for modality-specific compute requirements.

## Limitations
- The analysis relies heavily on theoretical arguments rather than extensive empirical validation of threshold effectiveness.
- The paper does not provide concrete implementation guidance for the proposed dynamic percentile-based approaches.
- Limited discussion of how to balance the increased complexity of multi-metric risk assessment with practical regulatory feasibility.

## Confidence
- **Confidence: Medium** in the claim that static FLOP thresholds will become obsolete due to optimization techniques.
- **Confidence: Low-Medium** in the assertion that we cannot predict capabilities at different scales.
- **Confidence: Medium** in the measurement loophole concerns.

## Next Checks
1. **Empirical capability gap analysis**: Collect a dataset of models with varying FLOP counts and post-training optimizations, then measure their actual performance on standardized risk-relevant benchmarks to quantify false positive/negative rates under current FLOP thresholds.

2. **Policy specification audit**: Review actual implementation documents from EU AI Act and US Executive Orders to identify specific measurement guidance gaps, then consult with policy implementers to assess how frequently these ambiguities arise in practice.

3. **Dynamic threshold simulation**: Implement a percentile-based threshold system using historical model data and simulate its performance over time compared to static thresholds, measuring coverage rates, false detection rates, and administrative burden changes.