---
ver: rpa2
title: 'CORI: CJKV Benchmark with Romanization Integration -- A step towards Cross-lingual
  Transfer Beyond Textual Scripts'
arxiv_id: '2404.12618'
source_url: https://arxiv.org/abs/2404.12618
tags:
- language
- languages
- target
- source
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-lingual transfer in
  languages with strong historical and linguistic connections, particularly among
  Chinese, Japanese, Korean, and Vietnamese (CJKV). The authors demonstrate that using
  a source language with high linguistic contact (Chinese) significantly outperforms
  using English for zero-shot cross-lingual transfer across multiple natural language
  understanding tasks.
---

# CORI: CJKV Benchmark with Romanization Integration -- A step towards Cross-lingual Transfer Beyond Textual Scripts

## Quick Facts
- arXiv ID: 2404.12618
- Source URL: https://arxiv.org/abs/2404.12618
- Reference count: 18
- Key outcome: Using Chinese as source language outperforms English for zero-shot cross-lingual transfer across CJKV languages, achieving average improvements of 1.77% on PAWSX, 1.06% on XNLI, 3.93 F1 points on UDPOS, 6.72 F1 points on PANX, 5.82 EM points on XQuAD, and 5.88 EM points on MLQA.

## Executive Summary
This paper addresses the challenge of cross-lingual transfer in languages with strong historical and linguistic connections, particularly among Chinese, Japanese, Korean, and Vietnamese (CJKV). The authors demonstrate that using a source language with high linguistic contact (Chinese) significantly outperforms using English for zero-shot cross-lingual transfer across multiple natural language understanding tasks. To enable this, they construct CORI, a benchmark dataset that includes pre-segmentation and Romanized transcriptions to capture orthographic and phonemic language contact. They further propose a method to integrate Romanized transcriptions with orthographic scripts via contrastive learning, leading to enhanced cross-lingual representations and improved performance on downstream tasks.

## Method Summary
The authors propose a framework for cross-lingual transfer among CJKV languages that leverages historical language contact. The method involves constructing the CORI benchmark dataset with pre-segmentation and Romanization, then integrating these views via contrastive learning objectives. Specifically, they fine-tune XLM-R base on Chinese training data while applying contrastive loss between orthographic and Romanized representations, with multi-view augmentation through code-switching on each linguistic modality separately.

## Key Results
- Chinese source language significantly outperforms English source language for zero-shot transfer to Japanese, Korean, and Vietnamese
- Integrating Romanized transcriptions with orthographic scripts via contrastive learning improves cross-lingual representations
- CORI benchmark dataset with pre-segmentation and Romanization enables more consistent word-level annotations across CJKV languages
- Quantitative improvements: 1.77% on PAWSX, 1.06% on XNLI, 3.93 F1 on UDPOS, 6.72 F1 on PANX, 5.82 EM on XQuAD, and 5.88 EM on MLQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a source language with high linguistic contact improves zero-shot cross-lingual transfer performance.
- Mechanism: Languages with historical contact share orthographic and phonemic features that can be leveraged by multilingual models, reducing representation discrepancy between source and target languages.
- Core assumption: Representation discrepancy between languages is a key bottleneck in cross-lingual transfer.
- Evidence anchors:
  - [abstract]: "We study the impact of source language for cross-lingual transfer, demonstrating the importance of selecting source languages that have high contact with the target language."
  - [section 4]: "We observe the average CKA score between ZH and target JKV languages is significantly higher than the EN source (0.0403>0.0225), validating the lower representation discrepancy."
  - [corpus]: Weak evidence; corpus shows related work on source language selection but lacks direct comparison studies.
- Break condition: If the target language has no historical contact with any high-resource language, or if the multilingual model cannot effectively learn cross-lingual representations.

### Mechanism 2
- Claim: Integrating Romanized transcriptions with orthographic scripts enhances cross-lingual representations via contrastive learning.
- Mechanism: Romanization captures phonemic similarities obscured by different writing systems. Contrastive learning encourages agreement between orthographic and Romanized views of the same text, improving multilingual representations.
- Core assumption: Phonemic similarities between languages are complementary to orthographic similarities for cross-lingual transfer.
- Evidence anchors:
  - [abstract]: "To comprehensively capture contact between these languages, we propose to integrate Romanized transcription beyond textual scripts via Contrastive Learning objectives."
  - [section 5.3]: "while ZH and JA share orthographic contacts... ZH and VI contain subtle phonemic contacts captured via Romanization such as gǔdiǎn (ZH) vs Cổ điển (VI)."
  - [section 6]: "We propose a simple framework to integrate textual scripts with Romanized transcriptions via Contrastive Learning (CL) objectives."
- Break condition: If the multilingual model is not sensitive to the additional phonemic signal, or if the Romanization quality is poor.

### Mechanism 3
- Claim: Pre-segmentation aligns word-level semantics across CJKV languages, improving label consistency and representation learning.
- Mechanism: CJKV languages form meaningful words by combining tokens without whitespace. Pre-segmentation provides consistent word boundaries, aligning annotations and enabling better cross-lingual word-level representations.
- Core assumption: Word-level semantic units are more meaningful than token-level units for cross-lingual transfer in CJKV languages.
- Evidence anchors:
  - [section 5.2]: "For CJK, we define each character as a token and the combination of tokens after segmentation as a word... VI is written in Latin script but still whitespace does not separate meaningful units of the sentence."
  - [section 5.2]: "Through additional pre-segmentation procedures, our CORI dataset provides more consistent word-level annotations across CJKV."
  - [corpus]: Weak evidence; corpus lacks direct studies on pre-segmentation impact in CJKV languages.
- Break condition: If the segmentation tools are inaccurate or if the downstream tasks do not benefit from word-level alignment.

## Foundational Learning

- Concept: Cross-lingual transfer and zero-shot learning
  - Why needed here: The paper aims to transfer knowledge from a source language to multiple target languages without direct supervision in the target languages.
  - Quick check question: What is the difference between zero-shot cross-lingual transfer and few-shot cross-lingual transfer?

- Concept: Language contact and linguistic typology
  - Why needed here: The paper leverages historical language contact between CJKV languages to improve cross-lingual transfer, requiring understanding of orthographic and phonemic similarities.
  - Quick check question: How does language contact influence the ease of cross-lingual transfer between two languages?

- Concept: Contrastive learning and representation learning
  - Why needed here: The paper uses contrastive learning to integrate Romanized transcriptions with orthographic scripts, requiring understanding of how contrastive objectives improve representations.
  - Quick check question: What is the goal of contrastive learning in representation learning, and how does it differ from supervised learning?

## Architecture Onboarding

- Component map: CORI dataset (MT translation, pre-segmentation, Romanization) -> XLM-R base with projection head -> Task-specific loss + contrastive loss -> Zero-shot evaluation on target languages

- Critical path:
  1. Preprocess source language data (segmentation, Romanization)
  2. Generate parallel views (orthographic and Romanized)
  3. Apply contrastive learning to align representations
  4. Fine-tune on source language task data
  5. Evaluate zero-shot on target language test sets

- Design tradeoffs:
  - Romanization quality vs. simplicity: Using simple Romanization tools instead of IPA for ease of use but potentially losing some phonetic detail
  - Contrastive learning vs. task-specific objectives: Balancing representation learning with task performance
  - Pre-segmentation accuracy vs. consistency: Using automatic tools for scalability but potentially introducing errors

- Failure signatures:
  - Poor cross-lingual transfer performance despite high source language performance
  - Contrastive loss dominating task-specific loss, leading to poor downstream task performance
  - Inconsistent segmentation leading to label misalignment across languages

- First 3 experiments:
  1. Ablation study: Compare model performance with and without Romanized transcriptions to quantify their contribution
  2. Sensitivity analysis: Vary the code-switching ratio in the contrastive learning objective to find the optimal setting
  3. Language pair analysis: Evaluate the impact of source language selection by comparing ZH vs. EN as source for each target language individually

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness relies heavily on quality of Romanization and pre-segmentation tools, which may introduce errors
- Evaluation focuses on specific language pairs with historical contact, leaving generalizability to other language pairs open
- Contrastive learning framework introduces additional hyperparameters that may require careful tuning

## Confidence
**High Confidence**: The observation that source language selection impacts cross-lingual transfer performance is well-supported by CKA score comparisons and aligns with established findings.

**Medium Confidence**: The mechanism by which Romanized transcriptions enhance cross-lingual representations through contrastive learning is theoretically sound but could benefit from more ablation studies.

**Low Confidence**: The assertion that representation discrepancy is the primary bottleneck in cross-lingual transfer is not directly tested in this work.

## Next Checks
1. Ablation Study on Preprocessing Quality: Systematically evaluate the impact of Romanization and segmentation quality by comparing models trained with automatic preprocessing versus human-annotated gold standards across all six tasks in CORI.

2. Generalizability Test: Apply the proposed framework to language pairs without historical contact (e.g., ZH→DE or JA→ES) to determine whether the benefits of Romanization and contrastive learning extend beyond CJKV languages.

3. Representation Analysis: Conduct a detailed analysis of the learned representations by examining the similarity between orthographic and Romanized views using metrics beyond CKA (such as Procrustes distance or canonical correlation analysis).