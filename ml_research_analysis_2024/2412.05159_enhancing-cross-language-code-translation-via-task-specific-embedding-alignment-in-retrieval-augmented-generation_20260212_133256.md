---
ver: rpa2
title: Enhancing Cross-Language Code Translation via Task-Specific Embedding Alignment
  in Retrieval-Augmented Generation
arxiv_id: '2412.05159'
source_url: https://arxiv.org/abs/2412.05159
tags:
- code
- translation
- codebleu
- aligned
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a task-specific embedding alignment method
  for cross-language code translation from Fortran to C++ using a Retrieval-Augmented
  Generation (RAG) framework. Unlike conventional RAG approaches that use generic
  embeddings, the proposed method aligns the retrieval model with the objective of
  maximizing translation quality as measured by CodeBLEU.
---

# Enhancing Cross-Language Code Translation via Task-Specific Embedding Alignment in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2412.05159
- Source URL: https://arxiv.org/abs/2412.05159
- Reference count: 40
- One-line primary result: Task-specific embedding alignment improves cross-language code translation by 14-15% without LLM fine-tuning

## Executive Summary
This paper introduces a task-specific embedding alignment method for cross-language code translation from Fortran to C++ using a Retrieval-Augmented Generation (RAG) framework. Unlike conventional RAG approaches that use generic embeddings, the proposed method aligns the retrieval model with the objective of maximizing translation quality as measured by CodeBLEU. The approach constructs a dataset of 25,000 Fortran code snippets and generates their C++ translations using LLaMA 3.1-8B, computing pairwise CodeBLEU scores to serve as supervision signals in a contrastive learning framework. The embedding model is optimized to retrieve Fortran-C++ pairs that enhance the language model's translation performance. Integrating these CodeBLEU-optimized embeddings into the RAG framework significantly improves both retrieval accuracy and code generation quality. On the HPC Fortran2C++ dataset, the average CodeBLEU score increased from 0.64 to 0.73 (14% relative improvement), and on the Numerical Recipes dataset, it increased from 0.52 to 0.60 (15% relative improvement). These gains are achieved without fine-tuning the language model, demonstrating the efficiency and practicality of the approach.

## Method Summary
The method involves constructing a dataset of 25,000 Fortran code snippets and generating their C++ translations using LLaMA 3.1-8B. Pairwise CodeBLEU scores between all generated C++ translation pairs are computed and used as supervision signals in a contrastive learning framework. The S-InfoNCE loss is employed to optimize the embedding model, which is then integrated into the RAG framework. The aligned embeddings enhance the performance of the language model by improving retrieval accuracy, resulting in better code translation quality without the need for fine-tuning the language model.

## Key Results
- CodeBLEU score improved from 0.64 to 0.73 on HPC Fortran2C++ dataset (14% relative improvement)
- CodeBLEU score improved from 0.52 to 0.60 on Numerical Recipes dataset (15% relative improvement)
- Achieved gains without fine-tuning the language model, demonstrating efficiency and practicality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CodeBLEU-optimized embeddings improve retrieval relevance for code translation.
- Mechanism: The embedding model is trained via contrastive learning using pairwise CodeBLEU scores as similarity signals, aligning the embedding space with the translation task's quality metric.
- Core assumption: CodeBLEU scores effectively capture semantic and syntactic similarity for code translation, and optimizing embeddings toward these scores improves retrieval relevance.
- Evidence anchors:
  - [abstract] "These scores serve as supervision signals in a contrastive learning framework, where we optimize the embedding model to retrieve Fortran-C++ pairs that are most beneficial for improving the language model's translation performance."
  - [section 3.2] "Our approach aims to learn a fine-tuned embedding module Ψ that utilizes St_ij to enhance code embedding alignment."
  - [corpus] Weak - no direct evidence about CodeBLEU's effectiveness in embedding alignment found in corpus.
- Break condition: If CodeBLEU does not correlate well with actual translation quality, or if the contrastive learning objective diverges from the true task goal.

### Mechanism 2
- Claim: S-InfoNCE loss provides better alignment than standard InfoNCE by using continuous similarity scores.
- Mechanism: The S-InfoNCE loss uses continuous CodeBLEU similarity scores instead of binary class membership, allowing finer-grained optimization of embedding similarity.
- Core assumption: Continuous similarity scores provide more informative gradients than binary labels in the contrastive learning setup.
- Evidence anchors:
  - [section 3.2] "Compared to the conventional InfoNCE loss for contrastive learning, our proposed loss differs in its usage of St_ij as a soft indicator for encoding a continuous similarity between the pair (i, j), rather than a binary indicator of class membership."
  - [section 3.2, Lemma 1] Mathematical proof shows how the optimal solution relates embedding similarity to CodeBLEU scores.
  - [corpus] Weak - no corpus evidence found about S-InfoNCE specifically.
- Break condition: If the continuous similarity scores do not provide meaningful gradients, or if the optimization becomes unstable with the temperature parameter.

### Mechanism 3
- Claim: RAG with aligned embeddings improves LLM translation performance without fine-tuning.
- Mechanism: By retrieving more relevant Fortran-C++ pairs through aligned embeddings, the LLM receives better context for generation, improving translation quality.
- Core assumption: Providing better retrieval examples during inference improves LLM performance more than fine-tuning the LLM itself.
- Evidence anchors:
  - [abstract] "Integrating these CodeBLEU-optimized embeddings into the RAG framework significantly improves both retrieval accuracy and code generation quality."
  - [section 3.2] "By incorporating the optimized embedding function Ψ into the RAG setup, we enhance the performance of the language model without the need for fine-tuning."
  - [section 4] Experimental results show 14-15% relative improvement in CodeBLEU scores without LLM fine-tuning.
- Break condition: If the retrieved examples are not actually used effectively by the LLM, or if the LLM cannot leverage the additional context effectively.

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Forms the basis for the S-InfoNCE modification used to align embeddings with CodeBLEU scores
  - Quick check