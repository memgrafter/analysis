---
ver: rpa2
title: Can We Trust Recommender System Fairness Evaluation? The Role of Fairness and
  Relevance
arxiv_id: '2405.18276'
source_url: https://arxiv.org/abs/2405.18276
tags:
- measures
- fairness
- fair
- relevance
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive empirical study of joint evaluation
  measures that account for both individual item fairness and relevance in recommender
  systems. The authors examine all existing fairness-and-relevance measures, comparing
  them with relevance-only and fairness-only measures across four real-world datasets
  and four recommenders.
---

# Can We Trust Recommender System Fairness Evaluation? The Role of Fairness and Relevance

## Quick Facts
- arXiv ID: 2405.18276
- Source URL: https://arxiv.org/abs/2405.18276
- Reference count: 40
- Joint fairness-and-relevance measures exhibit extremely small empirical ranges (≤10^-3) and tend to compress scores at the low end of their range, giving the illusion of fair recommendations even when relevance and fairness scores are low.

## Executive Summary
This paper presents a comprehensive empirical study of joint evaluation measures that account for both individual item fairness and relevance in recommender systems. The authors examine all existing fairness-and-relevance measures, comparing them with relevance-only and fairness-only measures across four real-world datasets and four recommenders. They find that most joint measures have extremely small empirical ranges, are insensitive to changes in rank position, and tend to compress scores at the low end of their range, giving the illusion of fair recommendations even when relevance and fairness scores are low. The study reveals that these measures often correlate more strongly with either relevance or fairness alone, rather than balancing both aspects. Based on these findings, the authors recommend using IBO/IWO measures with caution, avoiding score misinterpretation, and considering measuring fairness and relevance separately rather than jointly.

## Method Summary
The study evaluates joint measures of individual item fairness and relevance on four real-world datasets (Lastfm, Amazon-lb, QK-video, ML-10M) preprocessed with 5-core filtering. Four recommendation models (ItemKNN, BPR, MultiVAE, NCL) are trained using standard configurations with global temporal/random splits (6:2:2). A COMBMNZ re-ranker combines predicted relevance scores with fairness-based scores (1 - normalized coverage) to generate top-k recommendations. All joint measures (IAA, IFD, HD, MME, IBO/IWO, II-F, AI-F) plus comparison measures (HR, NDCG, Jain, Gini, etc.) are computed at k=10, and Kendall's τ correlations between measure orderings across recommenders are analyzed to assess sensitivity and correlation patterns.

## Key Results
- Most joint measures (IAA, IFD, MME, II-F, AI-F) exhibit extremely small empirical ranges (≤10^-3) across all tested datasets and recommenders.
- Joint measures are less sensitive to rank position changes than relevance- and fairness-only measures, showing reduced granularity in capturing differences.
- Most joint measures correlate more strongly with either relevance or fairness alone rather than balancing both aspects, with IAA, HD, and II-F aligning more with relevance measures, while IFD, MME, and AI-F align more with fairness measures.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint fairness-and-relevance measures compress scores at the low end of their range, masking poor performance.
- Mechanism: These measures combine exposure and relevance into a single score, but their mathematical formulations (e.g., mean max envy, pairwise disparities) tend to produce extremely small values (≤10^-3) even when both fairness and relevance are poor. This occurs because the measures normalize exposure/relevance values in ways that drive differences toward zero, especially when relevant items are scarce.
- Core assumption: The theoretical range of these measures is not empirically realized; the measures behave as if performance is always close to "fair" due to scale compression.
- Evidence anchors:
  - [abstract] "tend to compress scores at the low end of their range, giving the illusion of fair recommendations even when relevance and fairness scores are low."
  - [section] "several ↓Fair+Rel scores are extremely small (≤10^-3), and these scores do not allow to distinguish across models per dataset."
- Break condition: If a joint measure is redefined to preserve scale differences (e.g., using a different normalization or scoring scheme), the compression effect could be mitigated.

### Mechanism 2
- Claim: Joint measures often correlate more strongly with either fairness or relevance alone, rather than balancing both.
- Mechanism: Due to their formulation (e.g., exposure vs. relevance difference, pairwise disparities), certain joint measures inherit more sensitivity to one aspect. For example, IAA, HD, and II-F are more aligned with relevance measures; IFD, MME, and AI-F are more aligned with fairness measures. This occurs because the mathematical weighting in each measure emphasizes one term over the other.
- Core assumption: The weighting of exposure vs. relevance in the measure's formula determines its primary sensitivity.
- Evidence anchors:
  - [abstract] "most joint measures... correlate more strongly with either relevance or fairness alone, rather than balancing both aspects."
  - [section] "We find that out of 9 joint measures, 3 align with traditional relevance-only measures, 4 agree more with fairness-only measures..."
- Break condition: If a measure is explicitly designed to balance exposure and relevance contributions (e.g., equal weighting, normalized jointly), this bias could be reduced.

### Mechanism 3
- Claim: Joint measures are less sensitive to rank position changes than single-aspect measures, reducing granularity.
- Mechanism: When sliding down the ranking, relevance scores naturally decrease and fairness scores improve, but joint measures' combined scoring dampens these opposing trends. The mathematical combination of exposure and relevance masks the magnitude of change, so the joint score changes less perceptibly across rank positions.
- Core assumption: The opposing directional changes in relevance and fairness partially cancel in the joint score, reducing sensitivity.
- Evidence anchors:
  - [abstract] "are less sensitive to rank position changes than relevance- and fairness-only measures, meaning that they are less granular than traditional RS measures."
  - [section] "Changes with decreasing rank position in the single-aspect scores are up two magnitudes greater than in the joint measures, and the latter do not reflect these differences to the same scale."
- Break condition: If the measure formulation separates the influence of rank position from relevance/fairness (e.g., by normalizing per rank), sensitivity could be restored.

## Foundational Learning

- Concept: Min-max normalization of exposure and relevance values
  - Why needed here: Many joint measures (IAA, IFD, II-F, AI-F) normalize exposure and relevance to [0,1] before combining them; misunderstanding this step leads to incorrect score interpretation.
  - Quick check question: If an item's exposure is 5 and the max exposure in the dataset is 10, what is its normalized exposure value?

- Concept: Examination functions (linear, DCG, RBP, inverse)
  - Why needed here: These discount exposure by rank position and are integral to computing fairness; choosing the wrong one changes measure sensitivity.
  - Quick check question: Which examination function applies the harshest penalty for lower-ranked items?

- Concept: Pairwise disparity computation
  - Why needed here: IFD measures fairness by computing the average pairwise difference of exposure-relevance combinations; misunderstanding the set of pairs leads to wrong results.
  - Quick check question: In IFD, do we compare every pair of items or only pairs where one is more relevant than the other?

## Architecture Onboarding

- Component map: Data preprocessing (5-core filtering, timestamp-based splits, relevance binarization) -> Recommender models (ItemKNN, BPR, MultiVAE, NCL) -> Re-ranking (COMBMNZ fusion) -> Evaluation (Joint measures + single-aspect measures)

- Critical path: 1. Load and preprocess dataset -> 2. Train recommender -> 3. Generate top-k list -> 4. Apply re-ranker -> 5. Compute all measures -> 6. Analyze sensitivity and correlation

- Design tradeoffs:
  - Joint measures vs. separate measures: Joint measures are concise but can mask poor performance; separate measures are clearer but require more computation.
  - Examination function choice: Linear is simple but less punishing; inverse is harsh but may over-penalize lower ranks.

- Failure signatures:
  - Joint scores all near zero despite low relevance/fairness -> scale compression issue
  - Joint and single-aspect measures disagree wildly -> measure misalignment
  - No variation across models in joint scores -> insensitivity to rank position

- First 3 experiments:
  1. Compute all joint measures on a simple synthetic dataset where relevance and fairness are independently varied; check for scale compression.
  2. Compare sliding-window sensitivity of joint vs. single-aspect measures on a real dataset.
  3. Measure Kendall's tau correlation of each joint measure with relevance-only and fairness-only measures across multiple datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do joint measures perform when recommendation models are directly optimized for fairness instead of using a post-hoc re-ranker?
- Basis in paper: [explicit] The paper mentions that the used recommenders are not directly optimized for fairness and uses a re-ranker, suggesting future investigation into models directly optimized for fairness.
- Why unresolved: The paper only evaluates joint measures on models that weren't fairness-optimized, so the behavior of joint measures under different optimization scenarios remains unknown.
- What evidence would resolve it: Experiments comparing joint measure behavior on fairness-optimized models versus non-optimized models, showing how optimization affects their sensitivity and score distribution.

### Open Question 2
- Question: Would different examination functions beyond the three types studied (linear, discounted, inverse) significantly affect the behavior and interpretation of joint measures?
- Basis in paper: [explicit] The paper acknowledges using three types of examination functions and mentions that the most punishing is the inverse function while the least punishing is the DCG-based discount function.
- Why unresolved: The study is limited to three examination function types, leaving open the question of how alternative functions might change measure behavior.
- What evidence would resolve it: Comparative analysis of joint measures using different examination functions across various datasets and recommenders.

### Open Question 3
- Question: Can joint measures be effectively normalized to address their small empirical ranges and improve interpretability across different recommendation systems?
- Basis in paper: [explicit] The paper mentions that the issue of small empirical scales could be fixed via a priori/post hoc normalization based on experimental values of the measures.
- Why unresolved: While normalization is suggested as a potential solution, the paper doesn't implement or evaluate such normalization approaches.
- What evidence would resolve it: Implementation and evaluation of different normalization strategies showing improved interpretability and discrimination between recommendation systems.

## Limitations

- The study relies on four specific datasets and four recommendation algorithms, which may limit generalizability despite consistent patterns across all combinations.
- Joint measures exhibit extremely small empirical ranges (≤10^-3) that fundamentally undermine their utility for distinguishing between recommendation models.
- The theoretical explanation for why compression occurs is inferred from patterns rather than proven through controlled experiments that isolate specific mathematical components.

## Confidence

- **High Confidence**: The empirical findings regarding score compression and insensitivity to rank position changes. These were directly measured across multiple datasets and consistently observed.
- **Medium Confidence**: The claim that joint measures correlate more strongly with either fairness or relevance alone rather than balancing both aspects. While the correlation analysis was performed, the interpretation of "balancing" requires subjective judgment.
- **Low Confidence**: The theoretical explanation for why compression occurs, as this was inferred from patterns rather than proven through controlled experiments that isolate specific mathematical components.

## Next Checks

1. **Controlled Synthetic Testing**: Create synthetic datasets where relevance and fairness are independently manipulated to isolate whether compression is an inherent mathematical property or dataset-dependent artifact.

2. **Alternative Normalization Schemes**: Test whether alternative normalization approaches (z-score, rank-based, or non-linear transformations) can preserve the sensitivity of joint measures without introducing compression.

3. **User Study Validation**: Conduct a user study to determine whether the compressed joint scores actually align with human judgments of recommendation quality, establishing whether these measures capture meaningful aspects of recommendation quality despite their numerical properties.