---
ver: rpa2
title: Offline Model-Based Optimization via Policy-Guided Gradient Search
arxiv_id: '2405.05349'
source_url: https://arxiv.org/abs/2405.05349
tags:
- offline
- search
- optimization
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new learning-to-search perspective for offline
  optimization by reformulating it as an offline reinforcement learning problem. The
  key idea is to learn a policy that guides gradient-based search by predicting step-size
  vectors at each iteration.
---

# Offline Model-Based Optimization via Policy-Guided Gradient Search

## Quick Facts
- arXiv ID: 2405.05349
- Source URL: https://arxiv.org/abs/2405.05349
- Reference count: 22
- Key outcome: Policy-guided gradient search (PGS) significantly improves offline model-based optimization performance by learning to predict step-size vectors that correct surrogate model gradients.

## Executive Summary
This paper proposes a new learning-to-search perspective for offline optimization by reformulating it as an offline reinforcement learning problem. The key innovation is learning a policy that guides gradient-based search by predicting step-size vectors at each iteration. This policy is trained on random trajectories synthesized from the top percentile of offline data using conservative Q-learning. The approach is shown to significantly improve optimization performance over prior methods on multiple benchmarks by helping correct the surrogate model's gradients with respect to the true objective function.

## Method Summary
The method learns a policy π that predicts d-dimensional step-size vectors for gradient-based search on a surrogate model trained from offline data. First, a surrogate model ˆfθ is trained using regression on the offline dataset. Then, random trajectories are synthesized from the top p percentile of data points (sorted by objective values), and these trajectories are converted into state-action-reward tuples for offline RL. A conservative Q-learning algorithm (CQL or IQL) is used to train the policy to predict step-size vectors that improve the surrogate's gradients. At test time, PGS performs T gradient steps starting from high-value inputs in the data, using the learned policy to guide step sizes.

## Key Results
- PGS significantly outperforms existing offline optimization methods on multiple benchmarks
- The learned policy successfully corrects surrogate model gradients to better match true objective function improvements
- OSEL (offline state estimation) effectively selects the top percentile p for trajectory synthesis
- The approach maintains strong performance even with limited offline data coverage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learning-to-search reformulation improves optimization by directly predicting step-size vectors rather than relying solely on fixed gradient descent step sizes.
- **Mechanism:** The policy π(x) predicts a d-dimensional step-size vector α at each iteration, allowing the optimization process to adaptively scale each input dimension based on local gradient information and learned heuristics. This provides more expressive and statistically efficient updates compared to scalar learning rates.
- **Core assumption:** The policy can learn effective scaling factors for each dimension that improve convergence compared to fixed or manually tuned step sizes.
- **Evidence anchors:**
  - [abstract]: "The key idea is to learn a policy that guides gradient-based search by predicting step-size vectors at each iteration."
  - [section]: "We parameterize the action space of our gradient search in terms of a diagonal matrix of parameters B= diag(α) (i.e., a step-size vector) to strike a good balance between expressiveness and statistical efficiency for policy learning."
  - [corpus]: Weak - no direct comparison between scalar vs vector step sizes in related work.

### Mechanism 2
- **Claim:** Offline RL with conservative Q-learning (CQL) creates policies that are robust to out-of-distribution (OOD) inputs by learning lower bounds on value estimates.
- **Mechanism:** CQL adds a regularizer that minimizes Q-values for unseen actions sampled from a distribution μ, preventing overestimation when the policy encounters states/inputs outside the offline dataset. This conservatism ensures the learned policy doesn't make dangerous extrapolation errors.
- **Core assumption:** The offline dataset contains sufficient coverage of relevant states to learn meaningful conservative value estimates, and the regularization strength β is appropriately tuned.
- **Evidence anchors:**
  - [section]: "To address distributional shift, CQL learns a lower bound of the true policy value leading to a more reliable value function estimate."
  - [section]: "Our empirical results on multiple benchmarks demonstrate that the learned optimization policy can be combined with existing offline surrogates to significantly improve the optimization performance."
  - [corpus]: Weak - the cited CQL paper focuses on RL domains, not optimization, though the theoretical framework applies.

### Mechanism 3
- **Claim:** Trajectory synthesis from top percentile data aligns training with test-time search behavior, focusing policy learning on high-value regions.
- **Mechanism:** By constructing trajectories only from inputs in the top p percentile of the offline dataset (sorted by objective function values), the policy learns to navigate from promising starting points toward even better solutions. This matches the test-time procedure where PGS starts from high-value inputs.
- **Core assumption:** The top percentile contains diverse enough trajectories to learn generalizable policies, and the p value is appropriately selected via OSEL.
- **Evidence anchors:**
  - [section]: "Restricting the trajectory synthesis to the subset Dtop also allows more flexibility and robustness in constructing the trajectories for offline RL."
  - [section]: "Therefore, we consider a simple and scalable choice of picking the examples lying in the pth-percentile of the given offline dataset D sorted based on their objective function values."
  - [corpus]: Weak - no direct evidence in related work about percentile-based trajectory sampling for optimization.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP) formulation
  - Why needed here: The paper reformulates offline optimization as an MDP where states are candidate inputs, actions are step-size vectors, and rewards are improvements in objective function values. This allows leveraging RL algorithms for policy learning.
  - Quick check question: In the PGS MDP, what represents the "action" that the policy π learns to select?

- **Concept:** Conservative Q-learning and offline RL
  - Why needed here: Standard RL algorithms fail on OOD inputs; CQL specifically addresses this by learning lower bounds on Q-values, making it suitable for offline optimization where the policy must handle inputs outside the training data.
  - Quick check question: What is the key difference between standard RL and offline RL that makes CQL necessary for PGS?

- **Concept:** Surrogate modeling and gradient-based optimization
  - Why needed here: PGS requires a surrogate model ˆfθ to provide gradients for the search process, and understanding how these gradients relate to the true objective function is crucial for interpreting the policy's role.
  - Quick check question: Why does PGS need a surrogate model rather than directly optimizing the unknown objective function?

## Architecture Onboarding

- **Component map:** Surrogate model ˆfθ -> Trajectory synthesizer -> Policy π -> PGS executor -> OSEL tuner
- **Critical path:**
  1. Train surrogate model ˆfθ on offline dataset D
  2. Select top p percentile subset Dtop
  3. Synthesize trajectories from Dtop
  4. Convert trajectories to RL training data (s, a, s', r)
  5. Train policy π using CQL on RL data
  6. At test time: Start from x0 ∈ Dtop, perform T PGS steps using π and ˆfθ

- **Design tradeoffs:**
  - Top p percentile vs. entire dataset: Higher p increases diversity but may include low-value regions; lower p focuses on promising areas but may overfit
  - Fixed vs. learned step sizes: Fixed scalars are simpler but less expressive; learned vectors are more complex but can adapt per dimension
  - Conservative vs. aggressive RL: Conservative CQL prevents OOD errors but may be overly cautious; aggressive RL may explore better but risks divergence

- **Failure signatures:**
  - Surrogate model ˆfθ poorly trained → noisy or incorrect gradients → policy learns wrong corrections
  - OSEL selects poor p value → trajectories lack diversity or focus on wrong regions → policy underperforms
  - CQL regularization too strong → policy becomes overly conservative → fails to explore effectively
  - Gradient search steps T too large → policy may not have enough time to correct → suboptimal solutions

- **First 3 experiments:**
  1. **Sanity check:** Run PGS with a trivial policy (constant step size) on a simple 2D benchmark to verify the pipeline works and compare against standard gradient ascent
  2. **Ablation study:** Compare PGS with CQL vs. PGS with standard RL (e.g., SAC) on a continuous task to demonstrate the importance of conservative learning
  3. **Hyperparameter sensitivity:** Vary p (top 10%, 20%, 30%, 40%) on a benchmark task and plot OSEL scores to validate the selection procedure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do different choices of reward function in the MDP formulation impact the performance of policy-guided gradient search?
- **Basis in paper:** [explicit] The paper discusses reward function design choices and mentions that improved offline optimization performance can be achieved with more complex reward engineering.
- **Why unresolved:** The paper only considers a simple reward function based on the difference in objective function values between consecutive states. It acknowledges that more complex reward functions could be explored but does not provide empirical results.
- **What evidence would resolve it:** Empirical results comparing PGS performance using different reward functions (e.g., incorporating distance to oracle maxima, multi-step returns) on benchmark tasks.

### Open Question 2
- **Question:** How does the performance of PGS scale with increasing dimensionality of the search space?
- **Basis in paper:** [inferred] The paper evaluates PGS on tasks with varying input dimensions (56D to 237D) but does not systematically analyze how performance changes with dimensionality.
- **Why unresolved:** The experimental results show PGS works well on the tested benchmarks but do not provide insights into scalability limits or performance degradation with higher dimensions.
- **What evidence would resolve it:** Experiments evaluating PGS on synthetic benchmark tasks with systematically varying input dimensions, analyzing performance trends and identifying potential scalability bottlenecks.

### Open Question 3
- **Question:** How sensitive is PGS to the choice of trajectory length during policy training?
- **Basis in paper:** [explicit] The paper mentions using trajectories of length T=50 for policy training and discusses test-time mismatch when using different trajectory lengths.
- **Why unresolved:** The paper only evaluates PGS with a fixed trajectory length of 50 and mentions the potential impact of mismatch but does not explore the sensitivity to this hyperparameter.
- **What evidence would resolve it:** Systematic experiments varying the trajectory length during policy training and evaluating the impact on PGS performance across different benchmark tasks.

## Limitations

- The OSEL procedure for selecting the top percentile p is underspecified, creating potential reproducibility issues
- No direct ablation evidence comparing learned vector step sizes against fixed scalar step sizes
- The necessity of conservative CQL over standard RL approaches for optimization-specific domains remains untested
- Limited systematic analysis of performance scaling with input dimensionality

## Confidence

- **High confidence:** The MDP reformulation and general pipeline architecture are sound and well-specified
- **Medium confidence:** The effectiveness of learned step-size vectors vs. fixed scalars, and the necessity of CQL over standard RL
- **Low confidence:** The OSEL procedure details and optimal hyperparameter settings

## Next Checks

1. **Ablation study on CQL vs standard RL:** Compare PGS with CQL against PGS with SAC or TD3 on the same benchmarks to isolate the impact of conservative vs. aggressive offline RL approaches.

2. **Step-size dimension analysis:** Implement a variant of PGS that uses scalar learning rates instead of vector step sizes, and compare optimization performance to quantify the benefit of per-dimension scaling.

3. **OSEL procedure validation:** Create a synthetic optimization problem with known optimal p value, then evaluate whether the OSEL procedure can recover this value across multiple random seeds and dataset variations.