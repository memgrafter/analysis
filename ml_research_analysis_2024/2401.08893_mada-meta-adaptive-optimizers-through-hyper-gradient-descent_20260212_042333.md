---
ver: rpa2
title: 'MADA: Meta-Adaptive Optimizers through hyper-gradient Descent'
arxiv_id: '2401.08893'
source_url: https://arxiv.org/abs/2401.08893
tags:
- mada
- adam
- optimizers
- optimizer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MADA, a meta-adaptive optimizer framework
  that unifies multiple adaptive optimizers through a parameterized formulation and
  dynamically learns the optimal optimizer coefficients during training using hyper-gradient
  descent. MADA combines optimizers like Adam, AVGrad, Yogi, Adan, and Lion into a
  single framework, allowing interpolation between them.
---

# MADA: Meta-Adaptive Optimizers through hyper-gradient Descent

## Quick Facts
- arXiv ID: 2401.08893
- Source URL: https://arxiv.org/abs/2401.08893
- Reference count: 40
- This paper introduces MADA, a meta-adaptive optimizer framework that unifies multiple adaptive optimizers through a parameterized formulation and dynamically learns the optimal optimizer coefficients during training using hyper-gradient descent.

## Executive Summary
MADA presents a novel meta-adaptive optimization framework that unifies multiple adaptive optimizers (Adam, AVGrad, Yogi, Adan, Lion) through a parameterized formulation. The key innovation is using hyper-gradient descent to dynamically learn the optimal interpolation coefficients between these optimizers during training, allowing the optimizer to adapt to local loss landscape geometry. The authors demonstrate that MADA consistently outperforms Adam and other popular optimizers on vision and language tasks, achieving better validation performance and robustness to hyperparameter initialization. A key theoretical contribution shows that interpolating between optimizers can improve convergence bounds.

## Method Summary
MADA uses a parameterized optimizer formulation that interpolates between multiple base optimizers through coefficients that define a convex polytope in coefficient space. Hyper-gradient descent is employed to update these coefficients during training by computing gradients of the loss with respect to the optimizer parameters. The framework includes AVGrad, a modification of AMSGrad that replaces the maximum operator with averaging to enable better hyper-gradient flow. The method is evaluated on language modeling tasks using GPT-2, comparing against Adam, HyperAdam, Adan, Lion, and AVGrad baselines.

## Key Results
- MADA consistently outperforms Adam and other popular optimizers on vision and language tasks
- AVGrad modification improves hyper-gradient flow compared to AMSGrad by replacing the max operator with averaging
- Theoretical analysis demonstrates that interpolating between optimizers can improve convergence bounds up to constant factors
- MADA shows better robustness to hyperparameter initialization compared to fixed optimizers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MADA's parameterized optimizer formulation enables dynamic adaptation to local loss landscape geometry.
- Mechanism: The parameterized optimizer uses coefficients to interpolate between different base optimizers, forming a convex hull in coefficient space. Hyper-gradient descent updates these coefficients during training, allowing the optimizer to adapt to changing curvature and gradient statistics.
- Core assumption: The loss landscape contains regions where different base optimizers have complementary strengths, and smooth interpolation between them can improve performance.
- Evidence anchors:
  - [abstract]: "The key idea in MADA is to parameterize the space of optimizers and dynamically search through it using hyper-gradient descent during training."
  - [section 2]: "The parameters of a parameterized optimizer define a convex polytope... whose vertices may correspond to individual base optimizers, while the interior represents new optimizers formed by interpolating between them."
  - [corpus]: Weak - no direct evidence in neighbors about parameterized optimizer interpolation mechanisms.

### Mechanism 2
- Claim: Replacing AMSGrad's max operator with AVGrad's averaging improves hyper-gradient flow.
- Mechanism: The max operator in AMSGrad creates discontinuous gradient flow (0 gradients for the smaller term), preventing efficient hyper-gradient updates to coefficients. AVGrad's averaging provides smooth gradient paths for all coefficients.
- Core assumption: Hyper-gradient descent requires continuous, differentiable paths for coefficient updates to be effective.
- Evidence anchors:
  - [section 4.1]: "we conjecture that this is caused by the maximum operator in the second-moment term... back-propagation of gradients through max(a,b) corresponds to a simple routing operation to the larger one of a and b, where the smaller one is passed 0 gradients."
  - [section 4]: "We propose AVGrad, which replaces the maximum operator on the second-order moments with time-averaging, and results in better hyper-gradient flow and validation loss."
  - [corpus]: Weak - no direct evidence in neighbors about max operator vs averaging effects on hyper-gradients.

### Mechanism 3
- Claim: Interpolated optimizers can achieve better convergence bounds than individual base optimizers.
- Mechanism: Theorem 1 shows that the convergence bound for the interpolated optimizer between AVGrad and Adam improves when ρ=β2, demonstrating that interpolation can combine favorable properties of both base optimizers.
- Core assumption: The theoretical analysis of convergence bounds extends to practical training scenarios and that better bounds correlate with actual performance improvement.
- Evidence anchors:
  - [section 4.2]: "Our analysis shows that the interpolated optimizer improves the convergence bounds of the base optimizers up to constant factors."
  - [section 4.2]: "We plot the value of the bound in Theorem 1... The interpolated optimizer provides the best convergence bound when ρ=β2, which suggests an advantage for the interpolated optimizers."
  - [corpus]: Weak - no direct evidence in neighbors about convergence bound improvements through interpolation.

## Foundational Learning

- Concept: Hyper-gradient descent and its application to optimizer hyperparameter tuning
  - Why needed here: MADA uses hyper-gradient descent to update optimizer coefficients during training, avoiding expensive outer hyperparameter optimization loops
  - Quick check question: How does hyper-gradient descent differ from standard gradient descent in terms of what it optimizes?

- Concept: Parameterized representations of algorithms and convex hulls
  - Why needed here: The parameterized optimizer formulation creates a convex polytope of possible optimizers through interpolation coefficients
  - Quick check question: What geometric structure represents the space of possible optimizers in MADA's formulation?

- Concept: Adaptive moment estimation and its variants (Adam, AMSGrad, Yogi, Adan, Lion)
  - Why needed here: MADA unifies these specific optimizers through parameterization, requiring understanding of their update rules and differences
  - Quick check question: What key design choice distinguishes AMSGrad from Adam, and why did MADA replace it with AVGrad?

## Architecture Onboarding

- Component map:
  - Parameterized optimizer core: Maintains state (first/second moments) and update rules parameterized by coefficients
  - Hyper-gradient computation module: Computes gradients of loss w.r.t. optimizer coefficients using autograd
  - Coefficient update mechanism: Applies hyper-gradient descent steps with projection to coefficient domain
  - Base optimizer library: Contains implementations of Adam, AVGrad, Yogi, Adan, Lion update rules
  - Integration layer: Connects to training loop, handles state initialization and checkpointing

- Critical path:
  1. Initialize model parameters and optimizer coefficients
  2. For each training step:
     - Sample mini-batch and compute loss
     - Backpropagate to get parameter gradients
     - Update model parameters using current parameterized optimizer state
     - Compute hyper-gradients w.r.t. coefficients
     - Update coefficients using hyper-gradient descent
  3. Periodically checkpoint model and optimizer state

- Design tradeoffs:
  - Memory vs. adaptability: More coefficients enable finer adaptation but increase memory usage
  - Computational overhead: Hyper-gradient computation adds O(N) operations per step
  - Initialization sensitivity: Poor initial coefficients may slow early training
  - Hyper-learning rate tuning: Additional hyperparameters required for coefficient updates

- Failure signatures:
  - Coefficients diverging to boundary values (0 or 1) suggests poor initialization or learning rate
  - Oscillating coefficients indicate conflicting gradient signals or inappropriate learning rate
  - Coefficients remaining static suggests vanishing hyper-gradients or poor hyperparameter choices

- First 3 experiments:
  1. Verify basic functionality: Train a small model (e.g., 2-layer MLP on MNIST) with MADA initialized from Adam, compare convergence to Adam baseline
  2. Test coefficient adaptation: Train on a synthetic convex problem where Adam fails (as in Appendix C.3), verify MADA adapts toward AVGrad
  3. Benchmark robustness: Train on Shakespeare dataset with poor initial β values, compare MADA performance to Adam with same initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MADA change when interpolating between more than five optimizers, or when including other popular optimizers like AdamP, AdaBelief, or SAM?
- Basis in paper: [explicit] The authors note that their concrete parameterization interpolates between Adam, AVGrad, Yogi, Adan, and Lion, and mention that other optimizers could be included in principle.
- Why unresolved: The paper focuses on a specific parameterized optimizer with five base optimizers, and does not explore the effects of including additional optimizers or alternative choices.
- What evidence would resolve it: Empirical results comparing MADA performance with different sets of base optimizers, including additional or alternative optimizers, would clarify the impact of the choice of interpolated optimizers on overall performance.

### Open Question 2
- Question: What is the theoretical relationship between the convergence bounds of the interpolated optimizer and the individual base optimizers, beyond the constant factors mentioned in the paper?
- Basis in paper: [explicit] The authors state that their convergence analysis shows the interpolated optimizer improves the convergence bounds of the base optimizers up to constant factors, but do not provide a more detailed characterization of this relationship.
- Why unresolved: The convergence analysis is limited to a specific interpolation between AVGrad and Adam, and does not explore the general theoretical properties of interpolated optimizers.
- What evidence would resolve it: A more comprehensive theoretical analysis of the convergence properties of interpolated optimizers, potentially including lower bounds or more detailed characterizations of the relationship between the interpolated and base optimizers' convergence bounds, would provide deeper insights.

### Open Question 3
- Question: How does the performance of MADA change when using different learning rate schedules or weight decay strategies compared to the ones used in the experiments?
- Basis in paper: [inferred] The authors keep the learning rate schedule and weight decay identical across all optimizers in their experiments, except for Lion, which uses a lower initial learning rate as suggested in the original paper.
- Why unresolved: The paper does not explore the sensitivity of MADA's performance to different learning rate schedules or weight decay strategies, which could be important factors in its overall effectiveness.
- What evidence would resolve it: Empirical results comparing MADA's performance with different learning rate schedules or weight decay strategies would clarify the importance of these factors and potentially identify optimal configurations for MADA.

## Limitations

- The performance claims rely heavily on comparisons against a limited set of baseline optimizers without broader ablation studies
- Theoretical convergence bounds may not translate directly to practical performance gains across all tasks
- The hyper-gradient descent mechanism's sensitivity to learning rate choices for optimizer coefficients remains unexplored

## Confidence

- **High confidence**: The parameterized optimizer formulation and convex hull representation - this is a well-defined mathematical construction with clear implementation implications
- **Medium confidence**: AVGrad's improvement over AMSGrad for hyper-gradient flow - supported by theoretical reasoning but limited empirical validation beyond the specific examples
- **Low confidence**: Convergence bound improvements translating to practical performance gains - theoretical analysis exists but the practical significance and consistency across tasks is unclear

## Next Checks

1. **Coefficient stability analysis**: Track optimizer coefficient evolution across multiple training runs and initializations to verify that coefficients converge to meaningful values rather than boundary conditions or oscillating states
2. **Ablation study on optimizer library**: Systematically remove individual optimizers (Adam, Yogi, etc.) from the parameterized formulation to identify which components contribute most to performance improvements
3. **Cross-task generalization test**: Evaluate MADA on diverse problem types (vision, reinforcement learning, non-convex optimization problems) to assess whether performance gains generalize beyond the NLP benchmarks presented in the paper