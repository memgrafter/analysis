---
ver: rpa2
title: Black box meta-learning intrinsic rewards for sparse-reward environments
arxiv_id: '2407.21546'
source_url: https://arxiv.org/abs/2407.21546
tags:
- rewards
- learning
- intrinsic
- training
- loop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a black box meta-learning approach for training
  intrinsic reward functions to improve exploration in sparse-reward environments.
  Unlike prior work relying on meta-gradients, the method frames reward learning as
  a standard RL problem, avoiding second-order gradient computations.
---

# Black box meta-learning intrinsic rewards for sparse-reward environments

## Quick Facts
- arXiv ID: 2407.21546
- Source URL: https://arxiv.org/abs/2407.21546
- Reference count: 40
- Primary result: Meta-learned intrinsic rewards and advantage functions significantly outperform sparse and shaped extrinsic rewards on MetaWorld benchmarks

## Executive Summary
This paper introduces a black box meta-learning approach for training intrinsic reward functions to improve exploration in sparse-reward environments. Unlike prior work relying on meta-gradients, the method frames reward learning as a standard RL problem, avoiding second-order gradient computations. A stochastic LSTM-based reward network is trained via interaction with a distribution of continuous control tasks, using shaped rewards during meta-training but only sparse rewards during evaluation. Experiments on MetaWorld benchmarks show that training with learned intrinsic rewards significantly outperforms both sparse and shaped extrinsic rewards, with strong generalization across parametric task variations. A similar approach using meta-learned advantage functions achieves comparable results. The method offers computational advantages and flexibility, though it struggles with non-parametric task variations and requires access to training tasks.

## Method Summary
The approach uses an LSTM-based stochastic reward network trained via PPO to generate intrinsic rewards for sparse-reward environments. During meta-training, the reward network learns from shaped rewards, but during evaluation, it provides intrinsic rewards to replace sparse extrinsic rewards in a PPO inner loop. The outer loop treats the reward network as an RL agent, updating it to maximize the meta-learning objective using only first-order gradients. The method also explores meta-learning advantage functions as an alternative parameterization. Both components are trained on a distribution of continuous control tasks from MetaWorld, with evaluation showing significant improvements over sparse and shaped reward baselines.

## Key Results
- Meta-learned intrinsic rewards outperform both sparse and shaped extrinsic rewards on MetaWorld tasks
- The method generalizes well to parametric task variations but struggles with non-parametric variations
- Meta-learned advantage functions achieve similar performance to learned intrinsic rewards
- The black-box approach avoids expensive second-order gradient computations while maintaining effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The meta-learned LSTM reward network generates task-relevant intrinsic signals by encoding entire episode history into a continuous policy update signal.
- Mechanism: At each step, the LSTM receives `(s_t, a_t, π_θ(a_t|s_t), r_e_t, r_i_t-1, π_r_ϕ(r_i_t-1|D:t-1), d_t)` and outputs a stochastic reward `r_i_t` drawn from a Gaussian. This reward is then used in place of the sparse extrinsic reward during inner-loop PPO updates, shaping the policy gradient toward task-solving behavior without requiring dense environment feedback.
- Core assumption: The LSTM can learn to map interaction histories to useful reward signals that accelerate policy learning, even when trained only with shaped extrinsic rewards but evaluated with sparse ones.
- Evidence anchors:
  - [abstract]: "A stochastic LSTM-based reward network is trained via interaction with a distribution of continuous control tasks, using shaped rewards during meta-training but only sparse rewards during evaluation."
  - [section 3]: "We model the intrinsic reward function as a stochastic agent πr_ϕ(r_i_t | D:t) ... At each step t, the LSTM receives as input the tuple {s_t, a_t, π_θ(a_t | s_t), r_e_t, r_i_t-1, πr_ϕ(r_i_t-1 | D:t-1), d_t}"
  - [corpus]: No direct evidence; inference drawn from described LSTM training loop.
- Break condition: If the LSTM cannot generalize across parametric variations, intrinsic rewards will fail to improve sparse-reward learning, reverting performance to baseline sparse-reward PPO.

### Mechanism 2
- Claim: Meta-learning the reward function via standard RL avoids expensive second-order gradients while retaining effective credit assignment.
- Mechanism: Instead of backpropagating through policy parameter updates (meta-gradients), the outer loop treats the reward network as an RL agent maximizing the meta-learning objective. This black-box approach uses only first-order gradients, simplifying implementation and reducing computational cost.
- Core assumption: First-order RL updates on the reward network are sufficient to learn useful intrinsic reward signals without explicit modeling of their effect on policy parameters.
- Evidence anchors:
  - [section 3]: "This contrasts with the current standard of using meta-gradients when learning parts of the inner update... Unlike previous methods that rely on meta-gradients... our approach avoids the computation of second-order gradients."
  - [section 3]: "We achieve this by not modelling explicitly the influence intrinsic rewards have in the inner loop’s learning procedure; instead, we let the outer loop treat it as part of the stochasticity involved in its optimization process."
  - [corpus]: No direct evidence; consistent with described PPO outer-loop training of reward network.
- Break condition: If the outer-loop signal variance becomes too high, the reward network may fail to converge to useful reward functions, negating the computational advantage.

### Mechanism 3
- Claim: Meta-learning advantage functions provides an alternative parameterization of the inner-loop objective that can outperform intrinsic rewards in certain benchmarks.
- Mechanism: Instead of generating rewards, a meta-learned network outputs advantage estimates for each transition, which are used directly in PPO advantage computation. This changes the policy gradient estimate while keeping the same outer-loop training procedure.
- Core assumption: The advantage function can encode task-relevant value information that improves policy learning as effectively as (or better than) learned rewards.
- Evidence anchors:
  - [section 5.2]: "Instead of learning to assign partial credit to a transition and its preceding transitions, the network learns to evaluate each transition’s quality independently."
  - [section 5.2]: "Both methods exhibit similar qualitative behaviour. For the considered benchmarks, using the learned advantage function shows some benefits."
  - [corpus]: No direct evidence; inferred from described advantage function experiments.
- Break condition: If the advantage function fails to generalize to non-parametric task variations, it will not improve over sparse-reward baselines in those cases.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework operates on MDPs; understanding states, actions, rewards, and trajectories is essential to grasp how intrinsic rewards and advantages fit into the RL loop.
  - Quick check question: What is the difference between the agent's policy π_θ and the intrinsic reward function πr_ϕ in this setup?

- Concept: Policy Gradient Methods (PPO)
  - Why needed here: PPO is the inner-loop algorithm; understanding advantage estimation, clipping, and entropy regularization is crucial for interpreting the results and potential failure modes.
  - Quick check question: How does replacing extrinsic rewards with intrinsic rewards in the PPO update affect the policy gradient estimate?

- Concept: Meta-Learning (MAML-style)
  - Why needed here: The outer loop meta-trains components (rewards or advantages) to improve inner-loop learning; understanding the two-loop structure and objective is key to seeing why this accelerates sparse-reward learning.
  - Quick check question: In the black-box approach, what is the role of the outer-loop critic, and how does it differ from meta-gradients?

## Architecture Onboarding

- Component map: MetaWorld tasks -> Inner loop PPO agent -> Interaction collection -> Outer loop PPO trainer (reward/advantage network + critic)
- Critical path: Task sampling -> interaction collection -> inner loop PPO update (using meta signal) -> outer loop PPO update (on full lifetime)
- Design tradeoffs: Black-box RL for reward network vs. meta-gradients (simplicity vs. potentially lower variance); learned rewards vs. learned advantages (direct credit assignment vs. value estimation)
- Failure signatures:
  - Intrinsic rewards not improving performance -> check if LSTM is learning meaningful signals (inspect reward distributions, correlation with task progress)
  - Outer loop not converging -> check learning rate, reward normalization, or meta-objective formulation
  - Poor generalization -> verify that training tasks sufficiently cover the test task distribution
- First 3 experiments:
  1. Run inner-loop PPO with only sparse rewards (baseline) to confirm slow learning.
  2. Train meta reward network on shaped rewards, evaluate on sparse rewards, compare to baseline.
  3. Train meta advantage network, evaluate, compare both meta-methods and baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the black box meta-learning approach perform when trained on sparse rewards throughout both meta-training and evaluation?
- Basis in paper: [explicit] The paper states that sparse rewards are only used during evaluation, while shaped rewards are available during meta-training. It suggests this as a direction for future work.
- Why unresolved: The paper does not explore the performance of the method when trained solely on sparse rewards, which would be a more realistic and challenging setting.
- What evidence would resolve it: Conducting experiments where the meta-learning phase also uses sparse rewards instead of shaped ones, and comparing the performance to the current approach.

### Open Question 2
- Question: Would incorporating future interaction steps into the reward network's input improve performance or sample efficiency?
- Basis in paper: [inferred] The paper mentions that the reward network only looks at past data when generating rewards. It suggests using a network that has access to future interaction steps within the batch of collected data as a straightforward avenue for improvement.
- Why unresolved: The paper does not investigate the impact of including future information in the reward network's input on performance or sample efficiency.
- What evidence would resolve it: Modifying the reward network architecture to include future interaction steps and evaluating its performance and sample efficiency compared to the current approach.

### Open Question 3
- Question: How does the black box meta-learning approach compare to meta-gradient methods in terms of computational cost and performance?
- Basis in paper: [explicit] The paper discusses the computational advantages of the black box approach, which avoids second-order gradient computations. However, it also mentions that meta-gradients may provide a lower variance signal for meta-learning.
- Why unresolved: The paper does not provide a quantitative comparison between the black box approach and meta-gradient methods in terms of computational cost and performance.
- What evidence would resolve it: Implementing a meta-gradient based approach for comparison and conducting experiments to measure the computational cost and performance of both methods under the same conditions.

## Limitations

- The method struggles with non-parametric task variations where training tasks don't cover the test task distribution
- Meta-training requires access to a distribution of tasks with shaped rewards, which may not always be available
- The paper lacks detailed analysis of sample efficiency during meta-training and hyperparameter sensitivity

## Confidence

- Claims about computational efficiency gains: High confidence
- Claims about generalization benefits: High confidence
- Claims about effectiveness of meta-learned advantage function: Medium confidence
- Claims about limitations with non-parametric variations: Low confidence (limited mechanistic explanation)

## Next Checks

1. Verify that the meta-learned intrinsic rewards actually encode task-relevant information by visualizing reward distributions and correlating them with task progress across different training seeds
2. Test the approach on non-parametric task variations (e.g., different goal positions not seen during training) to quantify the generalization gap
3. Compare wall-clock training time and sample efficiency between the black-box approach and meta-gradient methods to validate the claimed computational advantages