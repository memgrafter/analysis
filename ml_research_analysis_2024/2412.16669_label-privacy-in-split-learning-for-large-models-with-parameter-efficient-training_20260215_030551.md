---
ver: rpa2
title: Label Privacy in Split Learning for Large Models with Parameter-Efficient Training
arxiv_id: '2412.16669'
source_url: https://arxiv.org/abs/2412.16669
tags:
- learning
- privacy
- training
- fine-tuning
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of private fine-tuning of large
  models over APIs, where the client's labels are at risk of being leaked. The authors
  analyze the vulnerability of popular parameter-efficient fine-tuning (PEFT) methods
  like LoRA to label leakage in a two-party split learning setup.
---

# Label Privacy in Split Learning for Large Models with Parameter-Efficient Training

## Quick Facts
- arXiv ID: 2412.16669
- Source URL: https://arxiv.org/abs/2412.16669
- Reference count: 23
- This paper analyzes and addresses label privacy vulnerabilities in PEFT methods during split learning, proposing P3EFT to achieve competitive privacy-utility trade-offs.

## Executive Summary
This paper tackles the problem of private fine-tuning of large models over APIs, where client labels are at risk of being leaked. The authors analyze the vulnerability of popular parameter-efficient fine-tuning (PEFT) methods like LoRA to label leakage in a two-party split learning setup. They propose P3EFT, a protocol that leverages the properties of PEFT to obfuscate gradients and activations, ensuring label privacy. P3EFT uses private backpropagation with noise injection and trains multiple adapter sets with mixing weights to prevent label inference. Experiments on DeBERTa-v2-XXLarge, Flan-T5-Large, and LLaMA-2 7B across various NLP tasks show that P3EFT achieves competitive privacy guarantees while maintaining high accuracy, outperforming existing methods like DC and PSLF in terms of the utility-privacy trade-off.

## Method Summary
The P3EFT protocol addresses label privacy in split learning by combining two main techniques: private backpropagation with noise injection and training multiple adapter sets with mixing weights. During training, the client maintains multiple independent adapter sets and combines their outputs using randomized mixing weights to prevent individual adapters from leaking label information. The private backpropagation mechanism ensures that no single server learns the true gradient by having the client send obfuscated gradients to multiple servers and reconstructing the true gradient locally. Adversarial regularization is applied to each adapter to prevent it from encoding label information that could be extracted by a linear classifier.

## Key Results
- P3EFT achieves competitive privacy guarantees while maintaining high accuracy on NLP tasks
- The protocol outperforms existing methods like DC and PSLF in terms of utility-privacy trade-off
- Experiments demonstrate effectiveness across DeBERTa-v2-XXLarge, Flan-T5-Large, and LLaMA-2 7B models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gradient obfuscation through private backpropagation prevents label leakage from gradients.
- **Mechanism**: By generating multiple noise vectors and mixing them with the true gradient, each server receives only a noisy version. The client reconstructs the true gradient by combining results from multiple servers, but no single server learns the original gradient.
- **Core assumption**: Backpropagation is conditionally linear in output gradients, allowing gradients to be expressed as a linear combination of noisy vectors.
- **Evidence anchors**:
  - [abstract]: "leverages the properties of PEFT to obfuscate gradients and activations"
  - [section]: "backprop is linear in terms of gh for any fixed x, θ" and "we obtain backprop(x, θ, gh + z) using an API call to server 1, whereas the second term backprop(x, θ, gh − z) translates to an API call to server 2"
- **Break condition**: If the attacker can determine the mixing coefficients αi or can access multiple consecutive parameter updates from the same server, they may recover the true gradient.

### Mechanism 2
- **Claim**: Multiple adapter sets with mixing weights prevent label leakage from activations.
- **Mechanism**: Instead of learning one adapter set, the client learns n independent adapter sets and combines their outputs using randomized mixing weights. This ensures that individual adapters don't reveal label information while the combined output still performs well.
- **Core assumption**: PEFT adapters are compact enough that maintaining multiple sets is computationally feasible for the client.
- **Evidence anchors**:
  - [abstract]: "trains multiple adapter sets with mixing weights to prevent label inference"
  - [section]: "a client creates n independent adapter sets θ1, ..., θn" and "During the forward pass, the outputs of different adapters are mixed together using randomized mixing weights W"
- **Break condition**: If the attacker can determine the mixing weights W or if the regularization fails to prevent individual adapters from learning to leak labels.

### Mechanism 3
- **Claim**: Adversarial regularization prevents individual adapters from learning to leak labels.
- **Mechanism**: For each adapter, a linear "head" is trained to predict labels from the adapter's activations. The adapter parameters are then updated adversarially to prevent this head from succeeding, ensuring the adapter doesn't encode label information.
- **Core assumption**: Linear classifiers can effectively detect whether an adapter's activations leak label information.
- **Evidence anchors**:
  - [abstract]: "trains multiple adapter sets with mixing weights to prevent label inference"
  - [section]: "we maintain this over the course of training with a privacy regularizer inspired by Ganin & Lempitsky (2015)" and "each head is trained to minimize cross-entropy"
- **Break condition**: If the attacker can bypass the adversarial regularization or if the linear heads are insufficient to detect complex label leakage patterns.

## Foundational Learning

- **Concept**: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: The entire privacy mechanism relies on PEFT's property of having few trainable parameters, making it feasible to maintain multiple adapter sets.
  - Quick check question: Why does PEFT make it possible to maintain multiple adapter sets without significant computational overhead?

- **Concept**: Split learning/Vertical federated learning
  - Why needed here: The two-party setup where one party holds the model and another holds the data is the fundamental architecture being protected.
  - Quick check question: In split learning, what information flows between the two parties during training?

- **Concept**: Differential privacy
  - Why needed here: Provides the theoretical framework for understanding when label information is sufficiently protected from inference.
  - Quick check question: How does differential privacy relate to the concept of preventing label leakage in this context?

## Architecture Onboarding

- **Component map**:
  Client side: Local model head, multiple adapter sets (θ1...θn), mixing weights generator, adversarial regularization component, optimizer
  Server side: Pre-trained model with forward/backprop APIs, multiple execution environments for handling multiple adapter sets
  Communication: Forward pass activations, obfuscated gradients

- **Critical path**:
  1. Client generates mixing weights and noise vectors
  2. Client sends forward request with adapter weights
  3. Server computes activations and returns them
  4. Client computes loss and gradients w.r.t. activations
  5. Client obfuscates gradients and sends to multiple servers
  6. Servers compute obfuscated gradients w.r.t. parameters
  7. Client reconstructs true gradients and updates parameters

- **Design tradeoffs**:
  - Privacy vs. accuracy: More noise/variance improves privacy but may hurt performance
  - Number of adapter sets (n): More sets improve privacy but increase computational overhead
  - Number of noise vectors (m): More vectors improve privacy but increase communication overhead
  - Regularization strength: Stronger regularization improves privacy but may hurt utility

- **Failure signatures**:
  - High accuracy but poor privacy: Regularization insufficient, mixing weights predictable
  - Poor accuracy: Too much noise, insufficient mixing, or adversarial regularization too strong
  - Inconsistent results: Implementation bugs in gradient reconstruction or mixing weight generation

- **First 3 experiments**:
  1. Verify gradient reconstruction: Send known gradients through private_backprop with 2 servers and verify exact recovery
  2. Test label leakage on single adapter: Train without privacy mechanisms and measure label inference accuracy
  3. Validate mixing weights: Check that individual adapters don't leak labels while combined output maintains accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the P3EFT approach be extended to provide holistic privacy protection for both input data and labels in a split learning setup?
- Basis in paper: [explicit] The paper discusses the current limitation of P3EFT in protecting only label privacy and suggests that future work could explore adapting P3EFT for input privacy or combining it with existing methods like Li et al. (2023b).
- Why unresolved: The paper focuses on label privacy and does not explore mechanisms to protect input data privacy, leaving the extension of P3EFT to input privacy as an open challenge.
- What evidence would resolve it: Developing and testing a modified version of P3EFT that incorporates input privacy mechanisms, followed by empirical validation showing improved privacy-utility trade-offs for both inputs and labels.

### Open Question 2
- Question: What are the theoretical limits of the attacker's capabilities in the white-box scenario of the private_backprop algorithm, and how do these limits translate to practical scenarios?
- Basis in paper: [explicit] The paper provides a theoretical analysis of the private_backprop algorithm under a white-box assumption, which is stronger than the practical setup, and acknowledges the complexity of determining the attacker's capabilities in the general case.
- Why unresolved: The theoretical analysis assumes a white-box scenario where the attacker knows the client-side model, which is not realistic in practice. The practical limits of the attacker's capabilities remain unclear.
- What evidence would resolve it: Conducting a comprehensive study to evaluate the attacker's capabilities in realistic scenarios, including empirical attacks and theoretical bounds, to determine the practical limits of privacy protection.

### Open Question 3
- Question: How does the number of adapter sets (n) in P3EFT affect the privacy-utility trade-off, and what is the optimal number of adapters for different model sizes and tasks?
- Basis in paper: [explicit] The paper includes an ablation study on the number of adapter sets (n) with DeBERTa, showing minimal influence on privacy and accuracy, but leaves the question of the optimal number of adapters open.
- Why unresolved: The ablation study provides initial insights but does not explore the impact of varying n across different model sizes, tasks, and privacy requirements, nor does it establish a clear criterion for determining the optimal number of adapters.
- What evidence would resolve it: Conducting extensive experiments across diverse models, tasks, and privacy budgets to identify patterns and derive guidelines for selecting the optimal number of adapters based on specific use cases and requirements.

## Limitations

- The protocol requires multiple API calls per iteration, which may be impractical for real-world deployment with rate-limited APIs
- The computational overhead of maintaining multiple adapter sets scales linearly with n, potentially making it infeasible for resource-constrained clients
- The theoretical privacy analysis assumes a passive adversary model and may not hold against more sophisticated active attacks

## Confidence

**High Confidence Claims:**
- The P3EFT framework architecture is technically sound and addresses a real problem in split learning
- The use of multiple adapter sets with mixing weights provides a reasonable approach to preventing label leakage
- The theoretical privacy guarantees under the assumed threat model are valid

**Medium Confidence Claims:**
- The experimental results showing competitive utility-privacy trade-off are likely reproducible, but exact numbers may vary with implementation details
- The effectiveness of the adversarial regularization in preventing label leakage is theoretically sound but may depend on implementation specifics

**Low Confidence Claims:**
- The practical effectiveness against adaptive adversaries who may exploit implementation details or side-channel information
- The scalability of the approach to larger models and more complex tasks beyond the evaluated ones
- The robustness of the privacy guarantees under realistic conditions where the assumptions may be violated

## Next Checks

1. **Implementation Fidelity Test**: Implement the gradient reconstruction mechanism with controlled noise injection and verify that true gradients are accurately recovered while individual server views remain obfuscated

2. **Adversarial Attack Benchmark**: Conduct comprehensive privacy attacks (beyond the reported spectral, norm, and K-means attacks) to evaluate the robustness of P3EFT against state-of-the-art inference attacks on adapter activations

3. **Scalability Evaluation**: Test the protocol with larger models (e.g., GPT-3 sized) and evaluate the impact on both computational requirements and privacy guarantees as model size increases beyond the evaluated range