---
ver: rpa2
title: 'DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation'
arxiv_id: '2402.11120'
source_url: https://arxiv.org/abs/2402.11120
tags:
- target
- domain
- dart
- source
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning adversarially robust
  models in the unsupervised domain adaptation (UDA) setting, where labeled data is
  available from a source domain but not the target domain. The key challenge is that
  conventional adversarial training methods cannot be directly applied due to the
  absence of target labels.
---

# DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation

## Quick Facts
- **arXiv ID**: 2402.11120
- **Source URL**: https://arxiv.org/abs/2402.11120
- **Reference count**: 40
- **Primary result**: Achieves up to 29.2% relative improvement in adversarial robustness on target domains in unsupervised domain adaptation

## Executive Summary
This paper addresses the critical challenge of learning adversarially robust models when labeled data is available only from a source domain but not the target domain. The authors develop DART (Divergence Aware adveRsarial Training), a principled defense framework that extends adversarial training to the unsupervised domain adaptation setting. By establishing a new generalization bound, DART jointly optimizes three key components: source loss, worst-case domain divergence, and an approximation of the ideal joint loss using pseudo-labels. Extensive experiments on a new testbed called DomainRobust demonstrate that DART significantly outperforms state-of-the-art methods in terms of robust accuracy on target data while maintaining competitive standard accuracy.

## Method Summary
DART is a unified defense framework that can be integrated with various UDA algorithms to achieve adversarial robustness. It operates by jointly minimizing three terms derived from a theoretical generalization bound: (1) source domain loss, (2) worst-case domain divergence between source and target, and (3) an approximation of the ideal joint loss using pseudo-labels for target data. The method supports general threat models and uses adversarial or KL-transformed source examples to better align source and worst-case target distributions. A pseudo-label predictor is maintained and periodically updated based on validation performance to improve the quality of target pseudo-labels over time.

## Key Results
- Achieves up to 29.2% relative improvement in robustness on certain source-target domain pairs
- Outperforms state-of-the-art methods in robust accuracy on target data across multiple datasets (DIGIT, OfficeHome, PACS, VisDA)
- Maintains competitive standard accuracy while significantly improving adversarial robustness
- Validated using both PGD attack and AutoAttack for comprehensive robustness assessment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DART bounds adversarial target loss by controlling source loss, domain divergence, and ideal joint loss simultaneously.
- **Mechanism**: The theoretical generalization bound shows that adversarial target loss can be decomposed into three additive terms: (1) source domain loss, (2) worst-case domain divergence, and (3) ideal joint loss. By minimizing all three terms, DART achieves better adversarial robustness than methods that ignore any term.
- **Core assumption**: The ideal joint loss with worst-case target can be approximated by using pseudo-labels for target data.
- **Evidence anchors**:
  - [abstract]: "We first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence."
  - [section 3]: "Theorem 3.1 states that the adversarial target loss can be bounded from above by three main terms... source loss, domain divergence, and the ideal joint loss."
  - [corpus]: Weak/no direct evidence. The related papers focus on UDA but don't discuss adversarial robustness bounds in this specific form.
- **Break condition**: If pseudo-labels are inaccurate (e.g., due to large domain shift or complex decision boundaries), the approximation of ideal joint loss breaks down, reducing robustness gains.

### Mechanism 2
- **Claim**: Using adversarial or KL-transformed source examples improves robustness transfer.
- **Mechanism**: By transforming source examples to their adversarial or KL-divergence-maximizing versions during training, DART better aligns the source and worst-case target distributions, reducing the gap between source-only and target robustness.
- **Core assumption**: Adversarial/KL-transformed source examples provide a better approximation of the worst-case target distribution than clean source examples.
- **Evidence anchors**:
  - [section 4]: "We investigate three natural choices of transformations of the source data... adversarial source: choose the source data that maximizes the adversarial source loss; i.e., ˜xs_i = argmax_˜x_i∈B(xs_i) ℓ(h(˜x_i); ys_i)... KL source: choose the source data that maximizes the Kullback-Leibler (KL) divergence of the clean and adversarial predictions."
  - [table 1]: DART(adv src) and DART(kl src) show slightly lower performance than DART(clean src) in some cases, suggesting the assumption isn't universally true.
  - [corpus]: No direct evidence. Related works don't explore source transformation for robustness transfer.
- **Break condition**: If the perturbation set B doesn't align well between source and target domains (e.g., different image statistics), adversarial/KL source transformations may not help and could even hurt performance.

### Mechanism 3
- **Claim**: Periodic pseudo-label update improves target model accuracy over time.
- **Mechanism**: DART maintains a pseudo-label predictor that periodically updates its weights based on the current model's performance on a validation proxy. This creates a feedback loop where pseudo-labels become more accurate as training progresses, improving the approximation of ideal joint loss.
- **Core assumption**: The validation proxy (e.g., accuracy on a small labeled target validation set) correlates with the true target accuracy, enabling effective pseudo-label updates.
- **Evidence anchors**:
  - [section 4]: "We maintain a pseudo-label predictor that aims at generating pseudo-labels for the target data... We then use these pseudo-labels to optimize the model... To improve the quality of the pseudo-labels, we periodically evaluate the model's performance (standard accuracy) based on the pre-selected proxy and assign the model weights to the pseudo-label predictor if the model performance has improved."
  - [table 2]: DART with fixed vs. self-updating labels shows performance degradation when labels are fixed, supporting the mechanism.
  - [corpus]: No direct evidence. Related works don't discuss periodic pseudo-label updating for robustness.
- **Break condition**: If the validation proxy is noisy or misaligned with true target accuracy (e.g., due to covariate shift), periodic updates may degrade rather than improve pseudo-labels.

## Foundational Learning

- **Concept**: Domain adaptation theory and H∆H-divergence
  - Why needed here: Understanding how domain divergence affects transfer learning is crucial for grasping why DART jointly minimizes source loss and domain divergence.
  - Quick check question: What does H∆H-divergence measure, and how does it relate to domain adaptation performance?

- **Concept**: Adversarial training and threat models
  - Why needed here: DART extends adversarial training to UDA by generating adversarial examples for both source and target domains within a general threat model B(x).
  - Quick check question: How does projected gradient descent (PGD) generate adversarial examples under the ℓ∞ threat model?

- **Concept**: Pseudo-labeling and self-training
  - Why needed here: DART uses pseudo-labels for target data to approximate the ideal joint loss term, requiring understanding of how pseudo-labels can be generated and refined.
  - Quick check question: What are the risks of using noisy pseudo-labels in semi-supervised learning, and how does DART mitigate them?

## Architecture Onboarding

- **Component map**: Feature extractor g -> Classifier f -> Domain classifier d (for DANN-based UDA) -> Pseudo-label predictor hp for target data
- **Critical path**: Training loop -> Adversarial example generation -> Network weight update -> Pseudo-label update (periodic). Each step depends on the previous; failure in adversarial generation propagates to weight updates.
- **Design tradeoffs**: Using adversarial/KL source examples may improve robustness but can hurt standard accuracy if perturbation sets don't align. Periodic pseudo-label updates add computational overhead but can improve accuracy.
- **Failure signatures**: Low robust accuracy despite high standard accuracy suggests poor domain alignment. Sudden drops in pseudo-label accuracy indicate distribution shift or noisy validation proxy.
- **First 3 experiments**:
  1. Implement DART with clean source examples on DIGIT dataset (SVHN→MNIST) and verify robust accuracy improvement over Natural DANN.
  2. Switch to adversarial source examples and measure impact on both standard and robust accuracy.
  3. Implement periodic pseudo-label updates and compare performance with fixed pseudo-labels on the same dataset pair.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved:

### Open Question 1
- Question: How does DART's performance scale with the size of the target domain dataset? Are there diminishing returns or performance plateaus?
- Basis in paper: [inferred] The paper uses a fixed size for the target domain dataset in its experiments but does not explore how performance changes with dataset size.
- Why unresolved: The experiments conducted use a standard dataset size, and there is no mention of experiments with varying dataset sizes to observe scaling effects.
- What evidence would resolve it: Experiments with varying sizes of the target domain dataset, showing performance metrics (e.g., robust accuracy) as a function of dataset size, would provide insights into scalability and potential diminishing returns.

### Open Question 2
- Question: Can DART be effectively adapted for non-image data, such as text or tabular data, under the unsupervised domain adaptation setting?
- Basis in paper: [inferred] The paper focuses on image classification tasks and does not discuss applicability to other data types.
- Why unresolved: The methodology and experiments are tailored to image data, leaving open questions about its effectiveness on different data modalities.
- What evidence would resolve it: Applying DART to text or tabular data under the UDA setting and evaluating its performance against state-of-the-art methods would demonstrate its versatility and effectiveness across different data types.

### Open Question 3
- Question: What is the impact of using different domain divergence metrics (e.g., Wasserstein Distance, Central Moment Discrepancy) on DART's performance in various scenarios?
- Basis in paper: [explicit] The paper mentions that DART is compatible with various domain divergence metrics but only explores a few (DANN, MMD, CORAL) in experiments.
- Why unresolved: While the paper demonstrates compatibility with multiple metrics, it does not exhaustively test all potential metrics or discuss their impact on performance in detail.
- What evidence would resolve it: Conducting experiments with a broader range of domain divergence metrics and analyzing their impact on DART's performance across different datasets and scenarios would provide a comprehensive understanding of the metric's influence.

## Limitations

- The theoretical generalization bound relies on assumptions about the ideal joint loss that may not hold in practice when pseudo-labels are noisy or when domain shift is extreme
- The effectiveness of source transformations (adversarial/KL) depends heavily on alignment between source and target perturbation sets, which may not hold across diverse domain pairs
- Periodic pseudo-label updates require a validation proxy that may not correlate well with true target performance in challenging UDA scenarios

## Confidence

- **High confidence**: The core claim that DART improves adversarial robustness in UDA by jointly optimizing source loss, domain divergence, and ideal joint loss is well-supported by extensive experiments across multiple datasets
- **Medium confidence**: The effectiveness of adversarial/KL source transformations shows mixed results across different dataset pairs, suggesting context-dependent benefits
- **Medium confidence**: The periodic pseudo-label update mechanism shows clear benefits in the DIGIT experiments but requires more validation on larger-scale datasets

## Next Checks

1. Test DART on challenging domain pairs with significant domain shift (e.g., sketch-to-real in PACS) to assess robustness when pseudo-labels may be less reliable
2. Evaluate the impact of different perturbation sets between source and target domains to understand when source transformations help or hurt
3. Conduct ablation studies on the frequency of pseudo-label updates to find optimal update schedules across different domain adaptation scenarios