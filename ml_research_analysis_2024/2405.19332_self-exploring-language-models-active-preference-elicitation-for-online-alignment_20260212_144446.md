---
ver: rpa2
title: 'Self-Exploring Language Models: Active Preference Elicitation for Online Alignment'
arxiv_id: '2405.19332'
source_url: https://arxiv.org/abs/2405.19332
tags:
- arxiv
- selm
- reward
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve online language model alignment
  by actively exploring out-of-distribution responses. The core idea is to add an
  optimism term to the reward-fitting objective, which biases the model toward generating
  responses with potentially high rewards that are currently unknown.
---

# Self-Exploring Language Models: Active Preference Elicitation for Online Alignment

## Quick Facts
- **arXiv ID:** 2405.19332
- **Source URL:** https://arxiv.org/abs/2405.19332
- **Reference count:** 30
- **Primary result:** SELM significantly boosts performance on instruction-following benchmarks compared to iterative DPO and other online alignment algorithms while maintaining low computational overhead

## Executive Summary
This paper introduces Self-Exploring Language Models (SELM), a novel approach for online alignment of large language models through active preference elicitation. SELM addresses the challenge of exploring out-of-distribution responses during online alignment by adding an optimism term to the reward-fitting objective. This mechanism biases the model toward generating responses with potentially high rewards that are currently unknown, enabling more efficient exploration than standard random sampling. The method achieves superior performance on instruction-following benchmarks while providing theoretical guarantees on sample efficiency through sublinear cumulative regret.

## Method Summary
SELM modifies the online alignment process by incorporating an optimism term into the reward-fitting objective, which encourages exploration of potentially high-reward responses that are currently unknown. The algorithm solves a bilevel optimization problem where the upper level trains a reward model to fit human preferences while the lower level optimizes the language model policy under a KL constraint. Responses are generated using the reference model, ranked by an external ranker (PairRM), and the preference dataset is updated with the best and worst responses. The method is trained iteratively with three phases, using different portions of the dataset in each iteration. SELM's self-exploration mechanism reduces the indiscriminate favoring of unseen extrapolations seen in DPO by selectively preferring responses with high potential rewards rather than blindly sampling unseen responses.

## Key Results
- SELM significantly outperforms iterative DPO on instruction-following benchmarks (AlpacaEval 2.0 win rates of 61.3% vs 52.9% for Zephyr-7B-SFT, 64.3% vs 54.2% for Llama-3-8B-Instruct)
- Theoretical analysis shows SELM achieves sublinear cumulative regret, indicating convergence to optimal policy within sufficient iterations
- Experimental results demonstrate higher fraction of data in high-reward regions compared to baselines
- Computational overhead remains low compared to other online alignment algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding an optimism term to the reward-fitting objective actively explores out-of-distribution regions by encouraging responses with potentially high rewards that are currently unknown.
- Mechanism: The optimism term α max_y r(x, y) biases the reward model toward fitting observed data well while maximizing the highest possible reward across all responses. This dual objective ensures that the greedy response y_u from the RM is either globally optimal or potentially good in unexplored areas where r(x, y_u) can be arbitrarily large due to the relaxed reward-fitting loss. Feedback from humans on these responses reduces uncertainty and trains a more accurate RM.
- Core assumption: The vast response space of natural language requires more than random sampling from standard reward-maximizing LLMs for effective exploration.
- Evidence anchors: Abstract mentions adding optimism term to bias toward unknown high-reward responses; Section 1 states random sampling alone is insufficient.

### Mechanism 2
- Claim: SELM reduces indiscriminate favor of unseen extrapolations in DPO by selectively preferring responses with high potential rewards rather than blindly sampling unseen responses.
- Mechanism: The self-exploration objective minimizes expected implicit reward under KL constraint, decreasing likelihood of generating low implicit reward responses. This process occurs in every iteration with updated reference models, ensuring high potential reward responses are selectively preferred while commonplace responses receive small probability mass. Consequently, the model mitigates indiscriminate favoring of unseen responses and improves exploration efficiency.
- Core assumption: The policy gradient of self-exploration objective is biased toward parameter regions that can elicit responses with high implicit rewards.
- Evidence anchors: Abstract contrasts SELM with DPO's indiscriminate favor of unseen extrapolations; Section 4.1 explains how optimism term biases gradient toward high implicit reward regions.

### Mechanism 3
- Claim: SELM is provably sample-efficient with sublinear cumulative regret indicating convergence to optimal policy within sufficient iterations.
- Mechanism: Establishing sublinear cumulative regret R(T) up to T iterations demonstrates that policy π_t at iteration t converges to optimal π* over algorithm's run. This is achieved through reduction technique connecting SELM's sample complexity to existing RL algorithms, using preference-based version of Generalized Eluder Coefficient (GEC) as complexity measure.
- Core assumption: Policy class Π is sufficiently comprehensive to include optimal policy, and bounded condition on log(π/π_ref) is satisfied.
- Evidence anchors: Abstract mentions significant boosts in instruction-following benchmarks; Section 5.2 explains sublinear cumulative regret indicates convergence to optimal policy.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: RLHF is the standard method for aligning LLMs with human intentions, and understanding its components (reward model, policy optimization) is crucial for grasping how SELM modifies the RLHF process.
  - Quick check question: What are the two main components of RLHF, and how do they interact in the alignment process?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: DPO is a key baseline for comparison in the paper, and understanding its limitations (e.g., indiscriminate favoring of unseen extrapolations) is essential for appreciating the improvements offered by SELM.
  - Quick check question: How does DPO differ from standard RLHF, and what are its main limitations in the context of online alignment?

- **Concept: Preference-based Exploration**
  - Why needed here: Preference-based exploration is the core mechanism of SELM, and understanding how it differs from standard exploration methods (e.g., random sampling) is crucial for grasping the innovation of the paper.
  - Quick check question: How does preference-based exploration in SELM differ from random sampling in standard online RLHF, and why is it more effective?

## Architecture Onboarding

- **Component map:**
  - LLM policy (π_θ) -> Reference model (π_ref) -> Reward model (RM) -> Preference dataset (D) -> External ranker (PairRM)

- **Critical path:**
  1. Initialize reference model π_ref and preference dataset D
  2. For each iteration: sample prompts from dataset, generate responses using π_ref, rank responses, update dataset with best/worst responses
  3. Train LLM π_θ using self-exploration objective with optimism term
  4. Update reference model π_ref with newly trained LLM π_θ
  5. Repeat until convergence or predefined number of iterations

- **Design tradeoffs:**
  - Optimism coefficient (α): Larger α encourages more exploration but may cause instability; smaller α results in behavior closer to standard DPO
  - Batch size: Larger batch size improves stability but requires more computational resources; smaller batch size is more efficient but may lead to noisier gradients
  - Learning rate: Higher learning rate speeds up convergence but can cause instability; lower learning rate is more stable but slower

- **Failure signatures:**
  - Overly optimistic responses: If α is too large, model may generate responses not beneficial for alignment, leading to poor performance on instruction-following benchmarks
  - Premature convergence: If KL constraint not properly balanced, model may deviate too much from reference policy, causing instability in learning process
  - Sample inefficiency: If policy class Π doesn't contain optimal policy or bounded condition is violated, sample efficiency guarantee may not hold

- **First 3 experiments:**
  1. Ablation study on optimism coefficient α: Test different values (0.005, 0.001, 0.0005, 0.0001) and measure impact on model's performance on instruction-following benchmarks
  2. Comparison of reward distributions: Analyze reward distributions of responses generated by SELM vs DPO, focusing on fraction of data in high-reward regions and shift in distributions across iterations
  3. Implicit reward analysis: Calculate difference in implicit rewards between SELM and DPO for chosen and rejected responses, visualize results to validate SELM's active exploration behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the optimism coefficient α for different base model architectures and sizes?
- Basis in paper: The paper mentions that α was searched over 0.005, 0.001, 0.0005, and 0.0001 for Zephyr-based models and selected 0.001, while for Llama3-It-based models 0.0001 was selected. The paper also recommends setting α = 0.005 or 0.001 for training SELM based on other models as it shows minimal sensitivity to variations.
- Why unresolved: The paper only tested a limited range of α values on two specific model architectures. Different model sizes, architectures, and training configurations might require different optimal values of α.
- What evidence would resolve it: A comprehensive ablation study testing a wider range of α values (e.g., 0.00001 to 0.01) across different model sizes (from 1B to 70B parameters) and architectures, while monitoring both performance metrics and training stability.

### Open Question 2
- Question: How does SELM's exploration efficiency compare to uncertainty-based exploration methods that use ensembles of reward models?
- Basis in paper: The paper mentions that Dwaracherla et al. (2024) proposed using ensembles of RMs to measure uncertainty for posterior-sampling active exploration, and contrasts this with SELM's optimism-based approach that doesn't explicitly estimate uncertainty.
- Why unresolved: The paper only provides theoretical analysis and empirical results for SELM but doesn't directly compare its exploration efficiency to ensemble-based methods. Both approaches aim to improve exploration but through fundamentally different mechanisms.
- What evidence would resolve it: Head-to-head experiments comparing SELM with ensemble-based exploration methods (like Dwaracherla et al. 2024) on the same base models and datasets, measuring sample efficiency, final performance, and computational overhead.

### Open Question 3
- Question: What is the theoretical sample complexity of SELM in the token-wise MDP formulation of LLM alignment?
- Basis in paper: The paper provides a theoretical analysis showing SELM achieves sublinear cumulative regret and is sample-efficient in the bandit formulation. It mentions that the analysis can be extended to the token-wise MDP formulation but only provides an informal theorem.
- Why unresolved: The paper only provides an informal theorem for the MDP formulation without rigorous proof. The connection between PGEC and the eluder dimension in preference-based MDPs is mentioned but not fully developed.
- What evidence would resolve it: A complete formal proof of SELM's sample complexity in the token-wise MDP setting, including precise bounds on the PGEC for practical LLM architectures and empirical validation of these theoretical guarantees.

## Limitations

- The improved performance may come primarily from the iterative nature of training rather than active exploration specifically, as no clear ablation study isolates these effects
- Theoretical analysis relies on assumptions about policy class and bounded conditions that may not hold in practice, particularly the assumption that optimal policy is contained within the policy class Π
- Experiments focus on single-turn conversations, limiting generalizability to multi-turn dialogue scenarios where exploration dynamics may differ significantly

## Confidence

- **High confidence:** The core algorithmic contribution and mathematical formulation are sound
- **Medium confidence:** The empirical results showing performance improvements on standard benchmarks
- **Medium confidence:** The claim that SELM reduces indiscriminate favoring of unseen extrapolations compared to DPO

## Next Checks

1. **Ablation Study:** Run experiments isolating the exploration component by comparing SELM with a variant that removes the optimism term but maintains the iterative training structure. This would clarify whether performance gains come from exploration or iteration.

2. **Multi-turn Extension:** Implement and test SELM on multi-turn conversation datasets to verify whether the exploration benefits transfer to more complex interaction scenarios where state tracking becomes important.

3. **Reward Distribution Analysis:** Conduct a detailed analysis of how reward distributions evolve across iterations for both SELM and baseline methods, specifically measuring the fraction of responses in high-reward regions and the KL divergence from the reference policy.