---
ver: rpa2
title: Modular Learning of Deep Causal Generative Models for High-dimensional Causal
  Inference
arxiv_id: '2401.01426'
source_url: https://arxiv.org/abs/2401.01426
tags:
- causal
- training
- graph
- distribution
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modular-DCM is the first modular training method for deep causal
  generative models with latent confounders that can leverage pre-trained image generators.
  It uses H-graph construction to identify c-components that can be trained separately,
  then trains them sequentially while freezing weights of already-trained sub-networks.
---

# Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference

## Quick Facts
- arXiv ID: 2401.01426
- Source URL: https://arxiv.org/abs/2401.01426
- Reference count: 40
- Primary result: First modular training method for deep causal generative models with latent confounders that can leverage pre-trained image generators

## Executive Summary
Modular-DCM introduces a novel approach for learning deep causal generative models with latent confounders by training c-components separately in a valid topological order. The method constructs an H-graph from the causal structure to identify which parts can be trained independently, enabling sequential training while freezing weights of already-trained sub-networks. This modular approach allows integration of pre-trained generative models and achieves faster convergence and better sample quality compared to joint training baselines on Colored-MNIST, COVIDx, and CelebA-HQ datasets.

## Method Summary
The method takes a known causal graph with latent confounders and observational data as input. It constructs an H-graph to determine the training order of c-components, then trains these components sequentially using adversarial training while freezing previously trained weights. The approach can integrate pre-trained generative models by treating them as frozen sub-networks, enabling the use of large pre-trained models for causal inference tasks.

## Key Results
- Modular-DCM outperforms joint training baselines in convergence speed and sample quality
- Achieves low total variation distance (TVD) and Frechet Inception Distance (FID) scores
- Enables domain-invariant classification by generating interventional datasets free of spurious correlations

## Why This Works (Mechanism)

### Mechanism 1
Modular training reduces complexity by training c-components separately in valid topological order. The H-graph construction identifies independent c-components, allowing sequential training while freezing weights of already-trained sub-networks. This simplifies the joint distribution matching problem.

### Mechanism 2
Pre-trained generative models can be integrated by freezing their weights during subsequent training steps. Once a c-component is trained or a pre-trained model is plugged in, its weights are frozen, allowing integration of large pre-trained models without affecting their learned representations.

### Mechanism 3
Sequential training achieves faster convergence and better sample quality by reducing loss function complexity at each phase. Training smaller c-components sequentially rather than the entire model jointly leads to more stable training and better convergence properties.

## Foundational Learning

- **Concept:** Structural Causal Models (SCMs) and causal graphs
  - Why needed here: Understanding SCMs is essential to grasp how interventions work and why certain variables can be trained separately
  - Quick check question: What is the difference between a causal effect P(Y|do(X)) and a conditional probability P(Y|X)?

- **Concept:** c-components and backdoor/front-door criteria
  - Why needed here: Identification of separable parts relies on understanding c-components and when interventions are identifiable
  - Quick check question: In a causal graph, when does the front-door criterion apply for identifying P(Y|do(X))?

- **Concept:** Generative adversarial networks (GANs) and adversarial training
  - Why needed here: Modular training uses adversarial training to match distributions
  - Quick check question: In a GAN, what is the role of the discriminator during training?

## Architecture Onboarding

- **Component map:** H-graph construction -> Module training loop -> Pre-trained model integration -> Distribution matching -> Sampling

- **Critical path:**
  1. Input causal graph and observational data
  2. Construct H-graph to determine training order
  3. For each h-node in topological order: identify modularity condition and ancestor set A, train mechanisms while freezing previous weights
  4. After convergence, sample from identifiable interventional distributions

- **Design tradeoffs:**
  - Flexibility vs. complexity: Modular training allows pre-trained model integration but requires complex H-graph construction
  - Convergence speed vs. sample quality: Sequential training may converge faster but could accumulate errors across modules
  - Scalability vs. accuracy: Larger c-components may capture more dependencies but are harder to train jointly

- **Failure signatures:**
  - Poor FID/TVD scores: Indicates failure to match the joint distribution
  - Mode collapse in generated samples: Suggests issues with adversarial training in one or more modules
  - Inconsistent interventional samples: Points to errors in modular training order or modularity condition identification

- **First 3 experiments:**
  1. Front-door graph with MNIST mediator: Test modular training on simple causal graph with high-dimensional image variables
  2. Diamond graph with two image variables: Test modular training on graph with multiple image nodes
  3. COVIDx CXR-3 dataset: Test modular training on real-world dataset with high-dimensional images

## Open Questions the Paper Calls Out

### Open Question 1
How can modular training be extended to non-Markovian causal models where confounders can affect any number of observed variables? The paper focuses on semi-Markovian models and extending to non-Markovian models would require handling confounders affecting multiple variables.

### Open Question 2
How can performance be improved when dealing with high-dimensional image data, particularly in terms of image quality and consistency with interventional distributions? The current approach maps high-dimensional images to low-dimensional representations, potentially leading to information loss.

### Open Question 3
How can Modular-DCM be applied to real-world datasets where the causal graph is not fully known or may contain errors? The method assumes a fully specified causal graph with latents as prior.

## Limitations
- Relies heavily on known causal graph structure and semi-Markovian assumption
- Integration of pre-trained models requires causal consistency with observational distribution
- Experimental validation limited to specific datasets (Colored-MNIST, COVIDx, CelebA-HQ)

## Confidence

- **High Confidence:** Theoretical soundness of modular training approach given known causal structure and semi-Markovian assumption
- **Medium Confidence:** Experimental results showing improved convergence speed and sample quality over joint training baselines
- **Low Confidence:** Practical feasibility of integrating large pre-trained models while maintaining causal consistency

## Next Checks

1. **Robustness to causal structure misspecification:** Systematically test how performance degrades when the assumed causal graph differs from the true generating process

2. **Scalability with increasing dimensionality:** Evaluate the method on datasets with progressively higher-dimensional image variables to identify bottlenecks

3. **Generalization across causal inference tasks:** Test the method on causal effect estimation tasks beyond front-door/backdoor scenarios, including mediation analysis and counterfactual inference