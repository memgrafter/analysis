---
ver: rpa2
title: Deep Class-guided Hashing for Multi-label Cross-modal Retrieval
arxiv_id: '2410.15387'
source_url: https://arxiv.org/abs/2410.15387
tags:
- bits
- loss
- dcgh
- hash
- hashing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DCGH, a deep hashing method for multi-label
  cross-modal retrieval that addresses two key challenges: intra-class aggregation
  and inter-class structural relationships. The method combines proxy loss, pairwise
  loss, and a variance constraint to generate high-quality hash codes that maintain
  both semantic similarity within classes and distinguish between different classes.'
---

# Deep Class-guided Hashing for Multi-label Cross-modal Retrieval

## Quick Facts
- arXiv ID: 2410.15387
- Source URL: https://arxiv.org/abs/2410.15387
- Authors: Hao Chen; Lei Zhu; Xinghui Zhu
- Reference count: 40
- Primary result: Introduces DCGH, a deep hashing method for multi-label cross-modal retrieval that achieves state-of-the-art performance on three benchmark datasets

## Executive Summary
This paper introduces DCGH, a deep hashing method for multi-label cross-modal retrieval that addresses two key challenges: intra-class aggregation and inter-class structural relationships. The method combines proxy loss, pairwise loss, and a variance constraint to generate high-quality hash codes that maintain both semantic similarity within classes and distinguish between different classes. Specifically, proxy loss ensures intra-class aggregation, pairwise loss maintains inter-class relationships, and the variance constraint prevents semantic bias. Experiments on three benchmark datasets (MIRFLICKR-25K, NUS-WIDE, and MS COCO) demonstrate that DCGH outperforms state-of-the-art methods, achieving mAP scores of up to 0.8633 (image-to-text) and 0.8413 (text-to-image) on MIRFLICKR-25K with 64-bit hash codes. The method also shows strong performance in NDCG@1000 and Precision-Recall curves.

## Method Summary
DCGH is a deep hashing framework for multi-label cross-modal retrieval that combines three key components: proxy loss for intra-class aggregation, pairwise loss for inter-class structural relationships, and a variance constraint to prevent semantic bias. The method uses Transformer-based feature extractors (ViT for images, GPT-2 for text) to learn semantic representations, which are then projected to hash codes through fully connected layers. The three losses are optimized jointly to generate hash codes that maintain semantic similarity within classes while distinguishing between different classes. The framework is evaluated on three benchmark datasets (MIRFLICKR-25K, NUS-WIDE, and MS COCO) and shows superior performance compared to state-of-the-art methods across multiple evaluation metrics.

## Key Results
- Achieves mAP scores up to 0.8633 (image-to-text) and 0.8413 (text-to-image) on MIRFLICKR-25K with 64-bit hash codes
- Demonstrates strong performance in NDCG@1000 and Precision-Recall curves across all three benchmark datasets
- Outperforms state-of-the-art methods in multi-label cross-modal retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DCGH's proxy loss component effectively maintains intra-class aggregation by assigning each label category a learnable proxy vector, pulling samples closer to their relevant proxies and pushing them away from irrelevant ones.
- Mechanism: For each sample, the cosine distance is computed with all proxies. Relevant proxies are minimized (cos+ loss) while irrelevant proxies are maximized (cos- loss). This ensures all samples with shared labels cluster around their proxies in the Hamming space.
- Core assumption: The proxy vectors can adequately represent the semantic center of each label category, and samples with shared labels can be pulled toward a consistent proxy.
- Evidence anchors:
  - [abstract] "proxy loss ensures intra-class aggregation"
  - [section] "proxy loss generates a proxy vector for each single-label category, learning the hash representation by reducing the distance between samples and their corresponding proxies and increasing the distance between samples and proxies of incorrect categories"
  - [corpus] Weak corpus evidence for proxy-based loss in multi-label settings. Most cited methods focus on pairwise/triplet loss or data-aware proxies but not explicit variance-constrained proxy loss.
- Break condition: If proxy vectors fail to represent true semantic centers due to label noise or imbalance, intra-class dispersion may re-emerge.

### Mechanism 2
- Claim: Pairwise loss preserves inter-class structural relationships by explicitly optimizing relative distances between all sample pairs, with asymmetric weighting to prevent intra-class dispersion.
- Mechanism: Relevant pairs (cospos) are pulled closer, irrelevant pairs (cosneg) are pushed apart. Small weights (α, β) are applied to avoid overwhelming the proxy loss, thus maintaining inter-class separation without causing intra-class dispersion.
- Core assumption: The similarity matrix S derived from multi-label annotations correctly encodes pairwise relevance, and the asymmetric weighting (α < β) prevents over-constraining positive pairs.
- Evidence anchors:
  - [abstract] "pairwise loss maintains inter-class structural relationships"
  - [section] "we assign different small weights to the positive and negative sample constraints of the pairwise loss"
  - [corpus] Some methods use pairwise loss but often without variance constraints or asymmetric weighting; the combination here is novel.
- Break condition: If the similarity matrix is noisy or the weighting is poorly tuned, either intra-class dispersion or inter-class collapse may occur.

### Mechanism 3
- Claim: Variance constraint prevents semantic bias by enforcing consistent distances between each sample and all its relevant proxies.
- Mechanism: For each sample, the variance of its distances to all relevant proxies is minimized. This ensures that no single proxy dominates the representation, maintaining balanced semantic relationships.
- Core assumption: Semantic bias arises when a sample has multiple labels but distances to their proxies are inconsistent, leading to clustering toward one proxy.
- Evidence anchors:
  - [abstract] "variance constraint prevents semantic bias"
  - [section] "variance constraint to ensure that the distances between each data point and its corresponding proxy are as consistent as possible"
  - [corpus] Limited direct evidence in corpus; variance constraints are not commonly discussed in cross-modal hashing literature.
- Break condition: If the variance constraint is too strong, it may over-regularize and prevent meaningful differentiation between classes.

## Foundational Learning

- Concept: Multi-label classification and similarity encoding
  - Why needed here: DCGH operates in a multi-label setting where each sample can belong to multiple categories. Understanding how to encode pairwise similarity from multi-label vectors is critical.
  - Quick check question: Given two samples with label vectors [1,0,1] and [0,1,1], what is their similarity score under the dot-product-based scheme used in DCGH?

- Concept: Proxy-based loss in deep learning
  - Why needed here: DCGH uses proxy loss instead of center loss or pairwise loss alone. Understanding how proxy vectors act as learnable class representatives is key to grasping the method.
  - Quick check question: In proxy loss, what happens to the gradient when a sample is close to its relevant proxy versus an irrelevant proxy?

- Concept: Variance as a regularization term
  - Why needed here: The variance constraint is a novel addition to prevent semantic bias. Knowing how variance regularization works in loss functions is important.
  - Quick check question: If a sample has distances [0.2, 0.3, 0.4] to three relevant proxies, what is the variance of these distances?

## Architecture Onboarding

- Component map: Images -> ViT Transformer -> Semantic Features -> Hash Codes; Text -> GPT-2 Transformer -> Semantic Features -> Hash Codes; Hash Codes -> Proxy Loss + Pairwise Loss + Variance Constraint -> Final Binary Codes
- Critical path: 1. Forward pass through Transformers → semantic features; 2. Project features to hash codes via FC layers; 3. Compute all three losses jointly; 4. Backpropagate to update Transformers, hash layers, and proxies
- Design tradeoffs:
  - Proxy vs. Center Loss: Proxy loss scales better with many classes; center loss requires maintaining explicit centers
  - Variance Constraint: Adds regularization but increases computation; may be tuned or omitted for speed
  - Asymmetric Pairwise Weights: Prevents intra-class dispersion but requires careful tuning
- Failure signatures:
  - High intra-class dispersion: Proxy loss too weak or variance constraint too strong
  - Poor inter-class separation: Pairwise loss too weak or mis-weighted
  - Semantic bias: Variance constraint missing or improperly scaled
- First 3 experiments:
  1. Ablation: Train with only proxy loss vs. only pairwise loss vs. full DCGH to confirm each component's contribution
  2. Sensitivity: Sweep α and β over {0.001, 0.01, 0.05, 0.1, 0.5, 0.8, 1} to find optimal weighting
  3. Embedding visualization: Use t-SNE on hash codes to inspect intra-class clustering and inter-class separation across modalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DCGH perform when extended to more than two modalities (e.g., image-text-video retrieval)?
- Basis in paper: [explicit] The authors mention future work includes extending DCGH to image-text-video multi-modal retrieval.
- Why unresolved: The current DCGH framework is designed and evaluated only for image-text retrieval tasks. Extending it to handle additional modalities requires architectural modifications and validation.
- What evidence would resolve it: Experimental results comparing DCGH's performance on multi-modal datasets (e.g., AV-MNIST, NUS-WIDE with video) against existing multi-modal hashing methods.

### Open Question 2
- Question: What is the impact of the variance constraint on semantic bias across different dataset sizes and class distributions?
- Basis in paper: [explicit] The authors introduce a variance constraint to prevent semantic bias when combining proxy and pairwise losses.
- Why unresolved: While the authors demonstrate effectiveness on three datasets, the sensitivity of the variance constraint to varying dataset sizes and class imbalances is not explored.
- What evidence would resolve it: Ablation studies and performance comparisons of DCGH with and without variance constraint across datasets with varying class distributions and sizes.

### Open Question 3
- Question: How does DCGH's performance scale with increasing hash code lengths beyond 64 bits?
- Basis in paper: [explicit] The authors evaluate DCGH on 16, 32, and 64-bit hash codes but do not explore longer codes.
- Why unresolved: The paper does not investigate the performance of DCGH with hash code lengths greater than 64 bits, leaving uncertainty about its scalability.
- What evidence would resolve it: Experimental results showing mAP, NDCG@1000, and other metrics for DCGH with hash code lengths of 128, 256, and 512 bits on the benchmark datasets.

## Limitations
- Performance heavily depends on careful hyperparameter tuning, particularly the asymmetric weights α and β in the pairwise loss
- The variance constraint, while theoretically sound, lacks extensive empirical validation in the cross-modal hashing literature
- The complexity of the three-loss framework may limit scalability to very large datasets or real-time applications

## Confidence
- High: DCGH outperforms state-of-the-art methods on benchmark datasets
- Medium: The three-loss framework effectively addresses intra-class and inter-class challenges
- Medium: Transformer-based feature extraction is superior to CNN-based methods for this task

## Next Checks
1. Conduct extensive hyperparameter sensitivity analysis for α and β across a wider range of values
2. Perform ablation studies isolating the variance constraint's impact on semantic bias
3. Evaluate DCGH's scalability and performance on datasets significantly larger than the benchmarks used