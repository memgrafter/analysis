---
ver: rpa2
title: From melodic note sequences to pitches using word2vec
arxiv_id: '2410.22285'
source_url: https://arxiv.org/abs/2410.22285
tags:
- notes
- context
- size
- different
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the relationship between the sequential
  order of musical notes and their pitches using a simplified neural network model.
  The approach involves treating melodies as sequences of words, where each note is
  encoded as a vector, and using a continuous bag-of-words (CBOW) model to predict
  upcoming notes based on preceding ones.
---

# From melodic note sequences to pitches using word2vec

## Quick Facts
- arXiv ID: 2410.22285
- Source URL: https://arxiv.org/abs/2410.22285
- Authors: Daniel Defays
- Reference count: 0
- Key outcome: CBOW embeddings from melodic sequences achieve ~0.80 multiple correlation with note pitches

## Executive Summary
This study investigates whether the sequential order of musical notes contains sufficient information to approximate their pitches using a simplified neural network model. The approach treats melodies as sequences of words, where each note is encoded as a vector, and uses a continuous bag-of-words (CBOW) model to predict upcoming notes based on preceding ones. The embeddings generated are of very low dimension (2D), allowing direct visualization and interpretation. Results show that the embeddings have a multiple correlation coefficient of approximately 0.80 with the pitches of the notes, indicating a strong link between the order of notes and their pitch information.

## Method Summary
The method involves encoding musical note sequences as multi-hot vectors and training a simple CBOW neural network with a 2-node hidden layer to predict the next note from its context. The network is trained on two datasets: 20 French children's songs and the first 42 measures of Bach's Sonata for Violin Solo in C Major. After training, the 2D embeddings are extracted from the hidden layer and regressed against actual pitch values to compute the multiple correlation coefficient. The approach uses 40 epochs of training with a learning rate of 0.01 and a 90/10 training/validation split.

## Key Results
- 2D CBOW embeddings achieve multiple correlation coefficient of approximately 0.80 with note pitches
- The relationship between sequential order and pitch information holds across different datasets (children's songs and Bach sonata)
- Results remain robust even when note order is scrambled, provided the note distribution remains consistent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sequential structure of melodies contains sufficient information to approximate pitch through CBOW embeddings.
- Mechanism: CBOW predicts a target note from its preceding context. Even though pitch values are never input directly, the embedding vectors learn to encode pitch-related patterns because sequences with similar intervals tend to have predictable pitch progressions.
- Core assumption: Pitch is implicitly recoverable from the sequential order of notes, not requiring explicit pitch coding in the input.
- Evidence anchors:
  - [abstract] "findings suggest that the sequential structure of melodies contains sufficient information to approximate pitch"
  - [section] "A multivariate analysis of the results shows that the semantic vectors representing the notes have a multiple correlation coefficient of approximately 0.80 with their pitches"
  - [corpus] Weak evidence. Related works use interval-based or context-aware models but do not establish a causal mechanism linking sequence order to pitch via CBOW.
- Break condition: If the melody contains large, irregular interval jumps that do not follow tonal patterns, the embedding will fail to approximate pitch.

### Mechanism 2
- Claim: Multi-hot encoding of contexts preserves pitch information better than one-hot encoding of single notes.
- Mechanism: Multi-hot encoding treats a context as a set (chord-like), allowing the model to learn from combinations of notes rather than individual note order, capturing harmonic and tonal proximity that correlates with pitch.
- Core assumption: The set of notes in a context is more informative for pitch approximation than their sequential order within the context.
- Evidence anchors:
  - [section] "multi-hot representation of the context emphasizes chords, as demonstrated in the paper by Madjiheurem, Qu, and Walder (2016)"
  - [section] "There are two main differences between the two types of encoding... multi-hot encoding does not differentiate the order of the notes within the context"
  - [corpus] Weak evidence. No corpus neighbor directly addresses multi-hot vs one-hot encoding for pitch prediction.
- Break condition: If context order matters (e.g., for specific melodic contours), multi-hot encoding will lose critical information.

### Mechanism 3
- Claim: Low-dimensional embeddings (2D) allow direct visualization and strong correlation with pitch.
- Mechanism: Reducing embedding dimension to 2 forces the network to compress pitch-related information into a space where geometric relationships (distances, angles) reflect semantic (pitch) relationships.
- Core assumption: A 2D space is sufficient to capture the main variance in pitch information without excessive loss.
- Evidence anchors:
  - [abstract] "The embeddings generated are of very low dimension (2D), allowing direct visualization and interpretation"
  - [section] "To graphically illustrate this result, a 3D plot of the regression plane... is provided below"
  - [corpus] Weak evidence. No corpus neighbor reports using 2D embeddings for symbolic music pitch prediction.
- Break condition: If pitch patterns require more than 2 degrees of freedom (e.g., octave displacement plus intervallic structure), the correlation will drop.

## Foundational Learning

- Concept: One-hot vs multi-hot encoding
  - Why needed here: Determines how note contexts are represented; multi-hot allows the model to treat contexts as sets, which affects how pitch information is learned.
  - Quick check question: What is the difference between a one-hot vector and a multi-hot vector for encoding a pair of notes?
- Concept: CBOW (Continuous Bag of Words) model
  - Why needed here: CBOW predicts a target note from its context, forming the basis of the embedding learning process used in this study.
  - Quick check question: In CBOW, how is the target note predicted from the context?
- Concept: Multiple correlation coefficient
  - Why needed here: Measures how well the 2D embeddings approximate pitch values; a high value indicates successful encoding of pitch information.
  - Quick check question: What does a multiple correlation coefficient of 0.80 imply about the relationship between embeddings and pitches?

## Architecture Onboarding

- Component map:
  - Note sequences -> Multi-hot encoding -> CBOW network (2-node hidden layer) -> Softmax output -> Cross-entropy loss
- Critical path:
  1. Encode note sequences as multi-hot vectors
  2. Feed into CBOW network
  3. Train to minimize cross-entropy loss
  4. Extract 2D embeddings from hidden layer
  5. Regress pitch values onto embeddings to compute multiple correlation
- Design tradeoffs:
  - Low embedding dimension (2D) → interpretable but may lose nuance
  - Multi-hot encoding → captures chord-like contexts but ignores order within context
  - CBOW vs Skip-gram → CBOW focuses on local context, simpler, but may miss long-range dependencies
- Failure signatures:
  - Low multiple correlation (<0.5) → embeddings do not capture pitch
  - Overfitting → high training accuracy but poor validation accuracy
  - Unstable embeddings → high variance across runs
- First 3 experiments:
  1. Train on children's songs with context size 2, compute multiple correlation
  2. Randomize note order within each song, retrain, compare correlation
  3. Increase embedding dimension to 3, compare correlation and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the predictive power of pitch information in melodies change when incorporating rhythm and dynamics into the embedding model?
- Basis in paper: [inferred] The paper mentions the potential to integrate rhythm, dynamics, and harmonic context in future work, as demonstrated by Liang et al. (2020), suggesting an unexplored area of research.
- Why unresolved: The current study focuses solely on pitch and note sequences, leaving the impact of rhythm and dynamics on predictive accuracy unexamined.
- What evidence would resolve it: Conduct experiments using the same embedding approach but include rhythm and dynamics as additional features, comparing the predictive power of pitch information with and without these elements.

### Open Question 2
- Question: What is the effect of using different encoding schemes, such as ordinal encoding, on the relationship between note sequences and pitch information?
- Basis in paper: [explicit] The paper contrasts the use of one-hot and multi-hot encoding with ordinal encoding, which considers the order relationship between pitches, suggesting a potential area for further exploration.
- Why unresolved: The study primarily uses one-hot and multi-hot encoding, leaving the impact of ordinal encoding on capturing pitch information unexplored.
- What evidence would resolve it: Perform the same embedding analysis using ordinal encoding and compare the resulting correlations between embeddings and pitches to those obtained with one-hot and multi-hot encoding.

### Open Question 3
- Question: How does the size of the vocabulary (number of unique notes) affect the multiple correlation between embeddings and pitches in more complex musical pieces?
- Basis in paper: [inferred] The study uses datasets with vocabularies of 16 and 25 notes, indicating that vocabulary size may influence the results, but this aspect is not deeply explored.
- Why unresolved: The paper does not systematically vary the vocabulary size across different datasets to assess its impact on the correlation between embeddings and pitches.
- What evidence would resolve it: Conduct experiments with datasets of varying vocabulary sizes, analyzing how the multiple correlation changes as the number of unique notes increases.

## Limitations
- The causal mechanism linking sequential order to pitch information remains unproven
- Multi-hot encoding superiority over one-hot encoding for pitch approximation lacks direct empirical evidence
- 2D embedding dimension may be suboptimal for capturing complex pitch structures

## Confidence
- Sequential order contains sufficient information for pitch approximation: Low confidence
- CBOW with multi-hot encoding is optimal for this task: Medium confidence
- 2D embeddings provide meaningful pitch representation: Medium confidence

## Next Checks
1. **Control encoding test**: Repeat the experiment using non-pitch-based encodings (e.g., instrument timbre descriptors or random categorical labels) to verify that pitch information is actually being learned from sequence structure rather than being encoded in the input representation.

2. **Dimensionality sensitivity analysis**: Systematically vary embedding dimensions (1D, 2D, 3D, 4D) and measure how the multiple correlation coefficient changes. This would determine whether 2D is optimal or simply sufficient for the task.

3. **Order preservation test**: Compare the multi-hot approach against a one-hot sequential model where context order is preserved. This would directly test whether treating contexts as sets (multi-hot) is superior to preserving melodic contour information (one-hot with order).