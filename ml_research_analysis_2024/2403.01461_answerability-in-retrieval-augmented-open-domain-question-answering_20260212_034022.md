---
ver: rpa2
title: Answerability in Retrieval-Augmented Open-Domain Question Answering
arxiv_id: '2403.01461'
source_url: https://arxiv.org/abs/2403.01461
tags:
- answer
- excerpts
- text
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the ability of models to abstain from answering
  when provided with semantically related but practically irrelevant text excerpts
  in open-domain question answering. A random pairing strategy for unanswerable questions
  shows poor generalization, with accuracy dropping from 98% to 1% when faced with
  semantically related excerpts.
---

# Answerability in Retrieval-Augmented Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2403.01461
- Source URL: https://arxiv.org/abs/2403.01461
- Reference count: 2
- Models struggle to identify unanswerable questions when provided with semantically related but irrelevant text excerpts

## Executive Summary
This study investigates how well retrieval-augmented open-domain question answering models can recognize when they should abstain from answering questions. The researchers found that when unanswerable questions are paired with semantically related but irrelevant text excerpts, random pairing strategies fail dramatically, with accuracy dropping from 98% to just 1%. However, incorporating unanswerable questions from SQuAD 2.0 into training significantly improves model performance, achieving nearly 100% accuracy in identifying unanswerable questions.

## Method Summary
The study examines answerability detection in retrieval-augmented open-domain question answering systems. Researchers tested models using unanswerable questions paired with semantically related but irrelevant text excerpts. They compared random pairing strategies against approaches that incorporated unanswerable questions from SQuAD 2.0 during training. The experimental setup measured how well models could distinguish between answerable and unanswerable questions when relevant information was present but did not contain the answer.

## Key Results
- Random pairing strategy for unanswerable questions achieves 98% accuracy initially but drops to 1% when faced with semantically related excerpts
- Incorporating SQuAD 2.0 unanswerable questions into training achieves nearly 100% accuracy
- Models exhibit confirmation bias, extracting the correct factual answer 28 times more often than other entities when the answer is not present

## Why This Works (Mechanism)
The mechanism behind these results relates to how models learn to identify answerability through exposure to unanswerable examples during training. When models are trained only on answerable questions or use random pairing for unanswerable ones, they lack the discriminative capability to recognize semantic similarity without relevance. The confirmation bias finding suggests models have learned to prioritize certain entities over others based on their training data, leading them to extract plausible-sounding but incorrect answers even when the true answer is absent.

## Foundational Learning

**Semantic similarity vs. relevance**: Understanding that semantically related text may still be irrelevant to answering a specific question is crucial for answerability detection. Quick check: Can you identify pairs of semantically similar but factually unrelated text passages?

**Answerability classification**: Models must learn to classify whether a question can be answered given a context, not just extract answers. Quick check: Given a question and context pair, can you determine if an answer exists?

**Confirmation bias in language models**: Models tend to favor extracting answers that seem plausible based on learned patterns rather than recognizing true absence of information. Quick check: Can you identify when a model might be extracting a plausible but incorrect answer?

## Architecture Onboarding

**Component map**: Question -> Retriever -> Context Selector -> Answer Extractor -> Answerability Classifier

**Critical path**: The answerability classifier represents the critical path, as it determines whether the system should provide an answer or abstain entirely.

**Design tradeoffs**: The main tradeoff involves balancing recall (retrieving potentially relevant documents) against precision (ensuring retrieved documents contain actual answers), while also maintaining the ability to recognize when no answer exists.

**Failure signatures**: Dramatic performance drops (98% to 1%) when semantically related but irrelevant contexts are introduced indicate insufficient training on unanswerable examples.

**First experiments**:
1. Test random pairing strategy with answerable questions to establish baseline performance
2. Evaluate model performance on SQuAD 2.0 unanswerable questions with relevant contexts
3. Measure confirmation bias by analyzing which entities models extract most frequently from unanswerable contexts

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on a single model architecture (DPR-QA), limiting generalizability to other retrieval-augmented systems
- Definition of "semantically related" relies on retrieval-based similarity metrics that may not capture all relevant forms of relatedness
- Extreme performance degradation (98% to 1%) may indicate an overly challenging experimental setup not representative of real-world scenarios

## Confidence
- Model performance degradation with random pairing: High
- Training with SQuAD 2.0 improves answerability detection: High
- Confirmation bias in answer extraction: Medium (requires contextual interpretation)

## Next Checks
1. Test the same experimental protocol across multiple retrieval-augmented architectures (e.g., RAG, Fusion-in-Decoder) to assess generalizability
2. Evaluate model performance on real-world unanswerable scenarios where semantically related but irrelevant text appears organically in retrieved contexts
3. Analyze whether the confirmation bias effect persists when models are fine-tuned on diverse, carefully curated training sets that explicitly address this bias