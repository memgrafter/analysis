---
ver: rpa2
title: 'Amphista: Bi-directional Multi-head Decoding for Accelerating LLM Inference'
arxiv_id: '2406.13170'
source_url: https://arxiv.org/abs/2406.13170
tags:
- amphista
- target
- decoding
- heads
- medusa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Amphista accelerates large language model inference by introducing
  a bi-directional multi-head decoding framework that improves upon Medusa's parallel
  drafting. It employs an Auto-embedding Block with bi-directional self-attention
  to enable collaborative information exchange among drafting heads, and Staged Adaptation
  Layers to bridge the semantic gap between autoregressive target models and non-autoregressive
  draft heads.
---

# Amphista: Bi-directional Multi-head Decoding for Accelerating LLM Inference

## Quick Facts
- **arXiv ID**: 2406.13170
- **Source URL**: https://arxiv.org/abs/2406.13170
- **Reference count**: 39
- **Primary result**: Achieves up to 2.75× speedup over vanilla autoregressive decoding and 1.40× over Medusa

## Executive Summary
Amphista introduces a bi-directional multi-head decoding framework that accelerates large language model inference by enabling parallel drafting of multiple tokens per step while maintaining generation quality. Building on Medusa's parallel drafting approach, Amphista adds an Auto-embedding Block with bi-directional self-attention that allows drafting heads to collaboratively exchange information, and Staged Adaptation Layers that bridge the semantic gap between autoregressive target models and non-autoregressive draft heads. Experimental results demonstrate significant speedups across Vicuna 7B/13B/33B models while preserving quality across translation, summarization, QA, math reasoning, and code generation tasks.

## Method Summary
Amphista accelerates LLM inference through a bi-directional multi-head decoding approach that enables multiple tokens to be drafted in parallel per decoding step. The method employs an Auto-embedding Block with bi-directional self-attention to facilitate collaborative information exchange among drafting heads, allowing earlier heads to attend to subsequent ones and vice versa. Staged Adaptation Layers are introduced to bridge the semantic gap between the autoregressive target model and non-autoregressive draft heads. The system is trained using a combined Cross-Entropy alignment loss and language modeling loss on Vicuna models, with evaluation showing substantial speed improvements while maintaining generation quality across multiple benchmarks.

## Key Results
- Achieves up to 2.75× speedup over vanilla autoregressive decoding on Vicuna models
- Outperforms Medusa by up to 1.40× in inference speed
- Maintains generation quality with competitive ROUGE scores in summarization tasks
- Demonstrates effectiveness across multiple task domains including translation, QA, math reasoning, and code generation

## Why This Works (Mechanism)
Amphista achieves superior inference acceleration through non-autoregressive drafting combined with bi-directional attention among heads. The Auto-embedding Block employs bi-directional self-attention to enable earlier heads to attend to subsequent ones and vice versa, allowing information flow in both directions. This creates collaborative predictions rather than independent ones. The Staged Adaptation Layers help align the semantic space of the non-autoregressive draft heads with the autoregressive target model, ensuring generated tokens are compatible with the target model's distribution.

## Foundational Learning

### Bi-directional Attention
**Why needed**: Enables collaborative information exchange among drafting heads, where earlier heads can attend to subsequent heads and vice versa, improving prediction accuracy.
**Quick check**: Verify that attention scores flow in both directions between drafting heads during implementation.

### Non-autoregressive Decoding
**Why needed**: Allows multiple tokens to be predicted in parallel rather than sequentially, enabling significant speedup over traditional autoregressive decoding.
**Quick check**: Confirm that draft heads can generate tokens independently without waiting for previous token predictions.

### Token Acceptance Strategy
**Why needed**: Determines which drafted tokens to accept or reject based on compatibility with the target model, balancing speed and quality.
**Quick check**: Monitor acceptance rates during inference to ensure they're neither too low (poor quality) nor too high (missed speedup opportunities).

## Architecture Onboarding

### Component Map
Input -> Auto-embedding Block (bi-directional self-attention) -> Staged Adaptation Layers -> Draft Heads -> Token Acceptance -> Output

### Critical Path
The Auto-embedding Block with bi-directional self-attention is the critical component that enables collaborative drafting. This block must efficiently handle multi-head attention in both directions while maintaining computational efficiency.

### Design Tradeoffs
- **Parallel vs Sequential**: More parallel drafting heads increase speed but may reduce acceptance rates and quality
- **Attention Complexity**: Bi-directional attention improves collaboration but adds computational overhead
- **Layer Depth**: Deeper adaptation layers improve alignment but increase latency

### Failure Signatures
- Low token acceptance rates indicate poor alignment between draft heads and target model
- Quality degradation suggests insufficient adaptation or overly aggressive parallelization
- Inconsistent speedups across different model sizes may indicate suboptimal architecture scaling

### 3 First Experiments
1. **Ablation Study**: Remove Auto-embedding Block and measure impact on acceptance rate and speed to quantify its contribution
2. **Layer Depth Analysis**: Vary the number of Staged Adaptation Layers to find optimal configuration for different model sizes
3. **Tree Width Scaling**: Test different numbers of draft heads to identify the sweet spot between speed and quality

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: What is the optimal number of self-attention layers in the Auto-embedding Block for different model sizes and tasks?
**Basis in paper**: The paper mentions that increasing the number of self-attention layers in the auto-embedding module (Amphista-α) led to improved performance, but only preliminary exploration was conducted.
**Why unresolved**: The paper only briefly mentions experimenting with two layers and suggests further optimization could improve performance. No systematic study of different layer configurations was presented.
**What evidence would resolve it**: A comprehensive ablation study varying the number of self-attention layers (e.g., 1, 2, 3, 4) across different model sizes (7B, 13B, 33B) and task types (translation, summarization, QA, math, code) would establish optimal configurations.

### Open Question 2
**Question**: How does Amphista's performance scale with larger draft tree configurations and what is the relationship between tree width and speed-up?
**Basis in paper**: The paper mentions that "an excessive number of tree nodes may lead to significant computation overhead" and shows results with varying numbers of nodes, but doesn't fully explore the scaling relationship.
**Why unresolved**: The paper only tests a limited range of tree configurations (22, 35, 45, 64 nodes) and doesn't provide a clear model for how performance scales with larger trees or what the optimal trade-off point is.
**What evidence would resolve it**: Systematic experiments testing tree configurations from 16 to 128+ nodes, measuring speed-up, acceptance rate, and computational overhead, would reveal the scaling relationship and optimal configuration.

### Open Question 3
**Question**: What is the impact of different positional encoding schemes in the Auto-embedding Block on generation quality and speed-up?
**Basis in paper**: The paper introduces learnable positional encodings in the Auto-embedding Block and shows that removing them causes a slight performance decline, but doesn't explore alternative positional encoding methods.
**Why unresolved**: The paper only tests one positional encoding approach (learnable embeddings) and mentions that alternative methods could be explored, but provides no comparison with other schemes like sinusoidal or rotary positional encodings.
**What evidence would resolve it**: Experiments comparing different positional encoding schemes (learnable, sinusoidal, rotary, relative positional embeddings) within the Auto-embedding Block, measuring both generation quality (ROUGE scores, accuracy) and speed-up metrics, would identify optimal approaches.

## Limitations
- Architectural details of the Auto-embedding Block and Staged Adaptation Layers are not fully specified
- Training procedure lacks complete implementation details, particularly around loss weighting and optimization
- Evaluation is limited to Vicuna models without demonstrating generalization to other architectures
- Quality validation across all claimed task domains is incomplete, with limited evidence for math reasoning and code generation

## Confidence

**High Confidence**: The claimed 1.40× speedup over Medusa is supported by ablation studies showing contributions from both Auto-embedding Block and Staged Adaptation Layers.

**Medium Confidence**: The 2.75× speedup over vanilla autoregressive decoding is plausible but depends heavily on unspecified implementation details and optimal acceptance rates.

**Low Confidence**: Generation quality maintenance across all claimed task domains (particularly math reasoning and code generation) is asserted but only partially validated with summarization benchmarks.

## Next Checks

1. **Architectural Specification Validation**: Implement the Auto-embedding Block and Staged Adaptation Layers with multiple architectural configurations to determine which specific design choices are critical for achieving reported speedups. Test different bi-directional attention mechanisms and layer depths.

2. **Training Procedure Replication**: Replicate the training process using exact hyperparameters provided, then systematically vary loss weighting between alignment and language modeling components to identify optimal balance. Measure sensitivity to training details.

3. **Cross-Architecture Generalization**: Train Amphista on LLM architectures beyond Vicuna (such as Llama or Mistral models) using different pretraining datasets to validate whether bi-directional multi-head approach generalizes across model families and domains.