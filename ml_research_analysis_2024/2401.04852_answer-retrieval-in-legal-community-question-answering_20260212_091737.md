---
ver: rpa2
title: Answer Retrieval in Legal Community Question Answering
arxiv_id: '2401.04852'
source_url: https://arxiv.org/abs/2401.04852
tags:
- question
- legal
- retrieval
- answer
- cross-encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses answer retrieval in legal Community Question
  Answering (CQA), tackling challenges from the knowledge gap between lawyers and
  non-professionals, and mixed informal/formal content. It introduces CEFS, a cross-encoder
  re-ranker that leverages fine-grained structured inputs, including question tags,
  to improve retrieval effectiveness.
---

# Answer Retrieval in Legal Community Question Answering

## Quick Facts
- arXiv ID: 2401.04852
- Source URL: https://arxiv.org/abs/2401.04852
- Reference count: 34
- Primary result: CE_FS achieves MAP@1k of 0.270 on LegalQA, outperforming MiniLM-MSMARCO (0.109) and CE_CAT (0.236)

## Executive Summary
This paper addresses answer retrieval in legal Community Question Answering (CQA), tackling challenges from the knowledge gap between lawyers and non-professionals, and mixed informal/formal content. It introduces CE_FS, a cross-encoder re-ranker that leverages fine-grained structured inputs, including question tags, to improve retrieval effectiveness. A new benchmark dataset, LegalQA, was created with 9,846 questions and 33,670 answers from certified lawyers. Experiments show CE_FS significantly outperforms strong baselines, including MiniLM-MSMARCO, achieving MAP@1k of 0.270 vs. 0.236 for CE_CAT and 0.109 for MiniLM-MSMARCO. The findings highlight the importance of incorporating question tags and structured inputs for improving cross-encoder re-rankers, with potential applications beyond the legal domain.

## Method Summary
The method employs a two-stage ranking pipeline: BM25 retrieves top-1000 answers, then CE_FS (cross-encoder with structured inputs) re-ranks them. CE_FS uses structured input with question subject, description, and tags separated by [S], [D], and [T] tokens, followed by the answer. The model is fine-tuned on LegalQA using MAP@1k as the primary metric, comparing against baselines including BM25, LMD, and CE_CAT models.

## Key Results
- CE_FS achieves MAP@1k of 0.270 on LegalQA, significantly outperforming CE_CAT (0.236) and MiniLM-MSMARCO (0.109)
- Structured input with question tags provides substantial effectiveness boost in cross-encoder re-rankers
- Fine-tuning on domain-specific LegalQA data outperforms pre-trained models fine-tuned on MS MARCO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured fine-grained inputs bridge the knowledge gap between lawyers and non-professionals.
- Mechanism: The cross-encoder CE_FS uses structured question components (subject, description, tags) separated by splitter tokens, allowing the model to separately process legal terminology (tags) from natural language (description), thereby mitigating lexical mismatches.
- Core assumption: The question tags contain high-density legal terminology that is semantically distinct from the natural language question description.
- Evidence anchors:
  - [abstract] "Our novel finding is that adding the question tags of each question besides the question description and title into the input of cross-encoder re-rankers structurally boosts the rankers’ effectiveness."
  - [section] "The novelties brought by CE FS can be summarized in two key aspects: – Structured Input. CE FS employs structured input by dividing the question into distinct sections – the subject, description, and tags."
  - [corpus] Weak anchor: neighbor papers focus on general QA or language-specific legal datasets, not specifically on structured input bridging professional-layperson gaps.
- Break condition: If question tags are sparse, generic, or misaligned with legal terminology, the structured input advantage disappears.

### Mechanism 2
- Claim: Incorporating question tags into cross-encoder input structurally boosts re-ranking effectiveness.
- Mechanism: The [T] splitter token followed by semicolon-separated tags forces the transformer to attend to legal terminology as a distinct semantic block, which correlates strongly with answer relevance in legal CQA.
- Core assumption: Legal question tags are high-precision indicators of the legal domain and relevant answer content.
- Evidence anchors:
  - [abstract] "Our novel finding is that adding the question tags of each question besides the question description and title into the input of cross-encoder re-rankers structurally boosts the rankers’ effectiveness."
  - [section] "We are also motivated by the fact that, in legal CQA, question tags consist of important legal terms related to the legal question and using them can potentially bridge the knowledge gap between lawyer content and questioner content."
  - [corpus] Weak anchor: corpus neighbors do not provide direct evidence on the role of tags in cross-encoder re-ranking; they focus on retrieval or generation frameworks.
- Break condition: If tags are automatically generated with low precision or become redundant with description content, the re-ranking boost diminishes.

### Mechanism 3
- Claim: Fine-tuning cross-encoders on domain-specific data (LegalQA) outperforms pre-trained models (MiniLM-MSMARCO) on legal answer retrieval.
- Mechanism: LegalQA fine-tuning adapts the cross-encoder to the legal domain's lexical and semantic peculiarities, reducing the performance gap caused by domain mismatch.
- Core assumption: The legal domain has unique terminology and discourse patterns that general-purpose models (trained on MS MARCO) cannot capture.
- Evidence anchors:
  - [abstract] "Experiments conducted on LegalQA show that our proposed method significantly outperforms strong cross-encoder re-rankers fine-tuned on MS MARCO."
  - [section] "The relatively low effectiveness of BM25 reveals a noticeable difference in lexical word overlap between the question and the best answer. This difference serves as an indicator of a knowledge gap between the questioner and the lawyer, resulting in a higher occurrence of lexical mismatches between the question and the relevant answer."
  - [corpus] Weak anchor: corpus neighbors do not directly compare MS MARCO vs. legal-domain fine-tuning; they focus on other legal QA frameworks.
- Break condition: If the training set size is too small or not representative, fine-tuning may overfit or fail to generalize.

## Foundational Learning

- Concept: Cross-encoder architecture and fine-tuning mechanics
  - Why needed here: The paper relies on fine-tuning a cross-encoder (e.g., MiniLM) to model relevance between query and document pairs in legal CQA.
  - Quick check question: What is the difference between a cross-encoder and a dual-encoder in ranking tasks?

- Concept: Tokenization and splitter tokens
  - Why needed here: CE_FS introduces custom splitter tokens ([S], [D], [T]) to structure the input; understanding how these tokens are handled by the tokenizer is essential for correct model behavior.
  - Quick check question: How do custom tokens like [S], [D], [T] affect BERT's attention distribution in a cross-encoder?

- Concept: Lexical vs. semantic retrieval mismatch
  - Why needed here: The paper highlights that lexical overlap (BM25) is insufficient due to the knowledge gap between lawyers and non-professionals; understanding this motivates the need for semantic models.
  - Quick check question: In what scenario would BM25 outperform a transformer-based re-ranker in legal CQA?

## Architecture Onboarding

- Component map: BM25 -> top-1000 answers -> CE_FS (cross-encoder) -> re-ranked list -> MAP@1k evaluation
- Critical path: BM25 → top-1000 answers → CE_FS → re-ranked list → MAP@1k evaluation
- Design tradeoffs:
  - Using structured inputs increases model complexity and inference time but improves accuracy in the legal domain.
  - Relying on automatically generated tags introduces variability; handcrafted tags could be more reliable but less scalable.
  - Fine-tuning on LegalQA improves domain fit but risks overfitting if the dataset is small.
- Failure signatures:
  - MAP@1k plateaus or drops if tags are generic or missing.
  - Re-ranking effectiveness collapses if the first-stage retriever returns irrelevant candidates.
  - Training divergence if the dataset is too small or unbalanced.
- First 3 experiments:
  1. Compare CE_FS vs. CE_CAT on LegalQA with BM25 baseline to confirm structured input benefit.
  2. Run ablation: remove [T] tag section, measure MAP@1k drop.
  3. Test zero-shot CE_CAT (MS MARCO) vs. fine-tuned CE_CAT (LegalQA) to quantify domain adaptation gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would dense retrievers be as first-stage retrieval for legal answer retrieval compared to BM25 and LMD?
- Basis in paper: [explicit] The paper explicitly states they left out investigating Transformer-based first-stage retrievers like dense retrievers, focusing instead on improving re-ranking effectiveness.
- Why unresolved: The paper only compared BM25 and LMD as first-stage retrievers, leaving a gap in understanding how modern dense retrieval methods would perform in this legal domain.
- What evidence would resolve it: Experiments comparing BM25, LMD, and dense retrievers like DPR or ANCE on the LegalQA dataset would provide concrete effectiveness comparisons.

### Open Question 2
- Question: How transferable is the CE_FS approach to other domains with structured question-answering data?
- Basis in paper: [explicit] The authors explicitly state "While our method is initially proposed for answer retrieval within the legal domain, we foresee its potential application in answer retrieval for community question-answering systems across various domains where question tags are provided alongside each question post."
- Why unresolved: The paper only validates the approach on legal CQA data, so transferability to other domains remains theoretical.
- What evidence would resolve it: Applying CE_FS to non-legal CQA datasets with question tags (like Stack Overflow or medical Q&A) and measuring effectiveness improvements would demonstrate transferability.

### Open Question 3
- Question: What is the impact of different types of question tags on retrieval effectiveness?
- Basis in paper: [inferred] The ablation study showed that query tags have less impact than question subject and description, but didn't analyze different types of tags or their semantic relationships.
- Why unresolved: The paper only used the provided question tags as a monolithic input without exploring whether certain types of tags (legal categories, keywords, concepts) contribute more to effectiveness.
- What evidence would resolve it: Analyzing different tag types separately or using semantic tag representations could reveal which tag characteristics are most beneficial for retrieval.

## Limitations

- Weak evidence anchors for core mechanisms - the mechanistic explanations about knowledge gap bridging and tag importance lack direct empirical validation
- Dataset creation process and tag generation method are not fully detailed, raising reproducibility concerns
- Findings may be dataset-specific to Chinese legal CQA, limiting generalizability to other languages or domains

## Confidence

**High Confidence**: The empirical results showing CE_FS outperforming MiniLM-MSMARCO and CE_CAT on the LegalQA benchmark (MAP@1k: 0.270 vs. 0.236 vs. 0.109) are well-supported by the experimental methodology and metrics. The two-stage ranking pipeline design is standard and clearly specified.

**Medium Confidence**: The claim that structured inputs with question tags improve re-ranking effectiveness is supported by the results but the mechanistic explanation about bridging the knowledge gap between lawyers and non-professionals is speculative. The evidence anchors for this mechanism are weak, relying mainly on the contrast with BM25's poor performance rather than direct analysis of tag-question-answer relationships.

**Low Confidence**: The claim that legal domain fine-tuning is necessary for optimal performance (vs. general pre-trained models) is based on comparing MS MARCO vs. LegalQA fine-tuning, but lacks ablation studies on dataset size effects or comparison with other legal datasets. The generalizability of the findings to other domains or languages remains untested.

## Next Checks

1. **Tag Content Analysis**: Conduct a detailed analysis of the automatically generated question tags to verify they contain high-density legal terminology and are not generic or redundant with the description. This would validate the core assumption behind Mechanism 2.

2. **Ablation Study on Dataset Size**: Perform experiments varying the training dataset size for LegalQA fine-tuning to determine whether the performance gains are due to domain adaptation or simply having more training data. This would test the validity of Mechanism 3.

3. **Cross-Domain Generalization Test**: Apply CE_FS to a non-legal CQA dataset (e.g., Stack Exchange) with and without question tags to determine whether the structured input approach generalizes beyond the legal domain, validating the broader applicability claims.