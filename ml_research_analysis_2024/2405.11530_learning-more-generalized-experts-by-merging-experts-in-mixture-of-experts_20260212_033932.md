---
ver: rpa2
title: Learning More Generalized Experts by Merging Experts in Mixture-of-Experts
arxiv_id: '2405.11530'
source_url: https://arxiv.org/abs/2405.11530
tags:
- experts
- learning
- expert
- task
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to address catastrophic forgetting
  and maintain plasticity in Mixture-of-Experts (MoE) models for multi-domain task
  incremental learning. The core idea is to merge the two most frequently selected
  experts and update the least frequently selected expert with their combination.
---

# Learning More Generalized Experts by Merging Experts in Mixture-of-Experts

## Quick Facts
- arXiv ID: 2405.11530
- Source URL: https://arxiv.org/abs/2405.11530
- Authors: Sejik Park
- Reference count: 40
- Key outcome: Proposed method improves transfer accuracy by 0.2%, average accuracy by 0.3%, and last accuracy compared to state-of-the-art on multi-domain task incremental learning

## Executive Summary
This paper addresses catastrophic forgetting and plasticity maintenance in Mixture-of-Experts (MoE) models for multi-domain task incremental learning. The proposed approach merges the two most frequently selected experts and updates the least frequently selected expert with their combination, creating more generalized experts that can learn shared features more effectively. Applied to a pretrained transformer-based CLIP model with lightweight adapters, the method shows improvements in transfer learning and mitigates catastrophic forgetting by reducing redundancy in the expert space.

## Method Summary
The method tracks expert selection frequency during training and periodically merges the two most frequently selected experts by averaging their weights. The merged expert then replaces the least frequently selected expert. This approach is implemented on a pretrained CLIP model with 55 experts using LoRA adapters, trained on 11 image classification datasets in a multi-domain task incremental learning setup. The model uses top-k expert selection with contrastive loss for training, and the expert merging occurs every 100 iterations.

## Key Results
- Transfer accuracy improved by 0.2% compared to state-of-the-art MA method
- Average accuracy across all tasks improved by 0.3%
- Last accuracy (final task performance) also improved
- Method demonstrates effectiveness in reducing catastrophic forgetting while maintaining plasticity

## Why This Works (Mechanism)

### Mechanism 1
Merging the two most frequently selected experts and replacing the least frequently selected expert with their combination reduces catastrophic forgetting by creating a more generalized feature representation. The two most frequently selected experts have learned similar or overlapping features due to their high usage, and averaging their weights creates a more generalized version of these features.

### Mechanism 2
The merged expert learns a more general feature distribution that reduces redundancy in the expert space. When two experts have learned the same feature in different ways (due to feature variance or skewed training distribution), their combination can learn a more central, generalized representation.

### Mechanism 3
Updating the least frequently selected expert with the merged expert maintains plasticity while preventing overfitting to specific tasks. The least used expert has the most capacity to learn new features without disrupting existing knowledge, making it ideal for repurposing with merged knowledge.

## Foundational Learning

- **Mixture-of-Experts (MoE) architecture and routing mechanisms**: Understanding how experts are selected and how their weights contribute to final predictions is crucial for grasping why expert merging affects performance.
  - Quick check: How does the top-k routing mechanism determine which experts contribute to each input, and what happens to the non-selected experts during that forward pass?

- **Catastrophic forgetting in continual learning**: The paper's motivation centers on preventing catastrophic forgetting, so understanding what it is and why it occurs is fundamental.
  - Quick check: What happens to the weights of experts that were important for previous tasks when training on new tasks, and why does this cause performance degradation?

- **Feature generalization vs. specialization in deep learning**: The core hypothesis is about creating more generalized features by merging specialized ones, so understanding the tradeoff between these is essential.
  - Quick check: Why might a network learn the same semantic feature in multiple different ways, and what causes this redundancy?

## Architecture Onboarding

- **Component map**: Image encoder (FI) and text encoder (FT) based on CLIP transformer architecture → Multiple blocks containing multi-head attention with layer norm, MLP with layer norm → Expert modules (NE experts) and router networks for expert selection → Lightweight adapters (LoRA) for expert parameters → Contrastive loss function for training

- **Critical path**: Input image passes through image encoder blocks → Each block computes MoE routing: router selects top-k experts, applies softmax gating → Selected experts process input and combine with residual connection → Final representation used for contrastive learning with text encoder output → Backpropagation updates expert weights and router parameters

- **Design tradeoffs**: Number of experts (55 used) vs. model capacity and computational efficiency; Frequency of expert merging (every 100 iterations) vs. stability vs. adaptability; Which experts to merge (top-2 frequent) vs. which to replace (least frequent) vs. potential information loss

- **Failure signatures**: Performance degradation when merging experts that learned orthogonal features; Increased variance in accuracy across tasks when merging disrupts established expert roles; Failure to improve transfer learning when merged experts don't create meaningful generalization

- **First 3 experiments**: 1) Run baseline MA algorithm on MTIL benchmark without any expert merging to establish performance baseline; 2) Implement expert frequency tracking and verify that expert selection patterns are consistent across runs; 3) Add the expert merging step with frequency-based selection and measure impact on transfer accuracy and average accuracy across all MTIL tasks

## Open Questions the Paper Calls Out

### Open Question 1
How does the distribution of selected experts across different blocks of the encoder evolve during training, and what implications does this have for the model's ability to learn generalized features?

### Open Question 2
What are the theoretical underpinnings that justify the merging of experts based on selection frequency, and how does this approach contribute to the creation of more generalized features?

### Open Question 3
How does the proposed method compare to other approaches for addressing catastrophic forgetting in multi-domain task incremental learning, such as regularization techniques or replay buffers?

## Limitations

- The core hypothesis about creating more generalized experts through frequency-based merging lacks empirical evidence showing that merged experts actually learn more generalized representations rather than just averaging weights.
- The method assumes that replacing the least frequently selected expert is beneficial, but this may not hold if the least used expert contains unique features for future tasks.
- The specific parameters for merging frequency (every 100 iterations) and the number of experts (55) are not thoroughly justified through ablation studies.

## Confidence

- **High Confidence**: Experimental results showing improved transfer accuracy, average accuracy, and last accuracy on the MTIL benchmark.
- **Medium Confidence**: Mechanism that merging frequently selected experts creates more generalized features, though theoretical justification is incomplete.
- **Low Confidence**: Hypothesis that the same feature being learned in various ways is the primary cause of difficulty in learning shared features.

## Next Checks

1. Conduct t-SNE or UMAP visualization of expert activations before and after merging to empirically verify that merged experts learn more generalized, central feature representations.

2. Test alternative expert selection strategies (e.g., merging experts with most similar activation patterns, or replacing the most frequent rather than least frequent expert) to determine whether the specific frequency-based approach is optimal.

3. Evaluate the model's ability to learn new tasks after multiple merging iterations to verify that the plasticity benefit is maintained and catastrophic forgetting is genuinely mitigated, not just postponed.