---
ver: rpa2
title: 'Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language
  Models'
arxiv_id: '2408.06458'
source_url: https://arxiv.org/abs/2408.06458
tags:
- task
- environment
- agent
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Self-Adaptive Language Agent (SALA), a novel
  in-context learning algorithm that enables a single language model to autonomously
  correct its own failures when solving tasks. SALA integrates reasoning, acting,
  observing, and self-correction mechanisms, allowing the model to adapt its strategy
  after failed attempts.
---

# Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models

## Quick Facts
- arXiv ID: 2408.06458
- Source URL: https://arxiv.org/abs/2408.06458
- Reference count: 18
- Key outcome: SALA achieves 83% success rate in ALFWorld, outperforming ReAct-based agent at 67%

## Executive Summary
This paper introduces SALA (Self-Adaptive Language Agent), a novel in-context learning algorithm enabling a single language model to autonomously correct its own failures during task completion. SALA integrates reasoning, acting, observing, and self-correction mechanisms, allowing the model to adapt its strategy after failed attempts. Experiments in the ALFWorld text-based game environment demonstrate that SALA using gemma-2-9b-it achieves 83% success rate across 12 tasks, outperforming a ReAct-based agent (67% success rate) and successfully completing two tasks that initially failed through self-adaptation.

## Method Summary
SALA employs a single LLM (gemma-2-9b-it) to generate thoughts, actions, and self-adaptations by concatenating ReAct exemplars (for reasoning/acting) and Reflexion exemplars (for reflection) into one prompt. When a task fails after maximum steps, SALA generates a self-adaptation text and prepends it to the next trial's prompt, conditioning the LLM on prior failure context. The method uses context compression to reduce context length while preserving adaptation influence, replacing full trajectory history with only the original prompt and adaptation text. SALA iteratively receives environment observations and adjusts plans, enabling autonomous adaptation without requiring multiple models.

## Key Results
- SALA achieves 83% success rate in ALFWorld across 12 tasks
- SALA successfully completes two tasks that initially failed through self-adaptation
- SALA outperforms ReAct-based agent (67% success rate) in the same environment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-correction improves task completion by generating adaptations from failed trials.
- Mechanism: When a task fails after maximum steps, SALA generates a self-adaptation text and prepends it to the next trial's prompt, conditioning the LLM on prior failure context.
- Core assumption: A single LLM can generate both actions and meaningful corrective feedback when prompted with failure status.
- Evidence anchors: SALA successfully completes two tasks that failed initially by applying corrections from previous trials; adaptation text is appended to prompt for second trial leading to successful completion.
- Break condition: If the LLM generates non-useful or repetitive adaptations, the self-correction loop stalls and task completion fails.

### Mechanism 2
- Claim: Compression reduces context length while preserving adaptation influence.
- Mechanism: Instead of carrying the full history, SALA replaces the initial state with only the original prompt and the adaptation text.
- Core assumption: The adaptation text alone is sufficient to influence early decision-making without needing the full trajectory.
- Evidence anchors: In the next step, SALA proposes to replace the initial state in the second trial with the initial state in the first trial to reduce context length.
- Break condition: If the adaptation omits critical details, compression may remove necessary context, causing repeated failures.

### Mechanism 3
- Claim: Single-LLM design reduces overhead and increases autonomy compared to dual-LLM Reflexion.
- Mechanism: SALA merges ReAct exemplars (for reasoning/acting) and Reflexion exemplars (for reflection) into one prompt.
- Core assumption: The combined prompt format is sufficient for the model to distinguish between reasoning, acting, and reflecting phases.
- Evidence anchors: SALA uses a single LLM to generate thoughts, actions, and self-adaptations by concatenating the two Reflexion exemplars after the two ReAct exemplars.
- Break condition: If the LLM confuses task phases, the single-LLM approach may degrade performance.

## Foundational Learning

- Concept: In-context learning with few exemplars.
  - Why needed here: SALA relies on carefully structured exemplars to guide reasoning, acting, and reflection without fine-tuning.
  - Quick check question: Can you construct a prompt that demonstrates both reasoning and action steps for a simple household task?

- Concept: Chain-of-thought reasoning.
  - Why needed here: SALA uses CoT-style intermediate reasoning steps to decompose tasks before acting.
  - Quick check question: Given a task "put a clean cloth in countertop," what intermediate reasoning steps would lead to successful completion?

- Concept: Environment feedback loops.
  - Why needed here: SALA iteratively receives observations and uses them to adjust plans, essential for autonomous adaptation.
  - Quick check question: If an action "take obj_id from recap_id" fails, what observation should the agent expect, and how should it adapt?

## Architecture Onboarding

- Single LLM approach: SALA uses one model (gemma-2-9b-it) for all phases - reasoning, acting, and reflection - by combining ReAct and Reflexion exemplars in a single prompt.
- In-context learning: No fine-tuning required; the model learns from carefully structured exemplars in the prompt.
- Iterative adaptation: The agent receives environment observations, attempts tasks, and generates self-adaptations when failures occur.
- Context compression: To manage context length, SALA retains only the original prompt and adaptation text rather than full trajectory history.
- Task-specific prompting: SALA prepends adaptation text to the next trial's prompt, conditioning the model on prior failure context.

## Open Questions the Paper Calls Out

- Scalability to more complex environments beyond ALFWorld
- Performance with different LLM sizes and architectures
- Impact of varying the number of exemplars in the prompt
- Generalization across different task domains
- Robustness of self-adaptations in noisy or partially observable environments

## Limitations

- Tested only in ALFWorld environment, limiting generalizability
- Performance depends on the quality of exemplars in the prompt
- Self-adaptations may become repetitive or ineffective for complex failures
- Context compression might omit critical information needed for successful adaptation
- Single-LLM approach may struggle with tasks requiring specialized reasoning capabilities

## Confidence

High confidence in core methodology and results. The paper provides clear experimental setup, ablation studies, and comparison with baseline approaches. The single-LLM design and context compression strategies are well-justified and empirically validated.

## Next Checks

- Verify that the gemma-2-9b-it model was the only LLM tested
- Confirm the exact number of exemplars used in the prompt
- Check whether the 83% success rate includes tasks that required multiple trials
- Examine the content and quality of generated self-adaptation texts
- Review the ablation study results for context compression effectiveness