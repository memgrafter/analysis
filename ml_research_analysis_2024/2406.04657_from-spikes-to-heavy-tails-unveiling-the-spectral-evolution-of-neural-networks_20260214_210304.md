---
ver: rpa2
title: 'From Spikes to Heavy Tails: Unveiling the Spectral Evolution of Neural Networks'
arxiv_id: '2406.04657'
source_url: https://arxiv.org/abs/2406.04657
tags:
- learning
- figure
- fb-adam
- after
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the emergence of heavy-tailed (HT) empirical
  spectral densities (ESDs) in neural network weight matrices without relying on gradient
  noise. The authors propose a two-stage training setup for a two-layer neural network
  learning a single-index teacher model using full-batch Gradient Descent (GD) and
  Adam.
---

# From Spikes to Heavy Tails: Unveiling the Spectral Evolution of Neural Networks

## Quick Facts
- arXiv ID: 2406.04657
- Source URL: https://arxiv.org/abs/2406.04657
- Reference count: 40
- This paper analyzes the emergence of heavy-tailed (HT) empirical spectral densities (ESDs) in neural network weight matrices without relying on gradient noise, proposing a two-stage training setup for a two-layer neural network learning a single-index teacher model using full-batch Gradient Descent (GD) and Adam.

## Executive Summary
This paper investigates the emergence of heavy-tailed empirical spectral densities (ESDs) in neural network weight matrices, challenging the conventional understanding that such phenomena arise primarily from gradient noise. The authors propose a two-stage training setup for a two-layer neural network learning a single-index teacher model using full-batch Gradient Descent (GD) and Adam optimizers. They identify specific learning rate scales at which the first Adam update induces a spike in the weight matrix ESD, facilitating feature learning. Through theoretical analysis of singular vector alignments between weight matrices and optimizer updates, the study reveals that multiple optimizer steps with sufficiently large learning rates can transform the bulk of the weight spectrum into a heavy-tailed distribution.

## Method Summary
The authors propose a two-stage training approach for a two-layer neural network learning a single-index teacher model. The first stage employs full-batch Gradient Descent (GD) to establish initial weights, while the second stage uses the Adam optimizer with varying learning rates. The study analyzes the singular value decomposition (SVD) of weight matrices and optimizer updates to understand the mechanisms driving heavy-tailed spectral evolution. By examining the scale of learning rates and their impact on the weight matrix ESDs, the authors identify conditions under which spikes and heavy-tailed distributions emerge in the spectrum.

## Key Results
- The first Adam update with sufficiently large learning rates induces a spike in the weight matrix ESD, facilitating feature learning.
- Multiple optimizer steps with large learning rates transform the bulk of the weight spectrum into a heavy-tailed distribution.
- Analysis of singular vector alignments between weight matrices and optimizer updates provides insights into the mechanisms driving heavy-tailed ESD emergence.
- The study explores the correlation between heavy-tailed weight spectra and generalization performance, highlighting the roles of learning rates, weight normalization, and regularization.

## Why This Works (Mechanism)
The emergence of heavy-tailed ESDs is driven by the interplay between learning rate scales and optimizer dynamics. When the learning rate is sufficiently large, the Adam optimizer induces a spike in the weight matrix ESD during the first update, which acts as a catalyst for feature learning. Subsequent optimizer steps amplify this effect, transforming the bulk of the weight spectrum into a heavy-tailed distribution. The singular vector alignments between weight matrices and optimizer updates reveal that the direction and magnitude of updates play a crucial role in shaping the spectral evolution. This mechanism operates independently of gradient noise, offering a new perspective on how heavy-tailed spectra emerge in neural networks.

## Foundational Learning
- **Empirical Spectral Density (ESD)**: The distribution of eigenvalues of a matrix, used to analyze the spectral properties of weight matrices. *Why needed*: Understanding ESDs is crucial for studying the spectral evolution of neural networks. *Quick check*: Verify that the ESD captures the distribution of singular values for non-square matrices.
- **Singular Value Decomposition (SVD)**: A factorization of a matrix into three matrices, revealing its singular values and vectors. *Why needed*: SVD is used to analyze the alignment between weight matrices and optimizer updates. *Quick check*: Confirm that the singular values represent the scaling factors in the decomposition.
- **Heavy-Tailed Distributions**: Probability distributions with tails heavier than exponential distributions, often characterized by power-law decay. *Why needed*: Heavy-tailed spectra are linked to improved generalization in neural networks. *Quick check*: Ensure that the tail behavior follows a power-law decay.
- **Gradient Descent (GD) vs. Adam Optimizer**: GD updates weights using a fixed learning rate, while Adam adapts the learning rate based on gradient moments. *Why needed*: Comparing these optimizers reveals the role of adaptive learning rates in spectral evolution. *Quick check*: Verify that Adam's adaptive learning rate induces spikes in ESDs more effectively than GD.
- **Learning Rate Scaling**: The magnitude of weight updates during training, influencing the spectral properties of weight matrices. *Why needed*: Identifying the critical learning rate scales is key to understanding heavy-tailed ESD emergence. *Quick check*: Test different learning rate scales to observe their impact on spectral evolution.

## Architecture Onboarding
- **Component Map**: Two-layer neural network -> Single-index teacher model -> Full-batch GD (Stage 1) -> Adam optimizer (Stage 2)
- **Critical Path**: Weight initialization -> Full-batch GD updates -> Adam optimizer with large learning rates -> Spike in ESD -> Heavy-tailed spectral evolution
- **Design Tradeoffs**: Using full-batch GD ensures stable initial weights, while Adam's adaptive learning rate facilitates spectral evolution. However, this setup may not generalize to stochastic optimization methods.
- **Failure Signatures**: If the learning rate is too small, the spike in ESD may not occur, preventing heavy-tailed spectral evolution. Conversely, excessively large learning rates may lead to instability or divergence.
- **First Experiments**: 1) Vary the learning rate scale in Adam to identify the critical threshold for spike induction. 2) Compare the spectral evolution of GD and Adam optimizers. 3) Analyze the impact of weight normalization on heavy-tailed ESD emergence.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses on a specific two-layer neural network architecture learning a single-index teacher model, which may not generalize to deeper or more complex architectures.
- The study relies on full-batch Gradient Descent and Adam optimizers, potentially limiting applicability to other optimization methods.
- The paper does not provide empirical validation across diverse datasets or real-world applications, which could affect the robustness of the findings.

## Confidence
- **High**: The identification of learning rate scales that induce spikes in weight matrix ESDs and facilitate feature learning.
- **Medium**: The theoretical analysis of singular vector alignments and their role in heavy-tailed spectral evolution.
- **Low**: The correlation between heavy-tailed weight spectra and generalization performance, as this requires more empirical validation.

## Next Checks
1. Extend the analysis to deeper neural network architectures and multiple teacher models to assess the generalizability of the findings.
2. Conduct experiments using different optimization algorithms and compare their effects on spectral evolution and heavy-tailed distribution emergence.
3. Perform empirical studies on diverse datasets to validate the correlation between heavy-tailed weight spectra and generalization performance in practical settings.