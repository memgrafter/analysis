---
ver: rpa2
title: 'Let''s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation'
arxiv_id: '2406.07867'
source_url: https://arxiv.org/abs/2406.07867
tags:
- speech
- dialogue
- arxiv
- audio-visual
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a face-to-face spoken dialogue model that
  processes audio-visual speech from user input and generates audio-visual speech
  as the response, marking the first step toward creating an avatar chatbot system
  without relying on intermediate text. To support this, the authors introduce MultiDialog,
  a large-scale multimodal spoken dialogue corpus containing 340 hours of approximately
  9,000 dialogues, with parallel audio-visual recordings and emotion annotations.
---

# Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation

## Quick Facts
- arXiv ID: 2406.07867
- Source URL: https://arxiv.org/abs/2406.07867
- Reference count: 19
- Primary result: Introduces first face-to-face spoken dialogue model using AV speech tokens without intermediate text

## Executive Summary
This paper presents a novel face-to-face spoken dialogue model that processes audio-visual speech directly from user input and generates audio-visual speech responses without relying on intermediate text. The approach leverages a pre-trained large language model (OPT-1.3B) and adapts it to the audio-visual domain through joint speech-text pretraining. The authors introduce MultiDialog, a large-scale multimodal spoken dialogue corpus with 340 hours of approximately 9,000 dialogues, featuring parallel audio-visual recordings and emotion annotations. Experimental results demonstrate improved semantic quality and audio-visual synchronization compared to state-of-the-art baselines, marking the first step toward creating avatar chatbot systems for real-time face-to-face conversations.

## Method Summary
The proposed method employs a multi-stage training approach: first, audio-visual speech features are extracted from input using AV-HuBERT and quantized into discrete tokens; these tokens are then treated as pseudo-text and processed by an adapted OPT-1.3B LLM through joint speech-text pretraining; the model is subsequently finetuned on pure AV speech token-based dialogue; finally, token-based speech and face decoders generate the audio and visual output. The training follows a progressive shift from mixed text-AV speech pretraining to pure AV speech finetuning, helping the model gradually adapt to AV speech tokens while retaining text-based dialogue generation quality. The system is trained on the MultiDialog dataset and evaluated using semantic quality metrics (BLEU, METEOR, F1) and audio-visual synchronization metrics (FID, LSE-C, LSE-D).

## Key Results
- Improved semantic quality (BLEU, METEOR, F1) compared to state-of-the-art baselines
- Enhanced audio-visual synchronization (LSE-C, LSE-D) demonstrating better temporal alignment
- Demonstrated robustness to acoustic noise with audio-visual input outperforming audio-only under various SNR levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint speech-text pretraining using a pre-trained LLM improves semantic quality over direct speech token initialization.
- Mechanism: The pre-trained LLM provides strong text generation capabilities, which are leveraged during the AVSR/TTS pretraining stages to guide the model in understanding and generating AV speech tokens while retaining dialogue modeling knowledge.
- Core assumption: Text-based knowledge from the LLM transfers effectively to the AV speech domain through joint training.
- Evidence anchors:
  - [abstract] "Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining."
  - [section] "Initial experiments show that using a pre-trained LLM as initialization and incorporating joint speech-text pretraining improves semantic quality compared to direct speech token initialization."
  - [corpus] Weak - corpus analysis shows related work but no direct comparison of pretraining strategies.

### Mechanism 2
- Claim: Using AV speech tokens as pseudo-text enables direct processing of multimodal input without intermediate text generation.
- Mechanism: AV speech tokens extracted from the AV-HuBERT model provide a discrete representation of both audio and visual speech features, which can be treated as text tokens for the LLM, enabling end-to-end AV speech processing.
- Core assumption: AV speech tokens contain sufficient linguistic and phonetic information to serve as a proxy for text in dialogue modeling.
- Evidence anchors:
  - [abstract] "Motivated by the recent success of the direct spoken dialogue model using discretized speech tokens... we introduce audio-visual (AV) speech tokens extracted by quantizing audio-visual speech features from a self-supervised model."
  - [section] "By combining the visual cues and the auditory information, the audio-visual speech tokens extract both linguistic and phonetic information. Then, we treat the AV speech tokens as pseudo text to train our Audio-Visual Spoken Dialogue LM."

### Mechanism 3
- Claim: The progressive shift from mixed text-AV speech pretraining to pure AV speech finetuning helps the model adapt to AV speech tokens without compromising text-based dialogue generation quality.
- Mechanism: The model first learns to jointly process text and AV speech tokens, then gradually shifts to focus solely on AV speech tokens, allowing it to retain text-based knowledge while specializing in AV speech processing.
- Core assumption: The model can effectively transfer knowledge from the mixed modality stage to the pure AV speech stage.
- Evidence anchors:
  - [abstract] "Then, we later finetune on pure AV speech token-based dialogue as in Fig. 2(d) for real-time face-to-face interaction. This progressive shift helps the model to gradually adapt to AV speech tokens without compromising the quality of dialogue generation of the text-based LLM."

## Foundational Learning

- Concept: Discrete speech tokenization
  - Why needed here: To convert continuous audio-visual speech into a format that can be processed by the LLM, which operates on discrete tokens.
  - Quick check question: How do AV speech tokens preserve both linguistic and phonetic information from the original audio-visual speech?

- Concept: Multimodal representation learning
  - Why needed here: To effectively combine audio and visual information into a unified representation that captures the nuances of face-to-face communication.
  - Quick check question: How does the AV-HuBERT model integrate audio and visual features to create a robust representation of speech?

- Concept: Progressive training strategies
  - Why needed here: To gradually adapt the model from text-based processing to AV speech processing while retaining knowledge from both modalities.
  - Quick check question: What are the potential benefits and risks of using a progressive training approach in this context?

## Architecture Onboarding

- Component map:
  AV-HuBERT -> Discrete AV speech tokens -> Adapted OPT-1.3B LLM -> AV speech tokens -> Token-based speech decoder + Token-based face decoder + Length predictor -> Audio and visual output

- Critical path:
  1. AV-HuBERT extracts and quantizes AV speech features into tokens
  2. Tokens are processed by the adapted LLM
  3. LLM generates AV speech tokens as response
  4. Token-based decoders generate audio and visual output

- Design tradeoffs:
  - Using a pre-trained LLM vs. training from scratch: Provides better text generation capabilities but requires careful adaptation to AV speech
  - Joint speech-text pretraining vs. direct AV speech training: Enables better transfer of text-based knowledge but adds complexity
  - Progressive training vs. direct finetuning: Allows gradual adaptation but increases training time

- Failure signatures:
  - Poor semantic quality: Indicates issues with the joint speech-text pretraining or AV speech token representation
  - Audio-visual desynchronization: Suggests problems with the token-based decoders or length predictor
  - Speaker voice mismatch: Points to issues with the speaker embedding integration in the speech decoder

- First 3 experiments:
  1. Ablation study on pretraining strategies (scratch vs. LLM initialization vs. joint pretraining) to validate the effectiveness of the proposed approach
  2. Comparison of semantic quality metrics (BLEU, METEOR, F1) between the proposed method and baselines to demonstrate improvements
  3. Evaluation of audio-visual synchronization (LSE-C, LSE-D) to assess the quality of the generated output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance degrade when only audio or only video input is used, and at what noise levels does visual information become more beneficial than audio alone?
- Basis in paper: [explicit] The paper analyzes robustness to acoustic noise, showing that audio-visual input performs better than audio-only input under noise corruption with different SNR levels.
- Why unresolved: The analysis focuses on comparing audio-only vs. audio-visual input under noise, but doesn't provide a detailed breakdown of performance degradation for each modality separately or specific SNR thresholds where visual information becomes more beneficial.
- What evidence would resolve it: Controlled experiments measuring performance degradation for audio-only and video-only inputs at various SNR levels, identifying the specific SNR threshold where visual information surpasses audio alone in performance.

### Open Question 2
- Question: What is the impact of emotion labels on the quality and naturalness of the generated responses, and how would incorporating emotion recognition from user facial expressions affect the dialogue system's performance?
- Basis in paper: [explicit] The paper mentions that the dataset includes emotion labels for each utterance but has not utilized them yet, and plans to integrate emotion recognition from users' facial expressions in future research.
- Why unresolved: The paper acknowledges the existence of emotion labels but doesn't explore their potential impact on response generation or how emotion recognition could be integrated into the system.
- What evidence would resolve it: Experiments comparing response quality and naturalness with and without emotion conditioning, and incorporating real-time emotion recognition from user facial expressions to generate emotion-aware responses.

### Open Question 3
- Question: How would the system perform if it simultaneously modeled the generation of both the speaker's and listener's faces, and what impact would this have on the naturalness and spontaneity of the conversation?
- Basis in paper: [explicit] The paper mentions that since the data provides parallel recordings of the speaker and listener, it can simultaneously model the generation of both faces for more spontaneous and natural conversation.
- Why unresolved: The paper acknowledges the potential for modeling both faces but doesn't explore this capability or its impact on conversation quality.
- What evidence would resolve it: Experiments comparing conversation naturalness and spontaneity when modeling both speaker and listener faces versus only the speaker's face, using metrics like user engagement and perceived conversational quality.

## Limitations

- The paper lacks human perceptual evaluations to validate whether quantitative improvements in automated metrics translate to meaningful improvements in actual face-to-face conversations.
- The specific architecture choices (AV-HuBERT configuration, OPT-1.3B size) are not empirically validated as optimal for this task, with no ablation studies examining alternative configurations.
- Limited analysis of how emotion labels in the dataset could be leveraged to improve response quality and naturalness, representing a missed opportunity for enhancing dialogue systems.

## Confidence

**High Confidence Claims:**
- The MultiDialog corpus construction methodology is well-documented and reproducible.
- The general approach of using discretized speech tokens for direct speech dialogue modeling has precedent in related work.

**Medium Confidence Claims:**
- The joint speech-text pretraining strategy improves semantic quality compared to baseline approaches.
- The progressive training curriculum facilitates effective adaptation to AV speech tokens.

**Low Confidence Claims:**
- The specific architecture choices (AV-HuBERT configuration, OPT-1.3B size) are optimal for this task.
- The AV speech tokens preserve sufficient information for high-quality dialogue generation.
- The quantitative improvements in automated metrics correspond to meaningful improvements in actual face-to-face conversations.

## Next Checks

1. **Ablation Study on Tokenization Strategy**: Conduct controlled experiments comparing the proposed AV speech token approach against alternative tokenization methods (e.g., text-based ASR with visual features, direct audio-visual feature concatenation) to isolate the contribution of the AV-HuBERT-based tokenization to overall performance.

2. **Human Perceptual Evaluation**: Design and execute a user study with human raters evaluating dialogue coherence, audiovisual synchronization, and naturalness of generated responses. Compare these perceptual scores against the automated metrics reported in the paper to validate whether the quantitative improvements align with human judgment.

3. **Cross-Domain Generalization Test**: Evaluate the model's performance on dialogue datasets from different domains (e.g., task-oriented dialogues, emotional conversations) to assess whether the learned representations generalize beyond the MultiDialog corpus and identify potential overfitting to the training data distribution.