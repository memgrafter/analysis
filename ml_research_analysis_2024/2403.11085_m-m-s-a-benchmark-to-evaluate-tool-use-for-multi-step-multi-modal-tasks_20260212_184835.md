---
ver: rpa2
title: 'm&m''s: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks'
arxiv_id: '2403.11085'
source_url: https://arxiv.org/abs/2403.11085
tags:
- multi-step
- tool
- planning
- feedback
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: m&m's is a new benchmark for evaluating large language models as
  planners for multi-step multi-modal tasks. It contains over 4,000 realistic user
  queries and human-verified executable plans using 33 diverse tools including ML
  models, APIs, and image processing modules.
---

# m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks

## Quick Facts
- arXiv ID: 2403.11085
- Source URL: https://arxiv.org/abs/2403.11085
- Reference count: 40
- Primary result: Multi-step planning in JSON format with feedback leads to best tool-use performance across 4,000+ multi-modal tasks

## Executive Summary
m&m's is a comprehensive benchmark designed to evaluate large language models as planners for complex multi-step multi-modal tasks. The benchmark includes over 4,000 realistic user queries and 33 diverse tools including ML models, APIs, and image processing modules, with human-verified executable plans. Through systematic evaluation of 10 popular LLMs across different planning strategies, plan formats, and feedback types, the study demonstrates that multi-step planning in JSON format with feedback consistently outperforms other approaches in overall tool-use performance.

## Method Summary
The study evaluates LLMs as planners for multi-step multi-modal tasks using the m&m's benchmark. The benchmark consists of 4,427 tasks with 33 diverse tools, including ML models, APIs, and image processing modules. Researchers evaluate 10 LLMs across two planning strategies (multi-step vs. step-by-step), two plan formats (JSON vs. code), and three feedback types (parsing, verification, execution). The evaluation measures tool selection accuracy (tool-F1), argument naming accuracy (argname-F1), and plan executability (pass rate) across all combinations.

## Key Results
- Multi-step planning consistently outperforms step-by-step planning across all models and metrics
- Verification and execution feedback improve plan executability at a small cost to tool selection
- JSON format yields more executable plans than code generation while maintaining comparable tool selection performance
- GPT-4 achieves highest tool-F1 (85.9) and pass rate (97.8) among all evaluated models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-step planning in JSON format with feedback consistently outperforms other planning strategies in tool-use performance.
- Mechanism: By generating the entire plan upfront and using structured JSON output, the LLM can reason about tool dependencies and execution flow before any tool is called. Feedback (parsing, verification, execution) allows iterative refinement that catches errors early and improves plan executability.
- Core assumption: The LLM can maintain coherent reasoning across multiple steps when planning the full sequence at once, and structured JSON output reduces parsing and execution errors compared to free-form code.
- Evidence anchors:
  - [abstract] "multi-step planning in JSON format with feedback leads to the best overall tool-use performance"
  - [section] "All planning agents perform better on tool-F1 with multi-step planning than with step-by-step planning"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

### Mechanism 2
- Claim: Verification and execution feedback improve LLMs' ability to generate overall executable plans and predict the correct argument names but don't necessarily improve their tool selection ability.
- Mechanism: Feedback mechanisms catch errors in argument naming and tool invocation that the LLM might miss, leading to more executable plans. However, tool selection is more about choosing the right tools for the task, which feedback doesn't directly address.
- Core assumption: Feedback mechanisms are effective at catching argument-related errors but don't provide information about the correctness of tool choices.
- Evidence anchors:
  - [abstract] "verification and execution feedback improve plan executability at a small cost to tool selection"
  - [section] "verification and execution feedback can help models predict correct argument names and generate more executable plans"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

### Mechanism 3
- Claim: JSON format yields more executable plans than code generation while maintaining comparable tool selection performance.
- Mechanism: The structured nature of JSON reduces syntax errors and makes it easier to parse and validate the plan before execution. This leads to more executable plans compared to free-form code generation.
- Core assumption: The rigidity of JSON format prevents common code generation errors like missing imports or incorrect variable access.
- Evidence anchors:
  - [abstract] "JSON format yields more executable plans than code generation while maintaining comparable tool selection performance"
  - [section] "LLMs perform comparably on tool-F1 with JSON-format and code generation, but most models produce more executable plans with JSON-format generation"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

## Foundational Learning

- Concept: Multi-modal tasks and tool use
  - Why needed here: Understanding the problem domain is crucial for designing effective planning agents that can handle diverse inputs and utilize various tools.
  - Quick check question: What are the three categories of tools used in the m&m's benchmark, and how do they differ in terms of input/output types?

- Concept: Large Language Models (LLMs) as planners
  - Why needed here: The paper evaluates different LLMs as planners, so understanding their capabilities and limitations is essential for interpreting the results.
  - Quick check question: How do step-by-step and multi-step planning strategies differ in terms of LLM output, and what are the potential advantages of each approach?

- Concept: Feedback mechanisms in planning
  - Why needed here: The paper investigates the impact of different feedback types on planning performance, so understanding how these mechanisms work is crucial for interpreting the results.
  - Quick check question: What are the three types of feedback used in the m&m's benchmark, and how do they contribute to improving plan executability?

## Architecture Onboarding

- Component map: User query → LLM planner → Parser → Verifier → Executor → Tool execution → Result
- Critical path: User query → LLM planner → Parser → Verifier → Executor → Tool execution → Result
- Design tradeoffs:
  - Planning strategy: Multi-step vs. step-by-step (tradeoff between upfront reasoning and iterative refinement)
  - Plan format: JSON vs. code (tradeoff between structure and flexibility)
  - Feedback: Parsing, verification, execution (tradeoff between error catching and computational cost)
- Failure signatures:
  - Tool-F1 low: LLM struggles with tool selection
  - Argname-F1 low: LLM has difficulty with argument naming
  - Pass rate low: LLM generates plans with execution errors
- First 3 experiments:
  1. Compare multi-step vs. step-by-step planning with JSON format and parsing feedback only
  2. Evaluate the impact of verification feedback on argname-F1 and pass rate
  3. Test the effect of execution feedback on pass rate and tool-F1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do planning agents perform when tasks require dynamic task plans that adapt based on intermediate outputs, as opposed to the sequential plans considered in m&m's?
- Basis in paper: [explicit] The paper acknowledges that m&m's only considers sequential task plans and mentions that some tasks might require dynamic task plans depending on the output for one subtask, which would require a more complex tool graph sampling procedure.
- Why unresolved: The current benchmark and evaluation methodology are designed for sequential plans. Dynamic planning introduces complexities in tool graph sampling, plan generation, and evaluation that have not been explored.
- What evidence would resolve it: Developing and evaluating a benchmark that includes dynamic task plans, comparing performance of different LLMs on these tasks, and analyzing how feedback mechanisms and planning strategies adapt to dynamic scenarios.

### Open Question 2
- Question: How do different prompt styles, beyond the direct and ReACT-style prompts used in the study, affect the performance of planning agents on multi-step multi-modal tasks?
- Basis in paper: [explicit] The paper states that it uses direct and ReACT-style prompting and excludes more sophisticated prompting strategies such as tree-of-thoughts prompting, leaving this for future work.
- Why unresolved: The study focuses on a specific set of prompt styles and does not explore the full range of available prompting techniques that could potentially improve planning performance.
- What evidence would resolve it: Systematic evaluation of various prompt styles, including tree-of-thoughts and other advanced techniques, on the m&m's benchmark, comparing their impact on tool selection, tool invocation, and overall plan executability.

### Open Question 3
- Question: How does the performance of multi-modal planners, which can process and generate inputs/outputs across multiple modalities, compare to LLM planners on the m&m's benchmark?
- Basis in paper: [explicit] The paper mentions that evaluation of multi-modal planners is left to future work, focusing on LLM planners due to their advanced abilities.
- Why unresolved: The study exclusively evaluates LLM planners and does not assess how planners with integrated multi-modal processing capabilities would perform on the same tasks.
- What evidence would resolve it: Implementing and evaluating multi-modal planners on the m&m's benchmark, comparing their performance in terms of tool selection, tool invocation, and plan executability against the LLM planners studied.

## Limitations
- Benchmark focuses on single-tool-per-step plans, not capturing complex tool composition scenarios
- Evaluation uses closed-source LLMs (GPT-4, GPT-3.5) making direct performance comparisons challenging
- Findings may not generalize to scenarios requiring mixed-initiative planning or dynamic task adaptation

## Confidence
- High confidence: Multi-step planning consistently outperforms step-by-step planning across all metrics and models
- Medium confidence: JSON format superiority for executable plans, as results may depend on specific tool implementations
- Low confidence: Generalizability of findings to scenarios with tool composition or mixed-initiative planning

## Next Checks
1. Test benchmark performance on tasks requiring tool composition to validate single-tool limitation
2. Evaluate impact of different prompt engineering strategies on multi-step planning effectiveness
3. Assess benchmark performance with additional feedback mechanisms combining verification and execution signals