---
ver: rpa2
title: Cost-Efficient Large Language Model Serving for Multi-turn Conversations with
  CachedAttention
arxiv_id: '2403.19708'
source_url: https://arxiv.org/abs/2403.19708
tags:
- cache
- caches
- time
- attentionstore
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AttentionStore, a system designed to improve
  the efficiency of large language model (LLM) serving for multi-turn conversations.
  The key problem addressed is the repetitive computation of key-value (KV) caches
  across conversation turns, which leads to high serving costs.
---

# Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention

## Quick Facts
- arXiv ID: 2403.19708
- Source URL: https://arxiv.org/abs/2403.19708
- Authors: Bin Gao; Zhuomin He; Puru Sharma; Qingxuan Kang; Djordje Jevdjic; Junbo Deng; Xingkun Yang; Zhou Yu; Pengfei Zuo
- Reference count: 40
- Primary result: AttentionStore reduces LLM serving costs by up to 70% for multi-turn conversations

## Executive Summary
AttentionStore addresses the computational inefficiency of serving large language models for multi-turn conversations by implementing a hierarchical key-value (KV) cache management system. The system eliminates redundant KV cache computations across conversation turns through intelligent caching strategies that leverage different memory/storage mediums. By overlapping cache access with GPU computation and employing sophisticated fetching/eviction schemes, AttentionStore significantly improves both latency and throughput for conversational AI applications.

## Method Summary
The paper introduces AttentionStore, a system that maintains hierarchical KV caching across multiple memory/storage layers to serve multi-turn conversations more efficiently. The approach involves layer-wise pre-loading of KV caches to overlap access with GPU computation, asynchronous saving schemes to prevent blocking, and scheduler-aware fetching to ensure optimal cache placement. A key innovation is the decoupling of positional encoding from KV caches to prevent context window overflow invalidation. The system also implements compression techniques to reduce storage overhead while maintaining performance.

## Key Results
- Reduces time to first token (TTFT) by up to 87% for multi-turn conversations
- Improves prompt prefilling throughput by up to 7.8x compared to baseline approaches
- Achieves up to 70% reduction in end-to-end inference cost through efficient cache management

## Why This Works (Mechanism)
The system works by maintaining persistent KV caches across conversation turns, eliminating redundant computations that typically occur in standard LLM serving. The hierarchical cache structure allows for intelligent placement of frequently accessed KV caches in faster memory layers while storing less frequently used caches in cost-effective storage. Layer-wise pre-loading ensures that KV cache access overlaps with GPU computation, hiding latency. The asynchronous saving scheme prevents cache writes from blocking inference operations, while the scheduler-aware fetching ensures that incoming requests can immediately access relevant cached data without waiting.

## Foundational Learning

**Hierarchical Memory Management**: Organizing storage across multiple tiers (e.g., HBM, DRAM, SSD) to balance performance and cost - needed to optimize cache placement based on access patterns; quick check: verify cache hit rates across different memory tiers.

**Attention Mechanism in Transformers**: The process of computing attention scores using key-value pairs for each token - fundamental to understanding why KV caching is important; quick check: confirm that KV cache sizes scale linearly with context length.

**Positional Encoding Decoupling**: Separating positional information from KV values to prevent cache invalidation when context window changes - needed to maintain cache validity across varying conversation lengths; quick check: verify that positional encoding can be recomputed efficiently when needed.

## Architecture Onboarding

Component map: User Request -> Scheduler -> KV Cache Manager -> GPU Computation -> Response
Critical path: Request reception → Cache lookup → (Cache miss: Computation → Cache store) → Response generation
Design tradeoffs: Higher cache hit rates vs. increased memory usage; faster access vs. storage costs
Failure signatures: Cache misses causing increased latency; cache eviction leading to redundant computations
First experiments: 1) Measure baseline TTFT without caching, 2) Test cache hit rates with varying conversation histories, 3) Benchmark end-to-end cost with different cache sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 7B parameter model on A100 GPUs, raising questions about scalability to larger models
- Complexity of hierarchical cache management may introduce practical implementation challenges
- Performance under concurrent multi-tenant workloads not thoroughly explored

## Confidence

High confidence in architectural design and theoretical benefits of hierarchical KV caching
Medium confidence in reported performance improvements due to limited evaluation scope
Low confidence in system behavior under production-scale, multi-tenant workloads

## Next Checks

1. Evaluate AttentionStore's performance across a range of model sizes (1B-70B parameters) and architectures to verify scalability claims
2. Test the system under concurrent multi-user workloads with varying request patterns to assess real-world performance and cache hit rates
3. Benchmark the end-to-end latency and throughput on different GPU types (H100, V100) and CPU-memory configurations to validate hardware portability