---
ver: rpa2
title: Dynamic Multi-Agent Orchestration and Retrieval for Multi-Source Question-Answer
  Systems using Large Language Models
arxiv_id: '2412.17964'
source_url: https://arxiv.org/abs/2412.17964
tags:
- data
- contract
- information
- system
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a multi-agent orchestration methodology combining\
  \ Retrieval-Augmented Generation (RAG), Text-to-SQL, and dynamic prompt engineering\
  \ to enable accurate, context-aware question-answering across structured and unstructured\
  \ data sources. Agents\u2014Router, RAG, SQL, and Graph\u2014dynamically route queries\
  \ and adapt responses based on the data type and user intent."
---

# Dynamic Multi-Agent Orchestration and Retrieval for Multi-Source Question-Answer Systems using Large Language Models

## Quick Facts
- arXiv ID: 2412.17964
- Source URL: https://arxiv.org/abs/2412.17964
- Authors: Antony Seabra; Claudio Cavalcante; Joao Nepomuceno; Lucas Lago; Nicolaas Ruberg; Sergio Lifschitz
- Reference count: 1
- One-line primary result: Multi-agent system combining RAG, Text-to-SQL, and dynamic prompt engineering achieves accurate, context-aware Q&A across structured and unstructured contract data sources

## Executive Summary
This paper presents a multi-agent orchestration methodology that integrates Retrieval-Augmented Generation (RAG), Text-to-SQL, and dynamic prompt engineering to enable accurate, context-aware question-answering across structured and unstructured data sources. The system employs specialized agents—Router, RAG, SQL, and Graph—that dynamically route queries and adapt responses based on data type and user intent. Evaluated with IT contract specialists on 75 contracts, the system demonstrated strong performance for direct questions and improved relevance for indirect ones, with users praising seamless integration of document and database data and automated visual summaries.

## Method Summary
The system implements a four-agent architecture where the Router Agent uses pattern matching to classify queries and route them to either the RAG Agent (for unstructured document retrieval) or SQL Agent (for structured database queries). The RAG Agent employs ChromaDB vector storage with text-davinci-002 embeddings (1536 dimensions) and cosine similarity for semantic search, while the SQL Agent translates natural language to executable SQL commands. Dynamic prompt engineering tailors LLM responses in real-time based on agent type and query context. The Graph Agent generates visual representations when applicable. The system was evaluated using 75 contract PDF documents and a structured SQLite database, with IT contract specialists providing qualitative feedback on accuracy and relevance.

## Key Results
- Strong performance on direct questions answered from PDFs and metadata
- Improved relevance for indirect questions requiring database information
- Users praised seamless integration of document and database data with automated visual summaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Router Agent dynamically routes queries to RAG or SQL agents based on query type, ensuring accurate data retrieval from the most appropriate source.
- Mechanism: Router Agent uses pattern matching (regular expressions) to detect whether a query involves unstructured document content (e.g., contract clauses) or structured database data (e.g., contract numbers, dates). It then directs the query to the RAG Agent or SQL Agent accordingly.
- Core assumption: Query structure and keywords reliably indicate the data source type needed for accurate answers.
- Evidence anchors:
  - [abstract] "Agents—Router, RAG, SQL, and Graph—dynamically route queries and adapt responses based on the data type and user intent."
  - [section 3.3] "The Router Agent uses regular expressions to identify keywords, patterns, or structures within the query."
  - [corpus] Weak - No corpus paper explicitly discusses query-type-based routing in multi-agent systems.
- Break condition: If queries are ambiguous or contain mixed data source requirements, the Router Agent may misroute, leading to incomplete or irrelevant responses.

### Mechanism 2
- Claim: Dynamic Prompt Engineering tailors LLM prompts in real time based on the query context and data source, improving response accuracy and relevance.
- Mechanism: Prompts are constructed dynamically for each agent type (RAG, SQL, Graph), embedding context-specific instructions and constraints to guide the LLM toward the desired output format and content.
- Core assumption: LLM responses can be significantly improved by carefully crafted, context-aware prompts that specify task, data source, and expected output.
- Evidence anchors:
  - [abstract] "To further improve accuracy and contextual relevance, we employ dynamic prompt engineering, which adapts in real time to query-specific contexts."
  - [section 3.4] "We utilize dynamic prompts that adapt according to the specific agent handling the query."
  - [corpus] Weak - No corpus paper explicitly discusses dynamic prompt adaptation in multi-agent orchestration.
- Break condition: If prompts are poorly constructed or lack sufficient specificity, the LLM may generate irrelevant or hallucinated responses.

### Mechanism 3
- Claim: Hybrid retrieval combining RAG for unstructured text and Text-to-SQL for structured data enables comprehensive, accurate answers across heterogeneous data sources.
- Mechanism: RAG retrieves semantically relevant text chunks from vectorized document sections, while Text-to-SQL translates natural language queries into precise SQL commands for structured database retrieval. Both approaches are integrated within the agent-based architecture.
- Core assumption: Combining semantic similarity-based retrieval with exact structured query execution provides more accurate and complete answers than either method alone.
- Evidence anchors:
  - [abstract] "This methodology leverages specialized agents-such as SQL agents, Retrieval-Augmented Generation (RAG) agents, and router agents - that dynamically select the most appropriate retrieval strategy based on the nature of each query."
  - [section 2.3] "Text-to-SQL is a powerful technique that bridges the gap between natural language queries and relational database systems by converting user inputs in plain text into executable SQL commands."
  - [corpus] Weak - No corpus paper explicitly discusses hybrid RAG + Text-to-SQL retrieval in multi-agent systems.
- Break condition: If document chunking or SQL translation fails, the system may retrieve irrelevant or incorrect data, leading to inaccurate responses.

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: Vector embeddings enable semantic search over unstructured text documents, allowing RAG to retrieve relevant text chunks based on meaning rather than exact keyword matching.
  - Quick check question: What is the main advantage of using cosine similarity over Euclidean distance for comparing text embeddings in RAG systems?
- Concept: Text-to-SQL translation
  - Why needed here: Text-to-SQL allows natural language queries to be converted into precise SQL commands, enabling accurate retrieval of structured data from relational databases.
  - Quick check question: What is the primary challenge in Text-to-SQL systems when dealing with complex, multi-table queries?
- Concept: Agent-based orchestration
  - Why needed here: Multi-agent orchestration enables dynamic routing of queries to specialized agents (Router, RAG, SQL, Graph), ensuring each query is handled by the most appropriate processing module.
  - Quick check question: How does the Router Agent determine which agent (RAG or SQL) should handle a given query?

## Architecture Onboarding

- Component map: User Interface (Streamlit) -> Router Agent -> RAG Agent / SQL Agent -> Vectorstore (ChromaDB) / Structured Database (SQLite) -> LLM with Dynamic Prompts -> Graph Agent (visual representation)
- Critical path:
  1. User submits query via UI
  2. Router Agent analyzes query and routes to RAG or SQL agent
  3. RAG Agent retrieves relevant text chunks from vectorstore or SQL Agent executes translated SQL query
  4. LLM generates response based on retrieved data and dynamic prompt
  5. Graph Agent may generate visual representation if applicable
  6. Response is displayed to user
- Design tradeoffs:
  - Chunking strategy: Section-based vs. token-based chunking affects retrieval accuracy and context preservation
  - Vector dimensionality: Higher dimensions (1536) provide better semantic representation but require more computational resources
  - Embedding model choice: text-davinci-002 balances accuracy and context understanding but may be resource-intensive
- Failure signatures:
  - Router Agent misrouting: Queries receive responses from wrong data source
  - RAG retrieval failure: Retrieved text chunks are irrelevant or lack necessary context
  - SQL translation error: Generated SQL queries fail to execute or return incorrect data
  - Prompt engineering failure: LLM generates irrelevant or hallucinated responses
- First 3 experiments:
  1. Test Router Agent routing accuracy with various query types (document vs. database)
  2. Evaluate RAG retrieval precision using different chunking strategies and embedding models
  3. Validate SQL Agent translation accuracy with complex natural language queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system perform in terms of accuracy and relevance when handling queries that require retrieving information from multiple documents simultaneously?
- Basis in paper: [inferred] The paper mentions that the system handles both structured and unstructured data, but it does not provide specific details on how it performs when queries require information from multiple documents at once.
- Why unresolved: The paper focuses on the system's performance with individual queries and does not explore scenarios where multiple documents need to be queried simultaneously.
- What evidence would resolve it: Conducting experiments with queries that require information from multiple documents and evaluating the system's accuracy and relevance in these cases.

### Open Question 2
- Question: What are the specific challenges and limitations of using cosine similarity as the similarity metric in the vectorstore, and how do these affect the system's performance?
- Basis in paper: [explicit] The paper mentions that cosine similarity is used as the similarity metric, but it does not discuss the challenges or limitations associated with this choice.
- Why unresolved: The paper does not provide a detailed analysis of the impact of using cosine similarity on the system's performance, especially in terms of retrieval accuracy and relevance.
- What evidence would resolve it: Conducting experiments comparing the performance of cosine similarity with other similarity metrics and analyzing the impact on retrieval accuracy and relevance.

### Open Question 3
- Question: How does the system handle queries that involve complex relationships between data in structured databases and unstructured documents?
- Basis in paper: [inferred] The paper mentions that the system integrates information from both structured and unstructured data sources, but it does not provide details on how it handles queries that involve complex relationships between these data types.
- Why unresolved: The paper does not explore the system's capability to handle complex queries that require correlating information from both structured and unstructured sources.
- What evidence would resolve it: Conducting experiments with complex queries that involve relationships between structured and unstructured data and evaluating the system's ability to accurately retrieve and correlate the required information.

## Limitations

- Router Agent accuracy depends on pattern-matching reliability, which may struggle with ambiguous or mixed-content queries
- Evaluation limited to IT contract specialists with 75 contracts, limiting generalizability to other domains
- Key implementation details (chunking strategy, regex patterns) not fully specified, creating reproducibility challenges

## Confidence

- **High confidence**: The core multi-agent architecture combining RAG and Text-to-SQL approaches is technically sound and well-grounded in established LLM practices.
- **Medium confidence**: Dynamic prompt engineering effectiveness is supported by the framework but lacks extensive empirical validation.
- **Medium confidence**: The hybrid retrieval approach (RAG + Text-to-SQL) shows promise but requires testing across diverse query types and data sources.

## Next Checks

1. **Router Agent accuracy testing**: Systematically evaluate routing performance across ambiguous queries that combine document and database requirements, measuring misclassification rates.
2. **Cross-domain generalization**: Test the system with contracts from different industries (legal, healthcare, finance) to assess adaptability beyond IT contracts.
3. **Performance scaling analysis**: Evaluate system response times and accuracy as document volume increases from 75 to 1000+ contracts, identifying potential bottlenecks in the multi-agent architecture.