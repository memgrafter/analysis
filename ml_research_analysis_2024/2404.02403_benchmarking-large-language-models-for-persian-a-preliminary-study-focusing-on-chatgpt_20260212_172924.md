---
ver: rpa2
title: 'Benchmarking Large Language Models for Persian: A Preliminary Study Focusing
  on ChatGPT'
arxiv_id: '2404.02403'
source_url: https://arxiv.org/abs/2404.02403
tags:
- persian
- tasks
- language
- gpt-3
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmarking study of large
  language models (LLMs) for the Persian language, focusing on ChatGPT (GPT-3.5-turbo)
  but also including GPT-4 and OpenChat-3.5. The study evaluates these models across
  diverse tasks categorized into classic NLP, reasoning, and knowledge-based domains.
---

# Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT

## Quick Facts
- arXiv ID: 2404.02403
- Source URL: https://arxiv.org/abs/2404.02403
- Reference count: 0
- Primary result: GPT-4 outperforms GPT-3.5 and OpenChat-3.5 on Persian language tasks, especially in reasoning and knowledge domains

## Executive Summary
This paper presents a comprehensive benchmarking study of large language models (LLMs) for the Persian language, focusing on ChatGPT (GPT-3.5-turbo) but also including GPT-4 and OpenChat-3.5. The study evaluates these models across diverse tasks categorized into classic NLP, reasoning, and knowledge-based domains. The authors introduce two new benchmarks for elementary school math questions and entrance exam problems due to limited existing Persian datasets. The primary finding is that while GPT-4 excels in tasks requiring reasoning abilities and broad general knowledge, it often lags behind smaller pre-trained models fine-tuned for specific tasks. GPT-3.5 performance improves when test sets are translated to English before input. GPT-4 emerges as the most effective model, followed by GPT-3.5 and OpenChat-3.5. The study highlights significant potential for enhancing LLM performance in Persian, considering its unique linguistic attributes such as distinct alphabet and writing styles.

## Method Summary
The study evaluates GPT-3.5, GPT-4, and OpenChat-3.5 on Persian language tasks using few-shot prompting with 0, 1, and 3 shots. Models are tested on classic NLP tasks (sentiment analysis, emotion recognition, NER, reading comprehension, translation, textual entailment, multiple-choice QA), reasoning tasks (elementary school math, entrance exam problems), and knowledge-based tasks. The authors use existing Persian datasets (ParsiNLU, ArmanEmo, ArmanNER, MIZAN, ConjNLI) and create new benchmarks for math problems. Prompts are designed in both Persian and English, and a subset of 200 samples per task is used for evaluation due to cost and time constraints.

## Key Results
- GPT-4 outperforms GPT-3.5 and OpenChat-3.5 across all Persian language tasks
- Translating Persian test samples to English improves GPT-3.5's performance in most tasks
- Few-shot learning consistently enhances GPT-4's performance, but not GPT-3.5 or OpenChat-3.5
- GPT-4 excels in reasoning and knowledge-based tasks but may lag behind fine-tuned models on specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 outperforms GPT-3.5 and OpenChat-3.5 across Persian NLP tasks, especially in reasoning and knowledge-based domains.
- Mechanism: GPT-4's larger parameter size and more extensive training data allow it to capture complex linguistic patterns and reasoning skills, leading to better performance on tasks requiring these abilities.
- Core assumption: The improvements in GPT-4 are due to architectural enhancements rather than just scale.
- Evidence anchors:
  - [abstract] "GPT-4 emerges as the most effective model, followed by GPT-3.5 and OpenChat-3.5."
  - [section 5.1] "GPT-4 consistently outperforms both GPT-3.5 and OpenChat-3.5 across all our Persian language tasks"
  - [corpus] Weak evidence; the corpus provides related works but not direct comparison data for this mechanism.
- Break condition: If the task requires highly specialized knowledge in Persian literature or culture, where smaller fine-tuned models might have an advantage.

### Mechanism 2
- Claim: Translating Persian test samples to English improves GPT-3.5's performance in most tasks.
- Mechanism: GPT-3.5, being trained primarily on English data, has a stronger grasp of English linguistic patterns and reasoning, leading to better performance when tasks are presented in English.
- Core assumption: The model's performance degradation is primarily due to language differences rather than task complexity.
- Evidence anchors:
  - [abstract] "GPT-3.5 performance improves when test sets are translated to English before input."
  - [section 5.2] "Translating examples into English appears to improve performance in most tasks when testing GPT-3.5."
  - [corpus] Weak evidence; the corpus provides related works on multilingual evaluation but not specific to this translation effect.
- Break condition: If the task requires deep understanding of Persian-specific cultural or linguistic nuances that are lost in translation.

### Mechanism 3
- Claim: Few-shot learning consistently enhances GPT-4's performance, but not GPT-3.5 or OpenChat-3.5.
- Mechanism: GPT-4's larger model capacity allows it to better learn from few examples and adapt to task-specific patterns, while smaller models struggle with this adaptation.
- Core assumption: The few-shot learning effect is due to model capacity rather than differences in training data or optimization.
- Evidence anchors:
  - [abstract] "Unlike GPT-4, in most cases of GPT-3.5 experiments, increasing the number of shots does not lead to better results"
  - [section 5.1] "We have also observed that few-shot learning consistently enhances the performance of GPT-4, resulting in improved results as the number of shots increases. However, this effect is not notably observed in GPT-3.5 and OpenChat-3.5"
  - [corpus] Weak evidence; the corpus provides related works on few-shot learning but not specific to this mechanism.
- Break condition: If the task is simple enough that zero-shot performance is already optimal, or if the few-shot examples are misleading or confusing.

## Foundational Learning

- Concept: Understanding of Persian language and its unique attributes (distinct alphabet, writing styles, dialects)
  - Why needed here: The study focuses on evaluating LLMs for Persian, a language with unique linguistic features that may affect model performance.
  - Quick check question: Can you name two unique attributes of the Persian language that might affect LLM performance?

- Concept: Familiarity with various NLP tasks (sentiment analysis, named entity recognition, textual entailment, machine translation, etc.)
  - Why needed here: The study evaluates LLMs across a diverse set of NLP tasks, requiring understanding of each task's requirements and evaluation metrics.
  - Quick check question: What is the difference between sentiment analysis and emotion recognition in NLP?

- Concept: Knowledge of large language models (LLMs) and their capabilities (few-shot learning, zero-shot learning, multilingual support)
  - Why needed here: The study compares different LLMs (GPT-3.5, GPT-4, OpenChat-3.5) and their performance across tasks, requiring understanding of their strengths and limitations.
  - Quick check question: What is the main difference between zero-shot and few-shot learning in the context of LLMs?

## Architecture Onboarding

- Component map:
  - Data collection and preprocessing: Gathering Persian NLP datasets, creating new benchmarks for reasoning tasks
  - Model evaluation: Running experiments with GPT-3.5, GPT-4, and OpenChat-3.5 on various tasks
  - Prompt engineering: Designing and testing prompts in Persian and English, with different shot settings
  - Analysis and comparison: Comparing LLM performance with state-of-the-art task-specific models

- Critical path:
  1. Collect and preprocess Persian NLP datasets
  2. Design and implement prompt templates for each task
  3. Run experiments with different models, prompt languages, and shot settings
  4. Analyze results and compare with state-of-the-art models
  5. Identify patterns and draw conclusions about LLM performance in Persian

- Design tradeoffs:
  - Using a subset of 200 samples per task for cost and time efficiency vs. using the full test sets for more comprehensive evaluation
  - Focusing on GPT-3.5 as the primary model vs. including other GPT models and open-source alternatives for a broader comparison
  - Creating new benchmarks for reasoning tasks vs. relying solely on existing datasets

- Failure signatures:
  - GPT-3.5 performs poorly on tasks with Persian prompts but improves with English prompts, indicating language dependency
  - Few-shot learning does not improve GPT-3.5 or OpenChat-3.5 performance, suggesting limited adaptation capabilities
  - GPT-4 outperforms other models in reasoning and knowledge tasks but may struggle with highly specialized Persian content

- First 3 experiments:
  1. Evaluate GPT-3.5 on sentiment analysis with Persian and English prompts, in zero-shot and few-shot settings, comparing with a fine-tuned sentiment analysis model
  2. Test GPT-4 on textual entailment using the ParsiNLU dataset with Persian and English prompts, comparing with the best performing fine-tuned model
  3. Assess OpenChat-3.5 on named entity recognition using the ArmanNER dataset, comparing its performance with a fine-tuned NER model and GPT-3.5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on Persian tasks when prompted in other low-resource languages besides English?
- Basis in paper: [inferred] The paper shows that English prompts generally yield better results than Persian prompts for GPT-3.5, suggesting that the model may perform better when prompted in other languages with more available training data.
- Why unresolved: The study only tested English and Persian prompts, leaving the performance of other low-resource language prompts unexplored.
- What evidence would resolve it: Evaluating GPT-3.5's performance on Persian tasks using prompts in various other languages and comparing the results to English and Persian prompts.

### Open Question 2
- Question: Can fine-tuning GPT-4 specifically for Persian tasks improve its performance compared to the zero-shot/few-shot results presented in the paper?
- Basis in paper: [explicit] The paper mentions that GPT-4 outperforms GPT-3.5 and OpenChat-3.5 across all tasks, but it does not explore the potential improvements from fine-tuning GPT-4 on Persian-specific data.
- Why unresolved: The study focuses on evaluating the models in their zero-shot/few-shot settings without exploring the benefits of fine-tuning for the Persian language.
- What evidence would resolve it: Fine-tuning GPT-4 on Persian-specific datasets and comparing its performance to the zero-shot/few-shot results presented in the paper.

### Open Question 3
- Question: How do LLMs handle Persian dialects and regional variations in language use?
- Basis in paper: [explicit] The paper mentions that Persian presents a unique challenge due to its multiple writing styles and dialects, but it does not specifically evaluate the models' performance on these variations.
- Why unresolved: The study uses standard Persian datasets without explicitly testing the models' ability to handle different dialects and regional variations.
- What evidence would resolve it: Evaluating the models' performance on Persian tasks using datasets that include various dialects and regional variations, and comparing the results to standard Persian datasets.

## Limitations

- The study relies on a subset of 200 samples per task for evaluation, which may not fully represent the diversity and complexity of the full test sets.
- The creation of new benchmarks for elementary school math and entrance exam problems introduces potential biases based on the specific selection criteria and sources used.
- The translation of test samples to English for GPT-3.5 evaluation may introduce artifacts or lose important contextual information, affecting the validity of comparisons.

## Confidence

High Confidence: The finding that GPT-4 consistently outperforms GPT-3.5 and OpenChat-3.5 across Persian language tasks is well-supported by the experimental results and aligns with the known capabilities of these models.

Medium Confidence: The observation that translating Persian test samples to English improves GPT-3.5's performance is supported by the data but may be influenced by factors such as sample selection and translation quality.

Low Confidence: The claim that few-shot learning consistently enhances GPT-4's performance but not GPT-3.5 or OpenChat-3.5 is based on limited observations and may not generalize to all task types or shot configurations.

## Next Checks

1. Expand the evaluation to include the full test sets for each task, where feasible, to assess the impact of sample size on model performance and generalization.

2. Conduct a detailed analysis of translation artifacts by comparing model performance on original Persian prompts, professionally translated English prompts, and machine-translated English prompts for a subset of tasks.

3. Investigate the impact of different shot configurations (e.g., 2 shots, 5 shots) on model performance to better understand the relationship between few-shot learning and model capabilities across various task types.