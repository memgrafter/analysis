---
ver: rpa2
title: Learning Parameter Sharing with Tensor Decompositions and Sparsity
arxiv_id: '2411.09816'
source_url: https://arxiv.org/abs/2411.09816
tags:
- sparsity
- parameter
- fips
- sharing
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fine-grained Parameter Sharing (FiPS), a
  novel compression algorithm that leverages parameter sharing, tensor decomposition,
  and sparsity to effectively compress large-scale Vision Transformers and Large Language
  Models. FiPS employs a shared base and sparse factors to represent neurons across
  MLP modules, with initialization guided by Singular Value Decomposition (SVD) and
  optimization through block-wise reconstruction error minimization.
---

# Learning Parameter Sharing with Tensor Decompositions and Sparsity

## Quick Facts
- arXiv ID: 2411.09816
- Source URL: https://arxiv.org/abs/2411.09816
- Reference count: 28
- This paper introduces Fine-grained Parameter Sharing (FiPS), a novel compression algorithm that leverages parameter sharing, tensor decomposition, and sparsity to effectively compress large-scale Vision Transformers and Large Language Models.

## Executive Summary
This paper presents Fine-grained Parameter Sharing (FiPS), a compression algorithm that combines parameter sharing, tensor decomposition, and sparsity to reduce the parameter count of MLP modules in Vision Transformers and Large Language Models. FiPS uses a shared dense basis matrix combined with sparse projection factors to represent neurons across layers, initialized via SVD and optimized through block-wise reconstruction error minimization. The method achieves 50-75% parameter reduction for ViTs and 40-50% for LLMs while maintaining accuracy within 1% of original models.

## Method Summary
FiPS compresses MLP modules by concatenating weight matrices along their longer dimension, performing SVD to obtain a shared basis U and sparse factors V, then optimizing V through gradual magnitude pruning. The algorithm first initializes U and V using SVD on concatenated MLP weights, then minimizes reconstruction error between original and compressed activations in a block-wise manner. After compression, an optional end-to-end fine-tuning stage can further improve performance. The method employs either static sparsity or dynamic sparse training (RigL) during optimization, with structured sparsity patterns (e.g., 2:4) available to enhance memory efficiency and computational speed.

## Key Results
- Achieves 50-75% parameter reduction for DeiT-B and Swin-L Vision Transformers
- Maintains ViT model accuracy within 1% point of original
- Reduces LLM parameters by 40-50% with negligible perplexity degradation
- Outperforms Adaptive Atomic Feature Mimicking (AAFM) and Global Feature Mimicking (GFM) methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FiPS achieves high compression rates while preserving accuracy by sharing neurons across MLP layers via a shared basis matrix combined with sparse projection factors.
- Mechanism: A dense shared basis U (Rd×r) is combined with sparse factors V (r×p) so that each neuron in the MLP can be reconstructed as a linear combination of basis neurons. Sparsity in V allows each layer to use only a subset of the basis, reducing total parameters.
- Core assumption: MLP neurons across layers are redundant enough that a shared basis can represent them without significant accuracy loss.
- Evidence anchors:
  - [abstract] "FiPS employs a shared base and sparse factors to represent neurons across multi-layer perceptron (MLP) modules"
  - [section 2] "only r columns of W will contain unique values. These r unique neurons are represented using a lookup table (basis matrix) U ∈ Rd×r"
  - [corpus] weak; no direct matches for FiPS or shared basis neuron representation
- Break condition: If neurons across layers are not sufficiently correlated, reconstruction error increases and accuracy degrades.

### Mechanism 2
- Claim: Optimal compression is achieved by concatenating MLP weight matrices along the longer dimension before SVD.
- Mechanism: Concatenating matrices along the longer dimension increases rank capture efficiency and reduces reconstruction error, as the larger matrix contains more redundancy amenable to low-rank factorization.
- Core assumption: The longer dimension of concatenated weight matrices has higher redundancy amenable to pruning.
- Evidence anchors:
  - [section 2.2] "Our experiments reveal that concatenating weights along the longer dimension consistently yields the lowest reconstruction errors, particularly at higher sparsity levels"
  - [section 2.1] "optimal reconstruction errors are achieved at sparsity levels between 60% and 80%, particularly when sparsity is applied to the larger factor matrix V"
  - [corpus] weak; no direct matches for concatenation dimension experiments
- Break condition: If MLP layers have highly distinct patterns, longer-axis concatenation may not capture unique features effectively.

### Mechanism 3
- Claim: Gradual magnitude pruning (GMP) with global pruning outperforms other sparsification methods for optimizing the sparse factors.
- Mechanism: GMP starts with moderate sparsity and progressively increases it, allowing the model to adapt gradually. Global pruning ensures consistent sparsity across all projection matrices, optimizing parameter allocation.
- Core assumption: Gradual sparsity increase with global consistency yields better parameter allocation than local or static methods.
- Evidence anchors:
  - [section 4.1] "GMP-based FiPS consistently outperformed alternatives like GFM, while demanding less computational and memory resources"
  - [section 5] "we employ the dynamic sparse training method, RigL, during this stage as it performs slightly better than Static Sparsity"
  - [corpus] weak; no direct matches for GMP or global pruning comparisons
- Break condition: If sparsity schedule is too aggressive or global constraints too rigid, the model may lose critical representational capacity.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is used to initialize the shared basis U and sparse factors V from concatenated MLP weight matrices, providing an optimal low-rank approximation.
  - Quick check question: What is the relationship between SVD rank and the number of unique neurons preserved in the compressed model?

- Concept: Tensor Decomposition
  - Why needed here: Tensor decomposition generalizes matrix factorization to higher-order structures, enabling efficient compression of multi-layer MLP modules by capturing shared patterns.
  - Quick check question: How does low-rank tensor decomposition differ from simple matrix factorization in the context of parameter sharing?

- Concept: Sparsity Patterns (Structured vs Unstructured)
  - Why needed here: Different sparsity patterns affect memory efficiency, computational speed, and model accuracy. Structured sparsity enables hardware acceleration while unstructured sparsity may yield better accuracy.
  - Quick check question: What trade-offs exist between structured sparsity (e.g., 2:4 patterns) and unstructured sparsity in terms of speed and accuracy?

## Architecture Onboarding

- Component map: MLP weight concatenation -> SVD -> sparse factor initialization -> block-wise reconstruction error minimization -> (optional) end-to-end fine-tuning
- Critical path: MLP weight concatenation → SVD → sparse factor initialization → block-wise reconstruction error minimization → (optional) end-to-end fine-tuning
- Design tradeoffs: Higher rank U increases representational capacity but also parameter count; more aggressive sparsity reduces parameters but may hurt accuracy; structured sparsity improves speed but may limit compression
- Failure signatures: (1) Large reconstruction error between original and compressed activations, (2) accuracy drop beyond acceptable thresholds, (3) convergence issues during sparse training
- First 3 experiments:
  1. Verify SVD rank selection: Test different rank values on a small subset of MLP layers and measure reconstruction error
  2. Evaluate sparsity schedule: Compare GMP vs. static sparsity on a single MLP module and measure final accuracy
  3. Test concatenation strategy: Compare weight concatenation along longer vs. shorter dimensions and measure reconstruction error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FiPS performance scale with model size beyond the tested Gemma-2 and Llama-3 variants?
- Basis in paper: [inferred] The paper tests FiPS on Gemma-2 (2B, 9B) and Llama-3 (3B, 8B) models, but does not explore larger or smaller variants.
- Why unresolved: The paper does not provide experimental data on models significantly larger or smaller than those tested, leaving uncertainty about performance trends at extreme scales.
- What evidence would resolve it: Empirical results showing FiPS compression and accuracy metrics on models spanning a broader range of sizes, particularly multi-hundred-billion parameter models and smaller sub-billion parameter models.

### Open Question 2
- Question: What is the impact of FiPS on downstream task performance for LLMs beyond perplexity on WikiText-2?
- Basis in paper: [inferred] The paper evaluates FiPS on LLMs using perplexity on WikiText-2, but does not test performance on diverse downstream NLP tasks.
- Why unresolved: Perplexity is a narrow evaluation metric that may not capture FiPS's impact on practical LLM applications like question answering, summarization, or code generation.
- What evidence would resolve it: Fine-tuning experiments on multiple benchmark datasets (e.g., GLUE, SuperGLUE, SQuAD) showing FiPS-compressed LLMs maintain or improve task-specific performance relative to uncompressed baselines.

### Open Question 3
- Question: How does FiPS compare to quantization-based compression methods in terms of computational efficiency and accuracy trade-offs?
- Basis in paper: [explicit] The paper focuses on parameter sharing, tensor decomposition, and sparsity but does not directly compare FiPS to quantization techniques.
- Why unresolved: The paper establishes FiPS's effectiveness against other sparsity and decomposition methods but does not benchmark against quantization, which is a widely used compression approach.
- What evidence would resolve it: Head-to-head comparisons of FiPS versus mixed-precision quantization (e.g., 8-bit, 4-bit) on the same models, measuring accuracy retention, inference speed, and memory usage.

## Limitations

- The paper's claims about effectiveness across multiple architectures and transfer learning tasks are supported by limited empirical evidence, focusing primarily on ViT and LLM variants without extensive cross-domain validation.
- The superiority over AAFM and GFM is demonstrated under specific conditions (ViT-S/DeiT-B on ImageNet-1k) and may not generalize across all architectures and tasks.
- The paper does not address whether FiPS's advantages persist when applied to smaller models or those trained from scratch rather than pre-trained checkpoints.

## Confidence

**High Confidence**: The core mathematical framework combining SVD-based initialization with sparse factor optimization is well-established and the implementation details for MLP concatenation and block-wise optimization are clearly specified. The empirical compression rates (50-75% for ViTs, 40-50% for LLMs) and accuracy preservation within 1% are directly measured and reported.

**Medium Confidence**: The superiority claims over AAFM and GFM are based on comparisons under specific conditions (ViT-S/DeiT-B on ImageNet-1k) but may not generalize across all architectures and tasks. The assertion that structured sparsity provides both memory and computational benefits assumes hardware-level support that varies across platforms.

**Low Confidence**: The paper's claims about FiPS being "effective across multiple architectures and transfer learning tasks" are supported by limited empirical evidence, with most results focused on ViT and LLM variants without extensive cross-domain validation.

## Next Checks

1. **Cross-Architecture Robustness**: Apply FiPS to a diverse set of architectures including CNNs and hybrid models, measuring whether the 1% accuracy preservation claim holds across different architectural families.

2. **Dynamic vs Static Sparsity Trade-off**: Conduct controlled experiments comparing FiPS with static sparsity patterns to quantify the claimed computational and memory benefits across different hardware backends.

3. **Rank-Aggressiveness Calibration**: Systematically vary the rank parameter and sparsity levels to map the Pareto frontier of compression ratio versus accuracy degradation, identifying optimal configurations for different model families.