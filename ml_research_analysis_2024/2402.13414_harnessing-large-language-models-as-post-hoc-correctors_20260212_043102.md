---
ver: rpa2
title: Harnessing Large Language Models as Post-hoc Correctors
arxiv_id: '2402.13414'
source_url: https://arxiv.org/abs/2402.13414
tags:
- llmcorr
- llms
- knowledge
- predictions
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLMCORR, a training-free framework that leverages
  Large Language Models (LLMs) as post-hoc correctors to improve predictions made
  by arbitrary Machine Learning (ML) models. The method constructs a contextual knowledge
  database from the dataset's label information and the ML model's predictions on
  the validation set, then uses an embedding-based retrieval approach to extract relevant
  contextual knowledge for a given query data.
---

# Harnessing Large Language Models as Post-hoc Correctors

## Quick Facts
- arXiv ID: 2402.13414
- Source URL: https://arxiv.org/abs/2402.13414
- Authors: Zhiqiang Zhong; Kuangyu Zhou; Davide Mottin
- Reference count: 36
- One-line primary result: LLMCORR framework consistently improves ML model predictions by up to 39% in RMSE for regression tasks using LLMs as post-hoc correctors

## Executive Summary
This paper introduces LLMCORR, a training-free framework that leverages Large Language Models (LLMs) as post-hoc correctors to improve predictions made by arbitrary Machine Learning (ML) models. The method constructs a contextual knowledge database from the dataset's label information and the ML model's predictions on the validation set, then uses an embedding-based retrieval approach to extract relevant contextual knowledge for a given query data. Finally, it queries the LLM with the retrieved knowledge to refine the ML model's predictions.

Experimental results on molecule graph property prediction tasks and text analysis tasks show that LLMCORR consistently improves the performance of various ML models, with up to 39% improvement in RMSE for regression tasks and notable improvements in classification accuracy. The framework demonstrates that LLMs can effectively learn and apply error patterns from validation data to correct predictions on unseen test data without requiring any model retraining.

## Method Summary
LLMCORR is a training-free framework that improves ML model predictions by leveraging LLMs as post-hoc correctors. It constructs a contextual knowledge database using validation set labels and ML model predictions, then employs embedding-based retrieval to find relevant knowledge for each test instance. The framework generates prompts with retrieved knowledge and queries the LLM to suggest corrections, implementing a self-correction mechanism to prevent hallucinations when significant modifications are proposed.

## Key Results
- Up to 39% improvement in RMSE for regression tasks on molecule graph property prediction
- Consistent performance improvements across multiple datasets and ML model types
- GPT-3.5 outperforms GPT-4 in the LLMCORR framework, with the authors hypothesizing this may be due to interventions such as reinforcement learning through human feedback
- Optimal performance achieved with k=5 retrieved instances using the Jump selection strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMCORR improves ML predictions by using LLMs to summarize ML model error patterns from validation data and apply that knowledge to correct test predictions.
- Mechanism: The LLM learns correlations between ML model predictions and true labels on the validation set, then uses in-context learning to refine predictions on test data.
- Core assumption: The validation set contains representative error patterns that the LLM can learn and generalize from.
- Evidence anchors:
  - [abstract] "we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset"
  - [section 4.3] "we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels"
  - [corpus] Weak evidence - corpus neighbors discuss LLM-based correctors but don't specifically validate this mechanism
- Break condition: If validation set is too small or unrepresentative, the LLM cannot learn meaningful error patterns to apply to test data.

### Mechanism 2
- Claim: Embedding-based information retrieval enables efficient knowledge transfer from validation data to test instances despite LLM token constraints.
- Mechanism: Similarity-based retrieval selects top-k validation examples most relevant to each test query, forming targeted prompts for the LLM.
- Core assumption: Similar molecules have similar prediction error patterns that the LLM can exploit.
- Evidence anchors:
  - [section 4.2] "we introduce an embedding-based information retrieval approach, which can efficiently locate similar data likely to offer relevant insights with the target data"
  - [section 5.2] "selecting top-k data yields optimal results, with Jump outperforming Random"
  - [corpus] No direct evidence - corpus focuses on LLM explainability rather than retrieval mechanisms
- Break condition: If embedding space doesn't capture relevant similarity for prediction errors, retrieval becomes ineffective.

### Mechanism 3
- Claim: Self-correction mechanism prevents LLM hallucinations by having the model verify its own corrections when they deviate substantially from ML predictions.
- Mechanism: When LLM modifications exceed a threshold (20% for regression, label reversal for classification), a second prompt asks the LLM to self-correct its proposed changes.
- Core assumption: LLMs can recognize when their corrections are likely hallucinations and self-correct with appropriate prompting.
- Evidence anchors:
  - [section 4.3] "we implement a self-correction mechanism when the LLM demonstrates a tendency to make substantial modifications"
  - [section 5.2] "this approach can prevent LLM from hallucinating incorrect corrections in many cases"
  - [corpus] Some evidence - corpus includes papers on LLM rationalization and self-correction, but not specifically this mechanism
- Break condition: If LLM becomes overly cautious after self-correction questioning, it may refuse to make necessary corrections.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: LLMCORR relies on LLMs learning from few examples in prompts rather than fine-tuning
  - Quick check question: Can you explain how few-shot prompting differs from fine-tuning in terms of computational cost and data requirements?

- Concept: Graph representation learning
  - Why needed here: Understanding how molecule graphs are encoded for both ML models and LLM prompts
  - Quick check question: What are the key differences between SMILES strings and geometry structures for representing molecular graphs?

- Concept: Embedding-based similarity search
  - Why needed here: LLMCORR uses embeddings to retrieve relevant validation examples for each test query
  - Quick check question: How does the choice of embedding model affect the quality of retrieved knowledge for correction?

## Architecture Onboarding

- Component map: ML model → Validation predictions → Contextual knowledge database → Embedding-based retrieval → LLM prompt engineering → LLM correction → Self-correction verification
- Critical path: The embedding retrieval and prompt engineering steps are most critical - poor retrieval leads to irrelevant context, poor prompting leads to ineffective corrections
- Design tradeoffs: Larger k values improve performance but increase token usage and cost; simpler prompts are faster but may miss important context
- Failure signatures: If LLM corrections consistently worsen predictions, check if retrieval is finding truly similar examples; if corrections are too conservative, check self-correction threshold
- First 3 experiments:
  1. Run LLMCORR with k=1 to verify basic functionality and establish baseline
  2. Test different embedding models to find optimal retrieval quality
  3. Compare performance with and without self-correction to measure hallucination prevention effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMCORR incorporate geometric structure information of molecules given current LLM input token constraints?
- Basis in paper: [explicit] The paper mentions that LLMCORR cannot directly incorporate geometric structure in the prompt due to limitations in the token count of generated descriptions over the LLM's constraints.
- Why unresolved: The current approach relies on text descriptions of molecules, which may not capture all relevant geometric information. The paper identifies this as a limitation but does not provide a solution.
- What evidence would resolve it: Developing a method to encode geometric structure information into a compact format that fits within LLM token limits, or using an LLM with higher token capacity.

### Open Question 2
- Question: What are the reasons for GPT-4's underperformance compared to GPT-3.5 in the LLMCORR framework?
- Basis in paper: [explicit] The paper observes that GPT-4 exhibits inferior performance compared to GPT-3.5 in this study and hypothesizes it may be due to interventions such as reinforcement learning through human feedback.
- Why unresolved: The paper does not conduct a detailed analysis to confirm the hypothesis or explore other potential reasons for the performance difference.
- What evidence would resolve it: A comprehensive study comparing the effects of different training interventions on GPT-4 and GPT-3.5 performance in the LLMCORR framework.

### Open Question 3
- Question: How can the self-correction mechanism in LLMCORR be improved to avoid hesitant and cautious behavior after questioning?
- Basis in paper: [inferred] The paper mentions that the self-correction component sometimes leads to uncertain impacts due to the LLM becoming hesitant and cautious after the questioning.
- Why unresolved: The current self-correction prompt template may not be optimal for encouraging confident corrections without introducing new errors.
- What evidence would resolve it: Developing and testing alternative self-correction prompt templates or mechanisms that encourage more confident and accurate corrections.

### Open Question 4
- Question: Can LLMCORR be effectively applied to domains beyond molecular property prediction and text analysis?
- Basis in paper: [explicit] The paper mentions that the verification of LLMCORR was mainly confined to structured molecular graph property prediction tasks and several text analysis tasks, and further extensive empirical investigations across diverse domains are warranted to establish its generalisability.
- Why unresolved: The paper does not explore the application of LLMCORR to other domains, leaving its effectiveness in those areas unknown.
- What evidence would resolve it: Conducting experiments applying LLMCORR to various other domains, such as computer vision, time series analysis, or recommendation systems, and evaluating its performance.

### Open Question 5
- Question: How can LLMCORR be extended to leverage more contextual knowledge beyond the current prompt-based approach?
- Basis in paper: [explicit] The paper mentions that while LLMCORR's prompt template could accommodate the insertion of molecule atom features and geometry structure descriptions, limitations stemming from the LLM's input token constraints prevented their inclusion in the prompt.
- Why unresolved: The current approach relies on a limited amount of contextual knowledge due to token constraints, which may not fully capture the complexity of the task.
- What evidence would resolve it: Developing methods to incorporate additional contextual knowledge, such as using retrieval-augmented generation (RAG) or advanced contextual knowledge retrieval techniques, and evaluating their impact on LLMCORR's performance.

## Limitations
- Performance depends heavily on the representativeness and size of the validation set for learning effective error patterns
- LLM token constraints limit the amount of contextual knowledge that can be provided in prompts, potentially missing important information
- The self-correction mechanism may be overly conservative, preventing necessary corrections while trying to avoid hallucinations

## Confidence

- **High Confidence**: The experimental results demonstrating consistent performance improvements across multiple datasets and ML models are well-supported by the provided evidence.
- **Medium Confidence**: The mechanism of using validation data to learn error patterns and apply them to test predictions is plausible but depends heavily on the quality and representativeness of the validation set.
- **Low Confidence**: The effectiveness of the self-correction mechanism in preventing hallucinations without overly constraining necessary corrections requires further validation, as does the generalizability of the embedding-based retrieval approach across diverse domains.

## Next Checks

1. **Validation Set Representativeness**: Systematically evaluate how the size and diversity of the validation set affects LLMCORR's performance to identify the minimum requirements for effective error pattern learning.

2. **Retrieval Quality Analysis**: Conduct ablation studies on different embedding models and similarity metrics to quantify their impact on the quality of retrieved knowledge and subsequent correction accuracy.

3. **Self-Correction Threshold Optimization**: Experiment with different thresholds for triggering the self-correction mechanism to find the optimal balance between preventing hallucinations and allowing necessary corrections.