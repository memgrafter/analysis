---
ver: rpa2
title: Multilingual Diversity Improves Vision-Language Representations
arxiv_id: '2405.16915'
source_url: https://arxiv.org/abs/2405.16915
tags:
- captions
- translated
- data
- english
- filtered
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance benefits of incorporating
  multilingual data in vision-language pre-training, beyond the common focus on non-English
  tasks. The authors systematically study how using more non-English image-text pairs
  (translated to English) affects performance on English-centric vision tasks.
---

# Multilingual Diversity Improves Vision-Language Representations

## Quick Facts
- arXiv ID: 2405.16915
- Source URL: https://arxiv.org/abs/2405.16915
- Reference count: 40
- Key finding: Training CLIP on translated multilingual captions outperforms English-only training across multiple vision benchmarks

## Executive Summary
This paper demonstrates that incorporating multilingual data in vision-language pre-training significantly improves performance on English-centric vision tasks. By translating non-English image captions to English and re-filtering the data, the authors show consistent gains across ImageNet, ImageNet distribution shifts, image-text retrieval, and 38 DataComp tasks. The largest improvements are observed on the geographically diverse GeoDE benchmark, with a 5.5% gain in Africa. The findings suggest that multilingual data provides complementary information beyond what monolingual datasets capture.

## Method Summary
The authors translate all non-English captions from a large web-crawled corpus (128M image-text pairs) to English using the NLLB model. They then filter the translated dataset using DFN cosine similarity scores and train CLIP models from scratch with ViT-B/32 encoders. The key innovation is using the same translated dataset but applying different filtering strategies based on DFN or CLIP cosine similarity scores to study how multilingual data affects model performance.

## Key Results
- CLIP trained on translated multilingual captions outperforms English-only training on ImageNet (+1.0%)
- Largest gains observed on GeoDE benchmark, with 5.5% improvement in Africa region
- Consistent improvements across 38 DataComp tasks and ImageNet distribution shifts
- Performance benefits are robust to choice of filtering network (DFN vs CLIP scores)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translation of non-English captions enables broader visual concept coverage in training data
- Mechanism: Non-English captions describe culturally specific or regionally unique visual elements that are underrepresented in English-only datasets. By translating these captions to English, these diverse visual concepts become accessible to English-trained vision models.
- Core assumption: Non-English captions contain meaningful visual descriptions that are semantically distinct from English captions for the same images
- Evidence anchors:
  - [abstract] "Multilingual data is inherently enriching not only because it provides a gateway to learn about culturally salient concepts, but also because it depicts common concepts differently from monolingual data"
  - [section 1] "Multilingual data enriches any monolingual data distribution; multilingual data brings attention to culturally salient concepts and introduces new perspectives and annotations for the same visual category"
  - [corpus] Weak - corpus contains related papers but lacks direct experimental evidence of this mechanism

### Mechanism 2
- Claim: Translation changes data filtering outcomes, leading to different image distributions in training
- Mechanism: When filtering web-crawled data using CLIP-based cosine similarity, English and non-English captions rank differently for the same images. Translation alters these rankings, causing different images to be selected as "high quality" training data.
- Core assumption: Image-text similarity scores are sensitive to caption language, and translation changes these scores substantially
- Evidence anchors:
  - [section 4.2] "We find that while 'Filtered raw captions' is dominated by English samples, (translated) non-English samples make up the majority of 'Filtered translated captions'"
  - [section 5.1] "We quantitatively show that English and non-English data are significantly different in both image and (translated) text space"
  - [corpus] Weak - corpus doesn't directly address how translation affects CLIP filtering outcomes

### Mechanism 3
- Claim: Complementary text distributions provide additional semantic information
- Mechanism: Even after translation to English, the text distribution from non-English sources remains distinct from native English text, providing additional semantic coverage that helps the model learn richer representations.
- Core assumption: Translation preserves enough semantic content while maintaining distributional differences in vocabulary and phrasing
- Evidence anchors:
  - [section 5.2] "When comparing raw English texts and non-English texts, both having been passed through the translation model... the resulting MAUVE score is relatively low (0.616)"
  - [section 5.2] "independent of differences in language, what is discussed in English captions and non-English captions differs in many ways"
  - [corpus] Weak - corpus lacks quantitative analysis of distributional differences in translated vs native text

## Foundational Learning

- Concept: Data filtering and curation for vision-language models
  - Why needed here: The paper's core experiment involves filtering a large web-crawled dataset based on image-text alignment scores, which is fundamental to how modern vision-language models are trained
  - Quick check question: What metric is typically used to measure image-text alignment in CLIP-based filtering?

- Concept: Multilingual NLP and translation quality assessment
  - Why needed here: The paper relies on translating non-English captions to English and must address translation quality to ensure the approach is valid
  - Quick check question: What metric did the authors use to assess translation quality in their dataset?

- Concept: Zero-shot evaluation and benchmark metrics
  - Why needed here: The paper evaluates model performance using zero-shot classification on multiple benchmarks, which is standard practice for vision-language models
  - Quick check question: Which metric measures a model's ability to generalize to distribution shifts in the data?

## Architecture Onboarding

- Component map: Web crawl -> Language detection -> Translation -> Filtering -> Training -> Evaluation
- Critical path:
  1. Translate all captions to English
  2. Filter data using cosine similarity between image and text embeddings
  3. Train CLIP model from scratch on filtered data
  4. Evaluate on standard vision tasks
- Design tradeoffs:
  - Translation quality vs. coverage: Using a fast translation model covers more languages but may introduce artifacts
  - Filtering selectivity vs. dataset size: More selective filtering yields higher-quality data but smaller datasets
  - Training duration vs. performance: Longer training generally improves performance but increases compute costs
- Failure signatures:
  - Performance drops when using translated captions without re-filtering
  - Similar performance between raw and translated captions suggests filtering is language-insensitive
  - Poor translation quality metrics indicate loss of semantic information
- First 3 experiments:
  1. Train CLIP on top 20% filtered raw captions vs. top 20% filtered translated captions to verify performance difference
  2. Replace translated captions with synthetic captions from BLIP2 to isolate contribution of text diversity
  3. Filter data using OpenAI CLIP scores instead of DFN to verify robustness across filtering methods

## Open Questions the Paper Calls Out

1. How do the performance benefits of using translated multilingual data persist when employing other data filtering methods beyond CLIP score or DFN, such as hyperbolic entailment filtering or T-MARS?
2. What is the relative contribution of increased image diversity versus caption diversity to the performance gains observed when using translated multilingual data?
3. How well do CLIP models trained on translated multilingual data adapt to non-English tasks without fine-tuning the image encoder, relying solely on text encoder adaptation?

## Limitations

- Evaluation bias: All performance metrics focus on English-centric tasks despite studying multilingual data benefits
- Translation quality dependence: Results rely heavily on NLLB translation quality, which may introduce artifacts
- Filtering sensitivity: Performance gains are highly dependent on specific filtering thresholds and methods

## Confidence

- High confidence: Core finding that multilingual data improves vision-language representations on English-centric tasks
- Medium confidence: Claim that translation changes data filtering outcomes
- Low confidence: Mechanism that non-English captions describe fundamentally different visual concepts

## Next Checks

1. Conduct a translation ablation study by training models using original non-English captions with translated text embeddings to isolate the contribution of visual vs. textual diversity
2. Evaluate model performance on non-English vision tasks to verify if multilingual training benefits extend beyond English-centric evaluation
3. Compare performance across different filtering methods (CLIP vs. DFN) and thresholds to determine if gains are method-dependent or robust