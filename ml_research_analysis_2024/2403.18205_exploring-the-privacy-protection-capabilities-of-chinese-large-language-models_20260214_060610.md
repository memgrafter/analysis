---
ver: rpa2
title: Exploring the Privacy Protection Capabilities of Chinese Large Language Models
arxiv_id: '2403.18205'
source_url: https://arxiv.org/abs/2403.18205
tags:
- privacy
- language
- large
- evaluation
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a three-tiered progressive framework to evaluate
  the privacy protection capabilities of Chinese large language models (LLMs). The
  framework consists of general privacy information evaluation, contextual privacy
  evaluation, and privacy evaluation under attacks.
---

# Exploring the Privacy Protection Capabilities of Chinese Large Language Models

## Quick Facts
- arXiv ID: 2403.18205
- Source URL: https://arxiv.org/abs/2403.18205
- Reference count: 40
- Primary result: Existing Chinese LLMs universally exhibit privacy protection shortcomings across general, contextual, and adversarial evaluation tiers

## Executive Summary
This paper proposes a three-tiered progressive framework to evaluate the privacy protection capabilities of Chinese large language models (LLMs). The framework consists of general privacy information evaluation, contextual privacy evaluation, and privacy evaluation under attacks. Experiments conducted on four open-source Chinese LLMs with parameter sizes ranging from 6B to 7B reveal that these models universally exhibit privacy protection shortcomings, with low refusal rates and high risks of privacy leakage across various task scenarios. The findings highlight the need for further optimization and improvement in handling sensitive information to ensure data security and privacy in LLM-based applications.

## Method Summary
The paper develops a three-tiered evaluation framework consisting of general privacy information evaluation, contextual privacy evaluation, and privacy evaluation under attacks. The study uses four open-source Chinese LLMs (ChatGLM2-6B, Baichuan2-7B, Qwen-7B, InternLM-7B) and tests them using zero-shot and few-shot prompts for privacy queries. Test data includes 500 records of personal information with fake privacy content generated via self-instruct. The evaluation measures refusal rates (probability of rejecting privacy-violating queries), accuracy of privacy-aware responses, and performance under five types of prompt attacks (multi-language, summarize, extra-task, refusal-suppress, object-compete).

## Key Results
- Existing Chinese LLMs universally exhibit privacy protection shortcomings with low refusal rates and high risks of privacy leakage
- Few-shot prompting significantly degrades privacy protection, with refusal rates dropping from ~0.99 to ~0.55-0.15 across models
- Prompt injection/jailbreak attacks exploit object competition within models, causing privacy directives to be overridden in most cases

## Why This Works (Mechanism)

### Mechanism 1
The three-tiered framework isolates privacy vulnerabilities by escalating task complexity, exposing both memorization and contextual reasoning failures. Tier 1 tests raw memorization of static privacy entities; Tier 2 tests contextual privacy reasoning in simulated conversations; Tier 3 tests robustness against prompt injection/jailbreak attacks that attempt to override privacy safeguards. Privacy failures at higher tiers stem from model inability to integrate privacy directives into contextual decision-making under adversarial pressure.

### Mechanism 2
Few-shot prompting degrades privacy protection because it induces the model to prioritize task completion over privacy adherence. With few-shot examples, the model learns to replicate the prompt's output style and may ignore safety instructions, increasing privacy leakage risk. Few-shot examples serve as a stronger behavioral conditioning signal than the explicit privacy instruction in the prompt.

### Mechanism 3
Prompt injection/jailbreak attacks exploit object competition within the model, causing privacy directives to be overridden. Attack prompts introduce competing objectives; if attack objectives dominate, the model abandons privacy constraints. The model's internal object competition mechanism prioritizes the most recent or salient instruction over earlier privacy constraints.

## Foundational Learning

- **Prompt injection attacks**: Why needed here: The tier 3 evaluation relies on simulating real-world prompt injection scenarios to test privacy robustness. Quick check: What is the difference between a jailbreak and a prompt injection attack?

- **Zero-shot vs few-shot learning in LLMs**: Why needed here: The tier 1 evaluation compares privacy protection under zero-shot and few-shot prompting to reveal instruction adherence differences. Quick check: How does few-shot prompting change the model's behavior compared to zero-shot in privacy-sensitive tasks?

- **Contextual reasoning in language models**: Why needed here: Tier 2 evaluates the model's ability to reason about privacy in dynamic conversational contexts. Quick check: Why is contextual reasoning critical for privacy protection in LLMs?

## Architecture Onboarding

- **Component map**: Prompt → Model output → Privacy violation detection → Score aggregation → Comparison across models
- **Critical path**: Test prompt → Model response → Privacy compliance check → Refusal rate calculation → Aggregate results
- **Design tradeoffs**: Using few-shot prompts increases task realism but reduces privacy protection; using binary-choice prompts limits output diversity but simplifies scoring
- **Failure signatures**: Sudden drop in refusal rates from Tier 1 to Tier 2 or Tier 3; consistent memorization of privacy entities despite refusal instructions
- **First 3 experiments**:
  1. Run Tier 1 zero-shot test on all four models and compare refusal rates
  2. Run Tier 2 response generation task and manually verify privacy compliance
  3. Run Tier 3 with multi-language prompt attack on a single model and observe refusal rate change

## Open Questions the Paper Calls Out

### Open Question 1
How do the privacy protection capabilities of Chinese LLMs compare to their Western counterparts, such as GPT-4 or Claude, when subjected to the same three-tiered evaluation framework? The study only evaluates Chinese LLMs, so there's no baseline for comparison with international models.

### Open Question 2
What specific architectural or training modifications could significantly improve the privacy protection capabilities of Chinese LLMs in contextual privacy evaluation and privacy evaluation under attacks? The paper identifies the problem but doesn't propose specific solutions or architectural changes.

### Open Question 3
How does the performance of Chinese LLMs in privacy protection vary across different parameter sizes (e.g., 6B, 7B, 13B, 33B, 65B) when evaluated using the three-tiered framework? The study only tests models within a narrow parameter range, leaving the relationship between model size and privacy protection unexplored.

## Limitations

- Dataset construction relies heavily on self-instruct generation of fake privacy content, which may not fully capture real-world privacy scenarios
- The refusal rate metric may not fully capture nuanced privacy violations where models provide partial information or indirect disclosures
- The evaluation focuses exclusively on open-source Chinese LLMs with parameter sizes between 6B-7B, limiting generalizability

## Confidence

**High Confidence**: The finding that existing Chinese LLMs universally exhibit privacy protection shortcomings is well-supported by the experimental results across all three tiers.

**Medium Confidence**: The claim about few-shot prompting degrading privacy protection has strong experimental support but requires further validation across different model architectures and task types.

**Medium Confidence**: The object competition hypothesis for prompt injection attacks is supported by observed behavior but lacks direct mechanistic evidence from the model internals.

## Next Checks

1. **Cross-architecture validation**: Test the same three-tiered framework on English-language LLMs and larger parameter models (70B+) to determine if the privacy protection patterns observed in Chinese 6B-7B models generalize across architectures and scales.

2. **Human evaluation study**: Conduct a blinded human evaluation of Tier 2 contextual responses to validate the automated scoring methodology and assess whether privacy violations are being correctly identified, particularly for nuanced cases where models provide indirect information disclosure.

3. **Attack surface expansion**: Design and implement additional attack scenarios beyond the five tested, including multi-turn jailbreaks, role-playing attacks, and context manipulation techniques to determine the completeness of the current attack evaluation and identify potential blind spots in the privacy protection assessment.