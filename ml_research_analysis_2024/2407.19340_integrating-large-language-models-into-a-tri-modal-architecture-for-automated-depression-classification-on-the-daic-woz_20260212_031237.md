---
ver: rpa2
title: Integrating Large Language Models into a Tri-Modal Architecture for Automated
  Depression Classification on the DAIC-WOZ
arxiv_id: '2407.19340'
source_url: https://arxiv.org/abs/2407.19340
tags:
- data
- architecture
- audio
- were
- depression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the first multi-modal architecture integrating\
  \ large language models (LLMs) for automated depression classification from clinical\
  \ interview recordings. The proposed system combines three data modalities\u2014\
  Mel Frequency Cepstral Coefficients (MFCCs) from audio, Facial Action Units (FAUs)\
  \ from video, and text transcripts processed by a GPT-4 model\u2014using a BiLSTM-based\
  \ model-level fusion approach."
---

# Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ

## Quick Facts
- arXiv ID: 2407.19340
- Source URL: https://arxiv.org/abs/2407.19340
- Reference count: 40
- First multi-modal architecture integrating LLMs for automated depression classification

## Executive Summary
This work introduces the first multi-modal architecture integrating large language models (LLMs) for automated depression classification from clinical interview recordings. The proposed system combines three data modalities—Mel Frequency Cepstral Coefficients (MFCCs) from audio, Facial Action Units (FAUs) from video, and text transcripts processed by a GPT-4 model—using a BiLSTM-based model-level fusion approach. The model is evaluated on the DAIC-WOZ dataset using both the AVEC 2016 Challenge train/validation/test split and Leave-One-Subject-Out cross-validation. It achieves state-of-the-art performance, with an accuracy of 91.01%, F1-score of 85.95%, precision of 80%, and recall of 92.86% on LOSOCV, significantly outperforming baseline and prior models. The results demonstrate the effectiveness of incorporating LLMs into multi-modal frameworks for mental health diagnostics, offering a promising, objective alternative to traditional clinical assessment methods. The model is also deployed in a clinician-facing web application to simulate real-world implementation.

## Method Summary
The study proposes a tri-modal architecture that integrates MFCCs from audio, FAUs from video, and GPT-4-processed text transcripts using a BiLSTM-based model-level fusion approach. The model processes each modality separately through individual BiLSTM networks, then concatenates the outputs for final classification. Hyperband optimization is used for hyperparameter tuning, and the model is evaluated using LOSOCV on the DAIC-WOZ dataset. The approach aims to overcome limitations of unimodal systems by capturing complementary information across modalities.

## Key Results
- Achieves state-of-the-art accuracy of 91.01% on LOSOCV for depression classification
- Outperforms baseline and prior models with F1-score of 85.95%, precision of 80%, and recall of 92.86%
- Successfully integrates GPT-4 into a multi-modal framework for mental health diagnostics
- Demonstrates the effectiveness of combining audio, video, and text modalities for depression detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 effectively compensates for scarce task-specific text training data in depression diagnosis.
- Mechanism: Large language models trained on broad corpora capture semantic and contextual features of depression-related language, allowing them to perform well with minimal fine-tuning or few-shot examples.
- Core assumption: Depression-related linguistic patterns are sufficiently represented in general training data for GPT-4.
- Evidence anchors:
  - [abstract]: "uses a two-shot learning based GPT-4 model to process text data"
  - [section]: "To date, no research has attempted to integrate Large Language Models (LLMs) into a multi-modal architecture for this task. Due to their training on large corpora, we hypothesized incorporating LLMs into such an architecture would improve the accuracy of depression diagnosis"
  - [corpus]: Weak evidence - corpus shows related works use various models but none specifically integrate LLMs as hypothesized here.
- Break condition: If depression-related language patterns are too subtle or domain-specific to be captured by general pretraining.

### Mechanism 2
- Claim: BiLSTM-based model-level fusion effectively captures both modality-specific patterns and cross-modal relationships.
- Mechanism: Individual modalities are processed through separate BiLSTM networks, then concatenated at the model level, allowing for learning of both temporal patterns within each modality and interactions between modalities.
- Core assumption: Temporal dependencies exist within each modality and are important for depression diagnosis.
- Evidence anchors:
  - [abstract]: "BiLSTM-based tri-modal model-level fusion architecture"
  - [section]: "Model-Level Fusion is a combination of these two strategies. In Model-Level Fusion, individual modalities are processed before concatenation. After concatenation, the data is processed even further to reach an output."
  - [corpus]: Weak evidence - corpus shows related works use various fusion strategies but doesn't specifically validate BiLSTM-based model-level fusion.
- Break condition: If temporal dependencies are not significant or if cross-modal interactions are better captured through alternative fusion strategies.

### Mechanism 3
- Claim: MFCCs and FAUs provide discriminative features for depression classification.
- Mechanism: MFCCs capture acoustic properties of speech that differ between depressed and non-depressed individuals, while FAUs capture facial muscle movements associated with emotional states.
- Core assumption: Depression manifests in measurable differences in speech acoustics and facial expressions.
- Evidence anchors:
  - [section]: "Derived using the Mel Scale, MFCCs provide a compact representation of the human perception of sound... Studies indicate such values are different between depressed and non-depressed patients"
  - [section]: "Research has shown facial expressions and movements are significantly different in depressed patients as opposed to non-depressed ones"
  - [corpus]: Weak evidence - corpus shows related works use MFCCs and FAUs but doesn't provide direct evidence for their discriminative power in depression classification.
- Break condition: If depression does not consistently manifest in measurable acoustic or facial features.

## Foundational Learning

- Concept: Mel Frequency Cepstral Coefficients (MFCCs)
  - Why needed here: MFCCs are the primary audio feature used to capture speech characteristics that may differ between depressed and non-depressed individuals.
  - Quick check question: What acoustic properties do MFCCs capture that might be relevant for depression detection?

- Concept: Facial Action Units (FAUs)
  - Why needed here: FAUs provide objective, low-level representation of facial muscle movements that can indicate emotional states associated with depression.
  - Quick check question: How do FAUs differ from other facial feature representations like facial landmarks?

- Concept: Model-level fusion
  - Why needed here: This fusion strategy allows the model to learn both modality-specific patterns and cross-modal relationships, which is crucial for accurate depression diagnosis from multi-modal data.
  - Quick check question: What are the advantages of model-level fusion compared to early or late fusion strategies?

## Architecture Onboarding

- Component map: Text → GPT-4 → LLM output → BiLSTM → Dense → Classification
  Audio → MFCC extraction → BiLSTM → Dense → Classification
  Video → FAU extraction → BiLSTM → Dense → Classification
  All → Concatenation → Dense layers → Classification

- Critical path: Text → GPT-4 → LLM output → BiLSTM → Dense → Classification
  Audio → MFCC extraction → BiLSTM → Dense → Classification
  Video → FAU extraction → BiLSTM → Dense → Classification
  All → Concatenation → Dense layers → Classification

- Design tradeoffs:
  - Using GPT-4 for text processing trades computational cost for improved performance without task-specific training data.
  - BiLSTM layers capture temporal dependencies but add computational complexity compared to simpler architectures.
  - Model-level fusion balances the benefits of early and late fusion but requires careful tuning of individual modality processing.

- Failure signatures:
  - Poor performance on either depressed or non-depressed class may indicate imbalance in feature extraction or fusion.
  - High variance in cross-validation results may suggest overfitting to specific subjects.
  - Slow inference times may indicate the need for optimization or hardware upgrades.

- First 3 experiments:
  1. Train and evaluate each modality independently to establish baseline performance and identify the strongest modality.
  2. Test different fusion strategies (early, late, model-level) with the same individual modality processing to validate the choice of model-level fusion.
  3. Vary the number of few-shot examples provided to GPT-4 to find the optimal balance between performance and data requirements.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the abstract or conclusions. However, the study raises several important questions about the generalizability of the model to different clinical settings, the long-term effects of using LLM-based depression classification models on patient outcomes and clinical practice, and how the model handles false positives and false negatives in depression classification.

## Limitations
- Reliance on a single dataset (DAIC-WOZ) limits generalizability to other clinical contexts
- Two-shot learning approach with GPT-4 requires careful prompt engineering not fully specified
- Computational requirements for running GPT-4 may limit real-world deployment feasibility

## Confidence
- **High confidence**: The effectiveness of the tri-modal approach combining audio, video, and text features for depression detection is well-supported by the results and aligns with established multi-modal learning principles.
- **Medium confidence**: The specific advantage of using GPT-4 over other text processing approaches is demonstrated, but the sensitivity to prompt engineering and few-shot examples remains uncertain without detailed implementation information.
- **Medium confidence**: The generalizability of the model to clinical settings outside the DAIC-WOZ dataset is promising but unproven, as real-world depression presentations may differ from interview-based assessments.

## Next Checks
1. **Dataset generalization test**: Evaluate the model on at least two additional depression datasets (e.g., DAIC-WOZ variants or other clinical interview datasets) to assess performance consistency across different recording conditions and patient populations.

2. **Ablation study with alternative LLMs**: Replace GPT-4 with smaller, more deployable language models (e.g., BERT, RoBERTa) using the same two-shot approach to determine if the performance gains are specifically due to GPT-4's capabilities or general LLM properties.

3. **Clinical validation study**: Conduct a small-scale pilot study with clinicians using the deployed web application to assess practical utility, including diagnostic accuracy, time efficiency, and user satisfaction compared to standard clinical assessment methods.