---
ver: rpa2
title: Explaining the role of Intrinsic Dimensionality in Adversarial Training
arxiv_id: '2405.17130'
source_url: https://arxiv.org/abs/2405.17130
tags:
- robustness
- adversarial
- smaat
- generalization
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explains why adversarial training (AT) improves robustness
  in vision models but often hurts generalization, while having the opposite effect
  on encoder-based language models (enc-LLMs). The key insight is that intrinsic dimensionality
  (ID) of intermediate layers governs the ratio of off-manifold adversarial examples
  (AEs), which enhance robustness, versus on-manifold AEs, which improve generalization.
---

# Explaining the role of Intrinsic Dimensionality in Adversarial Training

## Quick Facts
- arXiv ID: 2405.17130
- Source URL: https://arxiv.org/abs/2405.17130
- Authors: Enes Altinisik; Safa Messaoud; Husrev Taha Sencar; Hassan Sajjad; Sanjay Chawla
- Reference count: 40
- One-line primary result: SMAAT improves robustness of encoder-based language models by 6.0-28.8% while reducing training time by 25-33%.

## Executive Summary
This paper explains why adversarial training (AT) improves robustness in vision models but often hurts generalization, while having the opposite effect on encoder-based language models (enc-LLMs). The key insight is that intrinsic dimensionality (ID) of intermediate layers governs the ratio of off-manifold adversarial examples (AEs), which enhance robustness, versus on-manifold AEs, which improve generalization. Vision and decoder-based models have low ID in early layers, leading to more off-manifold AEs and higher robustness but lower generalization. Enc-LLMs have low ID in later layers, leading to better generalization but weaker robustness. Building on this, the authors introduce SMAAT, which applies AT to the layer with the lowest ID to maximize off-manifold AEs and thus robustness, while also reducing training time by 25-33% due to shorter backpropagation chains.

## Method Summary
SMAAT (Scalable Manifold Aware Adversarial Training) identifies the layer with the lowest intrinsic dimensionality using the twoNN estimator and applies adversarial training specifically to that layer. This approach generates more off-manifold adversarial examples that enhance robustness while reducing the backpropagation chain length, cutting GPU time by 25-33%. The method was evaluated across sentiment classification, safety filtering, and retrieval tasks, consistently improving robustness while maintaining or slightly improving generalization compared to standard AT.

## Key Results
- SMAAT improved robustness by 6.0-28.8% over strong baselines on AGNEWS, IMDB, and YELP datasets
- On safety filtering tasks, achieved 97-100% accuracy against adversarial prompts
- In retrieval tasks, reduced adversarial passage retrieval to near zero
- Reduced training time by 25-33% through shorter backpropagation chains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training improves robustness by generating off-manifold adversarial examples (AEs), while on-manifold AEs improve generalization.
- Mechanism: The data manifold conjecture posits that data resides on a low-dimensional manifold within a high-dimensional representation space. Adversarial examples that fall off this manifold lead to undefined behavior, enhancing robustness. Conversely, on-manifold AEs improve generalization by reinforcing the model's understanding of the data distribution.
- Core assumption: The data manifold conjecture holds true for the datasets and models used in this study.
- Evidence anchors:
  - [abstract] "We provide the first explanation for these trends by leveraging the manifold conjecture: off-manifold adversarial examples (AEs) enhance robustness, while on-manifold AEs improve generalization."
  - [section] "The data manifold is a potentially non-linear subspace spanned by the dataset, and its dimensionality influences whether adversarial examples (AEs) lie on the manifold (on-manifold AEs) or fall outside it (off-manifold AEs) during training."
  - [corpus] Weak evidence - corpus neighbors do not directly address the manifold conjecture or the relationship between off/on-manifold AEs and robustness/generalization.
- Break condition: If the data manifold conjecture is not valid for the specific dataset or model architecture, the relationship between off/on-manifold AEs and robustness/generalization may not hold.

### Mechanism 2
- Claim: Intrinsic dimensionality (ID) of intermediate layers governs the ratio of off-manifold to on-manifold AEs, affecting robustness and generalization.
- Mechanism: Lower ID in a layer corresponds to a higher proportion of off-manifold AEs, leading to enhanced robustness. Conversely, higher ID leads to more on-manifold AEs, improving generalization. By perturbing the layer with the lowest ID, SMAAT maximizes the generation of off-manifold AEs, thus improving robustness while maintaining generalization.
- Core assumption: The intrinsic dimensionality of a layer directly correlates with the proportion of off/on-manifold AEs generated when that layer is perturbed.
- Evidence anchors:
  - [abstract] "We show that vision and decoder-based models exhibit low intrinsic dimensionality in earlier layers (favoring off-manifold AEs), whereas encoder-based models do so in later layers (favoring on-manifold AEs)."
  - [section] "We base this conclusion on an empirical investigation into the relationship between the ID and the proportion of ONM/OFM AEs across different layers of various deep neural network models."
  - [corpus] Weak evidence - corpus neighbors do not directly address the relationship between ID and the proportion of off/on-manifold AEs.
- Break condition: If the relationship between ID and the proportion of off/on-manifold AEs is not consistent across different datasets or model architectures, perturbing the layer with the lowest ID may not always lead to improved robustness.

### Mechanism 3
- Claim: Perturbing intermediate layers with lower ID improves the scalability of adversarial training by reducing the length of backpropagation chains.
- Mechanism: Standard adversarial training perturbs the input layer, requiring backpropagation through the entire network. By perturbing intermediate layers with lower ID, SMAAT shortens the backpropagation chain, reducing computational cost. This allows for more efficient adversarial training without sacrificing robustness gains.
- Core assumption: The computational cost of adversarial training is directly related to the length of the backpropagation chain, and shorter chains lead to improved scalability.
- Evidence anchors:
  - [abstract] "Exploiting this property, we introduce SMAAT, which improves the scalability of AT for encoder-based models by perturbing the layer with the lowest intrinsic dimensionality. This reduces the projected gradient descent (PGD) chain length required for AE generation, cutting GPU time by 25â€“33%."
  - [section] "SMAAT leads to a significant speed-up of AT by avoiding a full backward pass as it calculates the gradients only until the last layer rather than the entire network."
  - [corpus] Weak evidence - corpus neighbors do not directly address the relationship between layer perturbation and backpropagation chain length.
- Break condition: If the computational savings from shorter backpropagation chains are offset by other factors (e.g., increased memory usage or overhead from ID calculation), the scalability benefits of SMAAT may be diminished.

## Foundational Learning

- Concept: Manifold conjecture
  - Why needed here: The manifold conjecture is the foundation for understanding the relationship between off/on-manifold AEs and robustness/generalization. It provides the theoretical basis for the mechanisms proposed in this paper.
  - Quick check question: What is the manifold conjecture, and how does it relate to the generation of adversarial examples?

- Concept: Intrinsic dimensionality (ID)
  - Why needed here: ID is a key factor in determining the proportion of off/on-manifold AEs generated when a layer is perturbed. Understanding ID is crucial for implementing SMAAT and interpreting its results.
  - Quick check question: How is intrinsic dimensionality estimated, and what does it tell us about the data manifold?

- Concept: Adversarial training (AT)
  - Why needed here: AT is the primary technique used in this paper to improve robustness. Understanding the standard AT approach and its limitations is essential for appreciating the contributions of SMAAT.
  - Quick check question: What is adversarial training, and how does it differ from standard training in terms of objectives and techniques?

## Architecture Onboarding

- Component map: Data manifold -> Intrinsic dimensionality (ID) -> Adversarial examples (AEs) -> Off-manifold AEs -> On-manifold AEs -> SMAAT

- Critical path:
  1. Estimate the ID of each layer using the twoNN method
  2. Identify the layer with the lowest ID (l*)
  3. Generate adversarial examples by perturbing the l* layer
  4. Train the model using the adversarial examples

- Design tradeoffs:
  - ID estimation accuracy vs. computational cost: More accurate ID estimation may require more computational resources
  - Layer perturbation vs. input perturbation: Perturbing intermediate layers may lead to improved scalability but could also introduce new challenges (e.g., gradient vanishing/exploding)
  - Robustness vs. generalization: Increasing robustness through off-manifold AEs may come at the cost of reduced generalization

- Failure signatures:
  - If the ID estimation is inaccurate, SMAAT may not effectively target the layer with the lowest ID, leading to suboptimal robustness improvements
  - If the relationship between ID and the proportion of off/on-manifold AEs is not consistent across different datasets or model architectures, SMAAT may not always lead to improved robustness
  - If the computational savings from shorter backpropagation chains are offset by other factors, the scalability benefits of SMAAT may be diminished

- First 3 experiments:
  1. Implement the twoNN method to estimate the ID of each layer in a simple feedforward network
  2. Generate adversarial examples by perturbing the layer with the lowest ID and compare the robustness gains to standard AT
  3. Measure the computational cost of SMAAT and compare it to standard AT to assess the scalability benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the intrinsic dimensionality trend in enc-LLMs versus vision models and dec-LLMs hold consistently across different model architectures, pretraining objectives, and dataset domains?
- Basis in paper: [explicit] The authors demonstrate that enc-LLMs (BERT, RoBERTa) show decreasing intrinsic dimensionality (ID) in deeper layers, while vision models and dec-LLMs show increasing ID. This trend is central to SMAAT's design.
- Why unresolved: The analysis is limited to a specific set of models and datasets. The generalizability of the ID trend across different architectures (e.g., GPT, CLIP), pretraining objectives (e.g., autoregressive, contrastive), and domains (e.g., code, biomedical text) is not established.
- What evidence would resolve it: A comprehensive study applying the twoNN ID estimation across diverse model families, pretraining objectives, and datasets to determine if the observed ID trends are consistent or architecture-dependent.

### Open Question 2
- Question: How does SMAAT's effectiveness vary when applied to different layers beyond the one with the lowest ID, and what is the relationship between ID and robustness gains at those layers?
- Basis in paper: [explicit] SMAAT targets the layer with the lowest ID to maximize off-manifold adversarial examples (AEs) and robustness. The authors show this layer is typically the last layer for enc-LLMs.
- Why unresolved: The paper focuses on perturbing the single layer with the lowest ID. It does not explore the robustness gains or computational trade-offs of perturbing other layers, especially those with intermediate ID values.
- What evidence would resolve it: Experiments applying SMAAT to multiple layers with varying IDs and comparing robustness, generalization, and computational cost against the single-layer approach.

### Open Question 3
- Question: Can SMAAT be extended to handle adaptive adversarial attacks that are aware of the specific layer being perturbed, and how would this affect its robustness?
- Basis in paper: [inferred] SMAAT's design relies on perturbing a specific layer to generate off-manifold AEs. Adaptive attacks, which tailor their strategy based on the defense mechanism, are not explicitly addressed.
- Why unresolved: The paper evaluates SMAAT against standard adversarial attacks (PWWS, TextFooler, BERT-Attack, GCG, poisoning attacks) but does not consider adaptive attacks that might circumvent the layer-specific perturbation strategy.
- What evidence would resolve it: Evaluating SMAAT's robustness against adaptive attacks that are designed to exploit the knowledge of which layer is being perturbed, such as attacks that adjust their strategy based on the ID of the targeted layer.

## Limitations

- The theoretical foundation linking intrinsic dimensionality to manifold properties remains underdeveloped
- The twoNN estimator used for ID computation may introduce noise in high-dimensional embedding spaces
- Findings may not generalize to encoder-decoder or decoder-only architectures beyond BERT and RoBERTa

## Confidence

- **High confidence**: SMAAT reduces training time by 25-33% through shorter backpropagation chains
- **Medium confidence**: The relationship between intrinsic dimensionality and the ratio of off/on-manifold adversarial examples relies on the manifold conjecture
- **Medium confidence**: The generalizability of findings across different enc-LLM architectures requires further validation

## Next Checks

1. Apply SMAAT to encoder-decoder models (T5, BART) and decoder-only models (GPT variants) to verify if the layer-specific ID patterns hold across architectures.

2. Compare the twoNN estimator against alternative intrinsic dimensionality estimation methods to assess robustness of the layer selection mechanism.

3. Test whether SMAAT-trained models maintain robustness against black-box attacks and transfer attacks from models trained with different AT methods.