---
ver: rpa2
title: 'SAM-SP: Self-Prompting Makes SAM Great Again'
arxiv_id: '2408.12364'
source_url: https://arxiv.org/abs/2408.12364
tags:
- prompts
- image
- sam-sp
- segmentation
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAM-SP, a self-prompting framework that extends
  the Segment Anything Model (SAM) for domain-specific segmentation tasks like medical
  images. The key innovation is a self-prompting module that generates its own prompts
  from previous predictions, eliminating reliance on expert prompts during inference.
---

# SAM-SP: Self-Prompting Makes SAM Great Again

## Quick Facts
- arXiv ID: 2408.12364
- Source URL: https://arxiv.org/abs/2408.12364
- Authors: Chunpeng Zhou; Kangjie Ning; Qianqian Shen; Sheng Zhou; Zhi Yu; Haishuai Wang
- Reference count: 19
- One-line primary result: SAM-SP achieves 15-40 percentage point improvements in Dice scores across 10 datasets without requiring user prompts during inference

## Executive Summary
This paper introduces SAM-SP, a self-prompting framework that extends the Segment Anything Model (SAM) for domain-specific segmentation tasks like medical images. The key innovation is a self-prompting module that generates its own prompts from previous predictions, eliminating reliance on expert prompts during inference. SAM-SP also incorporates LoRA-based fine-tuning and a self-distillation module for improved performance. Extensive experiments on 10 public datasets demonstrate that SAM-SP achieves superior segmentation results compared to the vanilla SAM, SAM-based approaches, and task-specific methods, without requiring any user prompts during inference.

## Method Summary
SAM-SP extends the Segment Anything Model by adding a self-prompting module that generates prompts from its own previous predictions, eliminating the need for expert prompts during inference. The framework uses LoRA-based fine-tuning to efficiently adapt SAM to domain-specific tasks while freezing the image encoder. A self-distillation module further enhances performance by using the final prediction to guide earlier predictions. The model is trained without any user prompts and demonstrates significant improvements across medical imaging, remote sensing, and infrastructure monitoring datasets.

## Key Results
- SAM-SP significantly outperforms vanilla SAM across all 10 datasets, with Dice score improvements ranging from 15-40 percentage points
- The model achieves state-of-the-art performance compared to both SAM-based approaches and task-specific methods
- Self-prompting alone improves performance, with additional gains from the self-distillation module and LoRA fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Self-Prompting Eliminates Expert Dependency
SAM-SP eliminates dependence on expert prompts by using its own previous iteration's output as the next iteration's prompt. The self-prompting module takes the initial mask prediction from the first iteration, converts it into a box prompt using max/min coordinates, and feeds this generated prompt back into the prompt encoder for the next iteration. This creates a self-reinforcing loop where the model learns to generate increasingly useful prompts without external input.

### Mechanism 2: Efficient Domain Adaptation with LoRA
LoRA-based fine-tuning enables efficient domain adaptation while maintaining computational efficiency. Instead of fine-tuning all parameters of SAM, SAM-SP freezes the image encoder (which dominates >95% of computational overhead) and only fine-tunes the prompt encoder and mask decoder. Additionally, LoRA approximates low-rank updates to the image encoder weights, allowing domain adaptation without the computational cost of full fine-tuning.

### Mechanism 3: Self-Distillation Enhances Learning
Self-distillation enhances the self-prompting process by using later predictions to guide earlier ones. The final prediction from the self-prompting process acts as a "teacher" to guide the earlier prediction through knowledge distillation. This creates mutual benefits: the final prediction provides additional supervisory signals, while earlier predictions provide more accurate prompts for subsequent predictions.

## Foundational Learning

- Concept: Vision Foundation Models (VFMs) and their zero-shot capabilities
  - Why needed here: Understanding how SAM works as a VFM and why it struggles with domain-specific tasks like medical images is crucial for appreciating the need for SAM-SP.
  - Quick check question: What is the primary limitation of applying SAM to medical images, and how does this differ from its performance on natural images?

- Concept: Fine-tuning strategies (full vs. parameter-efficient methods like LoRA)
  - Why needed here: SAM-SP uses LoRA-based fine-tuning, which is a parameter-efficient approach. Understanding the tradeoffs between full fine-tuning and parameter-efficient methods is important for appreciating the design choices.
  - Quick check question: Why does SAM-SP choose to freeze the image encoder and only fine-tune the prompt encoder and mask decoder, rather than performing full fine-tuning?

- Concept: Knowledge distillation and self-distillation
  - Why needed here: The self-distillation module is a key component of SAM-SP. Understanding how knowledge distillation works, and how it can be applied in a self-supervised manner, is important for understanding this mechanism.
  - Quick check question: How does the self-distillation module in SAM-SP differ from traditional knowledge distillation, and what are the mutual benefits it provides?

## Architecture Onboarding

- Component map: Image → Image Encoder → Image Embeddings → (Default/Generated Prompt) → Mask Decoder → Initial Prediction → Self-Prompting Module → Generated Box Prompt → Mask Decoder → Final Prediction → Self-Distillation Module → Guides Initial Prediction

- Critical path:
  1. Image → Image Encoder → Image Embeddings
  2. Image Embeddings + (Default/Generated) Prompt → Mask Decoder → Initial Prediction (ŷ₀)
  3. Initial Prediction → Self-Prompting Module → Generated Box Prompt
  4. Image Embeddings + Generated Prompt → Mask Decoder → Final Prediction (ŷ₁)
  5. Final Prediction → Self-Distillation Module → Guides Initial Prediction

- Design tradeoffs:
  - Freezing image encoder vs. full fine-tuning: Computational efficiency vs. potential loss of adaptation capability
  - Single self-prompting iteration vs. multiple iterations: Efficiency vs. potential performance gains
  - Self-distillation vs. no distillation: Potential performance improvement vs. additional complexity

- Failure signatures:
  - Poor performance on domain-specific tasks despite fine-tuning: May indicate insufficient domain adaptation or poor prompt generation
  - Minimal improvement from self-prompting: May indicate initial predictions are too inaccurate to generate useful prompts
  - Self-distillation not improving performance: May indicate unreliable final predictions or ineffective self-supervision

- First 3 experiments:
  1. Implement SAM-SP without self-distillation module to isolate the effect of self-prompting
  2. Test SAM-SP with different numbers of self-prompting iterations (1, 2, 3) to find optimal balance between performance and efficiency
  3. Compare SAM-SP with and without LoRA fine-tuning to quantify the contribution of parameter-efficient adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of self-prompting iterations for achieving the best segmentation performance without overfitting?
- Basis in paper: The paper states "we typically conduct the self-prompting process only once by default" but also investigates the impact of multiple self-prompting processes, showing diminishing returns after the first iteration.
- Why unresolved: The paper provides some empirical results showing performance trends with different numbers of iterations, but does not determine an optimal point or provide a theoretical justification for stopping.
- What evidence would resolve it: A systematic study comparing different iteration counts across multiple datasets with statistical analysis to identify the point of diminishing returns, along with analysis of convergence behavior.

### Open Question 2
- Question: How does the self-prompting module generalize to domains beyond medical imaging, remote sensing, and infrastructure monitoring?
- Basis in paper: The paper focuses on specific domains but claims SAM-SP can extend SAM's capabilities across "various downstream tasks," suggesting potential for broader applications.
- Why unresolved: The evaluation is limited to 10 specific datasets in specialized domains. There is no evidence of performance on natural images, video segmentation, or other potential applications of SAM.
- What evidence would resolve it: Extensive testing on diverse domains including natural images, video sequences, and other specialized applications with comparison to both SAM and task-specific methods.

### Open Question 3
- Question: What is the relationship between prompt quality in the training phase and inference performance when no prompts are provided?
- Basis in paper: The paper states "we do not use any user prompts during both training and evaluation" and investigates different training strategies, finding that using no prompts during training yields the best inference performance.
- Why unresolved: While the paper shows that training without prompts is optimal, it does not explain why this is the case or what the underlying mechanism is that makes training-prompt consistency important.
- What evidence would resolve it: Theoretical analysis of the training-inference alignment problem, along with ablation studies on different prompt qualities and their effects on learned representations.

## Limitations

- Implementation details for self-prompting and self-distillation modules are not fully specified, making exact reproduction challenging
- Hyperparameter details are sparse, with only ranges provided rather than specific values
- Limited evaluation to specific domains (medical, remote sensing, infrastructure) without testing on natural images or other potential applications

## Confidence

- **High confidence** in the overall framework design and experimental results (15-40% Dice score improvements are well-documented)
- **Medium confidence** in the LoRA fine-tuning approach (well-established method, but application details unclear)
- **Low confidence** in the exact implementation details of the self-prompting and self-distillation mechanisms

## Next Checks

1. Implement an ablation study comparing SAM-SP with and without each component (self-prompting, self-distillation, LoRA fine-tuning) to quantify individual contributions
2. Test the self-prompting mechanism with varying quality of initial predictions to determine failure thresholds and robustness
3. Evaluate the model's performance when using external prompts versus self-generated prompts to validate the necessity of the self-prompting approach