---
ver: rpa2
title: Understanding GNNs for Boolean Satisfiability through Approximation Algorithms
arxiv_id: '2408.15418'
source_url: https://arxiv.org/abs/2408.15418
tags:
- value
- which
- each
- problems
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the interpretability of Graph Neural Networks
  (GNNs) in the context of Boolean Satisfiability (SAT). The authors uncover connections
  between trained GNNs and two well-studied approximation algorithms: Belief Propagation
  and Semidefinite Programming Relaxations.'
---

# Understanding GNNs for Boolean Satisfiability through Approximation Algorithms

## Quick Facts
- arXiv ID: 2408.15418
- Source URL: https://arxiv.org/abs/2408.15418
- Reference count: 40
- Authors: Jan Hůla; David Mojžíšek; Mikoláš Janota
- Key outcome: GNNs trained on SAT learn to approximate Belief Propagation and SDP relaxations; curriculum training, decimation, and sampling improve accuracy and speed

## Executive Summary
This paper provides interpretability insights into Graph Neural Networks (GNNs) for Boolean Satisfiability (SAT) by connecting their message-passing dynamics to established approximation algorithms like Belief Propagation and Semidefinite Programming relaxations. The authors show that GNN embeddings form clusters corresponding to satisfying assignments, analogous to SDP solution vectors. Building on this understanding, they introduce a curriculum training procedure that accelerates convergence by over an order of magnitude and demonstrate that decimation and sampling techniques significantly increase the percentage of solved problems, especially for structured instances like Sudoku.

## Method Summary
The method uses a simplified NeuroSAT architecture with 16-dimensional hidden states, training on CNF formulas through message passing on literal-clause bipartite graphs. The key innovations are: (1) curriculum training that incrementally increases problem size and message-passing iterations based on validation accuracy thresholds, (2) decimation that fixes variables with extreme distances to average true/false vectors and simplifies the formula, and (3) sampling multiple random initializations of literal embeddings to explore diverse solutions. The training data includes random SAT instances and structured problems like Latin squares and Sudoku, with problems generated until unsatisfiable then modified by flipping one literal.

## Key Results
- Curriculum training achieves over an order of magnitude faster convergence to the same accuracy as baseline NeuroSAT
- Decimation and sampling techniques increase the percentage of solved problems, particularly for structured instances like Latin squares and Sudoku
- The GNN message passing process approximates the optimization of semidefinite programming relaxations of MAX-SAT, with embeddings forming clusters for satisfiable instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNN message passing approximates SDP relaxation optimization of MAX-SAT
- Mechanism: MP updates high-dimensional literal embeddings, with inner products mimicking SDP Gram matrix entries; updates drive embeddings to form two clusters for satisfiable instances
- Core assumption: Trained GNN implicitly learns energy function minimizing clause satisfaction
- Evidence anchors: Abstract mentions connections to Belief Propagation and SDP relaxations; section describes MP as continuous relaxation optimization
- Break condition: If embeddings fail to converge or form well-separated clusters for satisfiable instances

### Mechanism 2
- Claim: Curriculum training accelerates convergence by incremental complexity increase
- Mechanism: Early training on small problems with few MP iterations learns basic interactions; subsequent increases refine these for harder instances
- Core assumption: MP iterations needed correlates with problem difficulty, preventing catastrophic forgetting
- Evidence anchors: Abstract describes curriculum training with incremental problem complexity; section states MP iterations correlate with difficulty
- Break condition: If model overfits to curriculum order or fails to generalize to new sizes

### Mechanism 3
- Claim: Decimation and sampling improve accuracy by exploiting clustering tendency
- Mechanism: Multiple random initializations yield diverse solutions; decimation fixes extreme-distance variables, reducing problem size for repeated GNN runs
- Core assumption: Clustered embeddings reliably indicate correct truth values; sampling uncovers alternative valid solutions
- Evidence anchors: Abstract states decimation and sampling increase solved problems; section describes diverse solutions from random initializations
- Break condition: If decimation threshold poorly chosen or embeddings noisy, incorrect assignments may be locked

## Foundational Learning

- Concept: Boolean Satisfiability (SAT) and MAX-SAT
  - Why needed here: GNN predicts SAT/UNSAT and recovers satisfying assignments; understanding clause satisfaction is fundamental
  - Quick check question: In a 2-SAT clause (x₁ ∨ ¬x₂), what assignment satisfies it?

- Concept: Graph Neural Networks and message passing on bipartite graphs
  - Why needed here: GNN operates on literal-clause factor graphs; understanding MP updates is essential for algorithm-interpretability link
  - Quick check question: How does message from clause to literal differ from literal to clause in NeuroSAT?

- Concept: Semidefinite Programming (SDP) relaxations and vector lifting
  - Why needed here: Connection to SDP explains why GNN embeddings behave like lifted variables in continuous optimization space
  - Quick check question: In SDP relaxation of MAX-2-SAT, what does constraint Yᵢᵢ = 1 enforce?

## Architecture Onboarding

- Component map: CNF formula → literal-clause bipartite graph → LSTM encoder with message passing → literal embeddings → K-means clustering → SAT/UNSAT prediction + assignment

- Critical path: 1) Build literal-clause graph from CNF, 2) Initialize literal embeddings randomly, 3) Run T MP iterations, 4) Cluster embeddings or compute distances to average vectors, 5) Predict SAT/UNSAT and recover assignment

- Design tradeoffs: Simpler model (no MLPs for messages) → faster training but less expressive; fixed vs. curriculum T → curriculum speeds convergence but adds complexity; single vs. multiple initializations → multiple improves accuracy but increases inference cost

- Failure signatures: Embeddings don't form clear clusters → SDP analogy invalid; high variance across initializations → embeddings unstable; overfitting to small instances → poor generalization

- First 3 experiments: 1) Train baseline GNN on small random 2-SAT without curriculum; verify clustering for satisfiable cases, 2) Add curriculum with incremental size/T; compare convergence speed, 3) Apply decimation and sampling to curriculum-trained model; measure solved problem percentage improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine optimal decimation threshold to maximize solved problems?
- Basis: Paper used threshold of 1.9 after visual inspection but didn't optimize
- Why unresolved: No experimentation with different thresholds
- What evidence would resolve it: Testing various thresholds and measuring impact on solved problem percentage

### Open Question 2
- Question: How can we improve NeuroSAT accuracy by combining classification loss with SDP MAX-SAT objective?
- Basis: Model trained with SDP objective achieved only ~73% accuracy vs ~85% for classification loss, but improved quickly on large formulas
- Why unresolved: No exploration of combined loss functions
- What evidence would resolve it: Experimenting with different loss combinations and evaluating accuracy impact

### Open Question 3
- Question: How can we further improve curriculum training for faster convergence and higher accuracy?
- Basis: Paper introduced curriculum achieving order of magnitude speedup
- Why unresolved: No exploration of enhanced curriculum strategies
- What evidence would resolve it: Testing different curriculum approaches and evaluating training time/accuracy

## Limitations

- SDP analogy relies on empirical observations without direct proof of minimizing same objective as formal solvers
- Curriculum training threshold values appear tuned for experimental setup, raising generalizability questions
- Decimation and sampling improvements depend on embedding quality, which can be unstable for harder instances
- Ablation studies for individual mechanisms (curriculum, decimation, sampling) are incomplete

## Confidence

- SDP analogy and interpretability insights: Medium-Low
- Curriculum training convergence benefits: Medium
- Decimation and sampling accuracy improvements: Medium

## Next Checks

1. Compare GNN embedding evolution directly against SDP objective value during training to validate relaxation analogy quantitatively
2. Run ablation studies isolating curriculum, decimation, and sampling to measure their independent contributions to accuracy and convergence
3. Test curriculum scheduling on broader range of problem sizes and SAT variants (e.g., 3-SAT) to assess robustness and identify failure modes