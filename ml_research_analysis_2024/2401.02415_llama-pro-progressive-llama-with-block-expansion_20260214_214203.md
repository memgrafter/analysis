---
ver: rpa2
title: 'LLaMA Pro: Progressive LLaMA with Block Expansion'
arxiv_id: '2401.02415'
source_url: https://arxiv.org/abs/2401.02415
tags:
- arxiv
- llama
- language
- block
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a post-pretraining method for large language
  models (LLMs) called block expansion, which improves domain-specific abilities while
  preserving original general capabilities. The method works by adding copied Transformer
  blocks with zero-initialized linear layers to the base model and fine-tuning only
  these new blocks on domain-specific data.
---

# LLaMA Pro: Progressive LLaMA with Block Expansion

## Quick Facts
- **arXiv ID**: 2401.02415
- **Source URL**: https://arxiv.org/abs/2401.02415
- **Reference count**: 21
- **Primary result**: Post-pretraining method that expands LLaMA models with block expansion to improve domain-specific abilities while preserving general capabilities

## Executive Summary
This paper introduces block expansion, a post-pretraining method that enhances large language models' domain-specific abilities while preserving general capabilities. The method works by adding copied Transformer blocks with zero-initialized linear layers to the base model and fine-tuning only these new blocks on domain-specific data. Experiments demonstrate that LLAMA Pro-8.3B, created by expanding LLaMA2-7B with 8 blocks and fine-tuning on code and math data, outperforms existing LLaMA models on comprehensive benchmarks. The model excels in general tasks, programming, and mathematics, achieving state-of-the-art performance on HumanEval (28.66% pass@1), GSM8K (25.42%), and various language tasks. Further instruction tuning yields LLAMA Pro-Instruct, which achieves superior performance in chatbot interactions and multi-turn tool usage scenarios.

## Method Summary
Block expansion enhances LLMs by adding copied Transformer blocks to the base model, where the new blocks are initialized as identity mappings through zero-initialized output linear matrices. During fine-tuning, only the expanded blocks are trained while the original blocks remain frozen, preventing catastrophic forgetting. The blocks are arranged in an interleaved manner rather than stacked at the bottom or top, preserving the transformer's structural hierarchy. The method is applied to LLaMA2-7B by adding 8 blocks and fine-tuning on code and math corpora (80B tokens), followed by supervised instruction tuning on a combined dataset of approximately 1M samples.

## Key Results
- LLAMA Pro-8.3B achieves state-of-the-art performance on HumanEval (28.66% pass@1) and GSM8K (25.42%)
- The model demonstrates superior performance across general tasks, programming, and mathematics compared to existing LLaMA models
- LLAMA Pro-Instruct shows improved capabilities in chatbot interactions and multi-turn tool usage scenarios
- Block expansion effectively prevents catastrophic forgetting while improving domain-specific abilities

## Why This Works (Mechanism)

### Mechanism 1: Identity Mapping Through Zero Initialization
Zero-initializing the output linear matrices (W_O and W3) in the identity blocks creates a true identity mapping that preserves the base model's output. The zero initialization ensures that MHSA(RMSNorm(x)) = 0 and FFN(RMSNorm(x')) = 0, leaving only the residual connection to pass the input unchanged. This works because the LLaMA block's residual connections and lack of bias terms allow the entire block to reduce to an identity block when output weights are zero.

### Mechanism 2: Preventing Catastrophic Forgetting Through Block Freezing
Freezing the original LLaMA blocks while fine-tuning only the expanded blocks prevents catastrophic forgetting. By keeping the original weights fixed, the model maintains its general capabilities while the new blocks learn domain-specific knowledge. The expanded blocks are initialized as identity mappings, so they don't interfere with the base model's behavior during the initial training phase.

### Mechanism 3: Interleaved Block Placement for Structural Preservation
Interleaving identity blocks rather than stacking them at the bottom or top preserves both general and domain-specific performance. This maintains the model's structural hierarchy while providing sufficient capacity for domain adaptation. Deeper blocks in transformers encode more complex information, so interleaved blocks can learn hierarchical domain knowledge without disrupting the model's foundation.

## Foundational Learning

- **Transformer architecture fundamentals**: Understanding how residual connections, attention mechanisms, and feed-forward networks interact is crucial for grasping why block expansion works. *Quick check*: What happens to the output of a transformer block when all its linear weights are initialized to zero?

- **Catastrophic forgetting in continual learning**: The core motivation for block expansion is preventing the model from forgetting general knowledge when adapting to domain-specific tasks. *Quick check*: Why does fine-tuning all blocks on domain-specific data typically lead to performance degradation on general tasks?

- **Parameter-efficient fine-tuning methods**: Block expansion is positioned as an alternative to LoRA, prefix tuning, and other PEFT methods. *Quick check*: How does block expansion differ from LoRA in terms of parameter count and training dynamics?

## Architecture Onboarding

- **Component map**: LLaMA2-7B base model → 8 interleaved identity blocks → expanded LLaMA Pro-8.3B → instruction tuning → LLaMA Pro-Instruct
- **Critical path**: Block expansion (zero-initialization + freezing) → domain-specific pretraining → supervised fine-tuning → evaluation
- **Design tradeoffs**: Adding more blocks increases domain capacity but also computational cost and memory requirements
- **Failure signatures**: Performance drops on general tasks → likely original blocks were fine-tuned; poor domain adaptation → possibly insufficient training data or wrong block placement; training instability → potential issues with zero initialization or learning rate
- **First 3 experiments**:
  1. Verify identity block initialization by checking that base model outputs remain unchanged after expansion
  2. Compare interleaved vs stacked block placement on a small domain-specific task
  3. Test catastrophic forgetting by evaluating on general benchmarks after domain-specific training with different freezing strategies

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of Transformer blocks to add for different domain-specific tasks? The paper tests adding 1, 2, 4, 8, 16, and 32 blocks, finding that adding 8 blocks provides optimal performance with minimal cost compared to larger models. This remains unresolved because the paper only tests one specific domain (code and math) and one base model (LLAMA2-7B). Different domains may require different numbers of added blocks for optimal performance.

### Open Question 2
How does block expansion compare to other continual learning methods for LLMs in terms of long-term knowledge retention? The paper compares block expansion to LoRA and sequential fine-tuning on the TRACE benchmark, showing superior performance. This remains unresolved because the comparison is limited to one base model and one specific pretraining corpus. Long-term knowledge retention across multiple rounds of continual learning and domain adaptation is not thoroughly explored.

### Open Question 3
How does the position of added blocks (bottom, top, or interleaved) affect model performance across different task types? The paper finds that adding blocks at the bottom disrupts performance, while adding them at the top or interleaved preserves initial performance but with varying domain-specific task performance. This remains unresolved because the paper only tests this for one specific domain (code and math). Different task types may benefit from different block positioning strategies.

### Open Question 4
What is the relationship between the size of the expanded model and its efficiency in terms of training time and inference speed? The paper mentions that adding 8 blocks provides optimal performance with minimal cost compared to larger models. This remains unresolved because the paper does not provide detailed analysis of training time, inference speed, or computational efficiency for different model sizes.

### Open Question 5
How does block expansion perform in multimodal settings where the model needs to preserve both language and vision capabilities? The paper mentions that block expansion could be explored in multimodal settings to preserve original language ability while adding vision capabilities. This remains unresolved because the paper only focuses on text-based models and does not explore multimodal applications.

## Limitations

- Limited generalizability to domains beyond code and math adaptation
- Lack of ablation studies on different interleaving patterns and configurations
- Missing direct validation of the identity mapping property before fine-tuning

## Confidence

**High Confidence Claims:**
- Block expansion effectively prevents catastrophic forgetting when original blocks are frozen and new blocks are properly initialized
- The interleaved placement strategy outperforms simple stacking approaches
- LLaMA Pro-8.3B achieves state-of-the-art performance on HumanEval and GSM8K benchmarks

**Medium Confidence Claims:**
- Zero initialization of output matrices guarantees identity mapping in LLaMA blocks
- The specific configuration (8 expanded blocks) is optimal for the code/math domain
- Instruction tuning yields consistent improvements across all task types

**Low Confidence Claims:**
- Block expansion will generalize to all domain adaptation scenarios
- The computational efficiency claims hold for all model scales
- Performance gains are solely due to the block expansion method (vs. data quality)

## Next Checks

1. **Identity Mapping Verification**: Implement a controlled experiment that measures base model outputs vs. expanded model outputs before any fine-tuning, confirming the identity mapping property with numerical precision metrics.

2. **Ablation on Interleaving Patterns**: Systematically vary the interleaving configuration (different P, M, N values) and measure performance degradation/gains to identify optimal patterns for different domain adaptation scenarios.

3. **Cross-Domain Generalization**: Apply the same block expansion methodology to a completely different domain (e.g., medical text) with a different base model size to validate generalizability beyond code and math tasks.