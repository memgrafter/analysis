---
ver: rpa2
title: 'SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction'
arxiv_id: '2407.20214'
source_url: https://arxiv.org/abs/2407.20214
tags:
- graph
- scene
- surgical
- segmentation
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SANGRIA addresses the challenge of surgical workflow prediction
  by introducing an end-to-end framework for unsupervised scene graph generation and
  optimization. The method leverages spectral and temporal clustering with lightweight
  correspondence matching to generate dynamic scene graphs, which are then jointly
  optimized with the downstream task of phase segmentation using only weak surgical
  phase labels.
---

# SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction

## Quick Facts
- arXiv ID: 2407.20214
- Source URL: https://arxiv.org/abs/2407.20214
- Authors: Çağhan Köksal; Ghazal Ghazaei; Felix Holm; Azade Farshad; Nassir Navab
- Reference count: 35
- Outperforms state-of-the-art methods on CATARACTS dataset, achieving 8% higher accuracy and 10% higher F1 score in surgical workflow recognition

## Executive Summary
SANGRIA addresses the challenge of surgical workflow prediction by introducing an end-to-end framework for unsupervised scene graph generation and optimization. The method leverages spectral and temporal clustering with lightweight correspondence matching to generate dynamic scene graphs, which are then jointly optimized with the downstream task of phase segmentation using only weak surgical phase labels. SANGRIA outperforms state-of-the-art methods on the CATARACTS dataset, achieving 8% higher accuracy and 10% higher F1 score in surgical workflow recognition. The approach demonstrates the importance of understanding scene components and their spatiotemporal relations in contributing to the downstream task while addressing the annotation scarcity problem in surgical video analysis.

## Method Summary
SANGRIA generates unsupervised scene graphs from surgical videos by extracting patch features using DINO-B, then applying spectral clustering with DMON to group similar patches into semantic regions. LightGlue provides sparse temporal connections between frames to maintain consistency across the video sequence. The dynamic scene graphs are jointly optimized with a GCN-based phase segmentation model using weak labels, eliminating the need for dense annotations. The framework achieves state-of-the-art performance on the CATARACTS dataset while reducing computational costs compared to fully-connected temporal approaches.

## Key Results
- 8% higher accuracy and 10% higher F1 score compared to state-of-the-art methods on CATARACTS dataset
- Joint optimization of dynamic scene graphs with phase segmentation improves surgical workflow prediction
- Reduces computational costs compared to fully-connected temporal graph approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of dynamic scene graphs with phase segmentation improves surgical workflow prediction accuracy.
- Mechanism: By integrating graph-based spectral clustering with temporal matching via LightGlue, the system creates temporally consistent scene representations that are directly optimized for the downstream task. The DMON module clusters patch-based graphs while the GCN predicts surgical phases, creating a feedback loop that refines both representations.
- Core assumption: Temporal consistency across frames is critical for accurate phase segmentation, and sparse temporal connections via correspondence matching provide sufficient information for this.
- Evidence anchors:
  - [abstract] "By jointly optimizing the spatiotemporal relations and node features of the dynamic scene graph with the downstream task of phase segmentation..."
  - [section] "By disambiguating video representations in an end-to-end optimization setup, SANGRIA achieves state-of-the-art performance on surgical phase segmentation"
  - [corpus] Weak evidence - corpus contains related works but no direct mechanism confirmation
- Break condition: If temporal consistency is not the primary factor for phase segmentation accuracy, or if the correspondence matching introduces too much noise from anatomical movement or fluid.

### Mechanism 2
- Claim: Unsupervised scene graph generation eliminates the need for dense surgical video annotations.
- Mechanism: The system uses foundation models (DINO) to extract patch features, then applies spectral clustering with DMON to group similar patches without requiring semantic labels. LightGlue provides sparse temporal connections between frames to maintain consistency across the video sequence.
- Core assumption: Foundation models can extract meaningful features from surgical scenes that correlate with semantic regions, even without task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "leveraging the flexibility of graph-based spectral clustering and the generalization capability of foundation models to generate unsupervised scene graphs"
  - [section] "Our approach leverages the flexibility of graph-based spectral clustering and the generalization capability of foundation models"
  - [corpus] Weak evidence - related works exist but no direct validation of this unsupervised approach
- Break condition: If foundation model features do not correlate with semantic regions in surgical scenes, or if the clustering cannot distinguish between similar-looking tools.

### Mechanism 3
- Claim: Sparse temporal connections via LightGlue are more efficient and effective than fully-connected temporal graphs.
- Mechanism: Instead of computing pairwise correlations across all patches in a temporal window (O(wn²d) complexity), LightGlue identifies sparse matches between prominent features in consecutive frames, creating a dynamic graph with reduced computational cost while maintaining temporal consistency.
- Core assumption: Temporal relations require coarser attention than spatial dependencies, making sparse connections sufficient for phase segmentation.
- Evidence anchors:
  - [section] "As temporal relations require a coarser level of attention compared to spatial dependencies... we suggest a sparse dynamic linking mechanism"
  - [section] "LightGlue correspondences effectively facilitate better temporal learning while maintaining comparable performance and considerably reducing the amount of computation"
  - [table 2] Performance comparison showing LightGlue vs fully-connected graphs
- Break condition: If the sparse connections miss critical temporal information needed for phase transitions, or if the computational savings come at the cost of accuracy.

## Foundational Learning

- Concept: Graph Neural Networks and spectral clustering
  - Why needed here: The method relies on DMON for unsupervised graph clustering and GCN for task prediction, requiring understanding of message passing, graph convolutions, and spectral graph theory
  - Quick check question: What is the difference between spatial and spectral approaches to graph convolution, and why might spectral methods be preferred for clustering tasks?

- Concept: Foundation models and self-supervised learning
  - Why needed here: DINO features serve as the initial patch representations without requiring task-specific supervision, leveraging self-supervised pretraining on large datasets
  - Quick check question: How do vision transformers like DINO learn meaningful representations without semantic labels, and what are the limitations when applied to surgical domains?

- Concept: Temporal correspondence matching
  - Why needed here: LightGlue provides the sparse temporal connections between frames, requiring understanding of feature matching, attention mechanisms, and local feature extraction
  - Quick check question: What are the key differences between optical flow and feature-based correspondence methods, and when would each be more appropriate for video analysis?

## Architecture Onboarding

- Component map: Frame → DINO features → Patch graph → DMON clustering → Dynamic scene graph → GCN → Phase prediction
- Critical path: Surgical video frames flow through DINO feature extraction, graph construction, DMON clustering, and GCN-based phase segmentation in an end-to-end optimization framework
- Design tradeoffs:
  - Window size vs. temporal consistency: Larger windows capture longer context but increase computational cost and may introduce noise
  - Sparsity level in temporal connections: More connections improve temporal consistency but increase computation
  - Foundation model choice: DINO provides strong features but may have domain gap for surgical scenes
- Failure signatures:
  - Phase segmentation accuracy drops with window size changes → Temporal modeling issue
  - Semantic segmentation mIoU low for tools but high for anatomy → Foundation model domain gap
  - Training instability with joint optimization → Loss balancing problem
- First 3 experiments:
  1. Ablation study on window size (1, 4, 8, 16) with fixed other parameters to identify optimal temporal context
  2. Comparison of LightGlue vs fully-connected temporal graphs on a small validation set to quantify computational vs accuracy tradeoff
  3. Prototype matching evaluation on a held-out subset to measure annotation efficiency and semantic accuracy

## Open Questions the Paper Calls Out
No specific open questions were identified in the provided paper content.

## Limitations
- Reliance on foundation model features without task-specific fine-tuning may create domain gaps for surgical scenes
- Performance generalization beyond cataract surgery remains unverified
- Sparse temporal connections may miss critical information during rapid phase transitions

## Confidence

- **High Confidence**: The joint optimization framework and computational efficiency improvements are well-supported by the experimental results
- **Medium Confidence**: The effectiveness of unsupervised scene graph generation and the superiority of sparse temporal connections, as these rely on specific implementation details and assumptions about foundation model features
- **Low Confidence**: Claims about the method's generalizability to other surgical procedures, as only cataract surgery was evaluated

## Next Checks
1. Evaluate the foundation model domain gap by fine-tuning DINO on a small surgical dataset and comparing performance to unsupervised features
2. Conduct ablation studies on temporal window size and sparsity level to quantify the tradeoff between computational efficiency and accuracy
3. Test the framework on a different surgical procedure (e.g., cholecystectomy) to assess generalizability beyond cataract surgery