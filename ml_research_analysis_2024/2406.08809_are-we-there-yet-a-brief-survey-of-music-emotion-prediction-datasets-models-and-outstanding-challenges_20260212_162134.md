---
ver: rpa2
title: Are We There Yet? A Brief Survey of Music Emotion Prediction Datasets, Models
  and Outstanding Challenges
arxiv_id: '2406.08809'
source_url: https://arxiv.org/abs/2406.08809
tags:
- music
- emotion
- dataset
- datasets
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of music emotion
  recognition (MER) datasets, models, and challenges. The authors identify the need
  for standardized benchmarks, larger and more diverse datasets, and improved model
  interpretability to advance the field.
---

# Are We There Yet? A Brief Survey of Music Emotion Prediction Datasets, Models and Outstanding Challenges

## Quick Facts
- arXiv ID: 2406.08809
- Source URL: https://arxiv.org/abs/2406.08809
- Authors: Jaeyong Kang; Dorien Herremans
- Reference count: 40
- Key outcome: Identifies need for standardized benchmarks, larger and more diverse datasets, and improved model interpretability in music emotion recognition

## Executive Summary
This survey provides a comprehensive overview of music emotion recognition (MER) datasets, models, and challenges. The authors identify the need for standardized benchmarks, larger and more diverse datasets, and improved model interpretability to advance the field. Key findings include the dominance of Russell's dimensional emotion model, the importance of distinguishing between perceived and induced emotions, and the potential of multimodal approaches incorporating audio, MIDI, lyrics, and physiological signals. The survey also highlights the limitations of current datasets, such as subjective labels, cultural bias, and noise, and proposes future directions for research.

## Method Summary
The survey synthesizes findings from multiple MER datasets and models, analyzing their strengths and limitations. It focuses on identifying bottlenecks in dataset quality, annotation consistency, and model generalization. The authors examine the prevalence of dimensional vs. categorical emotion models, the impact of multimodal data, and the challenges of cross-dataset evaluation. The survey draws on existing literature to highlight gaps in the field and propose future research directions.

## Key Results
- Dominance of Russell's dimensional emotion model in MER datasets
- Importance of distinguishing between perceived and induced emotions
- Potential of multimodal approaches incorporating audio, MIDI, lyrics, and physiological signals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Current MER models are bottlenecked by dataset quality, annotation noise, and limited cross-dataset generalization.
- **Mechanism:** Systematic issues across multiple datasets (MTG-Jamendo, DEAM, PMEmo) include skewed label distributions, subjective and culturally biased annotations, and absence of standardized benchmarks, directly degrading model performance and hindering comparison.
- **Core assumption:** Dataset limitations and annotation inconsistencies are primary barriers to MER progress.
- **Evidence anchors:** [abstract] "issues related to dataset quality, annotation consistency, and model generalization"; [section] "Limited datasets hinder the development and evaluation of robust MER models"; [corpus] "Average neighbor FMR=0.495, average citations=0.0" suggesting low visibility/impact.
- **Break condition:** If future datasets achieve standardized splits and balanced labels, this bottleneck may dissolve.

### Mechanism 2
- **Claim:** Dimensional emotion models (valence/arousal) are more prevalent and easier to annotate than categorical ones, but still suffer from high inter-rater variability.
- **Mechanism:** Russell's dimensional model dominates, yet dynamic annotations require continuous input, introducing noise. Inter-rater reliability is low, especially across cultures and training levels.
- **Core assumption:** Dimensional models are both more common and more problematic due to their continuous nature.
- **Evidence anchors:** [abstract] "dominance of Russell's dimensional emotion model"; [section] "inter-rater reliability, a metric of agreement between raters often calculated using Cronbach's Alpha".
- **Break condition:** If categorical models gain traction with robust mappings to dimensional space, or if annotation interfaces improve, variability may decrease.

### Mechanism 3
- **Claim:** Multimodal approaches (audio + lyrics + physiological signals) improve emotion prediction accuracy but are limited by data scarcity and integration complexity.
- **Mechanism:** Datasets like DEAP and HKU956 combine audio with physiological signals, and models that fuse modalities show promise. However, such datasets are rare, and multimodal model performance gains are offset by increased complexity and integration challenges.
- **Core assumption:** Multimodal data inherently improves emotion prediction but is hard to scale.
- **Evidence anchors:** [abstract] "impact of different modalities, such as audio, MIDI, and physiological signals, on the effectiveness of emotion prediction models".
- **Break condition:** If large-scale multimodal datasets become available, or if unimodal models saturate in performance, multimodal approaches may become standard.

## Foundational Learning

- **Concept:** Dimensional vs. categorical emotion models
  - Why needed here: The survey repeatedly contrasts these representations; understanding them is key to interpreting model design choices.
  - Quick check question: What are the two axes in Russell's circumplex model, and how do they differ from categorical labels?

- **Concept:** Perceived vs. induced emotion
  - Why needed here: Datasets differ in which they capture; this affects model applicability (e.g., therapeutic vs. recommendation systems).
  - Quick check question: How do physiological signals relate to induced emotion, and why are they rarely public?

- **Concept:** Cross-dataset generalization
  - Why needed here: The paper stresses that models trained on one dataset often fail on another; this is a major research gap.
  - Quick check question: Why is it hard to map labels between datasets that use different emotion models?

## Architecture Onboarding

- **Component map:** Data ingestion → Preprocessing (spectrogram, MIDI parsing) → Feature extraction (audio embeddings, lyrics NLP) → Model (CNN, Transformer, multimodal fusion) → Output (classification/regression) → Evaluation (accuracy, MSE, PCC)
- **Critical path:** 1. Dataset selection and preprocessing 2. Feature engineering (audio + optional modalities) 3. Model training with cross-validation 4. Evaluation against standardized metrics
- **Design tradeoffs:** Unimodal vs. multimodal: simplicity vs. accuracy; Static vs. dynamic labels: ease of annotation vs. temporal richness; Regression vs. classification: precision vs. interpretability
- **Failure signatures:** Low inter-rater reliability → noisy labels; High variance across datasets → poor generalization; Skewed label distributions → biased predictions
- **First 3 experiments:** 1. Train a baseline CNN on MTG-Jamendo; report PR-AUC and ROC-AUC 2. Add lyrics features via a pretrained transformer; compare performance 3. Apply cross-dataset evaluation: train on MTG-Jamendo, test on DEAM; measure drop in metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop standardized benchmarks and evaluation protocols that enable fair cross-dataset comparisons in MER?
- Basis in paper: [explicit] The paper highlights the lack of standardized benchmarks and consistent evaluation metrics across different MER studies, making it difficult to compare results even when using the same dataset.
- Why unresolved: The heterogeneity of datasets (different genres, emotion models, annotation procedures) and inconsistent use of evaluation metrics and train/test splits hinder the development of standardized benchmarks.
- What evidence would resolve it: A set of standardized evaluation protocols, including a common set of metrics, consistent train/test splits, and dataset-agnostic benchmarking tools, that enable fair and reproducible comparisons across different MER studies.

### Open Question 2
- Question: How can we effectively handle the subjective and noisy nature of emotion annotations in MER datasets?
- Basis in paper: [explicit] The paper discusses the subjective nature of emotion perception, the potential for personal and cultural biases in annotations, and the challenges of low inter-rater reliability, all of which introduce noise into the labels.
- Why unresolved: The inherent complexity and subjectivity of emotions, along with individual differences in perception and cultural backgrounds, make it difficult to obtain consistent and reliable annotations.
- What evidence would resolve it: Development and validation of annotation methods that improve consistency and reliability, such as incorporating relative annotation strategies, using qualification tasks, and considering rater profiles in inter-rater reliability calculations.

### Open Question 3
- Question: How can we leverage multimodal data, including physiological signals, to improve the prediction of induced emotions in MER?
- Basis in paper: [explicit] The paper discusses the potential of using physiological data (e.g., EEG, heart rate) to predict induced emotions, as well as the importance of considering other modalities like video and lyrics in understanding emotional responses to music.
- Why unresolved: While some datasets include physiological signals, there is a lack of large-scale, publicly available datasets with multimodal or biosensor data, limiting the development of models for induced emotion prediction.
- What evidence would resolve it: Creation of larger, diverse datasets that include induced emotion labels and multimodal data, along with the development and validation of models that effectively integrate and leverage these different data sources for improved emotion prediction.

## Limitations
- Lack of standardized benchmarks and evaluation protocols across MER studies
- Subjective and noisy nature of emotion annotations introducing bias and inconsistency
- Limited availability of large-scale, multimodal datasets with induced emotion labels

## Confidence
- Confidence in identified bottlenecks: Medium
- Confidence in multimodal approaches improving accuracy: Low
- Confidence in cross-dataset generalization claims: Medium

## Next Checks
1. Conduct cross-dataset evaluation: Train models on MTG-Jamendo and test on DEAM to quantify generalization gaps.
2. Replicate multimodal experiments: Implement a baseline CNN + lyrics model and compare performance gains.
3. Investigate label noise: Apply label smoothing or co-teaching to mitigate the impact of subjective annotations and measure improvements in model robustness.