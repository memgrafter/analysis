---
ver: rpa2
title: 'No $D_{\text{train}}$: Model-Agnostic Counterfactual Explanations Using Reinforcement
  Learning'
arxiv_id: '2405.18563'
source_url: https://arxiv.org/abs/2405.18563
tags:
- cfwot
- rate
- rule-based
- features
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NTD-CFE is a reinforcement learning based method for generating
  counterfactual explanations without requiring training datasets. It formulates the
  counterfactual search as an RL problem where the environment is the predictive model,
  states are sequences of counterfactuals, and actions are small perturbations.
---

# No $D_{\text{train}}$: Model-Agnostic Counterfactual Explanations Using Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2405.18563
- **Source URL:** https://arxiv.org/abs/2405.18563
- **Authors:** Xiangyu Sun; Raquel Aoki; Kevin H. Wilson
- **Reference count:** 40
- **Primary result:** RL-based method achieves significantly better proximity (mean 54.7 vs 340.3) and sparsity (mean 31.2 vs 260.0) than four baselines without training data access

## Executive Summary
NTD-CFE introduces a reinforcement learning approach for generating counterfactual explanations without requiring training datasets. The method treats the predictive model as a black-box environment and uses RL to explore the counterfactual space through sequential perturbations. It supports multivariate time-series data with both continuous and discrete features while allowing users to specify non-actionable features, immutable features, preferred features, and causal constraints. In extensive experiments across 8 real-world datasets and 5 predictive models, NTD-CFE outperforms four baseline methods in both proximity and sparsity metrics.

## Method Summary
The method formulates counterfactual search as a Markov decision process where the predictive model serves as the environment. The RL agent generates sequences of counterfactuals by applying small perturbations to the input, with rewards combining prediction success and proximity to the original instance. A policy network parameterized with two hidden layers (1000, 100 neurons) learns to navigate the counterfactual space. The approach enforces user-specified constraints including non-actionable features, immutable features, preferred features via feasibility weights, and causal constraints encoded as partial structural causal models. State transitions apply perturbations while checking constraints, and the method operates without any access to training data.

## Key Results
- NTD-CFE achieved significantly better proximity (mean 54.7 vs 340.3) compared to four baselines
- The method demonstrated superior sparsity (mean 31.2 vs 260.0) across all evaluated datasets
- Success rates remained high while maintaining plausibility through causal constraint enforcement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL-based exploration avoids reliance on training data while still generating proximal counterfactuals.
- Mechanism: NTD-CFE formulates counterfactual search as a Markov decision process where the predictive model is the environment. The agent learns to perturb the input step-by-step to reach a target prediction without accessing training samples.
- Core assumption: The predictive model can be treated as a black-box environment that can be queried repeatedly.
- Evidence anchors:
  - [abstract] "formulates the counterfactual search as an RL problem where the environment is the predictive model"
  - [section] "CFWoT works for both static and time-series data, we focus on the harder setting of multivariate time-series data in this paper."
  - [corpus] Weak evidence. The corpus mentions large state spaces but does not explicitly confirm training-data-free RL CFEs.
- Break condition: If the model is too expensive to query repeatedly, the RL exploration becomes impractical.

### Mechanism 2
- Claim: Feature feasibility weights (Wfsib) guide the RL agent toward user-preferred features, improving actionability.
- Mechanism: The reward function includes a proximity term weighted by Wfsib, encouraging the agent to prefer small changes in user-specified features.
- Core assumption: Users can provide reasonable feasibility preferences or defaults (Wfsib=1).
- Evidence anchors:
  - [abstract] "allows users to specify...preferred features"
  - [section] "The proximity measure Dpxmt can be any suitable measures for the application domain...weighted by Wfsib"
  - [corpus] No direct corpus evidence. The related papers discuss counterfactual metrics but not feature feasibility weighting in RL.
- Break condition: If Wfsib is poorly set (e.g., too large for irrelevant features), the agent may waste effort on suboptimal paths.

### Mechanism 3
- Claim: Causal constraints encoded as partial SCM rules are enforced during state transitions, ensuring plausibility.
- Mechanism: After each perturbation, CFWoT checks the new state against CSCM rules and adjusts values to maintain causal consistency.
- Core assumption: Partial causal rules are available and can be checked efficiently.
- Evidence anchors:
  - [abstract] "causal constraints which CFWoT guarantees will be respected"
  - [section] "CFWoT works with partial SCMs...An (partial) SCM can be encoded as a set of rules"
  - [corpus] Weak evidence. The corpus mentions counterfactual explanations but not RL-based enforcement of partial causal rules.
- Break condition: If the rule set is incomplete or contradictory, plausibility guarantees may fail.

## Foundational Learning

- Concept: Reinforcement Learning fundamentals (states, actions, rewards, policy networks)
  - Why needed here: NTD-CFE uses RL to explore the counterfactual space without training data.
  - Quick check question: What does the RL agent receive as a reward when it generates a valid counterfactual?

- Concept: Counterfactual explanation properties (validity, actionability, proximity, sparsity, plausibility)
  - Why needed here: NTD-CFE explicitly optimizes these properties via its reward and constraint mechanisms.
  - Quick check question: How does the method ensure a counterfactual is both valid and actionable?

- Concept: Multivariate time-series representation and preprocessing
  - Why needed here: NTD-CFE operates on sequences of feature vectors over time and requires standardization.
  - Quick check question: Why is standardization assumed, and what happens if it is not possible?

## Architecture Onboarding

- Component map:
  - RL policy network (πθ) -> produces action distributions
  - State transition function (Fp) -> applies perturbations
  - Reward function (R) -> combines validity and proximity
  - Constraint checker (CSCM) -> enforces causal rules
  - In-distribution detector (optional) -> enforces plausibility
  - Predictive model (f) -> black-box environment

- Critical path:
  1. Initialize episode with original input
  2. Sample action from policy network
  3. Apply action via state transition
  4. Check constraints and adjust
  5. Query model for reward
  6. Update policy via RL
  7. Return best counterfactual found

- Design tradeoffs:
  - Exploration vs. exploitation in RL -> affects success rate
  - Constraint strictness -> affects plausibility vs. success trade-off
  - Feature feasibility weights -> user preference vs. model guidance

- Failure signatures:
  - RL agent stuck in invalid regions -> low success rate
  - Too many constraint violations -> low plausibility rate
  - Excessive query cost -> impractical runtime

- First 3 experiments:
  1. Run on a simple rule-based model (like those in Appendix B) with known ground truth to verify validity and proximity.
  2. Compare success rate vs. maximum episodes/iterations to understand exploration sufficiency.
  3. Test with and without causal constraints to measure plausibility impact.

## Open Questions the Paper Calls Out
- How does CFWoT's performance scale with increasing time-series length and feature dimensionality?
- What is the impact of different reinforcement learning algorithms on CFWoT's performance?
- How robust is CFWoT to violations of the standardization assumption for continuous features?

## Limitations
- Reliance on repeated model queries raises scalability concerns for expensive black-box models
- Performance depends heavily on well-specified user constraints and causal rules
- RL formulation assumes the model can be treated as a stable environment

## Confidence
- **High Confidence**: The RL formulation for counterfactual search is clearly specified and follows established MDP principles. The experimental design with multiple datasets and models is rigorous.
- **Medium Confidence**: The claim of being training-data-free is supported by the methodology but requires validation that the RL agent can learn effective policies without any pretraining or auxiliary data.
- **Low Confidence**: The causal constraint enforcement mechanism's effectiveness depends on the completeness and correctness of the partial SCM rules, which are difficult to verify without access to the full implementation.

## Next Checks
1. **Ground Truth Validation**: Test CFWoT on synthetic rule-based models with known ground truth counterfactuals to verify that the method can recover optimal solutions when they exist.

2. **Constraint Robustness Test**: Systematically remove or modify causal constraints and user preferences to quantify their impact on success rates and plausibility, establishing sensitivity to constraint specification.

3. **Query Efficiency Analysis**: Measure the number of model queries required for successful counterfactual generation across different model types and dataset complexities to establish practical scalability limits.