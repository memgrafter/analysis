---
ver: rpa2
title: On the Relationship Between Monotone and Squared Probabilistic Circuits
arxiv_id: '2408.00876'
source_url: https://arxiv.org/abs/2408.00876
tags:
- circuits
- circuit
- monotone
- squared
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the expressiveness gap between monotone and\
  \ squared probabilistic circuits (PCs), showing they are incomparable in general\u2014\
  each can be exponentially more efficient than the other for representing probability\
  \ distributions. To reconcile this, the authors propose InceptionPCs, which allow\
  \ latent variables to be summed inside or outside the square, thereby encompassing\
  \ both monotone and squared PCs as special cases."
---

# On the Relationship Between Monotone and Squared Probabilistic Circuits

## Quick Facts
- **arXiv ID**: 2408.00876
- **Source URL**: https://arxiv.org/abs/2408.00876
- **Reference count**: 30
- **Primary result**: InceptionPCs unify monotone and squared PCs, achieving better bits-per-dimension scores on MNIST (1.24-1.25 vs 1.30) and FashionMNIST (3.46-3.47 vs 3.56)

## Executive Summary
This paper addresses the expressiveness gap between monotone and squared probabilistic circuits (PCs), showing they are incomparable in general—each can be exponentially more efficient than the other for representing probability distributions. To reconcile this, the authors propose InceptionPCs, which allow latent variables to be summed inside or outside the square, thereby encompassing both monotone and squared PCs as special cases. InceptionPCs use complex parameters and are tractable due to structured decomposability. Empirically, InceptionPCs outperform both monotone and squared PCs on image datasets such as MNIST and FashionMNIST, with bits-per-dimension (bpd) scores improving from ~1.30 to ~1.24–1.25 for MNIST and from ~3.56 to ~3.46–3.47 for FashionMNIST. The method leverages tensorized architectures for scalability and uses Wirtinger derivatives for complex optimization.

## Method Summary
InceptionPCs are constructed by allowing two types of latent variables (U and W) at each sum node, with cardinality KU and KW. When KU > 1 and KW = 1, the model behaves like a monotone PC; when KU = 1 and KW > 1, it behaves like a squared PC. By choosing KU, KW > 1, InceptionPCs can capture dependencies that neither special case can express efficiently. The architecture uses complex weights and input functions, with the probability distribution defined as the squared modulus of the complex-valued circuit output. For scalability, tensorized architectures with quad-tree region graph structures and CANDECOMP-PARAFAC (CP) layers are employed. Training uses Wirtinger derivatives for optimization with complex parameters.

## Key Results
- InceptionPCs achieve better bpd scores than both monotone and squared PCs on MNIST (1.24-1.25 vs 1.30) and FashionMNIST (3.46-3.47 vs 3.56)
- The expressiveness gap between monotone and squared PCs is reconciled through InceptionPCs' unified framework
- Tensorized architectures enable scalability while maintaining structured decomposability for tractable inference

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: InceptionPCs achieve higher expressiveness than either monotone or squared PCs by allowing latent variables to be summed inside or outside the square, depending on whether the latent variable is associated with the U or W layer.
- **Mechanism**: The architecture uses two types of latent variables (U and W) at each sum node, with cardinality KU and KW. When summing over U inside the square (KU > 1, KW = 1), the model behaves like a monotone PC; when summing over W outside the square (KU = 1, KW > 1), it behaves like a squared PC. By choosing KU, KW > 1, the model can capture dependencies that neither special case can express efficiently.
- **Core assumption**: Structured decomposability ensures tractability of both the forward pass and the partition function computation, even with complex parameters.
- **Evidence anchors**:
  - [abstract]: "InceptionPCs thus offer a unified, more expressive framework for tractable probabilistic modeling."
  - [section 4.1]: "By choosing KU , KW > 1, we obtain a generalized PC model that is potentially more expressive than either individually"
  - [corpus]: The neighboring paper "Sum of Squares Circuits" corroborates that squared PCs can be more expressive than monotone PCs, while the paper itself shows the reverse is also true.
- **Break condition**: If the structured decomposability is lost, the tractable computation of the partition function becomes intractable, breaking the expressiveness claim.

### Mechanism 2
- **Claim**: Complex parameters enable richer modeling of dependencies than real parameters alone, because they allow negative and phase information to be captured within a single parameter.
- **Mechanism**: InceptionPCs use complex weights and input functions, with the probability distribution defined as the squared modulus of the complex-valued circuit output. This allows the model to represent subtractive mixtures of distributions (as in squared PCs) while retaining the non-negativity required for probability distributions.
- **Core assumption**: Wirtinger derivatives can be used to optimize complex parameters efficiently in the context of gradient descent.
- **Evidence anchors**:
  - [abstract]: "InceptionPCs use complex parameters and are tractable due to structured decomposability."
  - [section 4]: "One can also allow for weights and input functions that are complex, i.e. take values in the field C. Then, to ensure the non-negativity of the squared circuit, we multiply a circuit with its complex conjugate."
  - [corpus]: The paper "How to Square Tensor Networks and Circuits Without Squaring Them" discusses the complexity introduced by squaring, supporting the need for complex parameterization.
- **Break condition**: If the optimization landscape becomes too rugged due to complex parameters, training may fail to converge, limiting practical expressiveness.

### Mechanism 3
- **Claim**: Tensorized architectures enable InceptionPCs to scale to image datasets like MNIST and FashionMNIST while maintaining the benefits of structured decomposability.
- **Mechanism**: The paper uses quad-tree region graph structures with CANDECOMP-PARAFAC (CP) layers, where sum and product nodes are grouped into layers by scope. This allows efficient GPU acceleration and reduces the memory footprint compared to a fully expanded circuit.
- **Core assumption**: The tensorized structure preserves the smoothness and structured decomposability properties required for tractable inference.
- **Evidence anchors**:
  - [section 4.2]: "To implement InceptionPCs at scale and with GPU acceleration, we follow recent trends in probabilistic circuit learning [Peharz et al., 2020, Mari et al., 2023] and consider tensorized architectures, where sum and product nodes are grouped into layers by scope."
  - [abstract]: "InceptionPCs thus offer a unified, more expressive framework for tractable probabilistic modeling."
  - [corpus]: The neighboring paper "Circuit Compositions" discusses modular structures in transformer-based models, which is conceptually similar to the tensorized approach for scalability.
- **Break condition**: If the tensorized approximation loses too much expressivity, the performance gains over simpler architectures may disappear.

## Foundational Learning

- **Concept**: Structured decomposability in probabilistic circuits
  - **Why needed here**: Ensures that the partition function can be computed efficiently, which is critical for training and inference in InceptionPCs.
  - **Quick check question**: What is the computational complexity of computing the partition function in a structured-decomposable circuit versus a general circuit?

- **Concept**: Complex number arithmetic and Wirtinger derivatives
  - **Why needed here**: InceptionPCs use complex parameters, so understanding how to differentiate and optimize with respect to complex variables is essential.
  - **Quick check question**: How does the Wirtinger derivative differ from the standard complex derivative, and why is it necessary for optimization with complex parameters?

- **Concept**: Latent variable interpretation of probabilistic circuits
  - **Why needed here**: The InceptionPC construction relies on interpreting sum nodes as introducing categorical latent variables, and deciding whether to marginalize them inside or outside the square.
  - **Quick check question**: In the latent variable interpretation, what is the difference between summing out a latent variable before squaring versus after squaring?

## Architecture Onboarding

- **Component map**: Input layer -> Sum layers (U and W) -> Product layers -> Output layer (squared modulus)
- **Critical path**: Forward pass -> Compute squared modulus -> Backward pass via Wirtinger derivatives -> Update complex parameters -> Compute partition function (structured decomposable)
- **Design tradeoffs**:
  - **Expressiveness vs. efficiency**: Increasing KU and KW increases expressiveness but also increases training time quadratically.
  - **Real vs. complex parameters**: Complex parameters allow richer dependencies but may lead to noisier gradients during training.
  - **Tensorized vs. full circuit**: Tensorization improves scalability but may introduce approximation error.
- **Failure signatures**:
  - Training loss plateaus or diverges: Likely due to optimization issues with complex parameters.
  - Partition function computation becomes intractable: Indicates loss of structured decomposability.
  - Performance does not improve with larger KU or KW: Suggests the architecture is not capturing the right dependencies.
- **First 3 experiments**:
  1. Train a monotone PC (KU > 1, KW = 1) on MNIST and measure bits-per-dimension (bpd).
  2. Train a squared PC (KU = 1, KW > 1) with real parameters on MNIST and compare bpd to monotone PC.
  3. Train an InceptionPC (KU > 1, KW > 1) with complex parameters on MNIST and compare bpd to both baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can InceptionPCs with more than two layers of latent variables improve expressiveness and performance?
- **Basis in paper**: [explicit] The authors note that increasing KU beyond 2 does not provide much benefit and suggest this is a point needing further investigation.
- **Why unresolved**: The paper only empirically tested KU=2 and KU=8, leaving open whether deeper InceptionPC architectures might yield better results.
- **What evidence would resolve it**: Empirical evaluation of InceptionPCs with deeper latent variable structures (e.g., 3+ layers) on benchmark datasets, comparing expressiveness and performance to current models.

### Open Question 2
- **Question**: What optimization methods beyond Wirtinger derivatives can improve training efficiency for InceptionPCs?
- **Basis in paper**: [inferred] The authors mention designing more efficient architectures and deriving an EM-style algorithm as promising future directions.
- **Why unresolved**: While Wirtinger derivatives are used, the paper suggests potential improvements in optimization methodology are unexplored.
- **What evidence would resolve it**: Comparative studies of InceptionPC training using alternative optimization methods (e.g., EM algorithms, specialized gradient methods) and demonstrating improvements in convergence speed or model quality.

### Open Question 3
- **Question**: Are there theoretical guarantees on the expressive power of InceptionPCs relative to monotone and squared PCs?
- **Basis in paper**: [inferred] The paper establishes that monotone and squared PCs are incomparable in expressive efficiency, but does not formally characterize InceptionPCs' expressiveness.
- **Why unresolved**: While InceptionPCs encompass both monotone and squared PCs as special cases, their theoretical expressiveness relative to these models is not formally proven.
- **What evidence would resolve it**: Formal proofs demonstrating that InceptionPCs can efficiently represent any distribution representable by monotone or squared PCs, or identifying classes of distributions where InceptionPCs offer advantages.

### Open Question 4
- **Question**: How does the choice of scope decomposition affect InceptionPC performance and tractability?
- **Basis in paper**: [inferred] The paper uses structured decomposability for tractability but does not explore how different decompositions impact performance.
- **Why unresolved**: While structured decomposability is required for tractability, the paper does not investigate whether certain scope decompositions lead to better models.
- **What evidence would resolve it**: Systematic evaluation of InceptionPCs using different scope decomposition strategies, measuring both computational efficiency and modeling performance across various datasets.

## Limitations

- The computational overhead of complex parameter optimization and potential for training instability remain significant concerns
- While structured decomposability ensures tractability, the exact conditions under which this property is preserved during training are not fully specified
- The tensorized architecture, while enabling scalability, may introduce approximation errors that could limit the practical expressiveness gains

## Confidence

- **High**: Theoretical framework for InceptionPCs and their relationship to monotone/squared PCs
- **Medium**: Empirical performance improvements on MNIST and FashionMNIST datasets
- **Medium**: Claims about computational efficiency and scalability through tensorization
- **Low**: Long-term training stability with complex parameters

## Next Checks

1. **Training Stability Analysis**: Systematically vary the learning rate and batch size for complex parameter optimization to identify stable training configurations and failure modes.

2. **Expressiveness Scaling Study**: Measure the computational cost (in FLOPs) and performance (in bpd) as KU and KW increase, to quantify the trade-off between expressiveness and efficiency.

3. **Structured Decomposability Preservation**: Implement checks during training to verify that the circuit maintains structured decomposability properties, and measure the impact when these properties are violated.