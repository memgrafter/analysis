---
ver: rpa2
title: 'Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation
  of LLMs for Low-Resource NLP'
arxiv_id: '2408.04303'
source_url: https://arxiv.org/abs/2408.04303
tags:
- language
- languages
- tokens
- token
- tatar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces trans-tokenization, a cross-lingual vocabulary
  transfer method that adapts a high-resource monolingual LLM to an unseen target
  language by initializing target language token embeddings using weighted averages
  of semantically similar tokens from the source language. The method leverages translation
  resources to align tokens across languages and maps embeddings accordingly.
---

# Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP

## Quick Facts
- arXiv ID: 2408.04303
- Source URL: https://arxiv.org/abs/2408.04303
- Authors: François Remy; Pieter Delobelle; Hayastan Avetisyan; Alfiya Khabibullina; Miryam de Lhoneux; Thomas Demeester
- Reference count: 40
- Primary result: State-of-the-art zero-shot translation for Tatar using trans-tokenization

## Executive Summary
This paper introduces trans-tokenization, a method for adapting high-resource monolingual LLMs to unseen target languages by initializing target language token embeddings using weighted averages of semantically similar tokens from the source language. The approach leverages translation resources to align tokens across languages and maps embeddings accordingly. Experiments on low-resource languages (Tatar, Armenian) and mid-resource languages (Dutch) demonstrate competitive performance in language modeling, word analogies, text summarization, and machine translation. Notably, trans-tokenization achieves state-of-the-art zero-shot translation for Tatar without requiring high-quality parallel data, significantly lowering the data and computational requirements for developing LLMs for low-resource languages.

## Method Summary
Trans-tokenization adapts a high-resource monolingual LLM to an unseen target language by initializing target language token embeddings using weighted averages of semantically similar tokens from the source language. The method leverages translation resources (parallel corpora) to align tokens across languages, then maps embeddings accordingly. A Hydra LLM is created by combining the original source language model with the trans-tokenized target language model, enabling zero-shot translation by encoding in the source language and generating in the target language.

## Key Results
- Achieved perplexity of 11.1 on Tatar language modeling, compared to GPT-NEO's 21.2
- Demonstrated state-of-the-art zero-shot translation for Tatar without requiring high-quality parallel data
- Showed competitive performance across multiple tasks (word analogies, text summarization) for low-resource languages (Tatar, Armenian) and mid-resource languages (Dutch)

## Why This Works (Mechanism)

### Mechanism 1
Cross-lingual token embeddings can be effectively initialized using weighted averages of semantically similar tokens from a high-resource language. Translation resources (parallel corpora) are used to align tokens across languages, with the embedding of a target language token initialized as a weighted average of the embeddings of source language tokens it aligns with, with weights proportional to alignment frequency. Core assumption: Semantic similarity between aligned tokens is sufficient to transfer meaningful embeddings across languages. Evidence anchors: abstract, section 3, corpus notes. Break condition: If parallel corpora are too small or noisy, alignment accuracy drops, leading to poor embedding initialization.

### Mechanism 2
Trans-tokenization enables effective zero-shot translation by creating Hydra LLMs with swappable language modeling heads and embedding tables. A Hydra LLM combines the original source language model with a trans-tokenized target language model, allowing encoding in the source language and generating in the target language. Core assumption: The combined model can effectively leverage the source language's fluency and the target language's adapted embeddings for translation. Evidence anchors: abstract, section 4, corpus notes. Break condition: If the trans-tokenized model's embeddings are poor quality, zero-shot translation performance will suffer.

### Mechanism 3
Trans-tokenization outperforms character-based embedding reinitialization techniques for GPT-style models. Unlike character-based methods, trans-tokenization uses SMT-based alignment to capture token sequence information, which is crucial for auto-regressive models. Core assumption: GPT-style models require precise token sequence modeling information that cannot be recovered by character-based methods. Evidence anchors: section 6.5, appendix D, corpus notes. Break condition: If the parallel corpus is too small or the alignment tool is ineffective, the benefits of trans-tokenization over character-based methods may diminish.

## Foundational Learning

- Concept: Statistical Machine Translation (SMT) alignment
  - Why needed here: Trans-tokenization relies on SMT tools like FastAlign to create probabilistic token mappings between source and target languages.
  - Quick check question: How does SMT alignment differ from character-based alignment, and why is it more suitable for trans-tokenization?

- Concept: Subword tokenization and byte-pair encoding (BPE)
  - Why needed here: Understanding how BPE tokenizers work is crucial for grasping the challenges of adapting LLMs to new languages and the need for trans-tokenization.
  - Quick check question: What are the limitations of BPE tokenizers for low-resource languages, and how does trans-tokenization address these limitations?

- Concept: Embedding initialization and fine-tuning
  - Why needed here: Trans-tokenization involves initializing target language embeddings and fine-tuning the model for the new language. Understanding these concepts is essential for implementing and evaluating the method.
  - Quick check question: What are the key differences between initializing embeddings using trans-tokenization versus random initialization, and how do these choices impact model performance?

## Architecture Onboarding

- Component map: Source language LLM (Mistral-7B) -> Target language tokenizer (BPE) -> Parallel corpus for token alignment -> SMT alignment tool (FastAlign) -> Trans-tokenization library for embedding initialization -> Hydra LLM class for zero-shot translation

- Critical path:
  1. Create target language tokenizer
  2. Align tokens using parallel corpus and SMT tool
  3. Initialize target language embeddings using trans-tokenization
  4. Fine-tune model on target language data
  5. (Optional) Create Hydra LLM for zero-shot translation

- Design tradeoffs:
  - Using parallel corpora vs. word translation dictionaries for alignment
  - Freezing vs. fine-tuning different layers of the model
  - Creating Hydra LLMs vs. separate models for each language

- Failure signatures:
  - Poor perplexity on target language data
  - Low performance on downstream tasks
  - Ineffective zero-shot translation in Hydra LLMs

- First 3 experiments:
  1. Evaluate perplexity of trans-tokenized model on target language data
  2. Test performance on a simple downstream task (e.g., word analogies)
  3. Assess zero-shot translation quality using Hydra LLM

## Open Questions the Paper Calls Out

### Open Question 1
How can we improve cross-lingual vocabulary transfer for languages with significantly different morphological structures? The paper notes that Armenian, an agglutinative language, presents challenges for alignment models due to its rich morphological structure, resulting in more unique words compared to English. The paper suggests that better results could be achieved by cleaning the parallel data before computing the word-level mappings and using a better alignment tool, but does not explore these solutions in detail. Experiments comparing the performance of trans-tokenization with different alignment tools and data cleaning techniques on morphologically diverse languages would resolve this question.

### Open Question 2
What is the impact of using different source languages for initializing embeddings in Hydra LLMs? The paper explores the impact of using English and Russian as source languages for initializing Tatar embeddings, finding that neither performs significantly better than the other, and combining both initializations provides some benefit. The paper does not investigate the potential of using multiple source languages for other low-resource languages or explore the underlying reasons for the limited similarity between English and Russian initialized embeddings. Experiments comparing the performance of Hydra LLMs initialized with different combinations of source languages for various low-resource languages, along with analyses of the similarity between embeddings from different source languages, would resolve this question.

### Open Question 3
How can we improve the performance of trans-tokenization for GPT-style models compared to BERT-style models? The paper suggests that the key difference between BERT and GPT models lies in the auto-regressive nature of GPT models, which requires specific token sequences for proper generation, making the embedding initialization more critical. The paper does not provide a detailed analysis of the differences in embedding initialization strategies between BERT and GPT models or explore potential solutions to improve trans-tokenization for GPT models. Experiments comparing the performance of trans-tokenization on BERT and GPT models for various language pairs, along with analyses of the impact of different embedding initialization strategies on the quality of generated text, would resolve this question.

## Limitations
- Reliance on parallel corpora for token alignment, though quality requirements are not fully characterized
- Zero-shot translation effectiveness only demonstrated for one low-resource language pair (Tatar)
- Architectural approach of creating Hydra LLMs may introduce stability concerns across different operational modes

## Confidence
**High Confidence:** The core trans-tokenization mechanism for initializing target language embeddings using weighted averages of aligned source tokens is well-supported by experimental results across multiple languages and tasks.

**Medium Confidence:** The claim of achieving state-of-the-art zero-shot translation for Tatar is supported by results but based on a single language pair, limiting generalizability.

**Low Confidence:** The assertion that trans-tokenization significantly lowers data and computational requirements compared to training from scratch or using character-based methods is plausible but not rigorously quantified.

## Next Checks
1. **Cross-linguistic generalization test:** Apply trans-tokenization to a diverse set of language pairs spanning different language families (e.g., Turkic, Sino-Tibetan, Dravidian) to evaluate robustness across typologically diverse languages. Measure performance degradation relative to resource availability and linguistic distance from the source language.

2. **Parallel corpus sensitivity analysis:** Systematically vary the size and quality of parallel corpora used for token alignment to determine minimum effective corpus sizes and identify failure thresholds. Compare alignment quality metrics with downstream task performance to establish correlation patterns.

3. **Zero-shot translation stress test:** Evaluate Hydra LLM performance across multiple translation directions (source→target, target→source, and zero-shot) for the same language pair, then extend to additional pairs. Measure not only translation quality but also model stability and consistency across different operational modes.