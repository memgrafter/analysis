---
ver: rpa2
title: 'EDA Corpus: A Large Language Model Dataset for Enhanced Interaction with OpenROAD'
arxiv_id: '2405.06676'
source_url: https://arxiv.org/abs/2405.06676
tags:
- design
- openroad
- dataset
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EDA Corpus, a pioneering open-source dataset
  designed to facilitate the integration of large language models (LLMs) into physical
  design through the OpenROAD framework. The dataset addresses the lack of publicly
  available, permissively licensed data for training LLMs in electronic design automation
  (EDA).
---

# EDA Corpus: A Large Language Model Dataset for Enhanced Interaction with OpenROAD

## Quick Facts
- arXiv ID: 2405.06676
- Source URL: https://arxiv.org/abs/2405.06676
- Reference count: 20
- Primary result: Introduces EDA Corpus, an open-source dataset enabling LLM fine-tuning for OpenROAD-based physical design tasks

## Executive Summary
This paper introduces EDA Corpus, a pioneering open-source dataset designed to facilitate the integration of large language models (LLMs) into physical design through the OpenROAD framework. The dataset addresses the lack of publicly available, permissively licensed data for training LLMs in electronic design automation (EDA). EDA Corpus contains over 1000 data points across two formats: question-answer pairs and prompt-script pairs. The question-answer dataset covers general, tool-specific, and flow-related questions about OpenROAD, while the prompt-script dataset focuses on generating OpenROAD Python scripts for physical design tasks. The dataset was validated by OpenROAD experts and demonstrated significant improvements in LLM performance on EDA-specific tasks when fine-tuned, particularly for script generation, where accuracy increased from nearly 0% to 47%.

## Method Summary
The method involves curating a dataset of over 1000 data points in two formats: question-answer pairs covering OpenROAD knowledge and prompt-script pairs for generating OpenROAD Python scripts. The dataset was validated by domain experts through execution and correctness evaluation. Fine-tuning was performed using ChatGPT3.5 with "auto" fine-tuning settings, splitting the data into 95% training and 5% validation sets. Performance was evaluated by measuring script execution success rates and comparing against baseline LLM performance.

## Key Results
- EDA Corpus dataset contains over 1000 data points across question-answer and prompt-script formats
- Fine-tuned LLM performance improved significantly, with script generation accuracy increasing from nearly 0% to 47%
- Dataset enables LLM training without proprietary EDA tool licensing constraints through OpenROAD platform

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning with a high-quality, domain-specific dataset improves LLM performance on EDA-specific tasks.
- Mechanism: The dataset provides targeted examples of both question-answer pairs and prompt-script pairs that are relevant to OpenROAD, enabling the LLM to learn the specific language, concepts, and code patterns used in physical design.
- Core assumption: The curated dataset contains accurate and representative examples of the types of questions and tasks that users encounter when working with OpenROAD.
- Evidence anchors:
  - [abstract] "The dataset was validated by OpenROAD experts and demonstrated significant improvements in LLM performance on EDA-specific tasks when fine-tuned, particularly for script generation, where accuracy increased from nearly 0% to 47%."
  - [section] "Each datapoint in the prompt-script dataset undergoes validation by a domain expert, who executes the script within OpenROAD and evaluates the correctness of the output against the specified prompt."
- Break condition: If the dataset contains errors or biases that mislead the LLM, or if the tasks in the dataset are not representative of real-world use cases.

### Mechanism 2
- Claim: Using OpenROAD, an open-source EDA tool, mitigates licensing constraints and allows for public release of the dataset.
- Mechanism: OpenROAD provides a freely available platform for physical design tasks, eliminating the need for proprietary tools and their associated licensing restrictions. This enables the creation and distribution of a dataset that can be used by the wider community without legal barriers.
- Core assumption: OpenROAD is a sufficiently comprehensive and widely adopted platform that covers the essential aspects of physical design.
- Evidence anchors:
  - [abstract] "EDA Corpus is based on OpenROAD [6], a widely utilized open-source EDA tool for automated place and route tasks. Leveraging OpenROAD mitigates obstacles associated with proprietary EDA tools, enabling the public release of our dataset and facilitating its use with LLMs without licensing constraints."
  - [section] "Open-source physical design platforms such as OpenROAD [6] and iEDA [17] have been widely used for education and research."
- Break condition: If OpenROAD does not adequately represent the full scope of physical design tasks or if its API is not sufficiently stable or well-documented for LLM training.

### Mechanism 3
- Claim: Providing two formats of data (question-answer and prompt-script) allows for training LLMs on both comprehension and code generation tasks.
- Mechanism: The question-answer dataset enables the LLM to learn the domain knowledge and terminology used in OpenROAD, while the prompt-script dataset allows it to learn how to generate Python scripts for physical design tasks using the OpenROAD API. This dual approach provides a more comprehensive training experience.
- Core assumption: The two formats of data are complementary and together provide a sufficient basis for training LLMs on both comprehension and code generation tasks.
- Evidence anchors:
  - [abstract] "The dataset features over 1000 data points and is structured in two formats: (i) a pairwise set comprised of question prompts with prose answers, and (ii) a pairwise set comprised of code prompts and their corresponding OpenROAD scripts."
  - [section] "EDA Corpus consists of two types of data: (i) question and answer pairs, and (ii) prompt and script pairs."
- Break condition: If the two formats of data are not well-balanced or if one format is significantly more important than the other for the target tasks.

## Foundational Learning

- Concept: Electronic Design Automation (EDA) and the chip design process.
  - Why needed here: Understanding the context and terminology of EDA is crucial for curating a relevant dataset and interpreting the results of the LLM fine-tuning experiments.
  - Quick check question: What are the main stages of the chip design process, and how does OpenROAD fit into this process?

- Concept: Large Language Models (LLMs) and their capabilities and limitations.
  - Why needed here: Knowing how LLMs work and what they are good at is essential for designing an effective fine-tuning strategy and evaluating the results.
  - Quick check question: What are the main differences between a foundation model and a fine-tuned model, and how does fine-tuning improve performance on specific tasks?

- Concept: Python programming and the OpenROAD Python API.
  - Why needed here: The prompt-script dataset consists of Python scripts that use the OpenROAD API, so understanding Python and the API is necessary for curating the dataset and validating the generated scripts.
  - Quick check question: What are the main components of the OpenROAD Python API, and how do they relate to the different stages of the physical design process?

## Architecture Onboarding

- Component map: EDA Corpus dataset (question-answer pairs + prompt-script pairs) -> Fine-tuning process (ChatGPT3.5) -> Fine-tuned LLM -> Evaluation (script execution and accuracy measurement)
- Critical path: The critical path is the fine-tuning process, which involves selecting an appropriate LLM, preparing the dataset, training the model, and evaluating the results. The quality and representativeness of the dataset are crucial for the success of the fine-tuning process.
- Design tradeoffs: The main tradeoff is between the size and diversity of the dataset and the accuracy and consistency of the data. A larger and more diverse dataset may improve the LLM's performance on a wider range of tasks, but it may also introduce more noise and inconsistencies. A smaller and more focused dataset may be easier to curate and validate, but it may not cover all the relevant use cases.
- Failure signatures: If the fine-tuned LLM performs poorly on the validation set, it may indicate that the dataset is not representative of the target tasks or that the fine-tuning process is not effective. If the generated scripts do not execute correctly in OpenROAD, it may indicate that the prompt-script dataset contains errors or that the LLM has not learned the correct API usage patterns.
- First 3 experiments:
  1. Evaluate the performance of the fine-tuned LLM on the question-answer task using a held-out validation set and measure the accuracy of the generated responses.
  2. Evaluate the performance of the fine-tuned LLM on the prompt-script task using a held-out validation set and measure the accuracy of the generated scripts by executing them in OpenROAD.
  3. Compare the performance of the fine-tuned LLM with the baseline performance of the foundation model on both the question-answer and prompt-script tasks to quantify the improvement from fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum dataset size required to achieve optimal LLM performance for EDA tasks?
- Basis in paper: [explicit] The paper mentions that "Prior LLM works have shown that even small amounts of high-quality training data can produce high-quality models through fine-tuning foundation models"
- Why unresolved: The paper does not provide a quantitative analysis of how dataset size affects LLM performance for EDA tasks. It only mentions that "high-quality training data" can be effective, but doesn't specify the minimum or optimal size.
- What evidence would resolve it: Systematic experiments varying dataset sizes while keeping quality constant, measuring performance improvements on EDA tasks to identify the point of diminishing returns.

### Open Question 2
- Question: How does the performance of fine-tuned LLMs on EDA tasks generalize across different chip design domains (e.g., digital vs. analog, ASIC vs. FPGA)?
- Basis in paper: [inferred] The paper focuses on OpenROAD for digital physical design, but doesn't address whether the improvements would transfer to other EDA domains or toolchains.
- Why unresolved: The evaluation is limited to OpenROAD-based tasks, and there's no discussion of cross-domain generalization or transfer learning potential.
- What evidence would resolve it: Testing fine-tuned models on diverse EDA tools and design domains, comparing performance retention across different types of chip design tasks.

### Open Question 3
- Question: What is the long-term effectiveness of LLM assistance in reducing the learning curve for new OpenROAD users compared to traditional documentation and tutorials?
- Basis in paper: [explicit] The paper mentions that "LLMs have the capability to dramatically increase physical design accessibility for both new and seasoned chip designers alike"
- Why unresolved: The paper demonstrates short-term performance improvements but doesn't evaluate the sustained impact on user productivity or learning outcomes over extended periods.
- What evidence would resolve it: Longitudinal studies comparing new users working with LLM assistance versus traditional resources, measuring metrics like task completion time, error rates, and knowledge retention over weeks or months.

## Limitations

- Dataset size may be insufficient for comprehensive LLM training in complex EDA domain
- Evaluation methodology lacks rigorous quantitative metrics for script correctness assessment
- 47% accuracy for script generation indicates substantial room for improvement and questions about real-world usability

## Confidence

**High Confidence**: The dataset creation methodology and the fundamental approach of fine-tuning LLMs for domain-specific tasks are well-established and correctly implemented.

**Medium Confidence**: The reported improvements in LLM performance are credible given the domain-specific nature of the training data, but the evaluation methodology could be more rigorous.

**Low Confidence**: Claims about the dataset's ability to generalize to broader EDA workflows beyond OpenROAD are not well-supported by the evidence presented.

## Next Checks

1. **Dataset Coverage Analysis**: Conduct a systematic analysis of the question-answer pairs to assess coverage of OpenROAD's full functionality, identifying potential gaps in the dataset that could limit LLM performance on real-world tasks.

2. **Cross-Tool Generalization Test**: Evaluate the fine-tuned LLM's performance on a different but related EDA tool (such as iEDA) to determine whether the training transfers beyond the specific OpenROAD environment.

3. **Script Robustness Evaluation**: Test the generated scripts across multiple OpenROAD versions and configurations to assess their robustness and identify any version-specific dependencies or breaking changes that could affect practical utility.