---
ver: rpa2
title: 'FastVLM: Efficient Vision Encoding for Vision Language Models'
arxiv_id: '2412.13303'
source_url: https://arxiv.org/abs/2412.13303
tags:
- vision
- fastvlm
- resolution
- fastvithd
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FastVLM improves vision-language model efficiency by using FastViTHD,\
  \ a hybrid vision encoder that generates fewer tokens and faster encodes high-resolution\
  \ images. Compared to ViT-L/14, FastVLM achieves 3.2\xD7 faster time-to-first-token\
  \ while maintaining similar performance."
---

# FastVLM: Efficient Vision Encoding for Vision Language Models

## Quick Facts
- arXiv ID: 2412.13303
- Source URL: https://arxiv.org/abs/2412.13303
- Authors: Pavan Kumar Anasosalu Vasu; Fartash Faghri; Chun-Liang Li; Cem Koc; Nate True; Albert Antony; Gokul Santhanam; James Gabriel; Peter Grasch; Oncel Tuzel; Hadi Pouransari
- Reference count: 40
- Primary result: FastVLM achieves 3.2× faster time-to-first-token while maintaining similar performance compared to ViT-L/14 at high resolutions

## Executive Summary
FastVLM introduces FastViTHD, a hybrid vision encoder that combines convolutional and transformer architectures to significantly improve vision-language model efficiency. By generating fewer tokens and reducing encoding time for high-resolution images, FastVLM achieves 3.2× faster time-to-first-token compared to ViT-L/14 while maintaining competitive accuracy. At 1152×1152 resolution, it outperforms LLaVA-OneVision on multiple benchmarks with 85× faster TTFT and a 3.4× smaller vision encoder, demonstrating that efficient vision encoding is critical for high-performance VLMs.

## Method Summary
FastVLM uses a hybrid vision encoder (FastViTHD) with convolutional downsampling followed by transformer blocks, enabling native resolution scaling and reduced token generation. The model employs a 2-stage training pipeline (connector training + visual instruction tuning) based on LLaVA-1.5, with an optional 3-stage extension including resolution scaling pretraining. FastVLM scales input resolution directly rather than using tiling, achieving better accuracy-latency trade-offs. The architecture uses multi-scale feature aggregation with depthwise convolutions and RepMixer blocks in early stages, optimizing for both efficiency and performance across various vision-language tasks.

## Key Results
- Achieves 3.2× faster time-to-first-token compared to ViT-L/14 at equivalent resolutions
- At 1152×1152 resolution, outperforms LLaVA-OneVision on SeedBench, MMMU, and DocVQA with 85× faster TTFT
- Uses 3.4× smaller vision encoder while maintaining competitive accuracy on 10+ benchmarks
- Direct resolution scaling provides better accuracy-latency trade-off than tiling approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid convolutional-transformer architectures reduce token count and encoding latency at high resolution compared to pure ViT encoders.
- Mechanism: Convolutional stages downsample spatial dimensions before transformer layers, so fewer tokens are generated and fewer self-attention operations are performed on high-resolution images.
- Core assumption: The convolutional downsampling does not degrade visual quality for VLM tasks compared to larger token counts from ViT.
- Evidence anchors:
  - [abstract] "FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images."
  - [section 3.1] "Using FastViT at its CLIP-pretrained resolution (256×256) alone does not yield a strong VLM. The main advantage of a hybrid encoder like FastViT lies in its favorable image resolution scaling characteristics, meaning it generates 5.2 × fewer tokens than the ViT architecture with a patch size of 14."
  - [corpus] Weak: no explicit comparison of token counts between hybrid and ViT in corpus papers.
- Break condition: If convolutional downsampling introduces too much spatial information loss, VLM accuracy drops below acceptable thresholds.

### Mechanism 2
- Claim: Multi-scale feature aggregation improves VLM accuracy without significant latency increase.
- Mechanism: Features from earlier stages of the encoder are pooled and concatenated with penultimate layer features, enriching representation without re-encoding the whole image.
- Core assumption: Earlier-stage features contain complementary information useful for VLM tasks, and pooling does not dominate latency.
- Evidence anchors:
  - [section 3.1.1] "We identify hybrid vision encoders (convolutional layers followed by transformer blocks) as an ideal candidate for VLMs, as their convolutional component enables native resolution scaling, and their transformer blocks further refine high-quality visual tokens for consumption by the LLM."
  - [section 3.1.1] "We ablate between 2 designs to pool features from different stages, i.e. AvgPooling and 2D Depthwise convolutions. From Tab. 2, we find that using depthwise convolutions results in better performance."
  - [corpus] Weak: no explicit evidence in corpus that multi-scale pooling improves VLM accuracy.
- Break condition: If pooling introduces too much overhead relative to gains, latency trade-off becomes unfavorable.

### Mechanism 3
- Claim: Scaling input resolution directly is more efficient than tiling for high-resolution VLMs.
- Mechanism: Single-pass high-resolution encoding avoids multiple tile inferences and reduces inter-tile communication overhead, lowering TTFT.
- Core assumption: Memory bandwidth and tile overhead dominate at high resolution, making direct scaling preferable.
- Evidence anchors:
  - [section 3.2.2] "From Fig. 6, we see that setting the model's input resolution directly to the desired resolution offers the best accuracy-latency trade-off, with dynamic resolution benefiting only at extreme resolutions like 1536×1536, due to memory bandwidth limitations."
  - [section 3.2.2] "If dynamic resolution is desired, using a setting with fewer tiles exhibits better accuracy-latency tradeoff."
  - [corpus] Weak: no explicit latency comparison between tiling and direct scaling in corpus papers.
- Break condition: If memory constraints prevent direct scaling at very high resolutions, tiling becomes necessary.

## Foundational Learning

- Concept: Vision transformer token generation and scaling laws
  - Why needed here: Understanding how token count grows with resolution explains why hybrid architectures are more efficient
  - Quick check: Verify that token count scales linearly with resolution in pure ViT but sublinearly in hybrid architectures

- Concept: Multi-scale feature representation in vision models
  - Why needed here: The multi-scale feature aggregation is critical for maintaining accuracy while reducing tokens
  - Quick check: Confirm that earlier-stage features capture complementary information useful for VLM tasks

- Concept: Time-to-first-token (TTFT) latency measurement
  - Why needed here: TTFT is the primary efficiency metric and depends on both vision encoder and LLM prefilling
  - Quick check: Ensure TTFT measurements isolate vision encoder contribution from LLM inference

- Concept: CLIP-style vision encoder pretraining
  - Why needed here: FastViTHD is pretrained on DataCompDR-1B using CLIP-style contrastive learning
  - Quick check: Verify the contrastive loss formulation and data augmentation strategies used

- Concept: Vision-language instruction tuning pipeline
  - Why needed here: The 2-stage (and optional 3-stage) training follows LLaVA-1.5 methodology
  - Quick check: Confirm the projector-only training stage and full model fine-tuning stage implementation

- Concept: Resolution scaling in vision models
  - Why needed here: Direct resolution scaling vs tiling is a key design decision for high-resolution efficiency
  - Quick check: Verify memory bandwidth limitations at extreme resolutions that favor tiling approaches

## Architecture Onboarding

### Component Map
FastViTHD encoder -> Vision-Language Connector -> LLM

### Critical Path
Input image → FastViTHD (5 stages: RepMixer → RepMixer → RepMixer → Self-Attention → Self-Attention) → Multi-scale feature aggregation → Vision-language connector → LLM

### Design Tradeoffs
- Hybrid architecture vs pure transformer: Fewer tokens but potential information loss from downsampling
- Direct resolution scaling vs tiling: Better latency but higher memory requirements
- Multi-scale feature aggregation: Improved accuracy but added complexity
- RepMixer vs pure attention blocks: Efficiency gains vs potential representational power

### Failure Signatures
- High TTFT despite architectural optimizations: Vision encoder not properly optimized or LLM prefilling bottleneck
- Poor accuracy on text-rich benchmarks: Insufficient resolution or improper multi-scale feature aggregation
- Training instability: Data preprocessing issues or hyperparameter misconfiguration
- Memory errors at high resolutions: Direct scaling exceeding available memory, need tiling fallback

### First Experiments
1. Implement FastViTHD architecture and verify token count reduction compared to ViT-L/14 at equivalent resolutions
2. Test multi-scale feature aggregation with both AvgPool and DWConv to confirm performance differences
3. Compare TTFT of direct resolution scaling vs 2×2 grid tiling at 512×512 and 1152×1152 resolutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FastVLM's performance scale with further increases in instruction tuning dataset size beyond 12.5 million samples?
- Basis in paper: [explicit] The paper shows performance improvements when scaling the instruction tuning dataset from 1.1M to 12.5M samples, but does not explore beyond this point.
- Why unresolved: The paper only reports results up to 12.5M samples, leaving open whether further scaling would continue to yield improvements or reach diminishing returns.
- What evidence would resolve it: Training FastVLM with instruction tuning datasets of 25M, 50M, and 100M samples and evaluating performance on benchmarks like MMMU, DocVQA, and ChartQA to establish scaling laws.

### Open Question 2
- Question: What is the impact of using different vision encoder architectures (e.g., convolutional-only, transformer-only, or other hybrid designs) on FastVLM's accuracy-latency trade-off?
- Basis in paper: [explicit] The paper compares FastViTHD against ViT and ConvNeXT architectures, but does not explore other potential encoder designs.
- Why unresolved: The paper focuses on FastViTHD as the optimal design, but other encoder architectures might offer different trade-offs that could be beneficial for specific use cases.
- What evidence would resolve it: Implementing and benchmarking FastVLM with various encoder architectures (e.g., ConvNeXT-XXL, Swin Transformer, or other hybrid designs) at different resolutions and evaluating their accuracy-latency trade-offs.

### Open Question 3
- Question: How does FastVLM perform on specialized domains not covered in the evaluation benchmarks, such as medical imaging, satellite imagery, or industrial inspection?
- Basis in paper: [inferred] The paper evaluates FastVLM on general-purpose benchmarks like GQA, MMMU, and DocVQA, but does not test it on domain-specific tasks.
- Why unresolved: The paper's evaluation focuses on general vision-language understanding tasks, leaving open whether FastVLM's efficiency gains translate to specialized domains with unique requirements.
- What evidence would resolve it: Applying FastVLM to specialized datasets in domains like medical imaging (e.g., CheXpert), satellite imagery (e.g., xView), or industrial inspection (e.g., MVTec AD) and measuring performance and efficiency gains compared to domain-specific models.

## Limitations

- Architectural details of FastViTHD are not fully specified, making exact reproduction difficult
- Training pipeline extensions (Stage 1.5 and Stage 3) lack complete hyperparameter specifications
- Performance claims are highly dependent on specific hardware configurations and optimizations not fully disclosed
- Benchmark evaluation protocols and data preprocessing steps are not completely detailed

## Confidence

**High Confidence** in the core claim that hybrid convolutional-transformer architectures can reduce token count and encoding latency compared to pure ViT encoders.

**Medium Confidence** in the specific performance claims (3.2× faster TTFT, 85× faster at 1152×1152 resolution) due to implementation and evaluation protocol uncertainties.

**Low Confidence** in the ability to fully reproduce the results without additional architectural specifications and complete training pipeline details.

## Next Checks

1. **Architecture Specification Extraction**: Contact authors to obtain complete FastViTHD specifications including RepMixer block configurations and multi-scale feature aggregation implementation. Reproduce token count reduction from ViT-L/14 to FastViTHD at equivalent resolutions.

2. **Independent Benchmark Evaluation**: Implement FastViTHD architecture and conduct independent evaluations on GQA, TextVQA, DocVQA, and SeedBench using standardized protocols. Compare token counts, TTFT measurements, and accuracy against published baselines under identical conditions.

3. **Resolution Scaling Validation**: Systematically test claimed resolution scaling benefits by evaluating FastVLM at 256×256, 512×512, 1152×1152, and 1536×1536 resolutions. Compare against tiled approaches measuring both accuracy and TTFT to verify 3.4× smaller vision encoder performance claims.