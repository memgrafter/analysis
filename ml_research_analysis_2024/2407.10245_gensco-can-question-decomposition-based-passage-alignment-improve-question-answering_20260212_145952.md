---
ver: rpa2
title: 'GenSco: Can Question Decomposition based Passage Alignment improve Question
  Answering?'
arxiv_id: '2407.10245'
source_url: https://arxiv.org/abs/2407.10245
tags:
- question
- passages
- passage
- gensco
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method called GenSco for passage selection
  in retrieval-augmented generation (RAG) for multi-hop question answering (MHQA).
  The key idea is to leverage question decomposition to generate a sequence of sub-questions,
  then use a scorer LLM to select the most relevant passages for each sub-question.
---

# GenSco: Can Question Decomposition based Passage Alignment improve Question Answering?

## Quick Facts
- arXiv ID: 2407.10245
- Source URL: https://arxiv.org/abs/2407.10245
- Reference count: 40
- Key outcome: GenSco achieves absolute gains of 15.1 and 5.9 points in Exact Match score over the best performing baselines on MuSiQue and 2WikiMultiHop respectively.

## Executive Summary
This paper introduces GenSco, a novel approach for passage selection in retrieval-augmented generation (RAG) systems designed for multi-hop question answering (MHQA). The key innovation is leveraging question decomposition to generate a sequence of sub-questions, then using a scorer LLM to select the most relevant passages for each sub-question. This creates a passage sequence aligned with the reasoning steps needed to answer the original multi-hop question. The generator LLM then uses this aligned passage sequence to generate the final answer. Experiments on three MHQA datasets demonstrate that GenSco significantly outperforms strong baselines.

## Method Summary
GenSco employs a two-LLM architecture: a generator LLM for question decomposition and answer generation, and a scorer LLM for passage selection based on sub-question relevance. The method iteratively decomposes the original question into sub-questions, uses the scorer LLM to select the most relevant passage for each sub-question, and continues until stopping criteria are met. The resulting passage sequence is aligned with the reasoning chain implied by the original question. The generator LLM then uses this aligned passage sequence to generate the final answer.

## Key Results
- GenSco achieves absolute gains of 15.1 and 5.9 points in Exact Match score over the best performing baselines on MuSiQue and 2WikiMultiHop respectively.
- The method demonstrates high precision in passage retrieval, mitigating hallucinations in LLM responses.
- Ablation studies confirm the importance of question decomposition and passage ordering for performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question decomposition into sub-questions reduces the complexity of the reasoning chain, enabling more accurate passage selection.
- Mechanism: By decomposing a multi-hop question into simpler sub-questions, the system can focus on finding the most relevant passages for each step in the reasoning chain, rather than trying to find a single passage that answers the entire complex question.
- Core assumption: Decomposing the question into sub-questions that follow the reasoning chain implied by the original question will lead to a more aligned passage selection.
- Evidence anchors:
  - [abstract] "Our intuition behind choosing two separate LLMs is to complement the generator LLM with a scorer module in terms of semantic and grounded knowledge base."
  - [section] "Our novel approach of breaking down a complex query into a set of simpler elemental queries allows our method to leverage generative ability of LLMs for accurate retrieval and better answering performance."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.
- Break condition: If the sub-questions generated do not accurately reflect the reasoning chain implied by the original question, the passage alignment may be suboptimal, leading to decreased performance.

### Mechanism 2
- Claim: Using a scorer LLM to guide passage selection based on sub-question relevance improves the precision of retrieved passages.
- Mechanism: The scorer LLM computes the relevance of each passage to the current sub-question, allowing the system to select the most relevant passage for each step in the reasoning chain.
- Core assumption: The scorer LLM can accurately assess the relevance of passages to the sub-questions generated by the generator LLM.
- Evidence anchors:
  - [abstract] "The framework consists of two distinct LLMs: (i) Generator LLM, which is used for question decomposition and final answer generation; (ii) an auxiliary open-sourced LLM, used as the scorer, to semantically guide the Generator for passage selection."
  - [section] "The nodes at level i are evaluated using scorer S for relevance to the subquestion qi which presumably captures the ith reasoning step for the multi-hop question Q."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.
- Break condition: If the scorer LLM is not accurate in assessing passage relevance, the selected passages may not be the most relevant for the sub-questions, leading to decreased performance.

### Mechanism 3
- Claim: Presenting passages in the order of the reasoning chain improves the generator LLM's ability to generate accurate answers.
- Mechanism: By presenting the passages in the order they were selected based on the sub-questions, the generator LLM receives a more coherent and aligned context, improving its ability to generate accurate answers.
- Core assumption: The order in which passages are presented to the generator LLM affects its ability to generate accurate answers.
- Evidence anchors:
  - [section] "Since the subquestions could be indefinitely generated... note that it requires specifying an upper limit on the number of levels that can be explored."
  - [section] "Note that in our log-likelihood expressions, we compute the probability of the question conditioned on the passages rather than the other way around which may be less intuitive since we should be scoring the passages (and not the question)."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism.
- Break condition: If the order of passages does not align with the reasoning chain implied by the original question, the generator LLM may struggle to generate accurate answers, leading to decreased performance.

## Foundational Learning

- Concept: Question Decomposition
  - Why needed here: Decomposing multi-hop questions into simpler sub-questions allows for more accurate passage selection and improved reasoning.
  - Quick check question: Can you explain how decomposing a complex question into simpler sub-questions can improve the accuracy of passage selection and reasoning in a multi-hop question answering system?

- Concept: Relevance Scoring
  - Why needed here: Using a scorer LLM to assess the relevance of passages to sub-questions enables the selection of the most pertinent passages for each step in the reasoning chain.
  - Quick check question: How does using a scorer LLM to assess the relevance of passages to sub-questions improve the precision of retrieved passages in a multi-hop question answering system?

- Concept: Sequential Alignment
  - Why needed here: Presenting passages in the order of the reasoning chain provides a more coherent and aligned context for the generator LLM, improving its ability to generate accurate answers.
  - Quick check question: Why is it important to present passages in the order of the reasoning chain when using a generator LLM to generate answers in a multi-hop question answering system?

## Architecture Onboarding

- Component map: Generator LLM -> Scorer LLM -> Passage Tree
- Critical path:
  1. Generate sub-question using generator LLM.
  2. Select most relevant passage using scorer LLM.
  3. Repeat steps 1-2 until stopping criteria are met.
  4. Generate final answer using generator LLM with selected passage sequence.
- Design tradeoffs:
  - Using two separate LLMs (generator and scorer) provides complementary strengths but increases computational cost.
  - The greedy approach to passage selection is computationally efficient but may miss optimal passage combinations.
  - The stopping criteria (maximum levels or likelihood-based) balance between completeness and computational cost.
- Failure signatures:
  - If the generator LLM fails to produce relevant sub-questions, the passage selection will be suboptimal.
  - If the scorer LLM is not accurate in assessing passage relevance, the selected passages may not be the most pertinent.
  - If the stopping criteria are too restrictive, the system may miss important passages.
- First 3 experiments:
  1. Evaluate the performance of the system with different numbers of maximum levels to determine the optimal balance between completeness and computational cost.
  2. Compare the performance of the system using different scorer LLMs to assess the impact of the scorer's accuracy on overall performance.
  3. Test the system with different stopping criteria (e.g., likelihood threshold, maximum levels) to determine the most effective approach for balancing completeness and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GenSco's performance scale with increasing context window sizes of LLMs?
- Basis in paper: [inferred] The paper mentions that GenSco can complement traditional retrieval systems and that it's not directly comparable to full fine-tuning setups, implying that its effectiveness may depend on context window sizes.
- Why unresolved: The paper doesn't provide empirical results on how GenSco's performance changes with different context window sizes of LLMs.
- What evidence would resolve it: Conducting experiments with GenSco using LLMs with varying context window sizes and comparing the performance metrics.

### Open Question 2
- Question: What is the impact of using different scorer LLMs on GenSco's performance?
- Basis in paper: [explicit] The paper mentions using a 3B version of Open-llama as the scorer LLM but doesn't explore the impact of using different scorer models.
- Why unresolved: The paper only uses one specific scorer LLM and doesn't compare the performance with other potential scorer models.
- What evidence would resolve it: Running GenSco with different scorer LLMs and comparing the resulting performance metrics.

### Open Question 3
- Question: How does GenSco perform on datasets with more than 2-hop questions?
- Basis in paper: [explicit] The paper mentions that MuSiQue dataset is designed to necessitate connected reasoning and includes 2-hop questions, but doesn't explore performance on datasets with more hops.
- Why unresolved: The experiments are limited to 2-hop questions in MuSiQue and the paper doesn't provide results for datasets with more complex multi-hop questions.
- What evidence would resolve it: Evaluating GenSco on datasets with 3-hop or 4-hop questions and comparing the performance metrics.

### Open Question 4
- Question: What is the effect of varying the temperature parameter in the generator LLM on GenSco's performance?
- Basis in paper: [explicit] The paper mentions that setting the temperature to 0.5 gives small but consistent gains, but doesn't explore the full range of temperature values.
- Why unresolved: The paper only tests one specific temperature value and doesn't provide a comprehensive analysis of how temperature affects performance.
- What evidence would resolve it: Running experiments with GenSco using different temperature values for the generator LLM and analyzing the impact on performance metrics.

## Limitations

- The evaluation is constrained to in-domain performance on three datasets, with no robustness testing on out-of-distribution questions or adversarial examples.
- Critical implementation details remain underspecified, including the exact prompt templates, few-shot example selection, and stopping criteria configuration.
- The reliance on greedy search without beam search or backtracking raises concerns about sub-optimal passage sequences being permanently selected without recovery mechanisms.

## Confidence

- **High confidence**: The core finding that question decomposition enables more precise passage alignment shows strong empirical support through consistent performance gains across all three datasets.
- **Medium confidence**: The claim that separate scorer and generator LLMs provide complementary benefits is supported by ablation studies, though the specific advantage over integrated approaches needs more direct comparison.
- **Medium confidence**: The precision improvements in retrieval are well-documented, but the extent to which this reduces hallucination requires further validation with human evaluation of faithfulness.

## Next Checks

1. **Prompt Template Validation**: Conduct controlled experiments varying the prompt templates and few-shot examples for both generator and scorer LLMs to identify optimal configurations and assess sensitivity to prompt engineering.

2. **Alternative Scorer Comparison**: Replace the Open-LLaMA 3B scorer with other open-source models of varying sizes (1B, 7B, 13B) to determine the minimum effective model size and assess the scorer's impact on overall performance.

3. **Cross-Dataset Generalization**: Evaluate GenSco on a held-out dataset not used in the original experiments, including adversarial examples designed to test the robustness of the question decomposition and passage alignment approach.