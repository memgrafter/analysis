---
ver: rpa2
title: 'Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure
  for Jailbreak Attacks'
arxiv_id: '2407.00869'
source_url: https://arxiv.org/abs/2407.00869
tags:
- fallacious
- llms
- language
- attack
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose that large language models (LLMs) struggle
  to fabricate fallacious reasoning, often leaking correct answers even when instructed
  to produce deceptive solutions. They exploit this by designing a jailbreak attack,
  the Fallacy Failure Attack (FFA), which prompts an LLM to generate a fallacious
  procedure for a harmful task; the model's inability to deceive means the output
  is often factually harmful despite being framed as fake.
---

# Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks

## Quick Facts
- arXiv ID: 2407.00869
- Source URL: https://arxiv.org/abs/2407.00869
- Reference count: 11
- Key outcome: FFA achieves up to 90% attack success rates by exploiting LLMs' inability to generate fallacious reasoning, bypassing safety filters while producing harmful outputs

## Executive Summary
This paper identifies a fundamental vulnerability in large language models: their inability to reliably generate fallacious reasoning even when explicitly instructed to do so. The authors exploit this "fallacy failure" by designing the Fallacy Failure Attack (FFA), which prompts LLMs to produce a fallacious procedure for a harmful task. Due to LLMs' tendency to leak truthful answers under deceptive prompts, FFA generates harmful content while bypassing safety filters. The attack achieves competitive performance against existing jailbreak methods, with up to 90% success rates on various safety-aligned models.

## Method Summary
The FFA method constructs a jailbreak prompt with four components: a malicious query, a fallacious reasoning prompt, a deceptiveness requirement, and scene/purpose framing. The attack is evaluated on five safety-aligned LLMs (GPT-3.5-turbo, GPT-4, Gemini-Pro, Vicuna-1.5, LLaMA-3) using two benchmark datasets (AdvBench and HEx-PHI). The attack's effectiveness is measured using bypass rate, average harmfulness score, and attack success rate, compared against four baseline methods (GCG, AutoDAN, DeepInception, ArtPrompt) and three defense approaches (perplexity filtering, paraphrasing, retokenization).

## Key Results
- FFA achieves attack success rates up to 90% on some models
- FFA outperforms baseline methods in producing harmful outputs while bypassing safety filters
- Defense methods like perplexity filtering and paraphrasing show limited effectiveness against FFA
- LLaMA-3 demonstrates strong resistance to FFA