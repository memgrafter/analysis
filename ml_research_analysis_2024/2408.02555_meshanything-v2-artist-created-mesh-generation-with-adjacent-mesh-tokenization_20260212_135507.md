---
ver: rpa2
title: 'MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization'
arxiv_id: '2408.02555'
source_url: https://arxiv.org/abs/2408.02555
tags:
- mesh
- sequence
- face
- tokenization
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MeshAnything V2 addresses the challenge of generating artist-created
  meshes by introducing Adjacent Mesh Tokenization (AMT), which reduces token sequence
  length by representing faces with single vertices instead of three. This approach
  cuts sequence length by approximately half, improving computational efficiency and
  model performance.
---

# MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization

## Quick Facts
- arXiv ID: 2408.02555
- Source URL: https://arxiv.org/abs/2408.02555
- Authors: Yiwen Chen; Yikai Wang; Yihao Luo; Zhengyi Wang; Zilong Chen; Jun Zhu; Chi Zhang; Guosheng Lin
- Reference count: 40
- Primary result: MeshAnything V2 doubles face limit while maintaining accuracy, using Adjacent Mesh Tokenization to halve sequence length

## Executive Summary
MeshAnything V2 introduces Adjacent Mesh Tokenization (AMT) to address the challenge of efficient artist-created mesh generation. By representing faces with single vertices instead of triples, AMT reduces token sequence length by approximately half while maintaining or improving mesh quality. The approach doubles the face limit compared to previous methods without increasing computational costs, achieving better performance through more compact and well-structured sequences.

## Method Summary
MeshAnything V2 employs a transformer-based autoregressive model for mesh generation, enhanced with Adjacent Mesh Tokenization (AMT). AMT optimizes traditional face representation by using a single vertex per face when adjacent faces can share vertices, reducing token sequences by about half. The model takes point cloud shape conditions as input, encodes them into token sequences, and generates mesh tokens autoregressively. AMT uses special tokens ('&' and '$') to handle interruptions and vertex swaps, improving sequence compactness while maintaining mesh quality through cross-entropy loss training.

## Key Results
- AMT reduces token sequence length by approximately half by representing faces with single vertices instead of three
- The model doubles the face limit compared to previous methods while maintaining accuracy
- Experiments show AMT improves training speed and reduces memory usage without sacrificing mesh quality

## Why This Works (Mechanism)

### Mechanism 1
AMT reduces token sequence length by about half by representing each face with a single vertex instead of three. When faces are adjacent, AMT reuses the last two vertices from the previous face and only appends the new vertex to define the next face. This avoids redundant vertex repetitions that occur in traditional methods. The approach assumes adjacent faces share vertices in predictable ways, allowing the model to infer missing vertices from context.

### Mechanism 2
The compactness of AMT sequences improves computational efficiency without sacrificing mesh quality. Shorter sequences reduce memory usage and attention computation in transformers, while maintaining or improving accuracy metrics like Chamfer Distance and Normal Consistency. The reduced sequence length does not impair the model's ability to learn mesh structure because the token sequence remains regular and well-structured.

### Mechanism 3
The vertex swap mechanism with '$' token further improves sequence compactness by exploring more adjacent face options. When representing a face, if the next face doesn't share the last two vertices, the '$' token allows the model to swap to using the first and last vertices of the current face, increasing the chance of finding an adjacent face. This assumes meshes have enough connectivity that swapping vertex pairs can often find adjacent faces, reducing the need for '&' tokens.

## Foundational Learning

- **Autoregressive transformer sequence learning**: Why needed here: The model generates mesh tokens sequentially, similar to language models, requiring understanding of how transformers predict next tokens based on context. Quick check: What is the role of positional encoding in transformer-based mesh generation?

- **Graph-based data tokenization vs sequential data**: Why needed here: Meshes are inherently graph structures, but AMT converts them to 1D sequences; understanding this conversion is key to grasping AMT's design. Quick check: How does sorting vertices and faces before tokenization affect the uniqueness and regularity of the sequence?

- **Special token usage in sequence models**: Why needed here: AMT uses '&' and '$' tokens to handle interruptions and vertex swaps; understanding how special tokens influence model learning is critical. Quick check: What happens if a special token like '&' appears too frequently in the training data?

## Architecture Onboarding

- **Component map**: Point cloud shape condition → Point encoder → Token sequence TS → Transformer → Generated token sequence → Detokenization → Mesh
- **Critical path**: 1) Encode point cloud shape condition into fixed-length token sequence. 2) Tokenize ground truth mesh using AMT into compact vertex-based sequence. 3) Concatenate point cloud tokens and mesh tokens as transformer input. 4) Train transformer to predict next mesh token autoregressively. 5) During inference, generate mesh tokens conditioned on point cloud.
- **Design tradeoffs**: AMT vs traditional triple-vertex representation (AMT reduces sequence length but requires handling special tokens and adjacency logic); VQ-VAE vs direct coordinate discretization (direct discretization simplifies pipeline but may lose geometric abstraction benefits); Face count condition (adds controllability but introduces variability that may affect training stability).
- **Failure signatures**: Frequent '&' tokens in generated sequences (mesh topology too fragmented for AMT to handle efficiently); High perplexity during training (model struggles with AMT sequence patterns); Degraded mesh quality metrics (AMT sequence too irregular or special tokens misused).
- **First 3 experiments**: 1) Compare AMT-generated sequences vs traditional triple-vertex sequences in terms of length and adjacency regularity on a small mesh dataset. 2) Train a small transformer on AMT-tokenized meshes and evaluate perplexity and mesh quality on a validation set. 3) Test AMT with and without vertex swap on meshes of varying complexity to measure impact on sequence length and model performance.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Adjacent Mesh Tokenization (AMT) vary with different mesh topologies and complexities? The paper demonstrates AMT's effectiveness on Objaverse data but does not provide detailed analysis on how AMT performs with varying mesh complexities or topologies. This remains unresolved because the paper focuses on demonstrating AMT's effectiveness in reducing token sequence length and improving efficiency, but does not explore its performance across diverse mesh structures.

### Open Question 2
What are the specific trade-offs between using AMT with and without the swap operation in terms of computational efficiency and mesh quality? The paper introduces a swap operation in AMT to handle adjacent faces more effectively, mentioning that this operation introduces an additional special token but reduces interruptions and shortens the token sequence. However, it does not provide a detailed comparison of the trade-offs between using AMT with and without the swap operation.

### Open Question 3
How does the face count condition influence the overall quality and topology of generated meshes, especially when the specified face count is significantly different from the ground truth? The paper introduces a face count condition to allow users to specify an approximate number of faces, ensuring generated meshes align with desired specifications. However, it provides limited insights into how the face count condition affects the quality and topology of generated meshes, particularly when the specified face count deviates significantly from the ground truth.

## Limitations
- AMT's benefits diminish significantly when meshes contain many isolated faces or minimal adjacency, as special tokens become more frequent
- The vertex swap mechanism adds complexity without clear empirical justification for when it outperforms the base AMT approach
- The method's effectiveness depends heavily on specific face sorting and vertex ordering strategies during preprocessing

## Confidence

**High confidence** in the core claim that AMT reduces token sequence length by approximately half through adjacent face representation. This is directly supported by the algorithmic description and mathematical formulation in section 3.1.

**Medium confidence** in the claim that AMT improves computational efficiency and model performance without sacrificing mesh quality. While the mechanism is sound, the empirical validation relies on specific datasets and metrics that may not generalize to all mesh generation scenarios.

**Medium confidence** in the vertex swap mechanism's contribution to further improving sequence compactness. The theoretical justification exists, but the paper does not provide comprehensive ablation studies showing when swap is beneficial versus detrimental.

## Next Checks

1. **Topological robustness test**: Evaluate AMT performance across mesh datasets with varying levels of fragmentation and connectivity, measuring the frequency of special token usage and resulting sequence regularity.

2. **Vertex swap ablation study**: Conduct controlled experiments comparing AMT with and without vertex swap across meshes of different complexities, measuring not just compression ratios but also model perplexity, training stability, and generated mesh quality metrics.

3. **Cross-dataset generalization**: Test the trained AMT-based model on mesh datasets outside Objaverse to validate whether the performance improvements transfer to different mesh distributions and whether AMT's advantages persist with different mesh characteristics.