---
ver: rpa2
title: DASB - Discrete Audio and Speech Benchmark
arxiv_id: '2406.14294'
source_url: https://arxiv.org/abs/2406.14294
tags:
- speech
- discrete
- audio
- tokens
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DASB, a comprehensive benchmark for evaluating
  discrete audio tokens across various speech processing tasks. The authors benchmark
  semantic, compression, and hybrid tokenizers on discriminative tasks like speech
  recognition, speaker identification, emotion recognition, keyword spotting, and
  intent classification, as well as generative tasks like speech enhancement, separation,
  and text-to-speech.
---

# DASB - Discrete Audio and Speech Benchmark

## Quick Facts
- **arXiv ID**: 2406.14294
- **Source URL**: https://arxiv.org/abs/2406.14294
- **Reference count**: 40
- **Primary result**: Semantic tokens generally outperform compression tokens across speech tasks, with discrete WavLM achieving best overall performance

## Executive Summary
DASB introduces a comprehensive benchmark for evaluating discrete audio tokens across nine speech processing tasks. The benchmark systematically compares semantic, compression, and hybrid tokenizers using multiple datasets and metrics. Results show that semantic tokens outperform compression tokens on average, though a substantial performance gap remains between discrete and continuous representations. Discrete WavLM emerges as the top-performing model, suggesting its suitability for integration with multi-modal text+audio language models.

## Method Summary
The benchmark evaluates discrete audio encoders (semantic, compression, hybrid) on nine downstream tasks using frozen pretrained models. Each task employs two downstream architectures trained end-to-end with attention-based combination of multiple codebook embeddings. Tasks include speech recognition, speaker identification/verification, emotion recognition, keyword spotting, intent classification, speech enhancement, separation, and text-to-speech. Evaluation uses standard metrics including WER, accuracy, EER, and task-specific measures. The benchmark requires downloading multiple datasets, installing dependencies, and running scripts for training and evaluation.

## Key Results
- Semantic tokens outperform compression tokens on average across most discriminative and generative tasks
- Discrete WavLM achieves the best overall performance, making it a natural candidate for multi-modal text+audio LLMs
- Medium bitrate settings generally provide the best balance between information preservation and model complexity
- Performance gap between discrete and continuous representations remains substantial, highlighting need for further research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic tokens outperform compression tokens in most discriminative tasks because they capture high-level linguistic and semantic information rather than just acoustic details.
- Mechanism: Semantic tokens are derived from self-supervised learning models by clustering intermediate representations, which naturally encode phonetic, semantic, and syntactic information. This high-level content representation aligns better with discriminative tasks like speech recognition and emotion detection that depend on understanding meaning rather than perfect waveform reconstruction.
- Core assumption: The downstream tasks require semantic understanding more than acoustic fidelity.
- Evidence anchors:
  - [abstract]: "Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks."
  - [section]: "Semantic tokens primarily capture high-level information, such as phonetic, semantic, and syntactic information."
- Break condition: If a task depends heavily on acoustic details (e.g., speaker recognition), compression tokens may perform better.

### Mechanism 2
- Claim: Discrete WavLM achieves the best overall performance because it preserves both semantic content and paralinguistic features effectively.
- Mechanism: WavLM is trained with large-scale self-supervised learning and integrates speaker and channel information into its representations. When tokenized, these features remain accessible to downstream models, enabling strong performance across diverse tasks including speech recognition, speaker verification, and generation.
- Core assumption: WavLM's pretraining captures a broad range of speech characteristics that remain useful after discretization.
- Evidence anchors:
  - [abstract]: "Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field."
  - [section]: "discrete WavLM emerged as the top-performing model, making it a natural candidate for adoption in multi-modal text+audio LLMs."
- Break condition: If tokenization significantly degrades the unique features that make WavLM effective, performance may drop.

### Mechanism 3
- Claim: Medium bitrate settings provide the best balance between information preservation and model complexity.
- Mechanism: Lower bitrates lose too much information, hurting performance, while higher bitrates increase sequence length and model difficulty without proportional gains. Medium bitrates retain sufficient detail while keeping computational demands manageable.
- Core assumption: The downstream models can effectively process sequences at medium bitrates without overfitting or underfitting.
- Evidence anchors:
  - [section]: "Tables 2 and 3 show that a medium bitrate achieves the best results for both discriminative and generative tasks."
  - [section]: "Interestingly, higher bitrates, when available (e.g., for EnCodec and DAC), tend to degrade performance."
- Break condition: If downstream architectures are significantly scaled or specialized, the optimal bitrate may shift.

## Foundational Learning

- Concept: Self-supervised learning (SSL) for speech
  - Why needed here: The discrete tokens are derived from SSL models like Wav2Vec2, WavLM, and HuBERT, which learn rich speech representations without labels.
  - Quick check question: What is the main advantage of SSL models over traditional handcrafted features for speech processing?

- Concept: Vector quantization and discrete representations
  - Why needed here: Audio tokens are created by quantizing continuous speech representations into discrete codes, enabling compatibility with language models and efficient processing.
  - Quick check question: How does vector quantization transform continuous speech features into discrete tokens?

- Concept: Downstream architecture adaptation
  - Why needed here: Different tasks require different neural architectures (e.g., BiLSTM for ASR, ECAPA-TDNN for speaker tasks), and the choice affects token performance.
  - Quick check question: Why might a BiLSTM architecture perform better than ECAPA-TDNN for certain ASR tasks with discrete tokens?

## Architecture Onboarding

- Component map: Discrete Audio Encoder → Attention Layer → Downstream Model → (Decoder for generative tasks)

- Critical path: Encoder → Attention → Downstream Model → (Decoder for generative tasks)

- Design tradeoffs:
  - Bitrate vs. Information Retention: Higher bitrate preserves more detail but increases sequence length and computational cost.
  - Encoder Type vs. Task: Semantic encoders excel at content tasks; compression encoders better preserve speaker identity.
  - Decoder Choice: Built-in decoders for compression/hybrid tokens; scalable HiFi-GAN for semantic tokens.

- Failure signatures:
  - Poor ASR performance: Likely due to insufficient phonetic content in tokens or mismatched downstream architecture.
  - Speaker verification failures: May indicate loss of speaker-specific features during tokenization.
  - TTS quality issues: Could stem from inadequate decoder design or loss of fine-grained acoustic details.

- First 3 experiments:
  1. Compare discrete WavLM vs. EnCodec on a simple keyword spotting task at medium bitrate to observe semantic vs. compression performance differences.
  2. Test the same encoder across multiple downstream architectures (BiLSTM vs. ECAPA-TDNN) on speaker verification to see architecture impact.
  3. Vary bitrate (low, medium, high) for a single encoder on speech recognition to identify optimal compression level.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of speaker identity are better preserved by compression tokens compared to semantic tokens?
- Basis in paper: [explicit] The paper states that compression tokens outperform semantic tokens in speaker recognition tasks and preserve speaker identity better, as evidenced by superior speaker similarity (SpkSim) metrics in generative tasks.
- Why unresolved: While the paper demonstrates that compression tokens perform better for speaker-related tasks, it does not provide a detailed analysis of what specific features or characteristics of speaker identity are better encoded by compression tokens. This could include aspects like vocal tract characteristics, speaking style, or other paralinguistic features.
- What evidence would resolve it: A detailed feature analysis comparing the representations learned by compression and semantic tokens for speaker-related characteristics. This could involve techniques like feature importance analysis, visualization of token embeddings, or targeted ablation studies on different aspects of speaker identity.

### Open Question 2
- Question: How does the performance of semantic tokens in generative tasks vary with different decoder architectures?
- Basis in paper: [explicit] The paper mentions that the success of semantic tokens in generative tasks relies heavily on the effectiveness of the decoder architecture used, specifically mentioning a scalable HiFi-GAN-based decoder.
- Why unresolved: The paper only uses one type of decoder (scalable HiFi-GAN) for semantic tokens in generative tasks. It's unclear how the performance would change with different decoder architectures or if certain decoder types are more suited to specific types of semantic tokens.
- What evidence would resolve it: Experiments using various decoder architectures (e.g., different GAN-based decoders, autoregressive models, or flow-based models) with semantic tokens across multiple generative tasks. Comparing the performance across these configurations would reveal the impact of decoder choice on semantic token performance.

### Open Question 3
- Question: What is the relationship between bitrate and the amount of linguistic information preserved in discrete audio tokens?
- Basis in paper: [explicit] The paper observes that medium bitrates generally perform best, while higher bitrates can degrade performance due to increased output dimensionality. It also notes that semantic tokens have lower bitrates but better performance than compression tokens.
- Why unresolved: While the paper establishes a general trend for optimal bitrate, it doesn't provide a detailed analysis of how bitrate affects the preservation of specific types of linguistic information (e.g., phonetics, syntax, semantics). It's unclear if there's a trade-off between preserving different types of linguistic information at different bitrates.
- What evidence would resolve it: A systematic study varying bitrate across a range of values and analyzing the impact on different aspects of linguistic information preservation. This could involve linguistic feature extraction from token sequences and correlation analysis with bitrate and task performance.

## Limitations

- Dataset Diversity and Task Representativeness: Some datasets (particularly for TTS and speech enhancement) may not fully represent real-world deployment scenarios, limiting generalizability.
- Bitrate Selection Constraints: Analysis is constrained by available tokenization options for different encoders, making systematic comparison across full bitrate spectrum difficult.
- Downstream Architecture Standardization: Performance differences might reflect architectural preferences rather than inherent token quality due to task-specific architecture choices.

## Confidence

- **High Confidence**: Discrete WavLM consistently outperforms other tokenizers across most tasks, supported by systematic benchmarking across 9 tasks and multiple datasets.
- **Medium Confidence**: Semantic tokens generally outperform compression tokens, though performance gap varies significantly by task and compression tokens show competitive performance on speaker-related tasks.
- **Medium Confidence**: Medium bitrate provides optimal performance, based on observed trends but lacking systematic exploration of why higher bitrates degrade performance.

## Next Checks

1. **Cross-linguistic Generalization Test**: Validate whether semantic token superiority holds for non-English languages by benchmarking the same tokenizers on Mandarin, Arabic, and Finnish speech datasets.

2. **Architecture-Agnostic Performance Evaluation**: Re-run the benchmark using a single, standardized architecture (e.g., Conformer) across all tasks while keeping tokenizers constant to isolate whether performance differences stem from token quality or architectural suitability.

3. **Ablation Study on Bitrate Effects**: Systematically test a single tokenizer (discrete WavLM) across the full range of available bitrates on ASR and speaker verification tasks, including analysis of sequence length, computational requirements, and downstream model capacity.