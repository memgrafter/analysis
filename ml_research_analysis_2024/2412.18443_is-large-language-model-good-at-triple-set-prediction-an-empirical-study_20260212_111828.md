---
ver: rpa2
title: Is Large Language Model Good at Triple Set Prediction? An Empirical Study
arxiv_id: '2412.18443'
source_url: https://arxiv.org/abs/2412.18443
tags:
- triples
- rule
- information
- rules
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether large language models (LLMs) can effectively
  predict complete triples in knowledge graphs. It introduces a framework that combines
  LLM-based rule mining with triple set prediction.
---

# Is Large Language Model Good at Triple Set Prediction? An Empirical Study

## Quick Facts
- arXiv ID: 2412.18443
- Source URL: https://arxiv.org/abs/2412.18443
- Authors: Yuan Yuan; Yajing Xu; Wen Zhang
- Reference count: 39
- Primary result: LLM-based triple set prediction shows significant hallucination issues, with GPT-3.5-turbo achieving FTSP=0.049 and GPT-4o achieving FTSP=0.204 on CFamily dataset

## Executive Summary
This paper investigates whether large language models can effectively predict complete triples in knowledge graphs through a novel framework combining LLM-based rule mining with triple set prediction. The method extracts logical rules from relation lists, partitions knowledge graphs into subgraphs, and uses LLMs to predict missing triples using the extracted rules and relevant triples as context. Experiments on the CFamily dataset reveal that LLMs struggle with this task, exhibiting significant hallucination phenomena where they generate non-existent triples or fail to adhere strictly to given rules. While GPT-4o outperforms GPT-3.5-turbo, performance remains low across all metrics, suggesting fundamental limitations in LLMs' ability to understand structured relations in knowledge graphs.

## Method Summary
The proposed method uses LLM-based rule mining from relation lists to generate logical rules, then partitions the knowledge graph into subgraphs using vertex-cut methods. For each subgraph, the LLM predicts missing triples using the extracted rules and relevant triples as context, employing Chain-of-Thought prompting to simulate human-like reasoning. The approach evaluates performance using Joint Precision, Squared Test Recall, and TSP score metrics. The framework attempts to overcome LLM input length limitations through subgraph partitioning while leveraging the semantic information in relation names for rule generation.

## Key Results
- GPT-3.5-turbo achieved Joint Precision of 0.028, Squared Test Recall of 0.168, and TSP score of 0.049
- GPT-4o improved these metrics to Joint Precision of 0.14, Squared Test Recall of 0.374, and TSP score of 0.204
- LLMs exhibited significant hallucination phenomena, generating non-existent triples or failing to adhere strictly to given rules
- Performance remained low even with GPT-4o, suggesting fundamental limitations in LLM understanding of structured relations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can mine logical rules from relation lists without requiring statistical data.
- Mechanism: The LLM leverages semantic understanding of relation names to generate valid logical rules in the form of r ← r1 ∧ r2 ∧ ... ∧ rm.
- Core assumption: The LLM's pre-training corpus contains sufficient examples of logical reasoning patterns between relation names.
- Evidence anchors:
  - [abstract] "The relation list of KG embedded within rich semantic information is first leveraged to prompt LLM in the generation of rules."
  - [section] "LLM analyzes the interconnections and logical relationships among different relations based on the information in prompt and then generates effective rules."
  - [corpus] Found 25 related papers discussing LLM-based KG completion methods, suggesting this approach is an active research area.
- Break condition: If the relation names lack sufficient semantic context or if the LLM's pre-training data lacks logical reasoning patterns, rule mining quality will degrade significantly.

### Mechanism 2
- Claim: LLMs can predict missing triples using context from subgraphs and mined rules.
- Mechanism: The LLM uses extracted triples from subgraphs and specified rules as context to generate predictions, simulating human-like reasoning through Chain-of-Thought prompting.
- Core assumption: LLMs can effectively use structured triple information within their context window to maintain factual consistency.
- Evidence anchors:
  - [abstract] "For each subgraph, the specified rule is applied in conjunction with the relevant triples within that subgraph to guide the LLM in predicting the missing triples."
  - [section] "The information of triples is the basis for model inference" and "the thought of Chain-of-Thought (CoT) [35] in the design of Prompt to make LLMs simulate the human's step-by-step thinking process."
  - [corpus] Related works like "GS-KGC: A Generative Subgraph-based Framework" suggest subgraph-based approaches are being explored for LLM-based KGC.
- Break condition: If the context window is insufficient to capture all relevant triples or if the LLM cannot maintain coherence with the structured data, hallucination increases.

### Mechanism 3
- Claim: GPT-4o outperforms GPT-3.5-turbo in triple set prediction tasks.
- Mechanism: GPT-4o's larger context window and improved reasoning capabilities allow better handling of complex logical relationships and larger fact sets.
- Core assumption: The architectural improvements in GPT-4o directly translate to better performance on structured reasoning tasks.
- Evidence anchors:
  - [section] "When employing GPT-4o to predict the missing triples, the average classification metrics are JP recision = 0.14, ST Recall = 0.374, FT SP = 0.204" compared to GPT-3.5-turbo's "JP recision = 0.028, ST Recall = 0.168, FT SP = 0.049."
  - [corpus] The paper explicitly compares these two models, showing GPT-4o's superior performance.
- Break condition: If the task complexity exceeds GPT-4o's reasoning capabilities or if the context becomes too large, performance may degrade regardless of model improvements.

## Foundational Learning

- Concept: Knowledge Graph (KG) structure and triple representation
  - Why needed here: Understanding that KGs are represented as (head entity, relationship, tail entity) triples is fundamental to grasping the TSP task.
  - Quick check question: What are the three components of a knowledge graph triple?

- Concept: Closed World Assumption (CWA) in knowledge graph completion
  - Why needed here: The evaluation metrics and experimental design rely on assuming that triples not explicitly in the KG are false.
  - Quick check question: Under CWA, what is the status of a triple that doesn't exist in the knowledge graph?

- Concept: Logical rule mining and inference
  - Why needed here: The method depends on mining rules like r ← r1 ∧ r2 and using them for reasoning about missing triples.
  - Quick check question: What does the logical rule "uncleOf (X, Y) ← brotherOf (X, Z1) ∧ fatherOf(Z1, Y)" mean in plain language?

## Architecture Onboarding

- Component map: Relation list → Rule Miner → Graph Partitioner → Triple Predictor → Evaluator
- Critical path: Relation list → Rule Miner → Graph Partitioner → Triple Predictor → Evaluator
- Design tradeoffs:
  - Rule quality vs. quantity: Stricter filtering (higher confidence thresholds) reduces hallucination but may miss valid rules
  - Subgraph size vs. completeness: Smaller subgraphs reduce context window requirements but may fragment related information
  - LLM choice vs. cost: GPT-4o provides better performance but at higher cost than GPT-3.5-turbo
- Failure signatures:
  - Low JP recision: LLM generates mostly incorrect triples (hallucination problem)
  - Low ST Recall: LLM misses many correct triples (insufficient coverage)
  - High variance across runs: LLM predictions are unstable and context-dependent
- First 3 experiments:
  1. Run rule mining on a small KG with known logical rules and verify generated rules match expectations
  2. Test subgraph partitioning on KG to ensure entities are grouped meaningfully and subgraph sizes are balanced
  3. Perform single-subgraph triple prediction with known correct triples to validate LLM reasoning capability before full-scale evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific modifications to LLM architecture or training could improve their ability to understand structured relations in knowledge graphs?
- Basis in paper: [explicit] The paper discusses LLMs' difficulty in understanding structured relations compared to natural language information, leading to hallucinations.
- Why unresolved: The paper identifies the problem but doesn't propose specific architectural solutions or training modifications to address it.
- What evidence would resolve it: Empirical comparison of different LLM architectures (e.g., with graph neural network components) or fine-tuning strategies specifically for structured relation understanding.

### Open Question 2
- Question: How does the performance of LLM-based triple set prediction vary across different types of knowledge graphs (e.g., family, geographical, biological)?
- Basis in paper: [inferred] The paper only tests on the CFamily dataset, suggesting a need for broader evaluation across diverse knowledge graph domains.
- Why unresolved: The experiments are limited to a single family-based knowledge graph, leaving open questions about generalizability to other domains.
- What evidence would resolve it: Comparative experiments on multiple knowledge graph datasets representing different domains and relationship types.

### Open Question 3
- Question: What is the optimal strategy for balancing subgraph size and information completeness in LLM-based triple set prediction?
- Basis in paper: [explicit] The paper discusses graph partitioning to handle LLM input length limits but doesn't explore optimal subgraph sizes or their impact on prediction accuracy.
- Why unresolved: While the paper implements graph partitioning, it doesn't systematically investigate how different partition sizes affect performance.
- What evidence would resolve it: Controlled experiments varying subgraph sizes and measuring their impact on precision, recall, and hallucination rates.

## Limitations

- LLMs exhibit significant hallucination phenomena, generating non-existent triples or failing to adhere strictly to given rules
- Performance remains low even with GPT-4o, suggesting fundamental limitations in LLM understanding of structured relations
- Study only evaluates on CFamily dataset, limiting generalizability to other knowledge graph domains and relation types

## Confidence

**High Confidence**: The experimental results showing poor performance of both GPT-3.5-turbo and GPT-4o on triple set prediction, with specific metrics (JP precision = 0.028/0.14, ST Recall = 0.168/0.374, FTSP = 0.049/0.204). These quantitative results are directly reported and verifiable.

**Medium Confidence**: The attribution of poor performance to LLMs' inability to understand structured relations and reliance on context. While the evidence supports this conclusion, alternative explanations (such as prompt engineering limitations or the inherent difficulty of the TSP task) were not fully explored.

**Low Confidence**: The assertion that the method represents a novel approach combining LLM-based rule mining with triple set prediction. Without comparison to baseline methods or ablation studies, it's unclear whether the rule mining component actually contributes to the observed performance.

## Next Checks

1. **Ablation Study on Rule Mining**: Remove the rule mining component and directly prompt LLMs to predict triples from subgraph contexts. Compare performance to determine whether rule mining actually improves results or introduces additional complexity that degrades performance.

2. **Cross-Dataset Validation**: Evaluate the same method on other knowledge graph datasets (e.g., FB15k-237, WN18RR) to assess generalizability. Different datasets may reveal whether the CFamily-specific characteristics contribute to the observed limitations.

3. **Hallucination Analysis**: Conduct a detailed error analysis categorizing incorrect predictions into types (hallucinations, omissions, logical inconsistencies). This would validate whether the primary issue is indeed LLM hallucination or other factors like incomplete rule coverage or context window limitations.