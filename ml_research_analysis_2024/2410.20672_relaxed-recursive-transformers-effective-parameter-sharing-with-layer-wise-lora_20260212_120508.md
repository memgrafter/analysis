---
ver: rpa2
title: 'Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise
  LoRA'
arxiv_id: '2410.20672'
source_url: https://arxiv.org/abs/2410.20672
tags:
- recursive
- performance
- lora
- relaxed
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently deploying large
  language models (LLMs) by revisiting parameter sharing techniques, specifically
  "layer tying" in Transformers. The authors introduce Recursive Transformers, which
  compress LLMs by sharing parameters across recursively looped blocks of layers,
  and further improve performance by introducing Relaxed Recursive Transformers that
  add layer-wise low-rank adaptation (LoRA) modules.
---

# Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA

## Quick Facts
- **arXiv ID:** 2410.20672
- **Source URL:** https://arxiv.org/abs/2410.20672
- **Reference count:** 40
- **Primary result:** Recursive Transformers with layer-wise LoRA achieve performance on par with full-size models while using fewer parameters and enable 2-3x inference throughput gains through continuous depth-wise batching.

## Executive Summary
This paper addresses efficient deployment of large language models by revisiting layer tying in Transformers. The authors introduce Recursive Transformers that compress LLMs by sharing parameters across recursively looped blocks of layers, and further improve performance by adding layer-wise low-rank adaptation (LoRA) modules. Recursive Transformers are initialized from standard pretrained Transformers using a single block of unique layers that is repeated multiple times in a loop. The Relaxed Recursive Transformer variant adds LoRA modules to capture depth-specific variations while maintaining parameter efficiency. The primary results show that Recursive Transformers significantly outperform similar-sized vanilla pretrained models and can recover most of the original full-size model's performance, with potential 2-3x inference throughput gains through a new continuous depth-wise batching paradigm.

## Method Summary
Recursive Transformers compress LLMs by sharing parameters across recursively looped blocks of layers. The model uses a single block of unique layers initialized from a pretrained model, which is then repeated multiple times in a loop. To improve performance, the authors introduce Relaxed Recursive Transformers that add layer-wise LoRA modules, allowing each layer to specialize for different depths in the recursive loop. The LoRA modules are initialized using truncated SVD on residuals to align with the original model's layer differences. The approach also introduces Continuous Depth-wise Batching, a new inference paradigm enabled by the Recursive Transformer when paired with early exiting, which can lead to significant gains in inference throughput.

## Key Results
- Recursive Transformers significantly outperform similar-sized vanilla pretrained models (e.g., recursive Gemma 1B outperforms TinyLlama 1.1B and Pythia 1B)
- Relaxed Recursive Transformers with LoRA modules achieve performance on par with full-size models despite using fewer parameters
- Gemma 2B can be recovered to near-original performance using Recursive Transformer approach
- Continuous Depth-wise Batching has potential to lead to 2-3x gains in inference throughput compared to vanilla Transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive Transformers can achieve strong performance despite halving parameters by leveraging pretrained weights and recursive looping.
- Mechanism: By initializing the shared block of layers using weights from the original pretrained model (Stepwise method) and then fine-tuning with a limited number of tokens, the model preserves most of the original knowledge while benefiting from parameter sharing.
- Core assumption: Pretrained model weights contain sufficient information to initialize a recursive structure without significant loss of performance.
- Evidence anchors:
  - [abstract] "Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop."
  - [section] "To mitigate the potential performance degradation associated with reduced capacity in parameter-shared models, we propose several novel initialization methodologies to facilitate effective knowledge transfer from unshared, pretrained models to Recursive Transformers."
  - [corpus] Weak - no direct evidence of recursive initialization from corpus.
- Break condition: If the pretrained model's weights are too dissimilar from the task domain, or if the recursive structure cannot adequately capture the necessary complexity.

### Mechanism 2
- Claim: Adding layer-wise LoRA modules (Relaxed Recursive Transformers) allows the model to capture depth-specific variations while maintaining parameter efficiency.
- Mechanism: LoRA modules introduce low-rank deltas to the shared layer weights, allowing each layer to specialize for different depths in the recursive loop. SVD initialization aligns these deltas with the original model's layer differences.
- Core assumption: Low-rank adaptations are sufficient to capture the essential differences between layers at different depths.
- Evidence anchors:
  - [abstract] "we introduce Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules"
  - [section] "we propose novel initialization methods, especially designed for Relaxed Recursive Transformers. To effectively match the performance of the original full-size model... we aim for the sum of the tied weights (Φ′) and LoRA weights (ΔΦ′) to approximately recover the full-size model's weights (Φ)"
  - [corpus] Weak - no direct evidence of SVD-based LoRA initialization from corpus.
- Break condition: If the rank of LoRA modules is insufficient to capture necessary variations, or if the SVD approximation is poor.

### Mechanism 3
- Claim: Continuous depth-wise batching enables significant throughput gains by exploiting the recursive nature of the model.
- Mechanism: Because parameters are shared across layers and loop iterations, different samples at different depths can be processed in parallel within the same batch, maximizing GPU utilization.
- Core assumption: The computational graph for different loop iterations is identical and can be parallelized.
- Evidence anchors:
  - [abstract] "we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting"
  - [section] "This introduces a new dimension for continuous batching, which we call Continuous Depth-wise Batching. This technique enables the simultaneous computation of different iterations of the looped layer block for different samples"
  - [corpus] Weak - no direct evidence of continuous depth-wise batching from corpus.
- Break condition: If memory bandwidth becomes a bottleneck, or if the scheduling overhead outweighs the benefits.

## Foundational Learning

- Concept: Layer tying and parameter sharing in neural networks
  - Why needed here: Understanding how sharing weights across layers affects model capacity and performance is crucial for grasping the Recursive Transformer concept.
  - Quick check question: What is the primary trade-off when sharing parameters across layers in a neural network?

- Concept: Low-rank adaptation (LoRA) and truncated SVD
  - Why needed here: LoRA is the core mechanism for relaxing the parameter sharing constraint, and SVD is used for initialization.
  - Quick check question: How does LoRA achieve parameter efficiency while allowing for task-specific adaptation?

- Concept: Early exiting and dynamic computation graphs
  - Why needed here: Early exiting is paired with recursive transformers to achieve throughput gains, and understanding dynamic computation is key to grasping continuous depth-wise batching.
  - Quick check question: What is the main challenge in implementing early exiting in traditional transformer models?

## Architecture Onboarding

- Component map:
  Single block of unique layers (shared across all loop iterations) -> Layer-wise LoRA modules (optional, for relaxed variant) -> Early exiting mechanism (optional, for throughput gains) -> Continuous depth-wise batching scheduler (for throughput gains)

- Critical path:
  1. Initialize shared layer weights using pretrained model (Stepwise/Average/Lower method)
  2. Initialize LoRA modules using truncated SVD on residuals (if using relaxed variant)
  3. Fine-tune on target dataset with knowledge distillation (optional, for performance)
  4. Train for early exiting with aggressive coefficient strategy (optional, for throughput)
  5. Deploy with continuous depth-wise batching scheduler (for throughput)

- Design tradeoffs:
  - Number of looping blocks (B) vs. model size and performance
  - LoRA rank vs. parameter efficiency and expressivity
  - Early exiting aggressiveness vs. final performance and throughput
  - Knowledge distillation strength vs. training time and performance

- Failure signatures:
  - Training loss plateaus early: likely insufficient LoRA rank or poor initialization
  - Final performance significantly below baseline: likely distribution shift or insufficient fine-tuning
  - Throughput gains not realized: likely memory bandwidth bottleneck or scheduling overhead

- First 3 experiments:
  1. Convert a small pretrained model (e.g., Pythia 160M) to a 2-block Recursive Transformer using Stepwise initialization, fine-tune on SlimPajama, and evaluate few-shot performance.
  2. Add LoRA modules with rank 128 to the Recursive Transformer from experiment 1, initialize using SVD, and compare performance.
  3. Implement early exiting on the model from experiment 2, train with aggressive coefficient strategy, and measure hypothetical throughput gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Relaxed Recursive Transformers scale with model size (7B and beyond), and what are the uptraining cost implications?
- Basis in paper: [explicit] "Scaling up Recursive Transformers Scaling our approach to larger LLMs (7B and beyond) is a promising avenue for future research. While our methodology is expected to remain effective, achieving comparable performance may require significantly higher uptraining costs."
- Why unresolved: The paper focuses on 1B-2B parameter models and does not provide empirical data for larger models. The authors acknowledge that higher model sizes may require substantially more uptraining resources, but do not quantify this relationship.
- What evidence would resolve it: Empirical studies comparing Recursive Transformers of 7B+ parameters to their vanilla counterparts, including comprehensive analysis of uptraining costs, performance trade-offs, and memory efficiency at scale.

### Open Question 2
- Question: What is the optimal LoRA rank distribution across different Transformer components (Q, KV, Out, FFN) for maximizing performance and efficiency?
- Basis in paper: [explicit] "We further experimented with assigning different ranks to LoRA modules associated with each linear weights... The experimental results in Table G.1 reveal a strong correlation between performance and overall model sizes."
- Why unresolved: While the paper explores varying LoRA ranks for different components, it does not provide a definitive recommendation for optimal rank distribution. The authors note that FFN layers have the most significant impact on performance when rank is reduced.
- What evidence would resolve it: Systematic ablation studies across various model sizes and architectures, identifying the optimal rank distribution strategy that balances performance gains with computational efficiency.

### Open Question 3
- Question: How can we optimize LoRA computations for multi-LoRA layer serving to achieve theoretical throughput gains in practice?
- Basis in paper: [explicit] "Relaxed models require the computation of distinct LoRA modules during batched inference... hindering parallel computation... To mitigate this, we can explore optimized CUDA kernels for LoRA serving... and parallelization across accelerators."
- Why unresolved: The paper acknowledges that current multi-LoRA serving strategies introduce computational overhead and redundancy. While the authors propose concatenating LoRA weights, they recognize this creates inefficiencies and suggest this as a direction for future work.
- What evidence would resolve it: Implementation and benchmarking of optimized LoRA serving strategies, including specialized CUDA kernels, distributed computing approaches, and real-world deployment scenarios demonstrating throughput improvements.

### Open Question 4
- Question: How does continuous depth-wise batching perform with realistic confidence-based early-exiting algorithms versus oracle-exiting?
- Basis in paper: [explicit] "Our oracle-exiting approach assumes any intermediate prediction matching the final output can be exited. However, accurate throughput measurement requires confidence-based early-exiting algorithms."
- Why unresolved: The paper's throughput analysis relies on oracle-exiting assumptions, which may not reflect real-world performance. The authors acknowledge that practical early-exiting requires confidence estimation and may introduce additional computational overhead.
- What evidence would resolve it: Empirical evaluation of continuous depth-wise batching with practical early-exiting algorithms, measuring actual throughput gains, performance degradation, and computational overhead in real-world deployment scenarios.

## Limitations
- Empirical validation limited to narrow set of model sizes (160M-2.6B parameters) and decoder-only architectures
- Throughput improvements based on synthetic measurements rather than real-world deployment scenarios
- No investigation of encoder-decoder architectures or non-text modalities

## Confidence
- **Medium**: Recursive Transformers achieving competitive performance with halved parameters
- **Medium**: LoRA integration and SVD initialization effectively capturing layer-specific variations
- **Low**: 2-3x throughput improvements from continuous depth-wise batching in real-world scenarios

## Next Checks
1. **Scale validation**: Test Recursive Transformers on models 10B+ parameters to assess whether initialization methods and LoRA integration scale effectively, particularly examining if performance degradation occurs at larger scales.

2. **Cross-domain robustness**: Evaluate the approach on encoder-decoder models (e.g., T5 variants) and multimodal architectures to determine if the parameter-sharing benefits generalize beyond decoder-only text models.

3. **Real-world throughput measurement**: Implement continuous depth-wise batching on actual hardware with production workloads to measure genuine latency and throughput improvements, accounting for memory bandwidth constraints and scheduling overhead.