---
ver: rpa2
title: 'DisEmbed: Transforming Disease Understanding through Embeddings'
arxiv_id: '2412.15258'
source_url: https://arxiv.org/abs/2412.15258
tags:
- medical
- disease
- disembed
- tasks
- diseases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DisEmbed is a disease-focused embedding model trained on synthetic
  disease descriptions, symptoms, and Q&A pairs. It outperforms general medical models
  on disease-specific benchmarks, achieving 94.5%, 91.6%, and 93.7% accuracy on Disease
  Database, Medical Diagnosis Dialogue, and CoD-PatientSymDisease datasets, respectively.
---

# DisEmbed: Transforming Disease Understanding through Embeddings

## Quick Facts
- **arXiv ID**: 2412.15258
- **Source URL**: https://arxiv.org/abs/2412.15258
- **Reference count**: 19
- **Primary result**: DisEmbed outperforms general medical models on disease-specific benchmarks with 94.5% accuracy on Disease Database

## Executive Summary
DisEmbed is a disease-focused embedding model that transforms how AI systems understand and process disease information. Trained on synthetic disease descriptions, symptoms, and Q&A pairs generated by GPT-4o-mini, the model achieves superior performance on disease-specific tasks compared to general medical models like BioBERT and PubMedBERT. By focusing specifically on disease understanding rather than general medical knowledge, DisEmbed excels at distinguishing between related and unrelated diseases, making it particularly effective for disease retrieval and context identification tasks.

## Method Summary
The model was trained on a synthetic dataset generated from ICD-10-CM disease names using GPT-4o-mini, creating disease descriptions, symptoms, and Q&A pairs without explicitly including disease names. DisEmbed uses BAAI/bge-small-en-v1.5 as its base model and trains using Multiple Negatives Ranking Loss (MNRL) for 4 epochs. The evaluation framework employs triplet-based methodology to measure how well the model distinguishes between disease descriptions and their symptoms versus unrelated diseases, using cosine similarity as the primary metric.

## Key Results
- Achieved 94.5%, 91.6%, and 93.7% accuracy on Disease Database, Medical Diagnosis Dialogue, and CoD-PatientSymDisease datasets respectively
- Demonstrated strong discrimination between related (neuropathy) and unrelated (epilepsy syndrome) diseases with cosine similarity scores of 0.6333 vs 0.1062
- Outperformed general medical models on disease-specific benchmarks while maintaining a compact 33M parameter size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DisEmbed's disease-focused synthetic dataset improves embedding discrimination between related and unrelated diseases
- Mechanism: By training on disease descriptions without explicitly labeled symptoms, the model learns deeper associations between diseases and their manifestations rather than superficial medical terminology matching
- Core assumption: The model can extract meaningful disease patterns from synthetic data generated by GPT-4o-mini
- Evidence anchors:
  - [abstract] "DisEmbed is trained on a synthetic dataset specifically curated to include disease descriptions, symptoms, and disease-related Q&A pairs"
  - [section] "this approach was intended to encourage the model to better understand the underlying disease concepts without relying on the disease names themselves"
  - [corpus] Weak evidence - neighboring papers focus on general disease diagnosis but don't specifically address synthetic dataset approaches for embeddings
- Break condition: If the synthetic data quality degrades or contains too much noise, the model's ability to learn precise associations would deteriorate

### Mechanism 2
- Claim: Multiple Negatives Ranking Loss (MNRL) effectively trains the model to distinguish between disease-related contexts
- Mechanism: MNRL optimizes for minimizing similarity distance between anchor-disease pairs while maximizing distance from unrelated examples, creating clear separation in embedding space
- Core assumption: The disease-disease and disease-symptom pairs in the synthetic dataset accurately represent true relationships
- Evidence anchors:
  - [section] "The loss function used for training was the Multiple Negatives Ranking Loss (MNRL)"
  - [section] "The training objective was to minimize the similarity distance between these paired examples"
  - [corpus] Weak evidence - while triplet loss is mentioned in corpus papers, specific application to disease embeddings is not well-covered
- Break condition: If the margin parameter is set incorrectly, the model may fail to create sufficient separation between related and unrelated diseases

### Mechanism 3
- Claim: Disease-specific evaluation using triplet methodology provides meaningful performance metrics for DisEmbed
- Mechanism: Triplet evaluation ensures that similarity between disease descriptions and their symptoms exceeds similarity to unrelated diseases by a defined margin, directly measuring disease understanding capability
- Core assumption: The triplet datasets (Disease Database, Medical Diagnosis Dialogue, CoD-PatientSymDisease) accurately represent disease-symptom relationships
- Evidence anchors:
  - [section] "I employed a triplet-based evaluation strategy, using both the 'TripletEvaluator' from the 'sentence-transformers' library"
  - [section] "The accuracy is defined as the percentage of triplets satisfying the above inequality"
  - [corpus] Moderate evidence - corpus includes papers on disease diagnosis but none specifically address triplet evaluation for disease embeddings
- Break condition: If the triplet datasets contain incorrect disease-symptom mappings, the evaluation metrics would not accurately reflect model performance

## Foundational Learning

- Concept: Cosine similarity as a metric for semantic understanding
  - Why needed here: The paper uses cosine similarity to evaluate how well the model distinguishes between related and unrelated diseases
  - Quick check question: If two disease embeddings have a cosine similarity of 0.95, are they likely to be the same disease or different diseases?

- Concept: Triplet loss and evaluation methodology
  - Why needed here: The paper's evaluation framework relies on triplet learning where anchor-positive similarity should exceed anchor-negative similarity by a margin
  - Quick check question: In triplet evaluation, what happens to accuracy if the margin parameter is set too high?

- Concept: Synthetic data generation for specialized training
  - Why needed here: The model was trained on synthetic disease descriptions and symptoms generated by GPT-4o-mini rather than real medical data
  - Quick check question: What are the potential risks of using synthetic data for training medical AI models?

## Architecture Onboarding

- Component map: Synthetic dataset generation → MNRL training on disease-disease pairs → Triplet evaluation on disease-specific benchmarks
- Critical path: Data generation → Model training → Performance evaluation on disease-specific tasks
- Design tradeoffs: Smaller parameter size (33M) vs. better disease-specific performance vs. potential limitations in general medical tasks
- Failure signatures: High cosine similarity across unrelated diseases, poor performance on triplet evaluation, overfitting to synthetic data patterns
- First 3 experiments:
  1. Test cosine similarity between known related disease pairs (e.g., neuropathy and diabetic neuropathy) vs. unrelated pairs (neuropathy and epilepsy)
  2. Evaluate triplet accuracy on a small subset of the Disease Database to verify evaluation methodology
  3. Compare embeddings of disease descriptions with and without symptom names to test the model's ability to learn beyond surface terminology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DisEmbed's performance on disease-specific tasks compare to general medical models when evaluated on broader medical benchmarks beyond disease-focused datasets?
- Basis in paper: [inferred] The paper notes that while DisEmbed excels in disease-specific tasks, it may not generalize to broader medical benchmarks, suggesting a need for evaluation on more diverse medical datasets
- Why unresolved: The paper primarily focuses on disease-specific benchmarks and does not provide comparative results on general medical tasks, leaving the model's broader applicability unclear
- What evidence would resolve it: Testing DisEmbed on a variety of general medical benchmarks, such as clinical text classification or medical entity recognition tasks, and comparing its performance to established general medical models

### Open Question 2
- Question: What is the impact of synthetic dataset biases on DisEmbed's performance, and how can these biases be mitigated in future iterations?
- Basis in paper: [explicit] The paper acknowledges that the synthetic dataset may contain inherent biases due to its generation process, which could affect the model's performance in edge cases
- Why unresolved: The paper does not explore the specific nature or extent of these biases, nor does it propose methods for mitigating them
- What evidence would resolve it: Conducting bias analysis on the synthetic dataset and evaluating DisEmbed's performance on real-world data to identify and address potential biases

### Open Question 3
- Question: How does increasing the model's parameter size affect DisEmbed's performance and computational efficiency in disease-specific tasks?
- Basis in paper: [inferred] The paper suggests that increasing the model's parameters could improve its ability to capture complex disease relationships, but it does not explore the trade-offs between performance gains and computational costs
- Why unresolved: The paper does not provide experimental results on models with different parameter sizes, leaving the impact of parameter scaling unclear
- What evidence would resolve it: Training DisEmbed with varying parameter sizes and evaluating the performance and computational efficiency trade-offs in disease-specific tasks

## Limitations

- Synthetic data quality uncertainty: The model's performance depends entirely on the quality of synthetic data generated by GPT-4o-mini, which may contain biases or inaccuracies
- Evaluation scope limitation: Current evaluation focuses on disease-specific tasks and benchmark datasets, but doesn't demonstrate performance on real-world clinical scenarios
- Generalization concerns: Disease-specific performance may come at the cost of reduced capability on broader medical applications

## Confidence

**High Confidence**: The technical methodology (MNRL training, triplet evaluation framework, cosine similarity metrics) is sound and well-implemented.

**Medium Confidence**: The comparative performance claims against BioBERT and PubMedBERT are valid for the specific disease-focused tasks tested, but may not generalize to all medical NLP applications.

**Low Confidence**: Claims about the model's ability to understand "deeper disease concepts" beyond terminology matching are based on synthetic data evaluation and may not reflect real-world medical knowledge understanding.

## Next Checks

1. **Clinical Validation**: Test DisEmbed on a real-world clinical dataset with actual patient records to verify that synthetic data training translates to practical medical understanding.

2. **Bias Analysis**: Conduct a comprehensive analysis of the synthetic data for potential biases, particularly around disease prevalence, symptom representation, and demographic factors that could affect model performance.

3. **Long-term Performance Monitoring**: Implement continuous evaluation of the model on updated medical literature and clinical guidelines to assess whether synthetic training data becomes outdated as medical knowledge evolves.