---
ver: rpa2
title: Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action
  Localization
arxiv_id: '2407.07024'
source_url: https://arxiv.org/abs/2407.07024
tags:
- action
- videos
- video
- self-training
- ov-tal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using unlabeled web videos for self-training
  to improve open-vocabulary temporal action localization (OV-TAL). Existing OV-TAL
  methods rely on limited human-labeled datasets, hindering generalizability.
---

# Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization

## Quick Facts
- **arXiv ID**: 2407.07024
- **Source URL**: https://arxiv.org/abs/2407.07024
- **Reference count**: 40
- **Primary result**: Self-training with web-scale unlabeled videos significantly improves cross-category generalization for open-vocabulary temporal action localization.

## Executive Summary
This paper addresses the scalability challenge in open-vocabulary temporal action localization (OV-TAL) by proposing a self-training pipeline that leverages unlabeled web videos. The authors identify limitations in existing OV-TAL methods that rely solely on limited human-labeled datasets, which restricts generalizability to novel action categories. Their two-stage approach generates pseudo-labels from unlabeled videos using a base-trained action localizer, then retrains the model on this expanded dataset. Experiments demonstrate substantial improvements in cross-category generalization, particularly for actions outside the training distribution. The paper also introduces a new benchmark and evaluation setting to better assess OV-TAL performance.

## Method Summary
The paper proposes a two-stage self-training pipeline for OV-TAL. First, a class-agnostic action localizer is trained on a human-labeled TAL dataset to generate pseudo-labels for unlabeled videos. This localizer detects action instances without classifying them, using a decoupled architecture with an open-vocabulary action classifier based on a frozen VLM text encoder. Second, the large-scale pseudo-labeled dataset is used to retrain the localizer, improving its generalizability. The method employs the Mean Teacher framework for model updates and uses threshold-based filtering to ensure pseudo-label quality. The authors evaluate their approach on standard TAL datasets with a novel Kinetics-based category split that distinguishes base categories (in Kinetics-400) from novel categories (not in Kinetics-400).

## Key Results
- Self-training with web-scale videos (up to 332k videos) significantly enhances cross-category generalization of action localizers.
- The decoupled framework enables flexible integration of VLMs like CLIP and Gemini for open-vocabulary classification.
- Kinetics-based category split and generalized zero-shot evaluation setting provide more rigorous assessment of OV-TAL performance.
- Gemini-1.5 demonstrates strong TAL performance on the proposed benchmark, particularly for longer action instances.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-training with large-scale unlabeled web videos improves cross-category generalization of the action localizer.
- Mechanism: The action localizer trained on base categories is applied to unlabeled videos to generate pseudo-labels. These pseudo-labeled videos from diverse categories and domains are used to retrain the localizer, enriching its ability to detect novel action categories beyond the training set.
- Core assumption: Pseudo-labels generated by the base-trained localizer are sufficiently accurate to improve generalization without introducing too much noise.
- Evidence anchors:
  - [abstract] "leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer"
  - [section] "our self-training approach leads to improved cross-category generalizability"
  - [corpus] Weak corpus evidence - related works focus on few-shot or semi-supervised approaches, but none directly test scalability with web-scale videos for OV-TAL.
- Break condition: Pseudo-label quality degrades due to domain shift or noise, causing the localizer to learn incorrect patterns and lose performance on base and novel actions.

### Mechanism 2
- Claim: Decoupling the class-agnostic action localizer from the open-vocabulary action classifier enables flexible integration of VLMs for improved performance.
- Mechanism: The class-agnostic localizer detects all action instances without classification. The open-vocabulary classifier assigns categories using a frozen VLM's text encoder. This separation allows using pre-trained VLMs like CLIP or Gemini directly, leveraging their large-scale training without fine-tuning on small TAL datasets.
- Core assumption: The frozen VLM's text encoder can accurately map detected action instances to correct categories based on text embeddings.
- Evidence anchors:
  - [abstract] "a decoupled framework, composed of a class-agnostic action localizer and an open-vocabulary action classifier"
  - [section] "we decouple the localization and classification of actions in our architecture into two components"
  - [corpus] Weak corpus evidence - related works like STALE and UnLoc also use decoupled frameworks, but details on classifier flexibility are sparse.
- Break condition: The VLM's text encoder fails to generalize to novel action categories or the feature alignment between video and text embeddings degrades.

### Mechanism 3
- Claim: The proposed Kinetics-based category split and generalized zero-shot evaluation setting provide a more rigorous assessment of cross-category generalization.
- Mechanism: Base categories are defined as those present in Kinetics-400, and novel categories as those not in Kinetics-400. Evaluation includes both base and novel categories, revealing whether fine-tuning on base categories harms performance on novel ones.
- Core assumption: Kinetics-400 contains common, frequent actions, making it a valid basis for defining base vs. novel categories.
- Evidence anchors:
  - [abstract] "we introduce a generalized zero-shot setting where the target categories include both base and novel classes"
  - [section] "The action list in K400 is carefully selected to encompass actions that are commonly encountered in daily life"
  - [corpus] Weak corpus evidence - related works use random splits without considering category frequency or coverage in common datasets.
- Break condition: The Kinetics-based split fails to reflect true action frequency or distribution, leading to misleading conclusions about generalization.

## Foundational Learning

- Concept: Vision-Language Models (VLMs) like CLIP and their video adaptations
  - Why needed here: VLMs provide the backbone for both the action localizer and classifier, enabling open-vocabulary learning without extensive labeled data.
  - Quick check question: How does a VLM's text encoder map action names to embeddings used for classification?

- Concept: Self-training and pseudo-labeling in semi-supervised learning
  - Why needed here: Self-training leverages unlabeled web videos to improve the action localizer's generalization by generating and using pseudo-labels.
  - Quick check question: What factors influence the quality and reliability of pseudo-labels in self-training?

- Concept: Temporal action localization (TAL) evaluation metrics (mAP, tIoU)
  - Why needed here: Accurate evaluation of localization and classification performance is critical for validating improvements from self-training.
  - Quick check question: How does the choice of tIoU threshold affect mAP scores in TAL?

## Architecture Onboarding

- Component map:
  - Video VLM encoder (e.g., ViFi-CLIP) → Video snippet features
  - Class-agnostic action localizer (e.g., ActionFormer) → Action instances with actionness scores
  - Open-vocabulary action classifier (frozen VLM text encoder + RoI-Align) → Action categories and final scores
  - Pseudo-label generation and filtering → Self-training dataset
  - Mean Teacher framework → Teacher-student model updates

- Critical path:
  1. Extract video snippet features using VLM.
  2. Apply class-agnostic action localizer to detect action instances.
  3. Use VLM text encoder to classify action instances into categories.
  4. Fuse actionness and category scores for final confidence.
  5. Generate pseudo-labels from unlabeled videos and retrain localizer.

- Design tradeoffs:
  - Using frozen VLM vs. fine-tuning: Frozen VLM preserves generalization but may lack dataset-specific adaptation.
  - Class-agnostic localizer vs. one-stage methods: Decoupling allows flexible classifier integration but adds complexity.
  - Pseudo-label quality vs. quantity: Higher thresholds improve quality but reduce training data volume.

- Failure signatures:
  - Localization recall drops significantly for novel actions.
  - Classification accuracy degrades when base and novel categories are mixed.
  - Self-training causes overfitting to pseudo-labels, harming generalization.
  - Gemini underperforms on short action instances due to frame sampling.

- First 3 experiments:
  1. Evaluate base-trained localizer on novel action categories to establish baseline generalization.
  2. Generate pseudo-labels from unlabeled videos and analyze their quality vs. ground truth.
  3. Train localizer with pseudo-labeled data and compare cross-category mAP before and after self-training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal video length for effective action localization in large multimodal models like Gemini?
- Basis in paper: [explicit] The paper notes that Gemini underperforms on short action instances in THUMOS14, while it performs well on longer instances in FineAction. The average durations of action instances are provided: (5.1, 2.6) seconds for THUMOS14's (CB, CN) and (7.5, 12.4) seconds for FineAction's (CB, CN).
- Why unresolved: The paper only compares performance on two datasets with different action instance lengths, without exploring intermediate lengths or conducting a systematic study on the relationship between video length and model performance.
- What evidence would resolve it: Experiments varying video lengths within a single dataset, or using datasets with a wider range of action instance durations, would clarify the optimal video length for Gemini and similar models.

### Open Question 2
- Question: How does the quality of pseudo-labels affect the performance of self-training in open-vocabulary temporal action localization?
- Basis in paper: [explicit] The paper mentions that a threshold is applied during pseudo-labeling to filter out low-actionness action instances, and Fig. 6 shows a positive correlation between the actionness of pseudo-labels and their tIoU with ground truth instances. However, the paper uses a straightforward pseudo-labeling method and does not explore advanced techniques.
- Why unresolved: The paper only uses a fixed threshold for pseudo-label selection and does not investigate the impact of different pseudo-labeling strategies or the trade-off between quantity and quality of pseudo-labels.
- What evidence would resolve it: Experiments comparing different pseudo-labeling methods, such as adaptive thresholding or consistency-based filtering, and analyzing the impact of pseudo-label quality on downstream performance would clarify this.

### Open Question 3
- Question: Can the generalizability of open-vocabulary temporal action localization models be further improved by incorporating additional data modalities beyond video and text?
- Basis in paper: [inferred] The paper focuses on leveraging web-scale videos for self-training, but does not explore the use of other modalities like audio or motion capture data, which could provide complementary information for action localization.
- Why unresolved: The paper does not investigate the potential benefits of incorporating additional modalities, and it is unclear whether these modalities would improve performance or introduce noise.
- What evidence would resolve it: Experiments incorporating audio or motion capture data into the self-training pipeline, along with an analysis of the impact on performance and generalizability, would determine the value of multi-modal approaches.

## Limitations

- The scalability improvements depend heavily on the quality and diversity of unlabeled web videos used for self-training.
- The evaluation of open-vocabulary performance relies on specific category splits that may not fully capture real-world deployment scenarios.
- The paper does not explore the optimal video length for effective action localization in large multimodal models like Gemini.

## Confidence

- **High confidence**: The effectiveness of self-training for improving action localization on base categories, supported by clear experimental results and established methodology.
- **Medium confidence**: The cross-category generalization improvements, as these depend on pseudo-label quality and the assumption that web-scale videos provide sufficient diversity.
- **Low confidence**: The superiority of the Kinetics-based category split over random splits, as the paper lacks ablation studies comparing different evaluation schemes.

## Next Checks

1. **Pseudo-label quality analysis**: Systematically evaluate the correlation between actionness scores, tIoU with ground truth, and classification accuracy across different video domains to quantify pseudo-label reliability.
2. **Scale sensitivity study**: Measure performance gains from self-training at multiple scales (10k, 100k, 332k videos) to identify diminishing returns and optimal dataset size.
3. **Cross-dataset generalization test**: Evaluate the self-trained localizer on action categories from datasets not seen during any training phase to validate true open-vocabulary capability.