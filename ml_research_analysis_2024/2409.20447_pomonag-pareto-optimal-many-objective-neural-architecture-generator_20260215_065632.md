---
ver: rpa2
title: 'POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator'
arxiv_id: '2409.20447'
source_url: https://arxiv.org/abs/2409.20447
tags:
- architectures
- architecture
- search
- pomonag
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POMONAG extends DiffusionNAG by introducing a many-objective diffusion
  approach that simultaneously optimises neural architectures for accuracy, parameters,
  MACs, and inference latency. It employs four Performance Predictors to estimate
  these metrics and guide the diffusion process, enabling generation of Pareto-optimal
  architectures.
---

# POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator

## Quick Facts
- arXiv ID: 2409.20447
- Source URL: https://arxiv.org/abs/2409.20447
- Reference count: 40
- Primary result: Achieves state-of-the-art NAS performance with up to 89.96% accuracy (NASBench201) and 88.66% (MobileNetV3), while reducing parameters by up to 90% and 56%, MACs by up to 93% and 67%, and latency by up to 65% and 44% respectively.

## Executive Summary
POMONAG extends DiffusionNAG by introducing a many-objective diffusion approach that simultaneously optimises neural architectures for accuracy, parameters, MACs, and inference latency. It employs four Performance Predictors to estimate these metrics and guide the diffusion process, enabling generation of Pareto-optimal architectures. The method enhances DiffusionNAG through expanded Meta-Datasets (10,000 and 20,000 architecture-dataset pairs), refined Performance Predictors (Spearman correlations: 0.855 for accuracy of noisy architectures, 0.884 for denoised), and Pareto Front Filtering/Stretching techniques. Evaluated on NASBench201 and MobileNetV3 search spaces across 15 image classification datasets, POMONAG achieves state-of-the-art performance while requiring training only one architecture per dataset.

## Method Summary
POMONAG uses a diffusion model to generate neural architectures by reversing Gaussian noise addition, conditioned on dataset embeddings. The process is guided by four Performance Predictors that estimate accuracy, parameters, MACs, and latency. A Meta-Dataset containing 10,000-20,000 architecture-dataset pairs trains these predictors. Generated architectures are filtered through Pareto Front analysis to select non-dominated solutions across all objectives. Dynamic scaling factors for each objective are optimised using Optuna. The method employs Pareto Front Stretching to ensure a minimum number of candidate architectures, which are then evaluated to select the final optimal architecture.

## Key Results
- Achieves up to 89.96% accuracy on NASBench201 and 88.66% on MobileNetV3 search spaces
- Reduces parameters by up to 90% (NASBench201) and 56% (MobileNetV3)
- Reduces MACs by up to 93% (NASBench201) and 67% (MobileNetV3)
- Reduces latency by up to 65% (NASBench201) and 44% (MobileNetV3)
- Maintains state-of-the-art performance while training only one architecture per dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance Predictors trained on larger Meta-Datasets with refined encodings and ViT-B-16 embeddings achieve higher Spearman correlation (0.855 for noisy accuracy, 0.884 for denoised accuracy).
- Mechanism: Larger Meta-Datasets expose predictors to broader architecture-dataset combinations, improving generalisation. Refined encodings (NASBench201: binary operation/adjacency matrices; MobileNetV3: operator+adjacency matrices) reduce ambiguity. ViT-B-16 embeddings capture richer dataset semantics than ResNet18, improving prediction accuracy.
- Core assumption: Predictor architecture and training strategy are robust to dataset and search space variations.
- Evidence anchors:
  - Expanding the Meta-Dataset size from 4,630 to 10,000 architectures further improved the results... Spearman correlations of 0.842 for noisy architectures and 0.884 for denoised ones.
  - Replacing the ResNet18 Dataset Encoder with a Vision Transformer (ViT-B-16) feature extractor led to the best performance, achieving Spearman correlations of 0.855 and 0.884 for noisy and denoised architectures, respectively.

### Mechanism 2
- Claim: Pareto Front Filtering ensures only architectures that are non-dominated across accuracy, parameters, MACs, and latency are retained.
- Mechanism: After generation, architectures are compared across all four objectives. An architecture is kept only if no other architecture is better in all objectives. This forces the search to find balanced solutions rather than focusing on a single metric.
- Core assumption: The four objectives are independent enough that trade-offs exist, making Pareto optimality meaningful.
- Evidence anchors:
  - In POMONAG, an additional filtering step is introduced through the construction of a Pareto Front... This approach enables the selection of architectures that are Pareto-optimal...
  - The configuration POMONAGBal represents the architecture for which the ratio between predicted accuracy and the considered secondary metric is highest.

### Mechanism 3
- Claim: Dynamic scaling factor optimisation via Optuna balances guidance from multiple predictors during generation.
- Mechanism: Scaling factors k_ϕ, k_π, k_μ, k_λ weight the influence of accuracy, parameters, MACs, and latency predictors in the reverse diffusion gradient. Optuna searches for values that maximise mean predicted accuracy on held-out datasets, adjusting the trade-off between objectives during generation.
- Core assumption: Predictor gradients are differentiable and optimisable via hyper-optimisation; scaling factors can meaningfully shift the search distribution.
- Evidence anchors:
  - This optimisation leverages Optuna... The process involves 100 evaluations on four datasets... This procedure is replicated for each search space.
  - For efficient architectures... For highly accurate architectures...

## Foundational Learning

- Concept: Diffusion models reverse Gaussian noise addition to generate data from a learned score function.
  - Why needed here: POMONAG uses diffusion to generate neural architectures by denoising operator matrices conditioned on dataset embeddings.
  - Quick check question: What is the role of the Score Network in the diffusion process?

- Concept: Multi-objective optimisation and Pareto optimality.
  - Why needed here: POMONAG simultaneously optimises accuracy, parameters, MACs, and latency, selecting architectures that dominate others across all metrics.
  - Quick check question: What defines a Pareto-optimal architecture in this context?

- Concept: Neural Architecture Search search spaces and encodings.
  - Why needed here: POMONAG operates on NASBench201 (cell-based) and MobileNetV3 (supernet-based) encodings, requiring correct graph representation for generation and prediction.
  - Quick check question: How are NASBench201 architectures encoded for the diffusion model?

## Architecture Onboarding

- Component map: Meta-Dataset generator -> Score Network (Transformer) -> Four Performance Predictors -> Diffusion Generation -> Pareto Front Filter -> Optuna optimiser -> Final architectures
- Critical path: Meta-Dataset -> Score Network + Predictors -> Diffusion Generation -> Pareto Filter -> Final architectures
- Design tradeoffs:
  - Larger Meta-Datasets improve predictor accuracy but increase training time
  - More complex encodings (MobileNetV3) allow richer architectures but require more sophisticated sampling
  - Pareto filtering reduces output size but may discard useful near-optimal candidates
- Failure signatures:
  - Low validity (generated architectures invalid) -> encoding or sampling issue
  - Low uniqueness/novelty -> Meta-Dataset too small or sampling biased
  - Predictors underperform -> Meta-Dataset lacks diversity or training strategy poor
- First 3 experiments:
  1. Generate 256 architectures on NASBench201 with fixed scaling factors; check validity >90%.
  2. Train predictors on small Meta-Dataset; verify Spearman correlations >0.7 for accuracy.
  3. Apply Pareto filtering; ensure at least one architecture survives across 3 test datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does POMONAG's many-objective optimization approach scale to larger and more complex search spaces beyond NASBench201 and MobileNetV3?
- Basis in paper: The paper states "Extending POMONAG's approach to other computer vision tasks... is envisaged" and discusses MobileNetV3 as a more complex search space.
- Why unresolved: The current evaluation only covers two relatively small search spaces. The paper acknowledges that scaling to larger, more complex search spaces remains untested.
- What evidence would resolve it: Empirical results showing POMONAG's performance and efficiency metrics on significantly larger search spaces (e.g., NAS-Bench-1Shot1 with ~10^10 architectures or more complex architectures like ResNet variants) would demonstrate scalability.

### Open Question 2
- Question: How sensitive is POMONAG's performance to the choice of scaling factors for the different objectives in the many-objective optimization?
- Basis in paper: The paper discusses dynamic optimization of scaling factors using Optuna and presents specific optimal values for different search spaces, but acknowledges this requires dataset-specific tuning.
- Why unresolved: While the paper shows optimal scaling factors for specific datasets and search spaces, it doesn't explore how sensitive the results are to deviations from these values or whether these optimal values transfer across different domains.
- What evidence would resolve it: Systematic sensitivity analysis showing POMONAG's performance across a range of scaling factor values, and testing whether scaling factors optimized on one dataset transfer to others, would clarify the robustness of the approach.

### Open Question 3
- Question: How does POMONAG's performance compare to state-of-the-art single-objective NAS methods when evaluated on pure accuracy metrics?
- Basis in paper: The paper focuses on multi-objective optimization and compares against multi-objective and Transferable NAS methods, but doesn't directly compare against leading single-objective NAS methods that optimize only for accuracy.
- Why unresolved: The paper demonstrates POMONAG's superiority in multi-objective settings but doesn't establish whether the additional complexity of multi-objective optimization is justified purely in terms of accuracy when compared to simpler single-objective approaches.
- What evidence would resolve it: Direct head-to-head comparisons of POMONAG's accuracy against leading single-objective NAS methods (like DARTS variants, ENAS, or recent differentiable architecture search approaches) on the same datasets would establish whether the multi-objective approach provides accuracy benefits beyond the efficiency and resource optimization objectives.

## Limitations

- Performance Predictor effectiveness depends on Meta-Dataset diversity and size, with unknown generalization to architectures far from the training distribution.
- The method requires extensive hyperparameter optimization via Optuna, with optimal scaling factors being dataset-specific and not easily transferable.
- Scaling to larger search spaces remains untested, with computational costs potentially increasing exponentially with search space complexity.

## Confidence

- High confidence: The diffusion-based architecture generation framework is well-established through DiffusionNAG; the extension to many objectives follows logically.
- Medium confidence: Performance Predictor improvements (Meta-Dataset expansion, encoding refinements, ViT-B-16) are supported by reported correlations but lack comparative ablation studies.
- Low confidence: The claim of state-of-the-art performance relative to existing NAS methods is not benchmarked against specific baselines in the provided evidence.

## Next Checks

1. **Objective Correlation Analysis**: Compute pairwise Spearman correlations between accuracy, parameters, MACs, and latency across generated architectures to verify that Pareto filtering identifies meaningful trade-offs rather than trivial selections.

2. **Scaling Factor Sensitivity**: Systematically vary scaling factors k_ϕ, k_π, k_μ, k_λ around the Optuna-optimized values and measure impact on final architecture quality and Pareto Front diversity.

3. **Meta-Dataset Diversity Impact**: Compare Performance Predictor accuracy when trained on Meta-Datasets of different sizes (4,630 vs 10,000 vs 20,000) across all 15 datasets to quantify generalization benefits.