---
ver: rpa2
title: Exploring Straightforward Conversational Red-Teaming
arxiv_id: '2409.04822'
source_url: https://arxiv.org/abs/2409.04822
tags:
- attacker
- target
- arxiv
- harmful
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the effectiveness of using off-the-shelf large
  language models (LLMs) as red teamers to elicit harmful responses from target LLMs
  in both single-turn and multi-turn conversational settings. The study compares various
  attack tactics, including baseline, adaptive, insist, objective disclosing start
  (ODS), objective concealing start (OCS), and multi-attempt objective concealing
  start (MA-OCS).
---

# Exploring Straightforward Conversational Red-Teaming

## Quick Facts
- arXiv ID: 2409.04822
- Source URL: https://arxiv.org/abs/2409.04822
- Authors: George Kour; Naama Zwerdling; Marcel Zalmanovici; Ateret Anaby-Tavor; Ora Nova Fandina; Eitan Farchi
- Reference count: 25
- One-line primary result: Conversational tactics that conceal malicious intent are more effective at eliciting harmful responses from LLMs than single-turn attacks

## Executive Summary
This paper explores using off-the-shelf large language models as red teamers to elicit harmful responses from target LLMs in conversational settings. The study compares various attack tactics including baseline, adaptive, insist, and several objective-concealing strategies across both single-turn and multi-turn conversations. Results show that conversational tactics, particularly those that conceal malicious intent, are significantly more effective than direct single-turn attacks. The study also reveals a correlation between a model's susceptibility to being attacked and its effectiveness as an attacker.

## Method Summary
The researchers used the AttaQ dataset containing 100 adversarial questions across safety domains and tested four LLMs (Llama2-70b, Llama2-13b, Mixtral-8x7b-instruct-v0.1, GPT-3.5-Turbo) as both attackers and targets. Six attack tactics were implemented: Baseline, Adaptive, Insist, Objective Disclosing Start (ODS), Objective Concealing Start (OCS), and Multi-Attempt Objective Concealing Start (MA-OCS). Each conversation consisted of 5 turns, and harmfulness was evaluated using GPT-3.5-Turbo as a judge on a 1-5 scale. The study focused on last-turn harmfulness scores and compared effectiveness across tactics and model combinations.

## Key Results
- Conversational tactics that conceal malicious intent (OCS, MA-OCS) were significantly more effective than those disclosing objective at the start (ODS, Insist) for most models
- The MA-OCS tactic with look-ahead mechanism proved most effective across all tested models
- A model's susceptibility to being attacked correlates with its effectiveness as an attacker, with Mixtral8X7b emerging as the most potent attacker
- Single-turn adaptive tactics outperformed baseline attacks, showing LLMs can learn from previous attempts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conversational tactics that conceal the malicious intent (OCS and MA-OCS) are more effective than those that disclose the objective at the start (ODS).
- Mechanism: By concealing the objective, the attacker can build trust and context through small talk and seemingly benign interactions, making the target model more likely to cooperate when the harmful request is eventually made.
- Core assumption: The target LLM's alignment mechanisms are more easily bypassed when the harmful request is embedded in a longer, more natural conversation rather than presented directly.
- Evidence anchors:
  - [abstract] "Results show that conversational tactics, especially those that conceal the malicious intent, are more effective than single-turn attacks."
  - [section] "attack tactics that start by concealing their objective (OCS, MA-OCS) were significantly more effective than those disclosing their objective at the beginning (ODS, Insist) for most models"
  - [corpus] Weak - the corpus neighbors discuss red teaming broadly but don't specifically address intent concealment tactics.

### Mechanism 2
- Claim: A model's susceptibility to being attacked correlates with its effectiveness as an attacker.
- Mechanism: Models with lower alignment (less constrained by safety training) are both more vulnerable to attacks and more capable of generating harmful outputs when attacking others.
- Core assumption: The same factors that make a model less aligned (weaker safety constraints, less fine-tuning) apply both to being attacked and to successfully attacking others.
- Evidence anchors:
  - [abstract] "The study also finds that a model's susceptibility to being attacked correlates with its effectiveness as an attacker."
  - [section] "Mixtral8X7b model's training lacks extensive alignment, making it less secure than other models. As a result, it emerges as the most potent attacker against all target LLMs"
  - [corpus] Missing - no direct evidence in corpus about correlation between attack susceptibility and attacker effectiveness.

### Mechanism 3
- Claim: Allowing the attacker model to access previous attempts and target responses improves attack effectiveness.
- Mechanism: The attacker can learn from failed attempts and refine its approach based on the target model's patterns of refusal or engagement, similar to iterative prompt refinement.
- Core assumption: The attacker model can extract useful patterns from previous interactions to improve subsequent attacks.
- Evidence anchors:
  - [abstract] "The study compares various attack tactics, including... adaptive, insist, objective disclosing start (ODS), objective concealing start (OCS), and multi-attempt objective concealing start (MA-OCS)."
  - [section] "The results demonstrate that LLMs can learn from previous attempts, as evidenced by the Adaptive (single-turn) tactic achieving better attack outcomes than Base"
  - [corpus] Weak - the corpus discusses red teaming but doesn't specifically address learning from previous attempts.

## Foundational Learning

- Concept: Zero-shot prompting and instruction following
  - Why needed here: The attacker LLMs are used without fine-tuning, relying entirely on their ability to follow instructions in the prompt to perform red teaming
  - Quick check question: Can the attacker model understand and execute complex multi-step instructions without examples?

- Concept: Conversational context modeling
  - Why needed here: The attack tactics rely on maintaining and leveraging conversation history across multiple turns
  - Quick check question: Does the model properly maintain context across multiple dialogue turns when given the conversation history?

- Concept: Harmfulness evaluation and scoring
  - Why needed here: The MA-OCS tactic uses a harmfulness scorer to evaluate which attack attempt is most effective
  - Quick check question: Can the judge model reliably score harmfulness on a 1-5 scale while considering the full conversation context?

## Architecture Onboarding

- Component map:
  Attacker LLM -> Target LLM -> Judge LLM -> Attack tactic selector -> Conversation manager

- Critical path:
  1. Attacker generates prompt based on tactic and conversation history
  2. Target model processes prompt and generates response
  3. Judge evaluates response harmfulness
  4. Results are recorded and next turn is generated

- Design tradeoffs:
  - Single-turn vs multi-turn: Multi-turn allows for more sophisticated attacks but increases complexity and cost
  - Disclosing vs concealing intent: Disclosing is simpler but less effective; concealing requires more sophisticated conversation management
  - Fixed vs adaptive attacks: Adaptive attacks can learn but require more computational resources

- Failure signatures:
  - Target model consistently refuses regardless of tactic (high alignment)
  - Judge model fails to distinguish harmful from benign responses
  - Attacker model generates irrelevant or off-topic responses
  - Conversation manager loses context between turns

- First 3 experiments:
  1. Baseline comparison: Run all tactics against a single target model and verify the relative effectiveness ordering matches paper results
  2. Single model attack: Test the same model as both attacker and target to verify correlation between attack susceptibility and effectiveness
  3. Turn effectiveness: Measure harmfulness scores per turn to identify optimal conversation length before diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of red-teaming tactics vary when applied to multilingual models or when the objectives are translated into different languages?
- Basis in paper: [inferred]
- Why unresolved: The paper only evaluates attacks in English using English-language objectives, without exploring multilingual contexts or the adaptability of tactics across languages.
- What evidence would resolve it: Conducting experiments with multilingual models and objectives in various languages, comparing the success rates of the same tactics.

### Open Question 2
- Question: Can the red-teaming framework be extended to assess the helpfulness of model responses alongside their harmfulness, providing a more balanced evaluation of model safety?
- Basis in paper: [explicit]
- Why unresolved: The paper explicitly states that it does not evaluate the helpfulness of the model, focusing solely on harmfulness, which may lead to incomplete assessments of model behavior.
- What evidence would resolve it: Developing and implementing a dual evaluation metric that assesses both harmfulness and helpfulness, then applying this to the same set of attacks to compare results.

### Open Question 3
- Question: What is the impact of using different directive styles or additional few-shot examples on the effectiveness of the attacker models in eliciting harmful responses?
- Basis in paper: [explicit]
- Why unresolved: The paper uses a uniform directive across all models, which may limit the attacker's potential effectiveness, as indicated by the authors' suggestion for future work to enhance attacks with varied directives.
- What evidence would resolve it: Testing the same red-teaming tactics with varied directives, including few-shot examples, and comparing the harmfulness scores to those obtained with the uniform directive.

## Limitations

- Experiments were conducted on a specific dataset (AttaQ) and limited set of models, which may not represent the full diversity of commercial and open-source models
- Use of LLM-as-a-judge for harmfulness evaluation introduces potential bias and may not accurately reflect real-world harm potential
- The study focuses on achieving highest harmfulness score rather than measuring practical risk in real-world deployment scenarios

## Confidence

**High Confidence**: The finding that conversational tactics (especially OCS and MA-OCS) are more effective than single-turn attacks is well-supported by the experimental results across all tested models.

**Medium Confidence**: The correlation between attack susceptibility and attacker effectiveness is observed in the tested models but requires further validation across a broader range of architectures and training approaches.

**Low Confidence**: The MA-OCS tactic's look-ahead mechanism effectiveness depends heavily on the judge model's accuracy and consistency.

## Next Checks

1. **Cross-model validation**: Test the same attack tactics against a broader range of models including different architectures, training approaches, and sizes to verify the robustness of the effectiveness hierarchy and the attack-susceptibility correlation.

2. **Judge model ablation**: Compare harmfulness scores across multiple judge models and human evaluation for a subset of conversations to quantify the variance introduced by LLM-as-a-judge and establish confidence intervals for the reported scores.

3. **Real-world deployment simulation**: Implement a controlled experiment where the red-teamed conversations are evaluated by human safety experts to determine the practical risk level and identify any gaps between automated scoring and real-world harm potential.