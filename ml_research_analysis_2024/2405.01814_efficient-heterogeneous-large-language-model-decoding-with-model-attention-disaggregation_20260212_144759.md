---
ver: rpa2
title: Efficient Heterogeneous Large Language Model Decoding with Model-Attention
  Disaggregation
arxiv_id: '2405.01814'
source_url: https://arxiv.org/abs/2405.01814
tags:
- attention
- memory
- batch
- operators
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of GPU utilization during
  the decoding phase of large language model (LLM) inference. The authors observe
  that attention operators are memory-bound while other operators are computation-bound,
  leading to underutilization when using homogeneous hardware.
---

# Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation

## Quick Facts
- arXiv ID: 2405.01814
- Source URL: https://arxiv.org/abs/2405.01814
- Reference count: 40
- Primary result: 16.1-90.1% higher throughput with acceptable latency for interactive services

## Executive Summary
This paper addresses the inefficiency of GPU utilization during the decoding phase of large language model (LLM) inference. The authors observe that attention operators are memory-bound while other operators are computation-bound, leading to underutilization when using homogeneous hardware. To address this, they propose model-attention disaggregation, which separates the computation of attention operators onto memory-optimized devices while keeping other operators on computation-optimized accelerators. This heterogeneous approach allows for better resource utilization and cost efficiency. The authors implement a distributed system called Lamina that includes a fully host-bypased network stack and automated model conversion tools. Experimental results show that Lamina achieves significant throughput improvements compared to existing solutions with similar hardware costs, while maintaining latency within acceptable bounds for interactive services.

## Method Summary
The paper proposes model-attention disaggregation, a heterogeneous approach that separates memory-bound attention operators from computation-bound non-attention operators across different types of accelerators. The system uses computation-optimized GPUs for feed-forward networks and attention Q-projection, while memory-optimized GPUs handle attention computation and KV cache storage. A fully host-bypassed network stack (FHBN) enables efficient communication between heterogeneous devices using GPUDirect RDMA and PCIe P2P. The approach includes automated model conversion tools to partition transformer models, staggered pipelining with rotational scheduling to handle prefill-decode transitions, and resource utilization overlapping through head-level and request-level attention parallelism. The system is evaluated on LLaMA models using real-world request traces from Azure and Kimi.

## Key Results
- 16.1-90.1% higher throughput compared to homogeneous GPU solutions with similar hardware costs
- End-to-end latency of 33.0 µs for the fully host-bypassed network stack, representing a 50.5% reduction compared to NCCL
- Maintains latency within acceptable bounds for interactive services while achieving higher throughput
- Optimal degree of parallelism varies across different models and workloads, requiring performance profiling for configuration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-attention disaggregation exploits the differing computational characteristics of attention versus non-attention operators to improve resource utilization.
- Mechanism: By moving memory-bound attention operators to memory-optimized devices and computation-bound non-attention operators to computation-optimized devices, the system can achieve higher batch sizes for non-attention operators without memory constraints and avoid wasting computation resources on memory-bound tasks.
- Core assumption: Attention operators are consistently memory-bound regardless of batch size, while non-attention operators are computation-bound when batch size is sufficiently large.
- Evidence anchors:
  - [abstract] "Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators"
  - [section 2.2.2] "The attention operator, when processing a batch of requests, still performs a batched matrix-vector multiplication, where each query accesses and processes its own KV cache. As a result, the arithmetic intensity of the attention operator remains constant, irrespective of the batch size"
  - [corpus] Weak evidence - only general LLM disaggregation papers found, none specifically about attention disaggregation mechanisms

### Mechanism 2
- Claim: The disaggregated architecture enables flexible resource allocation that can be optimized for specific models and workloads.
- Mechanism: By maintaining separate pools of memory-optimized and computation-optimized devices, the system can adjust the ratio of each type based on the memory and computation demands of different models and workloads, preventing resource wastage from fixed ratios.
- Core assumption: Different LLMs and workloads have varying memory and computation requirements that cannot be optimally served by homogeneous hardware with fixed resource ratios.
- Evidence anchors:
  - [abstract] "Moreover, different LLMs and workloads present varying computation and memory resource requirements. Homogeneous accelerator solutions, however, can only provide a fixed ratio of computation and memory resources"
  - [section 3.1] "By pooling heterogeneous accelerators, we can adjust the number of each kind of accelerators to better match the LLM and workload and hence improve resource utilization"
  - [corpus] Weak evidence - only general disaggregation approaches found, no specific evidence about flexible resource allocation in heterogeneous LLM serving

### Mechanism 3
- Claim: Fully host-bypassed networking reduces communication overhead between heterogeneous devices to enable efficient attention offloading.
- Mechanism: By eliminating CPU involvement in both control and data paths of GPU-aware networking, the system achieves lower latency and higher bandwidth for frequent layer-wise communications between memory-optimized and computation-optimized devices.
- Core assumption: The frequent network communications required for attention offloading would otherwise introduce prohibitive latency due to CPU involvement in conventional networking stacks.
- Evidence anchors:
  - [section 4.1] "This approach not only eliminates the need for CPU involvement during the recv process, but also allows asynchronous launch of the polling kernel and subsequent computation kernels to the GPU stream"
  - [section 4.1] "To enable direct RDMA command submission on GPUs, we have to allow GPUs to directly access the UAR via PCIe P2P"
  - [section 6.3] "FHBN achieves an end-to-end latency of 33.0 µs, representing a 50.5% reduction compared to NCCL's 66.6 µs latency"

## Foundational Learning

- Concept: Roofline model for performance analysis
  - Why needed here: Understanding why GPUs are underutilized in LLM decoding requires analyzing the arithmetic intensity and memory bandwidth characteristics of different operators
  - Quick check question: What is the arithmetic intensity threshold that determines whether an operation is memory-bound or compute-bound on a given hardware platform?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper's core insight about disaggregation is based on the different characteristics of attention operators versus other transformer components
  - Quick check question: How does the arithmetic intensity of attention operators compare to that of feed-forward networks in transformer blocks?

- Concept: RDMA and GPU-aware networking
  - Why needed here: The disaggregated architecture requires efficient communication between heterogeneous devices, making understanding of networking optimizations crucial
  - Quick check question: What are the key differences between conventional GPU-aware networking and fully host-bypassed networking in terms of latency and bandwidth?

## Architecture Onboarding

- Component map:
  - Model workers: Computation-optimized GPUs handling non-attention operators
  - Attention workers: Memory-optimized GPUs handling attention operators and KV cache storage
  - NICs: Network interface cards for inter-device communication
  - Global scheduler: Coordinates request distribution and manages prefill-decode transitions
  - Request manager: Handles incoming requests and batch formation
  - Local scheduler: Manages task scheduling within each device pool

- Critical path:
  1. Request arrival and batch formation
  2. Q-projection computation on model workers
  3. Q transmission to attention workers
  4. KV-projection and attention computation on attention workers
  5. Attention output transmission back to model workers
  6. Feed-forward network computation on model workers
  7. Token generation and response formation

- Design tradeoffs:
  - Hardware cost vs. performance: Using cheaper memory-optimized devices improves cost efficiency but may increase communication overhead
  - Batch size vs. latency: Larger batch sizes improve throughput but increase token generation latency
  - Model partitioning granularity: More slices can improve resource utilization overlapping but increase scheduling complexity

- Failure signatures:
  - High network latency or packet loss: Indicates networking stack issues or insufficient bandwidth
  - GPU memory overflow: Suggests incorrect batch size calculations or KV cache management problems
  - CPU starvation: Points to insufficient host resources for scheduling and coordination tasks
  - Uneven device utilization: May indicate load balancing issues or incorrect hardware configuration

- First 3 experiments:
  1. Microbenchmark network stack: Test FHBN performance against NCCL using ping-pong tests between heterogeneous devices to verify latency and bandwidth improvements
  2. Single-layer disaggregation: Implement and measure attention offloading for a single transformer layer to validate the disaggregated execution model and identify bottlenecks
  3. Workload profiling: Run various request traces with different models to determine optimal DOP configurations and measure the impact on throughput and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal ratio between model workers and attention workers vary across different LLM architectures and workloads?
- Basis in paper: [explicit] The paper states "this indicates that the optimal ratio between model and attention workers varies for different models and workloads" and suggests conducting performance profiling to select the best hardware configuration.
- Why unresolved: The paper provides experimental results showing variation but does not derive a generalizable formula or methodology for predicting optimal ratios across different scenarios.
- What evidence would resolve it: Systematic experiments across a wide range of LLM architectures, context lengths, and request patterns showing how optimal ratios scale with specific workload characteristics.

### Open Question 2
- Question: What are the fundamental limitations of current networking technologies in supporting layer-wise communication for model-attention disaggregation, and how close are we to these limits?
- Basis in paper: [explicit] The paper discusses that "communication between heterogeneous GPUs must rely on data center networks (DCNs), such as Ethernet and InfiniBand, which provide only ~10% of the bandwidth of inter-chip interconnects (ICIs) like NVLink between homogeneous GPUs."
- Why unresolved: The paper establishes that current networking is sufficient but doesn't explore theoretical limits or what would happen if models continue to grow in size and complexity.
- What evidence would resolve it: Analysis of networking bottlenecks as model sizes increase, exploring what happens when communication demands exceed current DCN capabilities and what technological advances would be needed.

### Open Question 3
- Question: How do alternative heterogeneous devices like Processing-in-Memory (PIM) compare to GPU-based solutions for attention computation in terms of cost, performance, and scalability?
- Basis in paper: [explicit] The paper mentions that "we anticipate that Processing-in-Memory (PIM) devices will be a more suitable candidate for memory-optimized devices as they demonstrate even greater cost advantages alongside their larger capacity and higher bandwidth."
- Why unresolved: The paper speculates about PIM benefits but doesn't provide experimental comparisons or quantify the potential improvements over current GPU-based approaches.
- What evidence would resolve it: Direct performance and cost comparisons between GPU-based attention computation and PIM-based approaches across various model sizes and workloads, including scalability analysis.

## Limitations

- Hardware-specific results may not generalize to other platforms or GPU generations with different memory hierarchies
- Limited model coverage doesn't explore scalability to smaller models or models with alternative attention mechanisms
- Network infrastructure dependency on specific PCIe P2P and NIC configurations may not be replicable in all environments

## Confidence

- **High Confidence**: The core insight about attention operators being memory-bound while non-attention operators are compute-bound is well-supported by theoretical analysis and experimental evidence
- **Medium Confidence**: Specific performance improvements may be over-optimistic when generalized beyond tested hardware configurations and models
- **Low Confidence**: Scalability analysis to thousands of GPUs and robustness under production-scale conditions are not thoroughly validated

## Next Checks

1. **Cross-platform validation**: Implement and benchmark the disaggregated architecture on alternative GPU platforms (AMD MI300X, AWS Trainium2) and with different networking technologies (InfiniBand, Ethernet with GPUDirect) to verify that performance gains are not specific to the H100/H20 combination.

2. **Model architecture sensitivity**: Test the approach with models using different attention mechanisms (multi-query attention, gated attention, local attention) and with varying numbers of attention heads to determine whether disaggregation benefits persist across architectural variations.

3. **Production workload simulation**: Deploy the system in a production-like environment with mixed request patterns, varying batch sizes, and concurrent model serving to measure real-world performance degradation factors such as cache thrashing, scheduling overhead, and fault tolerance under load.