---
ver: rpa2
title: Temporal Fairness in Decision Making Problems
arxiv_id: '2408.13208'
source_url: https://arxiv.org/abs/2408.13208
tags:
- fairness
- solutions
- historical
- hfop
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces temporal fairness into decision-making optimization\
  \ problems by considering the fairness of solutions across both historical and future\
  \ time steps. The authors propose four formulations\u2014FOP, HFOP, DHFOP, and MSDHFOP\u2014\
  that incorporate fairness metrics accounting for historical unfairness and future\
  \ predictions."
---

# Temporal Fairness in Decision Making Problems

## Quick Facts
- arXiv ID: 2408.13208
- Source URL: https://arxiv.org/abs/2408.13208
- Authors: Manuel R. Torres; Parisa Zehtabi; Michael Cashmore; Daniele Magazzeni; Manuela Veloso
- Reference count: 25
- Primary result: Temporal fairness formulations (FOP, HFOP, DHFOP, MSDHFOP) produce more balanced solutions over time compared to traditional approaches

## Executive Summary
This paper introduces temporal fairness into decision-making optimization problems by considering fairness across both historical and future time steps. The authors propose four formulations that incorporate fairness metrics accounting for historical unfairness and future predictions. Experiments across four domains (course assignment, vehicle routing, task allocation, and nurse scheduling) demonstrate that considering temporal fairness leads to more balanced solutions over time compared to traditional approaches that only optimize for immediate fairness.

## Method Summary
The paper extends standard optimization problems to include fairness considerations by introducing four temporal fairness formulations. FOP incorporates fairness into single-step optimization, HFOP adds historical fairness metrics that accumulate unfairness over time, DHFOP applies discount factors to weight historical solutions differently, and MSDHFOP incorporates future forecasts through planning horizons. The approach uses quality metrics Q for optimization objectives and fairness metrics F (relative max-min, quadratic max-min gap, discounted historical variants) to ensure balanced resource allocation across time steps.

## Key Results
- Temporal fairness formulations compensate historical unfairness faster than single-shot approaches
- MSDHFOP formulation shows advantages in planning with future constraints by adjusting decisions based on anticipated scenarios
- Experiments across four domains demonstrate more balanced solutions over time compared to traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Temporal fairness formulation compensates historical unfairness faster than single-shot fairness approaches.
- **Mechanism:** HFOP and DHFOP formulations incorporate historical fairness metrics (FH and FH,γ) that consider past allocations, accumulating unfairness over time and penalizing solutions that perpetuate historical imbalances.
- **Core assumption:** Historical fairness metric accurately captures cumulative unfairness and the optimization problem can navigate the quality-fairness tradeoff controlled by parameter β.
- **Evidence anchors:** Abstract mentions more balanced solutions over time; section 2.3 discusses compensation of historical imbalances.
- **Break condition:** If discount factor γ is set too low, recent solutions dominate too heavily, potentially creating new unfairness. If too high, historical unfairness persists indefinitely.

### Mechanism 2
- **Claim:** Multi-step planning with MSDHFOP enables solutions that appear unfair in short-term but achieve long-term fairness.
- **Mechanism:** MSDHFOP formulation incorporates future forecasts through discount factor τ and planning horizon TF, allowing the optimizer to accept temporarily unfair solutions that enable fairer outcomes in constrained future scenarios.
- **Core assumption:** Future constraints and scenarios can be accurately predicted and incorporated into the optimization model.
- **Evidence anchors:** Abstract mentions advantages in planning with future constraints; section 2.4 discusses the "forgetting rate phenomenon."
- **Break condition:** If future predictions are inaccurate or planning horizon TF is too short, temporary unfairness may not lead to improved long-term outcomes.

### Mechanism 3
- **Claim:** Discount factors γ and τ allow flexible control over the importance of historical vs. future fairness.
- **Mechanism:** DHFOP and MSDHFOP use discount factors to weight past and future solutions differently, creating a sliding scale where users can emphasize recent events (lower γ) or maintain long-term fairness goals (higher γ).
- **Core assumption:** Discount factors appropriately model the "recency effect" and uncertainty in planning.
- **Evidence anchors:** Section 2.3 mentions attributing more importance to recent events; section 2.4 discusses τ modeling uncertainty in future planning.
- **Break condition:** If discount factors are mis-calibrated, the system may overemphasize either historical unfairness (γ too low) or future predictions (τ too high), leading to suboptimal solutions.

## Foundational Learning

- **Concept:** Optimization problem formulation with quality and fairness metrics
  - Why needed here: The entire temporal fairness framework builds on extending standard optimization problems to include fairness considerations
  - Quick check question: How does adding a fairness metric F(x) to an optimization problem change the solution space compared to optimizing only for quality Q(x)?

- **Concept:** Historical data accumulation and fairness metrics
  - Why needed here: Temporal fairness requires tracking fairness across multiple time steps to identify and compensate historical imbalances
  - Quick check question: Given a history of solutions where agent A received 10 tasks and agent B received 6 tasks, how would you define a fairness metric that measures this historical imbalance?

- **Concept:** Multi-step planning and discount factors
  - Why needed here: MSDHFOP requires reasoning about future constraints and scenarios, with discount factors controlling the weight of future predictions
  - Quick check question: If τ = 0.9 in a 3-step planning horizon, what is the relative weight given to the immediate next step versus the step after next?

## Architecture Onboarding

- **Component map:** Problem definition -> Historical tracking -> Discount factor calculation -> Optimization engine -> Solution output

- **Critical path:**
  1. Initialize problem with quality metric, constraints, and fairness metric
  2. Load historical solution data (if available)
  3. Calculate historical fairness metric with discount factor γ
  4. Run optimization to find solution balancing Q and FH,γ
  5. Store solution for future historical tracking

- **Design tradeoffs:**
  - Computational cost vs. fairness accuracy: More complex historical metrics increase computation time
  - Discount factor selection: Lower γ enables faster compensation but may create new unfairness
  - Planning horizon length: Longer horizons enable better long-term fairness but increase complexity

- **Failure signatures:**
  - Oscillation between unfair solutions: Discount factors too aggressive
  - Slow fairness convergence: Discount factors too conservative or β too low
  - Suboptimal quality: β too high, overemphasizing fairness
  - Numerical instability: Poor scaling between Q and F metrics

- **First 3 experiments:**
  1. Implement FOP baseline with simple fairness metric on course assignment problem
  2. Add historical tracking to create HFOP and compare solution trajectories
  3. Introduce discount factor γ in DHFOP and experiment with different values on the same problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fairness metrics interact with temporal fairness formulations, and which metrics are most appropriate for specific domains?
- Basis in paper: The paper mentions using different fairness metrics across domains but doesn't systematically compare their performance or provide guidance on metric selection.
- Why unresolved: The experiments use various fairness metrics but don't analyze their relative effectiveness or provide theoretical justification for choosing one metric over another.
- What evidence would resolve it: A comprehensive study comparing different fairness metrics (max-min, proportional equality, quadratic gap) across domains with sensitivity analysis on how metric choice affects temporal fairness outcomes.

### Open Question 2
- Question: What is the optimal way to set the discount factors γ and τ in the DHFOP and MSDHFOP formulations?
- Basis in paper: The paper mentions that "smaller values of γ lead to a faster compensation of historical fairness" but doesn't provide systematic methods for setting these parameters.
- Why unresolved: The experimental section uses arbitrary values (γ = 0.65, τ = 0.75) without justification or optimization framework, and the paper acknowledges these as "hyper-parameters" requiring control.
- What evidence would resolve it: Development of principled methods for setting discount factors based on domain characteristics, possibly through automated parameter tuning or theoretical bounds on fairness compensation rates.

### Open Question 3
- Question: How does the MSDHFOP formulation scale with increasing planning horizons and what are the computational trade-offs between longer-term planning and solution quality?
- Basis in paper: The paper introduces MSDHFOP as a multi-step planning approach but only provides limited experimental results with T = 4 semesters in the course assignment domain.
- Why unresolved: The paper doesn't explore the computational complexity of MSDHFOP as the planning horizon increases, nor does it provide guidelines on choosing appropriate planning horizons for different domains.
- What evidence would resolve it: Systematic experiments varying planning horizons across multiple domains with analysis of computational complexity, solution quality improvements, and diminishing returns as horizon length increases.

## Limitations
- Empirical validation scope is limited, lacking systematic comparison of discount factor calibration across domains
- Computational overhead analysis for different temporal formulations is not provided
- Robustness testing with noisy or incomplete historical data is missing
- Cross-domain generalizability beyond the four selected problems is not explored

## Confidence
- **High**: The theoretical formulation of temporal fairness metrics and their mathematical correctness
- **Medium**: The empirical demonstration that temporal formulations produce more balanced solutions over time
- **Low**: The specific parameter recommendations for discount factors and their domain-specific effectiveness

## Next Checks
1. **Discount Factor Sensitivity Analysis**: Systematically vary γ and τ across their plausible ranges and measure solution stability and fairness convergence rates
2. **Cross-Domain Generalization**: Apply the temporal fairness framework to a fifth domain (e.g., ride-sharing allocation) and compare performance to existing approaches
3. **Historical Data Quality Impact**: Introduce controlled noise in historical solutions and measure how each formulation's fairness outcomes degrade