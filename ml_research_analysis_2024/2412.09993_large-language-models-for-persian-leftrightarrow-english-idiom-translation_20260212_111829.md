---
ver: rpa2
title: Large Language Models for Persian $ \leftrightarrow $ English Idiom Translation
arxiv_id: '2412.09993'
source_url: https://arxiv.org/abs/2412.09993
tags:
- translation
- idiom
- persian
- idioms
- fluency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two parallel datasets for Persian\u2192\
  English and English\u2192Persian idiom translation, with Persian idioms sourced\
  \ from the newly created PersianIdioms resource. The study evaluates multiple large\
  \ language models (LLMs), neural machine translation (NMT) models, and their combinations\
  \ for idiom translation quality, focusing on idiom translation accuracy and fluency."
---

# Large Language Models for Persian $ \leftrightarrow $ English Idiom Translation

## Quick Facts
- arXiv ID: 2412.09993
- Source URL: https://arxiv.org/abs/2412.09993
- Authors: Sara Rezaeimanesh; Faezeh Hosseini; Yadollah Yaghoobzadeh
- Reference count: 8
- Primary result: Claude-3.5-Sonnet outperforms other models for Persian↔English idiom translation, with hybrid approaches improving weaker LLMs

## Executive Summary
This paper introduces two parallel datasets for Persian→English and English→Persian idiom translation, with Persian idioms sourced from the newly created PersianIdioms resource. The study evaluates multiple large language models (LLMs), neural machine translation (NMT) models, and their combinations for idiom translation quality, focusing on idiom translation accuracy and fluency. Results show that Claude-3.5-Sonnet performs best in both translation directions. For English→Persian, combining weaker LLMs with Google Translate improves performance, while Persian→English benefits from single prompts for simpler models and complex prompts for advanced ones. Automatic evaluation methods like GPT-4o as a judge, BLEU, and BERTScore are effective for comparing model performance.

## Method Summary
The study evaluates various LLMs (GPT-3.5, GPT-4o-mini, Claude-3.5-Sonnet, Qwen-2.5-72B, Command R+), NMT models (NLLB-200-3.3b, MADLAD-400-10b, Google Translate), and their combinations using different prompting strategies (single, chain-of-thought, multi-step) for idiom translation between Persian and English. The Persian idioms were sourced from the PersianIdioms resource containing 2,200 idioms with meanings and examples. The evaluation used both manual assessment (idiom translation accuracy and fluency) and automatic metrics (BLEU, BERTScore, COMET, GPT-4o as judge).

## Key Results
- Claude-3.5-Sonnet delivers the best performance in both translation directions
- Combining weaker LLMs with Google Translate improves performance for English→Persian translation
- Single prompts work better for simpler models while complex prompts benefit advanced models
- GPT-4o as a judge shows the highest correlation with manual idiom translation scores
- Models generally translate English idioms more effectively than Persian ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining weaker LLMs with Google Translate improves performance when the LLM's fluency is lower than that of the NMT model
- Mechanism: The hybrid approach leverages the fluency strengths of Google Translate to compensate for weaker LLMs' idiomatic translation weaknesses, especially in En→Fa direction
- Core assumption: The NMT model (Google Translate) produces more fluent but less idiomatic translations than the weaker LLM, and combining them preserves fluency while improving idiom handling
- Evidence anchors: [section] "When combined with Google Translate, these models show an increased BLEU score, benefiting from the strengths of both LLMs and NMT models"; [section] "This suggests that when an LLM's fluency is lower than that of an NMT model, combining the two can effectively close the performance gap with stronger LLMs"
- Break condition: If the LLM's fluency is already higher than Google Translate's, or if the LLM can handle idioms well independently, the hybrid approach may degrade performance

### Mechanism 2
- Claim: Claude-3.5-Sonnet delivers outstanding results in both translation directions due to its superior idiom detection and contextual understanding
- Mechanism: Advanced LLMs like Claude-3.5-Sonnet have better natural language understanding capabilities that allow them to accurately identify idioms and find appropriate target language replacements
- Core assumption: The model's architecture and training data include sufficient idiomatic expressions in both languages to enable effective cross-linguistic idiom translation
- Evidence anchors: [abstract] "Our experiments reveal that Claude-3.5-Sonnet delivers outstanding results in both translation directions"; [section] "Claude-3.5 Sonnet frequently selects appropriate English idioms as replacements, demonstrating a strong understanding of both Persian and English idioms"
- Break condition: If the source language contains idioms not well-represented in the model's training data, or if cultural context is too distant, performance may degrade

### Mechanism 3
- Claim: Automatic evaluation methods like GPT-4o as a judge, BLEU, and BERTScore are effective for comparing different aspects of model performance
- Mechanism: These metrics provide quantifiable measures that correlate well with manual evaluation scores for both fluency and idiom translation accuracy
- Core assumption: The automatic metrics capture the essential aspects of translation quality that humans evaluate, and their correlations with manual scores are stable across different models and prompts
- Evidence anchors: [abstract] "We also find that automatic evaluation methods like LLM-as-a-judge, BLEU, and BERTScore are effective for comparing different aspects of model performance"; [section] "Table 4 shows the correlations between manual and automatic evaluation scores... GPT-4o scores show the highest correlation with idiom translation in both directions"
- Break condition: If the evaluation task changes significantly (e.g., different languages, different types of figurative language), the correlation patterns may break down

## Foundational Learning

- Concept: Idiomatic expressions and their challenges in machine translation
  - Why needed here: The entire paper focuses on idiom translation between Persian and English, requiring understanding of what idioms are and why they're difficult for MT systems
  - Quick check question: What makes idioms particularly challenging for machine translation compared to literal language?

- Concept: Large Language Models vs Neural Machine Translation models
  - Why needed here: The paper compares LLMs (GPT-3.5, GPT-4o-mini, Claude-3.5-Sonnet) with NMT models (NLLB-200, MADLAD-400, Google Translate) for idiom translation
  - Quick check question: What are the key architectural differences between LLMs and traditional NMT models that might affect their idiom translation capabilities?

- Concept: Prompt engineering and chain-of-thought prompting
  - Why needed here: The paper evaluates different prompting methods (single prompt, CoT prompt, multi-prompt) for controlling LLM behavior in idiom translation
  - Quick check question: How does breaking down a complex translation task into multiple prompts affect an LLM's ability to handle idiomatic expressions?

## Architecture Onboarding

- Component map: PersianIdioms resource (2,200 idioms) -> Parallel datasets (200 sentences each direction) -> Translation models (LLMs: GPT-3.5, GPT-4o-mini, Claude-3.5-Sonnet; NMT models: NLLB-200, MADLAD-400, Google Translate) -> Evaluation metrics (manual: idiom translation accuracy and fluency; automatic: GPT-4o, BLEU, BERTScore) -> Performance analysis
- Critical path: Data collection → Model selection and prompting → Translation generation → Manual evaluation → Automatic evaluation correlation → Performance analysis
- Design tradeoffs: Using multiple evaluation metrics provides comprehensive assessment but increases complexity; combining models can improve performance but may introduce inconsistencies; focusing on Persian-English limits generalizability but allows deeper investigation
- Failure signatures: Poor BLEU scores despite good idiom translation (indicating paraphrasing issues); low GPT-4o scores despite high fluency (suggesting idiom detection problems); inconsistent results across different prompting methods (indicating model sensitivity to instruction format)
- First 3 experiments:
  1. Test each LLM with single prompt on a small subset of the dataset to establish baseline performance
  2. Test the same LLMs with CoT and multi-prompt approaches to evaluate the impact of prompting complexity
  3. Create hybrid models by combining weaker LLMs with Google Translate to verify the improvement mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM prompting strategies affect the quality of idiom translation across multiple languages, and can these findings be generalized to languages beyond Persian and English?
- Basis in paper: [explicit] The paper explores different prompting strategies for Persian-English idiom translation, but its findings are limited to these two languages
- Why unresolved: The study's scope is restricted to Persian and English, and the performance of prompting strategies may vary across languages with different linguistic structures
- What evidence would resolve it: Conducting similar experiments across a diverse set of languages, especially those with varying linguistic complexities, would provide insights into the generalizability of the findings

### Open Question 2
- Question: What are the underlying factors contributing to the superior performance of Claude-3.5-Sonnet in idiom translation compared to other LLMs and NMT models?
- Basis in paper: [explicit] The paper identifies Claude-3.5-Sonnet as the best-performing model but does not delve into the reasons for its superior performance
- Why unresolved: The paper focuses on performance outcomes without investigating the model's internal mechanisms or training data that might contribute to its effectiveness
- What evidence would resolve it: Analyzing the model's architecture, training data, and evaluation of its responses to various idiomatic expressions could reveal the factors behind its success

### Open Question 3
- Question: How does the combination of LLMs with NMT models impact the translation of idiomatic expressions in terms of both accuracy and fluency, and are there specific combinations that consistently outperform others?
- Basis in paper: [explicit] The study evaluates the combination of LLMs and NMT models, noting improvements in some cases, but does not provide a comprehensive analysis of the impact across different combinations
- Why unresolved: The paper's evaluation of hybrid models is limited in scope, and it does not explore all possible combinations or their consistent performance across various contexts
- What evidence would resolve it: Systematic testing of all possible LLM-NMT combinations and their performance metrics across a broader range of idiomatic expressions would clarify their impact

## Limitations
- The study uses a relatively small dataset of 200 sentences per translation direction, limiting generalizability
- Weak corpus support (average neighbor FMR=0.506) suggests limited external validation of the proposed methods
- The evaluation relies heavily on automatic metrics whose correlation with human judgment may not capture all nuances of idiom translation quality

## Confidence
- **High Confidence**: Claude-3.5-Sonnet's superior performance in both translation directions, and the general effectiveness of automatic evaluation methods (GPT-4o, BLEU, BERTScore) for comparing model performance
- **Medium Confidence**: The hybrid approach combining weaker LLMs with Google Translate showing improved performance, as this relies on limited examples and may not generalize to all weaker model combinations
- **Low Confidence**: The specific claim that models translate English idioms more effectively than Persian ones, as this requires deeper linguistic analysis beyond the current evaluation framework

## Next Checks
1. Test the proposed models and evaluation methods on a larger dataset (500+ sentences per direction) to verify the robustness of observed performance patterns and correlations
2. Apply the same evaluation framework to a different language pair (e.g., Spanish-English) to assess whether the observed patterns hold across language families
3. Conduct a focused study on idioms with strong cultural specificity to determine whether current models can handle cross-cultural idiom translation or if they exhibit systematic failures with culturally-bound expressions