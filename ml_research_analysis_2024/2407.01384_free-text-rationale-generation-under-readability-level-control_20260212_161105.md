---
ver: rpa2
title: Free-text Rationale Generation under Readability Level Control
arxiv_id: '2407.01384'
source_url: https://arxiv.org/abs/2407.01384
tags:
- readability
- text
- rationales
- offensive
- people
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how readability level instructions influence
  the generation of free-text rationales by large language models (LLMs). We prompt
  LLMs with readability levels ranging from sixth grade to college and evaluate the
  generated rationales across fact-checking, hate speech detection, and natural language
  inference tasks.
---

# Free-text Rationale Generation under Readability Level Control

## Quick Facts
- arXiv ID: 2407.01384
- Source URL: https://arxiv.org/abs/2407.01384
- Reference count: 38
- This study examines how readability level instructions influence the generation of free-text rationales by large language models (LLMs).

## Executive Summary
This study investigates how readability level instructions influence the generation of free-text rationales by large language models across fact-checking, hate speech detection, and natural language inference tasks. The researchers prompt LLMs with readability levels ranging from sixth grade to college and evaluate the generated rationales using automatic metrics and human judgments. The results show that while rationales adapt to readability instructions, the complexity differences are not fully aligned with predefined readability score ranges. Medium complexity (high-school level) rationales are most favored by both automatic metrics and human annotators, though human readers do not reliably perceive the prompted readability levels.

## Method Summary
The researchers use four datasets (HealthFC, HateXplain, CAD, SpanEx) and four open-weight LLMs (Mistral-0.2, Mixtral-0.1, OpenChat-3.5, Llama-3) with instruction-tuned variants. They implement instruction prompting with four readability levels (college, high school, middle school, sixth grade) mapped to desired FRE scores (30, 50, 70, 90). The prompt structure includes task description, few-shot in-context samples, and readability level instruction. Rationales are generated and evaluated for task accuracy, readability metrics (FRE, GFI, CLI), TIGERScore (for rationale quality), BERTScore (for similarity to references), and human annotation for a subset of samples.

## Key Results
- Rationales adapt to readability instructions, showing measurable differences in complexity across prompted levels
- The observed distinction between readability levels does not fully match defined complexity scores according to traditional readability metrics
- Medium complexity rationales (high-school level) are most favored by both automatic metrics (TIGERScore) and human annotators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Readability level control influences the complexity of free-text rationales generated by LLMs.
- Mechanism: Instruction prompting with readability levels acts as a perturbation signal that steers the LLM's text generation process towards different complexity levels.
- Core assumption: LLMs can interpret descriptive readability levels and adjust their output accordingly, even without fine-tuning.
- Evidence anchors: [abstract] "rationales adapt to readability instructions"; [section 2] "we explore LLM output in both prediction and free-text rationalization under the influence of readability level control"
- Break condition: If the LLM cannot interpret the descriptive readability levels or the perturbation signal is too weak to influence the generation process.

### Mechanism 2
- Claim: There is a gap between prompted readability levels and the actual complexity of generated rationales.
- Mechanism: The LLM's interpretation of descriptive readability levels does not perfectly map to predefined readability score ranges.
- Core assumption: Descriptive readability levels are only approximate guides, and the LLM's internal representation of complexity may differ from traditional readability metrics.
- Evidence anchors: [abstract] "the observed distinction between readability levels does not fully match the defined complexity scores according to traditional readability metrics"; [section 4.2] "the requested readability levels introduce notable distinction to text complexity, though the measured output readability may not fully conform with the defined score ranges"
- Break condition: If the LLM's output complexity is consistently misaligned with the prompted readability levels.

### Mechanism 3
- Claim: Medium complexity rationales (high-school level) are preferred by both automatic metrics and human annotators.
- Mechanism: The evaluation metrics and human perception favor rationales that balance informativeness and accessibility.
- Core assumption: There is an optimal level of complexity for explanations that balances informativeness and accessibility.
- Evidence anchors: [abstract] "rationales of medium complexity (high-school level) are most favored"; [section 5] "full-batch TIGERScore proportionally decreases along with text complexity"; [section 6.3] "human readers do not well perceive the prompted readability levels"
- Break condition: If evaluation metrics or human annotators consistently prefer rationales of different complexity levels.

## Foundational Learning

- Concept: Instruction prompting
  - Why needed here: To guide the LLM to generate rationales at specific readability levels without fine-tuning.
  - Quick check question: How does instruction prompting differ from fine-tuning in terms of adapting LLM behavior?

- Concept: Readability metrics (e.g., Flesch Reading Ease, Gunning Fog Index)
  - Why needed here: To measure the complexity of generated rationales and compare them to prompted readability levels.
  - Quick check question: What are the key factors considered by readability metrics, and how do they relate to text complexity?

- Concept: Automatic evaluation metrics for text generation (e.g., BERTScore, TIGERScore)
  - Why needed here: To assess the quality of generated rationales and compare them across different readability levels.
  - Quick check question: How do reference-based metrics (BERTScore) differ from reference-free metrics (TIGERScore) in evaluating text generation?

## Architecture Onboarding

- Component map: Datasets (HealthFC, HateXplain, CAD, SpanEx) -> LLMs (Mistral-0.2, Mixtral-0.1, OpenChat-3.5, Llama-3) -> Prompting (instruction prompts with readability levels) -> Evaluation (task accuracy, readability metrics, TIGERScore, BERTScore, human annotation)
- Critical path: Prompt construction → LLM inference → Rationale generation → Evaluation (automatic and human)
- Design tradeoffs: Using descriptive readability levels vs. specific score ranges in prompts; reference-based vs. reference-free evaluation metrics; human annotation for subjective assessment vs. automatic metrics for scalability
- Failure signatures: LLM fails to parse instructed output format (parsing errors); generated rationales do not align with prompted readability levels (misalignment); evaluation metrics produce inconsistent or unreliable scores (metric issues)
- First 3 experiments: 1) Test LLM's ability to generate rationales at different readability levels using simple prompts; 2) Compare generated rationales' complexity to prompted levels using readability metrics; 3) Evaluate rationales using TIGERScore and BERTScore to assess quality differences across readability levels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of perturbation beyond readability level (e.g., tone, formality, or emotional valence) affect the quality and faithfulness of free-text rationales?
- Basis in paper: [inferred] The paper focuses on readability level control as a form of perturbation but suggests exploring "other NLG-related methodologies" in the future.
- Why unresolved: The study only examined one type of perturbation (readability), leaving open whether other forms of instruction-based perturbation would have similar or different effects on rationale generation.
- What evidence would resolve it: Experiments applying various other types of perturbation (tone, formality, emotional valence) to free-text rationale generation and comparing their effects on rationale quality, faithfulness, and alignment with predictions.

### Open Question 2
- Question: Does the alignment between prompted readability levels and generated text complexity improve when rationales are generated recursively within the same session rather than in separate sessions?
- Basis in paper: [explicit] "Such divergence implies that less elaborated rationales tend to introduce more mistakes, but they are usually considered minor."
- Why unresolved: The paper notes that rationales were generated in separate sessions for each readability level, which may have limited the model's ability to differentiate between levels.
- What evidence would resolve it: Comparative experiments where rationales are generated recursively within the same session versus in separate sessions, measuring the alignment between prompted and actual readability levels.

### Open Question 3
- Question: What specific linguistic features (beyond overall readability scores) distinguish rationales that human annotators perceive as most coherent and informative?
- Basis in paper: [explicit] "We register an agreement of Krippendorff's α = 3.67% and Fleiss' κ = 13.92%."
- Why unresolved: While the paper measured overall coherence and informativeness, it did not analyze which specific linguistic features contribute to these perceptions.
- What evidence would resolve it: Detailed linguistic analysis of rationales rated highly versus poorly on coherence and informativeness, identifying specific features that correlate with positive human judgments.

## Limitations

- The reliance on descriptive readability levels without specifying exact score ranges may lead to inconsistent interpretation by different LLMs
- Human evaluation shows concerning reliability issues with low annotator agreement scores (0.31-0.40) across all dimensions
- The study provides limited evidence about why misalignment between prompted and measured readability occurs or whether alternative prompting strategies might improve alignment

## Confidence

**High Confidence**: The observation that rationales adapt to readability instructions is well-supported by data showing measurable differences in readability scores across prompted levels. The finding that medium complexity rationales are preferred by automatic metrics (TIGERScore) is also robust.

**Medium Confidence**: The claim about human preference for medium complexity rationales is less certain due to low annotator agreement and limited sample size for human evaluation.

**Low Confidence**: The broader claim that readability control is "feasible but imperfectly aligned" relies heavily on the assumption that traditional readability metrics accurately capture the complexity that LLMs interpret from descriptive instructions.

## Next Checks

1. Test whether specifying exact readability score ranges in prompts produces better alignment with measured complexity compared to descriptive levels.
2. Evaluate whether observed patterns hold consistently across all four tasks or if they are specific to certain domains like hate speech detection.
3. Compare TIGERScore and BERTScore results with alternative evaluation approaches to determine if the medium complexity preference is an artifact of the specific metrics used.