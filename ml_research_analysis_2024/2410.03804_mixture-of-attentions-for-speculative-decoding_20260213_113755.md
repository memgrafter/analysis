---
ver: rpa2
title: Mixture of Attentions For Speculative Decoding
arxiv_id: '2410.03804'
source_url: https://arxiv.org/abs/2410.03804
tags:
- tokens
- decoding
- layer
- mlarge
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational cost of Large Language Model
  (LLM) inference by improving speculative decoding through a novel Mixture of Attentions
  architecture. The method introduces Layer Self-Attention to mitigate partial observability,
  Cross-Attention for more on-policy training, and Target Layer Inference to balance
  drafting speed and accuracy.
---

# Mixture of Attentions For Speculative Decoding

## Quick Facts
- arXiv ID: 2410.03804
- Source URL: https://arxiv.org/abs/2410.03804
- Reference count: 28
- Improves speculative decoding with 9.5% faster decoding and 25% higher acceptance length

## Executive Summary
This paper addresses the computational cost of Large Language Model (LLM) inference by improving speculative decoding through a novel Mixture of Attentions architecture. The method introduces Layer Self-Attention to mitigate partial observability, Cross-Attention for more on-policy training, and Target Layer Inference to balance drafting speed and accuracy. In a single-device setup, the approach achieves 9.5% faster decoding and 25% higher acceptance length compared to EAGLE-2. In a client-server scenario, it delivers 84% speedup with 53% higher acceptance length, and maintains higher accuracy during disconnections, enabling LLM serving on edge devices. The method also supports privacy-preserving deployments by allowing sensitive data to remain on the client side.

## Method Summary
The paper introduces a Mixture of Attentions architecture that improves speculative decoding by addressing partial observability through Layer Self-Attention (LSA), enhancing on-policy training via Cross-Attention (CA), and introducing Target Layer Inference (TLI) for computational efficiency. LSA aggregates information across all MLarge layers into a single key-value representation, CA enables more realistic training by simulating real-world drafting conditions where only past MLarge activations are available, and TLI allows trade-off between drafting speed and accuracy by choosing which MLarge layer MSmall should predict. The approach is evaluated on Llama3-8B-Instruct with various small models (1.3B, 1.55B, 1.8B parameters) trained on Ultrachat dataset, showing consistent improvements across multiple benchmarks including SpecBench, MT-Bench, HumanEval, GSM8K, and CNN/DM.

## Key Results
- 9.5% faster decoding and 25% higher acceptance length compared to EAGLE-2 in single-device setup
- 84% speedup with 53% higher acceptance length in client-server scenario with 4G network
- Maintains higher accuracy during disconnections, enabling LLM serving on edge devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer Self-Attention (LSA) mitigates partial observability by aggregating information across all MLarge layers into a single key-value representation.
- Mechanism: LSA applies self-attention over the layer dimension of h≤t, treating each token independently, then reduces the dimension via mean aggregation. This creates a condensed representation (T, 2Ekv) that captures relevant token information from every layer.
- Core assumption: The KV cache of all MLarge layers contains sufficient information about the dynamic system state to improve drafting accuracy.
- Evidence anchors:
  - [abstract]: "Layer Self-Attention (LSA) to mitigate partial observability"
  - [section]: "We introduce Layer Self-Attention (LSA) followed by a mean aggregation operation to reduce its dimension to (T, 2Ekv) and extract the most relevant token information from every layer"
  - [corpus]: Weak - the corpus neighbors don't mention LSA specifically, but they discuss speculative decoding improvements that could benefit from better state representation
- Break condition: If the LSA layer fails to capture the most relevant information or if the mean aggregation loses critical distinctions between layers

### Mechanism 2
- Claim: Cross-Attention (CA) enables more on-policy training by simulating real-world drafting conditions where only past MLarge activations are available.
- Mechanism: CA uses queries from tokens and keys/values from the LSA-processed MLarge activations. The mask ensures queries can only attend to activations up to time t, forcing the model to predict future tokens without access to future MLarge states.
- Core assumption: Training with limited access to MLarge activations better prepares MSmall for real inference scenarios where it must draft without complete information.
- Evidence anchors:
  - [abstract]: "Cross-Attention (CA) to improve on-policyness and training efficiency"
  - [section]: "Having input queries for time t + 1 to t + K coming into the CA layer and keys-values from MLarge only up to time t effectively means the CA layer is K-step bounded"
  - [corpus]: Weak - corpus doesn't directly address on-policy training but discusses speculative decoding improvements
- Break condition: If the on-policy training doesn't adequately simulate real drafting conditions or if the K-step boundedness is insufficient for the task

### Mechanism 3
- Claim: Target Layer Inference (TLI) allows trade-off between drafting speed and accuracy by choosing which MLarge layer MSmall should predict.
- Mechanism: Instead of always predicting the final output layer, MSmall can predict intermediate layers (oL+1−TLI). Lower TLI values mean fewer transformations, making prediction easier and faster but potentially less accurate.
- Core assumption: Predicting intermediate layers is always easier than predicting deeper layers due to fewer transformations.
- Evidence anchors:
  - [abstract]: "a flexible Target Layer Inference (TLI) mechanism to balance computational efficiency and prediction accuracy"
  - [section]: "we challenge that assumption by hypothesising that targeting a deeper MLarge layer may be more advantageous in terms of draft quality"
  - [corpus]: Weak - corpus doesn't mention TLI specifically, but discusses various speculative decoding optimizations
- Break condition: If the relationship between TLI and accuracy/speed isn't consistent across different tasks or if the computational savings don't justify accuracy loss

## Foundational Learning

- Concept: Dynamic systems perspective of LLM decoding
  - Why needed here: The paper frames LLM decoding as a dynamic system evolution, which is crucial for understanding how speculative decoding modifies this system
  - Quick check question: What are the minimal components needed to represent the state of the dynamic system for LLM decoding?

- Concept: Markov Decision Processes and partial observability
  - Why needed here: The paper draws parallels between MDP partial observability and the challenges faced by small drafting models
  - Quick check question: How does partial observability affect decision-making in MDPs, and why is this relevant to speculative decoding?

- Concept: On-policy vs off-policy training
  - Why needed here: The paper emphasizes the importance of on-policy training for speculative drafting models to better match inference conditions
  - Quick check question: What's the key difference between on-policy and off-policy training, and why does it matter for speculative decoding?

## Architecture Onboarding

- Component map: LSA → CA (with masking) → SA → LM head prediction
  - LSA runs once per drafting cycle
  - CA and SA run autoregressively during drafting
  - Verification with MLarge happens after each draft

- Critical path: LSA aggregates MLarge layer information → CA attends to past MLarge activations with masking → SA provides awareness of drafted tokens → LM head generates predictions
  - LSA runs once per drafting cycle
  - CA and SA run autoregressively during drafting
  - Verification with MLarge happens after each draft

- Design tradeoffs:
  - TLI selection: Lower TLI = faster but less accurate; higher TLI = slower but more accurate
  - On-policy training: More realistic but computationally expensive
  - LSA vs direct KV input: LSA provides better information but adds computation

- Failure signatures:
  - Poor acceptance rates: Could indicate insufficient on-policy training or inadequate state representation
  - Slow drafting: Might be due to high TLI or inefficient attention mechanisms
  - Inaccurate predictions: Could result from poor LSA aggregation or insufficient training data

- First 3 experiments:
  1. Test different TLI values (0, 1, 3) on a small dataset to understand speed-accuracy tradeoff
  2. Compare on-policy training (with CA) vs off-policy training to quantify the benefit
  3. Evaluate LSA impact by comparing with direct KV cache input from MLarge layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of target layer inference (TLI) parameter affect the trade-off between drafting speed and response quality across different network conditions in the client-server setting?
- Basis in paper: [explicit] The paper discusses the introduction of TLI as a hyperparameter to refer to the target layer that MSmall should predict, stating that higher TLI increases the acceptance length but also increases the computational time of drafting.
- Why unresolved: While the paper shows that increasing TLI increases acceptance length, it does not provide a comprehensive analysis of how this trade-off plays out across different network conditions (4G vs 5G) or how to automatically select the optimal TLI value based on current network conditions.
- What evidence would resolve it: Empirical results comparing different TLI values across various network conditions, along with a method for dynamically adjusting TLI based on network metrics, would clarify this trade-off.

### Open Question 2
- Question: What is the impact of the Layer Self-Attention (LSA) mechanism on the performance of speculative decoding when the prompt length is very short or very long?
- Basis in paper: [inferred] The paper introduces LSA to mitigate partial observability by extracting the most relevant token information from every layer, but does not discuss its performance impact with varying prompt lengths.
- Why unresolved: The paper mentions that LSA reduces partial observability but does not analyze how its effectiveness changes with different prompt lengths, which could be crucial for understanding its practical applications.
- What evidence would resolve it: Performance comparisons of speculative decoding with and without LSA across a range of prompt lengths would provide insights into its effectiveness under different conditions.

### Open Question 3
- Question: How does the on-policy training approach introduced with the Cross-Attention (CA) layer impact the long-term performance and stability of the small model in speculative decoding?
- Basis in paper: [explicit] The paper introduces CA to improve on-policyness during training, stating that it allows MSmall to be trained more on-policy efficiently by simulating the generation process.
- Why unresolved: While the paper highlights the importance of on-policy training for improving drafting accuracy, it does not explore the long-term effects on model performance or stability when using this training approach.
- What evidence would resolve it: Long-term performance studies comparing models trained with and without the CA layer's on-policy training approach would elucidate its impact on model stability and performance over time.

## Limitations
- Limited experimental scope: Evaluation focuses on single-device and one client-server scenario with limited model size diversity
- Theoretical edge deployment claims: Privacy-preserving deployment on edge devices remains largely theoretical without practical validation
- Unanalyzed computational overhead: LSA mechanism introduces additional complexity but computational impact isn't thoroughly analyzed

## Confidence

**High Confidence**: The core claim that Mixture of Attentions improves speculative decoding performance compared to EAGLE-2, supported by consistent improvements across multiple benchmarks (9.5% faster decoding, 25% higher acceptance length in single-device setup).

**Medium Confidence**: The specific mechanisms (LSA, CA, TLI) contribute positively to performance, though the individual contribution of each component hasn't been isolated through ablation studies.

**Low Confidence**: The privacy-preserving edge deployment claims and the generalizability of results across different model sizes and task types, given the limited experimental scope.

## Next Checks

1. **Ablation Study on Mechanism Contributions**: Run controlled experiments removing each of the three key mechanisms (LSA, CA, TLI) individually to quantify their specific contributions to the observed performance improvements. This would clarify whether the 9.5% speedup comes from architectural synergy or specific component innovations.

2. **Edge Device Feasibility Assessment**: Implement a prototype deployment on representative edge hardware (e.g., mobile device or Raspberry Pi) to measure actual memory usage, inference latency, and computational overhead. This would validate or challenge the privacy-preserving deployment claims and identify practical constraints not apparent in server-based evaluations.

3. **Cross-Domain Generalization Test**: Evaluate the Mixture of Attentions approach on a broader range of tasks including mathematical reasoning (GSM8K), long-form generation (CNN/DM), and multilingual benchmarks. This would test whether the architecture's benefits extend beyond the dialogue and coding tasks emphasized in the current evaluation.