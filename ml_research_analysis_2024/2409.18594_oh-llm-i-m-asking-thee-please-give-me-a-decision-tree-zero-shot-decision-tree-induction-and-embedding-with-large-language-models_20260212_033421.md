---
ver: rpa2
title: '"Oh LLM, I''m Asking Thee, Please Give Me a Decision Tree": Zero-Shot Decision
  Tree Induction and Embedding with Large Language Models'
arxiv_id: '2409.18594'
source_url: https://arxiv.org/abs/2409.18594
tags:
- data
- trees
- test
- decision
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a zero-shot approach for generating decision
  trees using large language models (LLMs) without any training data. By leveraging
  the rich world knowledge encoded in LLMs, the method constructs interpretable decision
  trees purely from feature names and task descriptions.
---

# "Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree": Zero-Shot Decision Tree Induction and Embedding with Large Language Models

## Quick Facts
- arXiv ID: 2409.18594
- Source URL: https://arxiv.org/abs/2409.18594
- Authors: Ricardo Knauer; Mario Koddenbrock; Raphael Wallsberger; Nicholas M. Brisson; Georg N. Duda; Deborah Falla; David W. Evans; Erik Rodner
- Reference count: 40
- Key outcome: Zero-shot decision trees generated by LLMs can outperform data-driven trees on 27% of small tabular datasets and their embeddings achieve better performance than traditional data-driven embeddings

## Executive Summary
This paper introduces a novel approach for generating interpretable decision trees using large language models without any training data. By leveraging the world knowledge encoded in LLMs, the method constructs decision trees purely from feature names and task descriptions, then transforms them into binary embeddings for downstream models. The approach is particularly effective in low-data regimes, where the LLM's prior knowledge compensates for limited training examples.

## Method Summary
The method uses prompt engineering to instruct LLMs to generate decision trees based solely on feature names and task descriptions. The generated textual trees are parsed into structured formats and converted to Python functions for prediction. For embedding, the binary truth values of decision tree nodes are used to create vector representations that capture feature interactions. These embeddings serve as input to downstream machine learning models, effectively combining knowledge-driven insights with data-driven approaches.

## Key Results
- Zero-shot decision trees achieved statistically significantly worse performance than data-driven trees on average
- On 27% of datasets, zero-shot trees outperformed data-driven trees
- Tree-based embeddings derived from LLM-generated trees performed better than data-driven tree-based embeddings on average
- The approach showed particular promise on small datasets (≤500 samples) where data scarcity limits traditional methods

## Why This Works (Mechanism)

### Mechanism 1
Large language models can generate decision trees without any training data by leveraging compressed world knowledge from pretraining. The models interpret feature names semantically and use their internal representations of decision-making logic to create tree structures. This breaks down when feature names are ambiguous or meaningless.

### Mechanism 2
Zero-shot decision trees can outperform data-driven trees in low-data regimes by compensating for data scarcity with prior knowledge. The LLM's domain knowledge provides better initial feature selection and decision boundaries than purely data-driven approaches. This fails when the LLM lacks relevant domain knowledge.

### Mechanism 3
Decision trees generated by LLMs can serve as effective feature representations (embeddings) for downstream models. The binary truth values create structured, interpretable embeddings that capture important feature interactions. This breaks when generated trees are too shallow or fail to capture important feature interactions.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The entire approach relies on LLMs generating models without any training examples, which is the definition of zero-shot learning.
  - Quick check question: Can you explain the difference between zero-shot, few-shot, and traditional supervised learning?

- Concept: Decision tree fundamentals
  - Why needed here: Understanding how decision trees partition feature space and make predictions is crucial for interpreting the LLM-generated trees and their embeddings.
  - Quick check question: What is the primary advantage of decision trees over other machine learning models in terms of interpretability?

- Concept: Feature representation and embeddings
  - Why needed here: The embedding approach transforms tree structures into vector representations that downstream models can use, requiring understanding of representation learning.
  - Quick check question: How do tree-based embeddings differ from traditional feature embeddings in terms of interpretability?

## Architecture Onboarding

- Component map: LLM interface layer -> Decision tree parser -> Embedding transformer -> Downstream model trainer -> Evaluation pipeline

- Critical path: Prompt LLM with feature names and task description -> Parse generated tree into structured format -> Convert tree to binary embedding -> Train downstream model with embeddings -> Evaluate performance

- Design tradeoffs:
  - Temperature setting: Higher values increase diversity but reduce determinism
  - Tree depth: Deeper trees capture more complexity but reduce interpretability
  - Number of trees: More trees provide better coverage but increase computational cost
  - Feature name quality: Meaningful names enable better tree generation, but require domain knowledge

- Failure signatures:
  - LLM returns syntactically incorrect trees (check parsing logic)
  - Generated trees don't use relevant features (check feature name quality)
  - Embedding performance worse than baseline (check tree quality and downstream model)
  - API rate limiting or cost issues (implement caching or batching)

- First 3 experiments:
  1. Generate trees for a simple dataset with clear feature names (like iris) to verify basic functionality
  2. Compare performance of zero-shot trees vs data-driven trees on a small dataset to establish baseline effectiveness
  3. Test embedding approach with different numbers of trees and depths to find optimal configuration for a specific dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of zero-shot decision trees scale with increasing dataset size beyond 500 samples?
- Basis in paper: The study focuses on small datasets (≤500 samples) and notes that LLMs may effectively compensate for data scarcity in low-data regimes.
- Why unresolved: The study explicitly limits evaluation to small datasets and does not explore performance trends as sample sizes increase into medium or large ranges.
- What evidence would resolve it: Systematic experiments comparing zero-shot tree performance across datasets of varying sizes (e.g., 100, 500, 1000, 5000 samples) would clarify whether performance degrades, plateaus, or improves with more data.

### Open Question 2
- Question: What is the impact of using domain-specific LLMs versus general-purpose LLMs on zero-shot tree performance in specialized fields like healthcare?
- Basis in paper: The authors note that clinical LLMs were not publicly available during their study and used general-purpose models instead, acknowledging potential performance differences.
- Why unresolved: The evaluation relies solely on general-purpose LLMs, leaving the question of whether domain-specific knowledge in clinical LLMs would significantly improve tree quality and predictive performance.
- What evidence would resolve it: Direct comparison experiments using both general-purpose and domain-specific LLMs (once available) on the same healthcare datasets would quantify performance differences and reveal domain-specific benefits.

### Open Question 3
- Question: Can iterative refinement of zero-shot trees using small amounts of training data substantially improve their predictive accuracy compared to purely zero-shot approaches?
- Basis in paper: The authors suggest that iterative improvement using training data could boost performance and mention evolutionary algorithms as a potential method.
- Why unresolved: The current study presents purely zero-shot approaches without any fine-tuning or iterative refinement, leaving open whether even minimal training data could meaningfully enhance results.
- What evidence would resolve it: Experiments comparing purely zero-shot trees against trees refined with small training subsets (e.g., 10%, 25% of available data) using iterative methods would demonstrate the potential gains from hybrid knowledge-data approaches.

## Limitations

- The approach relies heavily on the quality of feature names for successful tree generation, with no clear methodology for handling poorly named features
- Limited experimental scope (13 public + 2 private datasets) may not generalize to all tabular domains
- No comparison against state-of-the-art tabular learning methods like GBDT or deep learning approaches

## Confidence

- Zero-shot tree generation mechanism: **High** - Well-explained with clear theoretical foundation
- Performance claims (27% better than data-driven): **Medium** - Based on limited dataset scope with mixed results
- Embedding effectiveness claims: **Medium** - Positive results shown but without comparison to established embedding methods
- Interpretability claims: **High** - Direct consequence of decision tree structure, though quality varies

## Next Checks

1. Test zero-shot approach on datasets with deliberately poor feature names (like "x1", "x2") to measure robustness threshold
2. Implement the approach on a larger benchmark of tabular datasets (e.g., OpenML-CC18) to validate generalization
3. Compare embedding performance against established methods like AutoInt, TabTransformer, and CatBoost on the same datasets