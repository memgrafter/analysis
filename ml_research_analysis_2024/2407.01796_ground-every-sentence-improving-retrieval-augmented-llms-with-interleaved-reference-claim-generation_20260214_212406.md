---
ver: rpa2
title: 'Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved
  Reference-Claim Generation'
arxiv_id: '2407.01796'
source_url: https://arxiv.org/abs/2407.01796
tags:
- reference
- answer
- claim
- citation
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReClaim, a fine-grained attributed text generation
  method for Retrieval-Augmented Large Language Models (LLMs) that interleaves reference
  and claim generation to improve verifiability and credibility. Unlike previous coarse-grained
  approaches, ReClaim generates sentence-level citations aligned with each answer
  sentence in long-form question answering tasks.
---

# Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation

## Quick Facts
- arXiv ID: 2407.01796
- Source URL: https://arxiv.org/abs/2407.01796
- Reference count: 29
- Key outcome: Achieves 90% citation accuracy while reducing citation length to ~20% of previous methods through interleaved reference-claim generation

## Executive Summary
This paper introduces ReClaim, a fine-grained attributed text generation method for Retrieval-Augmented Large Language Models (LLMs) that interleaves reference and claim generation to improve verifiability and credibility. Unlike previous coarse-grained approaches, ReClaim generates sentence-level citations aligned with each answer sentence in long-form question answering tasks. The method employs a dual-model training strategy: one model generates references from retrieved passages, and another generates claims based solely on the preceding reference. Extensive experiments on ASQA and ELI5 datasets show that ReClaim achieves 90% citation accuracy while significantly reducing citation length (to ~20% of previous methods) and maintaining high answer quality.

## Method Summary
ReClaim addresses the challenge of generating verifiable attributed text by breaking long-form answers into sentence-level claims, each paired with a supporting reference. The approach uses two separate fine-tuned models: a ReferModel that generates reference sentences from retrieved passages, and a ClaimModel that generates claim sentences based only on the preceding reference. During inference, these models alternate to produce interleaved reference-claim pairs. The method employs constrained decoding using a prefix tree structure to ensure reference-text consistency and prevent hallucinations. The authors evaluate two variants: RECLAIM Unified (one-step fine-tuning) and RECLAIM w/IG (dual-model fine-tuning with separate models).

## Key Results
- Achieves 90% citation accuracy compared to 59.5% for prompting baselines
- Reduces citation length to 22.7% of previous methods while maintaining 55.1% answer quality
- Demonstrates superior citation quality metrics including Correct Attribution Score (CAS) and Citation Redundancy Score (CRS)
- Maintains high consistency ratios through constrained decoding that ensures reference-text alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating reference and claim generation reduces input context interference.
- Mechanism: The model generates a reference sentence, then a claim sentence based only on that reference, avoiding contamination from full context.
- Core assumption: Claim generation requires only the preceding reference, not the entire input.
- Evidence anchors:
  - [abstract] "RECLAIM allows the model to add sentence-level fine-grained citations to each answer sentence in long-form question-answering using the RAG system."
  - [section 3.4] "During the claim generation stage, since the model has already selected sufficiently granular reference text to follow, which contain the answer information, the full input context is not required."
  - [corpus] Found 25 related papers; average neighbor FMR=0.502, indicating moderate relatedness to citation generation methods.
- Break condition: If the claim requires information from multiple references, this approach may fail.

### Mechanism 2
- Claim: Constrained decoding ensures reference-text consistency and reduces hallucinations.
- Mechanism: Prefix tree structure restricts generated references to exact sentences from source passages.
- Core assumption: Exact sentence matching prevents hallucinated content in references.
- Evidence anchors:
  - [abstract] "We also added decoding constraints to prevent inconsistencies between citations and source reference passages."
  - [section 3.5] "We employ two metrics to measure the Verifiability: ... Citation Length, where shorter citation text typically reduces the time needed for fact-checking; ... Consistency Ratio (CR) which determines the text consistency between the reference parts and the reference passages through string matching."
  - [corpus] No strong evidence of constrained decoding in related work; this appears novel.
- Break condition: If source passages contain errors, this mechanism will faithfully reproduce them.

### Mechanism 3
- Claim: Sentence-level citations improve attribution quality while reducing citation length.
- Mechanism: Breaking answers into sentence-level claims with supporting references enables precise attribution.
- Core assumption: Users can verify claims more easily with precise sentence-level citations than paragraph-level citations.
- Evidence anchors:
  - [abstract] "Unlike traditional coarse-grained attribution, RECLAIM allows the model to add sentence-level fine-grained citations to each answer sentence in long-form question-answering tasks."
  - [section 4.2] "Citation Quality... Citation Redundant Score (CRS), which identifies any redundant citation sentences."
  - [corpus] Moderate evidence of sentence-level citation methods in related work (Concise and Sufficient Sub-Sentence Citations).
- Break condition: If sentences are too long or complex, single-sentence attribution may be insufficient.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Provides structured format for alternating reference-claim generation
  - Quick check question: How does CoT format help organize interleaved reference and claim generation?

- Concept: Prefix trees (tries)
  - Why needed here: Enables constrained decoding by restricting output to valid reference sentences
  - Quick check question: How does a prefix tree ensure generated references match source passages?

- Concept: Natural Language Inference (NLI)
  - Why needed here: Evaluates attribution quality by checking if references support claims
  - Quick check question: What does NLI tell us about the relationship between a reference and its claim?

## Architecture Onboarding

- Component map: Question + Retrieved passages → ReferModel → ClaimModel → ReferModel → ClaimModel → ... → Interleaved reference-claim pairs

- Critical path: Question → ReferModel → ClaimModel → ReferModel → ClaimModel → ... → Output

- Design tradeoffs:
  - Separate models for reference/claim vs unified model
  - Constrained vs unconstrained decoding
  - Sentence-level vs paragraph-level citations

- Failure signatures:
  - Claims that don't logically follow references
  - References that don't match source passages
  - Excessive citation length reducing readability

- First 3 experiments:
  1. Prompting baseline: Test if simple prompting achieves comparable results
  2. Unified fine-tuning: Test if one-step generation improves over prompting
  3. Interleaving ablation: Test ReferModel-only vs ClaimModel-only vs both models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal number of reference-claim pairs for different task types beyond long-form QA?
- Basis in paper: [explicit] The paper mentions setting constraints on the number of reference-claim pairs (2-5 for ASQA, 4-6 for ELI5) but doesn't explore optimal values for other task types
- Why unresolved: The paper only experiments with two datasets and specific pair constraints without systematically exploring how different task types might benefit from different numbers of pairs
- What evidence would resolve it: Comparative experiments testing various pair limits (1-10) across multiple task types (short-form QA, summarization, multi-hop reasoning) measuring answer quality, citation quality, and generation efficiency

### Open Question 2
- Question: How does ReClaim perform when reference passages contain contradictory information?
- Basis in paper: [inferred] The paper doesn't address scenarios where retrieved passages contain conflicting information, yet this is a common challenge in real-world RAG systems
- Why unresolved: Current evaluation metrics and experimental design assume reference passages are consistent and don't measure the model's ability to handle contradictions or select the most reliable information
- What evidence would resolve it: Experiments using datasets with intentionally contradictory references measuring how well ReClaim identifies conflicts, resolves contradictions, and maintains citation accuracy

### Open Question 3
- Question: What is the computational overhead of ReClaim compared to standard RAG systems during inference?
- Basis in paper: [inferred] The paper focuses on quality improvements but doesn't report latency measurements or computational costs of the alternating generation process
- Why unresolved: The interleaving generation approach requires multiple model invocations and constrained decoding, which could impact real-time applications despite quality gains
- What evidence would resolve it: Detailed benchmarking of inference time, memory usage, and throughput comparing ReClaim to baseline RAG systems across different hardware configurations and batch sizes

## Limitations

- The method requires extensive training data (9,433 samples) constructed using ChatGPT and NLI filtering, which may not be feasible for all domains
- Constrained decoding may limit the model's ability to handle complex reasoning that requires combining information from multiple sources
- The evaluation relies on oracle references (ALCE Oracle-5), which may not reflect real-world retrieval performance with noisy or incomplete passages

## Confidence

**High Confidence**: The claim that sentence-level citations improve attribution quality and reduce citation length is well-supported by the experimental results (90% citation accuracy vs 59.5% for prompting, and 22.7% citation length vs 55.5% for prompting).

**Medium Confidence**: The assertion that alternating reference and claim generation reduces input context interference is supported by ablation studies, but the mechanism's effectiveness may depend heavily on the specific dataset and task characteristics.

**Low Confidence**: The claim that constrained decoding significantly reduces hallucinations is supported by consistency metrics, but the evaluation doesn't directly measure hallucination frequency in unconstrained generation scenarios.

## Next Checks

1. **Generalization Test**: Evaluate ReClaim on datasets with noisier retrieval (non-oracle references) to assess robustness to real-world retrieval errors and determine if the performance gains hold under imperfect retrieval conditions.

2. **Cross-Domain Transfer**: Test the trained models on out-of-domain questions (e.g., medical, legal, or technical domains) to verify if the sentence-level citation generation generalizes beyond the ASQA and ELI5 datasets.

3. **Human Evaluation of Hallucinations**: Conduct human evaluation studies comparing hallucination frequency between ReClaim and baseline methods, specifically measuring whether the constrained decoding approach meaningfully reduces fabricated content in practice.