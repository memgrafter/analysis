---
ver: rpa2
title: 'Score-CDM: Score-Weighted Convolutional Diffusion Model for Multivariate Time
  Series Imputation'
arxiv_id: '2405.13075'
source_url: https://arxiv.org/abs/2405.13075
tags:
- time
- series
- score-cdm
- data
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multivariate time series (MTS) imputation,
  aiming to reconstruct incomplete MTS data. The proposed Score-CDM (Score-weighted
  Convolutional Diffusion Model) uses a novel denoising function combining a Score-weighted
  Convolution Module (SCM) and an Adaptive Reception Module (ARM).
---

# Score-CDM: Score-Weighted Convolutional Diffusion Model for Multivariate Time Series Imputation

## Quick Facts
- arXiv ID: 2405.13075
- Source URL: https://arxiv.org/abs/2405.13075
- Authors: S. Zhang; S. Wang; H. Miao; H. Chen; C. Fan; J. Zhang
- Reference count: 16
- Key outcome: Achieves 12.14 MAE on AQI-36 dataset with 25% missing data, outperforming best baseline (PriSTI) by 3%

## Executive Summary
This paper introduces Score-CDM, a novel diffusion model for multivariate time series imputation that combines a Score-weighted Convolution Module (SCM) and Adaptive Reception Module (ARM) to balance local and global temporal features. The model uses FFT-based convolution in the spectral domain to adaptively adjust receptive fields while capturing global temporal correlations through a score-weighted approach. Extensive experiments on three real-world traffic datasets demonstrate that Score-CDM outperforms existing state-of-the-art methods by 3% on AQI-36 and shows consistent improvements across different missing data percentages.

## Method Summary
Score-CDM addresses multivariate time series imputation by combining SCM and ARM modules to create an effective denoising function. SCM generates a globally attentive convolution kernel by projecting the input onto learnable matrices and computing element-wise products, while ARM uses a Spectral2Time Window Block (S2TWB) that leverages Fast Fourier Transform to adaptively change the receptive field for local temporal features. The two modules work together through element-wise multiplication to balance local and global temporal dependencies. The model is trained using a diffusion probabilistic framework with 50 diffusion steps and evaluated on METR-LA, AQI-36, and PEMS-BAY datasets with missing data percentages ranging from 25% to 95%.

## Key Results
- Achieves 12.14 MAE on AQI-36 dataset with 25% missing data (3% improvement over PriSTI baseline at 12.57)
- Outperforms state-of-the-art methods across all three datasets (METR-LA, AQI-36, PEMS-BAY)
- Shows consistent performance across different missing data percentages (25%, 50%, 75%, 95%)
- Ablation studies confirm the importance of S2TWB in the Adaptive Reception Module

## Why This Works (Mechanism)

### Mechanism 1
SCM enables global temporal feature learning by creating a score map that acts as a convolution kernel. It projects the input time series onto learnable matrices WQ and WK, computes element-wise products between Q and K, applies softmax to obtain a globally attentive score map, and uses this as a convolution kernel. The core assumption is that global temporal correlations can be effectively captured by a single score map that weights all time steps.

### Mechanism 2
ARM with S2TWB enables adaptive balancing of local and global temporal features through FFT-based convolution. S2TWB generates basis operators using sinusoidal functions, combines them linearly, applies FFT to project to spectral domain, multiplies with the transformed time series, and applies inverse FFT to obtain the convolved result with adaptive receptive field. The core assumption is that FFT convolution can efficiently and flexibly adjust the receptive field to capture both local and global temporal dependencies.

### Mechanism 3
The combination of SCM and ARM creates a denoising function that outperforms attention-only or CNN-only approaches by balancing local and global features. SCM generates the global score map (convolution kernel), ARM generates the adaptive receptive field (time window), and their element-wise product performs convolution that balances both types of features. The core assumption is that neither attention nor CNN alone can adaptively balance local and global features as effectively as their combined approach.

## Foundational Learning

- Concept: Fast Fourier Transform (FFT) and convolution theorem
  - Why needed here: FFT enables efficient convolution operations in the spectral domain, which is crucial for ARM's adaptive receptive field mechanism
  - Quick check question: Can you explain why multiplication in the frequency domain corresponds to convolution in the time domain?

- Concept: Diffusion probabilistic models and reverse process
  - Why needed here: The model uses diffusion models for time series imputation, requiring understanding of the forward noise-adding process and reverse denoising process
  - Quick check question: What is the role of the score function in the reverse diffusion process?

- Concept: Attention mechanisms and their limitations
  - Why needed here: The paper compares against attention-based methods and positions SCM as an alternative for global feature learning
  - Quick check question: What are the computational complexity differences between self-attention and the proposed SCM approach?

## Architecture Onboarding

- Component map: Input → SCM (matrix projection + info exchange → score map) → ARM (S2TWB → receptive field) → element-wise product → convolution output → denoising function → diffusion model
- Critical path: Input → SCM → ARM → element-wise product → output (this sequence must be efficient for real-time imputation)
- Design tradeoffs: Global vs local feature balance (SCM provides global attention, ARM provides local adaptability); computational efficiency (FFT-based vs direct convolution); model complexity vs performance
- Failure signatures: Poor performance on datasets with predominantly local patterns (SCM may be too global); poor performance on datasets with long-range dependencies (ARM receptive field may be too limited); high computational cost on very long time series (FFT operations may become expensive)
- First 3 experiments:
  1. Compare MAE/MSE on METR-LA dataset with 25% missing data using full Score-CDM vs w/o(S2TWB) variant
  2. Compare training time and inference latency between Score-CDM and baseline attention-based diffusion model
  3. Visualize the learned score map and receptive field on sample time series to verify global-local balance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions remain unanswered based on the limitations and scope of the current work. These include the model's performance on extremely long time series data, its applicability to domains beyond traffic data, and how it handles non-random missing data patterns.

## Limitations

- Limited evaluation scope to three traffic-related datasets, raising questions about generalizability to other domains
- Significant model complexity with SCM and ARM modules may make implementation and optimization challenging
- No computational complexity analysis or runtime comparisons with baseline methods provided

## Confidence

- **High Confidence**: The core mechanism of using FFT-based convolution for adaptive receptive field adjustment is well-established and correctly applied
- **Medium Confidence**: The claim that Score-CDM outperforms state-of-the-art methods by 3% on AQI-36 is supported by the presented results
- **Low Confidence**: The assertion that the combined SCM+ARM approach is fundamentally superior to simply combining CNN and attention is not rigorously proven

## Next Checks

1. **Cross-domain validation**: Test Score-CDM on non-traffic datasets (e.g., healthcare time series, financial data) to assess generalizability beyond the current evaluation scope.

2. **Ablation of adaptive balancing**: Create a baseline that combines standard CNN and attention mechanisms with explicit balancing mechanisms (e.g., gating or weighting) to empirically verify the claim that simple combinations cannot achieve the same performance.

3. **Computational efficiency analysis**: Measure and compare training/inference time, memory usage, and FLOPs between Score-CDM and baseline methods to evaluate the practical trade-offs of the increased model complexity.