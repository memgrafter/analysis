---
ver: rpa2
title: 'MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical
  Knowledge'
arxiv_id: '2406.02919'
source_url: https://arxiv.org/abs/2406.02919
tags:
- medical
- llms
- knowledge
- evaluation
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MultifacetEval, a multifaceted evaluation\
  \ framework designed to assess large language models' (LLMs) mastery of medical\
  \ knowledge across multiple capabilities: comparison, rectification, discrimination,\
  \ and verification. The authors construct two datasets\u2014MultiDiseK based on\
  \ a clinical disease knowledge base and MultiMedQA based on the MedQA benchmark\u2014\
  by generating questions targeting each facet."
---

# MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical Knowledge

## Quick Facts
- **arXiv ID**: 2406.02919
- **Source URL**: https://arxiv.org/abs/2406.02919
- **Reference count**: 12
- **Key outcome**: Introduces a multifaceted evaluation framework (MultiDiseK, MultiMedQA) revealing that LLMs' medical knowledge mastery drops significantly under multi-faceted assessment compared to single-facet benchmarks.

## Executive Summary
This paper introduces MultifacetEval, a multifaceted evaluation framework designed to assess large language models' (LLMs) mastery of medical knowledge across four capabilities: comparison, rectification, discrimination, and verification. The authors construct two datasets—MultiDiseK based on a clinical disease knowledge base and MultiMedQA based on the MedQA benchmark—by generating questions targeting each facet. Experiments on 13 LLMs show that while models perform well on traditional single-facet benchmarks, their performance drops significantly under multifaceted evaluation. These findings indicate that current LLMs lack deep, precise, and comprehensive medical knowledge mastery, highlighting the need for better training and evaluation strategies in medical AI.

## Method Summary
The study constructs two multifaceted medical knowledge datasets: MultiDiseK (based on a clinical disease knowledge base with 3,167 knowledge points) and MultiMedQA (rephrased from the MedQA benchmark with 800 questions). Each dataset includes four question types targeting different facets of medical knowledge: multiple-choice questions (MCQs), rectified questions (RQs), multiple-answer questions (MAQs), and true/false questions (TFQs). The evaluation employs a five-shot learning approach with two settings: answer-only and Chain-of-Thought with Self-consistency (CoT+SC). Model performance is measured using the proportion of mastered knowledge points and facet-specific accuracies, with random guessing baselines established for each question type.

## Key Results
- GPT-3.5-turbo's mastery proportion drops by ~50% on MultiMedQA and ~40% on MultiDiseK under multifaceted evaluation compared to single-facet results
- Open-source models under 13B parameters perform close to random guessing on higher-level facets like verification and rectification
- Larger models show more balanced performance across facets but still exhibit significant performance degradation
- Multifaceted evaluation reveals substantial gaps in LLMs' comprehensive medical knowledge mastery not captured by traditional single-facet benchmarks

## Why This Works (Mechanism)
The multifaceted evaluation approach works by systematically probing different dimensions of medical knowledge mastery through four distinct question types. MCQs test basic recall, RQs assess the ability to identify and correct errors, MAQs evaluate discrimination between similar concepts, and TFQs verify precise understanding. By requiring models to demonstrate competence across all four facets for each knowledge point, the framework exposes limitations in models' ability to integrate and apply medical knowledge comprehensively, rather than relying on pattern matching or surface-level understanding.

## Foundational Learning
- **Medical knowledge representation**: Understanding how medical knowledge is structured in clinical databases (why needed: to construct valid test questions; quick check: verify question coverage against source knowledge base)
- **Multiple-choice question design**: Principles of constructing effective MCQs for knowledge assessment (why needed: to ensure reliable baseline measurements; quick check: pilot test with human experts)
- **Error detection and correction**: Cognitive processes involved in identifying and fixing incorrect medical information (why needed: to evaluate higher-order reasoning; quick check: measure precision of identified errors)
- **Knowledge integration assessment**: Methods for evaluating comprehensive understanding across multiple related concepts (why needed: to move beyond isolated fact recall; quick check: analyze correlation between facet performances)

## Architecture Onboarding
**Component Map**: Question generation -> Dataset construction -> Five-shot evaluation -> Answer extraction -> Performance aggregation
**Critical Path**: Dataset construction → Five-shot evaluation → Answer extraction → Performance aggregation
**Design Tradeoffs**: The framework trades breadth of medical domain coverage for depth of assessment within selected knowledge points, focusing on quality and variety of questions over quantity of topics.
**Failure Signatures**: Poor answer extraction accuracy, inconsistent CoT+SC aggregation, and random baseline deviation indicate implementation issues.
**First Experiments**:
1. Validate answer extraction accuracy on a sample of LLM responses
2. Compare single-facet vs multifaceted performance on a small subset of questions
3. Test random baseline calculation against theoretical probabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on multiple-choice and structured question formats that may not capture real-world medical knowledge application complexity
- Datasets based on existing medical knowledge bases, potentially limiting assessment of novel or emerging medical concepts
- Answer extraction using regular expressions and heuristics may introduce biases or errors, particularly for open-ended responses

## Confidence
**High Confidence**: The observation that LLMs show substantial performance drops when evaluated across multiple facets is well-supported by experimental results.
**Medium Confidence**: Open-source models under 13B parameters performing near random guessing on higher-level facets is supported by data but may not generalize to all medical domains.
**Low Confidence**: Claims about fundamental knowledge mastery deficiencies require further validation beyond the current evaluation methodology.

## Next Checks
1. Conduct cross-validation using alternative medical knowledge bases and question formats to assess framework robustness across different medical domains
2. Implement human expert evaluation of LLM responses to validate automated answer extraction accuracy
3. Perform ablation studies by systematically removing different facets to quantify individual contributions to overall performance drop