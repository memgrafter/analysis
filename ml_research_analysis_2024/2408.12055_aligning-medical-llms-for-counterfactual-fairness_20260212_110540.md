---
ver: rpa2
title: Aligning (Medical) LLMs for (Counterfactual) Fairness
arxiv_id: '2408.12055'
source_url: https://arxiv.org/abs/2408.12055
tags:
- uni00000013
- uni00000011
- uni00000003
- uni00000014
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a bias mitigation technique for Large Language
  Models (LLMs) used in medical applications by leveraging a knowledge distillation
  framework with preference optimization. The method aligns a target LLM by using
  a teacher model to generate unbiased reference answers, constructing preference
  datasets through red-teaming prompts that inject sensitive attributes, and then
  fine-tuning the target model using SimPO.
---

# Aligning (Medical) LLMs for (Counterfactual) Fairness

## Quick Facts
- arXiv ID: 2408.12055
- Source URL: https://arxiv.org/abs/2408.12055
- Reference count: 40
- This study introduces a bias mitigation technique for Large Language Models (LLMs) used in medical applications by leveraging a knowledge distillation framework with preference optimization.

## Executive Summary
This study introduces a bias mitigation technique for Large Language Models (LLMs) used in medical applications by leveraging a knowledge distillation framework with preference optimization. The method aligns a target LLM by using a teacher model to generate unbiased reference answers, constructing preference datasets through red-teaming prompts that inject sensitive attributes, and then fine-tuning the target model using SimPO. Experiments across three clinical tasks—pain management, treatment recommendations, and patient triage—demonstrate significant reductions in bias across multiple general-purpose and clinically tuned LLMs, with maximum probability differences between demographic subgroups decreasing by up to 50% in some cases.

## Method Summary
The method employs knowledge distillation with preference optimization to reduce bias in medical LLMs. It uses a teacher model (e.g., Gemini) to generate unbiased reference answers, then constructs preference datasets by injecting random sensitive attributes (gender, race, ethnicity) into clinical prompts. The target LLM generates candidate answers that are ranked by semantic similarity to the reference using cosine similarity of sentence embeddings. The top-ranked answer forms a preference pair with the reference, and the target LLM is fine-tuned using SimPO with LoRA on this preference dataset. The approach is evaluated across three clinical tasks using counterfactual fairness metrics and statistical significance tests.

## Key Results
- Maximum probability differences between demographic subgroups decreased by up to 50% in some cases
- Significant reductions in bias across multiple general-purpose and clinically tuned LLMs (Llama 3.1, Gemma 2, Meditron, Llama 3-OpenBioLLM)
- Method maintains model performance while reducing fairness disparities
- Approach remains effective even when the teacher model is the target LLM itself

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The knowledge distillation with preference optimization reduces biased patterns by aligning target LLM outputs to a reference (unbiased) teacher model.
- Mechanism: SimPO fine-tuning minimizes the divergence between target and teacher model responses across counterfactual demographic prompts, enforcing counterfactual fairness.
- Core assumption: Teacher model responses are unbiased and semantically equivalent across demographic variations.
- Evidence anchors:
  - [abstract] "aligns the Target LLM (i.e., the LLM that we aim to improve) using a preference optimization method within a knowledge distillation framework"
  - [section] "We leverage a teacher model to serve as a reference point, and guide the Target LLM towards fairer decision-making through a Preference Optimization (PO) method."
- Break condition: If teacher model is biased, the alignment will propagate or amplify those biases.

### Mechanism 2
- Claim: Random Prompt Modifier ensures systematic exposure of the model to demographic variations while maintaining semantic equivalence.
- Mechanism: By injecting random protected attributes into prompts, the model is forced to generate equivalent responses regardless of demographic context.
- Core assumption: Random selection of protected attributes avoids systematic bias in prompt construction.
- Evidence anchors:
  - [section] "Before presenting the detailed steps of our method, we present a very brief background on aligning LLMs... introduces a methodology to create a set of counterfactual questions... randomly chosen from a curated list"
- Break condition: If attribute list is incomplete or systematically biased, fairness gains may be limited.

### Mechanism 3
- Claim: Semantic Textual Similarity ranking ensures the preference dataset reflects true semantic equivalence between candidate and reference answers.
- Mechanism: Cosine similarity between sentence embeddings ranks answers by closeness to reference, ensuring preference pairs are semantically aligned.
- Core assumption: Embedding model captures all relevant semantic nuances for medical QA fairness.
- Evidence anchors:
  - [section] "we first extract the sentence embeddings produced by a text embeddings model specifically pre-trained for Semantic Textual Similarity... calculate the cosine similarity between the two candidate answers and the reference answer"
- Break condition: If embedding model misses subtle fairness-related semantic differences, preference ranking may not capture true fairness.

## Foundational Learning

- Concept: Counterfactual fairness
  - Why needed here: The method explicitly constructs counterfactual prompts (same clinical scenario, different demographics) to measure and reduce demographic-based response disparities.
  - Quick check question: What is the difference between counterfactual fairness and group fairness?

- Concept: Preference optimization (PO)
  - Why needed here: SimPO is used to fine-tune the model based on preference pairs derived from semantic similarity, rather than standard supervised learning.
  - Quick check question: How does SimPO differ from DPO in terms of reference policy requirement?

- Concept: Knowledge distillation
  - Why needed here: The teacher model provides unbiased reference answers that guide the target model's alignment process.
  - Quick check question: What is the role of the teacher model in knowledge distillation for fairness alignment?

## Architecture Onboarding

- Component map: Teacher LLM → Random Prompt Modifier → Target LLM (twice) → STS Embedding Model → Cosine Similarity → Preference Dataset → SimPO fine-tuning → Aligned Target LLM
- Critical path: Prompt generation → Semantic similarity ranking → Preference dataset construction → SimPO fine-tuning
- Design tradeoffs: Using the target model as its own teacher reduces computational cost but may limit bias reduction; stronger teacher models yield better fairness gains but require more resources.
- Failure signatures: If p-values remain low after alignment, bias persists; if accuracy drops significantly, alignment may be overcorrecting.
- First 3 experiments:
  1. Run red-teaming evaluation on base Llama 3.1 with Q-Pain dataset to establish baseline bias metrics.
  2. Apply proposed alignment method with Gemma 2 as teacher and re-evaluate bias metrics.
  3. Compare maximum probability differences pre- and post-alignment across all three clinical tasks.

## Open Questions the Paper Calls Out

- The paper acknowledges limitations regarding real-world implementation requiring careful consideration of ethical implications and potential unintended consequences beyond the scope of this study.
- The authors note that their findings are limited to the controlled vignette datasets used in the evaluation and may not generalize to real-world clinical scenarios with complex, unstructured patient data.

## Limitations
- Effectiveness depends on the assumed unbiasedness of the teacher model, which is difficult to verify independently
- Random prompt modifier's effectiveness depends on the completeness and neutrality of the protected attribute list
- Semantic textual similarity ranking may miss subtle fairness-related semantic differences in complex medical contexts

## Confidence
- High confidence: The method's overall framework (knowledge distillation + preference optimization) is technically sound and the experimental design is rigorous
- Medium confidence: The reported bias reduction metrics are compelling but depend on the assumed unbiasedness of the teacher model
- Low confidence: The claim that the method works when the teacher model is the target LLM itself is particularly uncertain

## Next Checks
1. Conduct a bias analysis on the teacher model (Gemini) itself using the same counterfactual fairness metrics to establish baseline bias levels before alignment
2. Perform ablation studies comparing different teacher models (including weaker models) to quantify the impact of teacher quality on fairness improvements
3. Test the method on additional clinical tasks and protected attribute combinations not covered in the original experiments to assess generalizability