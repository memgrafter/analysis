---
ver: rpa2
title: Extracting Interpretable Task-Specific Circuits from Large Language Models
  for Faster Inference
arxiv_id: '2412.15750'
source_url: https://arxiv.org/abs/2412.15750
tags:
- task
- circuit
- size
- ablation
- specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to automatically extract task-specific
  circuits from large language models (LLMs) for faster inference. The method leverages
  mechanistic interpretability techniques to identify and prune components that are
  not relevant to a specific task, resulting in smaller and more efficient models.
---

# Extracting Interpretable Task-Specific Circuits from Large Language Models for Faster Inference

## Quick Facts
- arXiv ID: 2412.15750
- Source URL: https://arxiv.org/abs/2412.15750
- Reference count: 22
- Primary result: Automatic extraction of task-specific circuits that are up to 82.77% smaller and faster while maintaining or improving accuracy

## Executive Summary
This paper proposes a novel approach to automatically extract task-specific circuits from large language models (LLMs) for faster inference and improved interpretability. The method leverages mechanistic interpretability techniques, using activation patching and KL divergence to identify and prune components that are not relevant to a specific task. Experiments on three tasks (acronym prediction, indirect object identification, and greater-than comparison) demonstrate that the resulting models are significantly smaller and faster while maintaining or even improving accuracy. The pruned models also align well with manually identified circuits from previous works, containing the most important components.

## Method Summary
The approach automatically identifies relevant components in a pretrained LLM and prunes the model to retain only the subset of components that efficiently perform the specific task. It uses activation patching with either zero or mean ablation schemes, computing KL divergence between patched and original model outputs to determine component importance. The algorithm iteratively removes components where the KL divergence change falls below a threshold α. Mean ablation replaces component activations with their mean value, preserving distributional consistency, while zero ablation sets them to zero. The method can optionally prune MLPs after attention heads, as MLPs offer greater size reduction due to having more parameters per layer.

## Key Results
- Pruned models are up to 82.77% smaller than the original GPT-2 Small model
- Mean ablation produces smaller models than zero ablation while maintaining accuracy
- The approach outperforms knowledge distillation baselines using less training data
- Extracted circuits align well with manually identified circuits from previous works

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning components that don't significantly affect KL divergence removes unnecessary computational overhead while preserving task performance
- Mechanism: The algorithm iteratively patches each component using either zero or mean ablation, computes the KL divergence change between the patched and original model, and permanently removes components where the change is below threshold α
- Core assumption: KL divergence between model outputs effectively measures component importance for the specific task
- Break condition: If the KL divergence metric fails to capture task-specific performance differences, or if components have non-additive effects on the residual stream that KL divergence cannot detect

### Mechanism 2
- Claim: Mean ablation preserves more task-relevant components than zero ablation because it maintains distributional consistency
- Mechanism: Mean ablation replaces component activations with their mean value across a reference distribution, which keeps downstream components in-distribution, whereas zero ablation sends components off-distribution by replacing with zeros
- Core assumption: Downstream components depend on upstream activations being in-distribution for proper functioning
- Break condition: If the task requires sharp discontinuities or if mean values happen to create spurious correlations that zero ablation would avoid

### Mechanism 3
- Claim: Pruning MLPs provides greater size reduction than pruning attention heads alone, with minimal performance degradation
- Mechanism: MLPs have more parameters per layer (2d·dmlp) than attention heads (4d·dhead), so their removal yields larger computational savings; the algorithm shows MLPs are pruned only after irrelevant attention heads are removed
- Core assumption: Attention heads are prioritized for pruning over MLPs, and MLPs can be removed without catastrophic task failure
- Break condition: If MLPs contain task-critical information that cannot be approximated by remaining components, or if their removal causes non-linear degradation in task performance

## Foundational Learning

- Concept: Transformer architecture and residual stream mechanics
  - Why needed here: Understanding how components read from and write to the residual stream is essential for understanding why zero/mean ablation work as pruning strategies
  - Quick check question: How do attention heads and MLPs modify the residual stream, and why does setting their output to zero effectively remove them?

- Concept: KL divergence as a distributional similarity metric
  - Why needed here: The pruning algorithm uses KL divergence to measure the impact of component removal on model behavior
  - Quick check question: What does KL divergence measure between two probability distributions, and why is it appropriate for comparing model outputs?

- Concept: Mechanistic interpretability and circuit discovery
  - Why needed here: The approach builds on circuit identification techniques to locate task-specific components
  - Quick check question: How do mechanistic interpretability techniques identify which components are responsible for specific tasks in neural networks?

## Architecture Onboarding

- Component map: Pretrained LLM → Dataset → Component patching → KL divergence evaluation → Pruning decision → Submodel construction
- Critical path: Dataset → Component patching → KL divergence evaluation → Pruning decision → Submodel construction
- Design tradeoffs: Zero ablation is more aggressive but requires more components to maintain performance; mean ablation preserves more components but yields smaller models; including MLPs increases size reduction but may slightly hurt accuracy
- Failure signatures: Models that fail to maintain task accuracy despite pruning, inconsistent results across runs (especially with zero ablation), or pruned models that are unexpectedly large
- First 3 experiments:
  1. Run the pruning algorithm on a simple task (like acronym prediction) with both zero and mean ablation to observe the difference in resulting model size
  2. Vary the threshold α parameter to find the optimal balance between size reduction and accuracy preservation
  3. Compare the pruned model's performance against a knowledge distillation baseline on the same task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on tasks involving multi-token predictions, such as completing an acronym to its full phrase?
- Basis in paper: [inferred] The paper evaluates the method on single-token prediction tasks, such as predicting the last letter of an acronym, but acknowledges the limitation to these types of tasks and suggests extending the evaluation to more complex tasks like multi-token prediction.
- Why unresolved: The current evaluation is limited to single-token prediction tasks, and the paper does not provide data on the method's effectiveness on multi-token prediction tasks.
- What evidence would resolve it: Experiments showing the method's performance on tasks requiring multi-token predictions, such as acronym completion to full phrases, would provide insights into its applicability and limitations for more complex tasks.

### Open Question 2
- Question: What is the impact of using different metrics, such as logit difference, compared to KL divergence in the circuit extraction process?
- Basis in paper: [explicit] The paper mentions that previous works used metrics different from KL divergence, such as logit difference, and that the KL divergence was found to be the most effective in yielding consistent results. However, it suggests that further research is needed to explore the impact of different metrics.
- Why unresolved: The paper focuses on using KL divergence and does not provide a comparative analysis of the method's performance with other metrics.
- What evidence would resolve it: Comparative studies using different metrics, such as logit difference, alongside KL divergence, would help determine the most effective metric for circuit extraction and its impact on the resulting models.

### Open Question 3
- Question: How does the proposed method scale when applied to larger, production-grade language models beyond GPT-2 Small?
- Basis in paper: [inferred] The paper evaluates the method on GPT-2 Small and acknowledges the limitation to this model. It suggests extending the evaluation to larger, production-grade models as future work.
- Why unresolved: The current evaluation is limited to GPT-2 Small, and the paper does not provide insights into how the method performs on larger models.
- What evidence would resolve it: Experiments applying the method to larger models, such as GPT-3 or GPT-4, would provide insights into its scalability and effectiveness on more complex and larger-scale models.

### Open Question 4
- Question: How does the proposed method handle tasks that involve more than one possible correct answer, such as the greater-than task where multiple end years are valid?
- Basis in paper: [explicit] The paper notes that the greater-than task involves multiple possible correct answers and mentions that the KL divergence is not correlated with how accuracy is computed in this context, leading to interesting phenomena that are out of scope for the current paper.
- Why unresolved: The paper acknowledges the challenge of handling tasks with multiple correct answers but does not provide a solution or detailed analysis of how the method addresses this issue.
- What evidence would resolve it: Studies exploring the method's performance on tasks with multiple correct answers, and how it handles the correlation between KL divergence and accuracy, would provide insights into its applicability to such tasks.

## Limitations
- The approach requires task-specific datasets for each circuit extraction, limiting scalability
- Zero ablation may introduce off-distribution activations that could affect downstream components
- The method's effectiveness may vary across different model architectures and task types

## Confidence

**High Confidence Claims:**
- The pruning algorithm successfully reduces model size while maintaining task performance
- Mean ablation generally produces smaller models than zero ablation while preserving more task-relevant components
- The approach outperforms knowledge distillation baselines with less data

**Medium Confidence Claims:**
- The extracted circuits align well with manually identified circuits from previous works
- MLP pruning provides greater size reduction than attention head pruning
- The approach is broadly applicable across different transformer architectures

**Low Confidence Claims:**
- The KL divergence threshold α can be reliably tuned for optimal performance across all tasks
- The extracted circuits capture all task-relevant information without losing important upstream dependencies

## Next Checks
1. Test the pruning approach on larger transformer models (e.g., GPT-2 Medium/Large) to evaluate scalability
2. Evaluate pruned models on out-of-distribution data to assess generalization
3. Compare the approach against other pruning methods like magnitude-based pruning to establish relative effectiveness