---
ver: rpa2
title: Accelerating the Surrogate Retraining for Poisoning Attacks against Recommender
  Systems
arxiv_id: '2408.10666'
source_url: https://arxiv.org/abs/2408.10666
tags:
- recommender
- attacks
- attack
- retraining
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of data poisoning
  attacks against recommender systems, where attackers iteratively retrain surrogate
  models to optimize fake user interactions. The key insight is that a change in one
  user/item representation triggers cascading effects through the interaction graph.
---

# Accelerating the Surrogate Retraining for Poisoning Attacks against Recommender Systems

## Quick Facts
- arXiv ID: 2408.10666
- Source URL: https://arxiv.org/abs/2408.10666
- Reference count: 40
- One-line primary result: Reduces surrogate retraining time by 26-43% and improves attack effectiveness by 18-177% for data poisoning attacks against recommender systems

## Executive Summary
This paper addresses the computational bottleneck in data poisoning attacks against recommender systems, where attackers must iteratively retrain surrogate models to optimize fake user interactions. The authors propose Gradient Passing (GP), a technique that explicitly passes gradients between interacted user-item pairs during backpropagation, accelerating retraining by approximating the cascading effects of representation changes through the interaction graph. Experiments on three real-world datasets demonstrate that GP significantly reduces retraining time while improving attack effectiveness when integrated with state-of-the-art attack methods.

## Method Summary
Gradient Passing accelerates surrogate retraining by exploiting the cascading effects of representation changes in the user-item interaction graph. When a fake user is injected, their interacted item's representation changes to maximize similarity, which propagates through the graph affecting other users. GP explicitly passes gradients between these interacted pairs during backpropagation, approximating this effect. The technique constructs a gradient passing matrix from the difference between gradient matrices at consecutive iterations, enabling a single update to achieve effects comparable to multiple original training iterations. GP is implemented with configurable thresholds and weights, and demonstrates strong generalizability when combined with pre-training and sampling strategies.

## Key Results
- Reduces retraining time by 26-43% compared to standard methods
- Improves attack effectiveness by 18-177% when integrated with state-of-the-art attacks
- Achieves 29.57-177.21% improvement in attack success rate over original methods
- Demonstrates strong generalizability across different datasets and attack configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient similarity between interacted user-item pairs enables GP to accelerate surrogate retraining
- Mechanism: When a fake user is injected, their interacted item's representation changes to maximize similarity. This change propagates through the interaction graph, affecting other users who interacted with that item. GP explicitly passes gradients between these interacted pairs during backpropagation, approximating this cascading effect and accelerating convergence.
- Core assumption: The similarity between interacted user-item pairs' gradients is high enough during training to justify explicit gradient passing
- Evidence anchors:
  - [abstract] "a change in the representation of one user/item will cause a cascading effect through the user-item interaction graph"
  - [section 5.4.1] "There is a clear difference between the similarity of interacted pairs and random ones. During early training, gradient similarity among interacted pairs initially rises exceeding 0.5"
  - [corpus] Weak - no direct corpus evidence found

### Mechanism 2
- Claim: GP approximates multiple training iterations with a single update
- Mechanism: The theoretical analysis shows that one training iteration using the gradient passing matrix (ùë®GP) can reach the state after two iterations with original gradients. This is achieved by constructing ùë®GP from the difference between ùë®grad at consecutive iterations.
- Core assumption: The gradient passing matrix can be approximated without computing future states
- Evidence anchors:
  - [abstract] "With just a single update, GP can achieve effects comparable to multiple original training iterations"
  - [section 4.2] "Proposition 4.2. There exists a gradient passing matrix ùë®GP... a single iteration using ùë®GP‚àáùëπ Lrec can reach the state after two iterations with the original gradients ‚àáùëπ Lrec"
  - [corpus] Weak - no direct corpus evidence found

### Mechanism 3
- Claim: GP enables closer approximation of surrogate to victim recommender
- Mechanism: By accelerating convergence, GP allows the surrogate recommender to more closely match the victim's behavior in fewer epochs. This improved approximation provides better guidance for optimizing fake users, leading to more effective attacks.
- Core assumption: A more accurate surrogate model leads to better attack optimization
- Evidence anchors:
  - [abstract] "Under the same number of retraining epochs, GP enables a closer approximation of the surrogate recommender to the victim"
  - [section 5.4.2] "Surrogate recommenders trained with GP consistently achieve higher similarity than the original ones under same epochs"
  - [corpus] Weak - no direct corpus evidence found

## Foundational Learning

- Concept: Matrix factorization for recommender systems
  - Why needed here: The paper focuses on two-tower CF models where users and items are represented as vectors in a shared embedding space
  - Quick check question: How does the dot product similarity between user and item embeddings relate to recommendation scores?

- Concept: Backpropagation and gradient computation
  - Why needed here: GP operates during backpropagation by explicitly passing gradients between interacted pairs, requiring understanding of how gradients flow through the network
  - Quick check question: What information does the gradient vector contain about how to update parameters to minimize loss?

- Concept: Graph neural networks and message passing
  - Why needed here: The paper draws inspiration from GNN message-passing but applies it to gradients in the backward pass rather than features in the forward pass
  - Quick check question: How does message passing in GNNs differ from standard neural network forward propagation?

## Architecture Onboarding

- Component map: Data ‚Üí Model forward pass ‚Üí Loss computation ‚Üí Original gradients ‚Üí GP gradient modification ‚Üí Parameter update ‚Üí Repeat
- Critical path: The GP layer sits between loss computation and parameter update, modifying gradients based on the interaction graph structure
- Design tradeoffs: GP adds computational overhead for message-passing but reduces total training epochs needed. The threshold parameters ùúâ control selectivity of gradient passing.
- Failure signatures: If GP threshold is too low, all gradients get passed indiscriminately; if too high, no gradients get passed. If weight parameters ùõº are too large, updates become unstable.
- First 3 experiments:
  1. Verify gradient similarity between interacted pairs exceeds threshold during early training
  2. Test GP on simple synthetic dataset with known interaction patterns
  3. Compare convergence speed with and without GP on small real dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Gradient Passing's effectiveness vary across different types of recommender systems beyond collaborative filtering, such as sequential recommendation or content-based filtering?
- Basis in paper: [explicit] The authors mention extending GP to other tasks like sequential recommendation as future work, suggesting current evaluation is limited to CF models.
- Why unresolved: The paper focuses on two-tower CF models and does not test GP on other recommendation paradigms or architectures.
- What evidence would resolve it: Experimental results showing GP's performance on sequential recommenders, content-based systems, or hybrid models across multiple datasets.

### Open Question 2
- Question: What is the theoretical upper bound on attack effectiveness improvement when using GP compared to standard retraining methods?
- Basis in paper: [inferred] The authors prove GP can approximate the effect of multiple iterations in one step, but do not establish a mathematical limit on performance gains.
- Why unresolved: While empirical results show significant improvements (29.57-177.21%), the theoretical maximum speedup or effectiveness ceiling is not quantified.
- What evidence would resolve it: Mathematical proofs establishing convergence bounds for GP-enhanced attacks, or empirical results showing diminishing returns at different hyperparameter settings.

### Open Question 3
- Question: How does Gradient Passing affect the robustness of recommender systems to other types of attacks, such as inference attacks or adversarial examples?
- Basis in paper: [explicit] The authors mention that robust defense methods like MF-APR do not consistently enhance resistance against attacks because they target parameter perturbation rather than data poisoning.
- Why unresolved: The paper focuses solely on data poisoning attacks and does not investigate whether GP introduces vulnerabilities to other attack vectors.
- What evidence would resolve it: Experiments testing GP-enhanced recommenders against inference attacks, adversarial examples, or membership inference attacks, comparing success rates to standard training.

## Limitations
- Theoretical foundation relies on strong assumptions about gradient similarity patterns that may not hold for all recommender architectures
- Hyperparameter sensitivity analysis is limited to a small grid search, potentially missing optimal configurations
- Generalizability claims for GP with pre-training and sampling strategies are based on limited ablation studies

## Confidence
**High Confidence:** The empirical results showing GP reduces retraining time (26-43%) and improves attack effectiveness (18-177%) are well-supported by extensive experiments across three datasets and multiple victim models.

**Medium Confidence:** The theoretical proposition that GP approximates multiple training iterations with a single update is mathematically sound but relies on assumptions about gradient similarity that are validated empirically rather than proven rigorously.

**Low Confidence:** The generalizability claims for GP when combined with pre-training and sampling strategies are based on limited ablation studies without sufficient evidence of synergistic effects.

## Next Checks
1. **Gradient Similarity Robustness Test:** Verify that the gradient similarity patterns observed in the paper hold across different recommendation architectures (e.g., Graph Neural Networks, Transformers) and dataset characteristics.

2. **Hyperparameter Sensitivity Analysis:** Conduct a more comprehensive grid search or Bayesian optimization over the GP hyperparameters (Œæ, Œ±, number of layers) to identify optimal configurations.

3. **Convergence Stability Analysis:** Monitor the training dynamics of GP-enhanced models to identify any convergence issues, particularly during early training phases where gradient similarity is changing rapidly.