---
ver: rpa2
title: Adversarial Prompt Distillation for Vision-Language Models
arxiv_id: '2411.15244'
source_url: https://arxiv.org/abs/2411.15244
tags:
- adversarial
- robustness
- prompt
- teacher
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of large pre-trained Vision-Language
  Models (VLMs) like CLIP to adversarial attacks, particularly image-based attacks.
  To enhance robustness without relying on robustly pre-trained backbones, the authors
  propose Adversarial Prompt Distillation (APD), a bimodal defense method that jointly
  optimizes visual and textual prompts while leveraging knowledge distillation from
  a clean teacher CLIP model.
---

# Adversarial Prompt Distillation for Vision-Language Models

## Quick Facts
- arXiv ID: 2411.15244
- Source URL: https://arxiv.org/abs/2411.15244
- Reference count: 40
- Key outcome: Bimodal adversarial prompt distillation achieves 3.13-3.58% higher robustness than baselines across 11 benchmark datasets

## Executive Summary
This paper addresses the vulnerability of large pre-trained Vision-Language Models (VLMs) like CLIP to adversarial attacks, particularly image-based attacks. The authors propose Adversarial Prompt Distillation (APD), a bimodal defense method that jointly optimizes visual and textual prompts while leveraging knowledge distillation from a clean teacher CLIP model. APD employs an online distillation strategy, where the teacher processes clean images and receives feedback from the student to improve alignment. Experiments on 11 benchmark datasets show that APD outperforms existing adversarial prompt tuning methods in both white-box and black-box robustness, as well as domain shift scenarios.

## Method Summary
APD improves adversarial robustness of non-robust pre-trained VLMs by jointly optimizing visual and textual prompts through online knowledge distillation. The method uses two identical CLIP models (teacher and student) where the teacher processes clean images while the student handles adversarial examples. Both models simultaneously fine-tune their prompts during training, with the teacher receiving feedback from the student via KL divergence loss. The approach generates adversarial examples on-the-fly using PGD and employs 16-shot few-shot learning for 50 epochs.

## Key Results
- APD achieves 3.13-3.58% higher robustness than the best baseline across different CLIP backbones
- Bimodal prompt optimization (visual + textual) provides better defense than unimodal approaches
- Online distillation outperforms offline distillation variants in both clean accuracy and adversarial robustness
- APD maintains effectiveness across diverse domain-shifted datasets (ImageNet-A, ImageNet-R, ImageNet-Sketch, ImageNet-V2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bimodal prompt optimization (visual + textual) improves adversarial robustness more than unimodal approaches.
- Mechanism: Joint optimization of both visual and textual prompts allows the model to correct adversarial perturbations in both modalities simultaneously, leveraging cross-modal alignment to recover from attacks that target either modality individually.
- Core assumption: Cross-modal alignment in CLIP is essential for maintaining classification accuracy under adversarial attacks, and optimizing prompts in both modalities provides complementary defenses.
- Evidence anchors:
  - [abstract] "APD optimizes prompts for both visual and textual modalities while distilling knowledge from a clean teacher CLIP model"
  - [section 3.2] "APD is a bimodal defense approach that jointly optimizes visual and textual prompts"
  - [corpus] Weak - only general CLIP knowledge; no direct evidence for bimodal prompt optimization effectiveness
- Break condition: If cross-modal alignment is not essential for the task or if one modality dominates the decision boundary, bimodal optimization may provide minimal benefit over unimodal approaches.

### Mechanism 2
- Claim: Knowledge distillation from a clean teacher model improves both clean accuracy and adversarial robustness of the student model.
- Mechanism: The clean teacher provides high-quality soft labels that guide the student to learn robust features while maintaining generalization capability. The teacher's feedback during online distillation helps the student correct its alignment under adversarial perturbations.
- Core assumption: A clean (non-robust) teacher can effectively transfer knowledge to improve student robustness without requiring the teacher to be adversarially trained itself.
- Evidence anchors:
  - [abstract] "APD employs knowledge distillation from a clean teacher CLIP model to improve the adversarial robustness and clean accuracy of the student CLIP model"
  - [section 3.2] "APD employs an online distillation strategy that simultaneously fine-tunes the teacher and student models"
  - [corpus] Weak - only general knowledge distillation concepts; no direct evidence for clean teacher improving robustness
- Break condition: If the teacher's soft labels are unreliable under the attack scenarios considered, or if the student model cannot effectively leverage the teacher's feedback, distillation may degrade rather than improve performance.

### Mechanism 3
- Claim: Online distillation (simultaneous fine-tuning of teacher and student) is more effective than offline distillation.
- Mechanism: By updating the teacher based on feedback from the student during training, the teacher remains student-aware and can provide more relevant guidance. This creates a dynamic feedback loop that better bridges the teacher-student gap compared to using a fixed pre-trained teacher.
- Core assumption: Teacher models benefit from being updated during distillation to better align with the student's learning process and attack scenarios.
- Evidence anchors:
  - [section 3.2] "APD employs an online distillation strategy that simultaneously fine-tunes the teacher and student models"
  - [section 5] "Our online approach enables the teacher model to be student-aware during distillation, bridging the teacher-student gap"
  - [corpus] Weak - only general distillation concepts; no direct evidence comparing online vs offline distillation for adversarial robustness
- Break condition: If the computational overhead of online distillation outweighs the benefits, or if the teacher model cannot effectively adapt to the student's needs during training.

## Foundational Learning

- Concept: Adversarial examples and adversarial training
  - Why needed here: The paper addresses robustness against adversarial attacks, which are inputs specifically crafted to fool the model. Understanding how adversarial examples work and how adversarial training defends against them is fundamental to grasping the paper's approach.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

- Concept: Knowledge distillation
  - Why needed here: APD uses knowledge distillation from a clean teacher model to improve the student's robustness and accuracy. Understanding the mechanics of knowledge distillation is crucial for understanding how APD works.
  - Quick check question: In knowledge distillation, what is the purpose of using soft labels from the teacher instead of hard labels from the dataset?

- Concept: Vision-Language Models (VLMs) and cross-modal alignment
  - Why needed here: CLIP is a VLM that aligns visual and textual representations in a shared embedding space. Understanding how VLMs work and the importance of cross-modal alignment is essential for understanding why bimodal prompt optimization is effective.
  - Quick check question: How does CLIP's contrastive learning objective encourage alignment between visual and textual representations?

## Architecture Onboarding

- Component map:
  Student CLIP model -> generates logits on adversarial images
  Teacher CLIP model -> generates logits on clean images
  Adversarial example generator -> creates perturbed images using PGD
  Loss functions -> cross-entropy for teacher, KL divergence for both

- Critical path:
  1. Generate adversarial examples for student using PGD
  2. Compute logits for both student (on adversarial examples) and teacher (on clean examples)
  3. Compute losses (cross-entropy for teacher, KL divergence for both teacher-student alignment)
  4. Update prompts for both teacher and student using gradients
  5. Repeat for each training batch

- Design tradeoffs:
  - Online vs offline distillation: Online distillation requires more computation but allows the teacher to adapt to the student's learning process
  - Number of PGD steps: More steps create stronger attacks but increase computation time
  - Prompt length and depth: Longer/deeper prompts provide more representation capacity but may require more data to train effectively
  - Hyperparameter β: Balances clean accuracy optimization vs student feedback incorporation

- Failure signatures:
  - Poor clean accuracy: Teacher may be receiving too much feedback from the student (β too high), causing it to lose clean accuracy
  - Poor adversarial robustness: Student may not be learning effective defenses, or teacher feedback may be unreliable under attack
  - Mode collapse: If either model overfits to one modality or fails to learn cross-modal alignment

- First 3 experiments:
  1. Ablation study comparing unimodal (text-only or visual-only) vs bimodal prompt optimization to validate the bimodal defense claim
  2. Comparison of online vs offline distillation variants to demonstrate the benefit of the online approach
  3. Sensitivity analysis of the hyperparameter β to understand the tradeoff between clean accuracy and adversarial robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of APD vary with different CLIP vision backbone architectures beyond ViT-B/16 and ViT-L/14?
- Basis in paper: [explicit] The paper evaluates APD on CLIP models with ViT-B/16 and ViT-L/14 backbones, showing consistent improvements. It mentions CLIP variants with different vision components.
- Why unresolved: The paper does not test APD on other CLIP architectures (e.g., ResNet-based, Swin Transformer) or compare its performance across a broader range of vision backbones.
- What evidence would resolve it: Experiments evaluating APD on additional CLIP architectures with varying model sizes and vision backbone types, comparing performance metrics across these variants.

### Open Question 2
- Question: What is the impact of using a robustly pre-trained teacher model versus a standard pre-trained teacher model in APD?
- Basis in paper: [explicit] The paper states that APD uses a clean (non-robust) teacher model and validates that using a non-robust teacher can improve robustness. It contrasts this with prior work that relies on robust teachers.
- Why unresolved: The paper does not empirically compare APD's performance when using a robustly pre-trained teacher versus a standard pre-trained teacher, leaving the trade-offs unclear.
- What evidence would resolve it: Comparative experiments using APD with both robustly pre-trained and standard pre-trained teacher models, measuring differences in robustness, clean accuracy, and computational efficiency.

### Open Question 3
- Question: How does APD perform against more sophisticated or adaptive adversarial attacks beyond PGD, AutoAttack, and TI-FGSM?
- Basis in paper: [explicit] The paper evaluates APD against PGD, AutoAttack, and TI-FGSM but does not test it against adaptive attacks specifically designed to circumvent knowledge distillation defenses.
- Why unresolved: Adversarial attacks are rapidly evolving, and there is no guarantee that APD's defense mechanisms will remain effective against future, more sophisticated attack strategies.
- What evidence would resolve it: Testing APD against adaptive adversarial attacks that are specifically designed to exploit or bypass knowledge distillation mechanisms, including white-box attacks that have access to the defense strategy.

### Open Question 4
- Question: What is the computational overhead of APD compared to other adversarial defense methods, particularly in real-time applications?
- Basis in paper: [inferred] The paper does not discuss computational costs or inference latency of APD compared to baseline methods.
- Why unresolved: While APD shows improved robustness, its computational efficiency relative to other defense methods is not addressed, which is critical for practical deployment in resource-constrained or real-time scenarios.
- What evidence would resolve it: Benchmarking the inference time and computational resources required by APD versus other adversarial defense methods, including memory usage and throughput measurements under identical hardware conditions.

## Limitations
- Limited evaluation to only two CLIP architectures (ViT-B/16 and ViT-L/14)
- No analysis of computational overhead or inference latency compared to baseline methods
- Performance against adaptive adversarial attacks specifically designed to circumvent knowledge distillation is untested

## Confidence
- High: The bimodal prompt optimization approach is well-justified by cross-modal alignment principles
- Medium: The online distillation mechanism shows promise but needs more comparative studies
- Low: Claims about clean teacher effectiveness require more extensive validation across different model architectures

## Next Checks
1. Conduct ablation studies comparing online vs offline distillation across different teacher-student model pairs and attack scenarios
2. Test APD's effectiveness against adaptive attacks specifically designed to circumvent the bimodal defense mechanism
3. Evaluate the method's performance on larger, more diverse datasets to assess scalability and generalization limits