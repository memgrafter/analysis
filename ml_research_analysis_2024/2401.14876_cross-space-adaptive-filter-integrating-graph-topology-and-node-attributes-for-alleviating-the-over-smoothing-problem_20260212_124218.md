---
ver: rpa2
title: 'Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes
  for Alleviating the Over-smoothing Problem'
arxiv_id: '2401.14876'
source_url: https://arxiv.org/abs/2401.14876
tags:
- filter
- graph
- kernel
- node
- low-pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-Space Adaptive Filter (CSF) addresses the over-smoothing
  problem in deep Graph Convolutional Networks (GCNs) by integrating both graph topology
  and node attribute spaces. The core idea involves deriving a high-pass filter from
  node attributes interpreted as a minimizer of semi-supervised kernel ridge regression,
  and combining it with the conventional low-pass topology-based filter using a multiple-kernel
  learning approach.
---

# Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem

## Quick Facts
- arXiv ID: 2401.14876
- Source URL: https://arxiv.org/abs/2401.14876
- Reference count: 40
- Primary result: CSF achieves 0.21-16.41% accuracy gains on node classification by combining graph topology and node attributes to combat over-smoothing

## Executive Summary
Cross-Space Adaptive Filter (CSF) addresses the over-smoothing problem in deep Graph Convolutional Networks by integrating both graph topology and node attribute spaces through a novel high-pass filter derived from node attributes. The method constructs a high-pass filter from node attributes interpreted as a minimizer of semi-supervised kernel ridge regression, and combines it with the conventional low-pass topology-based filter using a multiple-kernel learning approach. This cross-space adaptive filter captures adaptive-frequency information across both spaces, significantly improving performance especially on disassortative graphs.

## Method Summary
CSF constructs a high-pass filter from node attributes by solving a semi-supervised kernel ridge regression problem and deriving an interpretable filter that shrinks low-frequency signals more than high-frequency ones. The topology-based low-pass filter is cast as a Mercer's kernel (ð¾ð‘¡ð‘œð‘ = ð¼ âˆ’ Ëœð¿). Both filters are combined using a squared matrix-based multiple-kernel learning approach: K = (ð¾ð‘Žð‘¡ð‘¡ð‘Ÿ + ð¾ð‘¡ð‘œð‘)/2 + ð›¾ (ð¾ð‘Žð‘¡ð‘¡ð‘Ÿ âˆ’ ð¾ð‘¡ð‘œð‘) (ð¾ð‘Žð‘¡ð‘¡ð‘Ÿ âˆ’ ð¾ð‘¡ð‘œð‘). The combined kernel is used in a propagation rule with residual connection: ð»ð‘˜+1 = ðœŽ( Ë†ð·â»Â¹/Â² K Ë†ð·â»Â¹/Â² ð»ð‘˜ð‘Šð‘˜) âŠ• ð‘‹.

## Key Results
- CSF outperforms baselines by 0.21-16.41% in node classification accuracy across nine datasets
- Particularly effective on disassortative graphs where connected nodes have dissimilar attributes
- Maintains robustness to over-smoothing in deep GCNs (tested with 2, 5, 10, 20 layers)
- Shows 16.41% improvement on the Actor dataset (disassortative graph)

## Why This Works (Mechanism)

### Mechanism 1
CSF addresses over-smoothing by extracting high-frequency information from node attributes using a high-pass filter. The high-pass filter is derived from a Mercer's kernel through solving a semi-supervised kernel ridge regression problem. This filter shrinks low-frequency signals more than high-frequency ones, preserving node distinctiveness in deep layers. Core assumption: Node attribute correlations provide complementary high-frequency information not captured by graph topology alone.

### Mechanism 2
CSF unifies topology-based low-pass and attribute-based high-pass filters in Reproducing Kernel Hilbert Space (RKHS) using multiple-kernel learning. The topology-based low-pass filter is cast as a Mercer's kernel (ð¾ð‘¡ð‘œð‘ = ð¼ âˆ’ Ëœð¿). Both filters are combined using a squared matrix-based MKL: K = (ð¾ð‘Žð‘¡ð‘¡ð‘Ÿ + ð¾ð‘¡ð‘œð‘)/2 + ð›¾ (ð¾ð‘Žð‘¡ð‘¡ð‘Ÿ âˆ’ ð¾ð‘¡ð‘œð‘) (ð¾ð‘Žð‘¡ð‘¡ð‘Ÿ âˆ’ ð¾ð‘¡ð‘œð‘). Core assumption: Kernels from different spaces can be meaningfully combined to capture complementary frequency information.

### Mechanism 3
CSF outperforms baselines on disassortative graphs by leveraging attribute information when topology is unreliable. On disassortative graphs, connected nodes have dissimilar attributes or labels. CSF's attribute-based high-pass filter provides prior knowledge on node dissimilarity, while topology-based filters incorrectly push dissimilar nodes toward similar representations. Core assumption: Disassortative graphs have node attributes that are more reliable indicators of node labels than topology.

## Foundational Learning

- Concept: Semi-supervised kernel ridge regression
  - Why needed here: CSF's high-pass filter is derived as a minimizer of this optimization problem
  - Quick check question: How does kernel ridge regression differ from standard ridge regression?

- Concept: Mercer's kernel and Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: CSF casts both filters into kernel space to enable combination
  - Quick check question: What properties must a function satisfy to be a valid Mercer's kernel?

- Concept: Graph Fourier transform and spectral graph theory
  - Why needed here: Understanding how GCN's low-pass filter relates to the graph Laplacian's eigensystem
  - Quick check question: How does the normalized graph Laplacian relate to graph frequencies?

## Architecture Onboarding

- Component map: Input (Graph ðº, Adjacency ð´, Attributes ð‘‹) â†’ High-pass filter construction â†’ Low-pass filter construction â†’ Filter combination (MKL) â†’ Propagation with residual connection

- Critical path: Kernel construction â†’ Filter combination â†’ Propagation with residual connection

- Design tradeoffs:
  - Computational complexity: O(ð‘Â³) for kernel inversion vs. O(ð‘Â²) for vanilla GCN
  - Memory usage: Storing full kernel matrix vs. sparse adjacency matrix
  - Hyperparameter sensitivity: ð‘Ž2 and ð‘Ž3 control attribute filter strength

- Failure signatures:
  - Numerical instability in kernel inversion (ill-conditioned ð¾)
  - Over-regularization when ð‘Ž2 or ð‘Ž3 are too large
  - Under-performance on assortative graphs if attribute filter dominates

- First 3 experiments:
  1. Compare CSF performance with varying ð‘Ž2 values (0.1, 1, 10, 100) on Cora (assortative) and Actor (disassortative)
  2. Test CSF with different KNN neighbor counts (5, 10, 20, 50) on Pubmed
  3. Evaluate over-smoothing by increasing layer count (2, 5, 10, 20) and measuring accuracy degradation compared to GCN

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Computational complexity of kernel inversion (O(NÂ³)) may limit scalability to large graphs
- Assumes node attributes provide meaningful high-frequency information, which may not hold for all graph types
- Lacks extensive ablation studies to quantify individual component contributions

## Confidence
Confidence is **Medium** for the core claim that CSF alleviates over-smoothing through cross-space frequency integration. The theoretical derivation is mathematically sound, but empirical validation relies heavily on relative improvements over baselines without extensive ablation studies.

Confidence is **Medium** for the claim that CSF particularly excels on disassortative graphs. While accuracy improvements are shown, the paper doesn't thoroughly analyze failure modes on strongly assortative graphs where topology might be more reliable.

## Next Checks
1. **Ablation Study**: Remove the attribute-based high-pass filter component and evaluate performance degradation to quantify the contribution of cross-space integration versus pure topology-based methods.

2. **Scalability Test**: Implement CSF on graphs with >10,000 nodes using Nystrom approximation or other kernel approximation techniques to assess practical scalability and verify whether the claimed O(NÂ³) complexity is prohibitive.

3. **Attribute Quality Analysis**: Design experiments on graphs where node attributes are systematically degraded (noise injection, dimension reduction) to test the robustness of the high-pass filter when attribute quality varies.