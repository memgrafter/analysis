---
ver: rpa2
title: Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural
  Machine Translation
arxiv_id: '2402.18428'
source_url: https://arxiv.org/abs/2402.18428
tags:
- learning
- dcmcl
- mutual
- information
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap between autoregressive
  (AR) and non-autoregressive (NAR) neural machine translation (NMT) models. The authors
  propose a novel generic collaborative learning framework, DCMCL, which treats AR
  and NAR models as collaborators rather than teacher-student pairs.
---

# Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation

## Quick Facts
- arXiv ID: 2402.18428
- Source URL: https://arxiv.org/abs/2402.18428
- Reference count: 40
- This paper proposes DCMCL, a collaborative learning framework that treats AR and NAR models as collaborators rather than teacher-student pairs, achieving up to 1.38 and 2.98 BLEU score improvements for AR and NAR models respectively.

## Executive Summary
This paper addresses the performance gap between autoregressive (AR) and non-autoregressive (NAR) neural machine translation models by proposing a novel collaborative learning framework called DCMCL. Rather than treating AR and NAR models as teacher-student pairs, DCMCL treats them as collaborators that can mutually benefit from each other's contextual information. The framework incorporates token-level mutual learning and sequence-level contrastive learning to hierarchically leverage the bilateral contextual information between AR and NAR models. Extensive experiments on four widely used benchmarks demonstrate that DCMCL can simultaneously improve both AR and NAR models, outperforming current best-unified models.

## Method Summary
DCMCL is a generic collaborative learning framework that bridges the performance gap between AR and NAR neural machine translation models. It uses a shared Transformer encoder with separate AR and NAR decoders, training them jointly through multi-task learning. The key innovation is treating AR and NAR models as collaborators rather than teacher-student pairs. Token-level mutual learning transfers complementary contextual information by minimizing KL divergence between AR and NAR probability distributions at masked token positions. Sequence-level contrastive learning ensures semantic consistency by pulling together representations of AR and NAR outputs for the same source while pushing apart outputs for different sources. The framework optionally includes a hybrid teacher that fuses AR and NAR contextual information to provide richer training signals.

## Key Results
- DCMCL achieves up to 1.38 BLEU score improvement for AR models and 2.98 BLEU score improvement for NAR models on standard benchmarks
- Outperforms current best-unified models with up to 0.97 BLEU score improvements for both AR and NAR decoding
- Demonstrates consistent improvements across four widely used benchmarks: WMT14/WMT21 EN-DE, WMT16 EN-RO, IWSLT14 EN-DE, and IWSLT15 EN-VI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level mutual learning transfers complementary contextual information between AR and NAR models
- Mechanism: At each masked token position, AR and NAR models output probability distributions conditioned on different context sets. KL divergence minimization forces each model to align its predictions with the other's contextual view, effectively transferring bidirectional context knowledge from NAR to AR and refining NAR's predictions with AR's sequential coherence.
- Core assumption: The probability distributions produced by AR and NAR models at a given token position are informative about complementary aspects of the true target distribution, and KL divergence can effectively transfer this information.
- Evidence anchors:
  - [abstract]: "token-level mutual learning and sequence-level contrastive learning are adopted between AR and NAR models"
  - [section]: "The AR and NAR models are mutually optimized on the tokens that are predicted from the NAR model... For the masking strategy, we adopt the same strategies that are used in different NAR models."
  - [corpus]: Weak - no direct evidence of token-level KL divergence improving both AR and NAR; corpus mentions related "multi-view subset regularization" but not this exact mechanism.
- Break condition: If the token-level distributions are too dissimilar (e.g., due to fundamentally different modeling assumptions), KL minimization could lead to degenerate or conflicting gradients that harm both models.

### Mechanism 2
- Claim: Sequence-level contrastive learning ensures semantic consistency between AR and NAR outputs
- Mechanism: For each source sentence, AR and NAR model outputs form a positive pair in representation space, while outputs for other source sentences in the same batch form negative pairs. Contrastive loss pushes representations of the positive pair closer and pushes negative pairs apart, encouraging both models to generate semantically coherent outputs for the same source.
- Core assumption: Two different decoding strategies applied to the same source should produce outputs with similar semantic content, and this similarity can be captured by averaging hidden states and measuring cosine similarity.
- Evidence anchors:
  - [abstract]: "sequence-level contrastive learning can leverage the inter-sentence contextual information"
  - [section]: "We utilize the outputs of AR and NAR decoder for the same samples as the positive pair and randomly select outputs from other samples in the same batch as the negative pairs."
  - [corpus]: Weak - corpus includes "Contrastive Learning for Sentence Representation" but not specifically this cross-model contrastive setup.
- Break condition: If the AR and NAR outputs diverge too much semantically (e.g., due to mode collapse in NAR), the contrastive loss may become meaningless or even harmful.

### Mechanism 3
- Claim: Hybrid teacher stabilizes training by fusing AR and NAR contextual information
- Mechanism: At each token position, the hybrid teacher concatenates the AR and NAR hidden states and passes them through an MLP to produce a unified feature. This feature captures the union of contexts from both models. Regularizing both AR and NAR outputs to match the hybrid teacher provides a richer, more stable training signal than mutual learning alone.
- Core assumption: The concatenated AR+NAR hidden states contain strictly more contextual information than either alone, and the MLP can learn to fuse them into a useful representation for both models.
- Evidence anchors:
  - [section]: "the hybrid feature gathers the context information of both types of models, thus predicting words depending on the union context of both AR and NAR models"
  - [section]: "Since the contextual information of AR and NAR models is a subset of the hybrid teachers, it is able to provide more information to both types of models"
  - [corpus]: Weak - corpus mentions "mutual learning at sentence-level and token-level" but not this specific hybrid teacher approach.
- Break condition: If the AR and NAR hidden states are incompatible (e.g., different dimensionalities or semantic spaces), concatenation may produce noisy or uninterpretable features that harm training.

## Foundational Learning

- Concept: Kullback-Leibler divergence as a loss for probability distribution matching
  - Why needed here: DCMCL uses KL divergence to measure and minimize the difference between AR and NAR token probability distributions during token-level mutual learning.
  - Quick check question: Given two discrete probability distributions p and q over the same support, write the KL divergence formula and explain what it measures.

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: DCMCL uses a contrastive loss at the sequence level to pull together AR and NAR outputs for the same source while pushing apart outputs for different sources.
  - Quick check question: In contrastive learning, what is the role of negative samples, and how does the InfoNCE loss encourage the model to distinguish positive from negative pairs?

- Concept: Teacher-student knowledge distillation
  - Why needed here: DCMCL optionally uses a hybrid teacher that fuses AR and NAR features, similar to distillation where a student model is trained to match a teacher's output distribution.
  - Quick check question: In knowledge distillation, why might a soft target distribution (e.g., logits from a teacher) be more useful for training than a hard one-hot target?

## Architecture Onboarding

- Component map: Shared Encoder -> AR Decoder -> Output Layer, Shared Encoder -> NAR Decoder -> Output Layer, Optional Hybrid Teacher (MLP + Output Layer)
- Critical path:
  1. Encode source with shared encoder
  2. AR decoder generates predictions conditioned on previous tokens
  3. NAR decoder generates predictions conditioned on random masked subset
  4. Compute multi-task NLL losses for AR and NAR separately
  5. Compute token-level mutual learning KL losses on masked tokens
  6. Compute sequence-level contrastive losses between AR/NAR output representations
  7. (Optional) Compute hybrid teacher loss using fused features
  8. Backpropagate combined loss to update all parameters

- Design tradeoffs:
  - Shared vs separate encoders: Sharing reduces parameters but may force AR/NAR to share suboptimal representations; separate encoders allow specialization but increase model size.
  - Sinusoidal vs learnable pos emb: Sinusoidal ensures consistent positional encoding across AR/NAR but may be less flexible; learnable allows adaptation but may diverge between decoders.
  - Fixed vs random mask ratio: Fixed ratio provides stable training signal but may miss diverse contexts; random ratio exposes models to varied contexts but adds variance.

- Failure signatures:
  - Both AR and NAR performance degrade together: Likely issue with shared encoder or conflicting gradients from mutual learning.
  - NAR improves but AR degrades: Likely NAR context overwhelming AR's sequential bias; try reducing mutual learning weight.
  - AR improves but NAR degrades: Likely AR sequential bias corrupting NAR's bidirectional context; try reducing mutual learning weight or using hybrid teacher.
  - Training instability or divergence: Likely learning rate too high or loss terms unbalanced; try gradient clipping or adjusting λ_TML, λ_SCL.

- First 3 experiments:
  1. Train with only shared encoder and multi-task NLL (no mutual or contrastive learning) to verify baseline performance and check if sharing hurts either model.
  2. Add token-level mutual learning (no contrastive or hybrid) to see if KL divergence improves both models and identify any conflicts.
  3. Add sequence-level contrastive learning (with shared encoder) to verify if it helps when AR/NAR representations are aligned, and check for any degradation if representations are misaligned.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed DCMCL framework scale to extremely large datasets beyond the WMT21 data used in the experiments?
- Basis in paper: [explicit] The paper mentions that "our methods can scale to the large dataset with large models" and provides results on WMT21 DE-EN data, but does not extensively explore scalability to much larger datasets.
- Why unresolved: The paper only tests on one large dataset (WMT21), and does not provide insights into performance on datasets orders of magnitude larger or with different characteristics.
- What evidence would resolve it: Experiments on datasets significantly larger than WMT21 (e.g., 1B+ sentence pairs) with varying data distributions and training dynamics analysis.

### Open Question 2
- Question: What is the optimal balance between token-level mutual learning and sequence-level contrastive learning across different language pairs and domains?
- Basis in paper: [inferred] The paper uses fixed weights (λTML=0.5 for WMT14 EN-DE, 1 for others) but acknowledges that "the number of mutual learning tokens in Disco is more than in CMLM" and shows varying performance across datasets.
- Why unresolved: The paper does not explore systematic ablation of these weights across diverse language pairs or domains, nor does it investigate adaptive weighting strategies.
- What evidence would resolve it: Comprehensive experiments varying λTML and λSCL across multiple language pairs (e.g., low-resource vs high-resource, distant vs similar languages) with performance analysis.

### Open Question 3
- Question: How does DCMCL perform when integrated with pre-trained models like BERT or GPT-style architectures?
- Basis in paper: [explicit] The paper mentions that "NAR models also have the ability to enhance AR models" and references works that distill from bidirectional models, but does not test DCMCL with pre-trained architectures.
- Why unresolved: All experiments use Transformer-based architectures from scratch, leaving open whether DCMCL's benefits extend to pre-trained model fine-tuning scenarios.
- What evidence would resolve it: Experiments fine-tuning DCMCL with pre-trained encoders (BERT) or decoders (GPT-style) on translation tasks, comparing to standard fine-tuning approaches.

## Limitations
- The empirical evaluation focuses exclusively on BLEU, COMET, and ChrF scores, leaving open whether the claimed gains hold under other adequacy/fluency metrics or in low-resource settings
- The paper does not report sensitivity analyses to hyperparameter choices (λTML, λSCL, masking ratio) or explore the effect of alternative NAR architectures beyond CMLM
- No ablation studies isolate the individual contributions of token-level mutual learning, sequence-level contrastive learning, and hybrid teacher training

## Confidence
- **High confidence**: Claims about architecture design (shared encoder, separate decoders, joint training) and the use of multi-task NLL losses for AR and NAR models
- **Medium confidence**: Claims about the effectiveness of token-level mutual learning and sequence-level contrastive learning in improving both AR and NAR performance
- **Low confidence**: Claims about the hybrid teacher's ability to "provide more information" to both models by concatenating AR and NAR hidden states

## Next Checks
1. **Ablation study**: Train DCMCL variants with (a) only token-level mutual learning, (b) only sequence-level contrastive learning, and (c) only multi-task NLL (no collaboration). Compare BLEU scores to the full DCMCL model to isolate the contribution of each collaborative learning component.

2. **Hyperparameter sensitivity**: Perform a grid search over λTML and λSCL values for each dataset (e.g., {0.1, 0.5, 1.0, 2.0}). Report BLEU score trends to identify optimal settings and assess robustness to hyperparameter choice.

3. **Alternative NAR architectures**: Replace the CMLM NAR decoder with another NAR model (e.g., LevT or NAT) and evaluate whether DCMCL's collaborative learning framework still improves both AR and NAR performance. This checks the generality of the framework beyond a single NAR architecture.