---
ver: rpa2
title: Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5,
  GPT-4 and Bard
arxiv_id: '2402.14533'
source_url: https://arxiv.org/abs/2402.14533
tags:
- gpt-4
- gpt-3
- bard
- llms
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether different LLMs (GPT-3.5, GPT-4,
  Bard) exhibit distinct linguistic styles and whether these differences can be used
  for attribution. Using a linguistic analysis of 15,000 responses to diverse prompts,
  the researchers compare vocabulary, POS, dependency distributions, and sentiment
  across the three models.
---

# Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard

## Quick Facts
- arXiv ID: 2402.14533
- Source URL: https://arxiv.org/abs/2402.14533
- Authors: Ariel Rosenfeld; Teddy Lazebnik
- Reference count: 39
- Primary result: Distinct linguistic signatures enable 88% accurate attribution of texts to GPT-3.5, GPT-4, or Bard

## Executive Summary
This study systematically compares the linguistic output of three major LLMs—GPT-3.5, GPT-4, and Bard—across 15,000 responses to diverse prompts. Using quantitative linguistic analysis, the researchers identify significant differences in vocabulary size, part-of-speech usage, dependency structures, and sentiment patterns. These differences are sufficiently stable to enable high-accuracy attribution using a simple machine learning classifier. The work demonstrates that LLM-generated text carries identifiable stylistic fingerprints that can be leveraged for detection, attribution, and evaluation purposes.

## Method Summary
The researchers collected 15,000 responses from three LLMs (GPT-3.5, GPT-4, Bard) across diverse prompt categories including creative writing, scientific queries, social questions, and argumentative prompts. They performed quantitative linguistic analysis focusing on four feature sets: vocabulary characteristics (size, density, uniqueness), part-of-speech distributions, dependency structures, and sentiment analysis. An XGBoost classifier was trained on these linguistic features to attribute texts to their source model. Statistical comparisons were made between models to identify consistent linguistic patterns, and classification accuracy was measured to assess practical attribution capability.

## Key Results
- Bard responses are shorter with higher density and more diverse low-frequency POS usage compared to GPT models
- GPT-4 exhibits larger vocabulary size and higher density than GPT-3.5
- An XGBoost classifier using linguistic features achieves 88% accuracy in attributing texts to their LLM origin
- Significant differences in POS distributions and dependency patterns distinguish the three models

## Why This Works (Mechanism)
The observed linguistic differences arise from distinct training data, model architectures, and fine-tuning objectives across the three LLMs. Each model develops unique probabilistic distributions over word choices, syntactic constructions, and discourse patterns based on its training corpus and optimization process. These systematic biases manifest as measurable stylistic differences in vocabulary diversity, POS frequency distributions, and dependency structures. The differences are particularly pronounced in low-frequency linguistic features that reflect each model's unique exposure to and representation of rare linguistic constructions during training.

## Foundational Learning

- **Linguistic Feature Extraction**: Process of quantifying textual characteristics like vocabulary size, POS distributions, and dependency structures. Why needed: Provides measurable dimensions for comparing LLM outputs. Quick check: Ensure feature extraction correctly handles edge cases like punctuation and special characters.

- **Part-of-Speech Tagging**: Assigning grammatical categories (noun, verb, adjective, etc.) to words in text. Why needed: POS distributions reveal systematic differences in syntactic preferences between models. Quick check: Validate tagger accuracy on domain-specific vocabulary present in LLM outputs.

- **Dependency Parsing**: Analyzing grammatical relationships between words to identify syntactic structure. Why needed: Dependency patterns expose systematic differences in how models construct sentences. Quick check: Verify parser handles the long, complex sentences common in LLM responses.

- **Sentiment Analysis**: Quantifying emotional tone and affect in text. Why needed: Sentiment patterns may differ between models due to training data biases. Quick check: Test sentiment classifier on model-generated text to ensure it captures the intended affective dimensions.

- **Machine Learning Classification**: Using algorithms like XGBoost to predict model origin from linguistic features. Why needed: Validates whether observed differences are practically useful for attribution. Quick check: Perform cross-validation to ensure classifier generalizes beyond training data.

- **Statistical Significance Testing**: Methods for determining whether observed differences between models are meaningful. Why needed: Confirms that linguistic variations are systematic rather than random. Quick check: Verify appropriate statistical tests are used for each feature type and distribution.

## Architecture Onboarding

Component map: Prompt Collection -> Text Generation -> Linguistic Feature Extraction -> Statistical Analysis -> Classification -> Attribution

Critical path: The most critical sequence is Prompt Generation -> Text Generation -> Feature Extraction -> Classification. Each LLM must generate responses to the same prompts, which are then converted to linguistic features that feed into the attribution classifier. Delays or failures at any stage compromise the attribution accuracy.

Design tradeoffs: The study prioritizes breadth of prompt types over depth of individual conversations, enabling general attribution patterns but potentially missing conversation-specific stylistic markers. The choice of surface-level linguistic features provides interpretability but may miss deeper semantic or discourse-level distinctions. The use of a relatively simple XGBoost classifier balances interpretability with performance, though more complex models might achieve higher accuracy.

Failure signatures: Attribution accuracy drops significantly when prompts are too similar or when models are fine-tuned on similar domains. Classification performance degrades when responses are heavily edited or paraphrased, suggesting the model relies on surface-level markers. Statistical significance may be compromised if prompt diversity is insufficient to capture systematic linguistic differences.

First experiments:
1. Generate responses to identical prompts across all three models and verify consistent feature extraction
2. Train and evaluate the XGBoost classifier on a subset of the data to establish baseline attribution accuracy
3. Perform ablation studies by removing individual feature types to identify which contribute most to attribution performance

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses relatively short, structured prompts that may not capture all real-world LLM usage patterns
- Classification accuracy of 88% leaves room for improvement and potential false attributions
- The analysis focuses on surface-level linguistic features without deeper semantic or discourse-level examination
- Limited prompt diversity may not represent the full range of stylistic variations across different domains

## Confidence
- Confidence in observed linguistic differences: High (consistent statistical comparisons across multiple feature sets)
- Confidence in practical attribution utility: Medium (88% accuracy is promising but imperfect)
- Confidence in generalizability: Low (limited prompt types and potential for model updates to alter patterns)

## Next Checks
1. Test attribution robustness across longer, more naturalistic conversational contexts rather than short prompts
2. Evaluate model attribution accuracy when responses are paraphrased or edited to remove identifiable markers
3. Replicate the analysis after subsequent model updates to determine whether linguistic signatures persist through training iterations