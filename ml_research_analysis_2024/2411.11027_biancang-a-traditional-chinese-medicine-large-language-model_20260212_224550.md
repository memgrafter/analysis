---
ver: rpa2
title: 'BianCang: A Traditional Chinese Medicine Large Language Model'
arxiv_id: '2411.11027'
source_url: https://arxiv.org/abs/2411.11027
tags:
- medical
- chinese
- medicine
- language
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "BianCang is a traditional Chinese medicine (TCM) large language\
  \ model designed to address the challenges of TCM syndrome differentiation and diagnosis,\
  \ which are hindered by the unique theoretical framework of TCM and the scarcity\
  \ of specialized high-quality corpora. It employs a two-stage training process:\
  \ first, domain-specific knowledge is injected through continual pre-training using\
  \ extensive TCM and medical corpora, including real hospital records and the ChP-TCM\
  \ dataset; second, supervised fine-tuning aligns the model\u2019s knowledge with\
  \ targeted instructions."
---

# BianCang: A Traditional Chinese Medicine Large Language Model

## Quick Facts
- arXiv ID: 2411.11027
- Source URL: https://arxiv.org/abs/2411.11027
- Reference count: 40
- BianCang achieves 82.10% accuracy on TCMSD dataset using chain-of-thought inference

## Executive Summary
BianCang is a large language model specifically designed for Traditional Chinese Medicine (TCM) syndrome differentiation and diagnosis. The model addresses the challenges of TCM's unique theoretical framework and scarcity of specialized corpora through a two-stage training approach. By combining extensive TCM knowledge injection with supervised fine-tuning, BianCang demonstrates superior performance across 11 datasets spanning 4 tasks, outperforming existing open-source TCM and Chinese medical language models.

## Method Summary
BianCang employs a two-stage training process to build a TCM-specific language model. First, continual pre-training injects domain-specific knowledge by processing extensive TCM and medical corpora, including real hospital records and the ChP-TCM dataset derived from the Pharmacopoeia of the People's Republic of China. Second, supervised fine-tuning aligns the model's knowledge with targeted instructions using a high-quality, diverse instruction dataset. The approach uses Qwen2/2.5-7B or Qwen2.5-14B as foundational models and evaluates performance across syndrome differentiation, disease diagnosis, TCM examinations, and integrative medicine tasks.

## Key Results
- Achieves 82.10% accuracy on TCMSD dataset under chain-of-thought inference, surpassing baseline models by over 30 percentage points
- Demonstrates strong performance in TCM examinations and integrative medicine tasks, with results second only to the 670B DeepSeek-R1 model in some benchmarks
- Outperforms existing open-source TCM-specific and Chinese medical language models across 11 datasets spanning 4 tasks involving 31 models

## Why This Works (Mechanism)

### Mechanism 1
Two-stage training (continual pre-training + supervised fine-tuning) enables BianCang to achieve high syndrome differentiation and diagnosis accuracy. Continual pre-training injects extensive TCM-specific knowledge into the foundational model, while supervised fine-tuning aligns the model's internal knowledge with downstream task instructions, improving task-specific performance.

### Mechanism 2
Chain-of-thought (CoT) inference significantly improves BianCang's performance on TCM syndrome differentiation and diagnosis tasks. CoT reasoning allows the model to explicitly articulate its diagnostic reasoning process, leading to more accurate and consistent results, particularly for complex TCM tasks that require logical reasoning.

### Mechanism 3
BianCang's strong performance on integrated Chinese and Western medicine (CWM) exams demonstrates its ability to bridge TCM and modern medical knowledge. The model's training on both TCM and general medical corpora enables it to understand and apply both diagnostic paradigms, leading to superior performance on integrative medicine tasks.

## Foundational Learning

- **Concept: Continual pre-training**
  - Why needed here: To inject extensive TCM-specific knowledge into the foundational model before fine-tuning, addressing the knowledge gap that exists in general LLMs for TCM tasks
  - Quick check question: What is the primary purpose of continual pre-training in BianCang's training pipeline?

- **Concept: Supervised fine-tuning (SFT)**
  - Why needed here: To align the model's internal knowledge with downstream task instructions and improve task-specific performance on TCM syndrome differentiation and diagnosis
  - Quick check question: How does supervised fine-tuning differ from continual pre-training in BianCang's training approach?

- **Concept: Chain-of-thought (CoT) reasoning**
  - Why needed here: To improve performance on complex TCM tasks by allowing the model to explicitly articulate its diagnostic reasoning process
  - Quick check question: Why does BianCang show significant performance improvements under chain-of-thought inference compared to direct inference?

## Architecture Onboarding

- **Component map**: Foundational model (Qwen2/2.5-7B or Qwen2.5-14B) → Continual pre-training stage (injects TCM and medical knowledge) → Supervised fine-tuning stage (aligns knowledge with task instructions) → Evaluation pipeline (11 datasets across 4 tasks)
- **Critical path**: Pre-training → Fine-tuning → Evaluation
- **Design tradeoffs**: Model scale vs. performance (7B vs. 14B parameter models), training time vs. performance (longer training may improve results but increases cost), general knowledge vs. domain-specific knowledge (balance between broad capabilities and TCM expertise)
- **Failure signatures**: Poor performance on TCM-specific tasks indicates insufficient pre-training data or ineffective fine-tuning, overfitting to training data may reduce generalization to new cases, inconsistent performance between DI and CoT modes suggests knowledge alignment issues
- **First 3 experiments**: 1) Evaluate baseline Qwen2/2.5 performance on TCM tasks to establish knowledge gap, 2) Test continual pre-training impact by comparing pre-trained vs. non-pre-trained models on TCM tasks, 3) Assess fine-tuning effectiveness by comparing models with different fine-tuning strategies (one-stage vs. two-stage)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BianCang's performance on TCM syndrome differentiation and diagnosis compare to other large language models when evaluated using real-world clinical cases that were not included in the training data?
- Basis in paper: [explicit] The paper mentions evaluating BianCang on 11 datasets across 4 tasks involving 31 models, but does not specify whether these datasets include real-world clinical cases not seen during training
- Why unresolved: The paper does not provide details on whether the evaluation datasets contain real-world clinical cases that were not part of the training data, making it difficult to assess the model's generalization capabilities
- What evidence would resolve it: Providing results from an evaluation using real-world clinical cases that were not included in the training data would demonstrate the model's ability to generalize and perform well on unseen cases

### Open Question 2
- Question: What is the impact of different pre-training data compositions on BianCang's performance in TCM-specific tasks?
- Basis in paper: [explicit] The paper describes the pre-training data composition, including medical books, encyclopedias, literature, pharmacopoeia, medical records, and general domain corpora, but does not explore the impact of different compositions on performance
- Why unresolved: The paper does not provide an analysis of how different pre-training data compositions affect BianCang's performance in TCM-specific tasks, making it difficult to determine the optimal data mix
- What evidence would resolve it: Conducting experiments with different pre-training data compositions and comparing their impact on BianCang's performance in TCM-specific tasks would provide insights into the optimal data mix

### Open Question 3
- Question: How does BianCang's performance on TCM syndrome differentiation and diagnosis tasks compare to human experts in terms of accuracy and consistency?
- Basis in paper: [explicit] The paper mentions subjective evaluation by two hospital physicians specializing in TCM cardiology, but does not provide a direct comparison between BianCang's performance and human experts
- Why unresolved: The paper does not provide a quantitative comparison between BianCang's performance and human experts, making it difficult to assess the model's clinical relevance and potential impact
- What evidence would resolve it: Conducting a study that directly compares BianCang's performance on TCM syndrome differentiation and diagnosis tasks to human experts in terms of accuracy and consistency would provide insights into the model's clinical relevance and potential impact

## Limitations
- Data provenance concerns due to unclear preprocessing, anonymization procedures, and quality control measures for real hospital records and ChP-TCM dataset
- Limited transparency around baseline selection and dataset preprocessing reduces confidence in absolute performance claims
- Does not address whether BianCang's performance generalizes across different TCM theoretical frameworks or regional practices

## Confidence
- **High confidence**: The two-stage training methodology (continual pre-training + supervised fine-tuning) is well-established in the literature and the paper provides sufficient detail for implementation
- **Medium confidence**: The claimed performance improvements over baselines are supported by quantitative metrics, but the limited transparency around baseline selection and dataset preprocessing reduces confidence in absolute performance claims
- **Low confidence**: Claims about BianCang's ability to "bridge TCM and modern medical knowledge" lack empirical validation beyond exam-style benchmarks, which may not reflect clinical utility

## Next Checks
1. Reproduce pre-training corpus construction using publicly available TCM resources and evaluate the impact of corpus composition on model performance
2. Conduct an ablation study on training stages by training models with only pre-training, only fine-tuning, and both stages to quantify the contribution of each component
3. Test BianCang on TCM case data from hospitals or institutions not represented in the training data to assess generalization across different clinical practices