---
ver: rpa2
title: Induction Head Toxicity Mechanistically Explains Repetition Curse in Large
  Language Models
arxiv_id: '2505.13514'
source_url: https://arxiv.org/abs/2505.13514
tags:
- repetition
- induction
- heads
- head
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The repetition curse\u2014where large language models generate\
  \ endless repetitive token sequences\u2014has been a persistent problem, but its\
  \ underlying cause remained unclear. This paper identifies induction heads, a type\
  \ of attention mechanism used for in-context learning, as the primary driver of\
  \ this issue."
---

# Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models

## Quick Facts
- **arXiv ID**: 2505.13514
- **Source URL**: https://arxiv.org/abs/2505.13514
- **Reference count**: 7
- **Primary result**: Induction head dominance causes entropy collapse leading to repetition curse, mitigated by logarithmic scaling

## Executive Summary
This paper identifies induction heads as the primary mechanism behind the repetition curse in large language models, where models generate endless repetitive token sequences. The authors introduce "induction head toxicity" - the dominance of these heads in influencing model output logits, which suppresses other attention heads and enforces rigid pattern replication. Through causal intervention experiments using activation patching, they demonstrate that as induction heads dominate, token diversity collapses and repetitive loops emerge. They propose a dynamic scaling technique that reduces induction head influence as generation progresses using a logarithmic function, successfully preventing runaway repetition while maintaining model performance across multiple architectures.

## Method Summary
The methodology combines causal intervention experiments with activation patching to identify and measure the influence of induction heads during generation. The authors first partition attention heads into induction and non-induction categories based on their behavior in copy tasks, then measure the "toxicity ratio" τt to quantify when induction heads dominate output logits. To mitigate repetition, they apply logarithmic scaling to induction head outputs, dividing by log(t + c) where t is the generation step. This reduces the influence of induction heads over time while preserving their ability to perform in-context learning when needed.

## Key Results
- Induction heads become dominant during repetitive generation, causing entropy collapse and pattern continuation
- Logarithmic scaling of induction head outputs successfully prevents repetition while maintaining perplexity near baseline
- The toxicity ratio τt exceeding threshold γ=0.65 predicts when entropy collapse and repetition will occur

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Induction heads become dominant during repetitive generation, causing entropy collapse
- Mechanism: When induction heads match and promote repetitive patterns, they suppress contributions from other attention heads, creating a self-reinforcing cycle where pattern continuation becomes increasingly likely
- Core assumption: Induction heads are specialized for pattern matching and their dominance can be quantified through causal importance scores
- Evidence anchors:
  - [abstract] "we define the 'toxicity' of induction heads, which we define as their tendency to dominate the model's output logits during repetition, effectively excluding other attention heads"
  - [section 3.2] "Let τt denote the induction head toxicity ratio... We define the model experiences induction head toxicity at step t if τt ≥ γ"
  - [corpus] Weak - corpus papers discuss induction heads but don't directly address dominance during repetition
- Break condition: If other attention heads maintain sufficient contribution even when repetitive patterns are present, the toxicity mechanism would not manifest

### Mechanism 2
- Claim: The logarithmic scaling of induction head outputs preserves entropy and prevents runaway repetition
- Mechanism: By dividing induction head outputs by log(t + c), where t is generation step, the influence of these heads decreases over time, preventing their dominance from causing entropy collapse
- Core assumption: The causal influence of induction heads scales proportionally with their output magnitude
- Evidence anchors:
  - [abstract] "they propose a dynamic scaling technique that reduces the influence of induction heads as generation progresses, using a logarithmic function"
  - [section 4.1] "O(i,j)_t = Attn(i,j)(Q, K, V) · W^(i,j)_V / log(t + c)"
  - [corpus] Weak - corpus doesn't provide direct evidence for logarithmic scaling effectiveness
- Break condition: If induction head influence doesn't scale proportionally with output magnitude, or if the decay rate is inappropriate for maintaining performance

### Mechanism 3
- Claim: Once induction head toxicity begins, it propagates through subsequent steps via autoregressive generation
- Mechanism: The autoregressive nature means each token prediction influences the next, so if induction heads dominate at step t, the resulting distribution biases future steps toward continuing the pattern
- Core assumption: The autoregressive generation process creates positive feedback loops for dominant attention patterns
- Evidence anchors:
  - [section 3.2] "Lemma 3.4 (Toxicity Propagation). Once toxicity occurs (τt ≥ γ), subsequent steps satisfy E[τt+1] ≥ τt under the model's autoregressive generation process"
  - [section 3.2] "Proof. (Informal) Induction heads specialize in pattern completion. When dominant, their output logits lt bias the next token distribution pt+1 toward continuing existing patterns"
  - [corpus] Weak - corpus doesn't directly address autoregressive propagation of attention dominance
- Break condition: If the autoregressive process doesn't create sufficient feedback, or if other mechanisms can counteract the bias

## Foundational Learning

- **Concept**: Causal influence measurement through activation patching
  - Why needed here: The paper uses activation patching to quantify how much each attention head contributes to correct predictions, which is essential for identifying toxic induction heads
  - Quick check question: What does a high causal influence score I(i,j) indicate about an attention head's contribution?

- **Concept**: Attention head partitioning and identification
  - Why needed here: The paper partitions attention heads into induction and non-induction categories based on their behavior, requiring understanding of how to identify these specialized heads
  - Quick check question: How does the paper determine which attention heads are "induction heads"?

- **Concept**: Entropy calculation and its relationship to token diversity
  - Why needed here: The paper uses entropy of token distributions to measure diversity collapse during repetition, requiring understanding of information theory concepts
  - Quick check question: What happens to entropy when a model's output distribution becomes increasingly peaked on a single token?

## Architecture Onboarding

- **Component map**: Transformer architecture (attention heads, feedforward networks) -> Induction head identification pipeline (activation patching) -> Toxicity measurement framework (τt ratio) -> Logarithmic scaling mitigation mechanism
- **Critical path**: The critical path for understanding this paper involves: (1) identifying induction heads through activation patching, (2) measuring their toxicity ratio τt, (3) observing entropy collapse when τt exceeds threshold, (4) applying logarithmic scaling to mitigate toxicity
- **Design tradeoffs**: The logarithmic scaling preserves in-context learning capability while reducing repetition, but requires tuning the hyperparameter c; stronger scaling prevents repetition but may impair learning
- **Failure signatures**: If the mitigation fails, you'll see sustained high τt values (>0.65) with decreasing entropy; if over-mitigated, τt will be too low and model performance will degrade
- **First 3 experiments**:
  1. Replicate the induction head identification pipeline on a simple copy task to verify the causal patching methodology
  2. Measure τt values on both repetitive and non-repetitive prompts to confirm the toxicity threshold behavior
  3. Apply the logarithmic scaling with different c values to find the optimal balance between repetition control and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed mitigation strategies affect the model's ability to perform in-context learning tasks that legitimately require pattern recognition and repetition?
- Basis in paper: [inferred] The paper acknowledges that "selectively suppressing induction heads could inadvertently weaken in-context learning capabilities" and proposes mitigation strategies that reduce induction head influence during generation.
- Why unresolved: The paper does not provide empirical data on how the logarithmic descaling affects performance on standard in-context learning benchmarks or tasks that require legitimate pattern completion.
- What evidence would resolve it: Controlled experiments comparing model performance on in-context learning tasks with and without the induction head descaling applied, measuring both repetition control and task completion accuracy.

### Open Question 2
- Question: Does induction head toxicity manifest differently in encoder-decoder architectures compared to decoder-only transformers?
- Basis in paper: [inferred] The paper explicitly states that "our empirical evaluation is limited to decoder-only transformer architectures" and acknowledges uncertainty about whether findings extend to other architectures.
- Why unresolved: The study focuses exclusively on decoder-only models (Qwen2.5, LLaMA3, Gemma2), leaving the generalizability to other architectures unexplored.
- What evidence would resolve it: Applying the same analytical framework (induction head identification, toxicity measurement, and mitigation) to encoder-decoder models and comparing the patterns of repetition curse and induction head dominance.

### Open Question 3
- Question: What is the relationship between induction head toxicity and higher-level semantic repetition beyond token-level patterns?
- Basis in paper: [explicit] The limitations section states "our analysis focuses on repetition at the token level" and notes that "how induction head toxicity contributes to higher-level semantic repetition remains an open question."
- Why unresolved: The current framework measures toxicity through token-level causal influence and entropy, without examining whether the same mechanisms drive repetitive semantic structures or discourse-level patterns.
- What evidence would resolve it: Extending the causal analysis framework to measure the influence of induction heads on semantic coherence and topical consistency, potentially using semantic similarity metrics or discourse structure analysis.

## Limitations

- Analysis is primarily based on synthetic tasks and controlled experiments with limited evaluation on diverse real-world generation scenarios
- The logarithmic scaling approach demonstrates effectiveness but the optimal decay rate appears to be empirically chosen rather than theoretically derived
- The toxicity threshold γ=0.65 is introduced without comprehensive sensitivity analysis or theoretical justification for this specific value

## Confidence

**High Confidence:**
- The identification of induction heads as a mechanism for pattern completion and repetition in language models is well-established
- The causal intervention experiments demonstrating that induction head dominance correlates with entropy collapse and repetitive output
- The empirical effectiveness of logarithmic scaling in reducing repetition while maintaining performance

**Medium Confidence:**
- The generalization of induction head toxicity mechanism to explain all instances of repetition curse across different model architectures and tasks
- The claim that this represents a fundamental architectural limitation rather than a training or data issue
- The long-term stability of the mitigation approach across extended generation sequences

**Low Confidence:**
- The theoretical optimality of logarithmic scaling compared to other potential mitigation strategies
- The claim that induction head toxicity is the sole or primary driver of repetition curse without considering other potential mechanisms
- The scalability of the approach to extremely large models (beyond the tested range)

## Next Checks

1. **Cross-architecture validation**: Test the induction head toxicity mechanism and logarithmic scaling mitigation across a broader range of model architectures (different depths, widths, attention variants) to verify the universality of the proposed explanation.

2. **Real-world generation evaluation**: Apply the methodology to analyze repetition patterns in actual language generation tasks beyond synthetic copy tasks, including long-form generation scenarios where repetition typically manifests.

3. **Alternative scaling functions comparison**: Systematically compare logarithmic scaling against other decay functions (exponential, polynomial, adaptive) to determine whether the proposed approach is optimal or simply one of several viable solutions.