---
ver: rpa2
title: A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers
arxiv_id: '2411.02643'
source_url: https://arxiv.org/abs/2411.02643
tags:
- methods
- counterfactual
- explanations
- counterfactuals
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates five methods for generating counterfactual
  explanations for text classifiers, addressing the challenge of explaining black-box
  models like BERT. The methods tested include HotFlip, CLOSS, Polyjuice, FIZLE-naive,
  and FIZLE-guided, using two datasets (SST-2 and QNLI) and three evaluation metrics:
  label flip score (validity), mean normalized Levenshtein similarity (sparsity),
  and perplexity (plausibility).'
---

# A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers

## Quick Facts
- arXiv ID: 2411.02643
- Source URL: https://arxiv.org/abs/2411.02643
- Reference count: 40
- One-line primary result: White-box methods like CLOSS achieve high validity and sparsity while LLM-based methods like FIZLE produce more plausible and natural counterfactuals, with a trade-off between these objectives.

## Executive Summary
This study evaluates five methods for generating counterfactual explanations for text classifiers, addressing the challenge of explaining black-box models like BERT. The methods tested include HotFlip, CLOSS, Polyjuice, FIZLE-naive, and FIZLE-guided, using two datasets (SST-2 and QNLI) and three evaluation metrics: label flip score (validity), mean normalized Levenshtein similarity (sparsity), and perplexity (plausibility). Results show that white-box methods like CLOSS consistently achieve high validity and sparsity, while LLM-based methods (FIZLE variants) produce more plausible and natural counterfactuals. The study highlights a performance trade-off, with gradient-based methods excelling in validity and LLM methods in plausibility. It recommends combining these approaches to develop methods that achieve high validity, sparsity, and plausibility for generating high-quality counterfactual explanations.

## Method Summary
The study compares five counterfactual explanation methods for text classifiers across two datasets. A BERT-base binary classifier is fine-tuned on SST-2 (sentiment classification) and QNLI (natural language inference) datasets. The five methods evaluated are HotFlip (gradient-based token substitution), CLOSS (beam search with classifier gradients), Polyjuice (attribute-conditioned generation), FIZLE-naive (LLM-based without guidance), and FIZLE-guided (LLM-based with task-specific guidance). For each method, 1000 samples from each dataset are processed to generate counterfactual explanations, which are then evaluated using three metrics: label flip score for validity, mean normalized Levenshtein similarity for sparsity, and median perplexity for plausibility.

## Key Results
- White-box methods like CLOSS achieve consistently high validity scores (label flip score) by directly optimizing perturbations that flip the classifier's decision boundary
- LLM-based methods (FIZLE variants) produce more plausible and natural counterfactuals with lower perplexity scores compared to gradient-based methods
- A performance trade-off exists where gradient-based methods excel at validity while LLM methods excel at plausibility, with neither approach dominating across all metrics
- CLOSS consistently outperforms HotFlip in both validity and sparsity due to its beam search optimization strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based methods like CLOSS achieve high validity because they directly optimize perturbations that flip the classifier's decision boundary.
- Mechanism: By computing partial derivatives of the classifier's output with respect to each token, CLOSS identifies which tokens have the highest impact on the prediction and applies targeted substitutions to push the input across the decision boundary.
- Core assumption: The classifier's decision surface is smooth enough near the input that small, interpretable token changes can reliably induce a label flip.
- Evidence anchors:
  - [abstract] "white-box substitution-based methods are effective at generating valid counterfactuals that change the classifier's output"
  - [section 3.2] "CLOSS uses beam search to search through the space of possible substitutions for a valid counterfactual"

### Mechanism 2
- Claim: LLM-based methods like FIZLE-naive and FIZLE-guided produce high plausibility because they leverage instruction-following LLMs that generate natural text continuations.
- Mechanism: The LLM is prompted to produce text that is coherent and realistic by design, and it can rewrite sentences while preserving grammatical structure and context.
- Core assumption: The LLM's training distribution covers realistic text variants that satisfy the counterfactual constraint.
- Evidence anchors:
  - [abstract] "newer methods based on large language models (LLMs) excel at producing natural and linguistically plausible text counterfactuals"
  - [section 3.3] "FIZLE uses a custom system prompt to generate counterfactuals, eliminating the need for task-specific fine-tuning"

### Mechanism 3
- Claim: The trade-off between validity and plausibility arises because white-box methods optimize for the former while LLM methods optimize for the latter.
- Mechanism: White-box methods (e.g., CLOSS) focus on classifier gradients and minimal token changes, often sacrificing naturalness. LLM methods focus on fluent text, often sacrificing strict label flips.
- Core assumption: Optimizing for one metric (validity or plausibility) inherently conflicts with the other when using distinct mechanisms.
- Evidence anchors:
  - [abstract] "white-box methods... excel at generating valid counterfactuals... newer methods... excel at producing natural and linguistically plausible text counterfactuals but often fail to generate valid counterfactuals"
  - [section 5] "For the QNLI dataset... black-box LLM methods generate more natural and plausible text but do not always succeed in generating valid counterfactuals"

## Foundational Learning

- Concept: Counterfactual explanation definition and purpose
  - Why needed here: The entire evaluation is built around generating counterfactuals, so understanding the definition and motivation is foundational.
  - Quick check question: What is the mathematical formulation of a counterfactual explanation for a classifier?

- Concept: Evaluation metrics for text counterfactuals (LFS, similarity, perplexity)
  - Why needed here: The study uses three distinct metrics to assess validity, sparsity, and plausibility; knowing how they are computed is critical for interpreting results.
  - Quick check question: How is the label flip score computed, and what does it measure?

- Concept: BERT tokenization and WordPiece model behavior
  - Why needed here: HotFlip and CLOSS operate at the token level; understanding how BERT tokenizes text explains why certain substitutions succeed or fail.
  - Quick check question: Why do gradient-based methods use token-level perturbations rather than character-level changes in BERT?

## Architecture Onboarding

- Component map: Input sentence → tokenizer (WordPiece) → BERT encoder → classifier head → output label. Counterfactual generator (HotFlip/CLOSS/FIZLE) receives the sentence and classifier gradients (if white-box) or LLM prompt (if black-box), produces edited sentence, then classifier re-evaluates for validity.
- Critical path: Original sentence → counterfactual generation → classifier re-run → metric computation (LFS, similarity, perplexity). The bottleneck is counterfactual generation time, especially for beam search in CLOSS/HotFlip.
- Design tradeoffs: White-box methods are slower (gradient computation + beam search) but more valid; black-box LLM methods are faster but less reliable in validity. Tradeoff is between interpretability of method internals vs. quality of generated text.
- Failure signatures: Low LFS indicates failure to flip the label; high perplexity indicates unnatural text; low similarity indicates over-editing. If all metrics are low, the method may be fundamentally mismatched to the dataset or model.
- First 3 experiments:
  1. Run each method on a small held-out sample of SST-2 and verify that the classifier's output flips for white-box methods and that perplexity is reasonable for LLM methods.
  2. Compare similarity scores between HotFlip and CLOSS to confirm that gradient-based methods achieve higher sparsity.
  3. Test Polyjuice on QNLI to reproduce the formatting issue (missing [SEP] tokens) and confirm the need for post-processing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a unified method that combines the validity advantages of gradient-based approaches like CLOSS with the plausibility strengths of LLM-based methods like FIZLE?
- Basis in paper: [explicit] The authors explicitly recommend developing new methods that "combine the strengths of established gradient-based approaches and newer LLM-based techniques to generate high-quality, valid, and plausible text counterfactual explanations."
- Why unresolved: The paper demonstrates a clear performance trade-off where white-box methods excel at validity while LLM methods excel at plausibility, but does not propose a concrete solution to bridge this gap.
- What evidence would resolve it: A comparative study showing a new hybrid method achieving high scores across all three metrics (validity, sparsity, and plausibility) on multiple datasets.

### Open Question 2
- Question: What are the fundamental reasons why LLM-based methods like FIZLE perform poorly on certain datasets (e.g., QNLI) for generating valid counterfactuals?
- Basis in paper: [explicit] The authors note that "LLM-based methods generate plausible, low-perplexity counterfactuals for this dataset, these methods generally fail to achieve high validity scores" and suggest that "the importance of each token for the classifier's output cannot always be accurately determined without direct access to the classifier's gradients."
- Why unresolved: The paper identifies the problem but does not deeply investigate the specific linguistic or structural features of datasets that make them challenging for black-box methods.
- What evidence would resolve it: A systematic analysis comparing token importance estimation accuracy between gradient-based and LLM-based methods across different dataset types.

### Open Question 3
- Question: How does the performance of counterfactual explanation methods vary across different types of text classification tasks beyond sentiment analysis and natural language inference?
- Basis in paper: [inferred] The study only evaluates methods on SST-2 (sentiment classification) and QNLI (natural language inference), leaving open questions about generalizability to other NLP tasks.
- Why unresolved: The paper's conclusions about method effectiveness are limited to two specific dataset types, and the authors do not explore how these methods might perform on tasks like question answering, summarization, or topic classification.
- What evidence would resolve it: Comprehensive benchmarking of all five methods across a diverse set of NLP classification tasks with varying input lengths, vocabulary complexities, and decision boundaries.

## Limitations
- The evaluation focuses on binary classification tasks using BERT-base classifiers, limiting applicability to multi-class problems or other transformer architectures.
- The study relies on automated metrics rather than human evaluation of counterfactual quality, which may not fully capture interpretability or usefulness in practice.
- Conclusions about method effectiveness are limited to two specific dataset types (sentiment analysis and natural language inference), leaving generalizability questions unanswered.

## Confidence

- **High confidence**: White-box methods (CLOSS, HotFlip) achieve superior validity scores due to their direct optimization of classifier gradients - this mechanism is well-established and consistently observed across datasets.
- **Medium confidence**: LLM-based methods produce more plausible text because they leverage large language models' training on natural language - while supported by results, the specific impact of prompt engineering and model variations requires further investigation.
- **Low confidence**: The proposed performance trade-off between validity and plausibility represents a fundamental limitation rather than an artifact of current implementations - this claim assumes that combining gradient-based and LLM approaches cannot achieve both objectives simultaneously.

## Next Checks

1. Conduct human evaluation studies to assess whether automated metrics (LFS, similarity, perplexity) correlate with actual interpretability and usefulness of counterfactual explanations.
2. Test these methods on additional datasets including multi-class classification and different transformer architectures to evaluate generalizability beyond BERT-base binary classifiers.
3. Experiment with hybrid approaches that combine gradient-based optimization with LLM-generated text to determine whether the validity-plausibility trade-off can be overcome.