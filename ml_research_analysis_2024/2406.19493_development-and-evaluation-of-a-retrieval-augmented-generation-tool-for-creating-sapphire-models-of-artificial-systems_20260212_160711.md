---
ver: rpa2
title: Development and Evaluation of a Retrieval-Augmented Generation Tool for Creating
  SAPPhIRE Models of Artificial Systems
arxiv_id: '2406.19493'
source_url: https://arxiv.org/abs/2406.19493
tags:
- sapphire
- tool
- system
- systems
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Retrieval-Augmented Generation (RAG) tool
  for automatically generating SAPPhIRE models of artificial systems using Large Language
  Models. The tool uses Wikipedia as a knowledge source and employs hypothetical document
  embedding (HyDE) to improve retrieval of relevant context.
---

# Development and Evaluation of a Retrieval-Augmented Generation Tool for Creating SAPPhIRE Models of Artificial Systems

## Quick Facts
- arXiv ID: 2406.19493
- Source URL: https://arxiv.org/abs/2406.19493
- Reference count: 20
- Primary result: RAG tool achieves mean answer relevance of 0.86, groundedness of 0.81, and context relevance of 0.79 for SAPPhIRE model generation

## Executive Summary
This paper presents a Retrieval-Augmented Generation (RAG) tool that automatically generates SAPPhIRE models of artificial systems using Large Language Models. The tool retrieves relevant Wikipedia articles, embeds them into a vector database, and uses the retrieved context to ground LLM responses, reducing hallucinations. Tested on three systems (orifice plate, thermoelectric cooler, and solenoid valve), the tool achieved high automated quality scores across answer relevance, groundedness, and context relevance metrics. The approach uses Hypothetical Document Embedding (HyDE) to improve retrieval effectiveness and chain-of-thought prompting to help LLMs understand specialized ontology constructs.

## Method Summary
The tool takes a system name as input, retrieves relevant Wikipedia articles, and splits them into chunks (1024 characters with 256-character overlap) that are embedded into a ChromaDB vector store. For each SAPPhIRE construct, it generates hypothetical documents using GPT-4o with chain-of-thought prompting, then retrieves relevant context chunks using both the original query and hypothetical documents. Finally, it generates corrected SAPPhIRE model descriptions using GPT-3.5-turbo with grounding prompts. The tool is evaluated using the TruLens RAG Triad framework, measuring answer relevance, groundedness, and context relevance on a scale of 0-1. Experiments are repeated 10 times per system to assess consistency.

## Key Results
- Achieved mean answer relevance score of 0.86 (95% CI [0.83, 0.89]) across three test systems
- Moderate groundedness score of 0.81 (95% CI [0.77, 0.85]), with particular challenges for Parts and oRgans constructs
- Context relevance score of 0.79 (95% CI [0.76, 0.83]) indicating retrieved chunks are generally relevant to queries
- Demonstrated the potential of RAG to reduce hallucinations through grounding in reference knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding LLM outputs in reference documents.
- Mechanism: The tool retrieves relevant Wikipedia articles for a given system, embeds them into a vector database, and uses the retrieved context to correct the LLM's hypothetical responses, reducing hallucinations.
- Core assumption: The retrieved context contains accurate and relevant information that can correct or refine the LLM's initial output.
- Evidence anchors:
  - [abstract] "The tool achieved high answer relevance (mean 0.86, 95% confidence interval [0.83, 0.89]) and moderate scores for groundedness (mean 0.81, 95% CI [0.77, 0.85]) and context relevance (mean 0.79, 95% CI [0.76, 0.83]), demonstrating its potential for automating SAPPhIRE model creation while reducing hallucinations through grounding in reference knowledge."
  - [section] "However, the tool struggled to ground the response related to o Rgans and Parts. This difficulty likely stems from insufficient information in the Wikipedia articles concerning these SAPPhIRE constructs."
- Break condition: If the reference knowledge source lacks sufficient or accurate information about the target system, the grounding will fail, leading to low groundedness scores.

### Mechanism 2
- Claim: Hypothetical Document Embedding (HyDE) improves retrieval effectiveness for complex domain-specific queries.
- Mechanism: Instead of using the original query directly, HyDE generates a hypothetical document using an LLM, then uses this document to retrieve relevant chunks from the vector database, capturing document-to-document embedding similarity.
- Core assumption: The LLM-generated hypothetical document contains relevant terminology and concepts that align with the knowledge source, even if the document itself is not factually accurate.
- Evidence anchors:
  - [section] "researchers have shown that, by following the HyDE approach, we no longer rely on embedding similarity from a document to a query but seek document-to-document embedding similarity, resulting in superior retrieval results compared to using the query itself."
  - [corpus] Weak - the paper cites Gao et al. (2022) but does not provide direct experimental comparison data in this work.
- Break condition: If the LLM fails to generate a meaningful hypothetical document that captures the essence of the query, the HyDE approach will not improve retrieval.

### Mechanism 3
- Claim: Chain-of-thought prompting helps LLMs understand and generate appropriate content for specialized ontology constructs.
- Mechanism: The prompt template guides the LLM through intermediate reasoning steps specific to each SAPPhIRE construct before generating the final output, improving the relevance and structure of the generated content.
- Core assumption: Breaking down the generation task into structured reasoning steps helps the LLM overcome its limitations in understanding specialized domain terminology.
- Evidence anchors:
  - [section] "The results show the potential of our chain-of-thought prompting strategy (as shown in Fig. 4) in helping the LLM understand the SAPPhIRE constructs and generate appropriate outputs."
  - [section] "Despite setting the temperature to 0, we observed that the LLM tends to produce varied outputs in successive trials."
- Break condition: If the reasoning steps are not well-designed or do not align with how the LLM processes information, the approach may not improve output quality.

## Foundational Learning

- Concept: SAPPhIRE Model of Causality
  - Why needed here: The tool is designed specifically to generate structured descriptions of systems using this seven-level causal ontology (State-changes, Actions, Parts, Phenomena, Inputs, oRgans, Effects).
  - Quick check question: Can you list all seven SAPPhIRE constructs and explain what each represents in a simple system like a light bulb?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The tool uses RAG to ground LLM outputs in reference knowledge, reducing hallucinations and improving factual accuracy.
  - Quick check question: What are the three key components of a RAG system, and how do they work together to improve LLM responses?

- Concept: Vector Embeddings and Similarity Search
  - Why needed here: The tool splits Wikipedia content into chunks, embeds them into vectors, and performs similarity search to retrieve relevant context for each SAPPhIRE construct.
  - Quick check question: How does cosine similarity work in the context of vector embeddings, and why is it useful for information retrieval?

## Architecture Onboarding

- Component map: System name -> Wikipedia API -> Text extraction -> RecursiveCharacterTextSplitter -> OpenAIEmbeddings -> ChromaDB vector store -> HyDE generation -> Context retrieval -> GPT-3.5-turbo generation -> Output
- Critical path: Input -> Knowledge Source Embedding -> Hypothetical Document Embedding -> Context Retrieval -> Corrected Response Generation -> Output
- Design tradeoffs:
  - Wikipedia as knowledge source: Free and comprehensive but may lack depth on specialized topics (e.g., oRgans and Parts)
  - GPT-4o for hypothetical generation vs GPT-3.5-turbo for correction: Higher quality vs lower cost
  - Temperature=0 for deterministic output vs allowing some variation for creativity
- Failure signatures:
  - Low groundedness scores: Retrieved context does not support the generated response (knowledge gap or retrieval failure)
  - Low context relevance scores: Retrieved chunks are not relevant to the query (embedding or retrieval issue)
  - Inconsistent outputs across trials: Temperature settings or LLM model behavior
- First 3 experiments:
  1. Test with a simple system (e.g., "light bulb") where Wikipedia has comprehensive coverage to verify basic functionality
  2. Test with a system where Wikipedia has limited information (e.g., a specialized industrial component) to evaluate handling of knowledge gaps
  3. Test with modified prompts (e.g., removing chain-of-thought steps) to evaluate the impact on output quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the RAG tool vary when using different knowledge sources (e.g., technical databases, research papers) compared to Wikipedia?
- Basis in paper: [explicit] The paper mentions that Wikipedia was chosen as the knowledge source due to its comprehensive coverage and open API, but notes that "any structured or unstructured knowledge source can be used as D."
- Why unresolved: The paper only tested the tool using Wikipedia articles, leaving the performance on other knowledge sources unexplored.
- What evidence would resolve it: Comparative experiments testing the tool with various knowledge sources (technical databases, research papers, domain-specific repositories) and measuring the same quality metrics (answer relevance, groundedness, context relevance).

### Open Question 2
- Question: What is the impact of prompt engineering refinements on the groundedness scores for Parts and oRgans constructs?
- Basis in paper: [explicit] The paper identifies that the tool struggled to ground responses related to oRgans and Parts, suggesting this may be due to insufficient information in Wikipedia articles or ineffective context retrieval.
- Why unresolved: The paper mentions future work will focus on refining prompts and context retrieval but does not present any results from such refinements.
- What evidence would resolve it: Experiments comparing the current prompt design with optimized versions, measuring changes in groundedness scores specifically for Parts and oRgans constructs.

### Open Question 3
- Question: How does the tool's performance scale with increasing complexity of artificial systems?
- Basis in paper: [inferred] The paper tested the tool on three relatively simple systems (orifice plate, thermoelectric cooler, solenoid valve) but does not address how it performs with more complex systems.
- Why unresolved: The evaluation was limited to simple test cases, and the paper does not discuss performance thresholds or limitations for more complex systems.
- What evidence would resolve it: Systematic testing of the tool on artificial systems of increasing complexity (mechanical, electrical, hybrid systems) with corresponding quality metric evaluations.

### Open Question 4
- Question: What is the optimal chunk size and overlap for the text splitting process to maximize retrieval effectiveness?
- Basis in paper: [explicit] The paper mentions that chunks were set to 1024 characters with 256 character overlap, but states this was "arbitrarily set" without optimization.
- Why unresolved: The paper does not explore how different chunk sizes and overlaps affect retrieval quality or overall tool performance.
- What evidence would resolve it: Experiments varying chunk sizes and overlaps while measuring context relevance and groundedness scores to identify optimal parameters.

## Limitations

- Knowledge source dependency significantly impacts performance, particularly for specialized constructs where Wikipedia lacks depth
- Automated evaluation metrics may overestimate real-world performance compared to human expert validation
- Use of different LLM models (GPT-4o for HyDE, GPT-3.5-turbo for correction) introduces potential inconsistency

## Confidence

- High confidence: The tool successfully implements the described RAG architecture and achieves the reported automated evaluation scores
- Medium confidence: The tool reduces hallucinations compared to standard LLM approaches, but the extent varies by SAPPhIRE construct
- Medium confidence: The HyDE approach improves retrieval effectiveness compared to direct query embedding

## Next Checks

1. Conduct human expert evaluation of generated SAPPhIRE models for 10-15 additional systems across different domains to validate the automated metrics
2. Test the tool on systems with known Wikipedia coverage gaps to quantify the knowledge source dependency and identify performance thresholds
3. Implement a hybrid approach that falls back to alternative knowledge sources (technical manuals, patents) when Wikipedia coverage is insufficient