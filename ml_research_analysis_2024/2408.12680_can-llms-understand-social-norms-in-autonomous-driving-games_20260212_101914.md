---
ver: rpa2
title: Can LLMs Understand Social Norms in Autonomous Driving Games?
arxiv_id: '2408.12680'
source_url: https://arxiv.org/abs/2408.12680
tags:
- agents
- social
- driving
- norms
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) to
  understand and model social norms in autonomous driving scenarios. The authors introduce
  LLM-based agents into autonomous driving games, where they make decisions based
  on text prompts, to investigate the emergence of social norms among individual agents.
---

# Can LLMs Understand Social Norms in Autonomous Driving Games?

## Quick Facts
- arXiv ID: 2408.12680
- Source URL: https://arxiv.org/abs/2408.12680
- Reference count: 40
- Primary result: LLM-based agents can understand and model social norms in autonomous driving scenarios without explicit programming, demonstrating emergent behaviors like yielding at intersections and forming platoons.

## Executive Summary
This paper investigates whether large language models (LLMs) can understand and model social norms in autonomous driving scenarios through simulation games. The authors introduce LLM-based agents into autonomous driving games where they make decisions based on text prompts, exploring the emergence of social norms among individual agents. Using OpenAI's Chat API powered by GPT-4.0, the study simulates interactions in two driving scenarios: unsignalized intersections and highway platoons. The results demonstrate that LLM-based agents can handle dynamically changing environments in Markov games, with social norms evolving among agents in both scenarios. In intersection games, agents tend to adopt conservative driving policies when facing potential crashes, showcasing the potential of LLMs for modeling complex social behaviors in autonomous driving contexts.

## Method Summary
The study uses OpenAI Chat API with GPT-4.0 to simulate autonomous driving games where LLM-based agents make decisions based on textual prompts describing the game state and rules. The researchers implement turn-based Markov games with two scenarios: unsignalized intersections (9x9 grid) and highway platoons (2x9 grid). Agents receive prompts containing system messages (environment setup, rules, rewards) and user messages (observations from their perspective). The games run for 50 iterations per scenario with varying numbers of background vehicles. The team analyzes agent decisions, reward outcomes, and emergent behaviors to evaluate social norm formation, measuring metrics like crash rates, early stop frequencies, and platoon formation success.

## Key Results
- LLM-based agents successfully formed social norms in both unsignalized intersection and highway platoon scenarios without explicit programming
- In intersection games, agents adopted conservative driving policies, yielding to avoid crashes when faced with potential conflicts
- The turn-based simulation approach effectively prevented agents from accessing concurrent decisions by others while maintaining realistic interaction dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based agents can learn and follow emergent social norms in autonomous driving games without explicit programming
- Mechanism: Agents observe their environment through textual prompts and make decisions based on encoded game rules. Through repeated interactions, they converge on shared behavioral patterns that constitute social norms
- Core assumption: LLMs can internalize and apply abstract rules about driving behavior when provided with appropriate contextual information
- Evidence anchors:
  - [abstract] "social norms evolve among LLM-based agents in both scenarios"
  - [section] "The advantage of LLM-based agents in games lies in their strong operability and analyzability, which facilitate experimental design"
  - [corpus] Weak - corpus neighbors discuss social norms but don't specifically address autonomous driving applications
- Break condition: If prompts fail to capture essential driving rules or LLM's understanding of traffic conventions is insufficient, agents won't converge on consistent norms

### Mechanism 2
- Claim: Turn-based simulation approach allows agents to make decisions without knowledge of concurrent decisions by others
- Mechanism: Game state updates only after all agents have made decisions, preventing access to private observations of other agents and creating realistic decision-making environment
- Core assumption: Agents can make rational decisions based solely on their own observations without predicting others' actions
- Evidence anchors:
  - [section] "we simulate the traffic in a turn-based manner, where each turn represents a time step"
  - [section] "To further prevent access to concurrent decisions made by other agents, the game state is updated at the end of each time step"
  - [corpus] Weak - corpus neighbors don't discuss turn-based decision making in multi-agent systems
- Break condition: If agents require knowledge of others' decisions for optimal choices, turn-based approach may lead to suboptimal or inconsistent behavior

### Mechanism 3
- Claim: Reward structure design influences emergence and strength of social norms in driving scenarios
- Mechanism: Adjusting rewards associated with different actions influences agent behavior and norm formation
- Core assumption: LLMs can optimize decision-making to maximize cumulative rewards, leading to emergent behavioral patterns
- Evidence anchors:
  - [section] "We then investigate the crash cases... To eliminate the early stop behaviors and shorten the waiting time of vehicles, we increase the reward of Go action from -2 to 0"
  - [section] "The average number of early stops for the red car decreases from 1.42 to 0.48, suggesting that reward design can influence driving behavior"
  - [corpus] Weak - corpus neighbors discuss reward design but not specifically in autonomous driving contexts
- Break condition: If reward structure becomes too complex or relationship between rewards and desired behaviors is unclear, agents may fail to develop appropriate norms

## Foundational Learning

- Concept: Markov Games
  - Why needed here: Used to model dynamic interactions among agents in uncertain driving environments
  - Quick check question: How do Markov Games extend the concept of Markov Decision Processes to multi-agent settings?

- Concept: Large Language Models and Prompt Engineering
  - Why needed here: LLMs make decisions in driving games, with effectiveness depending on how well prompts capture game rules and observations
  - Quick check question: What are the key components of a prompt that effectively guides an LLM's decision-making in a game scenario?

- Concept: Social Norms and Emergence
  - Why needed here: Study investigates whether social norms can emerge among LLM agents in driving scenarios
  - Quick check question: How do social norms differ from formal rules, and what conditions facilitate their emergence in multi-agent systems?

## Architecture Onboarding

- Component map: OpenAI Chat API (GPT-4.0) -> Prompt generator -> Game simulator -> Data logger -> Analysis module

- Critical path:
  1. Initialize game environment and agents
  2. Generate prompts for each agent based on current state
  3. Send prompts to OpenAI API and receive decisions
  4. Update game state based on agent decisions
  5. Log decisions and outcomes
  6. Repeat until game termination conditions met
  7. Analyze results for norm emergence

- Design tradeoffs:
  - Turn-based vs. concurrent decision making: Turn-based ensures fairness but may not capture all real-world dynamics
  - Prompt complexity vs. response quality: More detailed prompts may lead to better decisions but increase API costs and response time
  - Reward structure simplicity vs. behavioral nuance: Simple rewards are easier to implement but may not capture all aspects of driving behavior

- Failure signatures:
  - Inconsistent agent behavior across similar game states
  - Failure to converge on any consistent norms across multiple game runs
  - Unexpected crashes or deadlocks in the game environment
  - API response errors or timeouts during decision-making steps

- First 3 experiments:
  1. Run unsignalized intersection game with varying numbers of background vehicles (0-4) to observe norm formation and crash rates
  2. Test highway platoon scenario with different reward structures to examine impact on platoon formation and lane-changing behavior
  3. Conduct ablation studies by removing different components of prompts to identify critical elements for norm emergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does emergence of social norms differ between LLM-based agents and human drivers in complex, real-world autonomous driving scenarios?
- Basis in paper: [explicit] Paper suggests comparing LLM agents with human players in future work
- Why unresolved: Study focuses on simulated driving games without human participants for direct comparison
- What evidence would resolve it: Experiments with both LLM agents and human drivers in realistic driving simulations or real-world scenarios to observe and compare decision-making processes and adherence to social norms

### Open Question 2
- Question: How do varying levels of background vehicle presence influence emergence and stability of social norms in autonomous driving scenarios?
- Basis in paper: [explicit] Paper examines effect of background vehicles on emergence of social norms but doesn't explore how different levels might impact this process
- Why unresolved: Study only tests limited range of background vehicle scenarios
- What evidence would resolve it: Experiments with broader range of background vehicle scenarios to observe how different levels influence emergence and stability of social norms

### Open Question 3
- Question: Can framework for simulating strategic interactions using LLMs be generalized and applied to other domains beyond autonomous driving?
- Basis in paper: [explicit] Paper suggests exploring construction of unified framework for simulating strategic interactions using LLMs in future work
- Why unresolved: Study focuses on autonomous driving scenarios without investigating framework applicability to other domains
- What evidence would resolve it: Testing framework in various domains (economics, social interactions, environmental management) to assess generalizability and effectiveness in simulating strategic interactions

## Limitations

- Exact prompt templates and reward structures are not fully specified, potentially affecting reproducibility of observed norm emergence
- Limited discussion of edge cases where agents might fail to develop appropriate norms or where turn-based approach breaks down
- No comparison with traditional autonomous driving approaches or baseline agents to contextualize LLM performance

## Confidence

- **High**: LLMs can handle dynamic environments in Markov games and make decisions based on textual prompts
- **Medium**: Social norms evolve among LLM-based agents in both scenarios, though specific norms and their consistency require further validation
- **Medium**: Turn-based simulation approach effectively prevents agents from accessing concurrent decisions, but may not capture all real-world dynamics

## Next Checks

1. **Prompt Ablation Study**: Systematically remove components from prompts (e.g., reward information, observation details) to identify which elements are critical for norm emergence and consistent agent behavior

2. **Baseline Comparison**: Implement and test non-LLM agents (e.g., rule-based or reinforcement learning agents) in same scenarios to compare norm formation, crash rates, and overall performance

3. **Robustness Testing**: Evaluate agent performance and norm consistency across wider range of scenarios including adverse weather conditions, unexpected obstacles, and varying traffic densities to assess generalizability and identify failure modes