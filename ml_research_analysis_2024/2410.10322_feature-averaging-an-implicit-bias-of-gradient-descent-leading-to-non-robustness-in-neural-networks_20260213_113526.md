---
ver: rpa2
title: 'Feature Averaging: An Implicit Bias of Gradient Descent Leading to Non-Robustness
  in Neural Networks'
arxiv_id: '2410.10322'
source_url: https://arxiv.org/abs/2410.10322
tags:
- lemma
- have
- network
- data
- know
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies "Feature Averaging" as an implicit bias in
  gradient descent training that contributes to the lack of adversarial robustness
  in deep neural networks. The authors prove that, even when multiple discriminative
  features exist in the input data, gradient descent tends to learn an average of
  these features rather than distinguishing them individually.
---

# Feature Averaging: An Implicit Bias of Gradient Descent Leading to Non-Robustness in Neural Networks

## Quick Facts
- **arXiv ID**: 2410.10322
- **Source URL**: https://arxiv.org/abs/2410.10322
- **Reference count**: 40
- **Primary result**: Feature averaging in gradient descent causes non-robustness; fine-grained supervision mitigates this

## Executive Summary
This paper identifies "Feature Averaging" as an implicit bias in gradient descent training that contributes to the lack of adversarial robustness in deep neural networks. The authors prove that, even when multiple discriminative features exist in input data, gradient descent tends to learn an average of these features rather than distinguishing them individually. They analyze two-layer ReLU networks on multi-cluster data distributions and show that learned weights converge to feature-averaging solutions, making networks vulnerable to adversarial perturbations. The paper demonstrates that this vulnerability can be mitigated through more granular supervision, proving that training with feature-level labels enables networks to learn individual features and achieve optimal robustness.

## Method Summary
The paper analyzes two-layer ReLU networks trained on multi-cluster data distributions using gradient descent. The method involves generating synthetic datasets with orthogonal cluster features, training networks with binary versus cluster-level labels, and comparing weight convergence patterns and robustness outcomes. Experiments extend to MNIST and CIFAR-10 datasets, using either CLIP embeddings or ResNet architectures. The key innovation is demonstrating how fine-grained supervision changes the implicit bias of gradient descent from feature averaging to feature decoupling.

## Key Results
- Gradient descent learns weighted averages of cluster features rather than individual features, even when multiple discriminative features exist
- Feature averaging makes networks vulnerable to adversarial perturbations aligned with negative directions of averaged features
- Training with feature-level labels enables learning of decoupled features and achieves optimal robustness
- Experimental results on synthetic, MNIST, and CIFAR-10 datasets validate theoretical findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature averaging occurs when gradient descent training causes neurons to align their weight vectors with an average of cluster-center features rather than distinguishing them individually
- Mechanism: In multi-cluster data distributions, gradient descent dynamics drive each neuron's weight vector to converge toward a weighted average of the cluster features in its assigned class, creating a single discriminative direction per class
- Core assumption: Data distribution consists of multiple clusters with mutually orthogonal centers, and gradient descent runs long enough with small initialization and learning rate
- Evidence anchors:
  - [abstract]: "gradient descent tends to learn an average of these features rather than distinguishing them individually"
  - [section 4.1]: "the weight vector associated with each hidden neuron is a weighted average of feature vectors"
  - [corpus]: No direct evidence; this mechanism is primarily theoretical
- Break condition: If data distribution lacks orthogonal cluster features, or if fine-grained supervision is provided, the averaging effect diminishes

### Mechanism 2
- Claim: Feature averaging reduces the robust radius of the learned network by making it vulnerable to perturbations aligned with the negative direction of averaged features
- Mechanism: When weights represent averaged features, the decision boundary becomes approximately linear in perturbation space, making small adversarial perturbations in the negative direction of averaged features sufficient to flip classifications
- Core assumption: The robust network exists that uses individual features and achieves larger robust radius, but gradient descent fails to find it
- Evidence anchors:
  - [abstract]: "making the network vulnerable to input perturbations aligned with the negative direction of the averaged features"
  - [section 4.1]: "the learned solution is non-robust w.r.t. the radius Ω(p d/k), while there exist solutions with optimal robust radius O(√d)"
  - [corpus]: No direct evidence; theoretical construction
- Break condition: If averaged features are not discriminative or if perturbation direction doesn't align with feature averages

### Mechanism 3
- Claim: Providing fine-grained supervision (feature-level labels) enables gradient descent to learn decoupled features, achieving optimal robustness
- Mechanism: When trained with cluster-level labels instead of binary labels, gradient descent converges to a solution where each neuron learns an individual cluster feature, creating a more complex decision boundary that resists perturbations
- Core assumption: The multi-class network architecture can represent individual feature classifiers, and gradient descent dynamics support this learning regime
- Evidence anchors:
  - [abstract]: "training with feature-level labels enables the network to learn individual features and achieve optimal robustness"
  - [section 4.2]: "gradient descent converges to a neural network that learns decoupled features"
  - [corpus]: No direct evidence; theoretical and experimental results only
- Break condition: If supervision granularity is insufficient or network capacity cannot represent individual features

## Foundational Learning

- Concept: Multi-cluster data distributions with orthogonal cluster features
  - Why needed here: Theoretical analysis depends on clusters being orthogonal to ensure distinct feature directions and simplify gradient dynamics analysis
  - Quick check question: If cluster features are not orthogonal, how does this affect the feature averaging mechanism?

- Concept: Implicit bias of gradient descent in over-parameterized networks
  - Why needed here: Paper's core argument is that gradient descent has implicit bias toward averaging features rather than distinguishing them, a specific instance of implicit bias theory
  - Quick check question: What other implicit biases have been observed in gradient descent training?

- Concept: Robustness metrics and ℓ2-robust accuracy
  - Why needed here: Paper measures adversarial robustness using ℓ2-robust accuracy, which quantifies model's ability to maintain correct predictions under ℓ2-bounded perturbations
  - Quick check question: How does ℓ2-robust accuracy differ from other robustness metrics like ℓ∞?

## Architecture Onboarding

- Component map: Two-layer ReLU network with M hidden neurons, binary classification output, gradient descent optimization with logistic loss
- Critical path: Data generation → Network initialization → Gradient descent training → Analysis of weight convergence → Robustness evaluation
- Design tradeoffs: Simpler network architecture enables theoretical analysis but may not capture all real-world complexity; fine-grained supervision improves robustness but requires additional labeling effort
- Failure signatures: If feature averaging occurs, network will show similar activation patterns across different clusters; if robustness is poor, small perturbations in feature average directions will cause misclassification
- First 3 experiments:
  1. Generate synthetic multi-cluster data with k=10 clusters in d=3072 dimensions. Train two-layer ReLU network (m=5 hidden neurons per class) using 2-class labels and record learned weights. Repeat with 10-class labels and compare weight-feature correlations to verify feature averaging vs. feature decoupling
  2. For CIFAR-10, either (a) extract CLIP ViT-B-32 embeddings and train two-layer network with 2-class vs 10-class labels, or (b) train ResNet18 from scratch with 2-class vs 10-class labels. Measure robust accuracy under PGD attacks for both ℓ2 and ℓ∞ norms
  3. Visualize results by plotting matrices of average cosine similarities between each cluster feature and each weight vector, and plot robust accuracy curves versus perturbation radius

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does feature averaging manifest in non-orthogonal cluster feature settings, and what is the relationship between the degree of orthogonality and the robustness degradation?
- Basis in paper: [explicit] Paper discusses relaxing exact orthogonal condition to nearly orthogonal setting in ablation study (Figure 9), showing feature averaging still occurs, but doesn't quantify relationship between orthogonality and robustness
- Why unresolved: Paper only provides qualitative observations about persistence of feature averaging under relaxed orthogonality assumptions, without quantifying how degree of orthogonality affects robustness degradation
- What evidence would resolve it: Controlled experiments varying angle between cluster features while measuring both extent of feature averaging (via weight-feature correlations) and resulting robustness degradation under adversarial attacks

### Open Question 2
- Question: Does feature averaging occur in deeper networks (beyond two-layer networks), and if so, how does depth of network influence averaging behavior and its impact on robustness?
- Basis in paper: [inferred] Theoretical analysis is limited to two-layer ReLU networks, and while experiments are conducted on ResNet18, paper doesn't explicitly analyze how feature averaging manifests in deeper architectures or how depth affects it
- Why unresolved: Theoretical framework is specifically developed for two-layer networks, and paper doesn't extend this analysis to deeper architectures or provide theoretical understanding of how depth influences feature averaging
- What evidence would resolve it: Analysis of weight-feature correlations in deeper networks trained on multi-cluster data, combined with theoretical work extending two-layer analysis to deeper architectures, showing how feature averaging scales with network depth

### Open Question 3
- Question: What is the precise mechanism by which fine-grained supervision (e.g., part-level segmentation) improves robustness in real-world datasets where multi-cluster assumption doesn't hold exactly?
- Basis in paper: [explicit] Paper mentions connections to Sitawarin et al. (2022) and Li et al. (2024) who showed part-level segmentation improves robustness, but doesn't provide theoretical explanation for why this works in more complex real-world settings
- Why unresolved: Theoretical analysis assumes clean multi-cluster structure that doesn't exist in real-world data, and paper doesn't explain mechanism by which fine-grained supervision improves robustness when data distribution is more complex
- What evidence would resolve it: Experiments isolating effect of fine-grained supervision on learned feature representations in real-world datasets, combined with analysis showing how supervision forces network to learn more discriminative and robust feature representations even when underlying data structure is complex

## Limitations
- Theoretical analysis relies on strong assumptions about data distribution (orthogonal cluster features, specific initialization regime) that may not hold in practical scenarios
- Proof techniques for two-layer ReLU networks don't directly extend to deeper architectures or more complex data distributions
- Gap between synthetic data analysis and real-world datasets remains significant despite experimental validation

## Confidence

**High Confidence**: The experimental demonstration that fine-grained supervision improves adversarial robustness is well-supported by empirical results across multiple datasets (synthetic, MNIST, CIFAR-10). The correlation between label granularity and feature learning is consistently observed.

**Medium Confidence**: The theoretical characterization of feature averaging in two-layer networks is mathematically rigorous but relies on idealized assumptions. The extension to practical deep learning scenarios involves approximations that may not fully capture real-world behavior.

**Low Confidence**: Claims about the mechanism being the primary cause of lack of robustness in practical deep networks are speculative. The analysis doesn't account for other factors like architecture design, training procedures, or data augmentation that also impact robustness.

## Next Checks

1. **Test non-orthogonal cluster features**: Generate synthetic data where cluster features are not perfectly orthogonal but still discriminative. Measure whether feature averaging still occurs and quantify the impact on robustness as feature orthogonality decreases.

2. **Evaluate deeper network architectures**: Extend the analysis to three-layer networks and residual architectures. Compare feature averaging behavior and robustness outcomes to understand how depth affects the mechanism.

3. **Investigate alternative optimization methods**: Repeat key experiments using Adam, SGD with momentum, and learning rate schedules. Determine whether feature averaging is specific to vanilla gradient descent or represents a broader phenomenon across optimization methods.