---
ver: rpa2
title: 'CLoG: Benchmarking Continual Learning of Image Generation Models'
arxiv_id: '2406.04584'
source_url: https://arxiv.org/abs/2406.04584
tags:
- task
- learning
- clog
- continual
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLoG, the first comprehensive benchmark for
  continual learning of image generation models. It systematically adapts three types
  of existing CL methods (replay-based, regularization-based, and parameter-isolation-based)
  to generative tasks and establishes unified evaluation protocols.
---

# CLoG: Benchmarking Continual Learning of Image Generation Models

## Quick Facts
- **arXiv ID:** 2406.04584
- **Source URL:** https://arxiv.org/abs/2406.04584
- **Reference count:** 40
- **One-line primary result:** No single CL method works well across all generative benchmarks, with replay-based methods facing mode collapse and regularization-based methods struggling to learn new tasks.

## Executive Summary
This paper introduces CLoG, the first comprehensive benchmark for continual learning of image generation models. It systematically adapts three types of existing CL methods (replay-based, regularization-based, and parameter-isolation-based) to generative tasks and establishes unified evaluation protocols. Experiments on seven label-conditioned and one concept-conditioned benchmarks reveal that no single method works well across all settings, with replay-based methods facing mode collapse issues and regularization-based methods struggling to learn new tasks. The study highlights the need for more advanced CLoG strategies and advocates shifting research focus from classification-based CL to CLoG, given the growing importance of generative foundation models.

## Method Summary
The paper benchmarks continual learning of image generation by adapting twelve existing CL methods to two generative architectures (StyleGAN2 for GANs and DDIM for diffusion models) across seven label-conditioned and one concept-conditioned benchmarks. The evaluation uses unified training protocols with fixed hyperparameters and standardized metrics including Average Final Quality (AFQ), Average Incremental Quality (AIQ), and Forgetting Rate (FR) computed via FID scores. The benchmark systematically tests methods on diverse datasets (MNIST, FashionMNIST, CIFAR-10, ImageNet-1k, Oxford-Flower, CUB-Birds, Stanford-Cars) with varying class orderings to assess robustness to task sequence.

## Key Results
- No single CL method works well across all seven label-conditioned and one concept-conditioned benchmarks
- Replay-based methods face significant mode collapse issues when learning new tasks
- Regularization-based methods struggle to learn new tasks while preserving old ones
- The benchmark establishes CLoG as a challenging open problem requiring new methodological approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifting from classification-based CL to generative CL is necessary due to the rising importance of generative foundation models and the unique challenges they pose.
- Mechanism: Generative models require modeling complex data distributions, unlike classification tasks which typically involve simpler categorical distributions. This complexity leads to more difficult optimization and incremental expansion during continual learning.
- Core assumption: The increased model complexity and optimization difficulty in generative models inherently make CLoG more challenging than traditional classification-based CL.
- Evidence anchors:
  - [abstract]: "CLoG typically necessitates the use of sophisticated generative models... These generative models are generally more complex to optimize and incrementally expand within a continual learning framework compared to classification architectures."
  - [section 2.2]: "CLoG is more challenging as its output space inherently possesses a significantly larger cardinality, while the output of classification-based CL is typically limited to discrete class indices."
  - [corpus]: Weak - corpus papers do not directly discuss the shift to generative CL or the challenges of model complexity.

### Mechanism 2
- Claim: Traditional CL methods (regularization, replay, parameter-isolation) are insufficient for CLoG, necessitating the development of new strategies.
- Mechanism: The paper benchmarks three types of existing CL methods adapted to generative tasks and finds that no single method works well across all settings, highlighting the need for more advanced CLoG strategies.
- Core assumption: The failure of traditional CL methods on CLoG benchmarks indicates that the unique characteristics of generative tasks require fundamentally different approaches.
- Evidence anchors:
  - [abstract]: "Experiments on seven label-conditioned and one concept-conditioned benchmarks reveal that no single method works well across all settings..."
  - [section 4.2]: "Except for Non-CL, no single method works well on all benchmarks... Above all, the current methods are not satisfactory enough and our CLoG benchmark remains an open challenge for developing new methods."
  - [corpus]: Weak - corpus papers do not directly discuss the inadequacy of traditional CL methods for generative tasks.

### Mechanism 3
- Claim: The imbalance between replayed samples and new task data in CLoG leads to mode collapse and low plasticity, making replay-based methods less effective than in classification-based CL.
- Mechanism: Limited replayed samples are seen and trained many times, making them easier to learn than new task data. This imbalance leads to mode collapse for previous tasks and low plasticity in learning new tasks.
- Core assumption: The sensitivity of CLoG to data imbalance is greater than in classification-based CL due to the difficulty of modeling complex data distributions.
- Evidence anchors:
  - [section 4.2]: "Replay-based methods face imbalance issue... CLoG turns to be more sensitive to these issues than classification-based CL possibly because the modeling of data distribution p(x) is more difficult than classification distribution p(y|x)..."
  - [section 3.3]: Mentions the use of FrÃ©chet inception distance (FID) as a quality metric, which is sensitive to mode collapse.
  - [corpus]: Weak - corpus papers do not directly discuss the imbalance issue in CLoG or its impact on replay-based methods.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding catastrophic forgetting is crucial for grasping the core challenge that CLoG aims to address.
  - Quick check question: What is catastrophic forgetting, and why is it a significant problem in continual learning?

- Concept: Generative models (GANs and Diffusion Models)
  - Why needed here: Familiarity with generative models is essential for understanding the specific challenges and methods discussed in the paper.
  - Quick check question: What are the key differences between GANs and Diffusion Models in terms of their architecture and training process?

- Concept: Continual learning methods (regularization, replay, parameter-isolation)
  - Why needed here: Understanding these three types of CL methods is necessary for comprehending the benchmarks and results presented in the paper.
  - Quick check question: How do regularization-based, replay-based, and parameter-isolation-based methods differ in their approach to preventing catastrophic forgetting?

## Architecture Onboarding

- Component map:
  - Task selection and dataset preparation -> Baseline method adaptation and implementation -> Evaluation metric design and computation -> Training specifics and hyperparameter tuning -> Result analysis and visualization

- Critical path:
  1. Select diverse and representative CLoG tasks
  2. Adapt existing CL methods to generative tasks
  3. Design unified evaluation metrics
  4. Implement baselines with standardized training specifics
  5. Conduct experiments and analyze results

- Design tradeoffs:
  - Balancing task diversity with computational efficiency
  - Choosing between different generative model architectures (GAN vs. Diffusion)
  - Deciding on the level of standardization for training specifics vs. method-specific optimizations

- Failure signatures:
  - Poor performance across all benchmarks (indicating fundamental issues with method adaptation)
  - Inconsistent results across different random class orders (suggesting sensitivity to task ordering)
  - Extremely high variance in results (pointing to potential instability in the methods or evaluation process)

- First 3 experiments:
  1. Run NCL (Naive Continual Learning) on MNIST with both GAN and DDIM backbones to establish baseline forgetting levels.
  2. Implement and test ER (Experience Replay) on FashionMNIST to assess the impact of replay on mitigating forgetting.
  3. Compare EWC (Elastic Weight Consolidation) performance on CIFAR-10 between GAN and DDIM backbones to evaluate regularization effectiveness across architectures.

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on two generative architectures (StyleGAN2 and DDIM), potentially missing other viable approaches
- Results show high variance across different random class orderings, suggesting sensitivity to task sequence that isn't fully explored
- Evaluation relies heavily on FID scores, which may not capture all aspects of generation quality in continual learning scenarios

## Confidence
- **High**: The claim that CLoG is more challenging than classification-based CL due to model complexity and data distribution modeling
- **Medium**: The assertion that no single traditional CL method works well across all CLoG benchmarks
- **Low**: The generalizability of findings to other generative architectures beyond GANs and diffusion models

## Next Checks
1. Test the same CL methods on alternative generative architectures (e.g., VAEs, autoregressive models) to verify if the observed performance patterns hold across different model families
2. Conduct ablation studies varying replay buffer sizes and learning rates to better understand the sensitivity of replay-based methods to hyperparameters in CLoG scenarios
3. Evaluate the methods on additional datasets with different characteristics (e.g., medical imaging, satellite imagery) to assess robustness across diverse domains beyond natural images