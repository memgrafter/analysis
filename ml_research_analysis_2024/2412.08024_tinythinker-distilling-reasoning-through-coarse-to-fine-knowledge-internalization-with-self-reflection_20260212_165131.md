---
ver: rpa2
title: 'TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization
  with Self-Reflection'
arxiv_id: '2412.08024'
source_url: https://arxiv.org/abs/2412.08024
tags:
- reasoning
- knowledge
- because
- incorrect
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TinyThinker is a knowledge distillation framework that structures
  reasoning into a three-stage process (recall-analyze-summarize) to progressively
  refine knowledge from coarse to fine granularity. It employs a two-phase training
  approach: reasoning acquisition followed by self-reflection with iterative DPO.'
---

# TinyThinker: Distilling Reasoning through Coarse-to-Fine Knowledge Internalization with Self-Reflection

## Quick Facts
- arXiv ID: 2412.08024
- Source URL: https://arxiv.org/abs/2412.08024
- Authors: Shengmin Piao; Sanghyun Park
- Reference count: 40
- Primary result: TinyThinker achieves superior performance on commonsense reasoning benchmarks through structured three-stage reasoning and self-reflection with DPO

## Executive Summary
TinyThinker is a knowledge distillation framework that structures reasoning into a three-stage process (recall-analyze-summarize) to progressively refine knowledge from coarse to fine granularity. It employs a two-phase training approach: reasoning acquisition followed by self-reflection with iterative Direct Preference Optimization (DPO). Experiments on commonsense reasoning benchmarks show TinyThinker achieves superior performance compared to baselines, with consistent improvements across different model sizes. Ablation studies validate the effectiveness of each component in the framework.

## Method Summary
TinyThinker is a knowledge distillation framework that transfers reasoning capabilities from large language models to smaller models through a structured three-stage process (recall-analyze-summarize) and self-reflection with iterative DPO. The method uses commonsense reasoning datasets (CSQA, OBQA, StrategyQA) with teacher-generated reasoning data via GPT-4o. The two-phase training framework comprises an initial reasoning acquisition phase using the three-stage process, followed by a self-reflection phase utilizing iterative DPO on self-generated data. The objective is to maximize accuracy on commonsense reasoning benchmarks through progressive knowledge refinement and self-reflective optimization.

## Key Results
- Achieves superior performance on commonsense reasoning benchmarks compared to baselines
- Shows consistent improvements across different model sizes (T5-Small, Base, Large)
- Ablation studies validate effectiveness of three-stage process and self-reflection components
- Demonstrates successful knowledge internalization through coarse-to-fine progressive refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Three-stage reasoning process enables knowledge internalization through progressive refinement
- Mechanism: Student model first retrieves general knowledge, then analyzes specific knowledge for each option using general knowledge as context, and finally integrates both to produce summary and answer
- Core assumption: Progressive knowledge refinement leads to better integration than direct training on complete reasoning traces
- Evidence anchors:
  - [abstract]: "three-stage process that incrementally guides the student model through the reasoning process, progressively refining knowledge from coarse to fine granularity"
  - [section 3.1]: Describes three-stage process with specific training objectives for recall, analyze, and summarize stages
  - [corpus]: Related work on "Coarse-to-Fine Process Reward Modeling" suggests coarse-to-fine approaches have merit in reasoning tasks
- Break condition: If student model lacks sufficient capacity to manage increasing complexity from general to specific knowledge, performance may degrade

### Mechanism 2
- Claim: Self-reflection through iterative DPO refines relationship between reasoning capabilities and underlying knowledge
- Mechanism: Student generates reasoning data using learned capabilities, then applies DPO with pairwise comparisons to reinforce accurate knowledge and refine incorrect knowledge
- Core assumption: Self-generated data with preference optimization can effectively refine reasoning capabilities without external supervision
- Evidence anchors:
  - [abstract]: "self-reflection phase utilizing self-generated data" and "iterative Direct Preference Optimization (DPO)"
  - [section 3.2]: Describes DPO+NLL training objective and binary approach to classify generated outputs
  - [corpus]: Related work on "Error-Aware Self-Reflection" suggests self-reflection can enhance reasoning
- Break condition: If student's initial reasoning capabilities are too poor, self-generated data may be too noisy for effective refinement

### Mechanism 3
- Claim: Two-phase training provides complete learning trajectory from basic reasoning to refined capabilities
- Mechanism: First phase establishes foundational reasoning through structured process; second phase consolidates and refines these capabilities through self-generated data and iterative optimization
- Core assumption: Sequential learning with distinct phases is more effective than single-phase training for complex reasoning tasks
- Evidence anchors:
  - [abstract]: "two-phase training framework comprising an initial reasoning acquisition phase followed by a self-reflection phase"
  - [section 3.3]: Describes iterative training strategy with distinct phases
  - [corpus]: Assumption based on pedagogical reasoning
- Break condition: If phases are not properly separated or self-reflection phase overfits to self-generated data, performance may not improve

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Understanding how CoT works is essential since TinyThinker builds on CoT concepts but addresses its limitations
  - Quick check question: What is the key difference between CoT prompting and TinyThinker's three-stage process?

- Concept: Knowledge distillation
  - Why needed here: TinyThinker is fundamentally a knowledge distillation framework that transfers reasoning capabilities from teacher to student models
  - Quick check question: How does TinyThinker's approach differ from standard supervised fine-tuning in knowledge distillation?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the core algorithm used in the self-reflection phase for refining reasoning capabilities
  - Quick check question: What is the main advantage of using DPO over standard supervised learning in the self-reflection phase?

## Architecture Onboarding

- Component map: Data curation pipeline -> Three-stage training loop (Recall -> Analyze -> Summarize) -> Self-reflection module (Iterative DPO) -> Evaluation framework
- Critical path: Data curation → Reasoning acquisition (3-stage training) → Self-reflection (iterative DPO) → Evaluation
- Design tradeoffs:
  - Three-stage process adds training complexity but enables better knowledge integration
  - Self-reflection relies on student's initial reasoning quality, which may be poor
  - Pairwise comparisons for DPO are simpler but less nuanced than continuous rewards
- Failure signatures:
  - Poor performance on OBQA and StrategyQA may indicate issues with analyze stage complexity
  - Inconsistent improvements across model sizes suggest capacity limitations
  - Failure to improve after self-reflection phase suggests poor quality of self-generated data
- First 3 experiments:
  1. Ablation study: Remove analyze stage and compare performance to full three-stage process
  2. Data quality analysis: Manually inspect teacher-generated data for hallucinations and errors
  3. Self-reflection ablation: Compare performance with and without DPO in the self-reflection phase

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would TinyThinker perform on non-MCQ reasoning tasks such as free-form QA or open-ended problem-solving?
- Basis in paper: [inferred] The paper focuses exclusively on multiple-choice question (MCQ) datasets and notes that "most commonsense reasoning datasets are structured in a multiple-choice question (MCQ) format"
- Why unresolved: The paper's experimental design limits evaluation to MCQ formats, and the authors explicitly note that "TinyThinker employs a structured reasoning process to progressively refine the student's knowledge" which may be specifically optimized for MCQ tasks
- What evidence would resolve it: Direct experimental comparison of TinyThinker against baselines on non-MCQ reasoning benchmarks would demonstrate whether the three-stage reasoning process generalizes beyond structured multiple-choice formats

### Open Question 2
- Question: What is the impact of data quality issues on TinyThinker's performance, particularly given the "error cascade" problem mentioned in the limitations section?
- Basis in paper: [explicit] The limitations section explicitly states "Quality of curated data" as a key concern, noting that "LLMs are capable of generating semantically coherent sentences, their inherent issue of hallucination sometimes leads to content that lacks factual accuracy and safety"
- Why unresolved: While the paper acknowledges this limitation, it does not empirically measure how much performance degradation occurs from noisy or hallucinated teacher-generated data
- What evidence would resolve it: Systematic experiments varying the quality of teacher-generated data would quantify the sensitivity of TinyThinker to data quality issues

### Open Question 3
- Question: How does the two-phase training approach compare to a unified end-to-end training approach for the same total number of training steps?
- Basis in paper: [inferred] The paper describes a two-phase approach with reasoning acquisition followed by self-reflection, but does not compare against a single-phase alternative
- Why unresolved: The current ablation studies only examine components within the two-phase framework but don't test whether the phased approach is necessary or optimal
- What evidence would resolve it: Direct comparison experiments where a baseline model receives the same total training steps but without the phased separation would reveal whether the separation provides benefits beyond just additional training time

## Limitations

- Quality of self-generated data during reflection phase may be poor if student's initial reasoning capabilities are inadequate
- Reliance on GPT-4o for teacher data generation creates dependency on specific model availability
- Three-stage process may not be optimal for all reasoning tasks and hasn't been compared against alternative reasoning frameworks

## Confidence

- **High confidence** in the general effectiveness of the two-phase training approach and overall framework design
- **Medium confidence** in the specific effectiveness of the three-stage process
- **Medium confidence** in the self-reflection mechanism

## Next Checks

1. **Self-generated data quality analysis**: Conduct thorough analysis of quality and diversity of self-generated data during reflection phase, including hallucination rates and reasoning consistency

2. **Teacher model ablation**: Repeat key experiments with different teacher models to assess framework's robustness to teacher quality variations and identify minimum teacher capability required

3. **Alternative reasoning structures**: Compare three-stage process against alternative reasoning frameworks to determine whether specific recall-analyze-summarize structure is optimal or if other structures might perform better for certain task types