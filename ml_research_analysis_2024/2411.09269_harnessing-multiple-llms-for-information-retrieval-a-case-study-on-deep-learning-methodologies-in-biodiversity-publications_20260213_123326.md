---
ver: rpa2
title: 'Harnessing multiple LLMs for Information Retrieval: A case study on Deep Learning
  methodologies in Biodiversity publications'
arxiv_id: '2411.09269'
source_url: https://arxiv.org/abs/2411.09269
tags:
- publications
- information
- learning
- pipeline
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of extracting detailed deep
  learning methodological information from scientific publications, which is critical
  for reproducibility. The authors developed a multi-Large Language Model (LLM) pipeline
  using Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B, and
  Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG) to extract and
  process DL methodological details from biodiversity publications.
---

# Harnessing multiple LLMs for Information Retrieval: A case study on Deep Learning methodologies in Biodiversity publications

## Quick Facts
- arXiv ID: 2411.09269
- Source URL: https://arxiv.org/abs/2411.09269
- Reference count: 5
- Primary result: Multi-LLM pipeline with RAG achieves 69.5% accuracy in extracting DL methodological information from biodiversity publications

## Executive Summary
This study tackles the challenge of extracting detailed deep learning (DL) methodological information from scientific publications to improve reproducibility. The authors develop a multi-Large Language Model (LLM) pipeline using Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B, and Gemma 2 9B combined with Retrieval-Augmented Generation (RAG). A voting classifier aggregates outputs from the five LLMs to ensure consistency and accuracy. Tested on two datasets of biodiversity publications, the pipeline achieves 69.5% accuracy in retrieving DL methodological information based solely on textual content, demonstrating a scalable approach applicable across scientific domains.

## Method Summary
The methodology involves extracting 25 deep learning-related keywords from TDWG abstracts to index publications from Ecological Informatics and a prior dataset. Publications are filtered using keyword-based indexing followed by RAG-assisted re-filtering to remove false positives. Five open-source LLMs (Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B, and Gemma 2 9B) process 28 Competency Questions across 464 publications. Textual responses are converted to categorical "yes/no" answers and aggregated using a voting classifier. Accuracy is evaluated by comparing LLM outputs against human annotations, achieving 69.5% accuracy (417 out of 600 comparisons).

## Key Results
- Multi-LLM pipeline with RAG achieves 69.5% accuracy in extracting DL methodological information
- RAG filtering reduces false positives, decreasing publications by 44.6% while improving precision
- Voting classifier aggregation reduces idiosyncratic errors across five different LLM architectures
- Methodology is scalable and applicable across scientific domains beyond biodiversity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Combining multiple LLMs with RAG improves retrieval accuracy over single-model approaches.
- **Mechanism**: Each LLM contributes partial, complementary information; voting aggregation reduces idiosyncratic errors and model-specific blind spots.
- **Core assumption**: Different LLM architectures and training datasets capture different aspects of methodological detail.
- **Evidence anchors**:
  - [abstract] "We built a voting classifier from the outputs of five LLMs to accurately report DL methodological information."
  - [section] "This approach allowed us to extract, organize, and link information from unstructured text into structured, queryable data..."
- **Break condition**: If all models share the same bias or are trained on very similar corpora, voting will not add value and may amplify errors.

### Mechanism 2
- **Claim**: Filtering publications for DL relevance before applying CQs improves precision.
- **Mechanism**: Initial keyword-based indexing yields many false positives; re-filtering with RAG detects actual DL content, removing noise.
- **Core assumption**: Mention of DL-related keywords does not guarantee methodological detail.
- **Evidence anchors**:
  - [section] "We choose a method that extracts publications based on selected keywords... even if a publication mentions any of the keywords only once, without providing the actual DL methodology, it will still be included."
  - [section] "To mitigate this issue, we filtered the extracted publications again using the RAG-assisted pipeline... the number of publications decreased by 44.6%."
- **Break condition**: If keyword filtering is too strict or RAG misclassifies relevant publications, recall will suffer.

### Mechanism 3
- **Claim**: Converting textual LLM responses to categorical "yes/no" simplifies evaluation and enables voting.
- **Mechanism**: LLMs are prompted to produce binary answers, making manual comparison with human annotations tractable.
- **Core assumption**: Binary conversion preserves essential information for methodological reproducibility assessment.
- **Evidence anchors**:
  - [section] "Next, we converted all preprocessed LLM textual responses into categorical 'yes' or 'no' answers."
  - [section] "Two annotators reviewed the different question-answer pairs generated by the LLM and provided their assessments..."
- **Break condition**: If nuanced methodological detail is lost in binarization, evaluation may be overly coarse.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG)
  - Why needed here: Allows LLMs to access and cite external document chunks rather than relying on parametric memory.
  - Quick check question: What two steps does RAG perform when answering a query?

- **Concept**: Voting classifier ensemble
  - Why needed here: Aggregates independent model outputs to reduce variance and improve robustness.
  - Quick check question: What voting strategy is used and why?

- **Concept**: Cosine similarity for textual alignment
  - Why needed here: Measures semantic overlap between LLM outputs to assess agreement.
  - Quick check question: What does a high cosine similarity between two LLM responses indicate?

## Architecture Onboarding

- **Component map**: Keyword extraction -> Publication indexing -> RAG query execution -> Textual preprocessing -> Categorical conversion -> Voting classifier -> Accuracy evaluation

- **Critical path**: 
  1. Keyword extraction → index filter
  2. RAG query execution → text aggregation
  3. Categorical conversion → voting
  4. Accuracy evaluation vs human annotations

- **Design tradeoffs**:
  - Model count vs. compute cost (5 models = ~228h GPU time)
  - RAG chunk size (1000) vs. recall vs. noise
  - Categorical binarization vs. nuance loss

- **Failure signatures**:
  - Low IAA scores → ambiguous or hallucinated responses
  - Voting deadlock → equal "yes" and "no" votes
  - High false negative rate → overly strict filtering

- **First 3 experiments**:
  1. Single LLM baseline: run Llama-3 70B alone, compare accuracy.
  2. Vary chunk size: test 500, 1000, 2000 and measure IAA.
  3. Skip binarization: keep textual outputs, evaluate with semantic similarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of LLMs and RAG configurations affect the accuracy and reliability of information retrieval across diverse scientific domains?
- Basis in paper: [explicit] The paper tests multiple LLMs and RAG configurations but focuses specifically on biodiversity publications.
- Why unresolved: The study demonstrates the effectiveness of a multi-LLM, RAG-assisted pipeline in biodiversity but does not explore how this approach performs in other scientific fields with different publication structures, terminology, and data formats.
- What evidence would resolve it: Comparative studies applying the same multi-LLM, RAG-assisted pipeline to other scientific domains (e.g., medicine, physics, or social sciences) and measuring accuracy, inter-annotator agreement, and retrieval success rates across these fields.

### Open Question 2
- Question: What is the optimal strategy for filtering publications to ensure only those containing detailed deep learning methodologies are processed, minimizing false positives and negatives?
- Basis in paper: [inferred] The paper notes that some publications may mention DL keywords without detailing methodologies, and filters are applied post-retrieval to improve accuracy, but the optimal filtering criteria are not explored.
- Why unresolved: The current filtering approach reduces false positives but still leaves a significant portion of queries unanswered, indicating room for improvement in publication selection and filtering strategies.
- What evidence would resolve it: Experimental comparison of different filtering strategies (e.g., keyword-based, semantic similarity, citation analysis) to identify the most effective method for isolating publications with detailed DL methodologies.

### Open Question 3
- Question: How does the environmental impact of multi-LLM pipelines compare to traditional manual information retrieval methods, and what are the trade-offs between computational cost and scalability?
- Basis in paper: [explicit] The paper quantifies the energy consumption and carbon footprint of the multi-LLM pipeline but does not compare these metrics to manual retrieval methods or explore cost-effectiveness.
- Why unresolved: While the environmental impact of the pipeline is assessed, the broader trade-offs between computational cost, scalability, and environmental sustainability compared to manual methods are not discussed.
- What evidence would resolve it: Comparative studies measuring the time, cost, and environmental impact of manual versus automated DL methodology extraction across similar datasets, including scalability assessments for large-scale applications.

## Limitations
- Limited generalizability: Only tested on biodiversity publications, not other scientific domains
- Oversimplification risk: Binary conversion of nuanced methodological details may lose critical context
- Ground truth uncertainty: Inter-annotator agreement scores not reported, making accuracy assessment uncertain

## Confidence

**High Confidence**: The core methodology of using multiple LLMs with RAG for information retrieval from scientific publications is well-defined and technically sound. The voting classifier approach for aggregating model outputs is a valid ensemble technique.

**Medium Confidence**: The reported accuracy of 69.5% is plausible given the complexity of the task, but the lack of IAA scores and detailed evaluation methodology reduces confidence in this specific figure. The effectiveness of binarization for preserving essential information is reasonable but not empirically validated.

**Low Confidence**: Claims about the scalability and generalizability of the approach across scientific domains are speculative, as the study only tests on biodiversity publications. The impact on actual reproducibility improvement is not measured beyond text extraction accuracy.

## Next Checks

1. **Inter-Annotator Agreement Assessment**: Calculate and report IAA scores for the human annotations to establish ground truth reliability. Compare IAA scores across different LLM pairs to identify which models align best with human judgment.

2. **Granularity Analysis**: Conduct an experiment comparing the voting classifier approach with and without categorical binarization. Keep some outputs in their original textual form and evaluate using semantic similarity metrics (e.g., cosine similarity of embeddings) to assess information loss.

3. **False Negative Analysis**: Analyze the 44.6% of publications excluded by the RAG filtering step. Manually review a sample of excluded publications to determine whether they contained relevant DL methodological information that was incorrectly filtered out, and calculate precision and recall for the filtering process.