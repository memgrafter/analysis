---
ver: rpa2
title: '"Let''s Argue Both Sides": Argument Generation Can Force Small Models to Utilize
  Previously Inaccessible Reasoning Capabilities'
arxiv_id: '2410.12997'
source_url: https://arxiv.org/abs/2410.12997
tags:
- argument
- generation
- reasoning
- language
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Argument Generation as a novel prompting
  technique to improve reasoning capabilities in large language models. The method
  generates arguments for and against each possible answer, then asks the model to
  rank these arguments and select the best one.
---

# "Let's Argue Both Sides": Argument Generation Can Force Small Models to Utilize Previously Inaccessible Reasoning Capabilities

## Quick Facts
- arXiv ID: 2410.12997
- Source URL: https://arxiv.org/abs/2410.12997
- Authors: Kaveh Eskandari Miandoab; Vasanth Sarathy
- Reference count: 17
- Primary result: Argument Generation prompting outperforms both zero-shot and chain-of-thought reasoning in 38 out of 81 test settings, particularly benefiting smaller models.

## Executive Summary
This paper introduces Argument Generation, a novel prompting technique that generates arguments for and against each possible answer, then asks the model to rank these arguments and select the best one. Tested across nine datasets and nine models of varying sizes, the approach shows promise, particularly for smaller models. The results demonstrate that Argument Generation outperforms both zero-shot prompting and chain-of-thought reasoning in 38 out of 81 test settings, with larger gains observed in smaller models. Notably, the technique shows significant improvements in tasks requiring argument quality ranking, suggesting its potential as a debiasing method.

## Method Summary
The Argument Generation method involves generating arguments for each possible answer choice, ranking these argument tuples, and selecting the answer with the strongest supporting argument. The process forces the model to explicitly reason about why each answer could be correct or incorrect, going beyond simple chain-of-thought reasoning. The method was tested across nine datasets spanning various reasoning tasks and nine models ranging from 1B to 70B parameters, comparing performance against zero-shot and chain-of-thought prompting baselines.

## Key Results
- Argument Generation outperformed zero-shot prompting in 38 out of 81 test settings
- Smaller models (1B-8B parameters) showed the most significant improvements
- The technique was particularly effective for argument quality ranking tasks
- Larger models showed diminishing returns, suggesting they already possess sufficient reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Argument Generation improves reasoning by forcing the model to generate and rank explicit arguments for and against each answer choice, which surfaces implicit assumptions needed for logical inference.
- Mechanism: The model first generates arguments supporting and attacking each candidate answer. Then it ranks these argument tuples and selects the answer with the strongest supporting argument. This forces deeper logical consideration than chain-of-thought alone.
- Core assumption: The correct answer will have the strongest argument supporting it, and weaker counterarguments, making it rank highest when the model evaluates argument strength.
- Evidence anchors:
  - [abstract] "Our method involves the generation of arguments for each possible inference result, and asking the end model to rank the generated arguments."
  - [section] "Argument Generation aims to take into consideration the possibility of the presence of a counterargument that is statistically more significant than the answer that is generated by pure chain-of-thought."
- Break condition: If the model can generate equally strong arguments for multiple answers, or if the correct answer's counterargument is statistically stronger, the ranking may fail.

### Mechanism 2
- Claim: Smaller models benefit more from Argument Generation because they lack the capability to generate convincing arguments for incorrect options, making valid arguments more distinguishable.
- Mechanism: Large models can generate high-quality arguments for incorrect candidates, making ranking harder. Smaller models generate weaker arguments for wrong answers, so valid arguments stand out more clearly.
- Core assumption: Model size correlates with ability to generate convincing false arguments, and this correlation affects ranking effectiveness.
- Evidence anchors:
  - [section] "Smaller models are not able to generate arguments of high quality for incorrect candidates, thus goading the model to rank the valid argument over the incorrect one."
  - [section] "We hypothesize that the reason behind the lower performance gain in larger models is due to their already impressive capability to infer the correct results without the requirement to introduce further information probing techniques."
- Break condition: If smaller models improve their argument generation quality, or if larger models develop biases toward certain answer patterns.

### Mechanism 3
- Claim: Argument Generation acts as a knowledge-probing technique that is only useful when additional reasoning is required beyond what zero-shot methods provide.
- Mechanism: The method forces the model to explicitly reason about why each answer could be correct or incorrect, which only helps when the model lacks sufficient knowledge to make the correct choice without this additional probing.
- Core assumption: When a model already has sufficient knowledge to answer correctly, additional argument generation doesn't provide new information and thus doesn't improve performance.
- Evidence anchors:
  - [abstract] "knowledge-probing techniques such as chain-of-thought reasoning and Argument Generation are only useful when further reasoning is required to infer a prediction"
  - [section] "forcing the model to perform self-reasoning or rank the validity of arguments and responses does not expose the model to previously hidden information, and does not necessarily increase the performance when additional information is not strictly required to respond to the input."
- Break condition: When tasks require simple recall rather than reasoning, or when models already possess complete knowledge about the answer.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Understanding how chain-of-thought works provides the baseline comparison for Argument Generation and explains why the latter might be superior in certain cases.
  - Quick check question: How does chain-of-thought prompting differ from simple zero-shot prompting in terms of the reasoning process it elicits from the model?

- Concept: Implicit assumptions in logical reasoning
  - Why needed here: Argument Generation explicitly generates implicit assumptions, so understanding what these are and how they function in logical inference is crucial.
  - Quick check question: What distinguishes an implicit assumption from an explicit premise in an argument?

- Concept: List-wise ranking in NLP
  - Why needed here: The method relies on ranking arguments across multiple candidates, so understanding how list-wise ranking works in NLP contexts is essential.
  - Quick check question: How does list-wise ranking differ from pairwise ranking when evaluating multiple options?

## Architecture Onboarding

- Component map: Input → Argument Generation (for/against each answer) → Argument Ranking → Answer Selection → Output
- Critical path: Input → Argument Generation (for/against each answer) → Argument Ranking → Answer Selection → Output
- Design tradeoffs: Single-pass prompting vs. multi-agent debate (computational efficiency vs. potential performance gains), explicit vs. implicit assumption generation (prompt complexity vs. reasoning depth), model size considerations (smaller models benefit more but may have lower overall quality).
- Failure signatures: Performance drops when models generate equally convincing arguments for multiple answers, when correct answers have strong counterarguments, when tasks don't require additional reasoning beyond zero-shot capabilities, or when ranking mechanisms fail to distinguish argument quality.
- First 3 experiments:
  1. Test Argument Generation vs. zero-shot and chain-of-thought on a simple reasoning dataset (like CommonsenseQA) to establish baseline effectiveness
  2. Compare implicit assumption generation vs. standard argument generation to determine which yields better performance across different model sizes
  3. Evaluate model size sensitivity by testing the same prompts on models spanning from 1B to 70B parameters to confirm the observed relationship between model size and prompting effectiveness

## Open Questions the Paper Calls Out

Based on the paper "Let's Argue Both Sides": Argument Generation Can Force Small Models to Utilize Previously Inaccessible Reasoning Capabilities, here are 3-5 open research questions:

### Open Question 1
- Question: How does Argument Generation perform in a multi-agent setting where argument generation and ranking are performed by separate models?
- Basis in paper: The authors mention this as a potential extension but focus on single-pass prompting for this study.
- Why unresolved: The paper only explores single-pass prompting and acknowledges the potential benefits of a multi-agent approach without empirical evaluation.
- What evidence would resolve it: Experimental results comparing single-pass vs. multi-agent Argument Generation approaches across various tasks and model sizes.

### Open Question 2
- Question: What is the relationship between model architecture and the effectiveness of Argument Generation prompting?
- Basis in paper: The authors observe that the effectiveness of prompting techniques might depend on both model size and architecture, but do not deeply explore this relationship.
- Why unresolved: The paper focuses on parameter count but does not systematically investigate how different architectural choices (e.g., transformer variants, attention mechanisms) affect Argument Generation performance.
- What evidence would resolve it: Comparative studies of Argument Generation performance across models with similar parameter counts but different architectures.

### Open Question 3
- Question: How can Argument Generation be adapted for open-ended questions without predefined answer options?
- Basis in paper: The authors acknowledge this as a limitation, noting that their method requires predefined answer options.
- Why unresolved: The paper mentions this constraint but does not propose or test methods for extending Argument Generation to open-ended questions.
- What evidence would resolve it: Development and evaluation of techniques to generate answer options internally before applying Argument Generation, or alternative approaches for handling truly open-ended queries.

### Open Question 4
- Question: What is the impact of training data composition on a model's ability to benefit from Argument Generation?
- Basis in paper: The authors suggest that further investigation into learning resources used in model training could provide insight into the relationship between prompting and model reasoning.
- Why unresolved: The paper does not analyze how different training datasets or pre-training objectives affect Argument Generation performance.
- What evidence would resolve it: Comparative studies of Argument Generation effectiveness across models trained on different types of data (e.g., diverse web text vs. curated datasets, different languages or domains).

## Limitations
- The method requires predefined answer options, limiting its applicability to open-ended questions
- Performance gains may be specific to the tested datasets and model architectures
- The computational overhead of argument generation and ranking may make the method impractical for resource-constrained environments

## Confidence

- Argument Generation improves reasoning capabilities: Medium
- Argument Generation acts as a debiasing mechanism: Low
- Model size sensitivity hypothesis: High

## Next Checks

1. **Open-ended task evaluation**: Test Argument Generation on datasets without predefined answer options, such as narrative completion tasks or creative writing prompts, to assess whether the method generalizes beyond multiple-choice scenarios. This would validate whether the technique can handle open-ended reasoning tasks where answer generation itself is part of the challenge.

2. **Cross-model architecture validation**: Evaluate the method across different model architectures (transformers vs. other architectures) and training paradigms (supervised vs. self-supervised) to determine whether the observed size-performance relationship holds universally or is specific to the models tested in the study.

3. **Argument quality ablation study**: Systematically vary the quality of generated arguments (using different prompt formulations, temperature settings, or argument templates) to isolate whether improvements stem from the ranking mechanism itself or from the generation of higher-quality supporting arguments. This would clarify whether the technique's effectiveness depends on generating convincing arguments or simply on the act of explicit reasoning.