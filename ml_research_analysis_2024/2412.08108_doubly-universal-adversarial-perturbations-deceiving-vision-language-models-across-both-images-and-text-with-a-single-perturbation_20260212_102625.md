---
ver: rpa2
title: 'Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models
  Across Both Images and Text with a Single Perturbation'
arxiv_id: '2412.08108'
source_url: https://arxiv.org/abs/2412.08108
tags:
- image
- value
- layer
- adversarial
- visualization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Doubly-UAP, the first universal adversarial
  perturbation tailored for vision-language models, achieving universality across
  both image and text inputs. By targeting the value vectors in the middle-to-late
  layers of the vision encoder's attention mechanism, Doubly-UAP disrupts visual interpretation
  while remaining a black-box attack to the large language model.
---

# Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation

## Quick Facts
- **arXiv ID**: 2412.08108
- **Source URL**: https://arxiv.org/abs/2412.08108
- **Reference count**: 40
- **Primary result**: Introduces Doubly-UAP, achieving 96.1% attack success rate on classification and 0.3% on VQAv2 by targeting vision encoder value vectors

## Executive Summary
This work introduces Doubly-UAP, the first universal adversarial perturbation specifically designed for vision-language models (VLMs). Unlike traditional UAPs that target either images or text, Doubly-UAP achieves universality across both modalities by optimizing perturbations that disrupt the value vectors in the middle-to-late layers of the vision encoder's attention mechanism. The approach remains effective even when the large language model is treated as a black box, achieving state-of-the-art attack success rates across classification, captioning, and VQA tasks.

## Method Summary
The method optimizes a universal perturbation that targets value vectors in the middle-to-late layers (layers 14-17 for CLIP, 14-28 for EVA-CLIP) of the vision encoder's attention mechanism. The perturbation is learned using a label-free optimization process with cosine similarity loss, applied to 200,000 ImageNet training images. The optimization uses Adam with learning rate 1/255 for 3 epochs with batch size 8, and is evaluated on ImageNet validation (5,000 images) and VQA benchmarks.

## Key Results
- Achieves 96.1% attack success rate on classification tasks
- Reduces VQAv2 performance to 0.3% attack success rate
- Outperforms baseline methods across all tested vision-language tasks
- Maintains effectiveness across both CLIP and EVA-CLIP architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Value vectors in middle-to-late layers are the most vulnerable components for disrupting VLM interpretation
- **Mechanism**: The perturbation optimizes value vectors by maximizing loss between original and adversarial value vectors, distorting essential visual information
- **Core assumption**: Value vectors contain patch-level information, and their corruption cascades to LLM outputs
- **Break condition**: If vision encoder output embeddings aren't the primary information source for the LLM

### Mechanism 2
- **Claim**: Perturbation remains effective as a black-box attack due to LLM's dependence on vision encoder output
- **Mechanism**: Targeting vision encoder attention indirectly affects LLM interpretation without requiring LLM access
- **Core assumption**: Vision encoder output is critical input for LLM, and disrupting it causes misinterpretation
- **Break condition**: If LLM has independent mechanisms to correct for corrupted visual information

### Mechanism 3
- **Claim**: Perturbation generalizes across diverse image and text inputs, achieving doubly-universal effect
- **Mechanism**: Perturbation optimizes to distort fundamental visual characteristics relevant across various inputs
- **Core assumption**: Universal visual characteristics exist that, when distorted, lead to misinterpretation across diverse inputs
- **Break condition**: If effectiveness depends on specific image or text characteristics rather than universal visual properties

## Foundational Learning

- **Attention mechanisms in neural networks**
  - Why needed here: Paper focuses on disrupting attention mechanism within vision encoder
  - Quick check question: What are the two primary components of the attention mechanism discussed in the paper, and what are their respective roles in visual interpretation?

- **Universal Adversarial Perturbations (UAPs)**
  - Why needed here: Introduces Doubly-UAP, a novel UAP specifically designed for VLMs
  - Quick check question: How does a UAP differ from image-specific adversarial perturbations, and why is this distinction important for the effectiveness of Doubly-UAP?

- **Vision-Language Models (VLMs)**
  - Why needed here: Paper focuses on attacking VLMs, so understanding their architecture is crucial
  - Quick check question: What are the two main components of a VLM, and how do they interact to enable multimodal understanding?

## Architecture Onboarding

- **Component map**: Vision Encoder → LLM → Response
- **Critical path**: Vision Encoder processes visual information using attention mechanisms → LLM interprets visual information and generates responses → Final output
- **Design tradeoffs**:
  - Targeting value vectors vs. attention weights: Value vectors more effective but potentially harder to optimize
  - Black-box optimization vs. white-box: Black-box more practical but potentially less effective
  - Label-free vs. label-guided optimization: Label-free more general but potentially less targeted
- **Failure signatures**:
  - Low attack success rate indicates perturbation isn't effectively disrupting vision encoder output or LLM interpretation
  - High similarity between original and adversarial responses suggests perturbation isn't significantly altering visual information
  - Dependence on specific images or text prompts implies perturbation isn't truly universal
- **First 3 experiments**:
  1. Evaluate effectiveness of targeting different attention mechanism components across various layers
  2. Compare attack success rates with baseline methods across different tasks and models
  3. Analyze impact on attention mechanism by visualizing value vector distributions before and after applying Doubly-UAP

## Open Questions the Paper Calls Out

- How does the universality of Doubly-UAP across different VLM architectures scale when vision encoders have significantly different layer structures or attention mechanisms?
- Can Doubly-UAP be extended to adversarial attacks on multimodal tasks beyond image-text pairs, such as video-language or audio-language models?
- What are the long-term effects of Doubly-UAP on the interpretability and explainability of attention mechanisms in VLMs?

## Limitations

- Effectiveness across different VLM architectures beyond the two tested models remains unknown
- Performance on real-world images outside ImageNet distribution has not been evaluated
- The perturbation's doubly-universal property may not hold when applied to more diverse visual domains or text prompts

## Confidence

**High Confidence**: State-of-the-art attack success rates on tested tasks and models are well-supported by experimental results

**Medium Confidence**: Value vectors being the most vulnerable components is plausible but could benefit from more direct ablation studies

**Medium Confidence**: Doubly-universal effectiveness is supported by experiments but limited by homogeneous ImageNet dataset

## Next Checks

1. Evaluate Doubly-UAP on additional VLM architectures beyond CLIP and EVA-CLIP, particularly those with different vision encoder designs

2. Apply Doubly-UAP to images from diverse real-world distributions and assess whether attack success rates remain consistent

3. Implement and test simple adaptive defenses such as value vector denoising or attention mechanism regularization to understand potential robustness improvements