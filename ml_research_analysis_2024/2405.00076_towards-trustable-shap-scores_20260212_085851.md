---
ver: rpa2
title: Towards trustable SHAP scores
arxiv_id: '2405.00076'
source_url: https://arxiv.org/abs/2405.00076
tags:
- shap
- scores
- feature
- characteristic
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of misleading SHAP scores in explainable
  AI, particularly when features with no influence are assigned more importance than
  those with high influence. The core method involves proposing new characteristic
  functions for computing SHAP scores, which respect properties like weak and strong
  value independence, compliance with feature relevancy, and numerical neutrality.
---

# Towards trustable SHAP scores

## Quick Facts
- arXiv ID: 2405.00076
- Source URL: https://arxiv.org/abs/2405.00076
- Authors: Olivier Letoffe; Xuanxiang Huang; Joao Marques-Silva
- Reference count: 30
- Primary result: New characteristic functions eliminate misleading SHAP scores by ensuring feature relevance and independence properties.

## Executive Summary
This paper addresses the issue of misleading SHAP scores in explainable AI, where features with no influence are sometimes assigned more importance than those with high influence. The authors identify that the core problem lies not in Shapley values themselves, but in the choice of characteristic functions used to compute them. By proposing new characteristic functions based on similarity predicates, the paper ensures SHAP scores align with feature relevance and independence properties. The work includes both theoretical analysis and practical implementation through modifications to existing SHAP tools.

## Method Summary
The paper proposes novel characteristic functions (υs, υa, υc, υn) based on a similarity predicate σ that determines whether output changes are distinguishable when inputs change. These functions are designed to respect weak and strong value independence, compliance with feature relevancy, and numerical neutrality. The authors show that these new characteristic functions can be computed efficiently for certain model families and provide polynomial-time algorithms where applicable. The approach is validated through theoretical proofs and modifications to existing SHAP implementations like sSHAP.

## Key Results
- New characteristic functions eliminate known cases of misleading SHAP scores
- Polynomial-time algorithms available for computing corrected SHAP scores in certain model families
- sSHAP tool shows fewer mismatches between irrelevant and relevant features compared to original SHAP
- Proposed functions respect strong value independence, ensuring consistency across equivalent model representations

## Why This Works (Mechanism)

### Mechanism 1
The core issue with misleading SHAP scores is the choice of characteristic function, not Shapley values themselves. By replacing the standard expected-value-based characteristic function with alternatives like υs, υa, or υc, the paper ensures SHAP scores align with feature relevance and independence properties. The core assumption is that misleading SHAP scores arise from characteristic functions that do not enforce weak/strong value independence or feature (ir)relevancy.

### Mechanism 2
Using the similarity predicate σ to define characteristic functions yields SHAP scores that avoid the pitfalls of existing approaches. Characteristic functions like υs(S; E) := E[σ(x) |xS = vS] and υa(S; E) := 1 if υs(S; E) = 1, else 0 are built on the similarity predicate, ensuring SHAP scores reflect feature influence more accurately. The core assumption is that the similarity predicate captures whether changes in input lead to distinguishable output changes.

### Mechanism 3
The proposed characteristic functions respect strong value independence, ensuring SHAP scores remain consistent under value transformations. By ensuring that characteristic functions are strongly value-independent, SHAP scores are invariant to surjective mappings of output values, maintaining their interpretability across different model representations. The core assumption is that strong value independence is necessary for SHAP scores to be meaningful across different but equivalent model representations.

## Foundational Learning

- Concept: Shapley values and their axioms (efficiency, symmetry, dummy player, additivity)
  - Why needed here: Understanding these axioms is crucial for recognizing why alternative characteristic functions can yield non-misleading SHAP scores
  - Quick check question: What are the four axioms that uniquely define Shapley values, and why are they important for feature attribution?

- Concept: Characteristic functions in cooperative game theory
  - Why needed here: The choice of characteristic function directly impacts the SHAP scores, and understanding their role is essential for implementing the proposed solutions
  - Quick check question: How does the characteristic function influence the computation of Shapley values, and what properties should it satisfy in the context of XAI?

- Concept: Abductive and contrastive explanations
  - Why needed here: These formal explanations provide a framework for understanding feature relevance, which is critical for assessing the correctness of SHAP scores
  - Quick check question: What is the relationship between abductive explanations and feature relevance, and how do they relate to the proposed characteristic functions?

## Architecture Onboarding

- Component map: ML model -> Characteristic function (υs, υa, υc, υn) -> SHAP score computation -> Feature importance scores
- Critical path:
  1. Define the similarity predicate σ for the given ML model and target sample
  2. Choose a characteristic function based on desired properties
  3. Compute the characteristic function values for all subsets of features
  4. Calculate SHAP scores using the chosen characteristic function
  5. Validate that the SHAP scores respect feature relevance and independence properties
- Design tradeoffs:
  - Computational complexity vs. accuracy: Polynomial-time algorithms are available for certain model families, but complexity may increase for more complex models
  - Numerical neutrality vs. model-specific adaptations: Characteristic functions based on similarity predicate ensure numerical neutrality but may require model-specific definitions of σ
- Failure signatures:
  - SHAP scores do not respect feature relevance: Indicates the characteristic function does not enforce the property that irrelevant features have zero SHAP scores
  - SHAP scores vary across equivalent model representations: Suggests the characteristic function lacks strong value independence
  - Incorrect feature importance ranking: May result from using a characteristic function that does not accurately capture feature influence
- First 3 experiments:
  1. Implement the similarity predicate σ for a simple classification model and verify its correctness
  2. Compute SHAP scores using the characteristic function υs for a regression model and compare with existing SHAP scores
  3. Test the characteristic function υa on a boolean classifier and validate that irrelevant features receive zero SHAP scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the complexity of computing SHAP scores using the novel characteristic functions compare to the complexity of computing them using the original expected value characteristic function?
- Basis in paper: [explicit] The paper states that the complexity of computing SHAP scores using the new characteristic functions is either the same as or harder than the complexity of computing them using the original expected value characteristic function
- Why unresolved: The paper does not provide a detailed analysis of the computational complexity of the new characteristic functions
- What evidence would resolve it: A detailed computational complexity analysis of the new characteristic functions, comparing them to the original expected value characteristic function

### Open Question 2
- Question: Can the proposed framework be extended to handle regression models with continuous-valued features?
- Basis in paper: [inferred] The paper assumes discrete-valued features, but mentions that the results can be generalized to continuous-valued features
- Why unresolved: The paper does not provide a detailed analysis of how the framework can be extended to handle continuous-valued features
- What evidence would resolve it: A detailed analysis of how the framework can be extended to handle continuous-valued features, including the definition of expected value and probability for continuous-valued features

### Open Question 3
- Question: How does the performance of the sSHAP tool compare to the original SHAP tool in practice?
- Basis in paper: [explicit] The paper shows that the sSHAP tool produces fewer mismatches between irrelevant and relevant features compared to the original SHAP tool
- Why unresolved: The paper does not provide a detailed comparison of the performance of the sSHAP tool and the original SHAP tool in practice
- What evidence would resolve it: A detailed comparison of the performance of the sSHAP tool and the original SHAP tool in practice, including running time and accuracy

## Limitations
- Theoretical framework is well-developed but empirical validation on diverse real-world datasets and models is limited
- Claims about manageable complexity lack specific computational complexity analysis or benchmarks
- Detailed performance comparisons against established baselines in practical scenarios are not provided

## Confidence

**Major Uncertainties:**
- The paper's claims about the effectiveness of new characteristic functions rely heavily on theoretical proofs and abstract properties. While the theoretical framework is well-developed, empirical validation on diverse real-world datasets and models is limited.

**Confidence Labels:**
- High Confidence: The theoretical foundation for why characteristic functions cause misleading SHAP scores is robust
- Medium Confidence: The proposed characteristic functions theoretically address the identified issues, but their practical effectiveness requires further validation
- Low Confidence: The claim that complexity is manageable with polynomial-time algorithms for certain model families lacks specific computational complexity analysis or benchmarks

## Next Checks
1. Implement the proposed characteristic functions (υs, υa, υc, υn) and compare SHAP scores against standard implementations on diverse datasets to assess improvements in feature importance attribution
2. Conduct a detailed analysis of the computational complexity of the proposed characteristic functions, including benchmarks for large-scale models and datasets
3. Test the robustness of the new SHAP scores against adversarial attacks to ensure they remain non-misleading under perturbations