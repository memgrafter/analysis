---
ver: rpa2
title: 'Towards Faithful Natural Language Explanations: A Study Using Activation Patching
  in Large Language Models'
arxiv_id: '2410.14155'
source_url: https://arxiv.org/abs/2410.14155
tags:
- faithfulness
- explanation
- causal
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Causal Faithfulness (CaF), a novel metric
  for measuring the faithfulness of natural language explanations generated by large
  language models (LLMs). The method leverages activation patching, a causal mediation
  technique, to quantify the consistency of causal attributions between explanations
  and corresponding model outputs.
---

# Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models

## Quick Facts
- arXiv ID: 2410.14155
- Source URL: https://arxiv.org/abs/2410.14155
- Authors: Wei Jie Yeo; Ranjan Satapathy; Erik Cambria
- Reference count: 27
- Key outcome: Introduces Causal Faithfulness (CaF) metric using activation patching to measure explanation faithfulness in LLMs, showing instruct-tuned models produce more faithful explanations

## Executive Summary
This paper introduces Causal Faithfulness (CaF), a novel metric for measuring the faithfulness of natural language explanations generated by large language models. The method leverages activation patching, a causal mediation technique, to quantify the consistency of causal attributions between explanations and corresponding model outputs. Unlike existing approaches that rely on perturbations at the explanation or feature level, CaF examines the model's internal computations by measuring causal effects at both token and layer levels. Experiments across six open-sourced LLMs (ranging from 2B to 27B parameters) on three benchmarks (CoS-E, e-snli, and ComVE) demonstrate that models undergoing alignment tuning produce more faithful and plausible explanations.

## Method Summary
The CaF metric measures explanation faithfulness by comparing causal effects between answer generation and explanation generation using activation patching. The approach involves creating counterfactual instances through token replacement, generating both answers and explanations from target LLMs, and applying activation patching across all token positions and layers. Causal matrices are created by comparing clean, corrupted, and patched runs, with cosine distance measuring distribution differences. The method examines consistency at both token and layer levels, providing a more reliable faithfulness assessment by avoiding out-of-distribution concerns that can undermine existing metrics like SHAP.

## Key Results
- Instruct-tuned models consistently achieve higher CaF scores and better task performance than pre-trained counterparts
- Positive correlations observed between plausibility scores and faithfulness across all evaluated models
- CaF provides more reliable faithfulness assessments by examining internal model states rather than external perturbations
- Layer-wise analysis captures model reasoning process more accurately than token-level methods alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation patching provides a causal measure of explanation faithfulness by comparing internal model states
- Mechanism: The approach measures causal effects by patching clean activation states into corrupted runs and comparing distributions between answer and explanation outputs
- Core assumption: Internal model states contain sufficient information to determine the causal relationship between reasoning and explanation
- Evidence anchors:
  - [abstract] "Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness"
  - [section] "We argue that assessing faithfulness according to the referenced definition, requires making judgments beyond just at the feature level"
  - [corpus] Found 25 related papers with average FMR=0.517, suggesting moderate relevance in the field
- Break condition: If the model's internal representations do not reliably encode the reasoning process, or if the patching intervention fails to capture meaningful causal effects

### Mechanism 2
- Claim: Instruct-tuned models produce more faithful explanations due to alignment with task structure
- Mechanism: Fine-tuning on instruction-following data helps models develop better internal representations of task requirements, leading to more consistent causal patterns between answers and explanations
- Core assumption: Alignment tuning improves the model's ability to internally reason about tasks before generating explanations
- Evidence anchors:
  - [abstract] "experiments across six open-sourced LLMs...demonstrate that models undergoing alignment tuning produce more faithful and plausible explanations"
  - [section] "instruct-tuned models tend to produce more faithful explanations, as indicated by CaF"
  - [corpus] Related work on faithful explanation generation suggests alignment improves faithfulness
- Break condition: If alignment introduces spurious correlations or if the task structure becomes too complex for the model to maintain consistent internal reasoning

### Mechanism 3
- Claim: Layer-wise analysis provides more accurate faithfulness assessment than token-level alone
- Mechanism: By examining causal effects at multiple layers, the approach captures the model's reasoning process over time rather than just final outputs
- Core assumption: Faithful explanations should reflect the model's internal processing at multiple stages, not just input-output relationships
- Evidence anchors:
  - [abstract] "CaF examines the model's internal computations by measuring causal effects at both token and layer levels"
  - [section] "we explore at a deeper level, by also examining consistency at the layer positions"
  - [corpus] Weak evidence - no direct citations found about layer-wise faithfulness assessment
- Break condition: If layer-wise patterns become too noisy or if the model's reasoning is primarily concentrated in specific layers that don't align with explanation generation

## Foundational Learning

- Concept: Causal mediation analysis
  - Why needed here: The paper relies on understanding how interventions propagate through model layers to measure causal effects
  - Quick check question: What is the difference between direct and indirect effects in causal mediation analysis?
- Concept: Activation patching technique
  - Why needed here: The core measurement method involves replacing model states and measuring output changes
  - Quick check question: How does activation patching differ from traditional feature attribution methods?
- Concept: Out-of-distribution concerns in model analysis
  - Why needed here: The paper emphasizes avoiding OOD samples that could invalidate faithfulness measurements
  - Quick check question: Why might Gaussian noise corruption lead to out-of-distribution samples in activation patching?

## Architecture Onboarding

- Component map: Forward pass generation (clean, corrupted, patched) -> Causal effect calculation across T tokens and L layers -> Distribution comparison using cosine distance -> Aggregation functions for different granularities
- Critical path: Clean run → Corrupted run → Patched run → Causal matrix generation → Distribution comparison → Faithfulness score
- Design tradeoffs: Layer-wise analysis provides more accurate faithfulness assessment but increases computational cost compared to token-level only methods
- Failure signatures: High disagreement between CaF and existing metrics, OOD sample generation causing model behavior changes, noisy causal distributions indicating poor internal consistency
- First 3 experiments:
  1. Implement basic activation patching on a small model and verify causal effects are non-zero
  2. Compare CaF scores between pre-trained and instruct-tuned versions of the same model
  3. Test the impact of layer aggregation (CaF(T) vs CaF(L)) on faithfulness measurements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CaF scale with model size and does the relationship between plausibility and faithfulness hold for larger models beyond 27B parameters?
- Basis in paper: [explicit] The paper shows positive correlation between plausibility and faithfulness in models ranging from 2B to 27B parameters, with instruct-tuned models performing better.
- Why unresolved: The study only covers models up to 27B parameters. The relationship between model scale, plausibility, and faithfulness may plateau or change for larger models like GPT-4 or Claude.
- What evidence would resolve it: Testing CaF across a broader range of model sizes (e.g., 70B+ parameters) while maintaining the plausibility-faithfulness correlation analysis.

### Open Question 2
- Question: Can CaF be adapted to work with encoder-decoder architectures or non-transformer models?
- Basis in paper: [inferred] The paper focuses exclusively on decoder-only transformer models and leverages their specific attention mechanisms for activation patching.
- Why unresolved: The methodology is deeply tied to transformer decoder architecture with its causal attention mechanism. Encoder-decoder models have different internal structures that may require modified approaches.
- What evidence would resolve it: Demonstrating CaF or a modified version on models like BERT, T5, or other non-transformer architectures.

### Open Question 3
- Question: How sensitive is CaF to the choice of token replacement strategy and could alternative counterfactual generation methods improve faithfulness measurement?
- Basis in paper: [explicit] The paper compares STR to GN and SHAP, showing STR's advantages, but doesn't explore other counterfactual generation strategies.
- Why unresolved: The study only evaluates one token replacement method (STR). Different counterfactual generation approaches might better capture model reasoning or be more robust to certain types of explanations.
- What evidence would resolve it: Comparative studies testing multiple counterfactual generation strategies (e.g., semantic-preserving edits, gradient-based modifications) against CaF.

### Open Question 4
- Question: Does CaF capture the faithfulness of explanations that involve reasoning chains or multi-step inference processes?
- Basis in paper: [inferred] The paper evaluates explanations on tasks requiring commonsense reasoning and natural language inference, but doesn't specifically analyze the complexity of reasoning chains.
- Why unresolved: The study doesn't differentiate between simple explanations and those involving complex multi-step reasoning. CaF might be less effective for evaluating explanations that require tracking reasoning through multiple intermediate steps.
- What evidence would resolve it: Analyzing CaF scores on tasks with varying reasoning complexity and comparing faithfulness scores for explanations of different reasoning depths.

### Open Question 5
- Question: How does the window size for multi-layer patching affect CaF scores and what is the optimal window size for different model sizes?
- Basis in paper: [explicit] The paper uses a fixed window size of 10 for multi-layer patching but doesn't explore how different window sizes affect the results.
- Why unresolved: The choice of window size is arbitrary and may impact the sensitivity of CaF measurements. Different model sizes or task types might benefit from different window sizes.
- What evidence would resolve it: Systematic experiments varying the window size across different models and tasks to identify optimal parameters and understand how window size affects faithfulness detection.

## Limitations
- Experimental scope limited to six models and three benchmarks, potentially limiting generalizability
- Template-based explanation generation could introduce artifacts affecting faithfulness measurements
- Reliance on activation patching assumes internal representations directly encode causal reasoning processes

## Confidence
- Claim: CaF is a reliable metric for measuring explanation faithfulness -> Medium
- Claim: Alignment tuning improves faithfulness -> Medium-High
- Claim: Layer-wise analysis captures reasoning better than token-level -> Low-Medium

## Next Checks
1. Test CaF on models with different architectural designs (e.g., sparse attention, mixture-of-experts) to verify generalizability across model families
2. Conduct ablation studies on the impact of explanation generation templates to isolate their effect on faithfulness measurements
3. Implement cross-task validation by applying CaF to domains beyond the current benchmarks (e.g., mathematical reasoning, code generation) to assess robustness