---
ver: rpa2
title: Efficient Few-shot Learning for Multi-label Classification of Scientific Documents
  with Many Classes
arxiv_id: '2410.05770'
source_url: https://arxiv.org/abs/2410.05770
tags:
- training
- label
- classification
- scientific
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes FusionSent, a method for few-shot multi-label\
  \ classification of scientific documents. FusionSent fine-tunes two sentence embedding\
  \ models\u2014one using contrastive loss on training examples, and one using contrastive\
  \ loss on training examples and their label texts\u2014and merges them using Spherical\
  \ Linear Interpolation (SLERP)."
---

# Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes

## Quick Facts
- **arXiv ID**: 2410.05770
- **Source URL**: https://arxiv.org/abs/2410.05770
- **Reference count**: 14
- **Primary result**: FusionSent improves few-shot multi-label classification of scientific documents by an average of 6.0 F1 points over strong baselines

## Executive Summary
FusionSent addresses the challenge of few-shot multi-label classification of scientific documents with many classes by fine-tuning two sentence embedding models using different contrastive approaches and merging them via SLERP. The method demonstrates significant performance improvements across multiple scientific document classification datasets, including a newly introduced dataset of 203,961 arXiv articles categorized into 130 classes. FusionSent is particularly effective in few-shot learning scenarios where labeled data is scarce, showing robust performance even with minimal label text information.

## Method Summary
FusionSent fine-tunes two sentence embedding models using contrastive learning: one trained on instance pairs sharing the same class (SetFit-style), and another trained on instance-label text pairs. The models are merged using Spherical Linear Interpolation (SLERP) to combine their complementary knowledge, then frozen and used with a logistic regression head for classification. The method works by contrasting similar instances against dissimilar ones in embedding space, with the second model additionally pushing instances toward semantically relevant label descriptions. This dual-model approach allows FusionSent to capture both instance-instance and instance-label relationships in the embedding space.

## Key Results
- FusionSent achieves an average improvement of 6.0 F1 points over strong baselines across multiple scientific document classification datasets
- The method demonstrates significant effectiveness in few-shot learning scenarios with minimal labeled examples
- FusionSent shows robust performance even when using simple label names instead of detailed descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FusionSent achieves better performance by combining two complementary sentence embedding models via SLERP
- Mechanism: The method fine-tunes one model on training instance pairs and another on instance-label text pairs, then merges them with SLERP to preserve and combine complementary knowledge
- Core assumption: Training instance pairs and instance-label text pairs provide complementary information that cannot be fully captured by a single fine-tuned model
- Evidence anchors:
  - [abstract]: "FusionSent uses available training examples and their respective label texts to contrastively fine-tune two different sentence embedding models... Afterward, the parameters of both fine-tuned models are fused to combine the complementary knowledge..."
  - [section]: "Conversely, the FusionSent results indicate that this limitation can be circumvented by training separate sentence embedding models with different contrastive learning approaches and subsequently merging their parameters."
  - [corpus]: No direct corpus evidence; relies on internal paper results
- Break condition: If instance-label text pairs do not provide meaningful complementary information, or if the SLERP merge introduces instability

### Mechanism 2
- Claim: Using label texts for contrastive training improves classification by pushing instances toward their label text representations in embedding space
- Mechanism: During training, instances are paired with their label texts as positives and with other label texts as negatives, encouraging the model to map instances close to semantically relevant label descriptions
- Core assumption: Label texts (either names or descriptions) provide semantically meaningful guidance that improves instance clustering for classification
- Evidence anchors:
  - [abstract]: "One model is fine-tuned to maximize similarities between training examples sharing the same class, and the other model is fine-tuned to maximize similarities between training examples and their corresponding label texts."
  - [section]: "The results demonstrate that using label texts for sentence embedding training can help to separate instances of different classes in the embedding space..."
  - [corpus]: No direct corpus evidence; inferred from experiment results
- Break condition: If label texts are irrelevant or noisy, causing incorrect embeddings that hurt classification

### Mechanism 3
- Claim: SLERP merging preserves embedding space geometry better than simple weight averaging
- Mechanism: SLERP performs spherical interpolation between parameter vectors, maintaining directional relationships and preventing collapse of learned representations
- Core assumption: Fine-tuned models lie on a spherical manifold in parameter space, and SLERP respects this geometry
- Evidence anchors:
  - [section]: "In the third step, the parameters of the fine-tuned sentence embedding models obtained in steps one and two are merged using Spherical Linear Interpolation (SLERP) (Shoemake, 1985)."
  - [section]: "This approach ensures that the individually trained models encode different information, whereas merging allows their respective knowledge to complement each other..."
  - [corpus]: No direct corpus evidence; relies on SLERP's established use in graphics and optimization
- Break condition: If parameter spaces are not well-aligned or if SLERP introduces gradient issues during merging

## Foundational Learning

- Concept: Contrastive learning for sentence embeddings
  - Why needed here: Core mechanism for aligning similar instances and pushing apart dissimilar ones in embedding space
  - Quick check question: What is the difference between cosine similarity loss and contrastive loss in the context of sentence embedding fine-tuning?

- Concept: Multi-label classification and embedding-based classification heads
  - Why needed here: FusionSent embeds instances and uses logistic regression heads for multi-label prediction
  - Quick check question: How does logistic regression operate on embedded features for multi-label classification compared to softmax-based classification?

- Concept: Model fusion techniques (weight merging, SLERP)
  - Why needed here: FusionSent merges two separately fine-tuned models into one via SLERP
  - Quick check question: What are the key differences between SLERP, weight averaging, and ensemble methods in model fusion?

## Architecture Onboarding

- Component map: Base PLM → Two fine-tuning branches (instance-pair contrastive, instance-label-pair contrastive) → SLERP merge → Frozen model body → Logistic regression head

- Critical path:
  1. Sample training pairs (positive/negative)
  2. Fine-tune first model (SetFit-style)
  3. Fine-tune second model (Label Embedding-style)
  4. Merge models via SLERP
  5. Freeze merged model
  6. Train logistic regression head on embedded training instances

- Design tradeoffs:
  - Separate fine-tuning + SLERP vs. sequential fine-tuning on one model
  - SLERP complexity vs. simple weight averaging
  - Small batch sizes (1-4) vs. larger batches for stability
  - Using GPT-4 for label descriptions vs. using simple label names

- Failure signatures:
  - Poor performance if SLERP merge is unstable
  - Overfitting if training pairs are too few
  - Label text noise causing degraded embeddings
  - Memory issues if models are too large for available hardware

- First 3 experiments:
  1. Run FusionSent with simple label names only (no GPT-4 descriptions) to test robustness
  2. Compare SLERP merge vs. weight averaging to quantify geometric benefits
  3. Vary SLERP interpolation factor (t) from 0.3 to 0.7 to see effect on performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored including the impact of different interpolation factors in SLERP, performance with poor quality label texts, and effects of using different base PLMs.

## Limitations
- Performance heavily depends on having semantically meaningful label texts, which may not always be available or may introduce noise if poorly generated
- The effectiveness of SLERP merging versus simpler methods is not conclusively demonstrated across diverse settings
- Scalability to extremely large class spaces (>500 classes) has not been thoroughly evaluated

## Confidence
- **High**: Core claim that FusionSent significantly improves few-shot multi-label classification, supported by consistent F1 score improvements across multiple datasets
- **Medium**: Mechanism of complementary knowledge capture via dual contrastive training, as the paper provides strong empirical results but limited ablation studies on why SLERP specifically outperforms alternatives
- **Medium**: Claim that label texts meaningfully enhance embeddings, since the benefit is dataset-dependent and not universally robust

## Next Checks
1. **Label Text Robustness**: Test FusionSent with minimal label texts (e.g., just category names) to determine how much performance relies on descriptive label generation
2. **SLERP vs. Alternatives**: Compare SLERP merging against simple weight averaging and ensemble methods to quantify geometric benefits in embedding space
3. **Scaling to Larger Class Spaces**: Evaluate FusionSent on datasets with >500 classes to assess its scalability and identify performance degradation thresholds