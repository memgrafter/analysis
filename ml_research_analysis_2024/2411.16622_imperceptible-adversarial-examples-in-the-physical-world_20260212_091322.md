---
ver: rpa2
title: Imperceptible Adversarial Examples in the Physical World
arxiv_id: '2411.16622'
source_url: https://arxiv.org/abs/2411.16622
tags:
- adversarial
- examples
- physical
- world
- imperceptible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating adversarial examples
  in the physical world, where image distortions in visual sensing systems (like printers
  and cameras) make traditional digital adversarial attacks ineffective. The authors
  propose a novel approach using a Straight-Through Estimator (STE) to overcome the
  non-differentiability of these distortions.
---

# Imperceptible Adversarial Examples in the Physical World

## Quick Facts
- arXiv ID: 2411.16622
- Source URL: https://arxiv.org/abs/2411.16622
- Reference count: 40
- Primary result: Straight-through estimator (STE) enables ℓ∞-bounded imperceptible adversarial examples in physical world against classification and detection models

## Executive Summary
This work addresses the challenge of generating adversarial examples in the physical world, where image distortions in visual sensing systems (like printers and cameras) make traditional digital adversarial attacks ineffective. The authors propose a novel approach using a Straight-Through Estimator (STE) to overcome the non-differentiability of these distortions. STE applies exact, non-differentiable distortions in the forward pass and uses the identity function in the backward pass, enabling backpropagation. The method is extended to patch perturbations by combining STE with differentiable rendering. Experiments using printout photos and the CARLA simulator demonstrate that STE enables the generation of ℓ∞-bounded adversarial examples that are imperceptible in the physical world.

## Method Summary
The paper introduces Straight-Through Estimator (STE) to enable gradient-based adversarial attacks in physical imaging pipelines. STE applies exact, non-differentiable distortions in the forward pass while using the identity function in the backward pass, allowing backpropagation through printers and cameras. For patch perturbations, STE is combined with differentiable rendering to approximate non-differentiable game engines. The approach is evaluated using printed images photographed with cameras against ResNet-50 classifiers, and CARLA simulations against Faster R-CNN detectors. The method successfully forces zero classification accuracy and reduces object detection AP50 from 43.29% to 4.22% with imperceptible perturbations.

## Key Results
- Zero classification accuracy on ResNet-50 using printed adversarial examples with STE and FGSM attack (ℓ∞ = 16/255)
- AP50 reduced from 43.29% to 4.22% in patch perturbation threat model using STE with differentiable rendering
- Imperceptible adversarial patches generated with ℓ∞ bounds as small as 8/255 remain effective against object detectors
- STE enables successful attacks where standard methods fail due to physical imaging pipeline non-differentiability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Straight-through estimator (STE) allows gradient backpropagation through non-differentiable imaging pipeline distortions.
- Mechanism: STE applies exact, non-differentiable distortions in the forward pass while using the identity function in the backward pass, enabling gradient flow despite non-differentiability.
- Core assumption: The sign of the gradient remains accurate even if the magnitude is approximated, which is sufficient for ℓ∞-bounded attacks that use gradient sign.
- Evidence anchors:
  - [abstract] "We employ STE to overcome the non-differentiability – applying exact, non-differentiable distortions in the forward pass of the backpropagation step, and using the identity function in the backward pass."
  - [section 3.1] "STE uses the non-differentiable output in the forward pass to obtain the exact loss function value but the identity function in the backward pass to approximate its gradient with respect to the input."
- Break condition: If the non-differentiable distortions cause large pixel value shifts that change the sign of gradients, the approximation becomes unreliable.

### Mechanism 2
- Claim: Differentiable rendering combined with STE enables imperceptible adversarial patches in 3D environments.
- Mechanism: The non-differentiable renderer is approximated by a differentiable renderer in the forward pass, with the difference detached as a constant using stop-gradient operation, allowing backpropagation through the rendering pipeline.
- Core assumption: The differentiable renderer produces similar outputs to the non-differentiable renderer for small perturbations, making the approximation error negligible.
- Evidence anchors:
  - [section 4.1] "gd(δ, x, e) outputs the same as c(p(x + δ), e) does in the forward pass, because r(x + δ) cancels out −r(x + δ) numerically."
  - [section 4.1] "As long as r(·) renders x + δ at the same location as in c(p(x + δ), e), we have the property c(p(x + δ), e) ≈ r(x + δ) for all pixels of the perturbed image x + δ"
- Break condition: When the environment or lighting conditions cause significant rendering differences between the differentiable and non-differentiable renderers.

### Mechanism 3
- Claim: Small ℓ∞ perturbations remain imperceptible after physical distortions in printing and camera capture.
- Mechanism: Physical distortions (printer color inaccuracies, camera sensor imperfections) preserve the relative structure of small perturbations, making them visually undetectable to humans.
- Core assumption: Human visual perception is less sensitive to small absolute changes in pixel values than to the relative structure of patterns.
- Evidence anchors:
  - [section 3.3] "We are able to force zero accuracy when we print the digital adversarial examples out on paper... In particular, we are able to force zero accuracy when we combine STE with the single-step FGSM attack bounded by ϵ = 16 /255."
  - [section 4.3] "The adversarial patches shown in the three center columns cause many hallucinations to the target object detection model but remain imperceptible."
- Break condition: If physical distortions amplify certain perturbation patterns beyond human perceptibility thresholds.

## Foundational Learning

- Concept: Straight-through estimator (STE) and its use in backpropagation through non-differentiable functions.
  - Why needed here: The core challenge is that physical imaging pipelines contain non-differentiable components (printers, cameras) that prevent standard gradient-based adversarial attack methods from working.
  - Quick check question: How does STE differ from simply ignoring non-differentiable operations during backpropagation?

- Concept: Differentiable rendering and its limitations compared to non-differentiable game engines.
  - Why needed here: To create adversarial patches in 3D environments like CARLA, we need to approximate the non-differentiable UE4 renderer with differentiable alternatives while maintaining effectiveness.
  - Quick check question: What are the key differences between differentiable renderers like PyTorch3D and non-differentiable engines like UE4 that could affect attack success?

- Concept: Threat models in adversarial machine learning (global vs patch perturbations).
  - Why needed here: The paper demonstrates STE effectiveness under both threat models, requiring understanding of how perturbations are constrained and applied differently in each case.
  - Quick check question: How does the optimization objective differ between global and patch perturbation threat models?

## Architecture Onboarding

- Component map: Digital perturbation → STE-wrapped physical pipeline → Target model → Loss computation → Backpropagation through STE → Perturbation update
- Critical path: Digital perturbation → STE wrapper → Physical imaging pipeline (printer/camera) → Target model → Loss → Backpropagation through STE → Update perturbation
- Design tradeoffs: STE provides exact forward pass values but approximate gradients; differentiable rendering is computationally efficient but may not perfectly match non-differentiable outputs
- Failure signatures: Attack success drops when physical distortions significantly alter perturbation structure; perceptibility increases when ℓ∞ bounds are too large relative to physical noise floor
- First 3 experiments:
  1. Implement STE wrapper around a simple printer-camera pipeline and verify gradient flow with a toy CNN classifier.
  2. Test FGSM with STE on ImageNet printouts with varying ℓ∞ bounds to find perceptibility thresholds.
  3. Combine STE with differentiable rendering to create adversarial patches in CARLA and measure AP50 degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can imperceptible adversarial patches be extended to 3D objects beyond rectangular screens in the physical world?
- Basis in paper: [explicit] "One natural extension is to produce imperceptible adversarial camouflage for 3D objects in the physical world" (page 8)
- Why unresolved: The paper only demonstrates 2D rectangular patches and mentions this as future work without experimental validation
- What evidence would resolve it: Successful experiments showing imperceptible adversarial textures on 3D objects that fool detection models in real-world conditions

### Open Question 2
- Question: How effective would STE-augmented attacks be against defenses specifically designed to detect physical adversarial examples?
- Basis in paper: [inferred] The paper mentions existing defenses detect "abnormal textures" and urges re-evaluation of physical adversarial threats, but doesn't test against such defenses
- Why unresolved: The experiments only measure effectiveness against undefended models, not defensive systems
- What evidence would resolve it: Experiments showing attack success rates against state-of-the-art physical adversarial defense methods

### Open Question 3
- Question: What is the optimal balance between imperceptible perturbations and attack success rate across different environmental conditions?
- Basis in paper: [explicit] "we also evaluate ℓ∞ attacks bounded by small ϵ to produce imperceptible adversarial patches" and discuss trade-offs (page 7)
- Why unresolved: The paper shows effectiveness for specific ℓ∞ bounds but doesn't systematically explore the relationship between imperceptibility and robustness
- What evidence would resolve it: A comprehensive study mapping ℓ∞ bounds to success rates across varying lighting, angles, and weather conditions

## Limitations

- STE's effectiveness depends on the assumption that gradient approximation remains reliable under varying physical distortion conditions, which may not hold for severe or non-uniform distortions
- The threat model assumes white-box access to the target model, limiting real-world applicability where such access is unavailable
- Imperceptibility is evaluated using ℓ∞ norm as a proxy for human perception without direct human subject studies validating visual imperceptibility

## Confidence

- Mechanism 1 (STE gradient approximation): Medium confidence due to limited empirical validation across diverse physical imaging conditions
- Mechanism 2 (Differentiable rendering approximation): Medium-Low confidence because the paper doesn't provide quantitative error bounds between differentiable and non-differentiable renderers
- Mechanism 3 (Perceptibility preservation): Medium confidence based on controlled experiments but lacking comprehensive human perception validation

## Next Checks

1. Cross-imager validation: Test STE effectiveness across multiple printer-camera combinations to quantify performance variance and identify failure modes when physical distortions vary significantly.

2. Human perception study: Conduct controlled user studies with varying viewing distances and lighting conditions to empirically validate that ℓ∞-bounded perturbations remain truly imperceptible to human observers.

3. Gradient error analysis: Systematically measure the discrepancy between STE-approximated gradients and analytical gradients (where computable) across different perturbation magnitudes and image regions to quantify approximation error.