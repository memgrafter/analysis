---
ver: rpa2
title: Romanization Encoding For Multilingual ASR
arxiv_id: '2407.04368'
source_url: https://arxiv.org/abs/2407.04368
tags:
- test
- speech
- data
- encoding
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes romanization encoding for multilingual and
  code-switching ASR systems. The method applies romanization (e.g., Pinyin) to script-heavy
  languages and combines it with a balanced concatenated tokenizer in a FastConformer-RNNT
  framework with a Roman2Char decoder.
---

# Romanization Encoding For Multilingual ASR

## Quick Facts
- arXiv ID: 2407.04368
- Source URL: https://arxiv.org/abs/2407.04368
- Reference count: 0
- Primary result: 63.51% vocabulary reduction with 13.72% and 15.03% performance gains on SEAME code-switching benchmarks

## Executive Summary
This paper proposes a romanization encoding approach for multilingual and code-switching ASR systems that addresses the vocabulary explosion problem in script-heavy languages. The method replaces character-based vocabularies with phonetic romanized representations (like Pinyin for Mandarin) combined with a balanced concatenated tokenizer, enabling significant vocabulary reduction while maintaining recognition accuracy. A novel Roman2Char decoder is introduced to map romanized predictions back to original characters, preserving the flexibility to output native scripts while simplifying the acoustic modeling task.

## Method Summary
The approach uses FastConformer-RNNT architecture with romanized text encoding and a balanced concatenated tokenizer. Mandarin, Korean, and Japanese text are converted to Pinyin, kroman, and romaji respectively using PyPinyin, kroman, and pykakasi toolkits. A concatenated tokenizer combines pre-trained BPE tokenizers for English and romanized script-heavy languages, maintaining separate vocabulary ranges for each language. The Roman2Char decoder, a Transformer-based model, learns to map romanized outputs back to original characters. Training uses Adam optimizer with Cosine Annealing, SpecAug data augmentation, and greedy search evaluation without external language models.

## Key Results
- 63.51% vocabulary reduction from romanization (5,178 to 1,239 units for Mandarin)
- 13.72% and 15.03% Mixed Error Rate improvements on SEAME Mandarin-English code-switching benchmarks
- Effective across multiple script-heavy languages (Mandarin-Korean, Mandarin-Japanese) through ablation studies

## Why This Works (Mechanism)

### Mechanism 1
Romanization reduces vocabulary size by replacing large character sets with smaller Latin-script phonetic representations. Phonetic scripts like Pinyin map multiple characters to single romanized tokens, compressing vocabulary from thousands to hundreds of units while preserving acoustic-phonetic information.

### Mechanism 2
Balanced concatenated tokenizers enable language-specific modeling while maintaining unified vocabulary space. Separate token index ranges per language allow the model to learn language-specific acoustic patterns without interference between languages.

### Mechanism 3
Roman-to-Character decoder enables efficient training while maintaining native script output capability. The separate decoder learns to map romanized predictions back to original characters without affecting acoustic model training.

## Foundational Learning

- **Phonetic scripts and romanization systems**: Understanding how characters map to phonetic representations is fundamental to grasping why vocabulary reduction occurs. *Quick check: How many distinct Pinyin representations typically exist for Chinese characters compared to the total number of characters?*

- **Tokenization strategies in multilingual ASR**: The concatenated tokenizer approach is central to how the method balances language-specific modeling with unified vocabulary. *Quick check: What are the advantages and disadvantages of byte-pair encoding versus character-based tokenization for script-heavy languages?*

- **End-to-end training architectures with auxiliary decoders**: The Roman2Char decoder architecture is key to understanding how the system maintains flexibility while reducing vocabulary complexity. *Quick check: How does training with multiple loss functions affect gradient flow and model convergence?*

## Architecture Onboarding

- **Component map**: Audio input → FastConformer → RNNT prediction → Roman2Char conversion → final output
- **Critical path**: Audio input → FastConformer encoder → RNNT decoder → Roman2Char decoder → native script output
- **Design tradeoffs**: Vocabulary size reduction vs. potential loss of script-specific information
- **Failure signatures**: Poor performance on tonal languages, decoder mapping errors, imbalanced token distribution
- **First 3 experiments**:
  1. Compare baseline character-based model vs. romanized model on monolingual Mandarin test set
  2. Measure vocabulary size reduction and training batch size increase with romanization
  3. Test Roman2Char decoder accuracy independently before integrating into full system

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the Romanization Encoding method compare to other text encoding approaches (e.g., BBPE, WPM) for multilingual and code-switching ASR tasks when applied to languages with complex scripts? The paper mentions that BBPE-based bilingual systems underperform and Google USM's WPM approach struggles with script diversity, but doesn't directly compare to these methods.

### Open Question 2
What is the impact of the Romanization Encoding method on the model's ability to handle code-switching points and language transitions in speech? The paper mentions decoupling acoustic and language modeling but doesn't explicitly discuss impact on code-switching points and language transitions.

### Open Question 3
How does the Romanization Encoding method affect the model's ability to generalize to unseen words or phrases in multilingual and code-switching ASR tasks? The paper mentions significant vocabulary reduction but doesn't discuss the impact on generalization to unseen words or phrases.

## Limitations

- Limited ablation studies on Roman2Char decoder performance in isolation, particularly for tonal languages where multiple characters share the same romanization
- Results primarily validated on Mandarin-English code-switching, with limited diversity in script-heavy language combinations
- No direct comparison to alternative text encoding approaches like BBPE or WPM for multilingual scenarios

## Confidence

- **Medium**: Vocabulary reduction claims based primarily on Mandarin transformation
- **Low**: Roman2Char decoder effectiveness with limited isolation testing
- **Medium**: Code-switching performance improvements specific to Mandarin-English scenario

## Next Checks

1. Test Roman2Char decoder accuracy on held-out romanized text with known character mappings before full system integration
2. Evaluate cross-lingual transfer by training on Mandarin-English and testing on Mandarin-Korean/Japanese test sets
3. Systematically vary romanization granularity to identify optimal vocabulary size reduction vs. accuracy tradeoff for different script-heavy languages