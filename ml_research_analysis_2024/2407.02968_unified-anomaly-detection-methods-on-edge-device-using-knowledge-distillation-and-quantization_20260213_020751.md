---
ver: rpa2
title: Unified Anomaly Detection methods on Edge Device using Knowledge Distillation
  and Quantization
arxiv_id: '2407.02968'
source_url: https://arxiv.org/abs/2407.02968
tags:
- performance
- anomaly
- quantization
- detection
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses anomaly detection in manufacturing using multi-class
  unified models, replacing separate one-class models per object class. Three lightweight
  knowledge-distillation-based methods (Uninformed Students, Reverse Distillation,
  and STFPM) are trained and tested on the MVTec AD dataset in a 15-class setup.
---

# Unified Anomaly Detection methods on Edge Device using Knowledge Distillation and Quantization

## Quick Facts
- arXiv ID: 2407.02968
- Source URL: https://arxiv.org/abs/2407.02968
- Reference count: 30
- Primary result: Multi-class unified anomaly detection models perform comparably to one-class models and can be efficiently deployed on edge devices with significant latency reduction.

## Executive Summary
This work presents a unified approach to anomaly detection on edge devices by replacing separate one-class models with multi-class unified models. Three knowledge-distillation-based methods (Uninformed Students, Reverse Distillation, and STFPM) are trained and tested on the MVTec AD dataset. The study demonstrates that multi-class models perform at par with one-class models when object classes are visually distinct. These lightweight models are deployed on CPU and NVIDIA Jetson Xavier NX, achieving 5-13x latency reduction on the edge device. The research also explores quantization techniques, showing that post-training quantization with random normal data calibration outperforms training data calibration, and quantization-aware training further improves performance.

## Method Summary
The study introduces three lightweight knowledge-distillation-based anomaly detection methods trained in a multi-class setup on the MVTec AD dataset. The methods include Uninformed Students (US), Reverse Distillation (RD), and STFPM, which leverage hierarchical feature matching from teacher networks to student networks. These models are then quantized using post-training quantization (PTQ) with random normal data calibration and quantization-aware training (QAT). The quantized models are deployed on edge devices, specifically the NVIDIA Jetson Xavier NX, to evaluate latency and accuracy trade-offs.

## Key Results
- Multi-class unified models achieve AUROC comparable to one-class models on MVTec AD dataset
- STFPM achieves the best balance with ~94% AUROC and ~52ms latency on Jetson Xavier NX
- Post-training quantization with random normal data calibration outperforms training data calibration, with 8-15% performance boost
- Quantization-aware training (QAT) brings INT-8 results close to FP-32 performance

## Why This Works (Mechanism)

### Mechanism 1
Multi-class unified models can match one-class performance because anomalies are deviations from class-specific normal patterns, and those deviations are distinct enough not to overlap across classes. Each object class has a unique normal feature distribution; anomalies are detected as deviations from the learned class-specific normality. Since classes are visually and structurally distinct, their anomaly signatures don't overlap, so a single model can learn multiple class normals and still detect anomalies accurately.

### Mechanism 2
Knowledge distillation with multi-scale feature matching preserves discriminative features across classes while enabling lightweight student models. Teacher networks extract hierarchical features; student networks learn to mimic these via distillation losses. Multi-scale matching ensures low-level and high-level features are aligned, improving anomaly detection across object sizes and textures.

### Mechanism 3
Post-training quantization with random normal data calibration preserves performance better than training-data calibration for unsupervised anomaly detection. PTQ quantizes FP-32 weights/activations to INT-8 using calibration statistics. In unsupervised settings, training data lacks anomalies, so calibration based on random normal distribution better captures the full activation range, including anomaly regions, reducing quantization error.

## Foundational Learning

- **Knowledge distillation (teacher-student framework)**: Enables lightweight student models to achieve performance close to larger teacher models by transferring discriminative features. Quick check: In the distillation setup, which network's output serves as the target for the student to regress upon?

- **Quantization-aware training (QAT) vs post-training quantization (PTQ)**: QAT simulates quantization error during training, allowing weights to adapt and maintain performance after quantization; PTQ does not involve retraining. Quick check: What is the main difference in how PTQ and QAT handle quantization error?

- **Multi-scale feature matching in anomaly detection**: Captures both low-level (texture) and high-level (shape) anomalies, improving detection across object sizes and types. Quick check: Which layers' features are typically matched in multi-scale distillation for anomaly detection?

## Architecture Onboarding

- **Component map**: Teacher network (WideResNet-50, ResNet-18) -> Student network (5-layer CNN, ResNet-18) -> Anomaly scoring (regression error, cosine similarity, feature difference) -> Quantization (PTQ with random normal calibration or QAT) -> Edge deployment (Jetson Xavier NX)

- **Critical path**: 1. Train teacher on multi-class normal data 2. Train student via distillation (multi-scale matching for RD/STFPM) 3. Quantize model (PTQ with random normal calibration or QAT) 4. Deploy on edge device (TensorRT on Jetson)

- **Design tradeoffs**: Model size vs. latency: STFPM (smaller, faster) vs. RD (larger, slower); Calibration strategy: Training data (standard) vs. random normal (better for unsupervised); Framework choice: Torch (QAT) vs. TensorRT (PTQ, faster on Jetson)

- **Failure signatures**: High AUROC drop after quantization → check calibration method or try QAT; Increased latency on edge device → verify TensorRT optimization or reduce model size; Poor multi-class generalization → ensure classes are sufficiently distinct

- **First 3 experiments**: 1. Train US, RD, and STFPM in multi-class mode; compare AUROC to one-class baselines 2. Apply PTQ with random normal calibration; measure latency and AUROC drop on Jetson 3. Apply QAT to top-performing models; compare AUROC to PTQ and FP-32 versions

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of multi-class models vary when applied to datasets with more visually similar object classes compared to the MVTec AD dataset? The paper suggests that multi-class models work well on MVTec AD due to distinct object appearances, but does not test on datasets with similar classes.

### Open Question 2
What is the impact of using different calibration data distributions (e.g., Gaussian vs. uniform) on the performance of post-training quantization in unsupervised anomaly detection tasks? The paper shows that random normal data calibration outperforms training data calibration, but does not explore other distributions.

### Open Question 3
How does the choice of quantization framework (e.g., TensorRT vs. PyTorch) affect the performance of anomaly detection models on different edge devices? The paper compares TensorRT and PyTorch quantization, showing TensorRT's superiority on Jetson, but does not explore other devices or frameworks.

### Open Question 4
What is the long-term stability and robustness of quantized multi-class anomaly detection models when deployed in real-world industrial environments? The paper evaluates quantized models in controlled experiments but does not address long-term deployment in dynamic industrial settings.

## Limitations
- Findings are based on MVTec AD dataset with 15 visually distinct object classes; generalizability to datasets with similar classes remains untested
- Implementation details for random normal data calibration and TensorRT quantization parameters are not fully specified
- Performance validated only on NVIDIA Jetson Xavier NX; results may vary on other edge devices

## Confidence
- **High Confidence**: Multi-class unified model performance being comparable to one-class models
- **Medium Confidence**: Effectiveness of random normal data calibration for PTQ
- **Medium Confidence**: STFPM method achieving best balance of high AUROC and low latency

## Next Checks
1. Test multi-class unified models on a dataset with more visually similar object classes to validate class distinctiveness assumption
2. Perform ablation study on random normal data calibration method, varying distribution parameters to determine optimal settings
3. Validate latency and accuracy results on a different edge device (e.g., Intel Neural Compute Stick 2) to assess portability of quantization and deployment pipeline