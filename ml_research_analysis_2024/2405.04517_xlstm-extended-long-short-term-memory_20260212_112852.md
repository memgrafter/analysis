---
ver: rpa2
title: 'xLSTM: Extended Long Short-Term Memory'
arxiv_id: '2405.04517'
source_url: https://arxiv.org/abs/2405.04517
tags:
- xlstm
- m2d2
- unsplit
- dolma
- s2orc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'xLSTM introduces two main modifications to LSTM: exponential gating
  with normalization and stabilization, and new memory structures (scalar sLSTM with
  memory mixing and matrix mLSTM with covariance update rule). These enhancements
  overcome LSTM limitations in storage revision and capacity.'
---

# xLSTM: Extended Long Short-Term Memory

## Quick Facts
- arXiv ID: 2405.04517
- Source URL: https://arxiv.org/abs/2405.04517
- Authors: Maximilian Beck; Korbinian Pöppel; Markus Spanring; Andreas Auer; Oleksandra Prudnikova; Michael Kopp; Günter Klambauer; Johannes Brandstetter; Sepp Hochreiter
- Reference count: 40
- Key outcome: xLSTM introduces exponential gating and matrix memory structures that overcome LSTM limitations, achieving state-of-the-art validation perplexity on 15B tokens of SlimPajama and strong performance on synthetic tasks.

## Executive Summary
xLSTM extends LSTM with two key innovations: exponential gating with normalization and stabilization, and new memory structures (scalar sLSTM with memory mixing and matrix mLSTM with covariance update rule). These enhancements overcome LSTM's inability to revise storage decisions and enhance storage capacities. xLSTM models demonstrate strong performance on language modeling tasks, achieving competitive results compared to Transformers and State Space Models, and excel at synthetic tasks requiring state tracking and long-range context handling.

## Method Summary
xLSTM modifies the traditional LSTM architecture in two fundamental ways. First, it introduces exponential gating with appropriate normalization and stabilization techniques, allowing the model to revise previous storage decisions based on new evidence. Second, it replaces the scalar LSTM memory cell with matrix memory structures - sLSTM uses scalar memory with enhanced storage capacities through memory mixing, while mLSTM employs d×d matrix memory with a covariance update rule for storing key-value pairs. These components are integrated into residual block backbones, creating xLSTM architectures that are then stacked for deep learning applications.

## Key Results
- xLSTM[1:0] with 125M parameters achieves 1.6 validation perplexity on 15B tokens of SlimPajama
- xLSTM[7:1] with 361M parameters achieves 3.0 validation perplexity on 15B tokens of SlimPajama, outperforming Transformer and Mamba baselines
- xLSTM demonstrates strong scaling behavior and excels at synthetic tasks requiring state tracking, associative recall, and long-range context handling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exponential gating with normalization and stabilization overcomes LSTM's inability to revise storage decisions.
- Mechanism: Input and forget gates can have exponential activation functions. A normalizer state sums the product of the input gate and all future forget gates, enabling the model to revise previous storage decisions based on new evidence.
- Core assumption: The combination of exponential activation and normalizer state provides sufficient flexibility to override previous gating decisions.
- Evidence anchors:
  - [abstract]: "Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques."
  - [section]: "To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization."
  - [corpus]: Found 25 related papers with average neighbor FMR=0.455, indicating moderate relevance of xLSTM to related work, but no direct evidence of storage revision capability in cited papers.
- Break condition: If the normalizer state becomes numerically unstable or if the exponential gates cause overflow that cannot be stabilized.

### Mechanism 2
- Claim: Matrix memory with covariance update rule enhances storage capacities and enables better handling of rare tokens.
- Mechanism: The mLSTM replaces scalar cell states with d×d matrix memory. The covariance update rule (Ct = Ct-1 + vt k⊤t) stores key-value pairs, where retrieval is performed via matrix multiplication, allowing for quadratic storage capacity in the coding dimension.
- Core assumption: The matrix memory structure can efficiently represent and retrieve high-dimensional information patterns that would be compressed in scalar LSTM cells.
- Evidence anchors:
  - [abstract]: "Secondly, we modify the LSTM memory structure, obtaining: (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule."
  - [section]: "To enhance storage capacities of LSTMs, we increase the LSTM memory cell from a scalar c ∈ R to a matrix C ∈ Rd×d."
  - [corpus]: Found 25 related papers; no direct evidence of covariance update rule effectiveness in cited papers, though related to Fast Weight Programmers.
- Break condition: If the d×d matrix operations become computationally prohibitive for large d, or if the matrix memory becomes saturated for very long sequences.

### Mechanism 3
- Claim: Pre-LayerNorm residual backbones with exponential gating and modified memory structures enable scalable and performant xLSTM architectures.
- Mechanism: xLSTM blocks integrate sLSTM and mLSTM variants into residual structures with pre-LayerNorm normalization. The post up-projection block is used for sLSTM (with memory mixing) and pre up-projection for mLSTM (without memory mixing), enabling both sequential and parallel processing.
- Core assumption: The residual block structure with appropriate normalization and gating can scale LSTM variants to billions of parameters while maintaining training stability.
- Evidence anchors:
  - [abstract]: "Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures."
  - [section]: "xLSTM Architecture. An xLSTM architecture is constructed by residually stacking building blocks... We rely on the most commonly used pre-LayerNorm residual backbones."
  - [corpus]: Found 25 related papers; no direct evidence of residual block effectiveness for xLSTM in cited papers, though consistent with general deep learning practice.
- Break condition: If the residual connections and normalization fail to stabilize training at scale, or if the block structure introduces bottlenecks that prevent efficient parallelization.

## Foundational Learning

- Concept: Gating mechanisms in recurrent networks
  - Why needed here: Understanding how input, forget, and output gates control information flow is essential for grasping how xLSTM's exponential gating differs from traditional LSTM gating.
  - Quick check question: What is the primary function of the forget gate in a standard LSTM, and how does exponential activation change this behavior?

- Concept: Matrix operations and covariance updates
  - Why needed here: The mLSTM's matrix memory and covariance update rule (Ct = Ct-1 + vt k⊤t) require understanding of outer products and matrix multiplication for efficient implementation.
  - Quick check question: How does the covariance update rule in mLSTM differ from the scalar update in traditional LSTM, and what computational advantages does it provide?

- Concept: Residual network architectures
  - Why needed here: xLSTM architectures are built by stacking residual blocks, so understanding skip connections and normalization layers is crucial for implementation and debugging.
  - Quick check question: Why are residual connections particularly important for training very deep networks, and how does pre-LayerNorm differ from post-LayerNorm in this context?

## Architecture Onboarding

- Component map:
  - xLSTM family: sLSTM (scalar memory, exponential gating, memory mixing) and mLSTM (matrix memory, covariance update, parallelizable)
  - Block structures: post up-projection for sLSTM, pre up-projection for mLSTM
  - Architecture: residual stacking with pre-LayerNorm normalization
  - Key innovations: exponential gating with stabilization, matrix memory, covariance update rule

- Critical path:
  1. Implement basic LSTM cell as reference
  2. Add exponential gating with normalization and stabilization to create sLSTM
  3. Implement matrix memory with covariance update to create mLSTM
  4. Create residual blocks with appropriate up-projection strategy for each variant
  5. Stack blocks into full architecture with pre-LayerNorm

- Design tradeoffs:
  - sLSTM vs mLSTM: memory mixing capability vs parallelizability
  - Exponential vs sigmoid gating: storage revision capability vs numerical stability
  - Matrix memory size (d×d) vs computational complexity and GPU memory usage
  - Residual block structure vs potential for vanishing/exploding gradients

- Failure signatures:
  - NaN or inf values: likely exponential gate overflow without proper stabilization
  - Poor training convergence: incorrect normalization or gate initialization
  - Memory errors: matrix operations exceeding GPU memory capacity
  - Degraded performance: inappropriate choice of sLSTM vs mLSTM for task requirements

- First 3 experiments:
  1. Compare vanilla LSTM vs sLSTM on nearest neighbor search task to verify storage revision capability
  2. Compare LSTM vs mLSTM on associative recall task to verify enhanced memory capacity
  3. Implement small xLSTM[1:1] model and train on Wikitext-103 to verify end-to-end functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the exponential gating mechanism in xLSTM specifically improve storage revision compared to traditional sigmoid gating in LSTMs?
- Basis in paper: [explicit] The paper states that xLSTM overcomes LSTM's inability to revise storage decisions through exponential gating with normalization and stabilization.
- Why unresolved: While the paper demonstrates improved performance on synthetic tasks requiring storage revision, it doesn't provide a detailed mathematical or empirical analysis of how the exponential gating mechanism specifically addresses this limitation compared to sigmoid gating.
- What evidence would resolve it: A detailed ablation study comparing xLSTM with different gating mechanisms (exponential vs sigmoid) on tasks requiring storage revision, coupled with a theoretical analysis of the gating dynamics and their impact on gradient flow.

### Open Question 2
- Question: What is the optimal ratio of mLSTM to sLSTM blocks in xLSTM architectures for different task types and model sizes?
- Basis in paper: [explicit] The paper explores different ratios (e.g., xLSTM[1:0], xLSTM[7:1], xLSTM[1:1]) and shows that xLSTM[7:1] performs best on language modeling tasks, but the optimal ratio likely varies depending on task characteristics and model scale.
- Why unresolved: The paper doesn't provide a comprehensive analysis of how the mLSTM:sLSTM ratio affects performance across different task types and model sizes, leaving open the question of optimal architecture design.
- What evidence would resolve it: A systematic study varying the mLSTM:sLSTM ratio across different task types (e.g., language modeling, synthetic tasks, code generation) and model sizes, with performance metrics and architectural insights.

### Open Question 3
- Question: How does the matrix memory in mLSTM scale with sequence length, and what are the limitations for very long contexts?
- Basis in paper: [inferred] The paper mentions that the matrix memory is independent of sequence length and doesn't show limitations for contexts up to 16k, but doesn't explore the scaling behavior for extremely long sequences.
- Why unresolved: While the paper demonstrates good performance on 16k context lengths, it doesn't investigate the memory requirements and potential limitations of the matrix memory for much longer sequences (e.g., millions of tokens).
- What evidence would resolve it: Experiments scaling the context length to extreme values (e.g., millions of tokens) and analyzing memory usage, performance degradation, and potential bottlenecks in the matrix memory operations.

## Limitations

- Limited evaluation scope: The paper focuses primarily on language modeling and synthetic tasks, with limited comparison on broader downstream benchmarks where Transformers currently excel.
- Computational efficiency concerns: While xLSTM claims better theoretical efficiency, the paper doesn't provide comprehensive benchmarks comparing FLOPs, memory usage, or inference latency against Transformers of equivalent performance.
- Scalability uncertainty: The largest model evaluated is 1.25B parameters, making claims about xLSTM's viability for trillion-parameter models extrapolations without empirical validation.

## Confidence

- **High**: The core mathematical formulations for exponential gating and matrix memory are well-defined and implementable. The architectural modifications (sLSTM, mLSTM, residual blocks) are clearly specified.
- **Medium**: The empirical results demonstrate strong performance on language modeling and synthetic tasks, but the comparisons to Transformers are limited to specific model sizes and datasets. The scaling behavior beyond 1B parameters remains to be fully validated.
- **Low**: The claim that xLSTM is a "serious competitor" to current LLMs is based on a narrow set of evaluations. The paper does not address practical considerations like inference efficiency, hardware requirements, or performance on the full range of tasks where Transformers are currently deployed.

## Next Checks

1. **Efficiency benchmarking**: Implement both xLSTM and Transformer models with equivalent parameter counts and benchmark them on identical hardware for training throughput, memory consumption, and inference latency.
2. **Broader task evaluation**: Test xLSTM on a wider range of benchmarks including code generation (HumanEval), multimodal tasks, and instruction following (to match current LLM evaluation standards).
3. **Scaling study**: Train xLSTM models across multiple scales (100M, 1B, 10B parameters) on the same datasets and compare both performance and computational requirements against Transformer scaling laws.