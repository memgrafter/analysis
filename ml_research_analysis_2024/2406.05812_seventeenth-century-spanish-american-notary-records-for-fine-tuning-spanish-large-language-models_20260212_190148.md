---
ver: rpa2
title: Seventeenth-Century Spanish American Notary Records for Fine-Tuning Spanish
  Large Language Models
arxiv_id: '2406.05812'
source_url: https://arxiv.org/abs/2406.05812
tags:
- spanish
- language
- llms
- notary
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SANRlite, a valuable resource of 160+ handwritten
  Spanish notary records from 17th-century Argentina for fine-tuning Spanish large
  language models (LLMs). The resource includes images, transcribed text, metadata
  with Wikidata URIs, and sentence embeddings.
---

# Seventeenth-Century Spanish American Notary Records for Fine-Tuning Spanish Large Language Models

## Quick Facts
- arXiv ID: 2406.05812
- Source URL: https://arxiv.org/abs/2406.05812
- Reference count: 32
- Key outcome: Fine-tuning BERT-based models on 17th-century Spanish notary records outperforms pre-trained Spanish models and ChatGPT, achieving 64.5% accuracy and 0.713 F1 score for classification.

## Executive Summary
This paper introduces SANRlite, a dataset of 160+ handwritten 17th-century Spanish notary records from Argentina, designed to fine-tune Spanish large language models (LLMs). The resource includes images, transcribed text, metadata with Wikidata URIs, and sentence embeddings. Using SANRlite, the authors fine-tune BERT-based models for classification and masked language modeling tasks. The fine-tuned models significantly outperform pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o across multiple metrics, demonstrating the value of domain-specific adaptation for historical Spanish text analysis.

## Method Summary
The authors fine-tune BERT-based models (BETO and M-BERT) on the SANRlite dataset for classification and masked language modeling. The dataset consists of 162 pages with 900+ sentences from 17th-century Argentine notary records, including images, transcriptions, and metadata enriched with Wikidata URIs. For classification, they experiment with different batch sizes and hyperparameters, comparing performance against pre-trained models and ChatGPT using zero-shot, four-shot, and five-shot prompting. For masked language modeling, they use 10% masking rate and evaluate with cosine similarity, exact match, BLEU score, and F1 score.

## Key Results
- Fine-tuned BERT models achieved 64.5% accuracy and 0.713 F1 score for multi-label classification, outperforming pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o
- For masked language modeling, fine-tuned models showed superior performance across cosine similarity, exact match, BLEU score, and F1 metrics compared to pre-trained BETO
- The SANRlite resource enables effective fine-tuning of Spanish LLMs for historical text analysis tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning BERT-based models on domain-specific historical Spanish text significantly improves classification and masked language modeling performance compared to pre-trained models and ChatGPT.
- Mechanism: Domain-specific fine-tuning adapts the general language understanding of BERT to the unique vocabulary, syntax, and historical context of 17th-century Spanish notary records. This adaptation allows the model to better recognize and process the specialized language used in these documents.
- Core assumption: The specialized language and context of 17th-century Spanish notary records differ significantly from general Spanish text, requiring domain-specific adaptation for optimal performance.
- Evidence anchors:
  - [abstract]: "Using SANRlite, the authors fine-tuned BERT-based models for classification and masked language modeling tasks. The fine-tuned models significantly outperformed pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o..."
  - [section]: "Through empirical evaluation, we demonstrate that our collection can be used to fine-tune Spanish LLMs for tasks such as classification and masked language modeling, and can outperform pretrained Spanish models and ChatGPT-3.5/ChatGPT-4o."
  - [corpus]: Weak - the corpus only shows related papers on Spanish language processing but doesn't directly support the specific fine-tuning mechanism.
- Break condition: If the domain-specific language of the notary records doesn't differ significantly from general Spanish text, or if the fine-tuning dataset is too small to capture the necessary patterns.

### Mechanism 2
- Claim: The inclusion of metadata with Wikidata URIs enhances the semantic richness of the dataset, potentially improving model performance.
- Mechanism: Wikidata URIs provide a standardized way to link class labels to broader concepts, allowing the model to leverage external knowledge and improve semantic understanding.
- Core assumption: The additional semantic information provided by Wikidata URIs is useful for the model to better understand and categorize the text.
- Evidence anchors:
  - [section]: "To semantically enrich the JSON metadata, for each class label, we searched Wikidata [31], a popular free and open knowledge base, to extract the uniform resource identifier (URI) for the class labels to precisely denote their meaning."
  - [corpus]: Weak - the corpus doesn't provide direct evidence for this specific mechanism.
- Break condition: If the Wikidata URIs don't provide meaningful semantic information for the specific domain of 17th-century Spanish notary records, or if the model doesn't effectively utilize this information.

### Mechanism 3
- Claim: The combination of original images and transcribed text provides a multimodal learning opportunity, potentially enhancing model performance.
- Mechanism: The model can learn to associate visual patterns in the handwritten text with the corresponding transcribed text, improving its ability to recognize and process historical handwriting.
- Core assumption: The visual information in the handwritten text contains useful patterns that can aid in language understanding and processing.
- Evidence anchors:
  - [abstract]: "Our resource is a collection of handwritten notary records from the seventeenth century obtained from the National Archives of Argentina. This collection contains a combination of original images and transcribed text..."
  - [section]: "SANRlite contained 162 pages containing 900+ sentences."
  - [corpus]: Weak - the corpus doesn't provide direct evidence for this specific multimodal learning mechanism.
- Break condition: If the visual information in the handwritten text doesn't contain useful patterns for language understanding, or if the model doesn't effectively utilize this multimodal information.

## Foundational Learning

- Concept: Fine-tuning pre-trained language models
  - Why needed here: The paper demonstrates how fine-tuning BERT-based models on a domain-specific dataset (SANRlite) significantly improves performance compared to pre-trained models.
  - Quick check question: What are the key steps involved in fine-tuning a pre-trained BERT model on a new dataset?

- Concept: Multi-label classification
  - Why needed here: The paper uses multi-label classification to categorize sentences in the notary records into different classes (e.g., "Parties", "Scope").
  - Quick check question: How does multi-label classification differ from multi-class classification, and why is it appropriate for this task?

- Concept: Masked language modeling
  - Why needed here: The paper uses masked language modeling to generate sentence embeddings, which are then used for training classifiers and performing clustering.
  - Quick check question: What is the objective of masked language modeling, and how does it help in learning meaningful sentence representations?

## Architecture Onboarding

- Component map:
  SANRlite dataset (images, transcribed text, metadata) -> Pre-trained BERT-based models (BETO, M-BERT) -> Fine-tuning pipeline (classification, masked language modeling) -> Evaluation framework (metrics like F1 score, accuracy, BLEU score)

- Critical path:
  1. Prepare SANRlite dataset (images, transcriptions, metadata)
  2. Fine-tune pre-trained BERT models on SANRlite for classification and masked language modeling
  3. Evaluate fine-tuned models using appropriate metrics
  4. Compare performance with pre-trained models and ChatGPT

- Design tradeoffs:
  - Using cased vs. uncased models: Cased models may better capture the nuances of historical text, but may also introduce more variability.
  - Batch size selection: Larger batch sizes may lead to faster training but may require more memory and may not always lead to better performance.
  - Choice of pre-trained model: BETO is specifically trained on Spanish text, while M-BERT is multilingual. The choice depends on the specific requirements of the task.

- Failure signatures:
  - Poor performance on classification or masked language modeling tasks
  - Overfitting or underfitting during fine-tuning
  - Inability to generalize to new notary records outside the training set

- First 3 experiments:
  1. Fine-tune a pre-trained BERT model on a small subset of SANRlite and evaluate its performance on a held-out test set.
  2. Compare the performance of cased and uncased versions of the fine-tuned model.
  3. Experiment with different batch sizes and learning rates during fine-tuning to find the optimal hyperparameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the fine-tuned models perform if the dataset size was significantly increased beyond the current 160+ pages?
- Basis in paper: [inferred] The authors used 160+ pages for fine-tuning and achieved promising results. The paper mentions the original dataset contains 220,000 images, suggesting potential for expansion.
- Why unresolved: The authors only evaluated their approach on the current dataset size. The impact of scaling up the training data on model performance remains untested.
- What evidence would resolve it: Training and evaluating the models on progressively larger subsets of the original dataset (e.g., 500 pages, 1000 pages, 5000 pages) and comparing performance metrics would provide empirical evidence.

### Open Question 2
- Question: How would the fine-tuned models perform on notary records from different time periods or regions within Spanish America?
- Basis in paper: [explicit] The authors note that the notary records were written by two specific notaries from 17th-century Argentina, suggesting potential limitations in generalizability.
- Why unresolved: The evaluation was conducted exclusively on records from two notaries in 17th-century Argentina. The models' ability to handle variations in handwriting, language, and content from different time periods or regions is unknown.
- What evidence would resolve it: Evaluating the fine-tuned models on notary records from different time periods and regions within Spanish America would demonstrate their generalizability.

### Open Question 3
- Question: What is the impact of using multimodal embeddings (combining text and image information) on the performance of the fine-tuned models?
- Basis in paper: [explicit] The authors mention the potential of using CLIP [26] to generate multimodal embeddings by learning on sentences and image annotations, but do not explore this approach.
- Why unresolved: The authors only used text-based embeddings for their experiments. The potential benefits of incorporating image information into the embeddings remain unexplored.
- What evidence would resolve it: Implementing and evaluating the models using multimodal embeddings (combining text and image information) and comparing performance with text-only embeddings would provide insights into the impact of multimodal learning.

## Limitations
- Evaluation focuses on a single historical domain (17th-century Argentine notary records), limiting generalizability to other historical Spanish texts or modern Spanish variants
- Dataset size (162 pages, 900+ sentences) is relatively small for fine-tuning large language models, raising concerns about potential overfitting and model robustness
- Comparison with ChatGPT models relies on prompting strategies that are not fully specified, making exact replication difficult

## Confidence
- High Confidence: The core finding that fine-tuning BERT models on domain-specific historical Spanish text improves performance compared to pre-trained models and ChatGPT
- Medium Confidence: The claim that Wikidata URIs semantically enrich the dataset
- Medium Confidence: The assertion that multimodal learning (combining images and text) enhances model performance

## Next Checks
1. **Generalization Test**: Evaluate the fine-tuned models on a separate corpus of 17th-century Spanish documents from a different region or notary office to assess domain transfer capabilities.

2. **Size Sensitivity Analysis**: Systematically vary the training dataset size (e.g., using 20%, 40%, 60%, 80%, 100% of SANRlite) to determine the minimum dataset size required for effective fine-tuning and identify potential overfitting thresholds.

3. **Multimodal Ablation Study**: Conduct experiments comparing model performance using only text, only images, and the combined multimodal approach to quantify the specific contribution of visual information to the overall improvement.