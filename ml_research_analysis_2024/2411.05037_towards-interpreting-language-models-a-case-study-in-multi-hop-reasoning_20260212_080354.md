---
ver: rpa2
title: 'Towards Interpreting Language Models: A Case Study in Multi-Hop Reasoning'
arxiv_id: '2411.05037'
source_url: https://arxiv.org/abs/2411.05037
tags:
- arxiv
- attention
- language
- memory
- multi-hop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces memory injections, a method to correct multi-hop\
  \ reasoning failures in transformer-based language models by injecting relevant\
  \ prompt-specific information during inference. The authors first analyze attention\
  \ head outputs to identify where multi-hop reasoning fails, then propose a gradient-free\
  \ approach to inject memories at critical locations in the model\u2019s hidden activations."
---

# Towards Interpreting Language Models: A Case Study in Multi-Hop Reasoning

## Quick Facts
- arXiv ID: 2411.05037
- Source URL: https://arxiv.org/abs/2411.05037
- Reference count: 40
- Primary result: Memory injections can increase correct token probability by up to 424% in multi-hop reasoning tasks

## Executive Summary
This work introduces memory injections, a method to correct multi-hop reasoning failures in transformer-based language models by injecting relevant prompt-specific information during inference. The authors first analyze attention head outputs to identify where multi-hop reasoning fails, then propose a gradient-free approach to inject memories at critical locations in the model's hidden activations. Experiments show that memory injections can significantly improve multi-hop reasoning performance while being computationally efficient compared to traditional fine-tuning approaches.

## Method Summary
The authors analyze attention head outputs to identify where multi-hop reasoning fails in transformer models, then propose injecting "memories" (relevant information) directly into the model's hidden activations during inference. They implement this by tokenizing memories, embedding them, and adding them to attention head outputs. The method uses different encoding styles (Embed, Unembed, Layer-wise) with varying computational costs. Performance is evaluated by measuring the percent increase in correct answer probabilities before and after memory injection.

## Key Results
- Memory injections increased correct token probability by up to 424% in multi-hop reasoning tasks
- Small subsets of attention heads significantly impact multi-hop reasoning performance
- Layer-wise memory encoding is most effective but computationally expensive for long sequences
- Different parts of speech show varying effects when used as injected memories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attention heads in transformer-based language models act as knowledge retrievers, responsible for recalling relevant information during inference.
- **Mechanism:** When processing prompts, attention heads selectively focus on and retrieve pertinent information from the input sequence. For multi-hop reasoning, certain attention heads fail to retrieve intermediary information needed to connect explicit and implicit entities in the prompt.
- **Core assumption:** The outputs of attention heads directly influence the model's ability to perform multi-hop reasoning by retrieving necessary knowledge.
- **Evidence anchors:** [abstract] "we find that in transformer-based models it is attention heads, rather than multi-layer perceptrons, that are responsible for retrieving memories critical to successful model predictions"

### Mechanism 2
- **Claim:** Memory injections can correct multi-hop reasoning failures by injecting relevant information directly into the model's hidden activations during inference.
- **Mechanism:** By encoding missing knowledge (memories) and injecting them into specific attention head outputs, the model can incorporate the needed information to complete multi-hop reasoning tasks successfully.
- **Core assumption:** The model has the capacity to utilize injected information if it is presented in a compatible format and location within the architecture.
- **Evidence anchors:** [abstract] "We then propose a mechanism that allows users to inject relevant prompt-specific information, which we refer to as 'memories,' at critical LM locations during inference"

### Mechanism 3
- **Claim:** Different parts of speech behave differently when used as injected memories, suggesting specialized roles for different attention layers in processing specific types of information.
- **Mechanism:** Attention layers may be specialized to process certain types of linguistic information (e.g., nouns, verbs, conjunctions), and injecting memories from different parts of speech can have varying effects on model performance.
- **Core assumption:** The transformer architecture has learned to divide labor among attention layers, with some dealing with processing related to specific parts of speech.
- **Evidence anchors:** [abstract] "We observe that small subsets of attention heads can significantly impact the model prediction during multi-hop reasoning"

## Foundational Learning

- **Concept:** Multi-hop reasoning
  - **Why needed here:** Understanding the difference between single-hop and multi-hop reasoning is crucial for identifying where models fail and how memory injections can help.
  - **Quick check question:** What is the key difference between single-hop and multi-hop reasoning prompts, and why do models struggle more with multi-hop prompts?

- **Concept:** Transformer architecture
  - **Why needed here:** Knowledge of how transformers process information through attention heads and residual streams is essential for understanding where and how to inject memories.
  - **Quick check question:** How do attention heads in transformers contribute to information retrieval and processing during inference?

- **Concept:** Activation engineering
  - **Why needed here:** Memory injections are a form of activation engineering, so understanding how to manipulate model activations is key to implementing this technique.
  - **Quick check question:** What is activation engineering, and how does it differ from traditional model fine-tuning or editing approaches?

## Architecture Onboarding

- **Component map:** Embedding layer -> Attention heads -> Residual stream -> Multi-layer perceptrons (MLPs) -> Unembedding layer
- **Critical path:** The critical path for multi-hop reasoning involves attention heads retrieving relevant information, which is then processed through subsequent layers to form the final prediction. Memory injections aim to correct failures in this retrieval step.
- **Design tradeoffs:** The main tradeoff is between computational cost and effectiveness. More complex memory encoding schemes (like layer-wise) are more effective but computationally expensive, while simpler schemes (like unembed) are less effective but more efficient.
- **Failure signatures:** Failure to improve multi-hop reasoning performance despite memory injections, or degradation in performance for other tasks, could indicate issues with the injection process or the chosen memories.
- **First 3 experiments:**
  1. Implement and test memory injections on a simple multi-hop reasoning task using a small GPT-2 model.
  2. Compare the effectiveness of different memory encoding schemes (embed, unembed, layer-wise) on the same task.
  3. Experiment with injecting memories from different parts of speech to observe their varying effects on model performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can memory injections be scaled to correct multiple types of model failures simultaneously?
- Basis in paper: [explicit] The authors mention that memory injections could be used to correct stale/incorrect information, remove private/harmful information, and combat bias during inference, suggesting potential for multiple failure types.
- Why unresolved: The current work only demonstrates memory injections for multi-hop reasoning failures, leaving other failure types unexplored.
- What evidence would resolve it: Empirical demonstrations showing memory injections effectively correcting various model failure modes (e.g., bias, harmful content, factual errors) in diverse scenarios.

### Open Question 2
- Question: What is the optimal objective function for training attention lenses to interpret knowledge retrieval in attention heads?
- Basis in paper: [explicit] The authors acknowledge that the current training objective (minimizing KL divergence between attention outputs and model predictions) may not be ideal for interpreting knowledge retrieval, and suggest this is an open problem.
- Why unresolved: The paper uses a preliminary objective function without exploring alternatives or their effectiveness.
- What evidence would resolve it: Comparative experiments demonstrating improved interpretability and localization accuracy using different objective functions tailored to knowledge retrieval tasks.

### Open Question 3
- Question: Can trained lenses transfer meaningfully across different model architectures or layers?
- Basis in paper: [explicit] The authors note that training lenses is computationally intensive and suggest investigating whether lenses trained for one layer can be applied to proximal layers or different models.
- Why unresolved: The current work only trains lenses for GPT2-Small without testing transferability.
- What evidence would resolve it: Empirical results showing successful lens transfer across layers within the same model and between different transformer architectures, quantified by disagreement measures like cross-entropy or KL divergence.

## Limitations
- Layer-wise memory injections are computationally expensive for longer sequences, limiting practical applicability
- Evidence for attention heads as knowledge retrievers is indirect rather than direct mechanistic proof
- The work doesn't address potential negative transfer effects where memory injections might degrade performance on other tasks

## Confidence
- **High Confidence**: Memory injections improve multi-hop reasoning performance (424% increase in correct token probabilities)
- **Medium Confidence**: Attention heads act as knowledge retrievers during inference (supported by literature and observed effects)
- **Low Confidence**: Part-of-speech specialization among attention layers (only noted effects without clear mechanistic explanation)

## Next Checks
1. **Cross-task generalization test**: Evaluate whether memory injections that improve multi-hop reasoning performance degrade performance on single-hop tasks or unrelated tasks.
2. **Attention head ablation study**: Systematically disable specific attention heads identified as critical for multi-hop reasoning to directly test their role as knowledge retrievers.
3. **Real-world sequence length validation**: Test memory injections on longer sequences (>1024 tokens) to verify computational expense claims and assess practical applicability.