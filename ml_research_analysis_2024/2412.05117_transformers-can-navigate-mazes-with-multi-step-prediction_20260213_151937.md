---
ver: rpa2
title: Transformers Can Navigate Mazes With Multi-Step Prediction
arxiv_id: '2412.05117'
source_url: https://arxiv.org/abs/2412.05117
tags:
- mazes
- token
- next
- training
- maze
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether explicitly predicting multiple
  steps ahead and backward can improve transformers'' ability to navigate mazes compared
  to standard next token prediction. The authors train parameter-matched transformers
  from scratch using two learning objectives: next token prediction and MLM-U, which
  explicitly predicts multiple steps ahead and backward by masking arbitrary subsets
  of the input sequence.'
---

# Transformers Can Navigate Mazes With Multi-Step Prediction

## Quick Facts
- arXiv ID: 2412.05117
- Source URL: https://arxiv.org/abs/2412.05117
- Authors: Niklas Nolte; Ouail Kitouni; Adina Williams; Mike Rabbat; Mark Ibrahim
- Reference count: 19
- Primary result: MLM-U objective improves transformer maze navigation from 20.6% to perfect accuracy on 20x20 mazes

## Executive Summary
This paper investigates whether transformers can better navigate mazes by explicitly predicting multiple steps ahead and backward rather than just the next token. The authors train parameter-matched transformers using two learning objectives: standard next token prediction and MLM-U, which masks arbitrary subsets of the input sequence to predict multiple steps in both directions. They evaluate performance across maze types (DFS and A*) and sizes (5x5 to 30x30).

The primary finding is that MLM-U considerably outperforms next token prediction, achieving perfect navigation accuracy on mazes up to 20x20 grid size while next token prediction peaks at 20.6%. MLM-U is also 4x more sample efficient and converges 2x faster in GPU training hours. Remarkably, MLM-U-trained transformers even outperform larger transformers trained with next token prediction using additional supervision from A* search traces. These results suggest that learning objectives play a critical role in transformers' ability to perform long-term planning tasks.

## Method Summary
The authors generate DFS mazes using the randomized depth-first search method and A* mazes with deterministic shortest paths, creating training sets of 100,000 mazes and test sets of 2,000 per setting across five maze sizes. They train parameter-matched transformers (3M, 8M, 25M parameters) using both standard next token prediction with a decoder-only architecture and MLM-U with an encoder-decoder architecture and uniform masking. Models are trained for up to 3000 epochs with AdamW optimizer and evaluated on maze navigation accuracy (exact path match) and per-token accuracy, while measuring data efficiency and training efficiency.

## Key Results
- MLM-U achieves perfect maze navigation accuracy on 20x20 mazes versus 20.6% for next token prediction
- MLM-U is 4x more sample efficient than next token prediction
- MLM-U converges 2x faster in GPU training hours
- MLM-U benefits from scaling to larger transformers for more complex mazes

## Why This Works (Mechanism)
The MLM-U objective forces transformers to predict arbitrary subsequences rather than just the next token, encouraging them to develop internal representations that capture long-range dependencies and plan multiple steps ahead. This multi-step prediction capability is crucial for maze navigation where the agent must maintain and update a mental map of the environment to find optimal paths. By masking random subsets of the sequence, MLM-U creates a harder but more informative learning signal that requires the model to reason about the entire path structure rather than just local transitions.

## Foundational Learning
- **Maze generation algorithms (DFS, A*)**: Understanding how mazes are procedurally generated helps in creating proper training and evaluation datasets
  - Why needed: To ensure mazes are diverse and representative of different path complexities
  - Quick check: Verify that generated mazes have unique solutions and appropriate difficulty levels

- **Transformer architectures (decoder-only vs encoder-decoder)**: Different architectures suit different learning objectives
  - Why needed: Next token prediction uses decoder-only while MLM-U requires encoder-decoder for masking capabilities
  - Quick check: Confirm that parameter counts match between architectures

- **Masked Language Modeling (MLM)**: Predicting masked tokens from context
  - Why needed: MLM-U builds on MLM by masking arbitrary subsets for multi-step prediction
  - Quick check: Validate that masking preserves enough context for solvability

- **Positional encodings (RoPE, 16-bit vs 32-bit precision)**: Position information affects performance on larger sequences
  - Why needed: Maze paths require precise position tracking, especially for larger grids
  - Quick check: Compare performance with different precision levels on 30x30 mazes

- **Learning objectives impact on generalization**: Different training targets lead to different capabilities
  - Why needed: The study compares how learning objectives affect planning abilities
  - Quick check: Measure performance gaps between objectives across all maze sizes

- **Sample efficiency metrics**: Training samples needed for convergence
  - Why needed: MLM-U claims 4x better sample efficiency, requiring proper measurement
  - Quick check: Track validation accuracy against training samples consumed

## Architecture Onboarding

Component map: Maze generation -> Transformer training -> Evaluation pipeline

Critical path: Maze generation → Model training (MLM-U or next token) → Path accuracy evaluation → Sample efficiency measurement

Design tradeoffs: The paper uses uniform masking in MLM-U for simplicity, though the authors note that semantically-informed masking might be optimal. They chose parameter-matched models rather than scaling experiments initially, focusing on learning objective impact first.

Failure signatures: Poor performance on larger mazes (15x15+) indicates positional encoding issues or insufficient model capacity. Divergence between training and validation accuracy suggests overfitting, particularly for next token models.

First experiments:
1. Train both objectives on 5x5 mazes to verify basic functionality and convergence patterns
2. Compare 3M parameter models on 10x10 mazes to observe early performance divergence
3. Test 16-bit vs 32-bit RoPE positional encodings on 20x20 mazes to identify precision bottlenecks

## Open Questions the Paper Calls Out
- How do different positional encoding strategies affect MLM-U performance on complex mazes?
- How does uniform masking in MLM-U compare to semantically-informed masking strategies?
- What is the impact of model scale on MLM-U's ability to solve increasingly complex mazes?

## Limitations
- Lack of detailed architectural specifications for the MLM-U encoder-decoder setup creates uncertainty about whether performance gains are due to learning objective or architectural differences
- Focus on grid-based maze navigation without testing more complex environments that better reflect real-world planning tasks
- Limited model size exploration (only 3M, 8M, 25M parameters) constrains conclusions about scaling behavior

## Confidence
High confidence: MLM-U significantly outperforms next token prediction for maze navigation across all tested maze sizes and types
Medium confidence: MLM-U is 4x more sample efficient and converges 2x faster (exact methodology for measuring GPU hours not specified)
Medium confidence: MLM-U benefits from scaling to larger transformers (only three model sizes tested)

## Next Checks
1. Train MLM-U with the exact same decoder-only architecture as the next token baseline to isolate whether performance gains come from learning objective or architectural differences
2. Evaluate whether MLM-U's benefits extend to other sequential planning tasks such as Sokoban puzzle solving or robotic path planning in continuous spaces
3. Systematically vary the masking ratio and strategy in MLM-U training to determine whether uniform masking is optimal or if performance can be further improved with alternative masking schemes