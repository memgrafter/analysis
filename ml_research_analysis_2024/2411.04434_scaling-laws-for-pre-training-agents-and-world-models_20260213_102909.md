---
ver: rpa2
title: Scaling Laws for Pre-training Agents and World Models
arxiv_id: '2411.04434'
source_url: https://arxiv.org/abs/2411.04434
tags:
- params
- scaling
- flops
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes scaling laws for embodied AI models, comparing
  pre-training losses in world modeling (WM) and behavior cloning (BC) tasks. Using
  transformer architectures, the study shows that scaling laws similar to those in
  large language models emerge in WM and BC when modeling human behavior datasets.
---

# Scaling Laws for Pre-training Agents and World Models

## Quick Facts
- arXiv ID: 2411.04434
- Source URL: https://arxiv.org/abs/2411.04434
- Reference count: 40
- Key outcome: Scaling laws for embodied AI models show that pre-training loss correlates with downstream performance, with optimal model size depending on tokenizer compression rate.

## Executive Summary
This paper analyzes scaling laws for embodied AI models, comparing pre-training losses in world modeling (WM) and behavior cloning (BC) tasks. Using transformer architectures, the study shows that scaling laws similar to those in large language models emerge in WM and BC when modeling human behavior datasets. Key findings include: (1) Optimal model size coefficients for WM depend on tokenizer compression rate, with lower compression favoring larger models. (2) BC with tokenized observations requires smaller models than WM due to sparser supervision and super-classed action labels. (3) Moving to continuous CNN embeddings in BC resolves this discrepancy. (4) Pre-training loss strongly correlates with both online performance and world model quality metrics. These results provide precise guidance for optimal scaling of embodied AI models and validate using pre-training loss as a proxy for downstream performance.

## Method Summary
The paper analyzes scaling laws for embodied AI models using transformer architectures on the "Bleeding Edge" video game dataset. The method involves training models for world modeling (predicting future observations) and behavior cloning (predicting future actions) tasks. VQGAN tokenizers are used to discretize image observations, and models of varying sizes are trained with different compute budgets. Scaling laws are analyzed by fitting power law relationships between loss and model size, dataset size, and compute. The study compares scaling behavior between WM and BC tasks, and across different tokenizer configurations.

## Key Results
- Pre-training loss in world modeling correlates strongly with video generation quality metrics (FVD, LPIPS correlation 0.77 and 0.83).
- Optimal model size coefficients for WM increase with decreasing tokenizer compression rate.
- BC with tokenized observations requires smaller models than WM due to sparser supervision (3% vs 97% token supervision).
- Continuous CNN embeddings in BC resolve the model size discrepancy with WM.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-training loss in world modeling correlates strongly with video generation quality metrics (FVD, LPIPS).
- **Mechanism**: As the model improves next-token prediction accuracy, it gains better understanding of environment dynamics, which directly translates to higher fidelity video generation.
- **Core assumption**: The generative objective captures sufficient environmental structure for downstream evaluation metrics.
- **Evidence anchors**:
  - [section] "We find correlations of 0.77, 0.83 for LPIPS and FVD respectively."
  - [abstract] "Pre-training loss strongly correlates with both online performance and world model quality metrics."
  - [corpus] Weak - corpus contains no direct metrics correlation studies for embodied AI
- **Break condition**: If environment contains stochastic elements poorly captured by next-token prediction, or if metrics don't align with task success.

### Mechanism 2
- **Claim**: Tokenizer compression rate inversely affects optimal model size coefficient in scaling laws.
- **Mechanism**: Lower compression (more tokens per observation) means each token is easier to predict, requiring less model capacity but more data to learn all tokens.
- **Core assumption**: Token prediction difficulty scales inversely with token count per observation.
- **Evidence anchors**:
  - [section] "We observe that the optimal parameter scaling coefficient increases with decreasing compression."
  - [abstract] "Optimal model size coefficients for WM depend on tokenizer compression rate, with lower compression favoring larger models."
  - [corpus] Weak - corpus lacks compression rate studies in embodied scaling
- **Break condition**: If token independence assumption breaks or if downstream task requires holistic understanding beyond token prediction.

### Mechanism 3
- **Claim**: Sparse supervision in BC-Token causes training curves to plateau later compared to WM-Token.
- **Mechanism**: BC-Token only supervises action tokens (~3% of total), while WM-Token supervises all image tokens (~97%), creating slower learning progress.
- **Core assumption**: Supervision density directly impacts learning efficiency and plateau timing.
- **Evidence anchors**:
  - [section] "A single observation-action pair is discretized into dz + da total tokens. With the large VQGAN tokenizer, world modeling receives supervision for dz/(dz + da) = 540/556≈97% tokens, while BC is supervised for da/(dz + da) = 16/556≈3% of tokens."
  - [abstract] "BC with tokenized observations requires smaller models than WM due to sparser supervision and super-classed action labels."
  - [corpus] Weak - corpus lacks supervision density analysis in BC tasks
- **Break condition**: If action prediction becomes bottleneck or if BC requires understanding full observation context.

## Foundational Learning

- **Concept: Power law relationships**
  - Why needed here: Scaling laws are fundamentally power law relationships between model size, dataset size, and loss.
  - Quick check question: If doubling model size reduces loss by 25%, what power law exponent describes this relationship?

- **Concept: Compute-optimal training**
  - Why needed here: The paper focuses on finding the most efficient allocation of compute between model size and dataset size.
  - Quick check question: Given a fixed compute budget, should you prioritize model size or dataset size when the optimal model size coefficient is 0.5?

- **Concept: Infinite data regime**
  - Why needed here: Scaling law analysis assumes models don't repeatedly see the same data, avoiding overfitting effects.
  - Quick check question: How many times can you reuse training tokens before leaving the infinite data regime according to the paper?

## Architecture Onboarding

- **Component map**: VQGAN tokenizer → Transformer → Loss function. VQGAN is frozen, transformer is trained. BC-CNN adds trainable CNN encoder and independent action prediction heads.
- **Critical path**: Tokenization → Transformer forward pass → Loss computation → Parameter update. Each component must be correctly configured for scaling analysis.
- **Design tradeoffs**: Tokenized vs CNN architectures offer different scaling behaviors; tokenized provides discrete control but sparse supervision, CNN provides continuous embeddings but requires more compute per prediction.
- **Failure signatures**: Non-overlapping training curves indicate insufficient compute budget; poor correlation between loss and metrics suggests objective mismatch; tokenizer artifacts manifest as poor reconstructions.
- **First 3 experiments**:
  1. Verify WM-Token-256 scaling matches LLM coefficients (~0.5) on small scale
  2. Test BC-CNN vs BC-Token scaling behavior on modest compute
  3. Vary tokenizer compression rate and measure effect on scaling coefficients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal model size coefficient scale with the compression rate of the tokenizer in world modeling tasks?
- Basis in paper: [explicit] The paper demonstrates that the optimal model size coefficient increases as the compression rate decreases (more tokens per image observation).
- Why unresolved: While the paper shows this relationship empirically for specific tokenizers, a theoretical explanation for why this occurs and a general formula predicting the coefficient based on compression rate remains unknown.
- What evidence would resolve it: A mathematical derivation explaining the relationship between compression rate and optimal model size coefficient, validated across diverse world modeling datasets and tokenizers.

### Open Question 2
- Question: What are the scaling laws for embodied AI models when trained on datasets of varying quality?
- Basis in paper: [inferred] The paper focuses on scaling laws in the infinite data regime with high-quality human behavior datasets, but doesn't explore how dataset quality affects scaling.
- Why unresolved: The paper's findings might not generalize to datasets with noise, bias, or limited diversity, which are common in real-world applications.
- What evidence would resolve it: Empirical studies comparing scaling laws across datasets with controlled variations in quality, and theoretical analysis of how dataset quality impacts the power law relationships.

### Open Question 3
- Question: How do scaling laws differ for embodied AI models when optimizing for inference efficiency rather than training efficiency?
- Basis in paper: [inferred] The paper focuses on training efficiency (compute-optimal model and dataset sizing) but doesn't consider inference costs, which are crucial for real-world deployment.
- Why unresolved: Scaling laws derived from pre-training loss might not translate to optimal inference performance, especially for models with different architectures or deployment scenarios.
- What evidence would resolve it: Comparative studies measuring scaling laws for both training and inference efficiency, and development of metrics that capture the trade-off between model size, dataset size, and inference speed.

## Limitations

- Dataset specificity limits generalizability to other embodied AI domains.
- Strong dependence on tokenizer compression rates without mechanistic explanation.
- Assumes infinite data regime, which may not hold for smaller datasets.

## Confidence

- **High Confidence**: Correlation between pre-training loss and world model quality metrics (FVD, LPIPS) with coefficients 0.77 and 0.83.
- **Medium Confidence**: Inverse relationship between tokenizer compression rate and optimal model size coefficient, lacking theoretical explanation.
- **Low Confidence**: Generalizability across different embodied AI domains and token independence assumption require further validation.

## Next Checks

1. Test scaling laws on diverse embodied AI datasets to assess generalizability and identify domain-specific variations.
2. Conduct systematic experiments varying tokenizer architectures and compression rates to isolate effects on scaling behavior.
3. Extend analysis to finite data regimes by varying dataset sizes and measuring impact on optimal model size coefficients.