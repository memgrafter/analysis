---
ver: rpa2
title: 'Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost'
arxiv_id: '2405.05695'
source_url: https://arxiv.org/abs/2405.05695
tags:
- auxiliary
- task
- primary
- methods
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve primary task performance
  using auxiliary labels while preserving single-task inference cost. The core idea
  is to design an asymmetric architecture with separate parameters for primary and
  auxiliary tasks, avoiding negative transfer.
---

# Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost

## Quick Facts
- arXiv ID: 2405.05695
- Source URL: https://arxiv.org/abs/2405.05695
- Authors: Yuan Gao; Weizhong Zhang; Wenhan Luo; Lin Ma; Jin-Gang Yu; Gui-Song Xia; Jiayi Ma
- Reference count: 12
- Primary result: Improves primary task performance using auxiliary labels while preserving single-task inference cost through asymmetric architecture and NAS

## Executive Summary
This paper addresses the challenge of improving primary task performance using auxiliary labels while maintaining single-task inference cost. The authors propose an asymmetric architecture that separates parameters for primary and auxiliary tasks, using neural architecture search (NAS) to automatically discover optimal connections. The method achieves significant improvements across multiple tasks and datasets while ensuring that inference cost remains at the level of a single-task network.

## Method Summary
The method uses soft parameter sharing with separate network branches for primary and auxiliary tasks, connected by layerwise fusion operations. NAS is employed to evolve the architecture by initializing bidirectional connections and then pruning auxiliary-to-primary connections through ℓ1 regularization. During inference, the pruned connections and auxiliary branch are removed, resulting in single-task inference cost. The approach can be combined with optimization-based auxiliary learning methods and scales linearly with additional auxiliary tasks.

## Key Results
- Significant performance improvements on NYU v2, CityScapes, and Taskonomy datasets across six tasks
- Maintains single-task inference cost despite using auxiliary labels during training
- Outperforms state-of-the-art methods on semantic segmentation, surface normal prediction, and object classification
- Scales linearly with additional auxiliary tasks in terms of training complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric architecture with separate parameters for primary and auxiliary tasks avoids negative transfer by preventing conflicting gradients from flowing through shared layers.
- Core assumption: Separate model parameters for different tasks are sufficient to avoid negative transfer without requiring complex optimization-based loss weight manipulation.
- Evidence: The paper demonstrates improved performance while maintaining single-task inference, but lacks detailed gradient analysis to confirm the absence of negative transfer.

### Mechanism 2
- Claim: Neural Architecture Search (NAS) can automatically discover the optimal primary-to-auxiliary connections while pruning auxiliary-to-primary connections to maintain single-task inference cost.
- Core assumption: The NAS optimization can effectively discover which connections to keep while pruning others, and the mixed models during search phase contribute to better convergence.
- Evidence: Experimental results show successful pruning of auxiliary-to-primary connections, but search space sensitivity and regularization strength impact are not thoroughly explored.

### Mechanism 3
- Claim: The fusion operation design allows the method to exploit auxiliary features during training while introducing negligible inference cost.
- Core assumption: The fusion operation can be designed such that auxiliary-related computations can be completely removed without affecting primary task performance.
- Evidence: Theoretical explanation provided about discarding 1x1 convolution when auxiliary-to-primary connections are pruned, but not experimentally validated through ablation studies.

## Foundational Learning

- Concept: Multi-Task Learning (MTL) and Negative Transfer
  - Why needed here: The paper addresses auxiliary learning, which is related to MTL but with the specific constraint of maintaining single-task inference cost. Understanding negative transfer is crucial because it's the primary problem the method aims to solve.
  - Quick check question: What is negative transfer in multi-task learning, and why does it occur when tasks share parameters?

- Concept: Neural Architecture Search (NAS) fundamentals
  - Why needed here: The method uses NAS to automatically discover the optimal asymmetric architecture. Understanding how NAS works, especially single-shot gradient-based NAS, is essential for grasping the proposed approach.
  - Quick check question: How does single-shot gradient-based NAS differ from other NAS approaches, and why is it suitable for this application?

- Concept: Soft vs Hard Parameter Sharing
  - Why needed here: The method uses soft parameter sharing, which is a key design choice that distinguishes it from traditional MTL approaches. Understanding the tradeoffs between soft and hard sharing is important for evaluating the method's effectiveness.
  - Quick check question: What are the advantages and disadvantages of soft parameter sharing compared to hard parameter sharing in multi-task learning?

## Architecture Onboarding

- Component map: Primary branch -> Fusion operations -> Auxiliary branch -> NAS controller -> Regularization

- Critical path:
  1. Initialize two single-task networks (primary and auxiliary)
  2. Add fusion operations at each layer between the two networks
  3. Initialize architecture weights for all connections (bidirectional)
  4. Train NAS controller to optimize architecture weights with ℓ1 regularization on auxiliary-to-primary connections
  5. After convergence, remove auxiliary-to-primary connections and auxiliary branch for inference

- Design tradeoffs:
  - Soft vs hard parameter sharing: Soft sharing avoids negative transfer but increases model complexity
  - NAS search space: Bi-directional connections provide flexibility but require careful regularization
  - Fusion operation design: Must balance effectiveness with negligible inference cost

- Failure signatures:
  - Primary task performance doesn't improve despite auxiliary task availability
  - Auxiliary-to-primary connections not properly pruned, leading to increased inference cost
  - NAS fails to converge or converges to suboptimal architecture

- First 3 experiments:
  1. Implement the asymmetric architecture with bidirectional connections but without NAS regularization, verify that it works but has inference cost issues
  2. Add NAS with ℓ1 regularization on auxiliary-to-primary connections, verify that connections are pruned and inference cost is reduced
  3. Test on a simple primary-auxiliary task pair (e.g., semantic segmentation with depth estimation) to verify performance improvement and single-task inference cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Aux-NAS scale with increasing numbers of auxiliary tasks beyond two, particularly for tasks that are more distantly related to the primary task?
- Basis in paper: [explicit] The paper mentions linear scalability in training complexity but only demonstrates up to two auxiliary tasks in experiments.
- Why unresolved: The experiments only test scalability up to two auxiliary tasks, leaving the upper limits of the method's effectiveness unexplored.
- What evidence would resolve it: Systematic experiments varying the number of auxiliary tasks (e.g., 1, 2, 4, 8) and measuring primary task performance, training/inference time, and architectural complexity.

### Open Question 2
- Question: What is the impact of different initialization strategies for the single-task backbones on the final performance and convergence of the Aux-NAS method?
- Basis in paper: [explicit] The paper states that single-task branches are initialized with pretrained single task model weights, but does not explore alternative initialization strategies.
- Why unresolved: The paper does not investigate how different initialization strategies (e.g., random initialization, transfer learning from related tasks) affect the search process and final performance.
- What evidence would resolve it: Comparative experiments using different initialization strategies for the single-task backbones and measuring their impact on convergence speed, final performance, and architectural characteristics.

### Open Question 3
- Question: How does the performance of Aux-NAS compare to other state-of-the-art methods for multi-task learning with more than two tasks, particularly when the tasks have varying degrees of relatedness?
- Basis in paper: [explicit] The paper focuses on auxiliary learning with one primary and one auxiliary task, but mentions the method can be extended to more tasks.
- Why unresolved: The paper does not compare Aux-NAS to other methods in the multi-task learning setting with multiple auxiliary tasks, nor does it explore how task relatedness affects performance.
- What evidence would resolve it: Experiments comparing Aux-NAS to other multi-task learning methods (e.g., soft parameter sharing, hard parameter sharing) across different numbers of tasks and varying degrees of task relatedness.

## Limitations
- Limited exploration of method's performance on tasks with significantly different data distributions or task complexities
- Insufficient analysis of the impact of regularization strength λ on pruning effectiveness
- Computational overhead during NAS search phase not reported
- Scalability to many auxiliary tasks (>2) mentioned but not empirically validated

## Confidence
- Mechanism 1 (Soft parameter sharing avoiding negative transfer): Medium - Supported by experimental results but lacks detailed gradient analysis
- Mechanism 2 (NAS discovering optimal asymmetric architecture): Medium - Results show convergence but search space and regularization sensitivity not fully explored
- Mechanism 3 (Fusion operation enabling negligible inference cost): Medium - Theoretical explanation provided but not experimentally validated through ablation

## Next Checks
1. Conduct ablation studies on the fusion operation design by removing the 1x1 convolution entirely and measuring the impact on performance and inference cost
2. Test the method's sensitivity to the regularization weight λ by running experiments with different values and analyzing the pruning effectiveness
3. Evaluate the method's performance on a dataset with tasks of significantly different complexities to assess its robustness to negative transfer