---
ver: rpa2
title: Learning to Route Among Specialized Experts for Zero-Shot Generalization
arxiv_id: '2402.05859'
source_url: https://arxiv.org/abs/2402.05859
tags:
- experts
- phatgoose
- zero-shot
- routing
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving zero-shot generalization
  in language models by recycling specialized expert models trained through parameter-efficient
  fine-tuning. The core method, called PHATGOOSE, learns to adaptively route different
  tokens to different expert modules at different layers in the model.
---

# Learning to Route Among Specialized Experts for Zero-Shot Generalization

## Quick Facts
- arXiv ID: 2402.05859
- Source URL: https://arxiv.org/abs/2402.05859
- Authors: Mohammed Muqeeth; Haokun Liu; Yufan Liu; Colin Raffel
- Reference count: 23
- One-line primary result: PHATGOOSE achieves 56.9 average score on T0 Held-In tasks, outperforming multitask baseline T0-3B at 51.6

## Executive Summary
PHATGOOSE addresses zero-shot generalization by learning to route different tokens to specialized expert modules trained via parameter-efficient fine-tuning. The method trains sigmoid gates post-hoc for each expert module and uses top-k routing during inference. Results show PHATGOOSE outperforms previous post-hoc routing methods and, in some cases, explicit multitask training baselines, particularly excelling at combining complementary capabilities from different experts.

## Method Summary
PHATGOOSE combines multiple parameter-efficiently fine-tuned expert modules (trained via LoRA) with learned routing gates. After training each expert on a specific dataset, a sigmoid gate vector is trained post-hoc while freezing all other parameters. During inference, top-k routing (k=2) selects which expert modules to use at each layer based on cosine similarity between standardized gates and activations. The method uses LM-adapted T5.1.1 XL (3B parameters) as the base model and trains rank r=16 LoRA modules for 1000 steps before training gates for 100 additional steps on the same datasets.

## Key Results
- PHATGOOSE achieves 56.9 average score on T0 Held-In tasks vs. 51.6 for T0-3B multitask baseline
- On BIG-bench Hard, PHATGOOSE scores 34.9 vs. 35.3 for best multitask model
- Performance on knowledge-heavy tasks decreased as expert pool scaled from 36 to 166 experts, while reasoning task performance increased
- Qualitative analysis shows PHATGOOSE makes adaptive per-token and per-module expert choices rather than consistently selecting single best expert

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PHATGOOSE improves zero-shot generalization by learning per-token and per-module routing among specialized experts.
- Mechanism: After each expert is trained via parameter-efficient fine-tuning, PHATGOOSE trains a sigmoid gate for each module. During inference, it uses top-k routing based on the dot product scores between normalized gates and activations.
- Core assumption: Different tokens require different expert capabilities at different layers, and adaptive routing can better combine these capabilities than single-expert retrieval.
- Evidence anchors:
  - [abstract] "PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model."
  - [section 3] "Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model."
  - [corpus] Weak - no direct evidence from corpus about per-token routing benefits.
- Break condition: If the learned gates do not correlate with task-relevant characteristics of activations, the routing strategy becomes ineffective.

### Mechanism 2
- Claim: Training gates post-hoc (after expert modules are frozen) prevents coadaptation and leads to better performance than joint training.
- Mechanism: The gate is trained while keeping all expert module parameters fixed, forcing the gate to learn associations with task-specific activation characteristics without adapting the modules themselves.
- Core assumption: Freezing expert parameters during gate training preserves the task-specific knowledge encoded in the modules while allowing the gate to specialize in routing decisions.
- Evidence anchors:
  - [section 3] "We note that fixing the rest of the model during gate training prevents the rest of the model from coadapting with the gate."
  - [section A] "A considerable drop in performance with joint training indicates that gates are effective when learned post-hoc as in PHATGOOSE"
  - [corpus] No direct evidence from corpus about post-hoc training benefits.
- Break condition: If the frozen modules are too rigid to work effectively with the learned gates, performance degrades.

### Mechanism 3
- Claim: PHATGOOSE's routing strategy differs from oracle routing but can outperform it by combining multiple expert capabilities.
- Mechanism: Instead of always selecting the single best expert, PHATGOOSE routes different tokens to different modules, creating an ensemble effect that leverages complementary strengths.
- Core assumption: Zero-shot tasks often require combining multiple types of knowledge or skills, which can be better achieved through ensemble routing than single-expert selection.
- Evidence anchors:
  - [section 4.4] "We found a Pearson correlation coefficient of âˆ’0.2, indicating a little to no correlation between PHATGOOSE's performance and its alignment with Oracle."
  - [section 4.4] "On CB, PHATGOOSE almost never uses the Oracle module and produces significantly better performance by using a wide range of modules."
  - [corpus] No direct evidence from corpus about ensemble routing benefits.
- Break condition: If the routing becomes too distributed and loses specialization, performance may degrade.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: PHATGOOSE builds on PEFT modules created by individual contributors, so understanding how LoRA and other PEFT methods work is essential.
  - Quick check question: How does LoRA modify the output of each linear layer in a model?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: PHATGOOSE implements a routing mechanism similar to MoE, so understanding top-k routing and gating mechanisms is crucial.
  - Quick check question: What is the purpose of using k=2 in top-k routing, and how does it affect computational efficiency?

- Concept: Zero-shot generalization evaluation
  - Why needed here: PHATGOOSE is evaluated on zero-shot generalization benchmarks, so understanding what constitutes zero-shot performance is important.
  - Quick check question: Why is it important that PHATGOOSE doesn't require simultaneous access to datasets used to create specialized models?

## Architecture Onboarding

- Component map:
  Base model (T5-3B) -> LoRA modules (r=16) -> Sigmoid gates -> Top-k router -> Final output
  Each LoRA module has a corresponding gate vector
  Gates are shared across sequence positions

- Critical path:
  1. Contributor trains LoRA module on their dataset
  2. Contributor trains gate vector while freezing all other parameters
  3. All modules and gates are combined into PHATGOOSE
  4. During inference, top-k routing is performed at each layer

- Design tradeoffs:
  - Post-hoc gate training vs. joint training (post-hoc performs better)
  - Single-expert retrieval vs. adaptive routing (adaptive routing enables ensemble effects)
  - Computational cost of top-k routing vs. potential performance gains

- Failure signatures:
  - Poor gate training leading to random routing
  - Gates converging to all-zeros or all-ones
  - Top-k routing selecting irrelevant modules consistently
  - Performance degradation when scaling expert pool size

- First 3 experiments:
  1. Train a single LoRA module and gate, then test routing on simple tasks to verify basic functionality
  2. Compare post-hoc gate training vs. joint training on a small expert pool to validate the training approach
  3. Test different values of k in top-k routing to find the optimal balance between performance and efficiency

## Open Questions the Paper Calls Out

- Does PHATGOOSE's performance degrade when the number of specialized experts exceeds a certain threshold?
  - Basis in paper: [inferred] The paper notes that PHATGOOSE's performance on knowledge-heavy tasks decreased as the expert pool was scaled up from 36 to 166 experts, while performance on reasoning-based tasks increased.
  - Why unresolved: The paper only tested two specific expert pool sizes (36 and 166). The relationship between expert pool size and performance is not fully characterized.
  - What evidence would resolve it: Experiments systematically varying the number of experts and measuring performance on different task types to identify an optimal expert pool size.

- Can PHATGOOSE be effectively applied to decoder-only Transformer models like GPT-style architectures?
  - Basis in paper: [explicit] The paper states "While we focused on the standard setting of adapting T5-family models for better zero-shot generalization, we would be interested in applying PHATGOOSE to decoder-only Transformers."
  - Why unresolved: The paper only evaluated PHATGOOSE on encoder-decoder models (T5 variants). The applicability to different model architectures remains untested.
  - What evidence would resolve it: Implementing and evaluating PHATGOOSE on decoder-only models like GPT-2/3/4, measuring performance compared to baselines.

- Does the order of modules in PHATGOOSE's routing strategy matter for performance?
  - Basis in paper: [inferred] The qualitative analysis shows PHATGOOSE uses different modules at different layers for the same task, suggesting routing order might be important.
  - Why unresolved: The paper does not systematically investigate whether rearranging the order of routed modules affects performance.
  - What evidence would resolve it: Experiments comparing PHATGOOSE's performance when using different orderings of the same set of routed modules for given inputs.

## Limitations

- The paper lacks theoretical grounding for why post-hoc gate training and top-k routing mechanisms work effectively
- Evaluation relies heavily on relative performance improvements rather than absolute metrics, making practical utility difficult to assess
- Study does not explore failure modes in depth or provide robustness analysis across different model scales and expert pool sizes

## Confidence

- High confidence: The experimental methodology and implementation details are clearly specified, allowing for potential reproduction. The relative performance improvements over baseline methods are well-documented.
- Medium confidence: The qualitative analysis of routing behavior provides useful insights, but the connection between routing patterns and task characteristics could be more rigorously established.
- Low confidence: The paper does not adequately address potential limitations such as computational overhead, scalability challenges with larger expert pools, or performance degradation on out-of-distribution tasks.

## Next Checks

1. Test PHATGOOSE's performance when scaling the expert pool from 166 to 500+ modules to assess scalability and routing stability
2. Evaluate PHATGOOSE on tasks that combine multiple skill types to verify whether the ensemble routing truly captures complementary capabilities rather than redundant ones
3. Compare PHATGOOSE against traditional multitask training on tasks where simultaneous data access is feasible to better understand the practical trade-offs between these approaches