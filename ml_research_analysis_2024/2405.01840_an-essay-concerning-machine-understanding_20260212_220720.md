---
ver: rpa2
title: An Essay concerning machine understanding
arxiv_id: '2405.01840'
source_url: https://arxiv.org/abs/2405.01840
tags:
- language
- they
- have
- which
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This essay argues that current artificial intelligence systems
  lack understanding because they rely solely on statistical relationships among tokens
  without deeper conceptual representations. The author proposes that true understanding
  requires machines to connect language to theory-like concepts that go beyond distributional
  semantics.
---

# An Essay concerning machine understanding

## Quick Facts
- arXiv ID: 2405.01840
- Source URL: https://arxiv.org/abs/2405.01840
- Authors: Herbert L. Roitblat
- Reference count: 0
- Primary result: Current AI systems lack true understanding because they rely solely on statistical token relationships without deeper conceptual representations.

## Executive Summary
This essay argues that current artificial intelligence systems, particularly large language models, lack genuine understanding because they operate purely on statistical relationships among tokens without accessing deeper conceptual representations. Drawing from the history of behaviorism and its critique, the author proposes that true understanding requires machines to connect language to theory-like concepts that go beyond distributional semantics. The essay suggests that understanding should be measured through experimental methods assessing whether machines can explain, predict, and entail concepts rather than merely reproduce learned patterns.

## Method Summary
The paper does not present a specific training procedure or experimental methodology but rather advocates for a research direction focused on developing machine understanding through theory-like conceptual representations. It suggests investigating properties such as contextually modulated similarity judgments, causal relations, and entailment relations. The proposed approach involves grounding language tokens in non-linguistic sensory or categorical representations rather than purely symbolic relationships. The paper calls for systematic error analysis to understand failure patterns and the development of test suites that distinguish between distributional semantics and conceptual understanding.

## Key Results
- Current LLMs operate purely on distributional semantics without accessing theory-like conceptual structures
- Understanding requires grounding symbols in non-linguistic representations (sensory, categorical, or combinatorial)
- True understanding enables novel insights and predictions beyond pattern replication

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current LLMs lack true understanding because they rely on statistical token patterns rather than deeper conceptual representations.
- Mechanism: LLMs operate purely on distributional semantics—mapping input tokens to output tokens based on learned probabilities—without accessing theory-like conceptual structures that explain, predict, and entail.
- Core assumption: Meaning cannot be reduced to statistical co-occurrence patterns; true understanding requires connections to non-linguistic, invariant conceptual representations.
- Evidence anchors:
  - [abstract] Current artificial intelligence systems lack understanding because they rely solely on statistical relationships among tokens without deeper conceptual representations.
  - [section] Current models rely on the listener to construct any potential meaning.
  - [corpus] Found 25 related papers; average neighbor FMR=0.447, average citations=0.0.
- Break condition: If statistical patterns alone could generate novel, predictive, and explanatory capabilities (e.g., Einstein's photoelectric effect), then distributional semantics would be sufficient.

### Mechanism 2
- Claim: Language models cannot guarantee that outputs are meaningful or true because their premises lack grounding in external observations.
- Mechanism: Without symbolic grounding—connections between language tokens and non-linguistic sensory or categorical representations—models can only produce statistically valid but semantically empty outputs.
- Core assumption: Meaningful language requires grounding symbols in sensory experience or invariant categorical features, not just in other symbols.
- Evidence anchors:
  - [abstract] Drawing from the history of behaviorism and its critique, the essay suggests that understanding should be measured through experimental methods that assess whether machines can explain, predict, and entail concepts.
  - [section] Harnad argued that symbols were grounded by three kinds of concepts: sensory representations, categorical representations, and combinatorial representations.
  - [corpus] Weak—corpus neighbors focus on automated essay scoring and transparency, not symbol grounding theory.
- Break condition: If models could demonstrate that their outputs reliably predict novel, non-linguistic phenomena without explicit grounding, the grounding requirement would be challenged.

### Mechanism 3
- Claim: Understanding requires the ability to generate novel, previously unsaid insights, not just reproduce learned patterns.
- Mechanism: Human invention (e.g., Einstein's photoelectric effect, Poincare's Fuchsian functions) involves combining concepts in ways not reducible to prior statistical distributions; LLMs cannot invent in this sense.
- Core assumption: Novelty in thought is evidence of conceptual understanding, not mere pattern replication.
- Evidence anchors:
  - [abstract] The author advocates for research into how machines might construct and use these theory-like representations, which would enable them to say things that have not been said before and reveal new insights.
  - [section] An essential feature of intelligence is to be able to say things that have not been said before, that reveal new facts, insights, or relations that have so far not been apparent.
  - [corpus] Weak—corpus neighbors discuss essay scoring and LLM controllability, not conceptual novelty.
- Break condition: If models could reliably generate testable, novel scientific hypotheses or mathematical insights, the novelty criterion would be met.

## Foundational Learning

- Concept: Distributional semantics
  - Why needed here: Explains why current LLMs fail to achieve understanding—they rely solely on token co-occurrence statistics.
  - Quick check question: If a model only sees the context "After struggling through three quarters..." twice, once followed by "won" and once by "lost," what would distributional semantics predict about the meaning of "won" vs. "lost"?

- Concept: Symbol grounding problem
  - Why needed here: Highlights the gap between syntactic token manipulation and semantic meaning—models need connections to non-linguistic representations.
  - Quick check question: If a system can perfectly translate between languages without understanding either, what does that reveal about the symbol grounding problem?

- Concept: Hypothetical constructs vs. intervening variables
  - Why needed here: Distinguishes between abstract entities that explain phenomena (like resistance explained by electron collisions) and mere statistical summaries (like Ohm's law itself).
  - Quick check question: Is "reasoning" in LLMs a hypothetical construct (explaining behavior) or an intervening variable (summarizing statistical patterns)?

## Architecture Onboarding

- Component map: Understanding module (theory-like concept builder) → grounding layer (sensory/categorical inputs) → language interface (token prediction). Current LLMs have only language interface.
- Critical path: Build conceptual representations → ground them in non-linguistic data → connect to language tokens → enable novel, predictive outputs.
- Design tradeoffs: Rich conceptual representations increase model complexity and data requirements but enable understanding; pure statistical models are simpler but limited to pattern replication.
- Failure signatures: Outputs that are statistically valid but semantically wrong (hallucinations), inability to handle novel contexts, failure to explain or predict beyond training data.
- First 3 experiments:
  1. Train a model to ground object names in visual features and test if it can correctly identify novel objects not seen during training.
  2. Create synthetic contexts where "won" and "lost" follow identical phrases, then test if models can learn to distinguish them using non-linguistic cues.
  3. Design a task where models must generate novel, testable scientific hypotheses from textual descriptions and evaluate novelty against training corpus.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What experimental methods can effectively distinguish between understanding and statistical pattern matching in large language models?
- Basis in paper: [explicit] The author advocates for research into experimental methods that assess whether machines can explain, predict, and entail concepts
- Why unresolved: Current methods often conflate successful performance on reasoning tasks with actual understanding, while failing to account for statistical memorization
- What evidence would resolve it: Development of experimental paradigms that isolate theory-like conceptual representations from distributional semantics, possibly drawing from cognitive psychology methods used to study human understanding

### Open Question 2
- Question: How can we construct and evaluate machine representations that go beyond distributional semantics to capture theory-like concepts?
- Basis in paper: [explicit] The author proposes that true understanding requires machines to connect language to theory-like concepts that go beyond distributional semantics
- Why unresolved: Current transformer architectures only model statistical relationships among tokens, lacking deeper conceptual representations
- What evidence would resolve it: Creation of machine learning architectures that demonstrate conceptual representations with properties like causal reasoning, object permanence, and typicality structures, validated through targeted experimental tests

### Open Question 3
- Question: What properties should theory-like conceptual representations in machines possess to enable understanding?
- Basis in paper: [explicit] The author lists properties including contextually modulated similarity judgments, representations of objects, causal relations, entailment relations, physics of motion, and typicality structures
- Why unresolved: While these properties are suggested, their specific implementation and necessity for machine understanding remains unexplored
- What evidence would resolve it: Empirical investigation of which properties are essential for different aspects of understanding, and how they can be computationally implemented in machine systems

## Limitations

- The essay presents a philosophical argument rather than empirical evidence, making many claims without concrete implementations or validation
- The distinction between statistical pattern matching and conceptual understanding remains poorly operationalized
- The paper does not address how to practically implement theory-like representations or how to measure them experimentally beyond vague suggestions

## Confidence

- Medium confidence in the core argument that current LLMs lack true understanding due to their reliance on distributional semantics
- Low confidence in the specific proposed solution (theory-like representations) since no concrete implementation is provided
- Medium confidence in the historical analysis of behaviorism and its relevance to current AI debates

## Next Checks

1. Design a benchmark test suite that distinguishes between distributional semantics responses and conceptually grounded responses using controlled synthetic data where statistical patterns are intentionally misleading
2. Implement a minimal prototype that grounds language tokens in non-linguistic representations (e.g., visual or sensorimotor data) and test whether this improves performance on out-of-distribution conceptual reasoning tasks
3. Conduct a systematic error analysis comparing models that use only distributional semantics versus models with some form of symbolic grounding, focusing on error patterns rather than accuracy rates to reveal underlying representation differences