---
ver: rpa2
title: 'PREDICT: Preference Reasoning by Evaluating Decomposed preferences Inferred
  from Candidate Trajectories'
arxiv_id: '2410.06273'
source_url: https://arxiv.org/abs/2410.06273
tags:
- preferences
- preference
- user
- task
- predict
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PREDICT, a method for inferring human preferences
  from user actions to enable more personalized AI interactions. PREDICT improves
  upon existing LLM-based preference inference by incorporating iterative refinement,
  decomposition of preferences into components, and validation across multiple examples.
---

# PREDICT: Preference Reasoning by Evaluating Decomposed preferences Inferred from Candidate Trajectories

## Quick Facts
- arXiv ID: 2410.06273
- Source URL: https://arxiv.org/abs/2410.06273
- Authors: Stephane Aroca-Ouellette; Natalie Mackraz; Barry-John Theobald; Katherine Metcalf
- Reference count: 40
- Primary result: PREDICT outperforms baselines by 66.2% on PICK UP and 41.0% on PLUME in inferring nuanced preferences, with further 17.9% improvement when combined with in-context learning

## Executive Summary
PREDICT introduces a method for inferring human preferences from user actions to enable more personalized AI interactions. The method improves upon existing LLM-based preference inference by incorporating iterative refinement, decomposition of preferences into components, and validation across multiple examples. PREDICT uses a preference-conditioned agent to generate candidate trajectories for comparison with user examples, breaking down complex preferences into simpler components for better refinement and validation. The approach is evaluated on a gridworld PICK UP task and a new text-domain environment PLUME, demonstrating significant improvements over baseline methods.

## Method Summary
PREDICT is a preference inference method that uses three key innovations: iterative refinement of inferred preferences through candidate trajectory comparison, decomposition of compound preferences into simpler components, and validation of preferences across multiple user examples. The method employs a preference-conditioned agent to generate candidate trajectories based on current inferred preferences, which are then compared to user examples to identify missing or incorrect preferences. Complex preferences are broken down into constituent components to enable more precise refinement, and each inferred preference component is validated against multiple examples using an LLM-based scoring system. The approach is evaluated on two environments: PICK UP (gridworld) and PLUME (text-domain), showing significant improvements over baseline methods.

## Key Results
- PREDICT outperforms baselines by 66.2% on PICK UP and 41.0% on PLUME in inferring nuanced preferences
- Iterative refinement improves preference inference quality by 9.0% mean improvement over non-iterative methods
- When combined with in-context learning, PREDICT achieves a further 17.9% improvement in preference inference

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative refinement using candidate trajectories improves preference inference by enabling direct comparison between current predictions and user examples.
- **Mechanism:** PREDICT generates candidate trajectories based on inferred preferences, then compares these trajectories to user examples. The LLM analyzes differences between the two to identify missing or incorrect preferences, enabling targeted refinement.
- **Core assumption:** The LLM can effectively analyze trajectory differences to identify specific preference gaps.
- **Evidence anchors:**
  - [abstract] "PREDICT incorporates three key elements: (1) iterative refinement of inferred preferences, (2) decomposition of preferences into constituent components, and (3) validation of preferences across multiple trajectories."
  - [section 3] "By comparing these contrasting trajectories to the user's examples, the inferring agent can more clearly pin point differences that indicate missing preferences, in turn enabling the inferring agent to extract more from a single user example."
  - [corpus] Weak - no direct corpus evidence for this specific iterative refinement mechanism.
- **Break condition:** The iterative process stops when the candidate trajectory matches the user example or the maximum number of refinement steps (3) is reached.

### Mechanism 2
- **Claim:** Decomposing compound preferences into components improves preference coverage and refinement precision.
- **Mechanism:** Complex preferences like "write as if events could happen with emojis interspersed" are broken down into simpler components like "write using conditional expressions" and "use emojis." This allows more precise refinement by modifying individual components rather than entire preferences.
- **Core assumption:** Breaking down preferences doesn't lose the semantic meaning or intent of the original preference.
- **Evidence anchors:**
  - [section 3] "This provides several advantages. Preference components provide greater coverage of the preference space with less data, e.g., three components can be combined to create nine profiles. Further, preference components make it easier to refine preference sets by adding, removing, or modifying single components of a preference rather than modifying full preferences."
  - [corpus] Weak - no direct corpus evidence for preference decomposition effectiveness.
- **Break condition:** The decomposition process could fail if components lose critical context or if the LLM cannot properly break down complex preferences.

### Mechanism 3
- **Claim:** Validating preferences across multiple user examples reduces false positives by ensuring preferences are consistently observed.
- **Mechanism:** Each inferred preference component is tested against multiple user examples using an LLM to determine if the user's behavior strongly confirms, somewhat confirms, is neutral toward, somewhat contradicts, or strongly contradicts the preference. Preferences scoring below a threshold (0.25) are discarded.
- **Core assumption:** Preferences should be consistently observable across similar contexts and time spans.
- **Evidence anchors:**
  - [section 3] "We test each preference component of the inferred preference set against each user example by prompting an LLM to determine whether the user's examples strongly confirm the preference, somewhat confirm the preference, is neutral toward the preference, somewhat contradict the preference, or strongly contradict the preference."
  - [corpus] Weak - no direct corpus evidence for validation effectiveness.
- **Break condition:** The validation could fail if the minimum of two user examples requirement allows spurious preferences to pass, or if the threshold is set incorrectly.

## Foundational Learning

- **Concept: Large Language Models and preference inference**
  - Why needed here: PREDICT relies on LLMs to infer preferences from user examples and trajectories. Understanding LLM capabilities and limitations is crucial.
  - Quick check question: What are the key differences between using LLMs for preference inference versus traditional rule-based approaches?

- **Concept: Natural Language Processing for trajectory analysis**
  - Why needed here: PREDICT converts trajectories to structured language descriptions and uses LLMs to analyze these descriptions. Understanding NLP techniques for this conversion is important.
  - Quick check question: How would you handle the partial observability challenge in the PICK UP environment where motion is difficult to encode in language?

- **Concept: Preference learning and personalization in AI**
  - Why needed here: PREDICT aims to create more personalized AI interactions by better inferring user preferences. Understanding the broader context of preference learning is valuable.
  - Quick check question: What are the main challenges in inferring preferences from implicit signals versus explicit feedback?

## Architecture Onboarding

- **Component map:**
  - User Examples -> Preference-Conditioned Agent -> Candidate Trajectories -> LLM Inference Engine -> Refined Preferences -> Decomposition Module -> Preference Components -> Validation Engine -> Aggregated Preferences

- **Critical path:**
  1. Retrieve relevant user examples
  2. Generate initial candidate trajectory with current preferences
  3. Compare candidate to user example and refine preferences
  4. Decompose refined preferences into components
  5. Validate components across multiple examples
  6. Aggregate validated preferences for task completion

- **Design tradeoffs:**
  - Iterative refinement vs. computational cost (more iterations = better preferences but more tokens)
  - Preference decomposition vs. semantic completeness (simpler components may lose context)
  - Validation threshold vs. false positives/negatives (higher threshold = fewer errors but may miss valid preferences)

- **Failure signatures:**
  - Preference inference fails to converge within maximum refinement steps
  - Validation consistently rejects valid preferences (threshold too high)
  - Decomposed preferences lose critical semantic meaning
  - Candidate trajectories become too similar to user examples, providing no learning signal

- **First 3 experiments:**
  1. Test iterative refinement without validation to measure standalone improvement from candidate comparisons
  2. Compare preference decomposition vs. compound preferences on a controlled task with clear preference boundaries
  3. Vary the validation threshold to find optimal balance between false positives and false negatives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the iterative refinement process scale with increasing complexity of user preferences and task environments?
- Basis in paper: [explicit] The paper discusses iterative refinement using candidate trajectories to improve preference inference quality, showing a 9.0% mean improvement over non-iterative methods.
- Why unresolved: The paper only tests on relatively simple gridworld and text-based environments. It doesn't explore how the iterative refinement process performs when dealing with more complex preference structures or highly complex environments where preference validation might become more challenging.
- What evidence would resolve it: Experiments testing PREDICT on environments with significantly more complex preference structures (e.g., hierarchical preferences, contextual dependencies) and more complex task environments (e.g., 3D environments, multi-agent scenarios) would show whether the iterative refinement process remains effective or becomes computationally prohibitive.

### Open Question 2
- Question: What is the impact of preference decomposition on the ability to capture nuanced, multi-faceted preferences?
- Basis in paper: [explicit] The paper discusses breaking down compound preferences into constituent components as one of PREDICT's key contributions, noting that this provides greater coverage of the preference space with less data and makes it easier to refine preference sets.
- Why unresolved: While the paper shows that decomposition improves consistency (lower variance), it also notes a trade-off where compound preferences can capture more complex, multi-faceted preferences that decomposed versions might miss. The paper doesn't quantify this trade-off or determine optimal conditions for when to use compound vs. decomposed preferences.
- What evidence would resolve it: A systematic study comparing preference inference quality when using compound preferences versus decomposed preferences across a range of preference complexity levels would clarify when decomposition helps versus when it limits the ability to capture nuanced preferences.

### Open Question 3
- Question: How does the validation threshold (currently set at 0.25) affect the balance between retaining correct preferences and filtering out incorrect ones?
- Basis in paper: [explicit] The paper states that a validation threshold of 0.25 is used across all experiments, but doesn't explore how different threshold values affect performance or discuss the sensitivity of results to this parameter choice.
- Why unresolved: The optimal validation threshold likely depends on the specific environment, the quality of the underlying LLM, and the complexity of preferences being validated. A threshold that's too low might retain incorrect preferences, while one that's too high might discard correct preferences, but the paper doesn't investigate this trade-off.
- What evidence would resolve it: A parameter sensitivity analysis testing different validation threshold values across multiple environments and preference complexity levels would show how this parameter affects the precision-recall trade-off in preference validation and identify optimal settings for different scenarios.

## Limitations

- The evaluation relies heavily on synthetic environments with controlled preference sets rather than real-world user data
- The lack of real-world validation makes it unclear how PREDICT would perform with naturally occurring preferences that may be ambiguous or contradictory
- The paper doesn't explore how the method handles evolving preferences over time or across different contexts

## Confidence

- **High Confidence:** The core mechanism of using candidate trajectories for preference refinement is well-supported by the theoretical framework and ablation results.
- **Medium Confidence:** The preference decomposition approach shows promise but lacks empirical validation on whether decomposed components maintain semantic equivalence to compound preferences.
- **Low Confidence:** The validation across multiple examples relies on an LLM-based scoring system that may not align with human judgment, particularly for nuanced preferences.

## Next Checks

1. **Real-World Validation:** Test PREDICT on real user preference data from existing datasets (e.g., MovieLens or Amazon reviews) to assess performance on naturally occurring preferences rather than synthetic ones.
2. **Preference Stability Analysis:** Evaluate how well PREDICT handles evolving preferences by measuring inference quality when user examples span different time periods or contexts.
3. **Ablation on Validation Threshold:** Systematically vary the validation threshold (currently 0.25) to quantify the tradeoff between false positives and false negatives in preference inference.