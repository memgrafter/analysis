---
ver: rpa2
title: 'Don''t Throw Away Data: Better Sequence Knowledge Distillation'
arxiv_id: '2407.10456'
source_url: https://arxiv.org/abs/2407.10456
tags:
- student
- teacher
- translation
- training
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MBR-n, a method that improves sequence knowledge
  distillation by using multiple high-scoring MBR translations rather than a single
  selected sequence. This approach better captures the diversity of teacher outputs
  and improves student model performance.
---

# Don't Throw Away Data: Better Sequence Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2407.10456
- **Source URL**: https://arxiv.org/abs/2407.10456
- **Reference count**: 17
- **Primary result**: MBR-n method improves sequence knowledge distillation by using multiple high-scoring MBR translations, showing consistent improvements over baseline methods across English-to-German and English-to-Japanese translation tasks.

## Executive Summary
This paper introduces MBR-n, a method that improves sequence knowledge distillation by using multiple high-scoring MBR translations rather than a single selected sequence. The approach captures a richer diversity of teacher outputs, leading to better student model performance. Experiments demonstrate consistent improvements over strong baseline methods across English-to-German and English-to-Japanese translation tasks, with optimal results achieved using 40 MBR candidates. The method also shows better data efficiency and generalizability across evaluation metrics and domains.

## Method Summary
The MBR-n method improves sequence knowledge distillation by generating 256 candidate translations per source sentence using epsilon sampling, then selecting the top N candidates based on MBR scoring using BLEURT as the metric function. These N candidates are used to train student models via supervised learning. The approach is evaluated on English-to-German and English-to-Japanese translation tasks using PaLM2 models of varying sizes, comparing against baseline methods including reference-based, beam search, random sampling, and MBR-1 approaches.

## Key Results
- MBR-n with N=40 consistently outperforms baseline methods including beam search and MBR-1 across multiple evaluation metrics
- The method demonstrates superior data efficiency, achieving similar or better results with fewer training samples compared to other methods
- MBR-n shows improved generalization across evaluation metrics (BLEURT, sacreBLEU, chrF, Comet22) and domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using multiple MBR candidates captures a richer distribution of teacher outputs than single-sequence distillation.
- Mechanism: MBR scoring selects candidates based on pairwise similarity to other candidates, not just to references. This exposes the student to high-quality variations that share semantic equivalence with the teacher's distribution.
- Core assumption: The top-N MBR candidates are all valid, high-quality translations that represent meaningful variation in the teacher's output space.
- Evidence anchors:
  - [abstract]: "using several high scoring MBR translations, rather than a single selected sequence, thus capturing a rich diversity of teacher outputs"
  - [section]: "MBR scoring starts with a set of candidate translations H sampled from a model, and then uses a reference-based metric u to estimate the expected loss with respect to the other candidates"
- Break condition: If MBR candidates become too diverse (low self-BLEU scores) and include poor-quality translations, the student may learn incorrect patterns.

### Mechanism 2
- Claim: MBR-n improves data efficiency by providing more informative supervision per training instance.
- Mechanism: Each source sentence provides N different target sequences for training, effectively increasing the diversity and volume of training data without requiring more monolingual input text.
- Core assumption: The computational cost of generating and scoring multiple candidates is justified by the improved student performance.
- Evidence anchors:
  - [section]: "MBR-n exhibits strong performance even with limited data (1K samples used in KD training)" and "MBR-n can achieve similar or better results compared to other methods within the same number of update steps"
- Break condition: When the cost of generating and scoring 256+ candidates becomes prohibitive relative to the performance gains.

### Mechanism 3
- Claim: MBR-n mitigates the capacity gap curse by providing more comprehensive teacher signal to the student.
- Mechanism: By exposing students to multiple high-quality outputs, MBR-n helps students better approximate teacher behavior even when there's a significant capacity difference between teacher and student.
- Core assumption: The capacity gap curse is primarily due to insufficient teacher signal rather than fundamental architectural incompatibility.
- Evidence anchors:
  - [section]: "We conduct extensive analysis focusing on data efficiency and capacity curse aspects to elucidate MBR-n"
- Break condition: If the capacity gap is too large, even multiple candidates cannot bridge the fundamental architectural differences.

## Foundational Learning

- **Concept**: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR scoring is the core mechanism for selecting high-quality candidates from a diverse pool
  - Quick check question: What distinguishes MBR from MAP decoding when selecting translations?

- **Concept**: Knowledge distillation fundamentals
  - Why needed here: Understanding how student models learn from teacher outputs is essential for grasping MBR-n's approach
  - Quick check question: How does sequence-level knowledge distillation differ from logit-based distillation?

- **Concept**: BLEURT and other evaluation metrics
  - Why needed here: MBR scoring uses BLEURT as its metric function, and understanding metric behavior is crucial for interpreting results
  - Quick check question: Why might MBR-n show improvements across multiple evaluation metrics despite using BLEURT for candidate selection?

## Architecture Onboarding

- **Component map**: Teacher model (PaLM2) -> Candidate generation (epsilon sampling) -> MBR scoring engine (BLEURT-based) -> Top-N candidate selection -> Student model (PaLM2) -> Training pipeline

- **Critical path**:
  1. Generate source sentences
  2. Teacher generates 256 candidate translations per source
  3. MBR scoring computes pairwise BLEURT scores for all candidates
  4. Top-N candidates selected based on MBR scores
  5. Student trained on this dataset

- **Design tradeoffs**:
  - More candidates → better student performance but higher MBR computation cost
  - Larger N → more diverse training data but potential inclusion of lower-quality candidates
  - Teacher capacity → higher quality candidates but increased computational requirements

- **Failure signatures**:
  - Student performance plateaus despite increasing N
  - Training becomes prohibitively slow due to MBR computation
  - Student overfits to BLEURT metric if candidates are too similar

- **First 3 experiments**:
  1. Compare MBR-1 vs MBR-5 vs MBR-10 on small dataset to find optimal N
  2. Test random candidate selection vs MBR scoring to validate importance of MBR
  3. Compare data efficiency: train with 1K vs 5K vs 10K instances using MBR-40

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of MBR-n outputs compare to other diversity-promoting decoding methods like top-k or nucleus sampling?
- Basis in paper: [explicit] The paper discusses diversity through self-BLEU metrics and notes that MBR-n introduces greater diversity compared to beam search, but questions why higher MBR-n values don't lead to higher output diversity.
- Why unresolved: The paper only compares self-BLEU of MBR-n to beam search, without exploring other diversity-promoting methods. The observation that higher MBR-n values decrease output diversity is not explained.
- What evidence would resolve it: A comparative study of self-BLEU scores across MBR-n, top-k, and nucleus sampling methods for various values of k and p would clarify how MBR-n's diversity compares to these established techniques.

### Open Question 2
- Question: What is the optimal number of MBR candidates (N) for different model sizes and language pairs, and how does this vary with computational constraints?
- Basis in paper: [explicit] The paper observes that N=40 generally performs best but notes exceptions, such as N=5 being optimal for the smallest student model. It also mentions the linear increase in computational cost with N.
- Why unresolved: The paper doesn't provide a systematic analysis of how N should be chosen based on model size, language pair complexity, or computational budget. The trade-off between performance gains and computational cost is not quantified.
- What evidence would resolve it: A comprehensive study varying N across different model sizes and language pairs, with performance and computational cost analysis, would identify optimal N values for various scenarios.

### Open Question 3
- Question: How does MBR-n perform when the teacher model is not a large language model but a traditional NMT model?
- Basis in paper: [explicit] The paper focuses exclusively on PaLM2 models as teachers, leaving open the question of performance with traditional NMT architectures.
- Why unresolved: The paper's experiments are limited to LLM teachers, so the effectiveness of MBR-n with traditional NMT teachers remains unknown. The capacity gap curse discussion suggests this could be an important distinction.
- What evidence would resolve it: Experiments comparing MBR-n performance using both LLM and traditional NMT teachers of comparable sizes would reveal whether the method's effectiveness depends on the teacher architecture.

## Limitations
- MBR scoring has quadratic computational complexity with respect to candidate count, making the approach potentially prohibitive for very large-scale applications
- The capacity gap curse persists despite using multiple candidates, indicating fundamental architectural limitations that MBR-n cannot fully overcome
- The method relies heavily on BLEURT for MBR scoring, raising questions about metric robustness across different domains and languages

## Confidence
- **High confidence**: MBR-n consistently improves student performance over baseline methods across multiple evaluation metrics and domains (English-to-German and English-to-Japanese translation)
- **Medium confidence**: Data efficiency improvements are well-demonstrated but the exact mechanisms by which MBR-n achieves better efficiency compared to other methods need further exploration
- **Medium confidence**: The claim about capturing richer teacher output distribution through multiple candidates is supported by empirical results but could benefit from more direct analysis of candidate diversity

## Next Checks
1. Conduct ablation studies comparing MBR-n with different candidate generation strategies (temperature sampling vs epsilon sampling) to isolate the impact of MBR scoring from candidate diversity
2. Measure the computational overhead of MBR scoring across different candidate counts and evaluate whether the performance gains justify the additional cost in production settings
3. Test MBR-n on additional language pairs and domains to validate generalizability beyond the English-to-German and English-to-Japanese translation tasks studied in the paper