---
ver: rpa2
title: 'Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting
  Strategies'
arxiv_id: '2402.17396'
source_url: https://arxiv.org/abs/2402.17396
tags:
- modulo
- expression
- solve
- prompting
- following
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically benchmarks GPT-4 on three algorithmic
  tasks requiring simplification of nested formulas, comparing its performance against
  GPT-3.5 and a specialized Neural Data Router architecture using seven prompting
  strategies. All models struggle with deeply nested formulas containing three or
  more operands per operation, with prompting techniques that generate explicit reasoning
  steps (especially verbal Chain-of-Thought and Self-consistency) significantly improving
  GPT-4's accuracy, particularly in arithmetic tasks.
---

# Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies

## Quick Facts
- arXiv ID: 2402.17396
- Source URL: https://arxiv.org/abs/2402.17396
- Reference count: 0
- All models struggle with deeply nested formulas containing three or more operands per operation

## Executive Summary
This study systematically benchmarks GPT-4 on three algorithmic tasks requiring simplification of nested formulas, comparing its performance against GPT-3.5 and a specialized Neural Data Router architecture using seven prompting strategies. The research reveals that while advanced prompting techniques like Chain-of-Thought and Self-consistency significantly improve GPT-4's accuracy, particularly for arithmetic tasks, none of the models exhibit proper systematic generalization to complex problem instances. The best performance was achieved by GPT-4 with Self-consistency prompting (79% accuracy on ListOps, 58% on Arithmetic, 52% on Algebra), though all models struggled with deeply nested formulas containing three or more operands per operation.

## Method Summary
The study evaluates three algorithmic reasoning tasks (ListOps, Arithmetic, Algebra) using synthetic data with varying levels of nesting complexity and number of operands. Nine data splits per task are tested using seven prompting strategies on GPT-4, GPT-3.5, and Neural Data Router. The Neural Data Router is trained on 400k samples per task, while LLMs are evaluated using test sets of 100 samples per data split (900 total per task). Accuracy is measured through exact match comparison, with semantic equivalence allowed for Algebra using SymPy.

## Key Results
- GPT-4 with Self-consistency prompting achieved the best overall performance (79% ListOps, 58% Arithmetic, 52% Algebra)
- Chain-of-Thought and Self-consistency prompting significantly improved accuracy by generating explicit reasoning steps
- All models struggled with deeply nested formulas containing three or more operands per operation
- Neural Data Router showed competitive performance with GPT-3.5, particularly on simpler ListOps problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Advanced prompting strategies improve GPT-4 performance on algorithmic reasoning tasks by encouraging explicit reasoning steps.
- Mechanism: Chain-of-Thought and Self-consistency prompting methods guide the model to generate intermediate solution steps, which helps in decomposing complex problems into manageable sub-problems.
- Core assumption: LLMs can effectively use intermediate reasoning steps