---
ver: rpa2
title: 'AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance
  Labels'
arxiv_id: '2410.20050'
source_url: https://arxiv.org/abs/2410.20050
tags:
- retrieval
- medical
- documents
- document
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of zero-shot medical information
  retrieval without relevance labels by proposing a self-learning framework called
  SL-HyDE. The method uses large language models to generate hypothetical documents
  from medical queries, which guide dense retrievers in finding relevant medical documents.
---

# AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels

## Quick Facts
- arXiv ID: 2410.20050
- Source URL: https://arxiv.org/abs/2410.20050
- Authors: Lei Li; Xiangxu Zhang; Xiao Zhou; Zheng Liu
- Reference count: 40
- Primary result: Zero-shot medical retrieval without relevance labels using self-learning framework

## Executive Summary
This paper addresses the challenge of zero-shot medical information retrieval without relevance labels by proposing a self-learning framework called SL-HyDE. The method uses large language models to generate hypothetical documents from medical queries, which guide dense retrievers in finding relevant medical documents. The self-learning mechanism progressively refines both the generator and retriever using unlabeled medical corpora, eliminating the need for labeled data. The authors also introduce CMIRB, a comprehensive Chinese medical information retrieval benchmark with five tasks and ten datasets.

## Method Summary
SL-HyDE employs a self-learning framework that begins with unlabeled medical corpora and completes training through an iterative process. The system uses an LLM (Qwen2-7B-Instruct) to generate hypothetical documents from medical queries, which are then used by a dense retriever (BGE-Large-zh-v1.5) to find relevant documents. The self-learning mechanism leverages the retriever's ranking capabilities to select high-relevance hypothetical documents that align with the generator's output, creating a closed-loop where each component improves the other. Both the generator and retriever are fine-tuned on the unlabeled medical corpus using contrastive learning objectives, with the highest-ranked hypothetical documents serving as supervision signals.

## Key Results
- SL-HyDE achieves up to 4.9% higher NDCG@10 compared to HyDE baseline
- Improves retrieval accuracy by 7.2% over BGE alone across CMIRB datasets
- Demonstrates effective zero-shot performance without requiring relevance-labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-learning framework progressively refines both pseudo-document generation and retrieval through an iterative feedback loop.
- Mechanism: The generator creates hypothetical documents, the retriever ranks them, and the highest-ranked hypothetical document is used to supervise the generator's training. This process creates a closed-loop where each component improves the other.
- Core assumption: The retriever's ranking of hypothetical documents correlates with their effectiveness in retrieving true target documents.
- Evidence anchors:
  - [abstract]: "The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data."
  - [section]: "The self-learning mechanism leverages the retrieval model's ranking capabilities to select high-relevance hypothetical documents that align with the output of the generator"
- Break condition: If the retriever consistently ranks poor hypothetical documents highly, the feedback loop would reinforce incorrect patterns.

### Mechanism 2
- Claim: Hypothetical documents bridge the semantic gap between queries and target documents in medical retrieval.
- Mechanism: LLMs generate hypothetical documents that encapsulate key medical context from queries, which are then used by the retriever to find relevant real documents through vector similarity.
- Core assumption: The hypothetical documents contain sufficient medical context to guide the retriever toward relevant documents.
- Evidence anchors:
  - [abstract]: "These generated documents encapsulate key medical context, guiding a dense retriever in identifying the most relevant documents."
  - [section]: "HyDE (Hypothetical Document Embeddings) employs zero-shot prompts to guide an instruction-following language model to generate hypothetical documents, effectively narrowing the semantic gap between the query and the target document."
- Break condition: If the hypothetical documents contain hallucinated or irrelevant medical information, the retriever would be misled.

### Mechanism 3
- Claim: Fine-tuning both generator and retriever on unlabeled medical corpora without labeled data is sufficient to achieve domain adaptation.
- Mechanism: The system uses self-generated supervision signals from the retriever to train the generator, and uses query-hypothetical document pairs to train the retriever, creating a fully self-supervised training loop.
- Core assumption: The unlabeled medical corpora contain sufficient domain-specific patterns that can be captured through self-supervision.
- Evidence anchors:
  - [abstract]: "SL-HyDE begins with unlabeled medical corpora and completes the training process through a self-learning mechanism, thereby circumventing the heavy reliance on labeled data"
  - [section]: "For the training data of the self-learning generator, there is no need to rely on supervision signals from labeled medical data. Instead, we utilize only unlabeled corpora"
- Break condition: If the unlabeled corpus lacks sufficient medical domain coverage, the learned representations would be inadequate.

## Foundational Learning

- Concept: Dense retrieval using vector embeddings
  - Why needed here: The paper relies on dense retrievers that encode queries and documents into vector spaces where similarity indicates relevance
  - Quick check question: How does dense retrieval differ from traditional sparse retrieval methods like BM25?

- Concept: Contrastive learning for representation learning
  - Why needed here: The retriever is trained using contrastive learning objectives to distinguish between relevant and irrelevant documents
  - Quick check question: What is the role of negative sampling in contrastive learning for retrieval?

- Concept: Self-supervised learning and pseudo-labeling
  - Why needed here: The entire training framework operates without human-labeled relevance data, relying on self-generated supervision signals
  - Quick check question: How does the system generate "labels" for training when no human annotations are available?

## Architecture Onboarding

- Component map: Query → Generator (LLM) → Hypothetical document → Retriever (Dense embedding) → Ranked documents
- Critical path: Query + Document → Generator → Hypothetical document → Retriever ranking → Supervision signal generation
- Design tradeoffs:
  - Using single vs. multiple hypothetical documents (inference time vs. coverage)
  - Fine-tuning both components vs. using pre-trained models directly
  - Temperature settings for deterministic vs. diverse generation
- Failure signatures:
  - Poor retrieval performance indicates either generator hallucination or retriever misalignment
  - Training instability suggests supervision signals are unreliable
  - Domain mismatch suggests corpus is insufficient for medical knowledge
- First 3 experiments:
  1. Baseline test: Run HyDE with pre-trained Qwen2 and BGE without fine-tuning
  2. Generator ablation: Fine-tune only the generator, keep retriever frozen
  3. Retriever ablation: Fine-tune only the retriever, keep generator frozen

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- The framework's effectiveness depends heavily on the quality of hypothetical documents generated by the LLM
- CMIRB benchmark may not fully capture real-world medical retrieval complexity with ambiguous or incomplete queries
- No ablation studies showing individual contribution of generator vs. retriever fine-tuning to performance gains

## Confidence
- **High confidence**: The core mechanism of using hypothetical documents to bridge semantic gaps between queries and documents is well-established in HyDE literature and the experimental improvements over baseline methods are statistically significant.
- **Medium confidence**: The self-learning refinement process is theoretically sound, but the paper lacks ablation studies showing how much each component contributes to overall performance gains.
- **Low confidence**: The claim that SL-HyDE achieves "significant improvements" needs context - while 4.9% and 7.2% improvements are reported, the absolute performance numbers and comparison to other zero-shot methods are not provided.

## Next Checks
1. Conduct ablation study to isolate contribution of generator fine-tuning versus retriever fine-tuning
2. Perform manual inspection of generated hypothetical documents to quantify hallucination rates
3. Evaluate SL-HyDE on out-of-domain medical queries to assess robustness beyond CMIRB benchmark