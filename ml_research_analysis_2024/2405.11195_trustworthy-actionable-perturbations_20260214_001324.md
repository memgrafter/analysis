---
ver: rpa2
title: Trustworthy Actionable Perturbations
arxiv_id: '2405.11195'
source_url: https://arxiv.org/abs/2405.11195
tags:
- latexit
- sha1
- base64
- vnmoc87shz0qb5qexsrq2vrfz2cpu7
- zu7zcodu9veelcoytggey5wfhobo1opjnthzji3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Trustworthy Actionable Perturbations (TAP),
  a framework for generating counterfactual explanations that both change a classifier's
  decision and affect true underlying class probabilities. TAP addresses the limitation
  of previous methods that often produce adversarial-like perturbations which fool
  classifiers without real-world impact.
---

# Trustworthy Actionable Perturbations

## Quick Facts
- arXiv ID: 2405.11195
- Source URL: https://arxiv.org/abs/2405.11195
- Reference count: 40
- Primary result: Introduces TAP framework that generates counterfactual explanations changing both classifier decisions and true class probabilities, outperforming existing methods in efficiency and effectiveness

## Executive Summary
This paper introduces Trustworthy Actionable Perturbations (TAP), a framework for generating counterfactual explanations that both change a classifier's decision and affect true underlying class probabilities. TAP addresses the limitation of previous methods that often produce adversarial-like perturbations which fool classifiers without real-world impact. The core method involves generating candidate perturbations using gradient-based optimization with a differentiable cost function, then verifying these candidates with a pairwise classifier to ensure they aren't adversarial. The framework includes novel target set definitions using statistical divergence and introduces PAC-learnability results for the verification procedure.

## Method Summary
TAP is a two-step method for creating counterfactual explanations. First, it generates candidate perturbations using gradient-based optimization that minimizes a cost function combining statistical distance to target sets and real-world change costs. Second, it applies a pairwise verification classifier to ensure the perturbations change true class probabilities rather than just fooling the original classifier. The framework introduces flexible target set definitions using f-divergences, application-specific cost functions, and provides PAC-learnability guarantees for the verification procedure.

## Key Results
- TAP eliminates 14-27% of generated perturbations through verification while maintaining high success rates
- Outperforms existing counterfactual methods on four real-world datasets (Adult Income, Law School Success, Diabetes Prediction, German Credit)
- Novel target set definitions using statistical divergence allow for more practical and personalized goals than simple classification changes
- Application-specific cost functions produce more efficient real-world advice than ℓ-norm based costs

## Why This Works (Mechanism)

### Mechanism 1
TAP's two-step verification eliminates adversarial examples that fool classifiers without affecting true probabilities. First generates candidate perturbations using gradient-based optimization, then applies pairwise classifier V to compare original and modified inputs, ensuring they belong to same class. Core assumption: V's classification task differs from M's task, so attacks targeting M won't be effective against V.

### Mechanism 2
TAP's flexible target set definition using statistical divergence allows for more practical and personalized goals than simple classification changes. Defines goals through target sets of acceptable outcomes using Kullback-Leibler divergence to measure distance from current probabilities to target set. Core assumption: Users can specify meaningful target sets that balance desired outcomes with practical feasibility.

### Mechanism 3
TAP's application-specific cost functions produce more efficient real-world advice than ℓ-norm based costs. Uses cost functions built specifically to reflect real-world costs of changes rather than generic ℓ-norm distances. Core assumption: Real-world costs can be accurately modeled as differentiable functions and differ significantly from ℓ-norm distances.

## Foundational Learning

- **Gradient-based optimization for constrained problems**: TAP uses gradient descent to solve the optimization problem of finding perturbations that satisfy both cost constraints and target set requirements. Quick check: How do you handle non-differentiable constraints in gradient-based optimization?

- **Statistical divergence measures (KL divergence, total variation)**: TAP uses f-divergences to measure the distance between current probabilities and target sets. Quick check: What properties must a function f have to be used in an f-divergence?

- **PAC-learnability and generalization bounds**: TAP's verification procedure requires understanding when the verifier will generalize to unseen data. Quick check: How does the number of classes k affect the sample complexity requirements for learning?

## Architecture Onboarding

- **Component map**: Input x -> Generator M -> Candidate perturbation ˜x -> Verifier V -> Output TAP or rejection
- **Critical path**: Input x → Generator M → Candidate perturbation ˜x → Verifier V → Output TAP or rejection. Optimization loop: Compute gradient of (dY(M(˜x), T) + λdX (˜x, x)) → Update ˜x → Check constraints → Repeat
- **Design tradeoffs**: Verification accuracy vs. computational cost: More complex V may be more accurate but slower. Target set flexibility vs. optimization difficulty: More complex targets may be harder to optimize. Cost function accuracy vs. differentiability: More accurate real-world costs may be harder to make differentiable
- **Failure signatures**: High rejection rate from verifier: Generator producing adversarial examples. Slow convergence: Poorly chosen cost function or target set. Poor real-world effectiveness: Cost function not accurately modeling real-world effort
- **First 3 experiments**: 
  1. Verify TAP produces valid perturbations on simple synthetic data with known ground truth
  2. Compare TAP's target set flexibility against baseline counterfactual methods on simple multi-class problems
  3. Test TAP's verification procedure on adversarial examples generated against the generator network

## Open Questions the Paper Calls Out

### Open Question 1
Does the verifier's effectiveness change when applied to adversarial examples generated using different attack methods beyond Carlini-Wagner? Basis in paper: [explicit] The paper states "the verifier was 100% effective at eliminating Carlini Wagner adversarial examples" but does not test other attack methods. Why unresolved: The paper only evaluates the verifier against Carlini-Wagner attacks. What evidence would resolve it: Testing the verifier against multiple adversarial attack methods (FGSM, PGD, black-box attacks) and reporting success rates at eliminating these examples.

### Open Question 2
What is the theoretical limit of the verifier's PAC-learnability bound as the number of classes k approaches infinity while keeping n fixed? Basis in paper: [explicit] Theorem 2.3 states the bound is O( k√n2−k2n )1/d! which depends on both n and k, but the paper doesn't analyze the asymptotic behavior as k→∞. Why unresolved: The paper presents the bound but doesn't analyze its behavior in extreme scenarios. What evidence would resolve it: Mathematical analysis of the bound's behavior as k→∞, showing whether it converges to a finite value or diverges.

### Open Question 3
How does the choice of f-divergence function f(x) affect the TAP generation process and the quality of the resulting counterfactuals? Basis in paper: [explicit] Theorem 2.2 states the closed-form solution works for any twice-differentiable f-divergence, but the paper only uses KL divergence in experiments. Why unresolved: Different f-divergences (KL, total variation, Hellinger) have different properties that could affect optimization landscapes and counterfactual quality, but the paper doesn't explore this. What evidence would resolve it: Comparative experiments using different f-divergence functions in TAP generation, measuring success rates, verification rates, and practical effectiveness of resulting counterfactuals.

## Limitations

- Verification procedure's effectiveness depends on the verifier being trained on a distribution that captures potential adversarial examples, which may not always be feasible in practice
- Flexible target set definitions, while theoretically appealing, may be difficult to specify meaningfully for complex real-world problems
- Application-specific cost functions require domain expertise to design effectively and may not generalize well across different contexts

## Confidence

- **TAP framework's core mechanisms**: Medium. Theoretical foundation appears sound but empirical validation relies heavily on synthetic target sets and four specific datasets
- **Learning framework's ability to provide real-world actionable advice**: Low to Medium. While theoretical framework is rigorous, practical implementation challenges remain

## Next Checks

1. **Adversarial Robustness Test**: Generate adversarial examples specifically targeting the pairwise verifier V and measure its robustness. This would validate the core claim that V provides meaningful protection against adversarial perturbations.

2. **Target Set Generalization**: Test TAP on problems with user-specified target sets that differ from the synthetic sets used in experiments. This would validate the flexibility claim and identify practical challenges in target set specification.

3. **Cost Function Transferability**: Implement TAP with cost functions designed by domain experts in a different application area (e.g., healthcare) and measure whether the resulting perturbations are more actionable than those generated with ℓ-norm costs.