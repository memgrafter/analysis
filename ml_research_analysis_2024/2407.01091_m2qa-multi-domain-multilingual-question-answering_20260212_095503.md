---
ver: rpa2
title: 'M2QA: Multi-domain Multilingual Question Answering'
arxiv_id: '2407.01091'
source_url: https://arxiv.org/abs/2407.01091
tags:
- language
- domain
- computational
- linguistics
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce M2QA, a multi-domain multilingual question answering
  benchmark that enables joint evaluation of language and domain transfer across three
  languages (German, Turkish, Chinese) and three domains (product reviews, news, creative
  writing). The dataset contains 13,500 SQuAD 2.0-style question-answer pairs annotated
  on naturally occurring texts in the respective languages.
---

# M2QA: Multi-domain Multilingual Question Answering

## Quick Facts
- **arXiv ID**: 2407.01091
- **Source URL**: https://arxiv.org/abs/2407.01091
- **Reference count**: 40
- **Primary result**: Multi-domain multilingual QA benchmark with 13,500 SQuAD 2.0-style QA pairs across German, Turkish, and Chinese languages

## Executive Summary
M2QA introduces a novel multi-domain multilingual question answering benchmark designed to evaluate cross-lingual and cross-domain transfer capabilities. The dataset contains 13,500 QA pairs across three languages (German, Turkish, Chinese) and three domains (product reviews, news, creative writing), enabling comprehensive assessment of model performance beyond single-language, single-domain settings. The benchmark reveals significant performance gaps in cross-lingual and cross-domain transfer, demonstrating that current models struggle when moving beyond in-language, in-domain scenarios.

## Method Summary
The M2QA benchmark employs SQuAD 2.0-style question-answer pairs annotated on naturally occurring texts in German, Turkish, and Chinese. The dataset spans three distinct domains: product reviews, news articles, and creative writing passages. The evaluation framework assesses both answerable and unanswerable questions, with models required to determine whether a valid answer exists within the passage. Multiple model architectures are evaluated including fine-tuned XLM-R, various large language models (GPT-3.5, Aya 23, Llama 2/3), and modular adapter-based approaches to establish comprehensive baseline performance across different transfer scenarios.

## Key Results
- Cross-lingual and cross-domain transfer remains challenging, with significant performance drops observed across all model sizes
- Performance varies considerably across domain-language combinations, indicating domain-specific transfer patterns
- The SQuAD 2.0 evaluation metric requires adjustment for multilingual evaluation due to its reliance on whitespace tokenization

## Why This Works (Mechanism)
M2QA works by providing a controlled environment to evaluate the generalization capabilities of multilingual QA models across linguistic and topical boundaries. The benchmark isolates transfer challenges by maintaining consistent annotation guidelines across languages while varying domain characteristics, enabling systematic analysis of cross-lingual and cross-domain performance degradation patterns.

## Foundational Learning

**Multilingual tokenizers**: Why needed - To handle language-specific tokenization challenges that affect model performance and evaluation consistency. Quick check - Verify consistent token coverage across target languages.

**Domain adaptation**: Why needed - To assess whether models can transfer knowledge between different topical contexts. Quick check - Compare in-domain vs cross-domain performance gaps.

**Cross-lingual transfer**: Why needed - To measure how well models leverage shared linguistic features across languages. Quick check - Evaluate performance drops when transferring to different language families.

## Architecture Onboarding

**Component map**: Data annotation pipeline -> QA pair generation -> Model fine-tuning -> Cross-lingual evaluation -> Domain transfer analysis

**Critical path**: Natural text selection → Multilingual annotation → Model training → Cross-lingual evaluation → Performance analysis

**Design tradeoffs**: Natural text authenticity vs. controlled difficulty vs. dataset size; comprehensive domain coverage vs. annotation consistency

**Failure signatures**: Large performance gaps in cross-lingual settings, inconsistent domain transfer patterns, tokenization-based evaluation errors

**3 first experiments**:
1. Evaluate baseline monolingual performance within each language-domain combination
2. Test cross-lingual transfer from high-resource to low-resource languages
3. Assess cross-domain transfer within the same language family

## Open Questions the Paper Calls Out

None

## Limitations
- Dataset scale (13,500 pairs) may be insufficient for robust characterization of complex transfer scenarios
- Natural text selection introduces uncontrolled variability in topic distribution across domains
- SQuAD 2.0-style unanswerable questions may not capture all nuances of multilingual comprehension challenges

## Confidence

**Performance gaps in cross-lingual transfer**: High confidence - consistently observed across multiple model architectures

**Domain-language combination sensitivity**: Medium-High confidence - patterns observed but require larger sample sizes for definitive conclusions

**Evaluation metric limitations**: Medium confidence - based on observed tokenization issues requiring further validation

## Next Checks

1. Conduct statistical power analysis to determine minimum dataset size for reliable cross-lingual transfer claims and assess M2QA's adequacy

2. Implement controlled experiments varying text difficulty and topic distribution within each domain to isolate confounding factors

3. Develop and validate alternative evaluation metrics accounting for multilingual tokenization variations, then re-evaluate all models to quantify metric impact