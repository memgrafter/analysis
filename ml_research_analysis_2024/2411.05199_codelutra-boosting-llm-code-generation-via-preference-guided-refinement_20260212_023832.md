---
ver: rpa2
title: 'CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement'
arxiv_id: '2411.05199'
source_url: https://arxiv.org/abs/2411.05199
tags:
- code
- data
- arxiv
- generation
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodeLutra, a preference-guided refinement
  framework that enhances LLM code generation by iteratively learning from both correct
  and incorrect code attempts. Unlike standard supervised fine-tuning, which only
  uses correct examples, CodeLutra constructs self-generated comparative data to refine
  the model's understanding of code quality.
---

# CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement

## Quick Facts
- arXiv ID: 2411.05199
- Source URL: https://arxiv.org/abs/2411.05199
- Authors: Leitian Tao; Xiang Chen; Tong Yu; Tung Mai; Ryan Rossi; Yixuan Li; Saayan Mitra
- Reference count: 13
- Primary result: CodeLutra improved Llama-3-8B accuracy from 28.2% to 48.6% on data science tasks using 500 samples

## Executive Summary
CodeLutra introduces a preference-guided refinement framework that enhances LLM code generation by iteratively learning from both correct and incorrect code attempts. Unlike standard supervised fine-tuning, which only uses correct examples, CodeLutra constructs self-generated comparative data to refine the model's understanding of code quality. Using a dual-loss approach combining preference optimization with likelihood regularization, the framework improves smaller open-source models without requiring massive datasets or auxiliary models.

On challenging data science tasks, CodeLutra improved Llama-3-8B's accuracy from 28.2% to 48.6% using just 500 samples, approaching GPT-4's level. On data query tasks, it achieved 76.6% execution accuracy, exceeding GPT-4. The method reduces syntax errors and improves incorrect answers across iterations, demonstrating effectiveness across different base models including Gemma-7B and StarCoder-7B.

## Method Summary
CodeLutra applies iterative preference-based refinement to code generation by comparing successful and unsuccessful outputs. The framework generates multiple code responses per input, executes them to classify as correct or incorrect, then constructs preference pairs (one correct vs one incorrect) for training. Using a dual-loss approach that combines preference optimization with likelihood regularization, CodeLutra refines the model iteratively. The method leverages both successful and failed code generated by the model itself, creating self-generated comparative data without requiring large-scale preference datasets or auxiliary models.

## Key Results
- Improved Llama-3-8B accuracy from 28.2% to 48.6% on data science tasks using 500 samples
- Achieved 76.6% execution accuracy on data query tasks, exceeding GPT-4's performance
- Reduced syntax errors and improved incorrect answers across iterative refinements
- Demonstrated effectiveness across base models including Llama-3-8B, Gemma-7B, and StarCoder-7B

## Why This Works (Mechanism)

### Mechanism 1
Iterative refinement using self-generated comparative data improves code generation quality by learning from both correct and incorrect outputs. CodeLutra generates multiple code responses per input, classifies them as correct or incorrect based on execution results, then constructs preference pairs (one correct vs one incorrect) to train the model iteratively. The dual-loss approach combines preference optimization with likelihood regularization to prevent degradation of correct solutions.

### Mechanism 2
Preference-guided refinement enables smaller models to close the performance gap with larger models without requiring massive datasets or auxiliary models. By leveraging both correct and incorrect code attempts, CodeLutra enables smaller models to learn from their own mistakes and iteratively improve, achieving performance comparable to larger models like GPT-4 with limited training data.

### Mechanism 3
Dual-loss function combining preference optimization with likelihood regularization maintains correct solution quality while improving incorrect outputs. The framework uses a Bradley-Terry model to compare correct vs incorrect code, optimizing the model to prefer correct solutions while simultaneously maximizing the likelihood of generating correct code to prevent degradation.

## Foundational Learning

- **Concept:** Supervised Fine-Tuning (SFT) limitations
  - **Why needed here:** Understanding why standard SFT is insufficient for code generation is crucial to appreciating CodeLutra's approach.
  - **Quick check question:** Why does SFT struggle with code generation when it works well for other NLP tasks?
  - **Answer:** SFT only maximizes likelihood of correct examples, missing valuable learning opportunities from incorrect attempts, and code requires both syntactic and semantic correctness.

- **Concept:** Preference Learning and Ranking
  - **Why needed here:** CodeLutra's core innovation relies on learning from preference pairs (correct vs incorrect).
  - **Quick check question:** What makes preference learning particularly effective for code generation compared to other tasks?
  - **Answer:** Code generation has clear binary correctness criteria (execution results), making preference learning more straightforward than in subjective tasks.

- **Concept:** Bradley-Terry Model for Pairwise Comparisons
  - **Why needed here:** This probabilistic model underlies CodeLutra's preference comparison mechanism.
  - **Quick check question:** How does the Bradley-Terry model help in ranking code quality?
  - **Answer:** It provides a probabilistic framework for comparing two items (correct vs incorrect code) and estimating their relative quality.

## Architecture Onboarding

- **Component map:** Base LLM -> Code generation and execution engine -> Preference dataset construction module -> Dual-loss training loop -> Evaluation and correctness verification system
- **Critical path:** 1. Generate multiple code responses per input 2. Execute and classify as correct/incorrect 3. Construct preference pairs 4. Train with dual-loss function 5. Iterate until convergence
- **Design tradeoffs:** Multiple generations per input increase computational cost but provide better coverage; execution-based correctness verification is reliable but may miss semantic issues; preference learning requires sufficient correct and incorrect examples to be effective
- **Failure signatures:** Model fails to generate any correct outputs in early iterations; preference pairs don't provide meaningful distinctions; dual-loss optimization causes performance degradation
- **First 3 experiments:** 1. Baseline: Run base model on test set to establish performance 2. Single iteration: Generate 10 responses per input, execute, construct preferences, train one epoch 3. Multi-iteration: Run 4 iterations and compare convergence behavior and final performance

## Open Questions the Paper Calls Out

The paper identifies several limitations and open questions:
- CodeLutra focuses on correctness of generated code, overlooking other vital aspects such as efficiency, readability, and adherence to specific formal specifications
- The framework relies on execution results as the sole indicator of code quality, which may not capture all semantic errors or edge cases
- The optimal number of iterations and preference pairs per iteration for CodeLutra is not fully explored, with the paper using 4 iterations and K=10 preference pairs per input without systematic ablation studies
- The framework's effectiveness across different programming languages and task types beyond data science queries remains unproven
- Computational cost of generating multiple responses per input and executing them could be prohibitive for resource-constrained applications

## Limitations

- Framework relies on execution-based correctness verification, which may not capture all semantic errors or edge cases
- Effectiveness across different programming languages and task types beyond data science queries remains unproven
- Computational cost of generating multiple responses per input and executing them could be prohibitive for resource-constrained applications
- Framework's performance with models smaller than 7B parameters has not been thoroughly investigated

## Confidence

- **High confidence**: The iterative refinement mechanism and preference learning approach are well-supported by results showing consistent improvement across multiple base models and datasets
- **Medium confidence**: The dual-loss optimization's effectiveness in preventing degradation while improving incorrect outputs, as this depends on specific hyperparameter settings that aren't fully detailed
- **Low confidence**: The framework's generalizability to non-data-science coding tasks and its performance with models smaller than 7B parameters

## Next Checks

1. **Cross-task validation**: Test CodeLutra on software engineering tasks (e.g., implementing algorithms, API usage) beyond data science queries to verify generalizability

2. **Robustness testing**: Evaluate performance when the model initially generates very few correct outputs (e.g., <5%) to test the framework's effectiveness in low-resource scenarios

3. **Ablation study**: Remove the likelihood regularization component to quantify its contribution to preventing performance degradation and maintaining correct solution generation probability