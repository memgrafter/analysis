---
ver: rpa2
title: What Can We Learn from State Space Models for Machine Learning on Graphs?
arxiv_id: '2406.05815'
source_url: https://arxiv.org/abs/2406.05815
tags:
- graph
- gssc
- graphs
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Graph State Space Convolution (GSSC) extends State Space Models
  (SSMs) to graphs by introducing a global, permutation-equivariant aggregation with
  factorizable graph kernels based on relative node distances. This preserves SSM
  advantages of efficiency, long-range dependency modeling, and generalization while
  maintaining graph structure.
---

# What Can We Learn from State Space Models for Machine Learning on Graphs?

## Quick Facts
- arXiv ID: 2406.05815
- Source URL: https://arxiv.org/abs/2406.05815
- Authors: Yinan Huang; Siqi Miao; Pan Li
- Reference count: 34
- Primary result: GSSC achieves SOTA on 7/11 benchmarks, including 98.5% accuracy on MNIST and 80.4% AUROC on ogbg-molhiv

## Executive Summary
This paper introduces Graph State Space Convolution (GSSC), a novel framework that extends State Space Models to graph-structured data by combining global, permutation-equivariant aggregation with factorizable graph kernels based on relative node distances. The approach preserves the efficiency, long-range dependency modeling, and generalization benefits of SSMs while maintaining graph structure awareness. GSSC demonstrates state-of-the-art performance across diverse graph benchmarks and shows theoretical expressiveness guarantees for counting graph substructures.

## Method Summary
GSSC extends SSMs to graphs through a two-component architecture: (1) a factorizable graph kernel that computes aggregation weights based on relative node distances, ensuring permutation equivariance, and (2) a global aggregation mechanism that captures long-range dependencies across the graph. The method transforms graph signals into a state space representation using learned state matrices, applies state transitions, and aggregates information through the factorizable kernel. This design allows GSSC to efficiently model complex graph structures while maintaining the theoretical advantages of SSMs, including linear complexity and the ability to capture long-range dependencies.

## Key Results
- Achieves state-of-the-art performance on 7 of 11 real-world graph benchmarks
- 98.5% accuracy on MNIST graph benchmark
- 80.4% AUROC on ogbg-molhiv molecular property prediction
- Outperforms subgraph GNNs in counting 3- and 4-cycles/paths
- Scales efficiently to graphs with up to 60k nodes

## Why This Works (Mechanism)
GSSC works by leveraging the strengths of SSMs in modeling sequential dependencies and extending them to the irregular structure of graphs. The key mechanism is the factorizable graph kernel that computes aggregation weights based on relative node distances, which preserves permutation equivariance while allowing the model to capture structural patterns. The global aggregation ensures that information can propagate across the entire graph efficiently, addressing the long-range dependency modeling capability that traditional GNNs struggle with. By combining these elements, GSSC can learn both local and global graph structures effectively.

## Foundational Learning
- **State Space Models (SSMs)**: Sequential models that transform input signals through learned state matrices; needed because traditional GNNs struggle with long-range dependencies; quick check: verify SSM's ability to model sequences of length T in O(T) time
- **Permutation Equivariance**: Property ensuring that permuting input nodes results in correspondingly permuted outputs; needed to ensure the model is invariant to node ordering; quick check: apply random node permutations and verify output transformation
- **Factorizable Graph Kernels**: Kernels whose weights depend only on relative node distances; needed to enable efficient computation while preserving structural information; quick check: verify kernel computation scales linearly with node count
- **1-WL Test Power**: Weisfeiler-Lehman test for graph isomorphism; needed as benchmark for expressive power comparison; quick check: verify that GSSC can distinguish graphs that 1-WL cannot

## Architecture Onboarding

**Component Map**: Input graph -> Factorizable Kernel -> Global Aggregation -> State Space Transformation -> Output

**Critical Path**: The critical path is the forward pass through the factorizable kernel and global aggregation, which enables efficient information propagation across the graph. This path determines the model's ability to capture long-range dependencies and is where most of the computational complexity resides.

**Design Tradeoffs**: The factorizable kernel assumption trades expressiveness for efficiency - by limiting aggregation weights to depend only on relative distances, the model gains linear complexity but may miss complex structural dependencies. The global aggregation provides long-range modeling capability but introduces additional parameters that could lead to overfitting on smaller graphs.

**Failure Signatures**: Performance degradation on graphs with complex local structures that violate the factorizable kernel assumption, or failure to capture fine-grained local patterns due to excessive emphasis on global aggregation. Also watch for scalability issues when the global aggregation becomes computationally prohibitive on very large graphs.

**3 First Experiments**:
1. Test GSSC on synthetic graphs with known substructures (triangles, cliques) to verify counting accuracy
2. Compare performance with and without the global aggregation component on long-range dependency tasks
3. Evaluate sensitivity to kernel hyperparameters (bandwidth, order) on graphs of varying size and density

## Open Questions the Paper Calls Out
None

## Limitations
- The factorizable graph kernel assumption may not hold for graphs with complex structural dependencies
- Theoretical expressiveness guarantees are limited to small substructures (3- and 4-cycles/paths)
- Scalability to truly massive graphs (millions of nodes) remains untested
- Comparison baselines could be more comprehensive, particularly for molecular property prediction

## Confidence
- High confidence: Core theoretical framework connecting SSMs to graph convolutions is sound
- Medium confidence: Empirical performance claims are supported but baselines could be stronger
- Medium confidence: Expressiveness claims for substructure counting are theoretically justified for small patterns only

## Next Checks
1. **Scalability validation**: Test GSSC on graphs with millions of nodes to verify claimed efficiency and identify bottlenecks
2. **Cross-domain robustness**: Evaluate GSSC on heterogeneous, temporal, and richly featured graphs to assess generality
3. **Ablation on aggregation**: Conduct controlled experiments removing global aggregation to quantify its specific contribution