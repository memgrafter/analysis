---
ver: rpa2
title: 'OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial
  Domain'
arxiv_id: '2412.13018'
source_url: https://arxiv.org/abs/2412.13018
tags:
- data
- evaluation
- document
- generated
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniEval, an omnidirectional and automatic
  RAG evaluation benchmark in the financial domain. The benchmark employs a multi-dimensional
  evaluation framework that includes a matrix-based RAG scenario evaluation system,
  multi-dimensional evaluation data generation using GPT-4-based automatic generation
  and human annotation, a multi-stage evaluation system assessing both retrieval and
  generation performance, and robust evaluation metrics combining rule-based and LLM-based
  ones.
---

# OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain

## Quick Facts
- arXiv ID: 2412.13018
- Source URL: https://arxiv.org/abs/2412.13018
- Authors: Shuting Wang; Jiejun Tan; Zhicheng Dou; Ji-Rong Wen
- Reference count: 40
- Primary result: Introduces OmniEval, a multi-dimensional RAG evaluation benchmark with 11.4k automatically generated and 1.7k human-annotated financial QA pairs

## Executive Summary
This paper introduces OmniEval, an omnidirectional and automatic RAG evaluation benchmark specifically designed for the financial domain. The benchmark employs a multi-dimensional evaluation framework that categorizes queries into five task classes and 16 financial topics, creating a structured assessment of diverse query scenarios. OmniEval features a multi-agent data generation pipeline combining GPT-4-based automatic generation with human annotation, achieving an 87.47% acceptance ratio in human evaluations. The benchmark evaluates both retrieval and generation performance using a combination of rule-based and LLM-based metrics, with preliminary experiments showing 74.4% accuracy for LLM evaluators.

## Method Summary
OmniEval uses a multi-agent approach for data generation, where GPT-4 automatically generates QA pairs across a 5×16 task-topic matrix, followed by quality inspection and human annotation. The knowledge corpus is constructed from multiple financial data sources and preprocessed using LlamaIndex. Retrievers are evaluated using MAP, MRR, Rouge-L, and F1 metrics, while generators are assessed using ACC, COM, HAL, UTL, and NAC metrics. An LLM evaluator is fine-tuned using Lora on human-annotated data for semantic evaluation. The benchmark contains 11.4k automatically generated and 1.7k human-annotated test examples.

## Key Results
- Automatic data generation pipeline achieves 87.47% acceptance ratio in human evaluations
- Benchmark contains 11.4k automatically generated test examples and 1.7k human-annotated examples
- LLM evaluator achieves 74.4% accuracy in preliminary experiments
- Matrix-based evaluation covers 5 task classes × 16 financial topics = 80 scenario combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional evaluation data generation combining GPT-4-based automatic generation and human annotation achieves high data quality
- Mechanism: Automatic generation provides flexibility and scalability while human annotation ensures quality control, creating a robust evaluation dataset
- Core assumption: The combination of automated and human processes can achieve higher quality than either approach alone
- Evidence anchors:
  - [abstract]: "achieves an 87.47% acceptance ratio in human evaluations on generated instances"
  - [section]: "The automatic data generation pipeline achieves an 87.47% acceptance ratio in human evaluations on generated instances"
  - [corpus]: Weak evidence - corpus shows related work on financial benchmarks but doesn't directly validate the 87.47% figure

### Mechanism 2
- Claim: Matrix-based RAG scenario evaluation using orthogonal task and topic taxonomies provides comprehensive assessment
- Mechanism: Cartesian product of 5 task classes and 16 financial topics creates structured assessment covering diverse query scenarios
- Core assumption: Real-world RAG applications can be adequately represented by the combination of these specific task and topic dimensions
- Evidence anchors:
  - [abstract]: "categorizes queries into five task classes and 16 financial topics, leading to a structured assessment of diverse query scenarios"
  - [section]: "We classified RAG scenarios into five common tasks... Moreover, in specialized domains like finance, user queries often fall into distinct domain topics"
  - [corpus]: Weak evidence - corpus shows existence of task-based financial benchmarks but doesn't validate the specific 5x16 matrix approach

### Mechanism 3
- Claim: Multi-stage evaluation assessing both retrieval and generation performance provides comprehensive RAG pipeline assessment
- Mechanism: Separate evaluation of retriever quality (MAP, MRR) and generator quality (rule-based and LLM-based metrics) captures full RAG system performance
- Core assumption: Retrieval quality and generation quality are independent factors that both significantly impact overall RAG performance
- Evidence anchors:
  - [abstract]: "a multi-stage evaluation system that evaluates both retrieval and generation performance"
  - [section]: "The quality of the retrieval and generation processes are both important when evaluating the RAG pipeline"
  - [corpus]: Weak evidence - corpus shows existence of multi-stage evaluation approaches but doesn't validate their effectiveness for RAG specifically

## Foundational Learning

- Concept: Matrix-based evaluation design
  - Why needed here: To systematically cover diverse RAG scenarios through orthogonal dimensions (tasks × topics)
  - Quick check question: Can you explain why a Cartesian product approach is more comprehensive than evaluating tasks and topics separately?

- Concept: Multi-agent data generation pipeline
  - Why needed here: To combine the scalability of LLM generation with the quality control of human annotation
  - Quick check question: What are the key differences between the roles of the data generation agent versus the quality inspection agent?

- Concept: Multi-stage evaluation metrics
  - Why needed here: To separately assess retrieval performance (MAP, MRR) and generation performance (accuracy, completeness, etc.)
  - Quick check question: Why might a high-quality retriever still produce poor RAG performance if paired with a weak generator?

## Architecture Onboarding

- Component map: Document → Topic Classification → Data Generation → Quality Inspection → Human Annotation → Test Set Creation → Model Evaluation
- Critical path: Document → Topic Classification → Data Generation → Quality Inspection → Human Annotation → Test Set Creation → Model Evaluation
- Design tradeoffs:
  - Automatic vs. human data generation: Speed/scale vs. quality control
  - Rule-based vs. LLM-based metrics: Stability/reproducibility vs. semantic understanding
  - Number of topics/tasks: Coverage vs. dataset size and evaluation complexity
- Failure signatures:
  - Low acceptance ratio (<70%) in human evaluation indicates data generation issues
  - High variance in performance across topic-task combinations suggests imbalanced coverage
  - Poor correlation between retriever and generator metrics may indicate evaluation design problems
- First 3 experiments:
  1. Validate topic classification accuracy by manually checking document-to-topic assignments
  2. Test data generation quality by comparing automatically generated instances against human-created ones
  3. Verify evaluation metrics by running known-good and known-bad RAG configurations through the full pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the automatic data generation pipeline handle the trade-off between generating diverse financial scenarios and maintaining high-quality, domain-specific content?
- Basis in paper: Explicit - The paper mentions integrating GPT-4-based automatic generation with human annotation to achieve 87.47% acceptance ratio, but does not detail how diversity vs. quality is balanced.
- Why unresolved: The paper lacks detailed methodology on how the automatic generation ensures both diversity across financial topics and tasks while maintaining high quality through human oversight.
- What evidence would resolve it: Detailed analysis showing the distribution of generated scenarios across topics/tasks and their quality metrics, along with methodology for balancing diversity and quality.

### Open Question 2
- Question: What are the specific challenges and limitations in evaluating numerical accuracy (NAC) in financial RAG systems, and how can these be addressed in future benchmarks?
- Basis in paper: Explicit - The paper introduces NAC as a metric for financial computations but does not discuss challenges in evaluating numerical accuracy.
- Why unresolved: While NAC is mentioned as a metric, the paper does not explore the difficulties in accurately assessing numerical computations in financial contexts.
- What evidence would resolve it: Case studies showing common errors in numerical computations, comparison of different evaluation approaches for NAC, and proposed solutions for improving accuracy assessment.

### Open Question 3
- Question: How does the performance of RAG systems vary across different financial topics and tasks when evaluated using the multi-stage evaluation system?
- Basis in paper: Explicit - The paper mentions evaluating both retrieval and generation performance but does not provide detailed analysis of performance variations across specific topics and tasks.
- Why unresolved: While the multi-stage evaluation system is described, the paper lacks detailed breakdown of how RAG systems perform on different combinations of financial topics and tasks.
- What evidence would resolve it: Comprehensive performance analysis showing retrieval and generation scores across all topic-task combinations, with insights into which combinations pose the greatest challenges.

## Limitations
- The 87.47% acceptance ratio is a critical validation point, but the paper doesn't provide detailed breakdowns by topic-task combinations or explain what happens to rejected instances
- The effectiveness of the multi-stage evaluation approach assumes retrieval and generation quality are independent factors, but this assumption isn't empirically tested
- The 74.4% accuracy for LLM evaluators leaves open questions about performance on edge cases and domain-specific financial queries

## Confidence
- **High Confidence**: The matrix-based evaluation design (5 tasks × 16 topics) is clearly specified and logically sound for comprehensive coverage
- **Medium Confidence**: The multi-agent data generation pipeline appears robust, but real-world performance depends on implementation details not fully disclosed
- **Medium Confidence**: The multi-stage evaluation metrics are well-defined, but their independence and relative importance need empirical validation

## Next Checks
1. **Acceptance Ratio Analysis**: Break down the 87.47% acceptance ratio by topic-task combinations to identify which specific areas need improvement in the automatic generation pipeline
2. **Independence Validation**: Test whether retriever and generator performance are truly independent by analyzing correlation between MAP/MRR scores and generation metrics across the dataset
3. **LLM Evaluator Robustness**: Evaluate the LLM evaluator's performance on deliberately challenging financial queries (ambiguous questions, domain-specific jargon, time-sensitive information) to assess real-world reliability