---
ver: rpa2
title: Investigating Semi-Supervised Learning Algorithms in Text Datasets
arxiv_id: '2401.01843'
source_url: https://arxiv.org/abs/2401.01843
tags:
- training
- data
- samples
- algorithms
- tri-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares semi-supervised learning (SSL) algorithms\
  \ that do not require data augmentation for text datasets. Four algorithms\u2014\
  self-training, co-training, tri-training, and tri-training with disagreement\u2014\
  are evaluated on four different text classification tasks using Turkish datasets."
---

# Investigating Semi-Supervised Learning Algorithms in Text Datasets

## Quick Facts
- arXiv ID: 2401.01843
- Source URL: https://arxiv.org/abs/2401.01843
- Reference count: 27
- This study compares SSL algorithms that do not require data augmentation for text datasets, with tri-training with disagreement showing the most promise.

## Executive Summary
This study investigates four semi-supervised learning algorithms—self-training, co-training, tri-training, and tri-training with disagreement—on four Turkish text classification tasks. The research focuses on algorithms that do not require data augmentation, evaluating their performance across various settings including sampling strategies, model initialization, and threshold configurations. Tri-training with disagreement consistently shows the most promise, achieving performance closest to the Oracle upper bound, particularly when the unlabeled dataset comprises 90% of the total data. The study reveals important insights about the effectiveness of dual-threshold strategies in self-training and the benefits of training new models from scratch in each iteration.

## Method Summary
The study evaluates semi-supervised learning algorithms on four Turkish text datasets using Berturk embeddings and a simple neural network architecture (768-16-16-output). The algorithms compared include self-training, co-training, tri-training, and tri-training with disagreement. Experiments use 3-fold cross-validation with 5 trials per fold, and the primary metric is classification accuracy. The research systematically investigates various aspects of each algorithm, including sampling strategies, model initialization approaches, and threshold settings, while comparing performance against supervised baselines and an Oracle upper bound.

## Key Results
- Tri-training with disagreement achieved the closest performance to the Oracle upper bound across all datasets.
- Self-training improved significantly when using two thresholds and training new models from scratch in each iteration.
- The performance gap between SSL methods and the Oracle remains significant, suggesting room for improvement in existing algorithms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tri-training with disagreement improves performance by requiring two models to agree and the third to disagree before adding a sample to training.
- Mechanism: This disagreement condition filters out samples that are less likely to be correctly labeled, since at least one model is uncertain.
- Core assumption: The models are diverse enough that when two agree and one disagrees, the disagreement indicates potential noise or error in the consensus.
- Evidence anchors:
  - [abstract] "Tri-training with disagreement showed the closest performance to the Oracle"
  - [section] "Tri-training with disagreement adds the condition that the third model makes a different prediction than the two other models"
  - [corpus] Weak; neighbor papers focus on image SSL or different text SSL methods, not tri-training disagreement specifically.
- Break condition: If models are too similar, disagreement may not provide meaningful signal; if models are too dissimilar, consensus may rarely occur.

### Mechanism 2
- Claim: Training a new model from scratch in each iteration improves self-training performance by allowing the model to escape local minima and adapt better to newly added samples.
- Mechanism: Fresh initialization resets the model weights, reducing bias from previous iterations and enabling more effective learning from the updated dataset.
- Core assumption: Neural networks' stochastic nature means different initializations can lead to significantly different learning trajectories.
- Evidence anchors:
  - [section] "self-training improved significantly by training a new model in each iteration"
  - [abstract] "the benefit of training new models from scratch in self-training"
  - [corpus] Weak; neighbor papers do not specifically address model reinitialization in SSL.
- Break condition: High computational cost may outweigh benefits; if dataset changes are minimal, reinitialization may not add value.

### Mechanism 3
- Claim: Using two thresholds (upper and lower bounds) in self-training allows inclusion of moderately confident samples while excluding both low-confidence and overly confident samples.
- Mechanism: This approach introduces diversity in training by including samples that are not too easy (already highly confident) nor too hard (low confidence), potentially improving model generalization.
- Core assumption: Overly confident samples provide little new information, while low-confidence samples are likely incorrectly labeled.
- Evidence anchors:
  - [abstract] "the importance of using two thresholds in self-training"
  - [section] "By including samples that are smaller than a certain confidence interval, the model's performance may be improved"
  - [corpus] Weak; neighbor papers do not discuss dual-threshold strategies in SSL.
- Break condition: If thresholds are set incorrectly, may exclude too many useful samples or include too many incorrect ones.

## Foundational Learning

- Concept: Semi-supervised learning (SSL)
  - Why needed here: The paper compares SSL algorithms, so understanding how SSL works is essential to grasp the methods and results.
  - Quick check question: What is the main goal of semi-supervised learning compared to supervised learning?

- Concept: Proxy-label methods
  - Why needed here: The algorithms studied (self-training, co-training, tri-training) are proxy-label methods, which rely on the model's own predictions to label unlabeled data.
  - Quick check question: How do proxy-label methods differ from consistency regularization methods in SSL?

- Concept: Neural network training and cross-validation
  - Why needed here: The experiments use neural networks as classifiers and employ three-fold cross-validation to evaluate performance reliably.
  - Quick check question: Why is cross-validation important when evaluating models on small datasets?

## Architecture Onboarding

- Component map:
  Input layer (768 neurons) -> Hidden layers (16-16 neurons, ReLU) -> Output layer (softmax, neurons equal to classes)

- Critical path:
  Data preprocessing (BERT embeddings) -> Model initialization -> Training with labeled data -> Iterative SSL (if applicable) -> Evaluation (3-fold CV, 5 trials each)

- Design tradeoffs:
  Simple neural network architecture vs. more complex models: Simplicity aids reproducibility and reduces training time but may limit performance.
  Using ensemble models vs. single best model: Ensembles may improve robustness but increase computational cost.

- Failure signatures:
  Overfitting on small labeled datasets
  Poor performance if SSL methods add too many incorrectly labeled samples
  High variance in results due to neural network stochasticity

- First 3 experiments:
  1. Baseline: Train supervised model on labeled data only, evaluate with cross-validation.
  2. Apply self-training with single threshold, compare performance to baseline.
  3. Apply tri-training with disagreement, compare performance to baseline and self-training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of tri-training with disagreement compare to other semi-supervised learning algorithms in text datasets, especially when the unlabeled dataset comprises a large percentage of the total data?
- Basis in paper: [explicit] The paper explicitly states that tri-training with disagreement showed the closest performance to the Oracle, particularly when the unlabeled dataset comprises 90% of the total data.
- Why unresolved: The paper provides experimental results showing the effectiveness of tri-training with disagreement, but it does not compare it to other SSL algorithms in terms of performance on text datasets with a large percentage of unlabeled data.
- What evidence would resolve it: Further experiments comparing tri-training with disagreement to other SSL algorithms on text datasets with varying percentages of unlabeled data would provide evidence to resolve this question.

### Open Question 2
- Question: How does the choice of sampling strategy affect the performance of tri-training and tri-training with disagreement in terms of initial model training and iterative learning?
- Basis in paper: [explicit] The paper discusses the impact of different sampling strategies on the performance of tri-training and tri-training with disagreement, noting that a small sample size reduces the success rate compared to a large sample size in initial training of tri-training.
- Why unresolved: While the paper provides insights into the effects of sampling strategies, it does not explore the specific impact on initial model training and iterative learning in detail.
- What evidence would resolve it: Conducting experiments that isolate the effects of sampling strategies on initial model training and iterative learning in tri-training and tri-training with disagreement would provide evidence to resolve this question.

### Open Question 3
- Question: What is the optimal threshold value for self-training in terms of model performance and label reliability?
- Basis in paper: [explicit] The paper discusses the importance of threshold values in self-training, noting that a high threshold increases label reliability but may not change the current weights of the models much, while a low threshold reduces label reliability.
- Why unresolved: The paper does not provide a definitive answer on the optimal threshold value for self-training in terms of model performance and label reliability.
- What evidence would resolve it: Conducting experiments with different threshold values in self-training and evaluating their impact on model performance and label reliability would provide evidence to resolve this question.

## Limitations
- Findings are limited to Turkish datasets and may not generalize to other languages or domains.
- The relatively simple neural network architecture may limit performance ceilings.
- Key hyperparameters like threshold values are not fully specified, affecting reproducibility.

## Confidence
- **High**: Tri-training with disagreement outperforms other SSL methods and approaches Oracle performance on 90% unlabeled data.
- **Medium**: Dual-threshold self-training improves performance by balancing sample diversity and confidence.
- **Medium**: Training new models from scratch in each iteration benefits self-training by reducing bias.

## Next Checks
1. Replicate experiments with larger, more diverse text datasets across multiple languages to test generalizability.
2. Conduct ablation studies to isolate the impact of specific hyperparameters (e.g., threshold values, sampling strategies) on SSL performance.
3. Compare the proposed SSL methods against modern SSL techniques (e.g., consistency regularization, pseudo-labeling with sharpening) to benchmark their effectiveness.