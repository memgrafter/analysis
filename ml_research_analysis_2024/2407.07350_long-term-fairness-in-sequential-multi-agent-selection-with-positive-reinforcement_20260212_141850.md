---
ver: rpa2
title: Long-Term Fairness in Sequential Multi-Agent Selection with Positive Reinforcement
arxiv_id: '2407.07350'
source_url: https://arxiv.org/abs/2407.07350
tags:
- fairness
- applicants
- reinforcement
- institution
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies long-term fairness in sequential multi-agent
  selection with positive reinforcement. It addresses the problem of multiple agents
  selecting from a common applicant pool, aiming to achieve long-term fairness targets
  for underrepresented groups through positive feedback.
---

# Long-Term Fairness in Sequential Multi-Agent Selection with Positive Reinforcement

## Quick Facts
- arXiv ID: 2407.07350
- Source URL: https://arxiv.org/abs/2407.07350
- Authors: Bhagyashree Puranik; Ozgur Guldogan; Upamanyu Madhow; Ramtin Pedarsani
- Reference count: 40
- Primary result: Multi-agent Fair-Greedy policy achieves long-term fairness under pure positive reinforcement but can cause negative feedback under role model reinforcement

## Executive Summary
This paper addresses long-term fairness in sequential multi-agent selection where multiple institutions choose from a common applicant pool. The core contribution is the Multi-agent Fair-Greedy (MFG) policy that balances score maximization with fairness objectives. Under identical score distributions across groups, the MFG policy provably converges to a long-term fairness target. However, the paper identifies a critical vulnerability: when selection creates "role models" who influence future applicant pools, uncoordinated agent behavior can trigger negative feedback loops that reduce representation of underrepresented groups. A centralized version of the policy is proposed to mitigate this effect.

## Method Summary
The method frames sequential selection as a Markov Decision Process where states represent applicant pool compositions and actions are admission proportions. Each institution maximizes a utility combining score-based rewards with fairness loss, parameterized by λ. The MFG policy computes optimal admission thresholds that balance these objectives. Under pure positive reinforcement, the system converges to a fairness target when score distributions are identical. For role model reinforcement, where only top-scoring admitted applicants influence future pools, the paper shows that decentralized MFG can cause negative feedback, proposing a centralized coordination mechanism (CMFG) as a remedy.

## Key Results
- MFG policy converges to long-term fairness target under pure positive reinforcement with identical score distributions
- Under role model reinforcement, MFG can cause negative feedback, reducing minority representation
- CMFG policy mitigates negative feedback effects through centralized threshold coordination
- Experiments with synthetic and semi-synthetic data validate theoretical findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MFG converges to fairness target under identical score distributions with pure positive reinforcement
- Mechanism: Institutions maximize utility combining score rewards and fairness loss, shifting admission proportions toward target, which drives pool composition via positive feedback
- Core assumption: Identical score distributions; large applicant pools (law of large numbers)
- Evidence anchors: Abstract and section 3 theorems on convergence under assumptions 1 and 2
- Break condition: Differing score distributions across groups

### Mechanism 2
- Claim: Role model reinforcement causes negative feedback under MFG
- Mechanism: Top-scoring admitted applicants become role models; higher-ranked institutions deplete minority role models, reducing future minority applicants
- Core assumption: Role model threshold sufficiently high relative to institutional thresholds
- Evidence anchors: Abstract and Proposition 1 in section 4
- Break condition: High role model parameter r or centralized coordination

### Mechanism 3
- Claim: CMFG mitigates negative feedback under role model reinforcement
- Mechanism: Central coordinator jointly optimizes thresholds across institutions to distribute fairness cost and prevent cascading depletion
- Core assumption: Central coordinator has utility knowledge and can enforce joint decisions
- Evidence anchors: Abstract and section 4.1 description of CMFG
- Break condition: Infeasible coordination or collusion concerns

## Foundational Learning

- Concept: Markov Decision Process (MDP) framework for sequential decision-making
  - Why needed here: Models state transitions as applicant pool compositions, actions as admission proportions
  - Quick check question: What determines the transition from state st to st+1 in the MDP formulation?

- Concept: Concave optimization and threshold policies
  - Why needed here: Concave score rewards enable closed-form optimal thresholds; Fair-Greedy balances with fairness
  - Quick check question: Why does concavity of the score-based reward function guarantee a unique optimal action?

- Concept: Positive and negative reinforcement dynamics in population models
  - Why needed here: Pool evolution depends on how admissions affect future applicant proportions
  - Quick check question: How does the sign of (πW t - st) in the pool update equation determine positive vs negative reinforcement?

## Architecture Onboarding

- Component map: State tracker -> Institution modules -> Pool evolution engine -> Role model detector -> Central coordinator (optional)
- Critical path: Initialize state and mean parameter → For each institution: compute optimal action → Update remaining pool → Compute weighted policy → Update mean parameter → Sample next applicant pool
- Design tradeoffs: Decentralized MFG simpler but vulnerable to negative feedback; CMFG robust but requires coordination
- Failure signatures: Minority proportion trending to zero (role model reinforcement failure); slow convergence or oscillation (poor λ choice); disproportionate low-scoring admissions (aggressive fairness)
- First 3 experiments: 1) Run MFG under pure positive reinforcement with identical distributions; 2) Run MFG under role model reinforcement with small r; 3) Run CMFG under same role model settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms can institutions implement to increase successful role models and mitigate negative feedback?
- Basis in paper: Paper identifies need for support mechanisms but doesn't specify implementation
- Why unresolved: Identifies need but provides no specific mechanisms or effectiveness data
- What evidence would resolve it: Empirical studies showing effectiveness of mentorship, skill development, or other support programs

### Open Question 2
- Question: How do evolution model variations affect MFG robustness under non-identical score distributions?
- Basis in paper: Explores order-based and weighted reinforcement models
- Why unresolved: Paper provides initial insights but lacks systematic exploration
- What evidence would resolve it: Comparative studies showing MFG performance across different evolution models and score distributions

### Open Question 3
- Question: What are trade-offs between immediate and long-term fairness objectives in MFG?
- Basis in paper: Discusses balance between score maximization and fairness; mentions temporary deprioritization by lower-ranked institutions
- Why unresolved: Paper doesn't analyze trade-offs or provide balancing strategies
- What evidence would resolve it: Analysis of λ impacts on short/long-term outcomes with case studies or simulations

## Limitations
- Assumes identical score distributions across groups, which may not hold in real-world settings
- Simplified pool evolution dynamics may not capture complex social feedback mechanisms
- Role model reinforcement model uses threshold-based mechanism that may not reflect actual pipeline effects

## Confidence
- Theoretical convergence under identical distributions: High confidence (formal proofs in appendix)
- Negative feedback analysis under role model reinforcement: Medium confidence (single illustrative example)
- CMFG as mitigation mechanism: Medium confidence (conceptual presentation without rigorous comparison)

## Next Checks
1. Test MFG convergence under varying λ values and score distribution parameters to identify robustness boundaries
2. Systematically explore role model parameter space to characterize conditions leading to negative feedback
3. Compare CMFG performance against simple coordination heuristics in multi-institution settings