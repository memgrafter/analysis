---
ver: rpa2
title: Linguistically Grounded Analysis of Language Models using Shapley Head Values
arxiv_id: '2410.13396'
source_url: https://arxiv.org/abs/2410.13396
tags:
- language
- clusters
- paradigms
- linguistic
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how linguistic knowledge is encoded in
  language models, focusing on morphosyntactic phenomena. Using Shapley Head Values
  (SHVs) and the BLiMP dataset, the authors analyze BERT and RoBERTa to identify attention
  heads responsible for processing specific linguistic constructions.
---

# Linguistically Grounded Analysis of Language Models using Shapley Head Values

## Quick Facts
- arXiv ID: 2410.13396
- Source URL: https://arxiv.org/abs/2410.13396
- Reference count: 15
- Key outcome: This paper investigates how linguistic knowledge is encoded in language models, focusing on morphosyntactic phenomena. Using Shapley Head Values (SHVs) and the BLiMP dataset, the authors analyze BERT and RoBERTa to identify attention heads responsible for processing specific linguistic constructions. They demonstrate that related phenomena cluster together in both models, supporting the hypothesis that language models develop subnetworks corresponding to linguistic theory. Pruning experiments show that removing top attention heads from these clusters significantly impacts model performance, validating the effectiveness of SHV-based attributions in identifying crucial model components.

## Executive Summary
This paper presents a novel approach to analyzing linguistic knowledge in language models by combining Shapley Head Values with the BLiMP dataset. The authors investigate how attention heads in BERT and RoBERTa encode morphosyntactic information by computing the marginal contribution of each head to grammaticality judgments across 67 linguistic constructions. Through clustering analysis, they demonstrate that attention heads responsible for processing related linguistic phenomena group together, suggesting modular organization of linguistic knowledge in these models. The pruning experiments validate these findings by showing that removing identified heads causes localized performance impacts on related constructions, confirming the effectiveness of SHV-based attributions.

## Method Summary
The authors analyze BERT and RoBERTa models using Shapley Head Values to identify attention heads responsible for morphosyntactic processing. They fine-tune models on the BLiMP dataset (67 constructions across 13 linguistic phenomena) using LoRA adapters for grammaticality judgment tasks. SHVs are computed for each attention head across all paradigms using Monte Carlo sampling. The authors then cluster paradigms based on SHV similarity using k-means clustering and validate the clusters through pruning experiments that compare performance impacts when removing in-cluster versus out-of-cluster heads. The methodology bridges computational linguistics and model interpretability by grounding the analysis in established linguistic theory.

## Key Results
- Related linguistic phenomena cluster together in both BERT and RoBERTa, with clustering purity scores of 70-80%
- Pruning experiments show localized performance impacts when removing top attention heads from identified clusters
- BERT and RoBERTa exhibit similar clustering patterns, suggesting consistent organization of linguistic knowledge across architectures
- Performance degradation from pruning ranges from 1-4% accuracy drops for in-cluster heads compared to out-of-cluster heads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHV-based attributions successfully identify attention heads responsible for processing specific morphosyntactic phenomena.
- Mechanism: Shapley Head Values measure the marginal contribution of each attention head to model performance on grammaticality tasks, allowing identification of heads crucial for specific linguistic constructions.
- Core assumption: The performance difference when including versus excluding an attention head accurately reflects its importance for processing the target phenomenon.
- Evidence anchors:
  - [abstract] "Using Shapley Head Values (SHVs) and the BLiMP dataset, the authors analyze BERT and RoBERTa to identify attention heads responsible for processing specific linguistic constructions."
  - [section] "Following Held and Yang (2023), we define SHV φh, for a single attention head Atth ∈ A1, to represent the mean performance improvement on the characteristic function V"
  - [corpus] Weak - corpus evidence is sparse and doesn't directly support this mechanism

### Mechanism 2
- Claim: Related linguistic phenomena cluster together in language models, indicating the development of subnetworks corresponding to linguistic theory.
- Mechanism: Clustering BLiMP paradigms based on SHV similarity reveals that attention heads processing related constructions group together, suggesting modular organization of linguistic knowledge.
- Core assumption: Attention heads processing related phenomena will have similar SHV patterns across different constructions.
- Evidence anchors:
  - [abstract] "Through quantitative pruning and qualitative clustering analysis, we demonstrate that attention heads responsible for processing related linguistic phenomena cluster together."
  - [section] "We cluster constructions based on SHVs, and assess the success of isolating heads responsible for processing aspects of morphosyntax using pruning based on relative importance of attention heads"
  - [corpus] Weak - corpus evidence is sparse and doesn't directly support this mechanism

### Mechanism 3
- Claim: Pruning experiments validate SHV-based attributions by showing localized performance impacts when relevant attention heads are removed.
- Mechanism: Removing top attention heads identified by SHVs causes greater performance degradation for related phenomena than for unrelated ones, confirming the attributions' accuracy.
- Core assumption: Performance degradation correlates with the importance of removed attention heads for processing the target phenomena.
- Evidence anchors:
  - [abstract] "Pruning experiments show that removing top attention heads from these clusters significantly impacts model performance, validating the effectiveness of SHV-based attributions"
  - [section] "Quantitative pruning validates our clusters by showing localized performance impacts when relevant attention heads are removed"
  - [corpus] Weak - corpus evidence is sparse and doesn't directly support this mechanism

## Foundational Learning

- Concept: Shapley Values from game theory
  - Why needed here: Provides the theoretical foundation for calculating fair attributions of attention heads' contributions to model performance
  - Quick check question: Can you explain why Shapley Values satisfy properties like efficiency, symmetry, and additivity?

- Concept: Morphosyntactic phenomena and linguistic theory
  - Why needed here: Understanding how linguistic constructions like anaphor agreement and filler-gap dependencies work is crucial for interpreting which model components process which phenomena
  - Quick check question: What's the difference between subject-verb agreement and determiner-noun agreement in terms of morphosyntactic processing?

- Concept: K-means clustering and inertia
  - Why needed here: Clustering paradigms based on SHV similarity requires understanding how to group similar vectors and determine optimal cluster count
  - Quick check question: How does inertia measure cluster quality, and what does it mean when inertia stops decreasing significantly?

## Architecture Onboarding

- Component map: BERT/RoBERTa models with attention heads → SHV computation → clustering → pruning → evaluation
- Critical path: Data preparation (BLiMP) → model fine-tuning with LoRA → SHV calculation → clustering → pruning experiments → analysis
- Design tradeoffs: Using LoRA adapters preserves original model knowledge but may limit fine-tuning capacity; k-means clustering is efficient but sensitive to initialization
- Failure signatures: Poor clustering results (low purity), pruning not causing localized impacts, SHV calculations not converging
- First 3 experiments:
  1. Verify SHV calculations on a small subset of paradigms to ensure correct implementation
  2. Test clustering on synthetic data with known groupings to validate the approach
  3. Perform basic pruning on individual paradigms to confirm the mechanism works before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified subnetworks correspond to linguistic knowledge across different languages?
- Basis in paper: [explicit] The paper mentions "potential implications for cross-linguistic model analysis" and notes that coverage of linguistic datasets for languages other than English lags behind BLiMP.
- Why unresolved: The analysis is currently limited to English, and there's a clear gap in cross-linguistic datasets that would enable this investigation.
- What evidence would resolve it: Testing the SHV clustering methodology on multilingual datasets like SLING (for Chinese) or RuBLiMP (for Russian) to see if similar linguistic subnetworks emerge across languages.

### Open Question 2
- Question: How do the SHV-identified subnetworks change during the training process of language models?
- Basis in paper: [inferred] The paper analyzes pretrained models (BERT and RoBERTa) but doesn't examine how linguistic knowledge develops over training time.
- Why unresolved: The study focuses on static analysis of fully trained models without examining developmental trajectories.
- What evidence would resolve it: Tracking SHV attributions and cluster formations at different stages of pretraining to identify when and how linguistic subnetworks emerge.

### Open Question 3
- Question: Can SHV-based pruning be used to improve model efficiency without significant performance loss?
- Basis in paper: [explicit] The paper uses pruning to validate cluster cohesion but doesn't explore using this for model compression.
- Why unresolved: While the paper demonstrates pruning impacts accuracy, it doesn't investigate whether removing less important heads identified by SHVs could lead to more efficient models.
- What evidence would resolve it: Systematically removing attention heads with low SHV scores across different linguistic phenomena and measuring the trade-off between model size reduction and performance degradation.

## Limitations
- Clustering purity scores of 70-80% indicate substantial noise in grouping related phenomena
- Pruning experiments show modest localized effects (1-4% accuracy drops) compared to random pruning baselines
- Reliance on BLiMP as the sole evaluation dataset limits generalizability to other linguistic phenomena or languages

## Confidence

**High Confidence**: The SHV computation methodology is sound and the implementation details are well-specified. The finding that related phenomena cluster together is consistently observed across both models, supporting the modular organization hypothesis.

**Medium Confidence**: The pruning experiments show localized performance impacts, but the effect sizes are small and could be influenced by other factors beyond the attributed attention heads. The clustering results, while promising, show significant variation in purity scores across different k values.

**Low Confidence**: Claims about specific attention heads being responsible for particular linguistic constructions are difficult to verify given the complex interactions between model components and the limited interpretability of attention mechanisms.

## Next Checks

1. **Replication with Additional Linguistic Phenomena**: Test the SHV clustering and pruning approach on additional linguistic datasets beyond BLiMP, such as CoLA or specialized psycholinguistic datasets, to verify generalizability.

2. **Cross-Model Consistency Analysis**: Perform detailed comparison of which specific attention heads are identified across BERT and RoBERTa for the same phenomena, and investigate cases where models disagree to understand architecture-specific vs universal patterns.

3. **Ablation Studies on SHV Parameters**: Systematically vary the Monte Carlo sampling parameters, LoRA adapter configurations, and k-means clustering settings to determine the sensitivity of results to these methodological choices.