---
ver: rpa2
title: 'LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model
  Merging'
arxiv_id: '2410.17146'
source_url: https://arxiv.org/abs/2410.17146
tags:
- tasks
- task
- mnist
- cars
- mnist20406080
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LiNeS is a post-training layer-wise scaling method that mitigates
  catastrophic forgetting by applying linear scaling to parameter updates, preserving
  shallow layers to maintain pre-trained generalization while allowing deeper layers
  to specialize. The method scales task vectors based on layer depth, effectively
  improving both single-task and multi-task performance across vision and NLP benchmarks.
---

# LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging

## Quick Facts
- arXiv ID: 2410.17146
- Source URL: https://arxiv.org/abs/2410.17146
- Authors: Ke Wang; Nikolaos Dimitriadis; Alessandro Favero; Guillermo Ortiz-Jimenez; Francois Fleuret; Pascal Frossard
- Reference count: 40
- Key outcome: Layer-wise scaling method that mitigates catastrophic forgetting and enhances model merging across vision and NLP tasks

## Executive Summary
LiNeS introduces a post-training layer scaling technique that linearly scales parameter updates based on layer depth to prevent catastrophic forgetting and improve model merging. By preserving shallow layers' pre-trained values while allowing deeper layers to retain task-specific features, LiNeS maintains generalization across tasks. The method demonstrates consistent improvements in both single-task and multi-task settings, with up to 4.5% accuracy gains in NLP tasks and 4.0% in vision tasks when merging multiple fine-tuned models.

## Method Summary
LiNeS applies linear scaling to parameter updates during post-training, with scaling factors increasing from shallow to deep layers. The method extracts task vectors (differences between fine-tuned and pre-trained weights) and applies depth-dependent scaling coefficients before recombining with pre-trained weights. This preserves general features in shallow layers while retaining task-specific knowledge in deeper layers, effectively mitigating catastrophic forgetting and improving multi-task model merging performance.

## Key Results
- Improves single-task performance by preserving generalization while maintaining task-specific knowledge
- Enhances multi-task model merging with up to 4.0% accuracy gains in vision tasks and 4.5% in NLP tasks
- Successfully integrated with WiSE-FT for improved out-of-distribution generalization
- Boosts performance when merging LLM policies aligned with different rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear scaling of parameter updates based on layer depth preserves pre-trained generalization while retaining fine-tuned task knowledge.
- Mechanism: LiNeS applies a layer-wise scaling factor that increases linearly from shallow to deep layers. Shallow layers are scaled down more aggressively to preserve general features, while deep layers retain more of the task-specific updates.
- Core assumption: Shallow layers encode more generalizable features, and deeper layers encode more task-specific features (Neyshabur et al., 2020; Yosinski et al., 2014).
- Evidence anchors:
  - [abstract] "LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations."
  - [section] "Motivated by the results of the previous section for mitigating forgetting, we propose LiNeS for Layer-increasing Network Scaling, a simple post-training technique that linearly rescales the updates of different layers in the task vector based on their depth in the network."
  - [corpus] Weak evidence - corpus papers focus on training-free model merging and task arithmetic but don't explicitly address layer-wise scaling mechanisms.
- Break condition: If the assumption that shallow layers encode generalizable features is invalid, or if the linear scaling schedule doesn't align with the actual feature distribution across layers.

### Mechanism 2
- Claim: LiNeS mitigates catastrophic forgetting by reducing the magnitude of updates to shallow layers after fine-tuning.
- Mechanism: By scaling down shallow-layer updates in the task vector (the difference between fine-tuned and pre-trained weights), LiNeS prevents distortion of pre-trained general features while maintaining task-specific knowledge in deeper layers.
- Core assumption: Updates to shallow layers during fine-tuning contribute minimally to target task performance but significantly impact generalization on other tasks.
- Evidence anchors:
  - [abstract] "LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations."
  - [section] "We propose LiNeS, Layer-increasing Network Scaling, a post-training, plug-and-play method that directly edits the residual... by applying a scaling coefficient that linearly increases with layer depth. This scaling effectively preserves the general features captured in the shallow layers of the pre-trained model while retaining task-specific features in the deep layers of the fine-tuned model."
  - [corpus] Weak evidence - corpus papers discuss model merging and interference but don't directly address catastrophic forgetting through layer-wise scaling.
- Break condition: If shallow-layer updates are actually critical for target task performance, or if task-specific features are distributed across all layers rather than concentrated in deeper layers.

### Mechanism 3
- Claim: LiNeS enhances multi-task model merging by reducing task interference through preservation of general features in shallow layers.
- Mechanism: When merging multiple task vectors, LiNeS scales each task vector to preserve general features in shallow layers while allowing task-specific features in deeper layers to merge more effectively, reducing negative interference between tasks.
- Core assumption: Task interference in model merging occurs partly because individual task vectors lose generalization ability after fine-tuning, and preserving shallow-layer features mitigates this interference.
- Evidence anchors:
  - [abstract] "In multi-task model merging scenarios, layer-wise scaling of merged parameters reduces negative task interference."
  - [section] "Moreover, we extend LiNeS to the multi-task model merging setting, where contributions from one task distort the general features also required by other tasks. By preserving the general features in the shallow layers, LiNeS mitigates task interference and improves multi-task performance."
  - [corpus] Weak evidence - corpus papers discuss model merging and interference but don't specifically address layer-wise scaling as a solution to task interference.
- Break condition: If task interference is primarily caused by factors other than loss of generalization in shallow layers, or if the linear scaling schedule doesn't effectively balance preservation of general features with retention of task-specific knowledge.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why fine-tuning on specific tasks degrades performance on previously learned tasks is fundamental to grasping why LiNeS is necessary.
  - Quick check question: What happens to a neural network's performance on tasks it wasn't trained on when it's fine-tuned on a new task?

- Concept: Feature hierarchy in deep networks
  - Why needed here: The assumption that shallow layers capture more general features while deeper layers capture more task-specific features is central to LiNeS's design.
  - Quick check question: According to the hierarchical feature learning theory, which layers of a neural network typically capture more abstract, generalizable features?

- Concept: Model merging and task interference
  - Why needed here: LiNeS's extension to multi-task settings relies on understanding how merging fine-tuned models can lead to interference between tasks.
  - Quick check question: What is the primary challenge when merging multiple models fine-tuned on different tasks into a single multi-task model?

## Architecture Onboarding

- Component map: Pre-trained model -> Task vector extraction -> Layer-wise scaling module -> Scaled task vectors -> Recombination with pre-trained weights -> Edited model
- Critical path: The layer-wise scaling operation itself - each layer's update magnitude must be correctly computed based on its depth, then applied to the corresponding parameters in the task vector before recombination.
- Design tradeoffs: The linear scaling schedule is simple but may not be optimal for all architectures. The method trades off between preserving generalization (favoring strong scaling of shallow layers) and retaining task-specific performance (requiring less aggressive scaling). The choice of scaling function (linear vs. quadratic vs. square root) represents a key design decision.
- Failure signatures: Poor performance on target tasks suggests over-scaling of shallow layers; poor generalization on control tasks suggests under-scaling of shallow layers; inconsistent improvements across different architectures may indicate the linear scaling assumption is too restrictive.
- First 3 experiments:
  1. Apply LiNeS to a single fine-tuned model and measure performance degradation on control tasks vs. original fine-tuned model.
  2. Test different scaling functions (linear, quadratic, square root) on the same model to determine optimal scaling schedule.
  3. Apply LiNeS to a multi-task merging baseline and compare performance against the baseline without LiNeS.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical scope limited to vision and a few NLP tasks, effectiveness on more diverse domains untested
- Architecture dependency concerns - the shallow-layer generalization assumption may not hold uniformly across all network types
- Linear scaling approach simplicity may not be optimal compared to adaptive or non-linear alternatives
- Computational overhead for very large models could be prohibitive despite being described as efficient

## Confidence

- **High Confidence**: The core mechanism of layer-wise scaling preserving generalization in shallow layers while retaining task-specific knowledge in deeper layers is well-supported by controlled experiments.

- **Medium Confidence**: The extension to multi-task model merging shows consistent improvements, but the magnitude of gains varies significantly across different task combinations and architectures.

- **Medium Confidence**: The out-of-distribution generalization improvements when combined with WiSE-FT are demonstrated but limited to specific vision benchmarks.

## Next Checks

1. **Cross-architecture Validation**: Test LiNeS on diverse architectures (e.g., Vision Transformers, RNNs, and different CNN variants) to verify the universality of the shallow-layer generalization assumption.

2. **Scaling Function Exploration**: Systematically compare linear scaling against quadratic, square root, and learned scaling functions to determine if the current choice is optimal or merely sufficient.

3. **Extreme Catastrophic Forgetting Scenarios**: Design experiments where models are fine-tuned on highly dissimilar tasks (e.g., ImageNet to medical imaging) to stress-test LiNeS's ability to preserve generalization under severe forgetting pressure.