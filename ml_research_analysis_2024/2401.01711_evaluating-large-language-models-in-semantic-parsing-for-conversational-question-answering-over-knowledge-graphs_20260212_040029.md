---
ver: rpa2
title: Evaluating Large Language Models in Semantic Parsing for Conversational Question
  Answering over Knowledge Graphs
arxiv_id: '2401.01711'
source_url: https://arxiv.org/abs/2401.01711
tags:
- queries
- question
- language
- parsing
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of large language models (LLMs)
  for semantic parsing in conversational question answering over knowledge graphs.
  Experiments on the SPICE benchmark dataset show that fine-tuned LoRA-7B models achieve
  the highest performance, with accuracy scores up to 97% for simple questions.
---

# Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs

## Quick Facts
- arXiv ID: 2401.01711
- Source URL: https://arxiv.org/abs/2401.01711
- Reference count: 6
- Key outcome: Fine-tuned LoRA-7B models achieve up to 97% accuracy on simple questions, with best model scoring 0.724 weighted average overall

## Executive Summary
This study investigates the effectiveness of large language models (LLMs) for semantic parsing in conversational question answering over knowledge graphs. The researchers evaluate multiple approaches including zero-shot prompting, few-shot prompting, and fine-tuning with LoRA adapters across different model sizes. Their experiments on the SPICE benchmark dataset demonstrate that fine-tuned LoRA-7B models significantly outperform prompting-based approaches, particularly for complex multi-turn questions requiring reasoning over multiple graph triples.

## Method Summary
The researchers employed a comprehensive experimental framework to evaluate LLM performance on semantic parsing for conversational question answering. They tested multiple models including LLaMA-2 variants (7B, 13B, 70B parameters) using zero-shot prompting, few-shot prompting with 10-100 examples, and fine-tuning with LoRA adapters of different ranks. The SPICE benchmark dataset served as the evaluation platform, with questions categorized by complexity from simple single-triple queries to complex multi-turn questions requiring reasoning. The team used the EMQAQA framework to transform SPARQL queries into executable formats and measured performance using accuracy metrics.

## Key Results
- Fine-tuned LoRA-7B models achieved highest performance with accuracy scores up to 97% for simple questions
- Few-shot prompting significantly improved zero-shot performance, especially for smaller models
- The best model (LoRA-7B-512) achieved a weighted average score of 0.724, outperforming baselines but falling short of top performers
- Performance degraded substantially for complex queries requiring multi-turn reasoning (30% accuracy)

## Why This Works (Mechanism)
The study demonstrates that semantic parsing for conversational question answering benefits from domain-specific fine-tuning rather than relying solely on prompting strategies. The LoRA fine-tuning approach effectively adapts pre-trained LLMs to the specific task of translating natural language questions into executable SPARQL queries while maintaining the model's ability to handle conversational context. The results suggest that smaller models with appropriate fine-tuning can outperform larger models using prompting approaches, indicating that task-specific adaptation is more valuable than model scale alone for this particular application.

## Foundational Learning
- **Semantic parsing**: Converting natural language to formal representations (why needed: core task of translating questions to SPARQL; quick check: can model handle both simple and complex query structures)
- **Conversational context handling**: Maintaining dialogue state across turns (why needed: essential for multi-turn question answering; quick check: can model resolve coreferences across conversation turns)
- **Knowledge graph querying**: Translating natural language to SPARQL (why needed: enables execution of parsed queries; quick check: can generated queries execute successfully on target KG)
- **Few-shot learning**: Using limited examples to guide model behavior (why needed: enables adaptation without full fine-tuning; quick check: does performance improve with 10-100 examples)
- **LoRA fine-tuning**: Parameter-efficient model adaptation (why needed: enables effective fine-tuning without full parameter updates; quick check: does LoRA rank affect performance)

## Architecture Onboarding

**Component Map**
Natural Language Questions -> LLM (Prompting/Fine-tuning) -> SPARQL Queries -> Knowledge Graph -> Answers

**Critical Path**
Question input → Context handling → Semantic parsing → SPARQL generation → Query execution → Answer retrieval

**Design Tradeoffs**
- Model size vs. computational efficiency: Smaller models with fine-tuning vs. larger models with prompting
- Fine-tuning depth vs. generalization: LoRA provides parameter-efficient adaptation
- Prompt engineering vs. model adaptation: Few-shot prompting vs. full fine-tuning approaches

**Failure Signatures**
- Context handling failures: Incorrect resolution of coreferences across conversation turns
- SPARQL generation errors: Invalid or non-executable queries
- Reasoning breakdowns: Inability to handle multi-triple logical operations
- Overfitting to training distribution: Poor generalization to unseen question types

**First Experiments**
1. Evaluate model performance on progressively complex question types from simple to multi-turn
2. Compare few-shot prompting effectiveness across different model sizes (7B, 13B, 70B)
3. Test LoRA fine-tuning with different rank values to identify optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Performance significantly drops for complex multi-turn questions requiring reasoning over multiple triples
- Evaluation limited to SPICE benchmark dataset, potentially missing real-world query diversity
- Limited ablation studies on hyperparameters and architectural choices beyond tested configurations

## Confidence
- High Confidence: LoRA fine-tuning outperforms zero-shot and few-shot prompting approaches
- Medium Confidence: LoRA-7B models achieve highest performance across tested configurations
- Low Confidence: Conversational QA over knowledge graphs remains challenging without extensive analysis of specific failure patterns

## Next Checks
1. Cross-dataset validation: Test best-performing models on additional conversational QA datasets to assess generalization
2. Ablation studies: Conduct comprehensive experiments varying key hyperparameters (learning rates, LoRA rank, prompt strategies)
3. Error analysis: Perform detailed analysis of failed queries to identify specific failure patterns, particularly for complex multi-turn questions