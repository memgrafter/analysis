---
ver: rpa2
title: Graph Transformers without Positional Encodings
arxiv_id: '2401.17791'
source_url: https://arxiv.org/abs/2401.17791
tags:
- graph
- attention
- learning
- https
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Eigenformer, a Graph Transformer architecture
  that eliminates the need for positional encodings by incorporating graph structural
  information directly into the attention mechanism. The core innovation is a spectrum-aware
  attention (SAA) mechanism that factorizes attention weights into fixed node-pair
  potentials derived from the graph Laplacian spectrum and learned frequency importances.
---

# Graph Transformers without Positional Encodings

## Quick Facts
- arXiv ID: 2401.17791
- Source URL: https://arxiv.org/abs/2401.17791
- Authors: Ayush Garg
- Reference count: 40
- Primary result: Graph Transformer that eliminates positional encodings by incorporating Laplacian spectrum directly into attention mechanism

## Executive Summary
This paper introduces Eigenformer, a Graph Transformer architecture that eliminates the need for positional encodings by directly incorporating graph structural information through a spectrum-aware attention mechanism. The key innovation is factorizing attention weights into fixed node-pair potentials derived from the graph Laplacian spectrum and learned frequency importances. The approach is theoretically grounded with proofs showing it can express various graph structural connectivity matrices while being invariant to eigenvector sign/basis ambiguities. Empirical evaluation demonstrates competitive performance against state-of-the-art Graph Transformers and GNNs on five benchmarks, with the best results achieved when combining spectral and feature attention.

## Method Summary
Eigenformer implements a spectrum-aware attention (SAA) mechanism that replaces traditional positional encodings with Laplacian spectrum-based node-pair potentials. For each node pair, the attention weight is computed using fixed potentials σk[i,j] = uk[i] · uk[j] (eigenvector products) weighted by learnable functions ϕ1 and ϕ2 of the Laplacian eigenvalues. These functions are implemented as MLPs that learn frequency importances from data. The attention mechanism can capture shortest path distances, k-hop neighborhood aggregations, and is invariant to eigenvector sign/basis ambiguities. The architecture optionally combines spectral attention with standard feature attention, followed by message passing, degree scaling, batch normalization, and a feed-forward network. Training uses AdamW optimizer with cosine annealing learning rate schedule.

## Key Results
- Eigenformer achieves competitive performance against state-of-the-art Graph Transformers and GNNs on five benchmarks from Dwivedi et al. [2020]
- Best performance achieved when combining spectral and feature attention, though spectral-only attention remains competitive
- Strong performance maintained on two long-range graph benchmarks from Dwivedi et al. [2022b]
- Attention visualization confirms ability to capture k-hop neighborhood interactions
- Performance remains strong even when using only ~40% of eigenvalues, trading off computational complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spectrum-aware attention replaces positional encodings by factorizing attention weights into fixed node-pair potentials and learned frequency importances
- Mechanism: Attention weight α[i,j] computed as softmax over sum of spectral potentials σk[i,j] weighted by learnable functions of Laplacian eigenvalues
- Core assumption: Laplacian spectrum contains sufficient information to capture graph topology for representation learning
- Evidence anchors: [abstract], [section 3.1], corpus has papers on "Positional Encodings"
- Break condition: If Laplacian spectrum fails to distinguish important graph substructures or node feature interactions require more complex positional information

### Mechanism 2
- Claim: Attention mechanism can express various graph structural connectivity matrices including shortest path distances and k-hop neighborhoods
- Mechanism: Functions ϕ1 and ϕ2 can be chosen such that attention matrix approximates polynomials of normalized adjacency matrix Anorm, corresponding to k-hop aggregations
- Core assumption: Continuous functions ϕ1 and ϕ2 are sufficiently expressive (MLPs) to map spectral information to structural connectivity patterns
- Evidence anchors: [section 3.1], [abstract], corpus has papers on "Principled Graph Transformers"
- Break condition: If required functions ϕ1 and ϕ2 cannot be learned for specific graph families or approximation error becomes too large

### Mechanism 3
- Claim: Attention mechanism is invariant to eigenvector sign and basis ambiguities
- Mechanism: Uses eigenvector products σk[i,j] = uk[i] · uk[j] and groups eigenvectors with same eigenvalue together as h(UkU T k), unchanged under sign flips or basis changes
- Core assumption: Learning task doesn't require distinguishing between sign-flipped or basis-transformed graph structures
- Evidence anchors: [section 3.1], [abstract], corpus has papers on "Comparing Graph Transformers via Positional Encodings"
- Break condition: If task requires distinguishing sign-flipped/basis-transformed structures or invariance prevents capturing important differences

## Foundational Learning

- Concept: Graph Laplacian and its spectrum
  - Why needed here: Entire SAA mechanism relies on Laplacian eigenvalues and eigenvectors to encode graph structure
  - Quick check question: What does the smallest non-zero eigenvalue of normalized Laplacian represent in terms of graph connectivity?

- Concept: Spectral graph convolution and Fourier transform on graphs
  - Why needed here: Motivation for SAA comes from connecting graph convolution in spectral domain to attention mechanisms
  - Quick check question: How does polynomial filter p(L) in spatial domain relate to filter in spectral domain?

- Concept: Universal approximation theorem for MLPs
  - Why needed here: Proofs rely on fact that functions ϕ1 and ϕ2 can be approximated by MLPs to arbitrary accuracy
  - Quick check question: What conditions allow two-layer MLP to approximate any continuous function on compact set?

## Architecture Onboarding

- Component map: Node features → SAA computation (σk potentials + MLP for ϕ1, ϕ2) → Attention weights → Message aggregation → Post-processing → Next layer

- Critical path: Node features → SAA computation (σk potentials + MLP for ϕ1, ϕ2) → Attention weights → Message aggregation → Post-processing → Next layer

- Design tradeoffs:
  - Computational complexity: O(N^3) for full SAA vs O(N^2k) when using subset of eigenvalues
  - Expressiveness: SAA can capture arbitrary k-hop neighborhoods vs bounded neighborhoods in traditional GNNs
  - Memory usage: High memory requirement for storing full attention matrix and spectral components
  - Performance: Better on graphs where spectral information is rich vs datasets where local features dominate

- Failure signatures:
  - Poor performance on datasets where node features contain most discriminative information
  - Memory errors on large graphs due to O(N^2) attention matrix storage
  - Slow training due to O(N^3) complexity, especially with full eigenvalue set
  - Underfitting if MLP for ϕ1, ϕ2 is too small to capture frequency patterns

- First 3 experiments:
  1. Implement basic SAA layer on small graph dataset (ZINC) with only spectral attention, verify ability to distinguish adjacent vs non-adjacent nodes by visualizing σk distributions
  2. Test effect of different numbers of eigenvalues (k) on performance and training time, confirm graceful degradation mentioned in Table 3
  3. Compare performance with and without feature attention on directed graph datasets (MNIST, CIFAR10) to verify observation that feature attention helps more on directed graphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Eigenformer's performance scale with increasing graph sizes, particularly for graphs larger than those tested?
- Basis in paper: [inferred] Paper mentions evaluating on specific benchmarks but doesn't explore performance on very large graphs
- Why unresolved: Experiments limited to specific benchmark datasets, scalability for larger graphs unexplored
- What evidence would resolve it: Experiments on datasets with significantly larger graphs like OGB large-scale datasets

### Open Question 2
- Question: What is impact of using subset of Laplacian spectrum on Eigenformer's performance across different graph tasks?
- Basis in paper: [explicit] Paper discusses trade-off between complexity and performance when using subset of spectrum
- Why unresolved: Provides some insights on ZINC dataset but doesn't generalize findings to other tasks
- What evidence would resolve it: Systematic experiments across different graph tasks using varying numbers of eigenvalues

### Open Question 3
- Question: How does inclusion of edge feature propagation affect Eigenformer's performance and when might it be beneficial?
- Basis in paper: [explicit] Paper mentions edge-feature propagation excluded due to complexity and marginal benefits
- Why unresolved: Decision based on initial observations, comprehensive analysis of impact lacking
- What evidence would resolve it: Experiments comparing Eigenformer with and without edge-feature propagation across diverse datasets and tasks

## Limitations

- Spectral Expressiveness: Assumption that Laplacian spectrum alone can capture sufficient graph structural information may not hold for graphs with complex local structures or heterophily
- Empirical Coverage: Evaluation focuses on specific benchmark datasets which may not represent full diversity of real-world graph learning problems
- Complexity Trade-off: O(N^3) complexity for full SAA makes it challenging to apply to large-scale graphs, though performance remains strong with ~40% of eigenvalues

## Confidence

**High Confidence**: Core mathematical framework and theoretical proofs regarding expressiveness and invariance properties are well-established and clearly presented

**Medium Confidence**: Empirical results showing competitive performance on benchmark datasets, though evaluation scope is limited to specific datasets

**Low Confidence**: Claim that Laplacian spectrum alone is sufficient to replace positional encodings for all graph learning tasks - requires broader empirical validation

## Next Checks

1. **Failure Case Analysis**: Systematically test Eigenformer on datasets with heterophily or complex local structures where spectral information alone might be insufficient, comparing against Graph Transformers with learned positional encodings

2. **Complexity Reduction Study**: Implement and evaluate approximation techniques beyond eigenvalue truncation (Nyström method, random Fourier features) to reduce computational complexity while maintaining performance, measuring trade-off between approximation quality and computational savings

3. **Cross-dataset Generalization**: Train Eigenformer on one dataset family (e.g., molecular graphs) and test on another (e.g., social networks or image graphs) to assess how well learned spectral patterns generalize across different graph domains and whether model adapts frequency importance functions appropriately