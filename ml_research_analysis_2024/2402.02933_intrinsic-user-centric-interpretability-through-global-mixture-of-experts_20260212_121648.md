---
ver: rpa2
title: Intrinsic User-Centric Interpretability through Global Mixture of Experts
arxiv_id: '2402.02933'
source_url: https://arxiv.org/abs/2402.02933
tags:
- features
- feature
- interpretcc
- each
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InterpretCC introduces a family of intrinsically interpretable
  neural networks that combine accurate predictions with human-centric explanations.
  The method uses conditional computation to adaptively and sparsely activate relevant
  features or feature groups before prediction, enabling minimal and interpretable
  feature use per instance.
---

# Intrinsic User-Centric Interpretability through Global Mixture of Experts

## Quick Facts
- **arXiv ID**: 2402.02933
- **Source URL**: https://arxiv.org/abs/2402.02933
- **Reference count**: 40
- **Primary result**: Interpretable neural networks match or exceed black-box baselines while providing human-centric explanations via sparse, adaptive feature activation.

## Executive Summary
InterpretCC introduces a family of intrinsically interpretable neural networks that combine accurate predictions with human-centric explanations. The method uses conditional computation to adaptively and sparsely activate relevant features or feature groups before prediction, enabling minimal and interpretable feature use per instance. Two architectures are presented: Feature Gating, which selects individual features, and Group Routing, a global mixture-of-experts model that routes features into human-defined topical subnetworks. Experiments on eight datasets across text, time series, and tabular modalities show that InterpretCC matches or exceeds state-of-the-art black-box baselines and outperforms other intrinsically interpretable methods. A user study with 56 teachers confirms that InterpretCC explanations are perceived as more actionable, useful, and trustworthy than existing alternatives. The approach achieves sparse, faithful, and human-centered interpretability without sacrificing predictive performance.

## Method Summary
InterpretCC uses conditional computation to adaptively select and activate a minimal set of features or feature groups per instance. The Feature Gating architecture applies a Gumbel Softmax-based discriminator to produce a sparse mask over individual features, which is then applied before prediction. The Group Routing architecture extends this to a global mixture-of-experts model, where features are statically assigned to human-defined topical groups, and a gating network outputs sparse weights for each group. The final prediction is a weighted sum over the activated experts. Both architectures use L1 regularization to enforce sparsity, ensuring concise, interpretable explanations. The method is evaluated on multi-modal classification tasks, showing strong performance and high user preference in interpretability studies.

## Key Results
- InterpretCC matches or exceeds state-of-the-art black-box baselines and outperforms other intrinsically interpretable methods across eight datasets.
- User study with 56 teachers shows InterpretCC explanations are perceived as more actionable, useful, and trustworthy than alternatives.
- Group Routing with human-defined groupings yields sparser explanations and better interpretability than Feature Gating, while maintaining competitive accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adaptive sparse activation per instance enables both high performance and human-friendly explanations.
- **Mechanism**: The discriminator network uses Gumbel Softmax sampling to produce a sparse mask over features or groups, activating only the subset most relevant to the current prediction. This per-instance routing ensures minimal, focused explanations while retaining full access to all parameters during training.
- **Core assumption**: Sparse masks preserve predictive signal without losing important cross-feature interactions.
- **Evidence anchors**:
  - [abstract] "InterpretCC achieves this through adaptive sparse activation of features before prediction, allowing the model to use a different, minimal set of features for each instance."
  - [section 3.1] "The Gumbel Softmax trick (Jang et al., 2017) is applied on each dimension of D(x) to select features in a differentiable way."
- **Break condition**: If the underlying feature space is uniformly important across instances, sparsity will discard useful information and degrade performance.

### Mechanism 2
- **Claim**: Global mixture-of-experts routing with human-defined groups preserves interpretability while matching or exceeding black-box performance.
- **Mechanism**: Features are statically assigned to topical subnetworks; a gating network outputs sparse weights for each group, and the final prediction is a weighted sum over the activated experts. This structure mirrors ensemble learning but with human-readable group concepts.
- **Core assumption**: Human-specified groupings capture meaningful signal and allow cross-feature interactions within each group.
- **Evidence anchors**:
  - [abstract] "extend this idea into an interpretable, global mixture-of-experts (MoE) model that allows users to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks for prediction."
  - [section 3.2] "the model output is a weighted sum of the output of each expert fi that only uses the features from the i-th group as input."
- **Break condition**: If users define meaningless or redundant groups, routing will not improve and may harm performance.

### Mechanism 3
- **Claim**: Using one group per feature (no duplication) guarantees faithful, non-overlapping explanations.
- **Mechanism**: Each feature is assigned to exactly one group, so when that group is activated, its contribution is fully accounted for in the prediction. This avoids double-counting and ensures the explanation matches the actual computation.
- **Core assumption**: Non-overlapping feature-to-group mapping yields clear attribution without hidden dependencies.
- **Evidence anchors**:
  - [abstract] "each feature routed to only one group" (in Group Routing description).
  - [section 2] "ICC differs by filtering the feature space instead of using all features (sparsity), using user-defined concepts instead of automated concepts (basis), and assigning each feature to a single group, making feature use explicit (predictive stage)."
- **Break condition**: If features have strong cross-group interactions, single assignment may force misleading simplifications.

## Foundational Learning

- **Gumbel Softmax sampling**
  - Why needed here: Enables differentiable sampling from discrete categorical masks so masks can be learned end-to-end.
  - Quick check question: What happens if τ (temperature) is set too low during training?

- **Mixture-of-experts routing**
  - Why needed here: Allows specialized subnetworks to handle distinct feature groups, improving both interpretability and performance.
  - Quick check question: How does the gating network decide which experts to activate for a given input?

- **L1 regularization for sparsity**
  - Why needed here: Enforces minimal feature activation per instance, directly improving explanation conciseness.
  - Quick check question: What is the trade-off between L1 penalty strength and prediction accuracy?

## Architecture Onboarding

- **Component map**: Input features → Discriminator (or group router) → Gumbel Softmax mask → Masked features/groups → Predictive subnetwork(s) → Outputs aggregated (weighted sum) → Final prediction

- **Critical path**:
  1. Input features → Discriminator (or group router) → Gumbel Softmax mask
  2. Masked features/groups → Predictive subnetwork(s)
  3. Outputs aggregated (weighted sum) → Final prediction

- **Design tradeoffs**:
  - FG: Simpler, no human effort, but may activate many features if all are important.
  - GR: Requires feature grouping, but can capture cross-feature interactions and yield sparser explanations.
  - Gumbel temperature: Low → sparse but risky; High → dense, stable training.

- **Failure signatures**:
  - Performance drops: Likely mask too sparse or groups miss important interactions.
  - Explanation overly long: L1 penalty too weak or temperature too high.
  - Training instability: Temperature too low early on; consider annealing.

- **First 3 experiments**:
  1. Train ICC FG on a small tabular dataset with L1 penalty tuned for ~10% feature activation; measure accuracy vs. baseline.
  2. Create simple two-group routing (e.g., numeric vs. categorical features) and compare FG vs. GR performance.
  3. Vary Gumbel temperature from 0.5 to 5.0; plot sparsity vs. accuracy to find stable operating point.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of feature grouping strategy (paper, pattern, GPT-4) impact model performance across different datasets and domains?
- **Basis in paper**: Explicit - discussed in Section 5.1 and Appendix H.3.2
- **Why unresolved**: While the paper shows that different grouping strategies affect performance, the specific impact on different datasets and domains is not fully explored. The paper mentions that automated LLM grouping and pattern-based human-defined grouping perform comparably on average, but doesn't provide a detailed analysis of their performance on individual datasets.
- **What evidence would resolve it**: A comprehensive study comparing the performance of different grouping strategies across all datasets and domains used in the paper, including detailed analysis of their strengths and weaknesses on specific types of data.

### Open Question 2
- **Question**: Can InterpretCC be extended to handle cross-feature or raw modality interactions effectively?
- **Basis in paper**: Inferred - discussed in Appendix I.5 as a potential extension
- **Why unresolved**: The paper suggests that graph-based models could be used in the discriminator stage for raw time series data, but doesn't provide concrete implementation details or experimental results. It's unclear how effective this extension would be in practice and how it would impact the interpretability of the model.
- **What evidence would resolve it**: Implementation and experimental validation of the proposed graph-based extension for raw time series data, including a comparison of its performance and interpretability with the standard InterpretCC architecture.

### Open Question 3
- **Question**: How does the interpretability of InterpretCC compare to other intrinsically interpretable methods in terms of human understandability and actionability?
- **Basis in paper**: Explicit - discussed in Section 5.3 through a user study
- **Why unresolved**: While the user study shows that InterpretCC is preferred over other intrinsically interpretable methods in terms of usefulness, trustworthiness, actionability, and conciseness, the study is limited in scope and doesn't provide a comprehensive comparison of the interpretability of different methods. It's unclear how InterpretCC's interpretability compares to other methods in terms of depth of understanding and ability to guide actionable decisions.
- **What evidence would resolve it**: A more extensive user study comparing InterpretCC to other intrinsically interpretable methods across multiple tasks and domains, including detailed analysis of the depth of understanding and actionability provided by each method's explanations.

## Limitations

- The specific logic for human-defined feature groupings in Group Routing (especially for text and time series modalities) is underspecified, which could affect reproducibility.
- The exact preprocessing thresholds and post-processing steps used in the user study are not fully detailed, potentially impacting the reported interpretability outcomes.
- While performance claims are strong, the lack of cross-dataset consistency metrics (e.g., standard deviations across runs) makes it difficult to assess robustness.

## Confidence

- **High Confidence**: The mechanism of using Gumbel Softmax for differentiable sparse feature selection is well-established and directly supported by cited work.
- **Medium Confidence**: The performance comparison against baselines is plausible given the architectural design, but lacks variance reporting across multiple runs.
- **Low Confidence**: The user study results are compelling but depend heavily on subjective human judgments and the specific context of the teaching scenario.

## Next Checks

1. **Sensitivity Analysis**: Systematically vary the Gumbel temperature and L1 penalty across datasets to identify stable parameter ranges and confirm the robustness of sparsity-accuracy trade-offs.
2. **Feature Grouping Validation**: Conduct ablation studies where features are assigned to random groups versus semantically meaningful groups to test whether interpretability gains depend on human-defined structure.
3. **User Study Replication**: Replicate the user study with a different cohort and simpler, domain-agnostic explanation formats to verify that the reported actionability and trustworthiness are not context-specific artifacts.