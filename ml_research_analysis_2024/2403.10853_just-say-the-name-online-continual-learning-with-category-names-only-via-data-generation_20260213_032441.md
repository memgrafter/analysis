---
ver: rpa2
title: 'Just Say the Name: Online Continual Learning with Category Names Only via
  Data Generation'
arxiv_id: '2403.10853'
source_url: https://arxiv.org/abs/2403.10853
tags:
- data
- images
- arxiv
- prompt
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of continual learning (CL) in
  scenarios where only category names are provided without any data samples. Existing
  CL methods rely on human supervision, which is costly and time-consuming, especially
  in real-time scenarios.
---

# Just Say the Name: Online Continual Learning with Category Names Only via Data Generation

## Quick Facts
- arXiv ID: 2403.10853
- Source URL: https://arxiv.org/abs/2403.10853
- Reference count: 40
- Outperforms supervised models by 9% (AAUC) on PACS OOD domain

## Executive Summary
This paper addresses the challenge of continual learning when only category names are provided without any data samples. The authors propose Generative name-only Continual Learning (GenCL), a framework that uses text-to-image generative models to create training data from category names. By employing hierarchical recurrent prompt generation (HIRPG) and complexity-guided data ensemble (CONAN), GenCL achieves significant performance improvements over prior arts, including models trained with fully supervised data, on class-incremental and multi-modal visual concept-incremental learning tasks.

## Method Summary
GenCL is a framework for online continual learning that generates training data from category names using text-to-image generative models. It employs HIRPG to create diverse text prompts by iteratively providing previously generated prompts as negative examples to an LLM. CONAN then selects a coreset of images with minimal overlap from multiple generative models based on their complexity using relative Mahalanobis distance. The continual learner is updated with streamed data and episodic replay, storing generated samples to mitigate forgetting without privacy concerns.

## Key Results
- GenCL achieves 9% and 13% improvements in AAUC on the PACS OOD domain compared to models trained with web-scraped and manually annotated data, respectively
- Outperforms prior arts on class-incremental learning (CIL) and multi-modal visual concept-incremental learning (MVCIL) tasks using benchmarks like PACS, DomainNet, Bongard-HOI, and Bongard-OpenWorld
- Generated data available at the provided link

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt diversity via negative example augmentation drives image variety
- Mechanism: HIRPG generates diverse text prompts using an LLM by iteratively providing previously generated prompts as negative examples, reducing semantic overlap and forcing exploration of the prompt space
- Core assumption: LLMs respond to negative constraints and maintain semantic coherence under recursive conditioning
- Evidence anchors: [abstract], [section 4.1], [corpus] Weak
- Break condition: LLM stops generating semantically distinct prompts; negative examples fail to reduce overlap

### Mechanism 2
- Claim: Complexity-guided ensemble sampling improves both diversity and class-representativeness
- Mechanism: CONAN uses relative Mahalanobis distance (RMD) to rank samples by their distance from both class prototypes and global prototypes, selecting high RMD samples (near class boundaries) for diversity while including class-representative samples through probabilistic sampling
- Core assumption: High RMD scores correspond to samples that increase diversity while maintaining class relevance
- Evidence anchors: [abstract], [section 4.3], [corpus] Weak
- Break condition: RMD fails to correlate with true sample difficulty; probabilistic sampling collapses to uniform selection

### Mechanism 3
- Claim: Episodic memory replay mitigates forgetting without compromising privacy
- Mechanism: GenCL stores previously generated samples in episodic memory for replay during continual learning, avoiding real-data storage privacy issues while maintaining performance on earlier concepts
- Core assumption: Generated samples retain sufficient fidelity for replay; privacy benefits of generated data over real data
- Evidence anchors: [abstract], [section 4], [corpus] Weak
- Break condition: Generated samples degrade too much for effective replay; memory storage becomes a bottleneck

## Foundational Learning

- Concept: Text-to-image (T2I) generative models and prompt engineering
  - Why needed here: Core of GenCL relies on generating concept-specific images from text prompts; understanding how prompts control output diversity is essential
  - Quick check question: How does adding a background descriptor to a prompt affect the diversity and realism of generated images?

- Concept: Continual learning setups (class-incremental, task-incremental)
  - Why needed here: GenCL operates in name-only continual learning; knowing how CIL differs from standard supervised learning is critical for architecture design
  - Quick check question: What is the primary challenge of class-incremental learning that episodic memory replay aims to solve?

- Concept: Mahalanobis distance and its use in sample selection
  - Why needed here: CONAN uses RMD for complexity-based sampling; understanding Mahalanobis distance is necessary to grasp how samples are ranked
  - Quick check question: Why is Mahalanobis distance preferred over Euclidean distance when measuring sample complexity in feature space?

## Architecture Onboarding

- Component map:
  - Prompt Generation Module (ψ): Hierarchical RNN-based prompt diversification
  - Generator Set (G): Multiple T2I models producing concept-specific images
  - Ensembler (∆): RMD-based probabilistic coreset selection
  - Continual Learner (fθ): Online model updated with streamed data + episodic replay
  - Episodic Memory (M): Stores generated samples for replay

- Critical path: Concept arrival → Prompt generation → Image generation → Coreset selection → Model update → Memory update

- Design tradeoffs:
  - More generators → higher diversity but higher compute cost
  - Deeper HIRPG hierarchy → more diversity but longer context and potential lost-in-the-middle
  - Larger coreset → better performance but slower adaptation
  - Larger episodic memory → better forgetting mitigation but higher storage cost

- Failure signatures:
  - Low RMD variance → poor diversity in ensemble
  - High overlap between prompts → redundant image generation
  - Memory overflow → degraded replay effectiveness
  - Model collapse → poor concept generalization

- First 3 experiments:
  1. Test HIRPG vs baseline prompt generation on diversity metrics with a fixed generator
  2. Validate CONAN coreset selection by comparing AAUC with and without ensembling on a simple dataset
  3. Integrate all components in a small-scale class-incremental task and measure forgetting via episodic replay ablation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality and diversity of images generated by GenCL compare to those obtained through web-scraping, particularly in terms of realism and relevance to the given concepts?
- Basis in paper: [explicit] The paper discusses the limitations of web-scraped data, including noise and privacy concerns, and claims that GenCL generates more diverse and controllable images
- Why unresolved: The paper provides qualitative comparisons and metrics like Recognizability and Diversity, but a comprehensive quantitative comparison of image quality and relevance between GenCL-generated and web-scraped images is lacking
- What evidence would resolve it: A study comparing image quality metrics (e.g., FID, IS) and human evaluations of relevance and realism between GenCL-generated and web-scraped images for the same concepts

### Open Question 2
- Question: What is the impact of using different text-to-image generative models within GenCL on the overall performance and diversity of the generated images?
- Basis in paper: [explicit] The paper mentions using multiple T2I models (SDXL, DeepFloyd IF, SD3, CogView2, and Auraflow3) but does not provide a detailed analysis of their individual contributions
- Why unresolved: The paper does not analyze the performance of GenCL when using different combinations or subsets of T2I models, nor does it compare the generated images from each model in terms of quality and diversity
- What evidence would resolve it: An ablation study comparing the performance of GenCL using different T2I models individually and in various combinations, along with a qualitative and quantitative analysis of the generated images

### Open Question 3
- Question: How does the proposed hierarchical recurrent prompt generation method (HIRPG) compare to other prompt generation techniques in terms of prompt diversity and image quality?
- Basis in paper: [explicit] The paper introduces HIRPG and claims it outperforms existing prompt generation methods in terms of image diversity and recognizability
- Why unresolved: While the paper provides comparisons with other prompt generation methods, it does not delve into the specific aspects of prompt diversity and how it affects image quality
- What evidence would resolve it: A detailed analysis of the diversity of generated prompts using metrics like semantic diversity and prompt novelty, along with a comparison of image quality metrics for images generated using different prompt generation methods

### Open Question 4
- Question: How does the complexity-guided data ensemble method (CONAN) perform compared to other data selection techniques in terms of computational efficiency and sample representativeness?
- Basis in paper: [explicit] The paper introduces CONAN and claims it outperforms other data ensemble methods in terms of efficiency and diversity
- Why unresolved: The paper provides a comparison with other data ensemble methods but does not analyze the computational efficiency of CONAN in detail or explore its impact on the representativeness of the selected samples
- What evidence would resolve it: A comparison of the computational time and memory requirements of CONAN with other data ensemble methods, along with an analysis of the representativeness of the selected samples using metrics like coverage and diversity

## Limitations
- Generalizability of HIRPG's negative prompt conditioning beyond evaluated datasets is uncertain
- Scalability of CONAN's complexity-guided ensemble as the number of concepts grows is unclear
- Privacy claims regarding episodic memory rely on implicit assumptions without empirical validation

## Confidence
- HIRPG mechanism: Medium - strong theoretical framing but limited ablation on prompt diversity impact
- CONAN effectiveness: Medium - RMD-based selection is well-motivated but no comparison to simpler diversity metrics
- Privacy benefits: Low - claimed but not empirically tested against real-data storage risks

## Next Checks
1. Ablation study isolating HIRPG's prompt diversity contribution by comparing with random prompt generation while keeping CONAN fixed
2. Scalability test measuring AAUC degradation as concept stream length increases beyond PACS/OOD scale
3. Privacy audit comparing information leakage between episodic memory of generated vs real samples using membership inference attacks