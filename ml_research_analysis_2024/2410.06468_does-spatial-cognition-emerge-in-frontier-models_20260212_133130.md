---
ver: rpa2
title: Does Spatial Cognition Emerge in Frontier Models?
arxiv_id: '2410.06468'
source_url: https://arxiv.org/abs/2410.06468
tags:
- image
- spatial
- goal
- your
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We develop a benchmark for evaluating spatial cognition in frontier
  models, inspired by decades of research in cognitive science. Our benchmark evaluates
  large-scale spatial mapping abilities, smaller-scale object shape and layout reasoning,
  and cognitive infrastructure such as spatial attention and memory.
---

# Does Spatial Cognition Emerge in Frontier Models?

## Quick Facts
- arXiv ID: 2410.06468
- Source URL: https://arxiv.org/abs/2410.06468
- Reference count: 40
- Key outcome: Contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on many classic tests of animal cognition.

## Executive Summary
This paper introduces a comprehensive benchmark for evaluating spatial cognition in frontier models, drawing from decades of cognitive science research. The benchmark assesses both large-scale spatial mapping abilities (environmental navigation, distance/direction estimation) and small-scale object-level reasoning (mental rotation, perspective taking). By providing parallel text and image presentations, the benchmark allows evaluation of both large language models and multimodal models. Results demonstrate that despite advanced capabilities in other domains, current frontier models exhibit spatial cognition deficits comparable to chance performance on many classic animal cognition tests.

## Method Summary
The authors developed the SPACE (Spatial Perception And Cognition Evaluation) benchmark using zero-shot prompting of various frontier models (GPT-4o, GPT-4v, Claude 3.5 Sonnet, Llama 3, Mistral, Yi). The benchmark includes ten environment layouts for navigation tasks and ten classic cognitive tests adapted for both text and image presentations. Interactive tasks use the Habitat simulator with SPL (success weighted by path length) metrics, while multiple-choice tasks use accuracy measures. The benchmark is available at https://github.com/apple/ml-space-benchmark.

## Key Results
- Frontier models perform near chance level on many classic tests of animal cognition
- Text-only models often outperform multimodal models on spatial tasks
- Interactive tasks requiring active spatial reasoning show the largest performance gaps
- Current models struggle with both large-scale environmental navigation and small-scale object manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark works by systematically evaluating both large-scale (environmental) and small-scale (object-level) spatial cognition, mirroring how cognitive science studies animal and human spatial abilities.
- Mechanism: It builds on decades of cognitive science research to create tasks that require building mental representations of environments (large-scale) and mentally manipulating objects (small-scale). By implementing both text and image presentations, it evaluates whether models can perform these tasks without embodiment.
- Core assumption: That tasks proven effective in cognitive science for assessing spatial cognition in animals and humans will reveal similar capabilities (or lack thereof) in artificial models.
- Evidence anchors:
  - [abstract] "Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities... smaller-scale reasoning about object shapes and layouts..."
  - [section 3] "We design a suite of spatial cognition tasks based on the cognitive science literature."
  - [corpus] Weak evidence - the corpus shows related benchmarks but doesn't directly validate the specific cognitive science grounding approach.
- Break condition: If the cognitive science tasks don't translate to artificial models due to fundamental differences in how spatial information is processed, the benchmark would fail to reveal meaningful insights about spatial cognition.

### Mechanism 2
- Claim: The benchmark reveals spatial cognition deficiencies by providing multimodal and text-only presentations, allowing comparison between vision-language models and pure language models.
- Mechanism: By creating parallel text and image versions of tasks, it isolates whether poor performance is due to lack of spatial cognition or inability to process visual inputs. The text versions are often simplified (e.g., 2D arrays instead of 3D shapes), allowing identification of modality-specific limitations.
- Core assumption: That the same cognitive task can be meaningfully represented in both text and image formats, and that differences in performance between modalities reveal specific capabilities.
- Evidence anchors:
  - [abstract] "For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models."
  - [section 3.2] "We design text-based and image-based presentations to evaluate both large language-only and vision-language models."
  - [section 4] "Performance of some model classes... on purely textual presentations is considerably higher than on multimodal presentations."
- Break condition: If the text and image versions of tasks don't actually measure the same cognitive ability (e.g., simplified text versions are fundamentally easier), then performance differences won't reveal modality-specific limitations.

### Mechanism 3
- Claim: The benchmark works by including interactive tasks that require active spatial reasoning rather than passive selection, revealing deeper cognitive capabilities.
- Mechanism: Tasks like route retracing, shortcut discovery, maze completion, and Cambridge spatial working memory require sequential decision-making and memory of spatial information over time, which is more demanding than multiple-choice questions and better reveals genuine spatial reasoning abilities.
- Core assumption: That interactive tasks requiring active navigation and memory are more diagnostic of spatial cognition than passive recognition tasks.
- Evidence anchors:
  - [section 3.1] "Route retracing... We formulate this as an interactive task... measure performance using the SPL metric"
  - [section 3.2] "Cambridge spatial working memory test (CSWM). This is an interactive game that evaluates visuospatial working memory"
  - [section 4] Results show models perform near chance on interactive tasks even when they can understand inputs correctly.
- Break condition: If models can succeed at interactive tasks through pattern matching or other non-spatial strategies rather than genuine spatial reasoning, the interactive nature wouldn't provide additional diagnostic value.

## Foundational Learning

- Concept: Cognitive science methodology in spatial cognition research
  - Why needed here: Understanding the historical context and experimental designs used in cognitive science is crucial for interpreting