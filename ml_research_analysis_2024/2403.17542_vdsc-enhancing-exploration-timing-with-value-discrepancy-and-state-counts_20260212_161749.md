---
ver: rpa2
title: 'VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts'
arxiv_id: '2403.17542'
source_url: https://arxiv.org/abs/2403.17542
tags:
- exploration
- learning
- state
- agent
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the underexplored question of when to\
  \ explore in deep reinforcement learning. While existing approaches focus on how\
  \ much or how to explore, this work proposes leveraging the agent's internal state\u2014\
  specifically, value discrepancy and state counts\u2014to determine exploration timing."
---

# VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts

## Quick Facts
- arXiv ID: 2403.17542
- Source URL: https://arxiv.org/abs/2403.17542
- Reference count: 40
- Primary result: VDSC outperforms traditional and advanced exploration methods on Atari, especially in hard exploration games

## Executive Summary
This paper addresses the underexplored question of when to explore in deep reinforcement learning by leveraging the agent's internal state through Value Promise Discrepancy (VPD) and SimHash-based state novelty tracking. The proposed VDSC method combines these two triggers using a homeostasis mechanism to dynamically set exploration thresholds, achieving superior performance on the Atari suite compared to traditional methods like ϵ-greedy and Boltzmann, as well as advanced techniques like Noisy Nets. The approach demonstrates that informed exploration timing, based on value function uncertainty and state novelty, can significantly enhance learning efficiency.

## Method Summary
VDSC integrates two exploration signals: Value Promise Discrepancy (VPD), which measures the gap between expected and actual rewards over a rolling history to capture value function uncertainty, and SimHash-based state novelty tracking, which creates compressed representations of high-dimensional states to approximate visit counts. These signals are combined using a homeostasis mechanism that dynamically adapts a threshold to convert continuous trigger signals into binary exploration decisions, maintaining a target exploration rate. The method is implemented within a Rainbow agent architecture and evaluated on the Atari Learning Environment.

## Key Results
- VDSC outperforms traditional methods like ϵ-greedy and Boltzmann on Atari games
- Significant improvements observed in hard exploration games such as Frostbite and Gravitar
- Demonstrates the effectiveness of informed exploration timing based on value discrepancy and state novelty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Value Promise Discrepancy (VPD) measures the gap between expected and actual rewards over a rolling history, providing a proxy for value function uncertainty.
- Mechanism: VPD computes the difference between the value estimate from k steps ago and the cumulative discounted reward actually observed over that interval, adjusted for the current value estimate. This captures prediction error that correlates with underexplored or high-variance regions.
- Core assumption: The discrepancy is informative about learning progress and worth exploring; it is not just transient noise from early training.
- Evidence anchors:
  - [abstract] mentions "Value Promise Discrepancy (VPD), which measures uncertainty in value predictions."
  - [section 4.1] defines VPD(t - k, t) := V(st - k) - Σ(γ^i * rt - i) - γ^k * V(st).
- Break condition: If the environment has high stochasticity in returns, VPD may remain elevated without reflecting unexplored states, causing spurious exploration.

### Mechanism 2
- Claim: SimHash-based state novelty tracking creates a compressed, collision-tolerant representation of high-dimensional states to approximate visit counts.
- Mechanism: Each observation is hashed to a k-bit code using a random projection matrix; counts are maintained per hash bucket. Lower counts imply novelty and trigger exploration.
- Core assumption: The hash function preserves similarity so that nearby states share hashes, smoothing novelty detection and avoiding extreme sparsity.
- Evidence anchors:
  - [section 4.2] describes SimHash encoding ϕ(s) = sgn(A · g(s)) and counts n(ϕ(s)).
  - [section 5.1] uses k = 256 bits for Atari state compression.
- Break condition: Excessive collisions can make many distinct states appear novel, leading to over-exploration and reduced exploitation.

### Mechanism 3
- Claim: A unified homeostasis mechanism dynamically adapts a threshold over time to convert continuous trigger signals into binary exploration decisions, maintaining a target exploration rate.
- Mechanism: Each trigger's signal is standardized and exponentially transformed; the moving average of these transformed signals is compared to the target rate to produce an exploration probability. Probabilities are averaged across triggers before sampling a Bernoulli decision.
- Core assumption: Maintaining a stable target exploration rate while allowing trigger-driven modulation improves sample efficiency versus static exploration rates.
- Evidence anchors:
  - [section 4.3] defines unified homeostasis with target rate ρ and time scale τ = min(t, 5/ρ).
  - [section 5.2] sets initial exploration parameters to 1 and decays linearly to 0.01 over 1M frames, matching baseline decay schedules.
- Break condition: If target rate is set too low, homeostatic smoothing may suppress meaningful trigger signals; too high, and exploration never converges.

## Foundational Learning

- Concept: Markov Decision Processes (MDP) formulation and value functions
  - Why needed here: VDSC operates on value estimates V(s) and action-value estimates Q(s,a); understanding the Bellman equations and optimal policy derivation is necessary to interpret VPD.
  - Quick check question: In a deterministic MDP, if Q*(s,a) is known exactly for all a, what is the optimal policy at s?

- Concept: Count-based exploration and hash function properties
  - Why needed here: SimHash compression and counting relies on properties of locality-sensitive hashing; understanding how collisions trade off specificity for tractability is key to tuning κ.
  - Quick check question: If κ = 64 bits, what is the approximate probability that two random, unrelated states collide under SimHash?

- Concept: Exploration-exploitation tradeoff and dithering methods
  - Why needed here: VDSC is positioned as a more informed alternative to ϵ-greedy and Boltzmann; knowing their mechanics and failure modes contextualizes the claimed improvements.
  - Quick check question: What is the main drawback of constant-ϵ greedy exploration in terms of trajectory deviation?

## Architecture Onboarding

- Component map: State preprocessor → SimHash encoder → count table lookup → exploration bonus → homeostasis module → Bernoulli sampling → exploration decision → Rainbow agent

- Critical path:
  1. Observe state st
  2. Compute SimHash ϕ(st) and exploration bonus 1/√n(ϕ(st))
  3. Maintain rolling buffer of (st - k, rt - k, ..., rt, st) for VPD calculation
  4. Standardize and transform both signals
  5. Homeostasis averages probabilities → Bernoulli sample → explore/exploit decision
  6. Execute action, store reward/value for next step

- Design tradeoffs:
  - κ (hash bits): larger → fewer collisions, more memory; smaller → more collisions, less memory.
  - k (VPD history length): longer → smoother but slower to react; shorter → more noise.
  - Target exploration rate ρ: higher → more consistent exploration, slower convergence; lower → more exploitation, risk of premature convergence.

- Failure signatures:
  - VPD consistently high → likely high environment stochasticity, not useful novelty signal.
  - Exploration bonus near constant → hash table saturated, novelty signal saturated.
  - Homeostasis never switches → target rate too low or moving averages stagnant.

- First 3 experiments:
  1. Run VDSC on a dense-reward game (e.g., Pong) with κ = 64, k = 5; verify exploration rate matches target.
  2. On a sparse-reward game (e.g., Gravitar), compare VPD vs exploration bonus magnitudes over training.
  3. Vary κ (32, 128, 256) on the same game; plot learning curves to see collision effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the combination method of VPD and exploration bonus signals affect exploration performance, and what alternative combination strategies could yield better results?
- Basis in paper: [explicit] The authors suggest that the way signals are combined is an open question, mentioning alternatives like taking the maximum probability value instead of averaging or introducing a linear combination optimized by an external meta-controller.
- Why unresolved: The current study uses a simple averaging method for combining signals, but the authors acknowledge that other combination strategies might lead to improved exploration timing and performance.
- What evidence would resolve it: Comparative experiments testing different combination methods (e.g., max, weighted sum, learned combination) across multiple environments would reveal which approach yields the best exploration performance.

### Open Question 2
- Question: Can VDSC be effectively extended to policy gradient methods, and how would integrating VPD and exploration bonus values into the policy update mechanism impact learning efficiency?
- Basis in paper: [explicit] The authors discuss extending the exploration strategy to policy gradient methods as a promising research direction, suggesting modifications to the policy's entropy based on uncertainty and novelty signals.
- Why unresolved: The current implementation focuses on value-based methods, and it's unclear how well the VPD and exploration bonus signals would translate to policy gradient frameworks where exploration is typically governed by stochastic policy updates.
- What evidence would resolve it: Implementing VDSC in policy gradient algorithms (e.g., PPO, A2C) and comparing their performance against standard exploration methods in various environments would demonstrate the effectiveness of this extension.

### Open Question 3
- Question: How would using an ensemble of networks for measuring value prediction discrepancies, instead of a single network, impact the accuracy and effectiveness of the VPD signal?
- Basis in paper: [explicit] The authors mention that using an ensemble of networks to gauge uncertainty, as presented in [24], is a possible alternative to the single-network approach used in VDSC.
- Why unresolved: The current implementation relies on a single network's value predictions to calculate VPD, but ensembles might provide a more robust measure of uncertainty by capturing model disagreement.
- What evidence would resolve it: Experiments comparing VDSC with single-network VPD against an ensemble-based version across different environments would reveal whether the ensemble approach leads to better exploration timing and overall performance.

### Open Question 4
- Question: Would a learned domain-specific hash function improve state encoding and exploration efficiency compared to the SimHash method used in VDSC?
- Basis in paper: [explicit] The authors suggest that using a learned domain-specific hash function, as shown in [30], might lead to further improvements in encoding relevant state information.
- Why unresolved: SimHash provides a generic hashing mechanism, but a learned hash function could potentially capture more meaningful state similarities and differences, leading to better exploration of novel states.
- What evidence would resolve it: Replacing SimHash with a learned hash function in VDSC and evaluating the impact on exploration performance and learning speed across various environments would demonstrate the benefits of this approach.

## Limitations

- Results are limited to a small set of Atari environments and may not generalize to other domains
- Implementation details of the homeostasis mechanism are unclear, making exact reproduction difficult
- The SimHash approach trades specificity for computational efficiency, but collision rates and their impact are unexamined

## Confidence

- Exploration timing superiority: Medium - demonstrated on limited Atari suite
- VPD as uncertainty proxy: Medium - correlation shown but not exhaustive validation
- SimHash novelty detection: Medium - effective but collision properties unexamined
- Homeostasis mechanism benefits: Low - implementation details unclear

## Next Checks

1. **Collision analysis**: Run VDSC on a single Atari game with varying κ values (32, 64, 128, 256 bits) and measure state collision rates and corresponding learning performance to quantify the trade-off between hash size and exploration effectiveness.

2. **Stochastic environment test**: Evaluate VDSC on a high-variance MDP (e.g., stochastic gridworld with probabilistic rewards) to determine whether VPD remains a reliable exploration trigger when returns are inherently noisy.

3. **Ablation on homeostasis**: Implement and compare alternative integration methods (simple averaging, max-pooling, learned weighting) for combining VPD and exploration bonus signals to isolate the contribution of the homeostasis mechanism versus the individual triggers.