---
ver: rpa2
title: Explaining Graph Neural Networks for Node Similarity on Graphs
arxiv_id: '2407.07639'
source_url: https://arxiv.org/abs/2407.07639
tags:
- graph
- similarity
- learning
- explanations
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates explaining node similarities computed
  by graph neural networks (GNNs) using two methods: mutual information (MI) and gradient-based
  (GB) explanations. While MI explanations identify relevant edges for predictions,
  GB explanations provide both magnitude and direction of influence on similarity
  scores.'
---

# Explaining Graph Neural Networks for Node Similarity on Graphs

## Quick Facts
- arXiv ID: 2407.07639
- Source URL: https://arxiv.org/abs/2407.07639
- Authors: Daniel Daza; Cuong Xuan Chu; Trung-Kien Tran; Daria Stepanova; Michael Cochez; Paul Groth
- Reference count: 40
- Primary result: Gradient-based explanations provide actionable and consistent explanations for GNN-based similarity scores, achieving effect overlap scores near 0.1 compared to MI's 0.4-0.6

## Executive Summary
This paper investigates methods for explaining node similarities computed by graph neural networks (GNNs). The authors compare mutual information (MI)-based explanations with gradient-based (GB) explanations across multiple graph datasets. While both methods identify relevant edges for predictions, GB explanations uniquely provide magnitude and direction of influence on similarity scores. The experiments demonstrate that GB explanations are actionable (predictable changes in similarity scores), consistent (distinct effects of selecting vs. discarding edges), and can be pruned significantly while retaining their effects.

## Method Summary
The paper implements GNNs (2-layer GCN or similar) trained using unsupervised methods (GAE, VGAE, DGI, GRACE) to learn node embeddings for similarity search. Two explanation methods are compared: mutual information-based (GNNExplainer) and gradient-based explanations. GB explanations compute the gradient of cosine similarity scores with respect to the adjacency matrix, providing both magnitude and direction of edge influence. The methods are evaluated on graph datasets (Cora, Citeseer, Pubmed, Chameleon, Actor, Squirrel, DBpedia50k) using fidelity metrics (Fid ùëé and Fidùëè), effect overlap (EO), and sparsity analysis. Interventions test whether explanations produce predictable changes when applied to the graph structure.

## Key Results
- GB explanations achieve effect overlap scores near 0.1 compared to MI's 0.4-0.6
- GB explanations can be reduced by up to 90% while preserving their effects on similarity scores
- Selecting vs. discarding edges produces distinct effects with GB explanations, but not with MI explanations
- GB explanations are actionable: selecting edges based on gradient values produces predictable changes in similarity scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based explanations provide both magnitude and direction of influence on similarity scores
- Mechanism: The gradient of the similarity score with respect to the adjacency matrix captures how much each edge contributes to the output and whether it increases or decreases the similarity
- Core assumption: The gradient of the cosine similarity function with respect to adjacency matrix entries is meaningful and differentiable
- Evidence anchors:
  - [abstract]: "GB explanations provide both magnitude and direction of influence on similarity scores"
  - [section]: "MGB B ‚àáAùëî(ùëìùúÉ (X, A))" - direct definition of gradient-based explanation
  - [corpus]: Weak - corpus papers focus on different explanation methods but don't directly contradict this mechanism
- Break condition: If the similarity function is not differentiable or if the GNN architecture introduces non-differentiable operations

### Mechanism 2
- Claim: GB explanations are actionable because selecting edges based on their gradient values produces predictable changes in similarity scores
- Mechanism: By computing gradients, we can determine which edges to add or remove to increase or decrease similarity scores in a controlled manner
- Core assumption: The learned GNN model is stable enough that small perturbations to the adjacency matrix produce predictable output changes
- Evidence anchors:
  - [abstract]: "they are actionable: selecting inputs depending on them results in predictable changes in similarity scores"
  - [section]: "GB explanations provide a magnitude and direction of influence" and "an appropriate threshold for selecting or discarding edges is ùë° = 0"
  - [corpus]: Missing - corpus papers don't address the predictability of interventions based on explanations
- Break condition: If the GNN model is highly non-linear or if the explanation threshold selection is arbitrary

### Mechanism 3
- Claim: GB explanations maintain consistency even when pruned to sparse subsets
- Mechanism: The gradient values provide a ranking of edge importance that remains stable under pruning, preserving the relative ordering of influence
- Core assumption: The gradient computation captures the true importance of edges in a way that survives sparsification
- Evidence anchors:
  - [abstract]: "they can be pruned significantly to obtain sparse explanations that retain the effect on similarity scores"
  - [section]: "we can further reduce the set of edges in the explanation by up to 90%, and the different effects on the similarity scores will be preserved"
  - [corpus]: Weak - corpus papers don't address sparsity preservation in explanations
- Break condition: If the gradient values are too noisy or if the sparsity threshold creates artificial dependencies between remaining edges

## Foundational Learning

- Concept: Graph Neural Networks and their architecture
  - Why needed here: The paper relies on understanding how GNNs compute node embeddings and how these relate to similarity scores
  - Quick check question: How does a 2-layer GCN compute node embeddings from adjacency matrix and feature matrix?

- Concept: Mutual Information and its application to explanations
  - Why needed here: MI-based methods are compared against gradient-based methods, so understanding the fundamental differences is crucial
  - Quick check question: What does maximizing mutual information between a subgraph and prediction accomplish in explanation methods?

- Concept: Cosine similarity and its properties
  - Why needed here: The similarity scores being explained are computed using cosine similarity, which determines the gradient calculation
  - Quick check question: How does the cosine similarity function behave with respect to its input vectors, and why is this important for gradient-based explanations?

## Architecture Onboarding

- Component map:
  GNN model (2-layer GCN or similar) ‚Üí computes node embeddings ‚Üí Similarity function (cosine similarity) ‚Üí computes pairwise scores ‚Üí Explanation module (MI or GB) ‚Üí Intervention mechanism ‚Üí modifies graph based on explanations ‚Üí Evaluation pipeline ‚Üí measures fidelity and consistency

- Critical path:
  GNN training ‚Üí similarity computation ‚Üí explanation generation ‚Üí intervention ‚Üí re-computation ‚Üí evaluation metrics

- Design tradeoffs:
  - MI methods vs GB methods: interpretability vs predictability
  - Explanation threshold selection: sensitivity vs specificity
  - Sparsity level: explanation complexity vs information retention

- Failure signatures:
  - Low fidelity scores despite high confidence in explanations
  - Inconsistent effects when selecting vs discarding edges
  - Unstable explanations across multiple runs with same inputs

- First 3 experiments:
  1. Implement both MI and GB explanation methods on a simple citation network dataset
  2. Measure fidelity and effect overlap metrics for both methods across multiple similarity pairs
  3. Test sparsity preservation by progressively pruning explanations and measuring metric stability

## Open Questions the Paper Calls Out
- How can we develop graph neural networks that are inherently explainable for similarity search, rather than requiring post-hoc explanations?
- Can gradient-based explanation methods be adapted to handle multi-modal graph data (e.g., text, images) for similarity search?
- How do the proposed explanation methods perform on dynamic graphs where node similarities evolve over time?

## Limitations
- The GB explanation effectiveness depends on GNN stability under small adjacency perturbations
- The comparison assumes both methods have access to the same information (learned embeddings)
- Performance on heterophilic graphs may be limited, particularly for DGI embeddings

## Confidence
- Gradient-based explanations provide predictable interventions: Medium
- GB explanations maintain consistency when pruned: Medium
- Effect overlap scores are meaningful across all graph types: Low

## Next Checks
1. Test gradient stability by systematically varying the explanation threshold and measuring how quickly intervention effects degrade
2. Compare explanation performance on heterophilic vs homophilic graphs to identify structural dependencies
3. Implement ablation studies removing specific graph components (edges, features) to isolate what each explanation method actually captures