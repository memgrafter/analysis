---
ver: rpa2
title: 'In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar
  Tasks'
arxiv_id: '2410.01548'
source_url: https://arxiv.org/abs/2410.01548
tags:
- task
- demonstrations
- source
- target
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem that synthesizing demonstrations
  for in-context learning using large language models is limited by the models' own
  capabilities and knowledge. The proposed method, In-Context Transfer Learning (ICTL),
  synthesizes demonstrations for target tasks by transferring labeled demonstrations
  from similar source tasks.
---

# In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar Tasks

## Quick Facts
- arXiv ID: 2410.01548
- Source URL: https://arxiv.org/abs/2410.01548
- Reference count: 40
- Improves ICL performance by 2.0% on average by transferring demonstrations from similar source tasks

## Executive Summary
This paper addresses the challenge of synthesizing high-quality demonstrations for in-context learning (ICL) by leveraging labeled demonstrations from similar source tasks. The proposed In-Context Transfer Learning (ICTL) method transfers demonstrations from source tasks to target tasks, improving performance by 2.0% on average compared to synthesis from scratch. The approach is particularly effective when sufficient similar source demonstrations are available, though performance degrades when source and target tasks are too dissimilar.

## Method Summary
ICTL consists of two main steps: source demonstration sampling and target demonstration transfer. The method first embeds target task definitions and filters source tasks based on similarity using Wasserstein distance. An optimization objective then guides the selection of source demonstrations that minimize transfer error. These sampled demonstrations are transferred to match the target task definition and format using LLMs, followed by verification to ensure consistency. The final demonstration set is sampled for use in ICL inference.

## Key Results
- ICTL improves ICL performance by 2.0% on average compared to synthesis from scratch
- Performance degrades sharply when source tasks have large gaps from target tasks
- Removing the transfer verification step results in a 3.3% average performance drop

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICTL improves ICL performance by transferring labeled demonstrations from similar source tasks rather than synthesizing from scratch
- Mechanism: Uses transfer learning principles to sample demonstrations from source tasks that are similar to the target task, then transfers these demonstrations to match the target task's definition and format
- Core assumption: Demonstrations from similar source tasks can effectively capture the task structure and patterns needed for the target task
- Evidence anchors:
  - Experiments on Super-NI show ICTL outperforms synthesis from scratch by 2.0% on average
  - Optimization objective minimizes transfer error by sampling source demonstrations similar to target tasks
- Break condition: If source tasks are too dissimilar from target tasks, transferred demonstrations may introduce noise

### Mechanism 2
- Claim: The optimization objective minimizes transfer error by ensuring sampled source demonstrations are similar to both the target task definition and the target demonstrations
- Mechanism: Combines two Wasserstein distance terms - one measuring divergence between source and target demonstrations, and another ensuring target demonstrations align with the target task definition
- Core assumption: Wasserstein distance effectively captures semantic similarity between task definitions and demonstrations in embedding space
- Evidence anchors:
  - Performance begins to decline sharply when source and target tasks have large gaps
  - Optimization objective derives from minimizing divergence between task distributions
- Break condition: If embedding model fails to capture semantic similarity accurately, optimization may select suboptimal source demonstrations

### Mechanism 3
- Claim: The two-step verification and sampling process ensures transferred demonstrations maintain quality and relevance to the target task
- Mechanism: First verifies that transferred demonstrations are consistent with the target task definition and format, then samples verified demonstrations to ensure they remain similar to source demonstrations while matching the target task
- Core assumption: LLM-based verification can reliably detect inconsistencies between transferred demonstrations and target task definitions
- Evidence anchors:
  - Removing transfer verification results in 3.3% performance drop on average
  - Verification checks consistency between transferred demonstrations and target task definitions
- Break condition: If verification LLM has biases or limitations in understanding task definitions, it may incorrectly filter valid demonstrations

## Foundational Learning

- Concept: Transfer Learning
  - Why needed here: ICTL is fundamentally based on transfer learning principles, applying knowledge from similar source tasks to improve performance on target tasks
  - Quick check question: What is the key difference between traditional transfer learning and ICTL's approach?

- Concept: Wasserstein Distance
  - Why needed here: Used to measure the divergence between distributions of task definitions and demonstrations, forming the core of the optimization objective
  - Quick check question: How does Wasserstein distance differ from other distance metrics like Euclidean distance in this context?

- Concept: In-Context Learning
  - Why needed here: The entire framework aims to improve ICL by providing better demonstrations to guide LLM inference
  - Quick check question: Why is demonstration quality so critical for ICL performance?

## Architecture Onboarding

- Component map: Source Task Collection -> Embedding Model -> Sampling Algorithm -> Transfer Module -> Verification Module -> Demonstration Pool

- Critical path:
  1. Embed target task definition
  2. Filter and rank source tasks by similarity
  3. Sample demonstrations using optimization objective
  4. Transfer demonstrations to target task format
  5. Verify transferred demonstrations
  6. Sample final demonstration set
  7. Use for inference with target task questions

- Design tradeoffs:
  - Sampling scale vs. computational efficiency: Larger sampling scales improve performance but increase runtime
  - Source task diversity vs. similarity: More diverse sources provide broader knowledge but may reduce relevance
  - Verification strictness vs. demonstration quantity: Stricter verification improves quality but may reduce available demonstrations

- Failure signatures:
  - Poor performance on tasks with few similar source demonstrations
  - Degradation when source tasks are too dissimilar from target tasks
  - Verification failures indicating format or semantic mismatches

- First 3 experiments:
  1. Test ICTL on a simple classification task with clearly similar source tasks to verify basic functionality
  2. Evaluate performance degradation when using source tasks with decreasing similarity to target task
  3. Measure the impact of verification module by comparing with and without verification enabled

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ICTL vary across different task categories when source tasks are not well-represented in the training set?
- Basis in paper: The paper notes that ICTL performs less effectively on Dialogue and Extraction tasks due to insufficient similar source demonstrations
- Why unresolved: The paper provides category-level analysis but does not systematically investigate how performance degrades when source tasks are scarce or poorly matched
- What evidence would resolve it: Controlled experiments varying the availability and similarity of source tasks across different categories, measuring performance impact

### Open Question 2
- Question: What is the optimal balance between the scale of source demonstrations and the diversity of source tasks for maximizing ICTL performance?
- Basis in paper: The paper discusses the impact of source demonstration scale and task scale separately but does not investigate their interaction or optimal combination
- Why unresolved: While the paper analyzes individual parameter effects, it does not explore how demonstration diversity and quantity interact to influence transfer quality
- What evidence would resolve it: Experiments systematically varying both demonstration scale and task diversity, identifying optimal combinations for different task types

### Open Question 3
- Question: How does ICTL performance change when using different embedding models for task similarity measurement?
- Basis in paper: The paper uses BGE-EN-ICL for embedding but does not compare its effectiveness against other embedding approaches
- Why unresolved: The paper relies on a single embedding model without evaluating how different similarity metrics might affect source task selection quality
- What evidence would resolve it: Comparative experiments using multiple embedding models or similarity metrics, measuring their impact on source sampling quality and final performance

### Open Question 4
- Question: Can ICTL be extended to handle tasks with significantly different input-output formats between source and target tasks?
- Basis in paper: The paper focuses on tasks with similar formats and does not address cases where source and target formats differ substantially
- Why unresolved: The verification step assumes format consistency, but the paper does not explore how to handle cases where source and target formats are incompatible
- What evidence would resolve it: Experiments testing ICTL on tasks requiring format transformation, measuring success rates and performance compared to format-matched tasks

## Limitations
- Performance heavily depends on availability of sufficiently similar source tasks in the dataset
- Computational overhead from multiple LLM calls for embedding, transfer, and verification
- Reliability of LLM-based verification step for ensuring transferred demonstration quality

## Confidence
- High confidence: Core claim that transferring demonstrations from similar tasks improves ICL performance over synthesis from scratch
- Medium confidence: Specific mechanism that Wasserstein distance optimization effectively selects optimal source demonstrations
- Medium confidence: Claim that the two-step verification and sampling process significantly improves performance

## Next Checks
1. Systematically evaluate ICTL performance across varying degrees of source-target task similarity to determine minimum similarity threshold required for effectiveness
2. Test verification step's sensitivity to different LLM models, prompt formulations, and task types to assess systematic bias potential
3. Benchmark ICTL against baseline methods measuring not just accuracy but also inference time and token usage to quantify practical tradeoffs