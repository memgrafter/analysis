---
ver: rpa2
title: 'PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency'
arxiv_id: '2410.07563'
source_url: https://arxiv.org/abs/2410.07563
tags:
- training
- https
- data
- japanese
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PLaMo-100B, a 100-billion parameter language
  model trained from scratch for Japanese proficiency. The model was trained on 2
  trillion tokens using advanced techniques like QK Normalization and Z-Loss for training
  stability.
---

# PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency

## Quick Facts
- arXiv ID: 2410.07563
- Source URL: https://arxiv.org/abs/2410.07563
- Reference count: 18
- Surpassed GPT-4 on Japanese benchmarks (Jaster, Rakuda)

## Executive Summary
PLaMo-100B is a 100-billion parameter language model trained from scratch to achieve superior Japanese language proficiency. The model employs advanced training techniques including QK Normalization and Z-Loss for stability, and underwent extensive post-training refinement through Supervised Fine-Tuning and Direct Preference Optimization. PLaMo-100B demonstrates competitive performance in both Japanese and English tasks, with particular strength in Japanese language understanding and generation.

## Method Summary
The model was pre-trained on 2 trillion tokens using a decoder-only transformer architecture with QK Normalization and Z-Loss for stability. Training employed 3D parallelism (data, tensor, and pipeline) across NVIDIA H100 GPUs with FP8 precision. The Japanese corpus comprised 0.7 trillion tokens, with 0.5 trillion from a custom-built dataset processed from WARC files. Post-training included SFT using instruction-following datasets followed by multiple rounds of DPO. The model is available under a non-commercial license.

## Key Results
- Surpassed GPT-4 on Japanese benchmarks Jaster and Rakuda
- Achieved competitive performance on English tasks including MT-Bench
- Demonstrated strong instruction-following capabilities through SFT and DPO refinement
- Showed training stability across 1.5 trillion tokens of pre-training

## Why This Works (Mechanism)

### Mechanism 1
QK Normalization improves training stability by normalizing the query-key interaction before self-attention. The normalization layer prevents exploding gradients common in large transformers. Core assumption: normalizing query-key values before interaction prevents instability without degrading performance. Evidence: Paper states QK Normalization ensured training stability; limited external validation.

### Mechanism 2
Z-Loss enhances numerical stability in softmax cross-entropy loss. The modified loss function addresses numerical issues in large-scale models with high vocabulary sizes. Core assumption: loss modification provides stability without performance impact. Evidence: Mentioned as part of architecture for stability; no direct comparative studies found.

### Mechanism 3
SFT and DPO refinement significantly improves task-specific performance. SFT aligns to instruction-following while DPO refines based on human preferences. Core assumption: combination effectively aligns without overfitting. Evidence: Paper states post-training techniques were applied; limited external validation in Japanese contexts.

## Foundational Learning

- Tokenization and Vocabulary
  - Why needed here: Understanding how the model processes Japanese text with complex characters and mixed scripts
  - Quick check question: How does the tokenizer handle Japanese text segmentation without explicit word boundaries?

- Attention Mechanisms
  - Why needed here: The model uses self-attention layers with QK Normalization
  - Quick check question: What role does QK Normalization play in self-attention, and how does it differ from standard normalization?

- Loss Functions and Optimization
  - Why needed here: The model uses Z-Loss for numerical stability
  - Quick check question: How does Z-Loss modify standard cross-entropy loss to address numerical stability issues?

## Architecture Onboarding

- Component map: Data Preprocessing -> Tokenizer -> Transformer Layers (with QK Normalization) -> Z-Loss -> SFT -> DPO -> Model Merging
- Critical path: Pre-training → SFT → DPO → Model Merging. Each stage builds upon previous to refine capabilities.
- Design tradeoffs: FP8 training for speed versus higher precision for critical components; training from scratch versus fine-tuning.
- Failure signatures: Training instability (diverging loss), poor post-training performance, inability to generalize to unseen data.
- First 3 experiments:
  1. Train smaller model with/without QK Normalization to observe stability differences
  2. Compare loss curves with/without Z-Loss to evaluate stability impact
  3. Measure performance on instruction-following task before/after SFT and DPO

## Open Questions the Paper Calls Out

1. Did training stability result from QK Normalization/Z-Loss or inherent setup stability? Paper states uncertainty about attribution of stability success. Unresolved due to high cost of re-running large-scale pre-training. Resolution requires controlled experiments on smaller models.

2. What is optimal balance between on-policy and off-policy preference data for DPO? Paper combined both but didn't systematically evaluate ratios. Unresolved due to lack of comparative analysis. Resolution requires controlled experiments varying data proportions.

3. How much of math reasoning gap stems from pre-training versus post-training? Paper notes underperformance in math but doesn't break down contributing factors. Unresolved due to insufficient analysis of data versus fine-tuning impact. Resolution requires detailed pre/post training comparison.

## Limitations

- QK Normalization and Z-Loss lack external validation from broader research community
- Performance claims against GPT-4 require independent verification
- Proprietary datasets used cannot be independently assessed for quality and composition

## Confidence

**High Confidence**: Model architecture follows established transformer principles; 3D parallelism is technically sound; post-training approach aligns with current best practices.

**Medium Confidence**: QK Normalization and Z-Loss implementations show theoretical merit but lack broader validation; training methodology depends on unverifiable proprietary datasets.

**Low Confidence**: Claims of surpassing GPT-4 on Japanese benchmarks need independent verification; custom tokenization strategy effectiveness unproven; long-term stability not demonstrated through extended testing.

## Next Checks

1. Conduct controlled experiments training smaller-scale models with and without QK Normalization and Z-Loss to empirically measure their impact on training stability and model quality.

2. Independently evaluate PLaMo-100B on Japanese and English benchmarks using standardized protocols, including GPT-4 in comparisons to verify claimed performance superiority.

3. Analyze composition and quality of Japanese training data, particularly the custom WARC-processed corpus, through ablation studies to determine impact on model performance.