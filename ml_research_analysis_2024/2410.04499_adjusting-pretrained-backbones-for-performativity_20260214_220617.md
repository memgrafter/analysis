---
ver: rpa2
title: Adjusting Pretrained Backbones for Performativity
arxiv_id: '2410.04499'
source_url: https://arxiv.org/abs/2410.04499
tags: []
core_contribution: "The paper introduces a modular method to adjust pretrained deep\
  \ learning models for performative label shift, where model deployment changes class\
  \ proportions in future data. The core idea is to train a shallow adapter module\
  \ that predicts label marginals from a sufficient statistic of the deployed model\
  \ and corrects the pretrained model\u2019s logits post-hoc using Bayes-optimal adjustment."
---

# Adjusting Pretrained Backbones for Performativity

## Quick Facts
- arXiv ID: 2410.04499
- Source URL: https://arxiv.org/abs/2410.04499
- Reference count: 40
- The paper introduces a modular method to adjust pretrained deep learning models for performative label shift, where model deployment changes class proportions in future data.

## Executive Summary
This paper addresses the challenge of adapting pretrained deep learning models when model deployment causes class proportions in future data to shift (performative label shift). The authors propose a novel approach that trains a shallow adapter module to predict label marginals from a sufficient statistic of the deployed model, then applies Bayes-optimal correction to the pretrained model's logits. This modular design allows reusing frozen backbones while adapting only through the adapter, drastically reducing data requirements and enabling zero-shot transfer across different backbones.

## Method Summary
The method involves training a shallow adapter module that learns to predict label marginals from a sufficient statistic (such as class accuracies) of the deployed model. This adapter then corrects the pretrained model's logits using Bayes-optimal adjustment, scaling the logits by the ratio of predicted to prior label marginals. The approach assumes that only the label distribution changes (P(Y|X) shifts) while the features remain stable, allowing the frozen backbone to continue extracting useful representations. The adapter is trained using a memory buffer that stores (statistic, label marginal) pairs from deployed models, with training optimized using KL-divergence loss.

## Key Results
- The PaP adapter achieves 3.31–4.25% accuracy gains over no adaptation across vision and language tasks under simulated adversarial sampling
- The method approaches Bayes-optimal performance and enables effective model selection before deployment
- PaP requires only 117K parameters and 0.07 GFLOPs per round, making it practical for large-scale deployment

## Why This Works (Mechanism)

### Mechanism 1
The adapter learns to predict label marginals from a sufficient statistic of the deployed model, enabling Bayes-optimal correction of logits. The adapter module T(S) estimates the future class distribution from the statistic S (e.g., class accuracies), which are used to scale the pretrained model's logits via λi(S) · [fpre(X)]i / Λpre_i, correcting for label shift without retraining the backbone.

### Mechanism 2
The modular design allows adaptation using only a shallow adapter while reusing frozen pretrained backbones, drastically reducing data requirements. The adapter module operates on top of frozen backbone representations, learning only the mapping from S to label marginals, isolating the performative mechanism from feature extraction.

### Mechanism 3
The adapter can anticipate model brittleness before deployment, enabling informed model selection. By evaluating the adapter's estimated post-deployment accuracy on current data, one can rank models by their robustness to anticipated label shifts without actual deployment.

## Foundational Learning

- **Concept**: Performativity and performative prediction
  - Why needed here: The entire paper addresses how model deployment changes future data distributions, requiring understanding of this feedback loop
  - Quick check question: What is the key difference between performative prediction and standard distribution shift?

- **Concept**: Label shift and Bayes-optimal correction
  - Why needed here: The method specifically addresses label shift by adjusting class priors in logits, requiring knowledge of how to correct for this type of shift
  - Quick check question: How does Bayes-optimal correction for label shift differ from covariate shift adaptation?

- **Concept**: Sufficient statistics and causal mechanisms
  - Why needed here: The method relies on identifying a low-dimensional statistic that fully characterizes the shift, requiring understanding of sufficient statistics in causal inference
  - Quick check question: What makes a statistic "sufficient" for characterizing a distribution shift?

## Architecture Onboarding

- **Component map**: Pretrained backbone (frozen) → Adapter module (T) → Adjusted logits → Final predictions
- **Critical path**: S → T(S) → λ(S) → [fpre(X)] → λ(S)·[fpre(X)]/Λpre → argmax
- **Design tradeoffs**: Expressivity of adapter vs. sample requirements (more layers = more data needed); full adapter vs. last-layer only (accuracy vs. efficiency)
- **Failure signatures**: No improvement over no adaptation; degradation after deployment; adapter training loss plateaus early
- **First 3 experiments**:
  1. Verify adapter learns label marginals from S by checking KL divergence on held-out data
  2. Test adjustment on synthetic label shift data with known marginals
  3. Compare accuracy trajectory with and without adapter on CIFAR100 under simulated shift

## Open Questions the Paper Calls Out

### Open Question 1
How does the PaP module's performance vary when the sufficient statistic is not explicitly known but must be learned from data? The paper assumes the sufficient statistic is known but mentions this could be an extension using causal representation learning tools.

### Open Question 2
What is the theoretical sample complexity required for the PaP module to achieve near-optimal performance under performative label shift? The paper states that theoretical guarantees for learning the performative mechanism are unanswered, including sample complexities.

### Open Question 3
How does the PaP module perform under combined label and domain shifts, and what is the impact of varying the relative strength of each shift? The paper includes a brief experiment combining label and domain shifts but does not provide a systematic analysis.

## Limitations
- All results rely on simulated performative shifts rather than real-world deployment data, raising questions about external validity
- The method assumes P(X|Y) remains constant across rounds, failing if the backbone's features become non-discriminative under the shift
- Performance degrades when shifts exceed 5-6 standard deviations from the mean, indicating boundaries of applicability

## Confidence

- **High confidence**: The modular design allowing reuse of frozen backbones while adapting only through a shallow adapter is well-supported by ablation studies
- **Medium confidence**: The Bayes-optimal correction mechanism is theoretically sound but relies on strong assumptions about feature stability
- **Low confidence**: Claims about pre-deployment model selection and zero-shot transfer across backbones are based on limited experiments and require broader validation

## Next Checks

1. **Real-world deployment test**: Deploy the method on an existing model in production to measure actual performative effects versus simulated performance, particularly testing the 5-6 standard deviation limitation boundary.

2. **Concept drift stress test**: Systematically evaluate performance when P(X|Y) changes alongside label shifts by introducing synthetic concept drift to the datasets and measuring degradation.

3. **Cross-domain transfer validation**: Test zero-shot transfer capability across multiple backbone architectures (CNNs, transformers) and diverse task domains beyond the single vision/language pair evaluated.