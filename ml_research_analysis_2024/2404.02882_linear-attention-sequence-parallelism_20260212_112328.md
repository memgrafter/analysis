---
ver: rpa2
title: Linear Attention Sequence Parallelism
arxiv_id: '2404.02882'
source_url: https://arxiv.org/abs/2404.02882
tags:
- lasp
- sequence
- linear
- attention
- gpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LASP is a new sequence parallelism approach for efficient distributed
  training of linear attention-based models. It leverages the right-product kernel
  trick of linear attention through a point-to-point ring-style communication mechanism,
  reducing communication overhead compared to existing SP methods.
---

# Linear Attention Sequence Parallelism

## Quick Facts
- arXiv ID: 2404.02882
- Source URL: https://arxiv.org/abs/2404.02882
- Authors: Weigao Sun; Zhen Qin; Dong Li; Xuyang Shen; Yu Qiao; Yiran Zhong
- Reference count: 40
- Key outcome: LASP achieves 8× longer sequence length scalability (up to 4096K) compared to existing SP methods while maintaining high throughput

## Executive Summary
LASP is a new sequence parallelism approach for efficient distributed training of linear attention-based models. It leverages the right-product kernel trick of linear attention through a point-to-point ring-style communication mechanism, reducing communication overhead compared to existing SP methods. LASP incorporates system engineering optimizations like kernel fusion and intermediate state caching for GPU efficiency, and ensures compatibility with all batch-level data parallel methods. Experiments show LASP scales sequence length up to 4096K on 128 GPUs, which is 8× longer than existing SP methods, while maintaining high throughput and convergence.

## Method Summary
LASP (Linear Attention Sequence Parallelism) distributes sequence chunks across GPUs and uses a ring-style point-to-point communication mechanism to exchange only the KV state (not full K and V matrices) between chunks. This leverages the associativity property of matrix products in linear attention to achieve sequence length-independent communication overhead. The method includes kernel fusion to reduce launch overhead and memory traffic, caches KV states to avoid recomputation during backward pass, and maintains compatibility with all batch-level data parallel methods including PyTorch DDP, FSDP, and ZeRO-series optimizers.

## Key Results
- Scales sequence length up to 4096K on 128 GPUs, 8× longer than existing SP methods
- Achieves sequence length-independent communication overhead (O(d²) regardless of sequence length N)
- Maintains compatibility with all batch-level data parallel methods while providing superior scalability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LASP leverages the right-product kernel trick of linear attention to achieve sequence length-independent communication overhead.
- **Mechanism:** By using a point-to-point ring-style communication mechanism that only exchanges the KV state (rather than both K and V), LASP reduces communication volume to a constant O(d²) regardless of sequence length N.
- **Core assumption:** The associativity property of matrix products allows the intermediate KV state to capture all necessary information for subsequent chunks without requiring full K and V transmission.
- **Evidence anchors:**
  - [abstract]: "We design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead"
  - [section 3.3]: "In SP, it is important to note that the forward pass requires communication for the KV∈Rd×d state in each linear attention module layer. The communication volume is determined by Bd²/h"
  - [corpus]: Weak - The corpus mentions related SP methods but doesn't provide direct evidence about the right-product kernel trick advantage
- **Break condition:** If the associativity property fails (e.g., due to numerical instability or non-standard linear attention formulations), the KV state would not be sufficient, breaking the mechanism.

### Mechanism 2
- **Claim:** Kernel fusion and intermediate state caching make LASP hardware-friendly on GPUs.
- **Mechanism:** Fusing intra-chunk and inter-chunk computations into single kernels reduces kernel launch overhead and memory traffic, while caching KV states avoids recomputation during the backward pass.
- **Core assumption:** Modern GPUs benefit significantly from kernel fusion due to reduced kernel launch overhead and better memory coalescing.
- **Evidence anchors:**
  - [section 3.4]: "We perform kernel fusion in both the intra-chunk and inter-chunk computations, and also fused the updates of KV and dKV into the intra-chunk and inter-chunk computations"
  - [section 3.4]: "To avoid recomputing activation KV during the backward pass, we choose to store it in the HBM of the GPU right after computing it in the forward pass"
  - [corpus]: Weak - The corpus doesn't provide specific evidence about GPU performance improvements from these optimizations
- **Break condition:** If GPU memory bandwidth becomes the bottleneck instead of computation, or if the fused kernels exceed register limits, the optimization may not provide benefits.

### Mechanism 3
- **Claim:** LASP's compatibility with all batch-level data parallel methods enables practical large-scale distributed training.
- **Mechanism:** By ensuring that sequence parallelism works seamlessly with PyTorch DDP, FSDP, and ZeRO-series optimizers, LASP allows distributed training across heterogeneous cluster configurations.
- **Core assumption:** The data-sequence hybrid parallelism design allows the sequence parallel size to be smaller than the world size while maintaining correctness.
- **Evidence anchors:**
  - [abstract]: "We meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods"
  - [section 3.5]: "LASP supports both tensor parallelism (TP) and pipeline parallelism (PP)" and discusses data-sequence hybrid parallelism
  - [corpus]: Weak - The corpus mentions related SP methods but doesn't provide evidence about LASP's specific compatibility features
- **Break condition:** If data parallel methods change their communication patterns in ways that conflict with LASP's sequence partitioning, compatibility could break.

## Foundational Learning

- **Concept:** Right-product kernel trick in linear attention
  - Why needed here: This is the fundamental mathematical insight that enables LASP's communication efficiency by allowing computation to proceed with only the KV state
  - Quick check question: In linear attention, why can we compute Q(K⊤V) instead of (QK⊤)V, and what advantage does this provide for distributed computation?

- **Concept:** Point-to-point ring-style communication
  - Why needed here: Understanding this communication pattern is essential for grasping how LASP achieves sequence length-independent overhead
  - Quick check question: How does a ring-style communication pattern differ from all-to-all communication in terms of bandwidth requirements and latency?

- **Concept:** Sequence parallelism vs. tensor parallelism
  - Why needed here: LASP operates at the sequence dimension while tensor parallelism operates at the model dimension; understanding both is crucial for hybrid parallelism
  - Quick check question: What are the key differences between sequence parallelism and tensor parallelism in terms of what they partition and how they communicate?

## Architecture Onboarding

- **Component map:** Data distribution layer -> Forward pass engine -> Backward pass engine -> System optimization layer -> Compatibility layer
- **Critical path:**
  1. Data distribution and chunking
  2. Intra-chunk attention computation (parallel across GPUs)
  3. P2P communication of KV states
  4. Inter-chunk attention computation
  5. KV state update and communication to next GPU
  6. Backward pass with cached states and reverse communication
- **Design tradeoffs:**
  - Memory vs. Communication: Caching KV states uses more memory but reduces recomputation
  - Flexibility vs. Performance: Supporting multiple data parallel methods adds complexity but enables practical deployment
  - Hardware optimization vs. Portability: Kernel fusion improves performance but may reduce portability across different GPU architectures
- **Failure signatures:**
  - Out of Memory (OOM) errors: Indicates insufficient GPU memory for cached KV states
  - Communication bottlenecks: Degraded throughput suggesting P2P communication overhead
  - Convergence issues: Loss divergence suggesting implementation bugs in the distributed computation
- **First 3 experiments:**
  1. Single GPU validation: Run LASP with sequence parallel size = 1 to verify correctness against baseline
  2. Small-scale scaling test: Test with 2-4 GPUs on short sequences to verify communication patterns
  3. Memory profiling: Measure memory usage with and without KV state caching on sequences of varying lengths

## Open Questions the Paper Calls Out

- **Open Question 1:** How does LASP's performance compare when applied to other linear sequence modeling methods beyond linear attention, such as state space models (SSMs) or linear RNNs?
  - Basis in paper: [explicit] The paper discusses the generalization of LASP to other linear sequence modeling methods and provides a theoretical framework for this generalization. It also mentions that LASP can be seamlessly integrated into most linear-complexity models.
  - Why unresolved: The paper only provides theoretical analysis and mentions of potential applications. It does not present empirical results or detailed performance comparisons for these other methods.
  - What evidence would resolve it: Conducting experiments applying LASP to SSMs and linear RNNs, and comparing their performance against other SP methods and their original implementations.

- **Open Question 2:** What is the impact of LASP on the convergence of hybrid models that interleave softmax attention and linear attention layers?
  - Basis in paper: [explicit] The paper discusses hybrid SP on inter-layer hybrid models and presents an experiment evaluating the feasibility of this approach. It mentions that hybrid SP effectively extends the maximum trainable sequence length for both TNL and Linear Transformer, while incurring only a slight reduction in training speed.
  - Why unresolved: The experiment only evaluates the feasibility and speed of hybrid SP, not the convergence of the hybrid models. The paper does not provide convergence results for these hybrid models when using LASP.
  - What evidence would resolve it: Conducting experiments to train hybrid models with LASP and comparing their convergence (e.g., loss curves) against hybrid models trained without LASP or with other SP methods.

- **Open Question 3:** How does the communication volume of LASP scale with the number of GPUs and sequence length in practical scenarios?
  - Basis in paper: [explicit] The paper states that the communication volume of LASP is independent of sequence length, which is a significant advantage. It also provides a theoretical comparison of communication volumes with other SP methods.
  - Why unresolved: While the paper claims that the communication volume is independent of sequence length, it does not provide empirical measurements of the actual communication volume in different scenarios (e.g., varying number of GPUs and sequence lengths).
  - What evidence would resolve it: Measuring and reporting the actual communication volume (e.g., in terms of data transferred per second) for LASP in various configurations, including different numbers of GPUs and sequence lengths, and comparing it with theoretical predictions and other SP methods.

## Limitations

- The evaluation focuses primarily on linear attention models, leaving uncertainty about performance on standard attention mechanisms
- Several key implementation details for kernel fusion thresholds and memory management strategies are not fully specified
- The paper relies heavily on proprietary implementations of competing methods (Megatron-LM, M6-LLM) for comparison, limiting reproducibility

## Confidence

- **High confidence**: The core mechanism of using KV state communication instead of full K and V transmission is mathematically sound and well-established in the linear attention literature.
- **Medium confidence**: The claimed 8× improvement in sequence length scalability is supported by experiments, but direct comparison with all competing SP methods would strengthen the claims.
- **Low confidence**: The specific GPU kernel optimizations and their contribution to overall performance gains are not fully detailed, making it difficult to assess their relative importance.

## Next Checks

1. Reproduce the sequence length scalability results on a smaller scale (16-32 GPUs) with varying sequence lengths to verify the claimed 8× improvement over baseline SP methods.
2. Conduct ablation studies to quantify the individual contributions of kernel fusion, KV state caching, and communication optimization to overall performance.
3. Test LASP's compatibility with different data parallel backends (PyTorch DDP vs. FSDP) on heterogeneous cluster configurations to verify the claimed flexibility.