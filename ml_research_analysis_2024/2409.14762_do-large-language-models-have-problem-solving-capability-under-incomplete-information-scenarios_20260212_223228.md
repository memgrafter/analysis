---
ver: rpa2
title: Do Large Language Models have Problem-Solving Capability under Incomplete Information
  Scenarios?
arxiv_id: '2409.14762'
source_url: https://arxiv.org/abs/2409.14762
tags:
- llms
- uni00000016
- uni00000047
- uni00000028
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BrainKing, a novel game combining "Twenty
  Questions" and "Who is Undercover" to evaluate Large Language Models' (LLMs) problem-solving
  capabilities under incomplete information scenarios. The game requires LLMs to identify
  target entities through limited yes-or-no questions while handling potential misleading
  answers.
---

# Do Large Language Models have Problem-Solving Capability under Incomplete Information Scenarios?

## Quick Facts
- arXiv ID: 2409.14762
- Source URL: https://arxiv.org/abs/2409.14762
- Reference count: 18
- Primary result: BrainKing game evaluates LLMs' problem-solving under incomplete information, with GPT-4 achieving 78.8% accuracy in hard mode

## Executive Summary
This paper introduces BrainKing, a novel game that evaluates Large Language Models' problem-solving capabilities under incomplete information scenarios by combining elements of "Twenty Questions" and "Who is Undercover." The game requires LLMs to identify target entities through limited yes-or-no questions while handling potential misleading answers. The study evaluates 12 LLMs across three difficulty modes (easy, medium, hard) using accuracy and rounds as metrics, revealing significant performance differences between models like GPT-4 and Claude-2 versus smaller models like Falcon-7B and Vicuna-7B.

## Method Summary
The BrainKing game evaluates LLMs by having them identify target entities from a dataset of 10,000 common entities through a series of yes-or-no questions. The game operates in three difficulty modes: easy (simple starting point), medium (harder starting point), and hard (simple starting point with misleading answers from similar entities). GPT-4 serves as the oracle answering questions posed by each LLM, with human performance as a baseline reference. The evaluation measures both accuracy (whether the entity is guessed within 20 questions) and rounds (number of questions taken), with additional assessment of the LLM's ability to recognize confusion in hard mode.

## Key Results
- GPT-4 and Claude-2 demonstrated superior performance, with GPT-4 achieving 78.8% accuracy in hard mode
- Smaller models like Falcon-7B and Vicuna-7B showed significantly lower performance, particularly in handling misleading information
- Accuracy vs rounds relationship revealed LLM reasoning efficiency and susceptibility to confusion
- The three difficulty modes effectively differentiated LLM capabilities across various aspects of problem-solving

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BrainKing combines questioning, knowledge retrieval, error detection, and path planning to evaluate LLMs under incomplete information.
- Mechanism: The game structure forces LLMs to generate yes/no questions, interpret answers, and adjust reasoning strategies based on feedback, mirroring real-world incomplete information problem solving.
- Core assumption: LLMs can use limited yes/no feedback to narrow down entities through iterative questioning.
- Evidence anchors:
  - [abstract] "encompassing capabilities such as questioning, knowledge search, error detection, and path planning."
  - [section] "requires LLMs to identify target entities with limited yes-or-no questions and potential misleading answers."
  - [corpus] Weak - corpus papers focus on mathematical or hierarchical reasoning but do not directly address the combination of questioning and error detection under incomplete information.
- Break condition: If LLM questioning strategy is too rigid or cannot adapt when misleading answers are introduced, accuracy will drop sharply.

### Mechanism 2
- Claim: Three difficulty modes (easy, medium, hard) progressively challenge LLMs' reasoning depth and robustness.
- Mechanism: Easy mode tests basic knowledge retrieval, medium mode tests ability to handle broader categories, hard mode tests resilience against misleading information.
- Core assumption: LLM performance degrades predictably as reasoning complexity increases, revealing specific capability gaps.
- Evidence anchors:
  - [abstract] "By setting up easy, medium, and hard difficulty modes, we comprehensively assess the performance of LLMs across various aspects."
  - [section] "Easy mode is to provide a simple starting point... Medium mode is to provide a harder starting point... Hard mode introduces a similar entity for generating wrong answers..."
  - [corpus] Moderate - MathOdyssey and HiBench also use progressive difficulty but in mathematical contexts, not incomplete information scenarios.
- Break condition: If LLM accuracy drops to near-random levels in medium or hard modes, the model lacks adaptive reasoning under uncertainty.

### Mechanism 3
- Claim: Accuracy vs rounds relationship reveals LLM reasoning efficiency and susceptibility to confusion.
- Mechanism: Monitoring how many questions are needed before correct inference shows whether LLMs use feedback effectively or waste rounds on confusion.
- Core assumption: Better LLMs will infer correctly in fewer rounds and recover faster from misleading information.
- Evidence anchors:
  - [abstract] "Our results explore five key questions regarding LLM performance in BrainKing, investigating the relationship between accuracy and rounds..."
  - [section] "Rounds measures how many questions it takes to infer the entity... If the entity is not inferred within Twenty Questions, we set its rounds as 30..."
  - [corpus] Weak - No corpus neighbor directly discusses rounds as a metric for incomplete information reasoning.
- Break condition: If LLM rounds increase dramatically in hard mode without accuracy improvement, it indicates inability to filter misleading cues.

## Foundational Learning

- Concept: Incomplete information scenarios
  - Why needed here: Understanding that real-world decision making often lacks full data is essential to appreciate why BrainKing is a valid benchmark.
  - Quick check question: What distinguishes incomplete information from complete information in decision-making contexts?

- Concept: Yes/no question strategy and narrowing search space
  - Why needed here: BrainKing relies on LLMs generating effective yes/no questions to progressively eliminate possibilities.
  - Quick check question: How does asking "Is it larger than a breadbox?" reduce the set of possible entities?

- Concept: Misleading cues and backtracking
  - Why needed here: Hard mode tests whether LLMs can detect and recover from incorrect inference paths caused by misleading answers.
  - Quick check question: What strategies can an LLM use to recognize and correct for a misleading answer during questioning?

## Architecture Onboarding

- Component map:
  - Dataset preparation: Select 10,000 common entities, generate hierarchical concept lists, identify similar entities for confusion
  - Prompt templates: Three difficulty modes with clear instructions
  - LLM execution: Run LLMs with prompts, record questions and answers
  - Evaluation metrics: Accuracy, rounds, win rates, and confusion recognition
  - Human baseline: Recruit volunteers to establish human performance reference

- Critical path:
  1. Load dataset and generate hierarchical concept lists
  2. Generate prompt for each difficulty mode
  3. Execute LLM questioning session
  4. Record answers and compute metrics
  5. Analyze performance trends across modes

- Design tradeoffs:
  - Dataset size vs. diversity: Using only 10,000 common entities ensures LLM knowledge coverage but limits novelty
  - Question limit (20) vs. thoroughness: Limits ensure game feasibility but may truncate deep reasoning
  - Similar entity selection vs. confusion realism: Too similar entities may be unsolvable; too dissimilar may not challenge reasoning

- Failure signatures:
  - Low accuracy across all modes: Indicates general reasoning or knowledge retrieval deficits
  - High rounds with low accuracy: Indicates inefficient questioning or inability to narrow search space
  - No recovery from misleading answers: Indicates lack of error detection or adaptive reasoning

- First 3 experiments:
  1. Run easy mode with GPT-4 and compare rounds to accuracy; verify whether fewer rounds correlate with higher accuracy
  2. Test medium mode with Claude-2; analyze how starting point difficulty affects rounds and accuracy
  3. Run hard mode with LLaMA2-7B; check if misleading answers cause dramatic accuracy drops without rounds recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can BrainKing be extended to evaluate multimodal LLMs (e.g., text-to-image models) by incorporating visual entities and questions?
- Basis in paper: [inferred] The paper evaluates LLMs' problem-solving capabilities using textual entities and yes-or-no questions, but doesn't explore multimodal extensions.
- Why unresolved: The current BrainKing benchmark is purely text-based, and it's unclear how well it would translate to multimodal scenarios where visual information is crucial.
- What evidence would resolve it: A study extending BrainKing to include visual entities and questions, evaluating multimodal LLMs' performance compared to text-only models.

### Open Question 2
- Question: How does the performance of LLMs in BrainKing correlate with their performance on other established benchmarks like MMLU or BIG-bench?
- Basis in paper: [inferred] The paper introduces BrainKing as a novel evaluation tool but doesn't compare its results with other benchmarks.
- Why unresolved: It's unclear whether BrainKing measures unique aspects of LLM capabilities or overlaps significantly with existing benchmarks.
- What evidence would resolve it: A comparative study analyzing correlations between BrainKing scores and scores on other major LLM evaluation benchmarks.

### Open Question 3
- Question: Can the BrainKing framework be adapted to evaluate LLMs' problem-solving capabilities in domain-specific scenarios (e.g., medical diagnosis, legal reasoning)?
- Basis in paper: [inferred] The paper discusses the importance of problem-solving under incomplete information in various fields but doesn't explore domain-specific adaptations.
- Why unresolved: The current BrainKing uses general knowledge entities, and it's unclear how well it would perform when tailored to specific professional domains.
- What evidence would resolve it: A study adapting BrainKing for specific domains (e.g., medical entities for diagnosis scenarios) and evaluating domain-specific LLM performance.

## Limitations
- Dependency on GPT-4 as an oracle for answering LLM questions introduces potential bias
- Corpus evidence for the combined questioning and error detection mechanism is weak
- Focus on yes/no questions may not fully capture real-world incomplete information scenarios

## Confidence
- High Confidence: The superiority of GPT-4 and Claude-2 in accuracy and resilience to misleading information is well-supported by empirical results across multiple difficulty modes
- Medium Confidence: The claim that the three difficulty modes progressively challenge LLM reasoning is supported by performance degradation patterns, though the exact mechanism requires further validation
- Low Confidence: The assertion that accuracy vs rounds relationship reveals LLM reasoning efficiency is weakly supported, as the corpus lacks direct evidence for rounds as a metric in incomplete information reasoning

## Next Checks
1. Validate whether GPT-4's responses as an oracle introduce bias by comparing LLM performance when using human-generated answers versus GPT-4's answers for the same questions
2. Test whether the observed performance trends hold across a broader range of entities and question types, including non-yes/no questions, to assess the framework's generalizability
3. Evaluate whether BrainKing's yes/no question structure accurately reflects real-world incomplete information problem-solving by testing LLMs on tasks requiring multi-step reasoning with partial information