---
ver: rpa2
title: 'Thunder : Unified Regression-Diffusion Speech Enhancement with a Single Reverse
  Step using Brownian Bridge'
arxiv_id: '2406.06139'
source_url: https://arxiv.org/abs/2406.06139
tags:
- speech
- diffusion
- process
- regression
- reverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Thunder, a unified regression-diffusion model
  for speech enhancement that leverages Brownian bridge process to enable both regression
  and diffusion modes without additional parameters. The key innovation is modifying
  the model to predict clean speech directly instead of the score function, which
  resolves gradient instability issues when operating near t=1 (regression mode).
---

# Thunder : Unified Regression-Diffusion Speech Enhancement with a Single Reverse Step using Brownian Bridge

## Quick Facts
- arXiv ID: 2406.06139
- Source URL: https://arxiv.org/abs/2406.06139
- Reference count: 0
- Primary result: Achieves PESQ 2.99, ESTOI 0.87, SI-SDR 19.6 with only 1 reverse step, outperforming diffusion baselines requiring 30 steps

## Executive Summary
Thunder introduces a unified regression-diffusion model for speech enhancement that leverages Brownian bridge processes to enable both regression and diffusion modes without additional parameters. The key innovation is modifying the model to predict clean speech directly instead of the score function, which resolves gradient instability issues when operating near t=1. This approach achieves competitive results on the VoiceBank+DEMAND dataset while using half the parameters of comparable diffusion models and enabling real-time inference with a single reverse step.

## Method Summary
Thunder modifies the NCSN++ architecture to predict clean speech instead of the score function, using a Brownian bridge process SDE with drift coefficient y-xt/(1-t) and diffusion coefficient 1. The model is trained with MSE loss between predicted and clean speech using Adam optimizer (2e-5) for 100 epochs with batch size 8. Inference uses a modified Euler-Maruyama method with a single reverse step, enabling real-time speech enhancement while maintaining competitive quality metrics.

## Key Results
- Achieves PESQ 2.99, ESTOI 0.87, and SI-SDR 19.6 with only 1 reverse step
- Outperforms diffusion baselines requiring 30 steps on VoiceBank+DEMAND dataset
- Uses half the parameters of comparable diffusion models
- Demonstrates superior generalization on out-of-domain LibriFSD50k dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Brownian bridge process enables both regression and diffusion modes without additional parameters by setting t close to 1 for regression mode.
- Mechanism: The Brownian bridge process has a drift coefficient that converges to the noise in the input speech as t approaches 1, allowing the model to act as a regression model at t = 1 by predicting the clean speech directly instead of the score function.
- Core assumption: The drift coefficient of the reverse Brownian bridge process converges to the noise in the input speech as t approaches 1.
- Evidence anchors:
  - [abstract] "We propose Thunder, a unified regression-diffusion model that utilizes the Brownian bridge process which can allow the model to act in both modes."
  - [section 2.1] "This particular SDE is referred to as the Brownian bridge process [17]. Its distinguishing feature is that it can linearly transform between the initial state (x0) with zero variance to the noisy speech y with zero variance, offering a capability to perform as a regression model at t = 1 (deterministic mode)."
  - [corpus] Weak evidence; the corpus neighbors do not directly support this mechanism, but the general concept of Brownian bridge processes in diffusion models is established.
- Break condition: If the drift coefficient does not converge to the noise in the input speech as t approaches 1, the regression mode will not function correctly.

### Mechanism 2
- Claim: Predicting clean speech instead of the score function avoids gradient instability issues when operating near t=1 in regression mode.
- Mechanism: Directly predicting the clean speech x0 instead of the score function eliminates the need to compute gradients of log pt(xt) which become unstable as σ(t) approaches 0 when t approaches 1.
- Core assumption: The gradient of the loss function is directly proportional to σ(t), making it impractical to minimize the loss when σ(t) is close to 0.
- Evidence anchors:
  - [section 3.1] "This hampers the model's ability to efficiently estimate the score function at t = 1 under one reverse step... To overcome this problem, we modify the model to predict x̃θ(xt, y, t), an estimation of clean speech x0, instead of the score function."
  - [abstract] "To mitigate this problem, we modify the diffusion model to predict the clean speech instead of the score function, achieving competitive performance with a more compact model size and fewer reverse steps."
  - [corpus] Weak evidence; the corpus neighbors do not directly support this mechanism, but the general concept of score-based diffusion models and their limitations is established.
- Break condition: If the model cannot accurately predict the clean speech x0, the regression mode will not function correctly.

### Mechanism 3
- Claim: The two-stage process (regression followed by diffusion) improves speech quality by first reducing noise deterministically, then refining with generative modeling to reduce artifacts.
- Mechanism: The regression mode improves the signal quality by predicting the clean speech directly, while the diffusion mode refines the output from the regression mode using a reverse diffusion process to reduce artifacts generated by the regression model.
- Core assumption: The regression mode can effectively reduce noise, but may introduce artifacts that can be reduced by the diffusion mode.
- Evidence anchors:
  - [section 3.3] "Following StoRM [8], our pipeline is a two-stage process which is a regression model followed by a generative model. The first improves the signal quality, while the latter aims to reduce the artifacts generated by the regression model."
  - [abstract] "Initializing the diffusion process with the enhanced audio generated by a regression-based model can be used to reduce the computational steps required."
  - [corpus] Weak evidence; the corpus neighbors do not directly support this mechanism, but the general concept of two-stage speech enhancement processes is established.
- Break condition: If the regression mode does not effectively reduce noise or the diffusion mode does not effectively reduce artifacts, the two-stage process will not improve speech quality.

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs)
  - Why needed here: SDEs are used to model the forward and reverse processes in diffusion-based speech enhancement.
  - Quick check question: What is the difference between the drift coefficient and the diffusion coefficient in an SDE?

- Concept: Score-based generative modeling
  - Why needed here: The model is trained to approximate the score function ∇xt log pt(xt) using denoising score matching.
  - Quick check question: How does denoising score matching work in the context of score-based generative modeling?

- Concept: Brownian bridge process
  - Why needed here: The Brownian bridge process is used to enable both regression and diffusion modes without additional parameters by setting t close to 1 for regression mode.
  - Quick check question: What is the distinguishing feature of the Brownian bridge process that allows it to perform as a regression model at t = 1?

## Architecture Onboarding

- Component map:
  - NCSN++ (Neural Conditional Score Network) -> Brownian bridge process SDE -> Modified Euler-Maruyama inference
  - Two-stage process: regression mode output → diffusion mode refinement

- Critical path:
  1. Train the model to predict clean speech x0 instead of the score function
  2. Use the Brownian bridge process to enable both regression and diffusion modes
  3. Apply the regression mode by setting t close to 1 to predict clean speech directly
  4. Apply the diffusion mode by computing the score function and performing the reverse diffusion process

- Design tradeoffs:
  - Predicting clean speech instead of the score function allows for regression mode but may require more complex training
  - Using the Brownian bridge process enables both modes without additional parameters but may introduce numerical instability issues when t approaches 1

- Failure signatures:
  - Poor performance in regression mode when t is close to 1
  - Numerical instability during training when σ(t) is close to 0
  - Artifacts introduced by the regression mode that are not effectively reduced by the diffusion mode

- First 3 experiments:
  1. Train the model to predict clean speech x0 instead of the score function and evaluate performance in both regression and diffusion modes
  2. Vary the interpolation weight α between the regression mode output and the original noisy input to find the optimal value for reducing artifacts
  3. Compare the performance of the model with different numbers of reverse diffusion steps to find the minimum number of steps required for acceptable performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal interpolation weight α for blending regression and diffusion outputs, and how does this vary with different noise types and signal-to-noise ratios?
- Basis in paper: [explicit] The paper states "This indicates that the regression mode generated excessive artifacts for the diffusion mode to refine. Despite this, the diffusion mode could still effectively eliminate artifacts when a sufficient degree of noisy speech y was added to reduce the artifacts" and shows that performance degrades when α > 0.8.
- Why unresolved: The paper only tested α values up to 1.0 and found optimal performance around 0.5-0.8, but did not systematically explore the full parameter space across different noise conditions or provide theoretical justification for why this range works best.
- What evidence would resolve it: A comprehensive study varying α across different noise types (stationary vs non-stationary), SNR levels, and speech characteristics, combined with theoretical analysis of the signal decomposition in the regression-diffusion pipeline.

### Open Question 2
- Question: How does the Brownian bridge process compare to other SDE formulations for speech enhancement when the goal is single-step inference, and what are the fundamental limitations of this approach?
- Basis in paper: [explicit] The paper claims "Its distinguishing feature is that it can linearly transform between the initial state (x0) with zero variance to the noisy speech y with zero variance, offering a capability to perform as a regression model at t = 1 (deterministic mode)" and compares against BBDE which used the same SDE but predicted scores instead.
- Why unresolved: The paper only compares against one alternative SDE formulation (SGMSE+) rather than systematically evaluating other possible SDEs that might offer better single-step performance or different trade-offs between regression and diffusion modes.
- What evidence would resolve it: A systematic comparison of multiple SDE formulations (Ornstein-Uhlenbeck, Variance Exploding, etc.) under the single-step constraint, measuring not just final quality metrics but also training stability, sensitivity to hyperparameters, and generalization across domains.

### Open Question 3
- Question: Can the Thunder framework be extended to handle more complex speech enhancement tasks such as dereverberation, noise removal, and source separation simultaneously, and what architectural modifications would be required?
- Basis in paper: [explicit] The paper concludes "For future work, we plan to extend Thunder to cover more general settings such as dereverberation."
- Why unresolved: The paper only demonstrates performance on single-microphone speech enhancement and mentions dereverberation as future work without exploring how the Brownian bridge framework might need to be modified for multi-task scenarios or multi-channel processing.
- What evidence would resolve it: Experimental validation of Thunder on multi-task datasets combining noise and reverberation, analysis of whether the Brownian bridge process remains appropriate for non-additive distortions, and exploration of whether the regression-diffusion paradigm extends to multi-channel or multi-source scenarios.

## Limitations

- The Brownian bridge process's effectiveness in enabling both regression and diffusion modes relies on theoretical convergence that may not hold in all practical scenarios.
- The two-stage process (regression followed by diffusion) introduces complexity that may not be necessary for all noise types or SNR conditions.
- The claim of superior out-of-domain generalization requires more extensive validation across diverse acoustic conditions and speech characteristics.

## Confidence

- **High Confidence**: The paper demonstrates improved performance metrics (PESQ 2.99, ESTOI 0.87, SI-SDR 19.6) with single-step inference compared to multi-step diffusion baselines. The reduction in model parameters (half the size of comparable diffusion models) is verifiable through the architecture description.
- **Medium Confidence**: The mechanism of using Brownian bridge process to enable both regression and diffusion modes is theoretically sound, but the empirical evidence for its effectiveness in speech enhancement is limited to one dataset. The claim of superior out-of-domain generalization on LibriFSD50k requires more extensive validation across diverse acoustic conditions.
- **Low Confidence**: The assertion that predicting clean speech directly is the primary factor for avoiding gradient instability near t=1 is not fully substantiated. Alternative explanations, such as the specific implementation of the Brownian bridge process or training hyperparameters, may contribute equally to the observed performance.

## Next Checks

1. **Gradient Stability Analysis**: Conduct ablation studies to isolate the effect of predicting clean speech versus score function on gradient stability during training. Measure the gradient norm and loss convergence across different t values, particularly near t=1, to verify the claimed mechanism.

2. **Brownian Bridge Process Validation**: Test the model's performance with alternative SDEs (e.g., Ornstein-Uhlenbeck process) to determine if the Brownian bridge process is essential for achieving single-step performance. Compare the drift coefficient behavior across different SDEs when t approaches 1.

3. **Two-Stage Process Investigation**: Evaluate the individual contributions of the regression and diffusion modes by testing the model in isolation (pure regression mode and pure diffusion mode) and in combination with varying interpolation weights. Measure the trade-off between noise reduction and artifact introduction to validate the claimed benefits of the two-stage approach.