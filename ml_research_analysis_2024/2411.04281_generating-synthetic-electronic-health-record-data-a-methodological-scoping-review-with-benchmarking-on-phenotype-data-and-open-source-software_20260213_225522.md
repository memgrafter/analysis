---
ver: rpa2
title: 'Generating Synthetic Electronic Health Record Data: a Methodological Scoping
  Review with Benchmarking on Phenotype Data and Open-Source Software'
arxiv_id: '2411.04281'
source_url: https://arxiv.org/abs/2411.04281
tags:
- data
- synthetic
- methods
- real
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive scoping review and benchmarking
  of synthetic EHR generation methods, addressing the gap in systematic evaluation
  of diverse approaches beyond GAN-based models. The study reviews 48 methods, categorizing
  them into rule-based, GAN-based, VAE-based, transformer-based, and diffusion-based
  approaches.
---

# Generating Synthetic Electronic Health Record Data: a Methodological Scoping Review with Benchmarking on Phenotype Data and Open-Source Software

## Quick Facts
- arXiv ID: 2411.04281
- Source URL: https://arxiv.org/abs/2411.04281
- Reference count: 40
- Seven open-source methods benchmarked on MIMIC-III/IV datasets

## Executive Summary
This paper addresses the critical need for systematic evaluation of synthetic EHR generation methods by conducting a comprehensive scoping review and benchmarking study. The authors identify 48 synthetic EHR generation methods and categorize them into five main approaches: rule-based, GAN-based, VAE-based, transformer-based, and diffusion-based. Through extensive benchmarking on MIMIC-III and MIMIC-IV datasets using seven open-source methods, the study reveals that GAN-based methods generally outperform others in fidelity and utility metrics, while rule-based methods excel in privacy protection. The research introduces SynthEHRella, an open-source Python package that enables streamlined evaluation of synthetic EHR methods across multiple metrics.

## Method Summary
The study conducts a scoping review of 48 synthetic EHR generation papers, selecting seven open-source methods representing all major categories for benchmarking. Methods are trained on MIMIC-III and evaluated on both MIMIC-III and MIMIC-IV to assess transportability. The evaluation framework includes four metric categories: fidelity (MMD, RMSPE, MAPE, CFD, COFD, discriminative prediction AUC/ACC), utility (analytical: logistic regression estimates; predictive: TSTR, TSRTR, TRTR scenarios with AUC/ACC), privacy (MIR mean/median, AIR F1-score), and computational cost. The authors implement a unified evaluation pipeline using the open-source SynthEHRella package and provide a decision tree to guide method selection based on application requirements.

## Key Results
- PBR best preserves real data distributions but lacks correlation modeling
- CorGAN excels in preserving distributional information and analytical utility
- MedGAN is optimal for predictive modeling
- GAN-based methods are most computationally efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PBR preserves marginal phenotype prevalence most accurately among all methods.
- Mechanism: By sampling each phenotype independently based on its empirical prevalence in the training data, PBR replicates the univariate distribution exactly without modeling inter-phenotype dependencies.
- Core assumption: Marginal prevalence is the primary fidelity requirement and preserving it alone is sufficient for downstream use cases.
- Evidence anchors:
  - [abstract] "PBR best preserves real data distributions but lacks correlation modeling"
  - [section] "PBR outperforms other methods on most metrics, except for its higher CFD score (46.9), which reflects its inability to model bivariate correlations by design"
  - [corpus] Weak - no direct citations supporting this claim in related papers.
- Break condition: If downstream tasks require realistic phenotype co-occurrence patterns, PBR will fail despite accurate marginal preservation.

### Mechanism 2
- Claim: CorGAN best preserves bivariate correlations and analytical utility.
- Mechanism: CorGAN explicitly models the correlation structure by learning the covariance matrix of phenotypes during training, then generating synthetic samples that respect this learned correlation structure.
- Core assumption: Preserving correlation structure is more important than exact marginal prevalence for many analytical tasks.
- Evidence anchors:
  - [abstract] "CorGAN excels in preserving distributional information and analytical utility"
  - [section] "CorGAN preserves distributional information best among the selected methods, particularly excelling in CFD due to its design for modeling associations"
  - [corpus] Weak - related papers focus on GAN architectures but don't specifically validate correlation preservation.
- Break condition: If the training and testing populations have significant distributional shifts, CorGAN's correlation structure may not generalize well.

### Mechanism 3
- Claim: MedGAN provides the best predictive utility despite lower fidelity in correlation preservation.
- Mechanism: MedGAN uses a GAN architecture that focuses on generating realistic binary vectors through adversarial training, which captures the overall data structure needed for predictive modeling even if fine-grained correlations are less accurate.
- Core assumption: For predictive tasks, capturing the overall data manifold is more important than preserving exact correlation structure.
- Evidence anchors:
  - [abstract] "MedGAN is optimal for predictive modeling"
  - [section] "MedGAN shows the smallest performance drop in AUC on both datasets, with a decrease of 0.063 on MIMIC-III and 0.076 on MIMIC-IV"
  - [corpus] Weak - no direct citations supporting this specific predictive advantage claim.
- Break condition: If the task requires accurate estimation of phenotype associations rather than prediction, MedGAN's lower correlation fidelity becomes a limitation.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD is the primary metric for measuring distributional distance between real and synthetic data at the marginal level.
  - Quick check question: What does a lower MMD value indicate about the similarity between real and synthetic data distributions?

- Concept: Correlation Frobenius Distance (CFD)
  - Why needed here: CFD quantifies how well synthetic data preserves the correlation structure between phenotypes, which is critical for analytical tasks.
  - Quick check question: Why might a method with good MMD performance still have poor CFD performance?

- Concept: Transportability in synthetic data
  - Why needed here: The study evaluates whether methods trained on MIMIC-III generalize to MIMIC-IV, which has overlapping but distinct populations.
  - Quick check question: What would it mean if a method performs well on MIMIC-III but poorly on MIMIC-IV?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (varies by method) -> Synthetic data generation -> Evaluation (fidelity, utility, privacy metrics)
- Critical path: Data preprocessing → Model training → Synthetic generation → Evaluation → Method selection
- Design tradeoffs: Fidelity vs privacy (better fidelity often means higher re-identification risk), marginal accuracy vs correlation preservation, computational cost vs performance
- Failure signatures: High CFD with low MMD indicates good marginal preservation but poor correlation modeling; poor TSTR performance indicates synthetic data may not substitute for real data in predictive tasks
- First 3 experiments:
  1. Run PBR baseline to establish upper bound for marginal prevalence preservation
  2. Compare MedGAN vs CorGAN on TSTR predictive utility task
  3. Evaluate privacy metrics (MIR, AIR) for methods with best fidelity scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for synthetic EHR generation when there is a distributional shift between training and testing populations, considering both fidelity and utility metrics?
- Basis in paper: [explicit] The paper identifies that GAN-based methods excel when distributional shifts exist between training and testing populations, but does not definitively determine the single best method across all metrics.
- Why unresolved: While the paper provides a decision tree and ranks methods across different metrics, it does not identify a single optimal method that consistently performs best across all scenarios involving distributional shifts.
- What evidence would resolve it: Comparative analysis of all benchmarked methods across multiple datasets with varying degrees of distributional shift, measuring performance on a composite metric that balances fidelity, utility, and privacy.

### Open Question 2
- Question: How do privacy concerns impact the choice of synthetic EHR generation methods across different application scenarios?
- Basis in paper: [explicit] The paper identifies a trade-off between privacy and other metrics (fidelity, utility) and notes that rule-based methods excel in privacy protection, but does not provide clear guidance on when privacy should be prioritized over other metrics.
- Why unresolved: The paper acknowledges the importance of privacy but does not establish clear thresholds or decision criteria for when privacy concerns should outweigh fidelity and utility considerations in different use cases.
- What evidence would resolve it: Empirical studies quantifying the real-world consequences of privacy breaches in synthetic EHR data across different applications, combined with user studies to determine acceptable privacy-utility trade-offs.

### Open Question 3
- Question: What are the most effective evaluation metrics for assessing the quality of multimodal and longitudinal synthetic EHR data?
- Basis in paper: [inferred] The paper focuses on cross-sectional phenotype data and identifies the need for comprehensive benchmarking of longitudinal and multimodal methods, suggesting current evaluation metrics may be insufficient for these complex data types.
- Why unresolved: Current evaluation metrics are primarily designed for cross-sectional, unimodal data. The paper identifies this gap but does not propose or validate new metrics specifically designed for multimodal and longitudinal EHR data.
- What evidence would resolve it: Development and validation of new evaluation metrics that capture the unique characteristics of multimodal and longitudinal EHR data, tested across multiple synthetic data generation methods and real-world datasets.

## Limitations
- Evaluation based solely on MIMIC-III/IV datasets limits generalizability to other EHR systems and populations
- Focus on binary phenotypes excludes time-series, free-text, and imaging data that comprise significant portions of real-world EHRs
- Privacy evaluation using MIR/AIR metrics may underestimate re-identification risk for smaller datasets or rare phenotypes

## Confidence
- High Confidence: Method categorization and seven-method benchmark selection are well-supported by the scoping review and transparent methodology
- Medium Confidence: Performance rankings within each metric category are reproducible given the standardized evaluation framework
- Low Confidence: Generalizability of findings beyond MIMIC datasets, the completeness of privacy risk assessment, and the decision tree's applicability to novel use cases

## Next Checks
1. **Cross-dataset validation**: Replicate the benchmark on at least two additional EHR datasets from different healthcare systems to assess method performance stability across population distributions and data collection practices.

2. **Privacy stress testing**: Conduct membership inference attacks using real patient identifiers (where permissible) or synthetic patient populations with known ground truth to validate MIR/AIR metric effectiveness in detecting actual re-identification vulnerabilities.

3. **Longitudinal extension validation**: Evaluate the selected methods on longitudinal EHR sequences rather than cross-sectional phenotype prevalence to determine how well they preserve temporal dependencies and patient trajectory patterns.