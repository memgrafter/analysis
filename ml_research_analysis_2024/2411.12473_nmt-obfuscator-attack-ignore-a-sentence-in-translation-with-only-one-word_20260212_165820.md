---
ver: rpa2
title: 'NMT-Obfuscator Attack: Ignore a sentence in translation with only one word'
arxiv_id: '2411.12473'
source_url: https://arxiv.org/abs/2411.12473
tags:
- adversarial
- attack
- translation
- sentence
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NMT-Obfuscator, a novel adversarial attack
  against Neural Machine Translation (NMT) models that inserts a single carefully
  chosen word between two sentences to prevent the model from translating the second
  sentence. The attack uses gradient projection in the embedding space of the NMT
  model to find an obfuscator word that maintains natural language fluency while causing
  the translation to ignore the target sentence.
---

# NMT-Obfuscator Attack: Ignore a sentence in translation with only one word

## Quick Facts
- **arXiv ID**: 2411.12473
- **Source URL**: https://arxiv.org/abs/2411.12473
- **Reference count**: 5
- **Primary result**: A novel adversarial attack that inserts one word between sentences to prevent NMT models from translating the second sentence while maintaining natural language fluency

## Executive Summary
This paper introduces NMT-Obfuscator, a white-box adversarial attack against Neural Machine Translation models that uses gradient projection in the embedding space to find a single word that, when inserted between two sentences, causes the model to ignore the second sentence. The attack maintains natural language fluency as measured by low perplexity scores while achieving over 50% attack success rate. Experiments against Marian and mBART50 models show the method outperforms prior approaches like Suffix-Dropper in preserving fluency while achieving comparable attack success rates.

## Method Summary
The NMT-Obfuscator attack uses a three-step iterative process: (1) gradient descent in the embedding space of the target NMT model to minimize the difference between adversarial and original translations, (2) projection of the resulting embedding to find the k nearest valid tokens, and (3) selection of the token that minimizes the loss of a pre-trained language model (GPT-2) to ensure natural fluency. The method requires white-box access to the NMT model and uses Adam optimizer with learning rate 0.04 for up to 100 iterations. The attack is evaluated on WMT14 English-to-French and English-to-German test sets, with success measured by Attack Success Rate (ASR), BLEU score, BERTScore, and perplexity.

## Key Results
- Successfully prevents translation of target sentences in over 50% of cases against Marian and mBART50 models
- Maintains high natural language fluency with perplexity scores close to the original text
- Outperforms prior approaches like Suffix-Dropper in preserving translation quality of non-target sentences
- The inserted word creates a natural boundary that NMT models interpret as sentence segmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The obfuscator word creates a natural language boundary that NMT models interpret as sentence segmentation, causing them to drop the second sentence
- Mechanism: When a single word is inserted between two sentences, the NMT model's attention mechanism and encoder-decoder architecture treat the boundary as a natural sentence break. The model then only translates up to this boundary point, effectively ignoring the target sentence that follows.
- Core assumption: NMT models rely heavily on contextual cues and natural language boundaries to determine translation scope, and a single natural word can create sufficient ambiguity to truncate translation.
- Evidence anchors: [abstract] "The word added between the two sentences is such that the whole adversarial text is natural in the source language." [section 3.1] "the final adversarial input should be natural" and "we use the loss of a pre-trained language model as a rough measure for the fluency of the final adversarial text"
- Break condition: If the NMT model implements explicit sentence boundary detection or uses very long context windows that don't rely on natural language cues for segmentation

### Mechanism 2
- Claim: Gradient projection in embedding space finds an obfuscator that minimally perturbs the natural language distribution while maximally disrupting translation
- Mechanism: The attack uses gradient descent in the continuous embedding space to minimize the difference between adversarial and original translations while projecting back to valid vocabulary tokens that maintain natural language fluency. This creates a token that appears natural but has specific semantic properties that confuse the model.
- Core assumption: Small perturbations in embedding space can produce words that maintain natural language properties while having disproportionate effects on model behavior.
- Evidence anchors: [section 3.2] "We use gradient projection in the embedding space of the target NMT model" and "we find the gradients of Ladv with respect to the embedding of ω" [section 3.2] "we find k nearest tokens as follows" and "Amongst these k candidate tokens, we choose the one that results in minimum loss of a pre-trained language model"
- Break condition: If the NMT model uses non-embedding-based representations or has robust embedding spaces that don't respond to small perturbations

### Mechanism 3
- Claim: The attack exploits the NMT model's inability to maintain context across natural language boundaries when processing adversarially crafted inputs
- Mechanism: Modern NMT models, particularly those using transformer architectures, have limited effective context windows and attention mechanisms that struggle with abrupt semantic shifts. The obfuscator creates an artificial context boundary that the model cannot bridge, causing it to reset its translation state.
- Core assumption: NMT models have inherent limitations in maintaining long-range dependencies and handling semantic discontinuities, which can be exploited through carefully crafted inputs.
- Evidence anchors: [abstract] "Our attack can successfully force the NMT models to ignore the second part of the input in the translation for more than 50% of all cases" [section 4.2] "the BLEU score and BERTScore are near perfect in all cases, which confirms that the translation of the adversarial text...is very similar to the translation of the input sentence"
- Break condition: If the NMT model implements enhanced context modeling techniques or has mechanisms to handle semantic discontinuities

## Foundational Learning

- Concept: Neural Machine Translation Architecture
  - Why needed here: Understanding how NMT models process input sequences and maintain context is crucial for comprehending why a single word can disrupt translation
  - Quick check question: How do transformer-based NMT models typically handle sequence processing, and what architectural components could be vulnerable to boundary-based attacks?

- Concept: Adversarial Machine Learning Fundamentals
  - Why needed here: The attack uses gradient-based optimization to find adversarial examples, requiring understanding of how gradients can be used to craft inputs that fool models
  - Quick check question: What is the fundamental difference between white-box and black-box adversarial attacks, and why does this attack require white-box access?

- Concept: Natural Language Processing and Fluency Metrics
  - Why needed here: The attack must maintain natural language fluency while disrupting translation, requiring understanding of how language models measure fluency and perplexity
  - Quick check question: How does a pre-trained language model like GPT-2 measure sentence fluency, and why is this important for making the attack stealthy?

## Architecture Onboarding

- Component map: Input sentences -> Embedding conversion -> Gradient optimization -> Token projection -> Perplexity check -> Translation generation -> Evaluation
- Critical path: Input sentences → Embedding conversion → Gradient optimization → Token projection → Perplexity check → Translation generation → Evaluation
- Design tradeoffs:
  - White-box access requirement vs. attack effectiveness
  - Natural language fluency vs. attack success rate
  - Computational cost of gradient optimization vs. attack quality
  - Number of neighbors (k) vs. search space coverage
- Failure signatures:
  - High perplexity scores indicate unnatural adversarial text
  - Low attack success rates suggest model robustness
  - Long optimization times may indicate difficult optimization landscape
  - Inconsistent results across different target sentences
- First 3 experiments:
  1. Test attack success rate on a simple Marian NMT model with predefined obfuscator words to establish baseline effectiveness
  2. Vary the number of neighbors (k) in the projection step to find optimal balance between search space and computational cost
  3. Test different target sentence lengths to understand how sentence complexity affects attack success rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the NMT-Obfuscator attack perform against real-world NMT systems like Google Translate or DeepL when deployed in production environments with potential defenses?
- Basis in paper: [explicit] The paper evaluates against Marian NMT and mBART50 models, and provides one example against DeepL API, but doesn't systematically test production systems
- Why unresolved: Production NMT systems may have different architectures, training data, and potential defenses that weren't tested
- What evidence would resolve it: Systematic evaluation of NMT-Obfuscator against major commercial NMT APIs under various conditions and potential defense mechanisms

### Open Question 2
- Question: Can the NMT-Obfuscator attack be detected automatically in real-time translation scenarios, and if so, what detection mechanisms would be most effective?
- Basis in paper: [inferred] The paper notes that adversarial examples "are stealthy and less detectable" but doesn't explore detection methods or their effectiveness
- Why unresolved: While the attack preserves fluency, the insertion of specific words that disrupt translation might create detectable patterns
- What evidence would resolve it: Development and testing of detection algorithms that could identify NMT-Obfuscator attacks with high accuracy in real-time

### Open Question 3
- Question: How effective would fine-tuning NMT models on adversarial examples (including NMT-Obfuscator attacks) be at improving their robustness against such attacks?
- Basis in paper: [explicit] The paper focuses on attacking existing models but doesn't explore defensive strategies or model robustness
- Why unresolved: The paper demonstrates vulnerability but doesn't investigate whether models can be made more robust through adversarial training
- What evidence would resolve it: Experiments showing how well NMT models retain translation quality after being fine-tuned on a dataset containing NMT-Obfuscator attacks

## Limitations
- White-box requirement significantly limits real-world applicability where black-box attacks are more relevant
- Task specificity may not generalize to other adversarial objectives or more complex translation tasks
- Computational cost of iterative gradient optimization makes it expensive compared to rule-based approaches
- Language dependency - effectiveness across different language pairs and scripts remains unverified

## Confidence

- **High confidence**: The core mechanism of using gradient projection in embedding space to find natural-looking obfuscator words is well-supported by the experimental results and aligns with established adversarial ML principles
- **Medium confidence**: The claim that a single word can effectively prevent translation of subsequent sentences relies on specific NMT architecture assumptions that may not hold across all models
- **Low confidence**: The attack's effectiveness against production-grade, defensively trained NMT systems remains untested, as the experiments used standard models without robustness training

## Next Checks

1. **Black-box adaptation test**: Implement a transferability-based black-box variant of the attack to evaluate performance when model access is restricted, using the same evaluation metrics on the WMT14 test sets

2. **Cross-lingual robustness evaluation**: Test the attack against additional language pairs (e.g., English-to-Chinese, English-to-Arabic) to assess whether the single-word obfuscation mechanism generalizes beyond Indo-European languages

3. **Production model stress test**: Evaluate attack success rates against defensively distilled or adversarially trained NMT models to determine real-world robustness, comparing ASR, perplexity, and fluency metrics against the baseline Marian and mBART50 models