---
ver: rpa2
title: LLMs Are Few-Shot In-Context Low-Resource Language Learners
arxiv_id: '2403.16512'
source_url: https://arxiv.org/abs/2403.16512
tags:
- languages
- alignment
- label
- language
- x-icl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines in-context learning (ICL) for low-resource languages,
  identifying that label alignment in cross-lingual ICL (X-ICL) often degrades performance.
  The authors propose query alignment as a more effective alternative, which improves
  understanding by aligning the semantics of input examples.
---

# LLMs Are Few-Shot In-Context Low-Resource Language Learners

## Quick Facts
- arXiv ID: 2403.16512
- Source URL: https://arxiv.org/abs/2403.16512
- Authors: Samuel Cahyawijaya; Holy Lovenia; Pascale Fung
- Reference count: 40
- This work examines in-context learning (ICL) for low-resource languages, identifying that label alignment in cross-lingual ICL (X-ICL) often degrades performance.

## Executive Summary
This paper investigates in-context learning (ICL) for low-resource languages, revealing that traditional label alignment approaches in cross-lingual ICL often underperform. The authors propose query alignment as a more effective alternative, which improves understanding by aligning the semantics of input examples. Through comprehensive experiments across 32 languages and 4 datasets, the study demonstrates that uniform source labels outperform label alignment, and cross-lingual semantic similarity improves exemplar selection. The findings suggest that few-shot ICL can effectively bridge semantic and linguistic gaps for low-resource language understanding.

## Method Summary
The study evaluates cross-lingual in-context learning (X-ICL) effectiveness for low-resource languages using 25 low-resource and 7 higher-resource languages across 4 datasets. Experiments employ three-shot in-context learning with XGLM-7.5B and BLOOM-7B models, testing different alignment formats (alignment-after, alignment-before, tabular), label configurations (source-only, target-only, label alignment), and retrieval methods (random, cross-lingual semantic similarity, translation semantic similarity). Performance is measured using weighted F1 scores, comparing zero-shot, monolingual ICL, X-ICL with different configurations, and machine translation baselines.

## Key Results
- Uniform source labels outperform label alignment in cross-lingual ICL
- Query alignment significantly improves cross-lingual ICL performance compared to label alignment
- Cross-lingual semantic similarity models improve exemplar retrieval quality and downstream performance

## Why This Works (Mechanism)

### Mechanism 1
Label alignment in cross-lingual ICL often degrades performance because it shifts the semantic space of labels from source to target language without guiding the model on how to interpret the target-language query. The model understands label semantics in the target language but lacks alignment between the query in the target language and the exemplars in the source language, leading to degraded performance. Core assumption: Label alignment text alone is insufficient for bridging the semantic gap between the query and exemplars.

### Mechanism 2
Query alignment improves cross-lingual ICL by providing alignment of input distribution, allowing the model to understand the query in the target language as well as in the source language. The model aligns the query in the source language to the target language, enabling better understanding of the query in both languages, leading to performance similar to monolingual ICL. Core assumption: Aligning the query in the source language to the target language is more effective than aligning the labels.

### Mechanism 3
Cross-lingual semantic similarity improves exemplar retrieval quality, leading to better cross-lingual ICL performance. The model retrieves exemplars that are semantically similar to the query across languages, improving the quality of in-context learning. Core assumption: Cross-lingual semantic similarity models can effectively retrieve semantically relevant exemplars across languages.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the foundation for understanding how LLMs can perform tasks using short in-context information without parameter updates.
  - Quick check question: Can you explain how ICL differs from traditional fine-tuning methods?

- Concept: Cross-lingual in-context learning (X-ICL)
  - Why needed here: X-ICL extends ICL to cross-lingual tasks, allowing LLMs to transfer task understanding from a high-resource source language to a low-resource target language.
  - Quick check question: What are the key challenges in implementing X-ICL for low-resource languages?

- Concept: Semantic similarity
  - Why needed here: Semantic similarity is crucial for exemplar retrieval and alignment, ensuring that the model retrieves and aligns semantically relevant information across languages.
  - Quick check question: How does cross-lingual semantic similarity differ from monolingual semantic similarity?

## Architecture Onboarding

- Component map:
  Cross-lingual semantic similarity -> Exemplar retrieval -> In-context alignment (label/query) -> Prompt formatting -> ICL task execution

- Critical path:
  1. Retrieve semantically similar exemplars using cross-lingual semantic similarity
  2. Apply in-context alignment (label or query alignment) to bridge the semantic gap
  3. Format the prompt for consistency and clarity
  4. Execute the ICL task and evaluate performance

- Design tradeoffs:
  - Label alignment vs. query alignment: Label alignment may shift the semantic space without guiding the query, while query alignment provides better input distribution alignment but may require more parallel data.
  - Alignment formatting: Higher consistency in formatting may improve performance for high-resource languages but may not benefit low-resource languages as much.

- Failure signatures:
  - Degraded performance due to ineffective label alignment
  - Poor exemplar retrieval due to low cross-lingual semantic similarity model performance
  - Ineffective query alignment due to lack of parallel data

- First 3 experiments:
  1. Compare label alignment vs. query alignment on a small set of low-resource languages to evaluate performance differences
  2. Test different cross-lingual semantic similarity models (e.g., XLMR STS, LaBSE) for exemplar retrieval
  3. Evaluate the impact of prompt formatting consistency on ICL performance for both high-resource and low-resource languages

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of cross-lingual in-context learning (X-ICL) scale with increasing model size, particularly for low-resource languages? Basis in paper: [inferred] The paper suggests that larger multilingual LLMs might exhibit different scaling behaviors for low-resource languages, but this was not directly tested due to computational constraints. Why unresolved: The authors were limited by computational resources and could only experiment with smaller models (XGLM-7.5B and BLOOM-7B). What evidence would resolve it: Experimental results comparing X-ICL performance across a range of model sizes, from small to very large multilingual LLMs, on a diverse set of low-resource languages.

### Open Question 2
What is the impact of using parallel corpora from culturally relevant sources versus general parallel corpora on the effectiveness of in-context query alignment for low-resource languages? Basis in paper: [explicit] The paper mentions the potential benefit of using culturally relevant parallel corpora like Bloom Library, but does not empirically test this hypothesis. Why unresolved: The authors did not have access to or did not experiment with culturally specific parallel corpora. What evidence would resolve it: Comparative experiments using both general and culturally specific parallel corpora for in-context query alignment, measuring downstream task performance on low-resource languages.

### Open Question 3
How does the choice of cross-lingual semantic similarity model affect the performance of X-ICL across different language families and resource levels? Basis in paper: [explicit] The paper compares different cross-lingual semantic similarity models and finds that their effectiveness varies depending on the language, but does not provide a comprehensive analysis across language families. Why unresolved: The analysis was limited to a subset of languages and did not systematically explore the relationship between model choice and language family or resource level. What evidence would resolve it: A systematic study comparing the performance of different cross-lingual semantic similarity models across multiple language families and resource levels, identifying which models work best for specific language groups.

## Limitations

- Experimental scope covers only 32 languages, which may not be representative of full diversity in low-resource language scenarios
- Limited to three-shot setting without exploring how sample size affects different alignment strategies
- Results may be constrained by the quality of machine translation, which varies significantly across language pairs

## Confidence

**High Confidence**: The observation that uniform source labels outperform label alignment in cross-lingual ICL has strong empirical support across multiple languages and datasets. The finding that prompt formatting consistency doesn't consistently benefit low-resource languages is also well-supported by experimental results.

**Medium Confidence**: The claim that query alignment significantly improves performance compared to label alignment is supported by the results, but the effect size varies considerably across languages. The assertion that cross-lingual semantic similarity improves exemplar retrieval quality is plausible but based on limited comparisons with translation-based methods.

**Low Confidence**: The generalization of findings to languages outside the 32 studied languages requires caution, as the study doesn't systematically explore how results might differ for languages with different typological features or script systems.

## Next Checks

1. **Cross-Lingual Semantic Similarity Model Comparison**: Conduct controlled experiments comparing XLMR STS and LaBSE performance across different language families and language pairs to determine which model performs better for specific linguistic contexts, and test additional semantic similarity models not included in the original study.

2. **Sample Size Sensitivity Analysis**: Systematically vary the number of few-shot examples (from 1 to 10) for both label alignment and query alignment approaches to determine how sample size affects the relative performance of different alignment strategies, particularly for low-resource languages.

3. **Machine Translation Quality Impact Study**: Measure the quality of machine translation (using metrics like chrF++) for each language pair used in the experiments and analyze how MT quality correlates with downstream ICL performance, identifying thresholds below which alignment methods break down.