---
ver: rpa2
title: A Practitioner's Guide to Continual Multimodal Pretraining
arxiv_id: '2408.14471'
source_url: https://arxiv.org/abs/2408.14471
tags:
- continual
- learning
- pretraining
- arxiv
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FoMo-in-Flux is a new benchmark for controlled continual multimodal
  pretraining, comprising 63 datasets with over 2.5M image-text pairs. The key finding
  is that simple fine-tuning coupled with model merging offers the most promise for
  continual model pretraining across longer update cycles, outperforming other approaches
  like parameter-efficient tuning and continual learning methods.
---

# A Practitioner's Guide to Continual Multimodal Pretraining

## Quick Facts
- **arXiv ID**: 2408.14471
- **Source URL**: https://arxiv.org/abs/2408.14471
- **Reference count**: 40
- **Primary result**: Simple fine-tuning coupled with model merging offers the most promise for continual multimodal pretraining across longer update cycles, outperforming other approaches.

## Executive Summary
This paper introduces FoMo-in-Flux, a new benchmark for controlled continual multimodal pretraining, comprising 63 datasets with over 2.5M image-text pairs. The authors systematically evaluate various continual pretraining strategies including fine-tuning, parameter-efficient tuning, continual learning methods, and model merging techniques. The key finding is that simple fine-tuning combined with model merging achieves the best balance of knowledge accumulation and retention over long update cycles. The work provides practical insights into learning rate schedules, compute budgets, and model scaling effects for real-world continual pretraining scenarios.

## Method Summary
The authors introduce FoMo-in-Flux, a benchmark for continual multimodal pretraining that controls for compute budget, pretraining data pool, and update sequence. They evaluate multiple adaptation methods including simple fine-tuning, LoRA, EWC, replay-based methods, and various model merging strategies across different compute budgets and model sizes. The benchmark uses Memory-Adjusted FLOPs (MAFs) as a fair metric accounting for both FLOPs and memory usage. Experiments are conducted on CLIP models trained on LAION-2B, with continual updates on diverse image-text datasets.

## Key Results
- Simple fine-tuning coupled with model merging (especially EMA-Merge and ZeroShot-Merge) achieves the best balance of knowledge accumulation and retention
- Learning rate schedules and meta-schedules significantly impact continual pretraining performance, with meta-schedules improving retention
- Increasing model size within the same compute budget improves both accumulation and retention capabilities
- Data-centric strategies (task sequence ordering) have less impact than method-centric approaches on final performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Simple fine-tuning coupled with model merging achieves both strong knowledge accumulation and retention over long update cycles.
- **Mechanism:** Model merging (especially EMA-Merge and ZeroShot-Merge) allows integration of new task-specific knowledge while preserving the original pretraining generalization by interpolating between old and new weights.
- **Core assumption:** The interpolation weight schedule balances adaptation and retention; model merging can operate within the same compute budget as simple fine-tuning.
- **Evidence anchors:**
  - [abstract] "simple fine-tuning coupled with model merging offers the most promise for continual model pretraining across longer update cycles, outperforming other approaches"
  - [section] "Judiciously merging model weights exhibits unique long-horizon continual pretraining behaviour, allowing for significantly consistent accumulation across update tasks with maximal retention."
- **Break condition:** If interpolation weights are poorly chosen (e.g., too high w reduces accumulation, too low increases forgetting), or if the compute budget is insufficient for both fine-tuning and merging operations.

### Mechanism 2
- **Claim:** Meta learning rate schedules that adapt ηmax based on cumulative training steps improve retention without sacrificing accumulation.
- **Mechanism:** Autoregressive meta-schedules compute ηmax for each task by extending the cosine schedule across all previous tasks, keeping the effective learning rate higher later in the sequence and mitigating forgetting.
- **Core assumption:** The pretraining learning rate schedule is known and can be extended; maintaining a higher ηmax later in training reduces the stability gap.
- **Evidence anchors:**
  - [abstract] "Learning rates matter, and can naturally be accounted for in long-horizon continual pretraining via meta learning rate schedules across incoming tasks."
  - [section] "Switching from task-independent to meta learning rate schedules notably changes the accumulation versus retention tradeoff behaviour... meta-schedules allow for significantly better retention of initial zero-shot transfer performance."
- **Break condition:** If the pretraining LR schedule is unknown or differs significantly from the assumed cosine schedule; if the meta-schedule becomes too aggressive and causes instability.

### Mechanism 3
- **Claim:** Increasing model size improves the ability to accumulate new knowledge while retaining pretraining performance.
- **Mechanism:** Larger models have greater capacity to represent both original and new concepts, reducing interference between tasks and enabling positive backward transfer.
- **Core assumption:** The increase in parameter count does not proportionally increase overfitting risk; the pretraining data coverage scales with model capacity.
- **Evidence anchors:**
  - [abstract] "increasing model size helps it acquire new knowledge while retaining its foundational properties, even within the same compute budget."
  - [section] "Scaling Model Size... with a controlled increase of model size, the ability to continually pretrain over longer minor update cycles improves... zero-shot retention AZS improves."
- **Break condition:** If the model becomes too large relative to the available compute budget, causing training instability or excessive memory use; if the pretraining data is insufficient to leverage the added capacity.

## Foundational Learning

- **Concept: Memory-Adjusted FLOPs (MAFs)**
  - Why needed here: Provides a fair, practical metric for comparing methods under realistic compute constraints, accounting for both FLOPs and memory usage.
  - Quick check question: How would you compute MAFs for a method that uses 1.5× the memory of full fine-tuning but only 0.8× the FLOPs?

- **Concept: Continual Pretraining vs. Continual Learning**
  - Why needed here: Clarifies the distinction—continual pretraining focuses on updating foundation models over long horizons with compute constraints, whereas continual learning often assumes small-scale, rehearsal-free updates.
  - Quick check question: What key difference in data access distinguishes continual pretraining from standard continual learning?

- **Concept: Data-Centric vs. Method-Centric Strategies**
  - Why needed here: Shows that the ordering and composition of update tasks (data-centric) can be as important as the adaptation method (method-centric) for balancing accumulation and retention.
  - Quick check question: Which data stream ordering (easy-to-hard, concept-frequency, similarity, random) is expected to yield the best initial zero-shot retention?

## Architecture Onboarding

- **Component map:** Base CLIP model (ViT-B/16 pretrained on LAION-2B) -> Pretraining data pool P (e.g., LAION-400M subset) -> Task update pools Dj (new adaptation datasets) -> Replay buffer B (stores all past Dj) -> Adaptation method M (fine-tuning, LoRA, EWC, model merging, etc.) -> Compute budget F (in MAFs) -> Evaluation datasets (22 held-out for zero-shot retention, 41 for accumulation)

- **Critical path:**
  1. Initialize model θ0 from pretrained checkpoint.
  2. For each task j: sample Sj from P, Dj, B; train M on Sj within budget F; add Dj to B.
  3. Evaluate on zero-shot and finetuned metrics after each task.

- **Design tradeoffs:**
  - Larger models improve retention but increase compute/memory demands.
  - More frequent updates improve responsiveness but increase forgetting risk.
  - Replay from buffer improves retention more than replay from pretraining pool.

- **Failure signatures:**
  - Rapid loss of zero-shot performance → forgetting dominates; consider model merging or increased replay.
  - Minimal knowledge gain → adaptation capacity too low; consider larger models or less restrictive methods.
  - Training instability or OOM → compute budget exceeded; reduce model size or batch size.

- **First 3 experiments:**
  1. Run simple fine-tuning on the random stream with default compute budget; verify AKA/AZS trends.
  2. Compare LoRA (r=4) vs. full fine-tuning on the same stream; measure impact on retention.
  3. Apply EMA-Merge with w=0.9 to the same stream; observe changes in retention and accumulation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pretraining data pool impact knowledge retention in continual pretraining?
- Basis in paper: [explicit] The authors find that the choice of pretraining data pool significantly impacts zero-shot retention (AZS), with DataComp-Small yielding up to 2.4% gains in AZS compared to other pools like LAION-400M and CC-3M.
- Why unresolved: While the authors observe a difference, they do not investigate the underlying reasons for this impact. They speculate that DataComp-Small's multilingual and cultural diversity might be beneficial, but this is not empirically verified.
- What evidence would resolve it: Controlled experiments ablating specific characteristics of the pretraining data pools (e.g., language diversity, image quality, concept coverage) and their impact on knowledge retention.

### Open Question 2
- Question: What is the optimal compute budget allocation for continual pretraining under realistic constraints?
- Basis in paper: [inferred] The authors show that scaling compute budgets does not benefit all methods equally, with model merging exhibiting the most favorable properties. They also find that larger compute budgets can lead to disproportionate losses in knowledge retention.
- Why unresolved: The authors do not provide a specific formula or guideline for determining the optimal compute budget allocation for a given continual pretraining task and method. They only show general trends and trade-offs.
- What evidence would resolve it: A study systematically varying compute budgets across different tasks, methods, and model sizes to derive an optimal allocation strategy based on desired knowledge accumulation and retention trade-offs.

### Open Question 3
- Question: How do different data-centric task sequences impact continual pretraining performance?
- Basis in paper: [explicit] The authors investigate six different data stream orderings (e.g., easy-to-hard, concept frequency, concept similarity) and find that they significantly impact intermediate model update stages but have minimal impact on final endpoints within the accumulation and retention space.
- Why unresolved: While the authors provide insights into the impact of different orderings, they do not explore more complex or realistic task sequences that might be encountered in real-world deployment scenarios. They also do not investigate the interaction between task sequences and other factors like learning rate schedules or model sizes.
- What evidence would resolve it: Experiments with more diverse and realistic task sequences, including those based on real-world deployment data or user feedback, and their interaction with other continual pretraining factors.

## Limitations
- Dataset and Task Generalization: The benchmark is built on publicly available datasets which may not fully represent real-world, proprietary, or domain-specific data distributions.
- Compute Budget Realism: The MAFs metric is practical but may not reflect all real-world resource constraints like variable latency or edge deployment.
- Model Architecture Bias: Findings focus on CLIP-based models and may not directly transfer to other architectures.

## Confidence
- **High Confidence**: The superiority of simple fine-tuning coupled with model merging for balancing accumulation and retention; the positive impact of increasing model size within fixed compute budgets; the benefit of meta learning rate schedules for retention.
- **Medium Confidence**: The relative performance of method-centric vs. data-centric strategies; the effectiveness of replay from task buffers versus pretraining data pools; the robustness of observed trends across different task stream orderings.
- **Low Confidence**: Generalization of findings to non-CLIP architectures; long-term stability beyond 10 update cycles; performance in highly specialized or low-resource data regimes.

## Next Checks
1. **Architecture Transfer**: Replicate the main experiments using a different multimodal architecture (e.g., BLIP, SigLIP) to assess whether the observed trends in model merging, scaling, and meta-schedules hold across model families.

2. **Long-Horizon Stability**: Extend the continual pretraining sequence to 50+ update cycles on a subset of the benchmark to evaluate whether the observed accumulation and retention trends persist or degrade over time.

3. **Domain-Specific Adaptation**: Apply the benchmarked methods to a domain-specific dataset (e.g., medical or industrial images) to test whether the same strategies for balancing accumulation and retention remain effective outside the general-purpose pretraining regime.