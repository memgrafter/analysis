---
ver: rpa2
title: 'Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for
  Multimodal Automatic Pain Assessment using Facial Videos and fNIRS'
arxiv_id: '2407.19809'
source_url: https://arxiv.org/abs/2407.19809
tags:
- pain
- fnirs
- accuracy
- assessment
- maskout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Twins-PainViT, a modality-agnostic multimodal
  framework for automatic pain assessment using facial videos and fNIRS. The approach
  employs a dual Vision Transformer (ViT) configuration with waveform representations
  of fNIRS and video embeddings, eliminating the need for domain-specific models.
---

# Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for Multimodal Automatic Pain Assessment using Facial Videos and fNIRS

## Quick Facts
- arXiv ID: 2407.19809
- Source URL: https://arxiv.org/abs/2407.19809
- Authors: Stefanos Gkikas; Manolis Tsiknakis
- Reference count: 40
- Primary result: Achieved 46.76% accuracy in multilevel pain assessment using facial videos and fNIRS

## Executive Summary
This study introduces Twins-PainViT, a modality-agnostic multimodal framework for automatic pain assessment using facial videos and fNIRS. The approach employs a dual Vision Transformer (ViT) configuration with waveform representations of fNIRS and video embeddings, eliminating the need for domain-specific models. Pre-trained using multi-task learning on diverse datasets, the framework achieved 46.76% accuracy in multilevel pain assessment. Attention maps revealed that specific neurons target distinct modalities or aspects of them, indicating holistic data analysis. The results outperformed baseline methods, highlighting the efficacy of multimodal fusion and waveform representation in advancing pain assessment. Future work should focus on refining interpretability for clinical deployment.

## Method Summary
The Twins-PainViT framework consists of two identical ViT models: PainViT-1 and PainViT-2. PainViT-1 extracts embeddings from both video frames and fNIRS channels, which are then visualized as 2D waveform diagrams. These diagrams are fed into PainViT-2 for the final pain assessment. The framework employs a cascaded attention mechanism with token mixing to capture both local and global patterns in the input data. Before training on the pain assessment task, the model is pre-trained using a multi-task learning strategy on nine diverse datasets including emotion recognition and biosignal classification tasks. Data augmentation techniques such as RandAugment, TrivialAugment, AugMix, and MaskOut are applied during fine-tuning.

## Key Results
- Achieved 46.76% accuracy in multilevel pain assessment (No Pain, Low Pain, High Pain)
- Attention maps showed specific neurons targeting distinct modalities or aspects of them
- Outperformed baseline methods in automatic pain assessment
- Demonstrated the efficacy of multimodal fusion and waveform representation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The modality-agnostic dual ViT architecture enables effective fusion of video and fNIRS data by treating both as 2D waveform images.
- **Mechanism:** The PainViT–1 model extracts embeddings from both video frames and fNIRS channels, which are then visualized as 2D waveform diagrams. These diagrams are fed into PainViT–2 for the final pain assessment. This approach eliminates the need for domain-specific models and allows the same architecture to process both modalities.
- **Core assumption:** Visual waveform representations preserve the temporal and spatial patterns in both video and fNIRS data sufficiently for the ViT to learn discriminative features.
- **Evidence anchors:**
  - [abstract] "Employing a dual ViT configuration and adopting waveform representations for the fNIRS, as well as for the extracted embeddings from the two modalities, demonstrate the efficacy of the proposed method"
  - [section] "The proposed framework, Twins-PainViT, consists of two models: PainViT–1 and PainViT–2... PainViT–1 is provided with the corresponding video frames and the visualized fNIRS channels and functions as an embedding extractor. PainViT–2 receives the visual representation of the embeddings and completes the final pain assessment."
  - [corpus] Weak evidence - no directly comparable studies found in the neighbor corpus
- **Break condition:** If the waveform representation fails to capture modality-specific patterns, the ViT cannot learn modality-discriminative features, leading to poor performance.

### Mechanism 2
- **Claim:** Multi-task pre-training on diverse datasets (emotion recognition, biosignal analysis) improves the model's ability to extract meaningful features for pain assessment.
- **Mechanism:** Twins-PainViT models are pre-trained using a multi-task learning strategy on four emotion recognition datasets (AffectNet, RAF-DB basic, RAF-DB compound, Compound FEE-DB) and five biosignal datasets (EEG-BST-SZ, Silent-EMG, BioVid ECG, EMG, GSR). This pre-training helps the model learn general feature representations that are transferable to the pain assessment task.
- **Core assumption:** Features learned from emotion recognition and biosignal analysis are relevant and transferable to pain assessment from facial videos and fNIRS.
- **Evidence anchors:**
  - [abstract] "Pre-trained using multi-task learning on diverse datasets, the framework achieved 46.76% accuracy in the multilevel pain assessment."
  - [section] "Before the automatic pain assessment training process, the Twins-PainViT models were pre-trained using a multi-task learning strategy. Four datasets, which include images for emotion assessment tasks, were employed... Additionally, five datasets containing biosignals were also utilized."
  - [corpus] Weak evidence - no directly comparable pre-training approaches found in the neighbor corpus
- **Break condition:** If the pre-training tasks are too dissimilar from pain assessment, the learned features may not transfer effectively, resulting in poor performance on the target task.

### Mechanism 3
- **Claim:** The cascaded attention mechanism with token mixing improves the model's ability to capture both local and global patterns in the input data.
- **Mechanism:** Each PainViT block features a Cascaded-Attention module that partitions the input embedding into smaller segments, each directed to a distinct attention head. This approach allows the computation to be distributed across the heads, enhancing efficiency. The Token-Mixer module, positioned before and after the Cascaded-Attention, enhances the incorporation of local structural information through depthwise convolution and batch normalization.
- **Core assumption:** The combination of cascaded attention and token mixing effectively captures both local details and global context in the input data, leading to better feature representations for pain assessment.
- **Evidence anchors:**
  - [section] "The Cascaded-Attention module employs a cascaded mechanism that partitions the full input embedding into smaller segments... This approach allows the computation to be distributed across the heads, enhancing efficiency... Regarding the attention mechanism, there is a single self-attention layer."
  - [section] "To enhance the incorporation of local structural information, the token T is processed through a depthwise convolution layer"
  - [corpus] Weak evidence - no directly comparable cascaded attention mechanisms found in the neighbor corpus
- **Break condition:** If the cascaded attention mechanism fails to effectively capture global context or the token mixing fails to preserve local information, the model's feature representations may be incomplete or noisy, leading to poor performance.

## Foundational Learning

- **Concept:** Vision Transformers (ViTs) and their attention mechanisms
  - **Why needed here:** The entire framework is built on ViT architectures, so understanding how ViTs process visual data and their attention mechanisms is crucial for implementing and modifying the model.
  - **Quick check question:** How does the multi-head self-attention mechanism in ViTs differ from traditional convolutional neural networks in processing visual data?

- **Concept:** Multimodal learning and data fusion techniques
  - **Why needed here:** The framework fuses video and fNIRS data, so understanding how to effectively combine information from different modalities is essential for improving the model's performance.
  - **Quick check question:** What are the advantages and disadvantages of early fusion versus late fusion in multimodal learning?

- **Concept:** Functional Near-Infrared Spectroscopy (fNIRS) and its applications in pain assessment
  - **Why needed here:** fNIRS is one of the two main modalities used in this framework, so understanding how fNIRS measures brain activity and its relevance to pain assessment is important for interpreting the results and potential improvements.
  - **Quick check question:** How does fNIRS measure brain activity, and what are its advantages and limitations compared to other neuroimaging techniques like fMRI?

## Architecture Onboarding

- **Component map:** PainViT–1 (embedding extractor) -> Waveform visualization -> PainViT–2 (fusion and assessment) -> Pain intensity classification

- **Critical path:** Video/fNIRS input → PainViT–1 embedding extraction → Waveform visualization → PainViT–2 fusion and assessment → Pain intensity classification

- **Design tradeoffs:**
  - Using a single architecture for both modalities simplifies implementation but may not capture modality-specific patterns as effectively as specialized models
  - Waveform visualization allows for modality-agnostic processing but may lose some temporal or spatial information compared to raw data
  - Multi-task pre-training improves feature extraction but requires additional computational resources and may introduce bias from pre-training tasks

- **Failure signatures:**
  - Poor performance on one modality but not the other suggests modality-specific feature extraction issues
  - Similar performance with and without pre-training indicates ineffective feature transfer
  - Attention maps showing equal focus on all regions suggest the model is not learning discriminative features

- **First 3 experiments:**
  1. Train PainViT–1 on video frames only and evaluate performance to establish a baseline for the video modality
  2. Train PainViT–1 on fNIRS channels only and evaluate performance to establish a baseline for the fNIRS modality
  3. Combine the best-performing unimodal models using early fusion (concatenation) and evaluate the multimodal performance to compare with the proposed waveform-based fusion approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the waveform representation of fNIRS data affect the performance of the Twins-PainViT model compared to using raw fNIRS data?
- Basis in paper: [explicit] The paper states that waveform representation is applied to the original fNIRS data and the learned embeddings from both modalities.
- Why unresolved: The paper does not provide a direct comparison between the performance of the model using waveform representation and raw fNIRS data.
- What evidence would resolve it: Conducting experiments using both waveform representation and raw fNIRS data and comparing their performance would provide a definitive answer.

### Open Question 2
- Question: How do the attention maps generated from the PainViT-2 layer contribute to the interpretability of the model's decision-making process?
- Basis in paper: [explicit] The paper mentions that attention maps were generated from the last layer of PainViT-2, which processed the unified image visualizing both the video and HbO embedding waveforms.
- Why unresolved: The paper does not provide a detailed analysis of how the attention maps contribute to the interpretability of the model's decision-making process.
- What evidence would resolve it: Conducting a thorough analysis of the attention maps and their correlation with the model's predictions would provide insights into the interpretability of the model's decision-making process.

### Open Question 3
- Question: How does the pre-training strategy using multi-task learning affect the performance of the Twins-PainViT model?
- Basis in paper: [explicit] The paper states that the Twins-PainViT models were pre-trained using a multi-task learning strategy on diverse datasets.
- Why unresolved: The paper does not provide a comparison between the performance of the model with and without pre-training using multi-task learning.
- What evidence would resolve it: Conducting experiments with and without pre-training using multi-task learning and comparing their performance would provide a definitive answer.

## Limitations
- Moderate performance ceiling (46.76% accuracy) suggests the waveform representation may not fully capture temporal dynamics in both video and fNIRS data
- Modality-agnostic design trades off potentially superior modality-specific feature extraction for architectural simplicity
- Multi-task pre-training strategy relies on datasets that may not be optimally aligned with pain assessment, introducing potential negative transfer

## Confidence
- **High confidence**: The architectural design of the dual ViT framework and the waveform representation methodology are well-specified and reproducible
- **Medium confidence**: The performance claims are supported by the reported results, though the absolute accuracy suggests room for improvement
- **Low confidence**: The effectiveness of the multi-task pre-training strategy, as no ablation studies are provided to quantify its specific contribution

## Next Checks
1. **Ablation study**: Remove the multi-task pre-training phase and retrain the model to quantify its specific contribution to performance
2. **Alternative representation**: Replace waveform visualization with direct concatenation of video and fNIRS features to compare modality-agnostic versus modality-specific fusion approaches
3. **Attention analysis**: Conduct a more detailed analysis of the attention maps across different pain intensity levels to verify that the model is learning meaningful modality-specific patterns rather than generic features