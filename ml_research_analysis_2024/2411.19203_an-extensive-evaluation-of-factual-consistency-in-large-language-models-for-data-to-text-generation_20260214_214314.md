---
ver: rpa2
title: An Extensive Evaluation of Factual Consistency in Large Language Models for
  Data-to-Text Generation
arxiv_id: '2411.19203'
source_url: https://arxiv.org/abs/2411.19203
tags:
- factual
- consistency
- llms
- datasets
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides an extensive evaluation of factual consistency
  in large language models (LLMs) for data-to-text generation (DTG) tasks. The study
  examines five popular LLM families (T5, BART, OPT, BLOOM, Llama 2) across five DTG
  datasets (E2E, ViGGo, WikiTableText, DART, WebNLG) using four automatic metrics
  (SummaC-Conv, NEOverlap, AlignScore, QAF actEv al) and human evaluation.
---

# An Extensive Evaluation of Factual Consistency in Large Language Models for Data-to-Text Generation

## Quick Facts
- **arXiv ID**: 2411.19203
- **Source URL**: https://arxiv.org/abs/2411.19203
- **Reference count**: 16
- **Key outcome**: Llama 2 consistently produces the most factually consistent text for data-to-text generation tasks

## Executive Summary
This paper provides a comprehensive evaluation of factual consistency in large language models (LLMs) for data-to-text generation (DTG) tasks. The study examines five popular LLM families (T5, BART, OPT, BLOOM, Llama 2) across five DTG datasets using both automatic metrics and human evaluation. The research reveals that while larger models generally produce more factually consistent text, smaller models like T5 and BART can achieve strong performance on specific dataset types. Llama 2 emerges as the top-performing family across automatic metrics, with human evaluation corroborating these findings.

## Method Summary
The study evaluates five LLM families across five DTG datasets using four automatic metrics (SummaC-Conv, NEOverlap, AlignScore, QAFactEval) and human evaluation. The datasets include E2E, ViGGo, WikiTableText, DART, and WebNLG. Experiments systematically compare model performance across different dataset characteristics, including size and lexical diversity. Human evaluation is conducted to validate automatic metric findings, though specific methodology details are limited in the paper.

## Key Results
- Llama 2 consistently produces the most factually consistent text across all automatic metrics
- Smaller models (T5, BART) can outperform larger models on large, lexically less-diverse datasets
- Source-reference divergence negatively impacts LLM performance in DTG tasks
- Human evaluation results corroborate automatic metric findings, showing Llama 2 outperforming other families

## Why This Works (Mechanism)
The paper does not explicitly explain the mechanisms behind factual consistency in LLMs for DTG tasks.

## Foundational Learning
- **Factual consistency in text generation**: Understanding what constitutes factual consistency and why it matters in data-to-text generation tasks. Quick check: Can you identify factual errors in sample generated text?
- **Automatic evaluation metrics**: Familiarity with SummaC-Conv, NEOverlap, AlignScore, and QAFactEval metrics and their strengths/limitations. Quick check: Can you explain the difference between these metrics?
- **Dataset characteristics**: Understanding how dataset size, lexical diversity, and structure affect model performance. Quick check: Can you predict how a model might perform on a new dataset based on its characteristics?
- **Human evaluation methodology**: Understanding best practices for conducting human evaluation of generated text. Quick check: Can you design a rubric for human evaluation of factual consistency?

## Architecture Onboarding
**Component map**: LLM model -> Data-to-text generation -> Factual consistency evaluation -> Automatic metrics + Human evaluation
**Critical path**: Input data → LLM generation → Factual consistency check → Performance scoring
**Design tradeoffs**: Model size vs. factual consistency, automatic vs. human evaluation, dataset diversity vs. model generalization
**Failure signatures**: Factual errors, hallucinations, inconsistency with source data, performance degradation on lexically diverse datasets
**First experiments**: 1) Compare baseline LLM performance across all five datasets 2) Evaluate automatic metrics correlation with human judgment 3) Test model performance on synthetic datasets with controlled characteristics

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Limited evaluation scope to five LLM families and five datasets may miss important model-dataset interactions
- Human evaluation methodology details are insufficient, raising questions about rater expertise and inter-annotator agreement
- The observation about source-reference divergence requires further investigation into underlying mechanisms
- Dataset-model size interactions need deeper exploration to understand when smaller models outperform larger ones

## Confidence
- High confidence in Llama 2 consistently producing the most factually consistent text
- Medium confidence in relative performance rankings between model families
- Medium confidence in the general relationship between model size and factual consistency
- Low confidence in causal mechanisms explaining source-reference divergence effects

## Next Checks
1. Conduct experiments with additional LLM families and architectures (including instruction-tuned and domain-specific models) across the same five datasets to test the generalizability of the Llama 2 performance advantage.
2. Perform ablation studies varying dataset size, lexical diversity, and structure independently to isolate which dataset characteristics most strongly influence the model size-accuracy relationship.
3. Implement controlled human evaluation with detailed annotation guidelines, multiple raters per example, and inter-annotator agreement metrics to validate the automatic metric rankings and identify potential systematic biases.