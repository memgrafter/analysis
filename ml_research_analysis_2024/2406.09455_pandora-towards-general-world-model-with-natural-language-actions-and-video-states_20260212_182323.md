---
ver: rpa2
title: 'Pandora: Towards General World Model with Natural Language Actions and Video
  States'
arxiv_id: '2406.09455'
source_url: https://arxiv.org/abs/2406.09455
tags:
- video
- world
- arxiv
- generation
- pandora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Pandora, a hybrid autoregressive-diffusion
  model that simulates world states by generating videos and allows real-time control
  with natural language actions. Pandora addresses the limitations of current foundation
  models by integrating a pretrained LLM and a pretrained video model, requiring only
  lightweight fine-tuning.
---

# Pandora: Towards General World Model with Natural Language Actions and Video States

## Quick Facts
- arXiv ID: 2406.09455
- Source URL: https://arxiv.org/abs/2406.09455
- Authors: Jiannan Xiang; Guangyi Liu; Yi Gu; Qiyue Gao; Yuting Ning; Yuheng Zha; Zeyu Feng; Tianhua Tao; Shibo Hao; Yemin Shi; Zhengzhong Liu; Eric P. Xing; Zhiting Hu
- Reference count: 40
- Primary result: Hybrid autoregressive-diffusion model simulating world states with natural language actions and video states

## Executive Summary
Pandora is a general world model that generates videos while responding to natural language actions in real-time. The model combines a pretrained LLM with a pretrained video model through a two-stage training approach: large-scale pretraining for domain generality and video consistency, followed by instruction tuning for enhanced action controllability. By leveraging existing foundation models and requiring only lightweight fine-tuning, Pandora achieves cross-domain generalization and on-the-fly control capabilities that address limitations of current world models.

## Method Summary
Pandora employs a hybrid autoregressive-diffusion architecture that processes natural language actions and video states sequentially. The model initializes with pretrained components (Vicuna-7B-v1.5 LLM and DynamiCrafter video model), then undergoes two training stages: pretraining to align the LLM backbone with the video model using video caption data, and instruction tuning on a curated dataset to enhance action controllability. The architecture includes vision encoders and adapters that connect the LLM to the video generator, enabling the model to generate next video states based on previous states and actions. This approach achieves domain generality through separate pretraining of language and video components, while instruction tuning with high-quality data enhances the model's ability to follow natural language instructions.

## Key Results
- Generates consistent videos across diverse domains including human, animal, and object-centric content
- Enables real-time control through natural language actions during video generation
- Demonstrates successful transfer of action controllability from specific domains to unseen domains
- Achieves domain generality through large-scale pretraining and lightweight fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Large-scale pretraining of language and video models separately enables domain generality and video consistency. The pretrained LLM backbone provides general text understanding while the pretrained video model provides consistent video generation, with capabilities transferred via lightweight fine-tuning. The core assumption is that representations learned during separate pretraining are sufficiently aligned for the combined model to function effectively.

### Mechanism 2
Autoregressive architecture enables on-the-fly control with natural language actions. The model sequentially processes actions and previous states, generating the next state at each step, allowing natural language actions to be input at any time during video generation. The core assumption is that the autoregressive architecture can effectively model temporal dependencies between states and actions.

### Mechanism 3
Instruction tuning with high-quality data enhances action controllability and generalization. The model is fine-tuned on curated video-action pairs, learning to generate videos that accurately reflect specified actions, allowing generalization to new domains and actions. The core assumption is that the curated dataset is representative of diverse actions and states the model will encounter.

## Foundational Learning

- **Diffusion models for video generation**: Needed to understand how the pretrained video model generates videos by iteratively denoising random noise samples. Quick check: How do diffusion models generate data by iteratively denoising a random noise sample?

- **Autoregressive models for sequence generation**: Needed to understand how the model processes sequences of actions and states by predicting the next element based on previous elements. Quick check: How do autoregressive models generate sequences by predicting the next element based on previous elements?

- **Instruction tuning for model adaptation**: Needed to understand how instruction tuning differs from traditional fine-tuning and its benefits for enhancing action controllability. Quick check: How does instruction tuning differ from traditional fine-tuning, and what are its benefits?

## Architecture Onboarding

- **Component map**: Vision encoder -> Adapters -> LLM backbone -> Query embeddings -> Video generator

- **Critical path**: 1) Encode previous state (video) using vision encoder and adapter, 2) Process action (text) using LLM backbone, 3) Generate output embeddings using LLM backbone and query embeddings, 4) Generate next state (video) using video generator and output embeddings

- **Design tradeoffs**: Using pretrained models reduces training costs but may limit flexibility; autoregressive architecture enables on-the-fly control but may be slower than non-autoregressive approaches; instruction tuning enhances action controllability but requires high-quality data

- **Failure signatures**: Inconsistent video generation (may indicate issues with pretrained video model or alignment), poor action understanding (may indicate issues with pretrained LLM or instruction tuning), slow video generation (may indicate issues with autoregressive architecture)

- **First 3 experiments**: 1) Test video generation with fixed initial state and action, 2) Test action understanding with fixed initial state and varying actions, 3) Test on-the-fly control by varying actions during video generation

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of natural language action instructions impact the performance and controllability of Pandora across different domains? The paper states that instruction tuning with high-quality action-state sequential data boosts real-time controllability but does not quantify the relationship between instruction quality and model performance. Experiments varying the quality and specificity of action instructions and measuring their impact on video generation accuracy and controllability across diverse domains would resolve this.

### Open Question 2
What is the upper limit of video length and quality that Pandora can achieve with autoregressive generation, and how does it scale with increased compute and larger backbone models? While the paper demonstrates 8-second video generation, it does not explore the limits of video length or quality achievable with autoregressive generation. Experiments generating videos of increasing length and complexity, measuring quality metrics, and comparing performance with different model sizes and training compute would resolve this.

### Open Question 3
How well does Pandora's action controllability transfer to completely novel domains not seen during training, and what factors influence this transfer ability? The paper demonstrates transfer to some unseen domains but does not systematically evaluate performance on completely novel domains or analyze factors contributing to successful transfer. Experiments testing Pandora on a wide range of novel domains, measuring action controllability, and analyzing internal representations to identify transferability factors would resolve this.

## Limitations

- Reliance on pretrained models without detailed architectural specifications for vision encoder and adapters
- Curated instruction tuning dataset lacks specific details about composition, size, and quality metrics
- Evaluation primarily relies on qualitative assessments and user studies rather than comprehensive quantitative metrics
- Substantial computational resources required for training on massive video and text data

## Confidence

- **Domain Generality**: Medium confidence - demonstrates performance across multiple domains but lacks comprehensive quantitative evaluation
- **Video Consistency**: Medium confidence - uses pretrained diffusion models but empirical evidence limited to qualitative assessments
- **Action Controllability**: Medium confidence - instruction tuning approach well-justified but evaluation relies heavily on user studies
- **On-the-fly Control**: Medium confidence - autoregressive architecture theoretically enables real-time control but practical latency measurements not provided

## Next Checks

1. Conduct comprehensive quantitative evaluation using established metrics (FID, CLIP score, user preference studies) across multiple domains to objectively measure video quality and consistency

2. Design standardized benchmark with specific action sequences and success criteria to quantitatively evaluate model's ability to follow instructions across different domains

3. Measure real-time control capability by evaluating inference latency for video generation and computing resource requirements, comparing against non-autoregressive alternatives