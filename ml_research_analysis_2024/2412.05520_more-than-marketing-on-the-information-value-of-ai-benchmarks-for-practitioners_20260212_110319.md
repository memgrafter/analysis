---
ver: rpa2
title: More than Marketing? On the Information Value of AI Benchmarks for Practitioners
arxiv_id: '2412.05520'
source_url: https://arxiv.org/abs/2412.05520
tags:
- benchmarks
- benchmark
- were
- information
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Practitioners use AI benchmarks primarily to make relative comparisons
  between models, with benchmark scores serving as initial indicators of performance.
  However, product and policy practitioners found existing benchmarks insufficient
  for substantive decisions, often developing internal evaluations tailored to specific
  use cases.
---

# More than Marketing? On the Information Value of AI Benchmarks for Practitioners

## Quick Facts
- arXiv ID: 2412.05520
- Source URL: https://arxiv.org/abs/2412.05520
- Reference count: 40
- Practitioners use AI benchmarks primarily for relative comparisons and as negative indicators to disqualify models, not for substantive decision-making

## Executive Summary
This study investigates how AI benchmarks are actually used by practitioners across academia, product, and policy domains. Through semi-structured interviews with 19 practitioners, the research reveals that while benchmarks serve as initial indicators of performance and enable relative comparisons in research settings, they are insufficient for substantive decisions in product and policy contexts. Practitioners across all domains found existing benchmarks inadequate for real-world decision-making, often developing internal evaluations tailored to specific use cases. The study identifies that benchmarks are more frequently used to disqualify models than to qualify them, with construct validity gaps between benchmark tasks and actual deployment scenarios being a major limitation.

## Method Summary
The study employed qualitative analysis using grounded theory methodology with semi-structured interviews conducted across academia, policy, and industry. Interview data was collected from 19 practitioners who have used or decided against using benchmarks in their work. The data was transcribed using Otter.ai and analyzed through open and axial coding using Taguette for qualitative coding and thematic analysis. The analysis developed 13 high-level code categories and 85 codes to understand benchmark usage patterns, perceived effectiveness, limitations, and impact on decisions.

## Key Results
- Practitioners use benchmarks primarily as negative indicators to disqualify models rather than positive indicators to qualify them
- Research settings accept benchmark improvement alone as success, while product and policy settings require additional assessments due to real-world impact concerns
- Product and policy practitioners develop internal evaluations tailored to specific use cases due to insufficient construct validity in existing benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Benchmarks are adopted when they provide standardized relative comparisons that enable "hill climbing" in research settings.
- Mechanism: Standardized metrics allow researchers to compare new methods against baselines, creating feedback loops for iterative improvement.
- Core assumption: Relative performance differences are meaningful for tracking progress within a research community.
- Evidence anchors:
  - [section] "I-11 describes the ability of benchmarks to evaluate whether a new method is an improvement over previous ones"
  - [section] "These evaluations were accepted to the extent that I-17 described benchmark performance as essential to publication"

### Mechanism 2
- Claim: Benchmarks fail adoption in product/policy settings when they lack construct validity for specific use cases.
- Mechanism: Practitioners require absolute signal for high-stakes decisions, but general benchmarks only provide relative comparisons that don't map to real-world applications.
- Core assumption: The gap between benchmark tasks and actual deployment scenarios prevents meaningful adoption.
- Evidence anchors:
  - [abstract] "practitioners found existing benchmarks insufficient for substantive decisions, often developing internal evaluations tailored to specific use cases"
  - [section] "I-9... cited the fact 'that benchmarks today are not based on the end user behaviors which are most interesting'"

### Mechanism 3
- Claim: Benchmark adoption requires meeting all UTAUT criteria, with performance expectancy being the most critical factor.
- Mechanism: Even with social influence (widespread use in publications/marketing) and facilitating conditions (infrastructure), benchmarks fail without demonstrating perceived usefulness for practitioners' specific needs.
- Core assumption: The UTAUT framework applies to technology adoption including AI benchmarks.
- Evidence anchors:
  - [abstract] "benchmarks were more frequently used as negative indicators (to disqualify models) than positive ones (to qualify them)"
  - [section] "participants remained hesitant to use them for substantive decisions and identified overreliance on benchmark scores for decision-making as a substantial risk"

## Foundational Learning

- Concept: Grounded Theory methodology
  - Why needed here: The study uses grounded theory coding to analyze interview data and derive theory from empirical observations.
  - Quick check question: What is the key difference between grounded theory and hypothesis-driven research approaches?

- Concept: Construct validity
  - Why needed here: The paper discusses how benchmarks often lack construct validity - the extent to which a test measures what it claims to measure.
  - Quick check question: Why is construct validity particularly challenging for AI benchmarks compared to traditional benchmarks?

- Concept: Hill climbing in research
  - Why needed here: The paper references how benchmarks enable researchers to make incremental progress by comparing against previous work.
  - Quick check question: How does the ability to make frequent comparisons accelerate research progress in AI?

## Architecture Onboarding

- Component map: Interview data collection -> qualitative coding -> thematic analysis -> theory development. Key components include interview guides, coding frameworks, and analytical tools (Otter.ai, Taguette).
- Critical path: 1) Design interview protocol with domain-specific questions, 2) Conduct interviews with diverse practitioners, 3) Transcribe and code data using grounded theory approach, 4) Analyze themes and relationships, 5) Validate findings through team discussions.
- Design tradeoffs: The semi-structured interview approach allows flexibility but introduces potential interviewer bias. Snowball sampling provides depth but may limit diversity. The grounded theory approach is exploratory but requires more data analysis effort than hypothesis-driven methods.
- Failure signatures: If interview questions don't capture relevant benchmark usage patterns, if coding becomes inconsistent across team members, or if findings don't align with observed benchmark practices in the field.
- First 3 experiments:
  1. Test interview protocol with 2-3 pilot participants to identify unclear questions and refine the guide.
  2. Conduct first full interview and immediately transcribe/code to validate the coding framework and make adjustments.
  3. Analyze initial 3-4 interviews to identify emerging themes and determine if sample size is sufficient or if additional recruitment is needed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do benchmark developers determine which domain experts to consult during the benchmark design process, and what specific criteria should be used to evaluate the quality and relevance of their input?
- Basis in paper: [explicit] The paper mentions that incorporating domain experts is critical but doesn't specify selection criteria or evaluation methods.
- Why unresolved: The paper identifies this as important but doesn't provide concrete guidance on implementation or quality assessment.
- What evidence would resolve it: Studies comparing benchmarks designed with different expert selection criteria, or frameworks for evaluating expert input quality.

### Open Question 2
- Question: What specific metrics or evaluation methods can be developed to measure the real-world applicability gap between benchmark performance and actual deployment outcomes?
- Basis in paper: [inferred] The paper highlights a persistent gap between benchmark scores and real-world performance but doesn't propose concrete measurement approaches.
- Why unresolved: While the gap is acknowledged, there's no systematic way to quantify or track it across different use cases or domains.
- What evidence would resolve it: Case studies comparing benchmark results with post-deployment performance data, or frameworks for measuring predictive validity of benchmarks.

### Open Question 3
- Question: How can the benchmark development community create standardized processes for proprietary data collection and contamination prevention that balance transparency with practical implementation constraints?
- Basis in paper: [explicit] The paper mentions proprietary data collection and contamination prevention as critical but notes tension with transparency.
- Why unresolved: The paper identifies this as important but doesn't provide specific approaches for balancing competing needs or ensuring consistent implementation.
- What evidence would resolve it: Documentation of successful contamination prevention protocols, or comparative studies of different approaches to balancing transparency and protection.

## Limitations
- The study relies on self-reported data from a relatively small sample (19 practitioners) through semi-structured interviews, which introduces potential recall bias
- The snowball sampling approach may have limited diversity in perspectives and could overrepresent certain viewpoints
- The temporal aspect is uncertain as benchmark usage and perceptions may have evolved since the interviews were conducted

## Confidence
- High Confidence: Findings about benchmarks being used primarily as negative indicators (to disqualify models) rather than positive ones are well-supported by multiple participant accounts
- Medium Confidence: Claims about construct validity gaps between benchmarks and real-world applications are strongly supported but may be domain-specific
- Low Confidence: Specific quantitative claims about adoption rates or the relative importance of different factors affecting benchmark usage are not directly measured

## Next Checks
1. Conduct a systematic mapping study to compare benchmark task specifications against real-world deployment scenarios across multiple domains, quantifying the alignment gap
2. Implement a survey-based study tracking how the same practitioners' benchmark usage patterns and perceptions evolve over a 12-18 month period
3. Design a case-control study comparing product teams that rely heavily on benchmarks versus those developing internal evaluations, measuring differences in deployment success rates and post-deployment performance