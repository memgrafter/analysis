---
ver: rpa2
title: Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge
  Distillation
arxiv_id: '2407.03056'
source_url: https://arxiv.org/abs/2407.03056
tags:
- class
- prompt
- learning
- kdpl
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Knowledge Distillation Prompt Learning (KDPL),
  a method that improves the zero-shot generalization of learned prompts for Vision-Language
  Models (VLMs) by leveraging unsupervised knowledge distillation. The core idea is
  to adapt lightweight VLMs using knowledge distilled from more powerful VLMs without
  requiring labeled training examples.
---

# Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation

## Quick Facts
- arXiv ID: 2407.03056
- Source URL: https://arxiv.org/abs/2407.03056
- Reference count: 40
- Primary result: KDPL improves zero-shot generalization of learned prompts by 2-7% on multiple benchmark datasets

## Executive Summary
This paper introduces Knowledge Distillation Prompt Learning (KDPL), a method that enhances the zero-shot generalization of learned prompts for Vision-Language Models (VLMs) through unsupervised knowledge distillation. By replacing labeled supervision with probability distributions from a more powerful teacher model, KDPL eliminates the need for ground-truth labels while maintaining strong performance across domain generalization, cross-dataset transfer, and base-to-novel class generalization tasks. The approach integrates seamlessly with existing prompt learning techniques and demonstrates consistent improvements across over ten benchmark datasets.

## Method Summary
KDPL adapts lightweight VLMs using knowledge distilled from more powerful VLMs without requiring labeled training examples. The method replaces ground-truth labels in traditional prompt learning with probability distributions generated by a teacher VLM performing zero-shot classification. A symmetric KL-divergence loss is computed between teacher and student probability distributions, and only the prompt parameters are updated via backpropagation. The approach also introduces a class-agnostic adaptation setting where the teacher automatically selects relevant class names from a large dictionary based on batch content, enabling adaptation without knowledge of training class names.

## Key Results
- KDPL consistently outperforms baseline prompt learning methods by 2-7% across domain generalization benchmarks
- The method achieves significant improvements in cross-dataset transfer (ImageNet→Caltech101, OxfordPets, etc.)
- Class-agnostic adaptation shows effectiveness even without knowledge of training class names
- Performance gains are maintained across different backbone architectures (ResNet-50, ViT-B/32)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KDPL improves zero-shot generalization by aligning student probabilities with teacher distributions without labeled data
- Mechanism: Teacher model generates probability distributions via zero-shot classification; symmetric KL-divergence loss aligns student probabilities with teacher; only prompt parameters are updated
- Core assumption: Teacher probability distribution provides useful supervision signal without ground-truth labels
- Evidence anchors: Abstract states "eliminates the need for labeled training examples by distilling knowledge from a large Vision-Language Model"; Section 3.2 introduces KDPL concept
- Break condition: Poor teacher model calibration leads to ineffective learning signals

### Mechanism 2
- Claim: KDPL enables label-agnostic and class-agnostic adaptation through automatic class selection
- Mechanism: Teacher ranks class names by relevance to each batch; top-K classes are selected for distillation; student learns without knowing training class names
- Core assumption: Teacher can effectively rank class names by relevance to batch content
- Evidence anchors: Section 3.3 proposes strategy for automatically selecting class names; teacher model selects most relevant classes
- Break condition: Unreliable teacher ranking or too large class dictionaries degrade performance

### Mechanism 3
- Claim: KDPL generalizes better to unseen classes through distributional alignment rather than label fitting
- Mechanism: Traditional prompt learning overfits to seen classes by maximizing performance on labeled examples; KDPL optimizes distributional similarity capturing broader semantic relationships
- Core assumption: Matching probability distributions leads to better generalization than fitting to hard labels
- Evidence anchors: Section 4.4 shows KDPL outperforms baselines across multiple datasets; greater generalization evident even for fine-grained datasets
- Break condition: Peaked or low-diversity teacher distributions prevent robust learning for unseen classes

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and dual encoder architecture
  - Why needed here: Understanding CLIP's image-text alignment in shared embedding space is crucial for grasping prompt learning and knowledge distillation mechanics
  - Quick check question: What is the main goal of training a VLM like CLIP?

- Concept: Prompt learning and parameter-efficient adaptation
  - Why needed here: KDPL builds upon existing prompt learning techniques by replacing labeled supervision with knowledge distillation
  - Quick check question: How do traditional prompt learning methods differ from KDPL in terms of supervision requirements?

- Concept: Knowledge distillation and its application to prompt tuning
  - Why needed here: KDPL fundamentally applies knowledge distillation to the prompt learning setting
  - Quick check question: What is the difference between forward and reverse KL-divergence, and why does KDPL use the symmetric version?

## Architecture Onboarding

- Component map: Images → Student VLM (with learnable prompts) → Student probabilities → KL-divergence loss ← Teacher VLM (zero-shot classification) ← Teacher probabilities

- Critical path:
  1. Prepare batch of images and set of classes (known or auto-selected)
  2. Teacher model performs zero-shot classification to generate probabilities
  3. Student model generates probabilities using current prompts
  4. Compute symmetric KL-divergence loss
  5. Backpropagate through student model to update only prompt parameters
  6. Repeat for multiple epochs or until convergence

- Design tradeoffs:
  - Teacher model size vs. computational cost: Larger teachers provide better signals but increase inference cost
  - Number of selected classes (K) in class-agnostic scenarios: Larger K increases memory usage but may improve coverage
  - Symmetric vs. asymmetric KL-divergence: Symmetric is more robust in class-agnostic settings but may be less discriminative
  - Prompt learning method choice: Textual, visual, or multimodal prompts affect model compatibility and performance

- Failure signatures:
  - Poor generalization: Student probabilities diverge significantly from teacher's
  - Overfitting to seen classes: Performance on unseen classes degrades despite good source performance
  - Memory issues: Large class dictionaries or high K values cause out-of-memory errors
  - Training instability: Loss becomes NaN or explodes due to poor teacher calibration

- First 3 experiments:
  1. Validate basic KDPL integration: Replace labeled supervision in CoOp with KDPL on OxfordPets and compare to zero-shot CLIP and supervised CoOp
  2. Test class-agnostic adaptation: Run KDPL with auto-selected classes on ImageNet and evaluate on domain generalization benchmarks
  3. Ablate KL-divergence choice: Train with forward, reverse, and symmetric KL-divergence on validation set to confirm symmetric KL is optimal

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation primarily focuses on datasets where CLIP's zero-shot performance is already relatively strong, potentially inflating perceived benefits
- Class-agnostic evaluation relies heavily on teacher model's class ranking ability, which may not generalize to highly specialized domains
- Computational overhead of using large teacher model for each batch may limit scalability to real-time applications

## Confidence
- High Confidence: KDPL's core mechanism of using symmetric KL-divergence for knowledge distillation is well-established
- Medium Confidence: Claimed improvements in zero-shot domain generalization and cross-dataset transfer are supported by results on multiple benchmarks
- Medium Confidence: Class-agnostic evaluation setting demonstrates flexibility but depends on teacher model's class ranking quality

## Next Checks
1. Conduct ablation study on teacher model size (ViT-B/32 vs. ViT-L/14) to quantify impact on performance and computational efficiency
2. Evaluate KDPL's robustness to teacher calibration by intentionally perturbing confidence scores or applying temperature scaling
3. Test KDPL on completely different domain dataset (medical imaging or satellite imagery) to validate effectiveness with minimal semantic overlap to teacher's training distribution