---
ver: rpa2
title: 'LLM-based NLG Evaluation: Current Status and Challenges'
arxiv_id: '2402.01383'
source_url: https://arxiv.org/abs/2402.01383
tags:
- evaluation
- llms
- language
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of LLM-based natural
  language generation (NLG) evaluation methods, categorizing them into four types:
  LLM-derived metrics, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative
  evaluation. The survey covers over 100 works published since 2023, analyzing their
  approaches, strengths, and limitations.'
---

# LLM-based NLG Evaluation: Current Status and Challenges

## Quick Facts
- arXiv ID: 2402.01383
- Source URL: https://arxiv.org/abs/2402.01383
- Reference count: 40
- Primary result: Comprehensive survey of over 100 LLM-based NLG evaluation methods since 2023, identifying four main categories and key challenges including position bias and lack of unified benchmarks.

## Executive Summary
This paper provides a comprehensive survey of LLM-based natural language generation (NLG) evaluation methods, categorizing them into four types: LLM-derived metrics, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation. The survey analyzes over 100 works published since 2023, examining their approaches, strengths, and limitations. Key findings include that prompting LLMs achieves strong performance with correlation coefficients around 0.52-0.71 on standard benchmarks like SummEval, while fine-tuning smaller open-source models offers reproducibility and lower costs. The paper identifies several challenges including position bias, self-bias, and the need for unified benchmarks across diverse languages and task scenarios.

## Method Summary
The paper systematically reviews LLM-based NLG evaluation methods through comprehensive literature analysis of over 100 papers published since 2023. It categorizes methods into four main approaches: LLM-derived metrics (using embeddings or probabilities), prompting LLMs (with various techniques like scoring and comparison), fine-tuning LLMs (using labeled evaluation data), and human-LLM collaborative evaluation. The review analyzes performance metrics, implementation details, and limitations of each approach, drawing on standard benchmarks like SummEval, GEM, and WMT. The paper synthesizes findings across different NLG tasks including summarization, translation, dialogue, and story generation.

## Key Results
- Prompting LLMs achieves correlation coefficients around 0.52-0.71 on standard benchmarks like SummEval
- Fine-tuning smaller open-source models offers reproducibility and lower costs compared to prompting large proprietary models
- Major challenges include position bias, self-bias, and the need for unified benchmarks across diverse languages and task scenarios
- Human-LLM collaboration shows promise for achieving accurate and efficient evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-derived metrics leverage semantic embeddings and generation probabilities to capture linguistic quality more effectively than n-gram overlap metrics.
- Mechanism: These methods replace traditional embeddings with larger LLM representations or use the probability distribution under different conditions to infer quality. For example, embedding-based metrics use cosine similarity between vectors, while probability-based metrics like GPTScore sum log probabilities of token sequences.
- Core assumption: LLMs encode richer semantic and contextual information than smaller language models, allowing them to assess text quality beyond surface-level matching.
- Evidence anchors:
  - [abstract] "Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years."
  - [section] "The embedding-based methods, like BERTScore, generally utilize representations of language models and thus compute the semantic similarity between the reference and the target text to evaluate, with different possible ways of implementation."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.469, average citations=0.0. Top related titles: Leveraging Large Language Models for NLG Evaluation: Advances and Challenges, Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability.
- Break condition: If the LLM's embedding space is not aligned with human judgments of quality, or if the probability distributions do not correlate with human ratings, the metrics will fail to provide meaningful evaluation scores.

### Mechanism 2
- Claim: Prompting LLMs for evaluation allows flexible, criterion-specific assessments that mimic human evaluation processes.
- Mechanism: Users craft detailed prompts that include instructions, evaluation criteria, and input text, and the LLM directly generates a score or comparison. This approach allows for fine-grained, aspect-based evaluation such as consistency, fluency, or relevance.
- Core assumption: LLMs can follow complex instructions and reason about text quality in a way that aligns with human judgments when given clear, structured prompts.
- Evidence anchors:
  - [abstract] "Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation."
  - [section] "Chiang and Lee (2023a) have conducted relevant studies early, using a Likert scale from 1 to 5 to evaluate story generation and adversarial attacks with InstructGPT (Lee 2024) and ChatGPT 1, showing that the evaluation results of LLMs are consistent with expert human evaluators."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.469, average citations=0.0. Top related titles: Leveraging Large Language Models for NLG Evaluation: Advances and Challenges, Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability.
- Break condition: If prompts are ambiguous, the LLM may produce inconsistent or irrelevant evaluations. Position bias and self-bias in LLMs can also skew results.

### Mechanism 3
- Claim: Fine-tuning smaller, open-source LLMs on high-quality evaluation data produces specialized evaluators that are more cost-effective and reproducible than prompting large proprietary models.
- Mechanism: By constructing diverse, labeled evaluation datasets and training open-source LLMs (e.g., LLaMA, Mistral), these models learn to perform evaluations without requiring expensive API calls to proprietary LLMs. The fine-tuned models can be deployed locally with lower latency.
- Core assumption: The quality of the fine-tuning data is high enough to teach the model to align with human judgments, and the open-source model architecture is sufficiently capable to learn the evaluation task.
- Evidence anchors:
  - [abstract] "Fine-tuning LLMs (ยง 4): using labeled evaluation data to fine-tune existing LLMs and improving their NLG evaluation capabilities."
  - [section] "In response, recent research has shifted towards fine-tuning smaller, open-source LLMs specifically for evaluation purposes, aiming to achieve performance close to GPT-4 in NLG evaluation."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.469, average citations=0.0. Top related titles: Leveraging Large Language Models for NLG Evaluation: Advances and Challenges, Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability.
- Break condition: If the training data is noisy or unrepresentative, the fine-tuned model may not generalize well. If the open-source model is too small, it may not capture the nuanced judgments needed for high-quality evaluation.

## Foundational Learning

- Concept: NLG Evaluation Metrics
  - Why needed here: Understanding traditional metrics like BLEU, ROUGE, and BERTScore is essential to appreciate the limitations that LLM-based methods aim to overcome.
  - Quick check question: What are the main weaknesses of n-gram overlap metrics in evaluating generated text?

- Concept: Prompt Engineering
  - Why needed here: Crafting effective prompts is critical for obtaining reliable evaluations from LLMs, as the output quality depends heavily on the clarity and structure of the instructions.
  - Quick check question: How does the inclusion of evaluation criteria in a prompt affect the LLM's output?

- Concept: Fine-tuning vs. Prompting
  - Why needed here: Knowing the tradeoffs between fine-tuning a model and prompting a pre-trained model helps in choosing the right approach for a given evaluation task.
  - Quick check question: What are the advantages of fine-tuning a small open-source LLM over prompting a large proprietary LLM for evaluation?

## Architecture Onboarding

- Component map:
  - Input Layer: NLG task inputs (source text, references, external knowledge)
  - Prompt Construction Module: Generates detailed prompts for LLM evaluation
  - LLM Evaluator: Either a prompted proprietary LLM or a fine-tuned open-source model
  - Output Processor: Extracts and formats evaluation results (scores, explanations)
  - Quality Control: Post-processing to mitigate biases or inconsistencies

- Critical path:
  1. Receive NLG outputs and task context
  2. Construct evaluation prompts or load fine-tuned model
  3. Submit to LLM for evaluation
  4. Process and aggregate results
  5. Return evaluation scores and explanations

- Design tradeoffs:
  - Flexibility vs. Cost: Prompting LLMs offers high flexibility but is expensive; fine-tuning is cheaper but less flexible.
  - Reproducibility vs. Performance: Fine-tuned models are more reproducible but may underperform compared to prompting GPT-4.
  - Bias Mitigation: Prompting can introduce position and self-bias; fine-tuning can mitigate some but may inherit biases from training data.

- Failure signatures:
  - Inconsistent scores across similar inputs (prompt sensitivity)
  - Systematic preference for longer or self-generated text (self-bias)
  - High variance in results (model instability)
  - Poor performance on low-resource languages (data bias)

- First 3 experiments:
  1. Compare LLM-derived metrics (e.g., GPTScore) against traditional metrics (BLEU, ROUGE) on SummEval benchmark.
  2. Test prompt sensitivity by varying the order of input texts and evaluating consistency.
  3. Fine-tune a small open-source LLM (e.g., LLaMA-7B) on a subset of SummEval data and evaluate performance vs. prompting GPT-4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop unified benchmarks for LLM-based NLG evaluation that address the current limitations of existing human judgments (task coverage, dataset scale, evaluation methods, and recency of evaluated texts)?
- Basis in paper: [explicit] Section 6.3 discusses the urgent need for large-scale, high-quality human evaluation data covering various NLG tasks and evaluation methods as a benchmark.
- Why unresolved: Existing human judgments are limited to single tasks, evaluation methods, outdated texts, and small scales, making comprehensive evaluation difficult.
- What evidence would resolve it: Creation and validation of a comprehensive benchmark dataset covering multiple NLG tasks, evaluation methods, large-scale human judgments, and recent texts generated by advanced LLMs.

### Open Question 2
- Question: Can fine-tuning smaller open-source LLMs with specific training data achieve evaluation performance comparable to prompting proprietary LLMs like GPT-4 while avoiding issues like position bias and self-bias?
- Basis in paper: [explicit] Section 4.4 notes that while fine-tuning can alleviate shortcomings of prompting LLMs, inherent biases from GPT-4 may persist, and it's unclear if more powerful foundation LLMs would improve evaluation performance.
- Why unresolved: Despite fine-tuning approaches showing promise, the trade-offs between performance, bias mitigation, and computational costs remain unclear, especially when comparing different foundation LLMs.
- What evidence would resolve it: Systematic comparative studies of fine-tuned models using different foundation LLMs on standard benchmarks, measuring performance, bias levels, and computational efficiency.

### Open Question 3
- Question: What is the optimal role distribution and interaction strategy between humans and LLMs in collaborative evaluation to maximize accuracy while minimizing human effort?
- Basis in paper: [explicit] Section 5.3 discusses the advantages of human-LLM collaboration but notes challenges in prompt sensitivity, confidence calibration, and determining when to trust LLM judgments.
- Why unresolved: While collaboration offers benefits, the optimal balance of human and LLM involvement, and how to structure their interaction for different tasks and contexts, remains unclear.
- What evidence would resolve it: Empirical studies testing different human-LLM collaboration strategies across various NLG tasks, measuring accuracy, efficiency, and human effort required.

## Limitations
- Most studies focus on high-resource languages, with insufficient exploration of low-resource languages and novel NLG tasks
- Performance metrics may vary with model versions and prompt formulations, affecting reproducibility
- Limited disclosure of training data construction details and hyperparameter choices in many fine-tuning studies

## Confidence

- High confidence: Categorization of evaluation methods (LLM-derived metrics, prompting, fine-tuning, and human-LLM collaboration) based on systematic review of over 100 works
- Medium confidence: Reported performance metrics (e.g., correlation coefficients around 0.52-0.71 on SummEval), as these may vary with model versions and prompt formulations
- Low confidence: Scalability and reproducibility of fine-tuned models due to limited disclosure of training data construction details and hyperparameter choices

## Next Checks

1. Replicate the correlation results of LLM-derived metrics against traditional metrics like BERTScore on multiple NLG tasks beyond summarization, including translation and dialogue.
2. Conduct prompt sensitivity analysis by systematically varying prompt formulations and text orderings to quantify position bias and self-bias effects.
3. Fine-tune a small open-source LLM on a standardized evaluation dataset (e.g., SummEval) and compare its performance and cost-effectiveness against prompting GPT-4, documenting the full training pipeline for reproducibility.