---
ver: rpa2
title: 'Introducing Milabench: Benchmarking Accelerators for AI'
arxiv_id: '2411.11940'
source_url: https://arxiv.org/abs/2411.11940
tags:
- benchmarks
- milabench
- performance
- benchmark
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Milabench is a benchmarking suite for AI accelerators developed
  by Mila to reflect real-world deep learning workloads. It was created through an
  extensive literature review of 867 papers and surveys with 1,000+ researchers, resulting
  in 26 primary benchmarks and 16 optional ones.
---

# Introducing Milabench: Benchmarking Accelerators for AI

## Quick Facts
- arXiv ID: 2411.11940
- Source URL: https://arxiv.org/abs/2411.11940
- Authors: Pierre Delaunay; Xavier Bouthillier; Olivier Breuleux; Satya Ortiz-Gagné; Olexa Bilaniuk; Fabrice Normandin; Arnaud Bergeron; Bruno Carrez; Guillaume Alain; Soline Blanc; Frédéric Osterrath; Joseph Viviano; Roger Creus-Castanyer Darshan Patil; Rabiul Awal; Le Zhang
- Reference count: 40
- Key outcome: Milabench shows H100 outperforms A100 by 93% on average, while MI300X and Gaudi2 lag due to software stack maturity despite strong FLOP counts

## Executive Summary
Milabench is a benchmarking suite developed by Mila to evaluate AI accelerators using real-world deep learning workloads. The suite was designed through an extensive literature review of 867 Mila papers and surveys with over 1,000 researchers, resulting in 26 primary benchmarks across domains like NLP, computer vision, reinforcement learning, and graph learning. The benchmarks use popular frameworks like PyTorch and JAX without vendor-specific optimizations to ensure fair comparisons. Results demonstrate that while H100 significantly outperforms A100, newer hardware like MI300X and Gaudi2 underperform relative to their theoretical FLOP capabilities due to software stack maturity issues.

## Method Summary
Milabench employs a comprehensive literature review methodology to select representative AI workloads, using GPT-4o to annotate 867 Mila papers and validate the results with manual annotation of 110 papers. The benchmarks measure end-to-end performance using community-supported libraries (PyTorch, JAX) without vendor-specific optimizations. Performance is aggregated using weighted geometric mean across domains, with results showing H100's 93% improvement over A100 on average. The suite includes both primary benchmarks for comprehensive evaluation and optional synthetic FLOP benchmarks to assess hardware capabilities independent of software maturity.

## Key Results
- H100 outperforms A100 by 93% on average across all benchmarks
- MI300X and Gaudi2 show strong FLOP performance but underperform in real-world workloads due to software stack maturity
- Weighted geometric mean across 26 primary benchmarks provides balanced performance comparison
- BERT achieves 1.5-1.9x speedup with fp16 over fp32 on A100 and H100

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Milabench captures real-world AI workload patterns by selecting benchmarks from an extensive literature review of 867 Mila papers.
- **Mechanism**: The literature review process uses GPT-4o to annotate papers, extracting domains, models, and datasets. Manual validation on 110 papers shows high recall (94%) and precision (94%) for domains, ensuring benchmark selection reflects actual research activity.
- **Core assumption**: GPT-4o annotations accurately represent the research diversity at Mila and can be reliably aggregated into a hierarchical taxonomy.
- **Evidence anchors**:
  - [section]: "To validate the accuracy of GPT-4o and assess the utility of the resulting statistics, we manually annotated 110 papers... These results served as targets to balance the benchmarks in Milabench"
  - [abstract]: "Its design was informed by an extensive literature review encompassing 867 papers, as well as surveys conducted with Mila researchers."
- **Break condition**: If GPT-4o fails to capture emerging research trends or if the hierarchical aggregation obscures important distinctions between domains.

### Mechanism 2
- **Claim**: Milabench ensures fair hardware comparison by using standardized implementations without vendor-specific optimizations.
- **Mechanism**: Benchmarks rely on community-supported libraries (PyTorch, JAX) rather than vendor-contributed pipelines. Performance is measured using geometric mean across all benchmarks, weighted by domain importance.
- **Core assumption**: Community libraries provide comparable performance across hardware vendors when used without vendor-specific optimizations.
- **Evidence anchors**:
  - [section]: "We avoid using models or frameworks contributed by vendors to minimize potential biases in favor of those vendors."
  - [abstract]: "Milabench provides a fair, reproducible, and representative evaluation of AI hardware for procurement and research."
- **Break condition**: If community libraries have vendor-specific optimizations or if hardware differences create unavoidable performance gaps.

### Mechanism 3
- **Claim**: Milabench identifies software stack maturity differences by comparing synthetic FLOP performance with real-world benchmark results.
- **Mechanism**: Optional benchmarks measure raw FLOP performance across data types (fp16, tf32, fp32). Comparing these with main benchmark results reveals discrepancies between theoretical peak performance and actual workload performance.
- **Core assumption**: Software stack maturity affects real-world performance more than hardware specifications alone.
- **Evidence anchors**:
  - [section]: "Despite the strong performance of MI300X and Gaudi2 observed in the synthetic FLOP benchmark, the results do not fully translate into real-world workloads... We attribute this performance disparity between FLOP counts and real-world performance to differences in software stack maturity."
  - [abstract]: "Results show H100 outperforms A100 by 93% on average, while MI300X and Gaudi2 lag due to software stack maturity, despite strong FLOP counts."
- **Break condition**: If software stack differences diminish as vendors mature their AI software support.

## Foundational Learning

- **Concept**: Literature review methodology for benchmark selection
  - **Why needed here**: Understanding how Milabench selects representative benchmarks from academic research requires knowledge of systematic literature review techniques and validation methods.
  - **Quick check question**: What validation approach did Milabench use to ensure GPT-4o annotations accurately represented research domains?

- **Concept**: Geometric mean aggregation for benchmark scoring
  - **Why needed here**: Milabench uses weighted geometric mean to combine performance across diverse benchmarks, requiring understanding of this statistical method versus arithmetic mean.
  - **Quick check question**: Why does Milabench use geometric mean instead of arithmetic mean for aggregating benchmark scores?

- **Concept**: Software stack maturity impact on AI hardware performance
  - **Why needed here**: The performance discrepancies between synthetic FLOP counts and real-world benchmarks highlight the importance of understanding how software ecosystems affect hardware utilization.
  - **Quick check question**: How do software stack differences between CUDA and ROCm/HIP explain performance gaps in Milabench results?

## Architecture Onboarding

- **Component map**: 
  - milabench (benchmark orchestration)
  - benchmate (shared utilities)
  - voir (instrumentation and monitoring)
  - torchcompat (vendor compatibility layer)
  - Individual benchmark packages (isolated with shared dependencies)

- **Critical path**: 
  1. Install dependencies and datasets
  2. Execute benchmarks with timing events
  3. Aggregate results using weighted geometric mean
  4. Generate performance reports

- **Design tradeoffs**:
  - Isolated vs shared dependency environments (size vs conflict prevention)
  - Vendor-agnostic vs vendor-optimized implementations (fairness vs peak performance)
  - Full pipeline vs synthetic benchmarks (real-world relevance vs execution time)

- **Failure signatures**:
  - Missing benchmarks → incomplete coverage
  - Timing synchronization issues → inaccurate measurements
  - Dependency conflicts → benchmark failures
  - Vendor-specific optimizations → biased comparisons

- **First 3 experiments**:
  1. Run bert-tf32-fp16 benchmark on single GPU to verify timing mechanism
  2. Execute resnet50-noio to measure baseline performance without I/O overhead
  3. Compare bert-fp16 vs bert-fp32 to observe precision format performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much of the performance gap between synthetic FLOPs benchmarks and real-world performance is due to software stack immaturity versus hardware architectural differences?
- Basis in paper: [inferred] The paper notes that despite MI300X matching H100 in low-precision FLOPs, it underperforms in real-world benchmarks, attributing this to "software stack maturity."
- Why unresolved: The paper does not provide quantitative analysis isolating software versus hardware contributions to performance differences.
- What evidence would resolve it: Direct comparison of performance improvements when updating software stacks independently, or controlled experiments isolating hardware differences.

### Open Question 2
- Question: What is the optimal balance between benchmark diversity and practical execution time for procurement decisions?
- Basis in paper: [explicit] The authors mention that all 42 benchmarks can be executed in under 2 hours, but note that some hardware configurations required different PyTorch versions, affecting performance comparability.
- Why unresolved: The paper does not explore how reducing the number of benchmarks would impact the representativeness of results or how to weigh computational cost against benchmark coverage.
- What evidence would resolve it: Empirical studies comparing procurement decisions made with different-sized benchmark subsets versus full suite.

### Open Question 3
- Question: How will the performance gap between vendor implementations evolve as software stacks mature, particularly for RL and graph learning domains?
- Basis in paper: [explicit] The authors note that "RL is now leveraging GPUs for environment execution using JAX, introducing new usage patterns" and that graph learning "typically rely on sparse operations, which are not supported by all vendors."
- Why unresolved: The paper only provides a snapshot in time and does not predict how performance gaps will narrow or widen as software stacks develop.
- What evidence would resolve it: Longitudinal studies tracking performance across multiple software stack versions and vendor implementations over time.

## Limitations

- Literature review methodology may not capture all relevant research trends beyond Mila's institutional focus
- Performance comparisons assume comparable hardware architectures, which may not account for architectural differences between vendors
- Results may not generalize to emerging workloads like multimodal models or specialized scientific computing tasks

## Confidence

- **High Confidence**: The methodology for benchmark selection through literature review and researcher surveys is well-documented and validated. The hardware performance comparisons showing H100's 93% improvement over A100 are based on direct measurements using standardized implementations.
- **Medium Confidence**: The claim that software stack maturity explains MI300X and Gaudi2 performance gaps assumes that all other factors (hardware architecture, memory bandwidth, etc.) are comparable. This may not fully account for architectural differences between NVIDIA and AMD/Intel GPUs.
- **Low Confidence**: The extrapolation of results to broader AI hardware procurement decisions may be limited by the specific focus on Mila's research domains and the absence of certain emerging workloads like multimodal models or specialized scientific computing tasks.

## Next Checks

1. Replicate the literature review validation process on an independent set of papers from diverse institutions to assess whether the Mila-centric approach generalizes to broader AI research communities.

2. Test benchmark performance across different PyTorch/JAX versions and with vendor-specific optimizations enabled to quantify the performance gap between fair comparisons and optimized implementations.

3. Conduct additional benchmarking with emerging workloads (multimodal models, scientific computing tasks) to evaluate whether Milabench's current coverage remains representative as AI research evolves.