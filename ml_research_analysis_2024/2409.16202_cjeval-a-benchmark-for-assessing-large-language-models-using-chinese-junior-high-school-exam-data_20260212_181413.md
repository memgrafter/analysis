---
ver: rpa2
title: 'CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior
  High School Exam Data'
arxiv_id: '2409.16202'
source_url: https://arxiv.org/abs/2409.16202
tags:
- question
- uni0000004c
- uni00000048
- uni00000056
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CJEval, a benchmark designed to assess Large\
  \ Language Models (LLMs) using Chinese junior high school exam data. CJEval comprises\
  \ 26,136 samples across four educational tasks\u2014knowledge concept tagging, question\
  \ difficulty prediction, question answering, and question generation\u2014covering\
  \ ten subjects."
---

# CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior High School Exam Data

## Quick Facts
- arXiv ID: 2409.16202
- Source URL: https://arxiv.org/abs/2409.16202
- Reference count: 12
- Primary result: CJEval is a benchmark with 26,136 samples across four educational tasks that evaluates LLMs using Chinese junior high school exam data, showing fine-tuning smaller models on high-quality data significantly enhances performance.

## Executive Summary
This paper introduces CJEval, a benchmark designed to assess Large Language Models (LLMs) using Chinese junior high school exam data. CJEval comprises 26,136 samples across four educational tasks—knowledge concept tagging, question difficulty prediction, question answering, and question generation—covering ten subjects. The benchmark includes detailed annotations such as question types, difficulty levels, knowledge concepts, and answer explanations, providing a comprehensive evaluation framework. Experiments were conducted on various proprietary and open-source LLMs, with fine-tuning using full-parameter and LoRA-based methods. Results indicate that fine-tuning smaller models on high-quality data, particularly with answer explanations, significantly enhances performance. CJEval offers valuable insights into the opportunities and challenges of applying LLMs in education, highlighting the need for improved reasoning and language generation capabilities in educational contexts.

## Method Summary
CJEval is a benchmark for assessing LLMs using Chinese junior high school exam data across four educational tasks: knowledge concept tagging, question difficulty prediction, question answering, and question generation. The benchmark includes 26,136 samples with detailed annotations like question types, difficulty levels, knowledge concepts, and answer explanations. The method involves fine-tuning proprietary and open-source LLMs using both full-parameter and LoRA-based methods on a dataset of 20,820 samples, followed by multi-task evaluation using metrics such as precision, recall, F1-score, TAcc (tolerant accuracy), and EduScore.

## Key Results
- CJEval consists of 26,136 samples across four educational tasks covering ten subjects.
- Fine-tuning smaller models on high-quality data, particularly with answer explanations, significantly enhances performance.
- LoRA-based fine-tuning can outperform full-parameter tuning for educational LLMs, especially when data quality is high.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality, multi-dimensional annotations improve LLM performance in educational tasks more than raw question-answer pairs alone.
- Mechanism: Detailed metadata (question type, difficulty, knowledge concepts, answer explanations) provides richer context for models to learn reasoning patterns, not just memorization.
- Core assumption: Models can effectively utilize structured educational metadata when fine-tuned, and this metadata correlates with improved task generalization.
- Evidence anchors:
  - [abstract]: "Results indicate that fine-tuning smaller models on high-quality data, particularly with answer explanations, significantly enhances performance."
  - [section]: "We utilized both full-parameter and LoRA-based fine-tuning to enhance the models’ ability to answer questions and master knowledge across various subjects."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.567. Weak corpus support for metadata-driven fine-tuning claims.
- Break condition: If models cannot generalize beyond seen metadata patterns or if annotations are noisy/inconsistent, performance gains may plateau or degrade.

### Mechanism 2
- Claim: Task diversity (KCT, QDP, QA, QG) captures broader LLM capabilities than single-task benchmarks.
- Mechanism: Multiple tasks force models to develop complementary skills (classification, regression, generation, reasoning) rather than overfitting to one modality.
- Core assumption: Educational systems require integrated reasoning, not isolated skill execution; diverse tasks better simulate real-world demands.
- Evidence anchors:
  - [abstract]: "CJEval consists of 26,136 samples across four application-level educational tasks covering ten subjects."
  - [section]: "CJEval simultaneously provides four core tasks: knowledge concept tagging, question difficulty prediction, question answering, and question generation."
  - [corpus]: Weak corpus support for multi-task educational benchmarking.
- Break condition: If tasks are too loosely coupled or evaluation metrics poorly aligned, overall benchmark validity suffers.

### Mechanism 3
- Claim: LoRA fine-tuning on high-quality data can outperform full-parameter tuning for educational LLMs.
- Mechanism: LoRA efficiently adapts pre-trained weights to task-specific patterns without catastrophic forgetting, especially when data quality is high.
- Core assumption: Educational tasks benefit from parameter-efficient adaptation when training data is clean, annotated, and representative.
- Evidence anchors:
  - [abstract]: "Results indicate that fine-tuning smaller models on high-quality data... significantly enhances performance."
  - [section]: "Full-parameter fine-tuning did not yield the best results... low-loss, high-efficiency schemes like LoRA are highly valuable."
  - [corpus]: Weak corpus support for LoRA superiority in educational settings.
- Break condition: If task complexity exceeds LoRA’s representational capacity or if data is insufficient, full fine-tuning may be necessary.

## Foundational Learning

- Concept: Multi-label classification for knowledge concept tagging.
  - Why needed here: Models must assign multiple relevant concepts per question, reflecting real educational knowledge structures.
  - Quick check question: If a question covers both "algebra" and "functions," can the model output both labels simultaneously without conflict?

- Concept: Difficulty level encoding and tolerance-based evaluation.
  - Why needed here: Educational difficulty is ordinal and subjective; small deviations shouldn’t be penalized harshly.
  - Quick check question: If true difficulty is "medium" and prediction is "relatively easy," does the tolerance function still count it as correct?

- Concept: Answer explanation integration during fine-tuning.
  - Why needed here: Explanations provide reasoning traces that help models learn problem-solving strategies, not just final answers.
  - Quick check question: Does including answer explanations in training data improve QA accuracy compared to using only question-answer pairs?

## Architecture Onboarding

- Component map: Data ingestion -> Annotation parsing -> Task-specific prompt engineering -> Model fine-tuning (LoRA/Full) -> Multi-task evaluation -> EduScore aggregation.
- Critical path: High-quality annotation creation -> Task-specific prompt formulation -> Efficient fine-tuning pipeline -> Robust evaluation metrics.
- Design tradeoffs: Full-parameter tuning offers maximum flexibility but is resource-heavy; LoRA is efficient but may underfit complex reasoning tasks. Balancing annotation depth vs. scalability is key.
- Failure signatures: Overfitting to metadata patterns (low generalization), poor reasoning on open-ended questions (low QA accuracy), uneven performance across subjects (data imbalance).
- First 3 experiments:
  1. Compare LoRA vs. full fine-tuning on QA task using 10% of annotated data.
  2. Evaluate KCT performance with and without answer explanations in training data.
  3. Test zero-shot vs. one-shot performance across all four tasks to quantify prompt engineering impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of RAG methods impact the performance and applicability of LLMs in educational applications?
- Basis in paper: [explicit] The paper mentions that RAG is optional in the system design and should be applied judiciously, but does not delve into the advantages and disadvantages of RAG methods.
- Why unresolved: The paper focuses on the capabilities of LLMs in handling question-based tasks and does not explore the impact of RAG methods on performance and applicability.
- What evidence would resolve it: Experimental results comparing the performance of LLMs with and without RAG methods in various educational tasks would provide insights into their impact.

### Open Question 2
- Question: What are the specific challenges and limitations of using LLMs for subjects that demand higher-order reasoning skills, such as math, physics, and chemistry?
- Basis in paper: [explicit] The paper notes that performance drops significantly in subjects requiring higher-order reasoning skills, but does not provide detailed analysis of the challenges and limitations.
- Why unresolved: The paper highlights the need for further training of LLMs in applied contexts but does not specify the particular challenges and limitations in these subjects.
- What evidence would resolve it: Detailed analysis of the errors and difficulties encountered by LLMs in these subjects, along with proposed solutions, would clarify the challenges and limitations.

### Open Question 3
- Question: How can the quality of data used for fine-tuning LLMs be further improved to enhance their performance in educational tasks?
- Basis in paper: [explicit] The paper suggests that high-quality data can help LLMs develop capabilities effectively and that fine-tuning on high-quality data is preferable to relying on larger models.
- Why unresolved: The paper does not provide specific strategies or methods for improving the quality of data used for fine-tuning LLMs.
- What evidence would resolve it: Research on data augmentation techniques, data selection methods, and annotation quality improvement would provide insights into enhancing data quality for fine-tuning.

## Limitations

- Data quality and annotation consistency are not detailed, which is critical for understanding the reliability of the benchmark.
- The evaluation metrics robustness, particularly the EduScore aggregation method, is not clearly specified.
- The generalizability of results to other educational systems or age groups is unclear.

## Confidence

- **High Confidence:** The core claim that CJEval provides a comprehensive multi-task benchmark for educational LLMs is well-supported.
- **Medium Confidence:** The claim that fine-tuning smaller models on high-quality data significantly enhances performance is supported but could benefit from more rigorous ablation studies.
- **Low Confidence:** The assertion that LoRA fine-tuning consistently outperforms full-parameter tuning across all educational tasks lacks sufficient experimental validation.

## Next Checks

1. **Replication Study with Alternative Fine-Tuning Methods:** Conduct a controlled experiment comparing LoRA, full-parameter fine-tuning, and other parameter-efficient methods (e.g., adapters) on the same dataset to validate the superiority claims.

2. **Cross-Cultural Generalization Test:** Apply CJEval to educational data from a different country or educational system to assess the benchmark's generalizability and identify potential cultural or linguistic biases.

3. **Annotation Quality Audit:** Perform an inter-annotator agreement analysis on a subset of the data to quantify annotation consistency and reliability, particularly for subjective tasks like difficulty prediction and answer explanation quality.