---
ver: rpa2
title: Written Term Detection Improves Spoken Term Detection
arxiv_id: '2407.04601'
source_url: https://arxiv.org/abs/2407.04601
tags:
- text
- training
- speech
- unpaired
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes JOSTER, a method to incorporate unpaired text
  data into end-to-end keyword search (KWS) systems. The key idea is to jointly train
  the KWS model with an auxiliary text retrieval task, allowing the model to leverage
  large text-only corpora.
---

# Written Term Detection Improves Spoken Term Detection

## Quick Facts
- arXiv ID: 2407.04601
- Source URL: https://arxiv.org/abs/2407.04601
- Reference count: 40
- This paper proposes JOSTER, a method to incorporate unpaired text data into end-to-end keyword search systems, significantly improving search accuracy.

## Executive Summary
This paper addresses the challenge of improving end-to-end keyword search (KWS) by incorporating large amounts of unpaired text data. The proposed JOSTER method jointly trains a KWS model with an auxiliary text retrieval task, allowing the model to leverage text-only corpora to improve document representations. The approach is evaluated across multiple languages (Turkish, Swahili, Lithuanian) and feature types (XLS-R, LPCNet), demonstrating consistent improvements over baseline end-to-end KWS systems. The method also shows promise for domain adaptation when paired speech-text data is scarce.

## Method Summary
JOSTER improves end-to-end keyword search by jointly training the KWS model to retrieve text queries from both spoken and masked written documents. The shared document encoder learns better representations of words and phrases by solving the text-to-text retrieval task in addition to the speech-to-text retrieval task. The method uses masking (rate π=0.3) and duration modeling (ρ=2) to simulate speech-like temporal structure for text inputs. Training uses modified binary cross-entropy loss with tolerance parameter ϕ=0.7 and positive weight λ=5. The model is trained on paired Babel corpus data (10 hours per language) plus unpaired text data (5-6x more text than paired data).

## Key Results
- JOSTER achieves 68.5% MTWV on Turkish development sets vs 55.8% for baseline BeKWS with multilingual pretraining and speed perturbation
- Significant improvements across all languages tested (Turkish, Swahili, Lithuanian) and feature types (XLS-R, LPCNet)
- Effective domain adaptation when using unpaired text from target domain
- Improvements are most pronounced for queries present in unpaired text corpus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JOSTER improves keyword search by jointly training the KWS model to retrieve text queries from both spoken and masked written documents.
- Mechanism: The shared document encoder is forced to learn better representations of words and phrases by solving the text-to-text retrieval task in addition to the speech-to-text retrieval task.
- Core assumption: The text-to-text retrieval task is non-trivial enough to provide meaningful supervision for improving document representations.
- Evidence anchors:
  - [abstract] "we jointly train it to retrieve text queries from masked written documents."
  - [section III-B] "the model is also trained to predict the locations of written queries in masked written sentences."
  - [corpus] Weak: Only 1 related paper mentions multimodal contrastive learning; no direct evidence on masking utility.
- Break condition: If the masking rate is too high (π > 0.9), the text retrieval task becomes too trivial and provides no learning signal.

### Mechanism 2
- Claim: The improvement is achieved because the proposed method improves document representations for words in the unpaired text.
- Mechanism: The shared encoder learns richer, more generalizable representations that benefit both speech and text inputs, particularly for words present in the unpaired text.
- Core assumption: The shared encoder parameters are updated by both tasks in a way that improves their ability to represent word semantics.
- Evidence anchors:
  - [abstract] "These improvements are achieved because the proposed method improves document representations for words in the unpaired text."
  - [section IV-G] "JOSTER improves the document encoder's representation of words which are in the augmentation text regardless of whether or not they actually exist in spoken form in the paired data."
  - [corpus] Missing: No neighbor papers directly support the document representation claim.
- Break condition: If the unpaired text is too small or lacks diversity, the representation improvement is minimal.

### Mechanism 3
- Claim: JOSTER can be used for domain adaptation in settings where in-domain paired data is scarce or nonexistent.
- Mechanism: By training with unpaired text from the target domain, the model learns domain-specific linguistic patterns even without paired speech-text data.
- Core assumption: Text from a domain contains sufficient signal about that domain's linguistic characteristics to improve KWS performance.
- Evidence anchors:
  - [abstract] "we show that the proposed method can be used for domain adaptation in settings where in-domain paired data is scarce or nonexistent."
  - [section IV-H] "Training with unpaired text from a domain improves performance on test sets in that domain, and therefore provides a viable solution for dealing with domain mismatches."
  - [corpus] Weak: Only 1 related paper on multimodal contrastive learning for robustness; no direct evidence on domain adaptation via unpaired text.
- Break condition: If the domain mismatch is too extreme (e.g., very different acoustic conditions), unpaired text alone may not suffice.

## Foundational Learning

- Concept: Multitask learning
  - Why needed here: JOSTER trains two retrieval tasks simultaneously to improve the shared document encoder.
  - Quick check question: What is the key difference between joint training and sequential training in this context?

- Concept: Masking and duration modeling
  - Why needed here: Text documents are masked and repeated to simulate speech-like temporal structure for the text encoder.
  - Quick check question: Why do we mask text tokens before feeding them to the text encoder?

- Concept: End-to-end keyword search (KWS)
  - Why needed here: JOSTER is an E2E KWS system that predicts query occurrence probabilities directly from audio.
  - Quick check question: How does E2E KWS differ from ASR-based KWS in terms of training and indexing?

## Architecture Onboarding

- Component map: Query encoder (shared) -> Document encoder (shared) -> Dot product + sigmoid -> Per-frame occurrence probabilities
- Critical path:
  1. Encode query with shared query encoder
  2. Encode document with modality-specific encoder → shared document encoder
  3. Compute dot product and sigmoid to get per-frame occurrence probabilities
- Design tradeoffs:
  - Masking rate π: Higher masking increases regularization but risks making task too hard
  - Duration parameter ρ: Longer duration increases computation but may help with temporal modeling
  - Number of shared layers: More shared layers allow better transfer but risk interference
- Failure signatures:
  - Degraded performance on OO queries (out-of-vocabulary in both LLP and FLP)
  - Performance improvement only on queries present in unpaired text
  - Sensitivity to masking rate and duration parameters
- First 3 experiments:
  1. Train JOSTER vs baseline on same data; compare MTWV on dev set
  2. Vary masking rate π (0, 0.15, 0.3, 0.6, 0.9); measure impact on MTWV
  3. Use unpaired text from different domains; test domain adaptation on target domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating TTS-based unpaired text augmentation affect KWS performance compared to JOSTER, particularly in noisy test sets?
- Basis in paper: [explicit] The paper compares JOSTER with TTS-based text augmentation and notes that TTS performs better on "clean" test sets but worse on more acoustically-challenging "other" sets.
- Why unresolved: The paper does not provide a detailed analysis of the impact of TTS-based augmentation in different acoustic conditions or explore ways to improve its performance on noisy test sets.
- What evidence would resolve it: Conducting experiments with TTS-based augmentation using artificially generated noise, reverberations, or room impulse responses and comparing the results to JOSTER on various test sets with different acoustic conditions.

### Open Question 2
- Question: How can approximate inner-product search methods like hashing or vector quantization be effectively applied to improve the deployment efficiency of E2E-KWS systems?
- Basis in paper: [explicit] The paper mentions that E2E-KWS systems rely on inner-product based search in vector spaces and could benefit from approximate inner-product search methods to build fast vector indexes with sub-linear memory cost.
- Why unresolved: The paper does not provide specific implementation details or performance comparisons of these methods for E2E-KWS systems.
- What evidence would resolve it: Implementing and evaluating various approximate inner-product search methods for E2E-KWS systems, including their impact on search accuracy, speed, and memory usage.

### Open Question 3
- Question: How can JOSTER be extended to other spoken retrieval tasks such as hotword spotting and spoken question-answering, and what are the potential challenges and benefits of such extensions?
- Basis in paper: [inferred] The paper suggests extending JOSTER to other spoken retrieval tasks where available paired text-to-text data dwarfs paired speech-to-text data, and mentions hotword spotting and spoken question-answering as examples.
- Why unresolved: The paper does not provide a detailed analysis of the specific requirements, challenges, and potential benefits of applying JOSTER to these tasks.
- What evidence would resolve it: Conducting experiments with JOSTER on hotword spotting and spoken question-answering tasks, including a detailed analysis of the performance improvements, challenges encountered, and potential modifications to the model or training process.

## Limitations

- Performance gains are most pronounced for queries present in unpaired text corpus, raising questions about effectiveness for truly out-of-vocabulary queries
- Method introduces additional hyperparameters (π and ρ) that may require careful tuning for different languages and domains
- Computational overhead of processing masked text documents during training could be substantial for large unpaired text corpora

## Confidence

**High Confidence**: The core claim that JOSTER improves KWS performance compared to baseline end-to-end systems is well-supported by extensive experimental results across multiple languages and feature types.

**Medium Confidence**: The claim about domain adaptation effectiveness has moderate support but limited validation scope and doesn't explore extreme domain mismatches.

**Low Confidence**: The mechanism explanation that improvements are "achieved because the proposed method improves document representations for words in the unpaired text" lacks direct empirical validation.

## Next Checks

1. **Out-of-Vocabulary Query Performance**: Conduct systematic experiments measuring JOSTER's performance on queries completely absent from both paired speech data and unpaired text data to validate generalization beyond words present in unpaired text.

2. **Cross-Domain Transfer Analysis**: Test JOSTER's domain adaptation capability by training on unpaired text from one domain and evaluating on a significantly different domain (e.g., conversational speech vs broadcast news) to reveal robustness to domain mismatches.

3. **Hyperparameter Sensitivity Study**: Perform comprehensive analysis of how JOSTER's performance varies with different masking rates (π) and duration parameters (ρ) across multiple languages to establish best practices for hyperparameter selection.