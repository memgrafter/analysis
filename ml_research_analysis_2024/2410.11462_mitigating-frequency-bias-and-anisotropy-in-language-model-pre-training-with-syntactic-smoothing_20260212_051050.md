---
ver: rpa2
title: Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with
  Syntactic Smoothing
arxiv_id: '2410.11462'
source_url: https://arxiv.org/abs/2410.11462
tags:
- language
- tokens
- anisotropy
- smoothing
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of frequency bias in language models,
  where models rely too heavily on frequent tokens and struggle with infrequent ones.
  The authors introduce Syntactic Smoothing, a method that distributes the learning
  signal during pre-training to syntactically similar tokens, helping infrequent tokens
  learn from more frequent ones.
---

# Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing
## Quick Facts
- arXiv ID: 2410.11462
- Source URL: https://arxiv.org/abs/2410.11462
- Reference count: 24
- The paper introduces Syntactic Smoothing to reduce frequency bias in language models, showing improved generalization to rare tokens and reduced representation anisotropy

## Executive Summary
This paper addresses the pervasive problem of frequency bias in language models, where models disproportionately rely on frequent tokens while struggling with rare ones. The authors propose Syntactic Smoothing, a novel pre-training method that distributes learning signals across syntactically similar tokens, allowing infrequent tokens to benefit from the stronger learning signals of their more common counterparts. They develop a new metric to quantify frequency bias based on sentence-level perplexity relative to token frequency. The method is evaluated on a small English language model, demonstrating reduced frequency bias, improved performance on linguistic tasks (BLiMP), and lower representation anisotropy compared to standard training approaches.

## Method Summary
The paper introduces Syntactic Smoothing as a method to mitigate frequency bias during language model pre-training. The approach works by identifying syntactic neighbors for each token and distributing the learning signal from frequent tokens to their syntactically similar but less frequent counterparts. During training, when a frequent token is encountered, the model's loss is computed not only for that token but also for its syntactic neighbors, effectively transferring knowledge. The authors develop a novel metric to quantify frequency bias by measuring the relationship between sentence-level perplexity and token frequency, allowing them to track improvements systematically. This syntactic guidance helps infrequent tokens learn from more frequent ones that share similar syntactic properties.

## Key Results
- Syntactic Smoothing successfully reduces frequency bias in a small English language model, as measured by the proposed perplexity-frequency correlation metric
- Models trained with syntactic smoothing show improved performance on BLiMP linguistic tasks, demonstrating better generalization to rare tokens
- The method reduces representation anisotropy (clustering) in the model's embedding space, with lower frequency bias models exhibiting lower anisotropy

## Why This Works (Mechanism)
Syntactic Smoothing works by leveraging syntactic relationships between tokens to redistribute learning signals during pre-training. When a model encounters a frequent token with many syntactic neighbors, the learning signal is shared among all these related tokens. This mechanism allows infrequent tokens to benefit from the stronger gradient signals that frequent tokens receive, effectively bootstrapping their learning. The syntactic similarity ensures that the transferred knowledge is relevant and meaningful, as tokens that share syntactic properties are likely to behave similarly in language contexts.

## Foundational Learning
1. **Frequency Bias**: The tendency of language models to rely disproportionately on frequent tokens, leading to poor performance on rare tokens. Needed to understand the core problem being addressed; quick check: compare model performance on frequent vs. rare tokens.

2. **Representation Anisotropy**: The phenomenon where word representations cluster in certain directions in embedding space, reducing representational capacity. Needed to understand the secondary benefit of syntactic smoothing; quick check: measure cosine similarity distribution across token pairs.

3. **Syntactic Neighborhoods**: Groups of tokens that share similar syntactic properties (part-of-speech, syntactic roles). Needed to understand how knowledge transfer occurs; quick check: verify syntactic similarity metrics produce meaningful groupings.

4. **Perplexity-Frequency Correlation**: The metric developed to quantify frequency bias by measuring how sentence perplexity relates to token frequency. Needed to evaluate the effectiveness of mitigation strategies; quick check: establish baseline correlation in standard models.

## Architecture Onboarding
**Component Map**: Input text -> Tokenizer -> Embedding layer -> Transformer blocks -> Output layer -> Loss computation (standard + syntactic smoothing)

**Critical Path**: During training, frequent tokens trigger expanded loss computation that includes their syntactic neighbors, creating a knowledge transfer pathway from frequent to infrequent tokens through shared syntactic properties.

**Design Tradeoffs**: The method adds computational overhead during training due to expanded loss computation but doesn't require architectural changes. The tradeoff favors improved generalization to rare tokens at the cost of increased training time.

**Failure Signatures**: If syntactic neighborhoods are poorly defined, the method could transfer irrelevant knowledge. If frequency bias is too severe, even syntactic smoothing may not provide sufficient gradient signals for rare tokens.

**First 3 Experiments**:
1. Measure baseline frequency bias using the perplexity-frequency correlation metric on a standard model
2. Apply syntactic smoothing and verify reduction in the frequency bias metric
3. Evaluate downstream performance on BLiMP to confirm improved linguistic generalization

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit: whether the results generalize to larger models, how the method performs across different languages, and whether the relationship between frequency bias and anisotropy is causal or merely correlational.

## Limitations
- Results are demonstrated only on a small English language model, limiting generalizability to larger or multilingual models
- The proposed frequency bias metric requires further validation across different datasets and languages to confirm its reliability
- The relationship between frequency bias and anisotropy, while observed, needs more investigation to establish causation

## Confidence
- High: The syntactic smoothing method effectively reduces frequency bias in the evaluated small English model
- Medium: The proposed frequency bias metric reliably quantifies the phenomenon across different settings
- Medium: The observed relationship between frequency bias and anisotropy represents a general principle in language models

## Next Checks
1. Evaluate syntactic smoothing on larger models (e.g., 1B+ parameters) to assess scalability and performance impact
2. Test the frequency bias metric across multiple languages and corpora to verify its generalizability
3. Conduct ablation studies to isolate the contribution of syntactic smoothing from other factors in reducing anisotropy