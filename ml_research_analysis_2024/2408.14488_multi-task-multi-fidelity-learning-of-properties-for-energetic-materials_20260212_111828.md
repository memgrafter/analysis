---
ver: rpa2
title: Multi-Task Multi-Fidelity Learning of Properties for Energetic Materials
arxiv_id: '2408.14488'
source_url: https://arxiv.org/abs/2408.14488
tags:
- properties
- detonation
- materials
- data
- energetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of data scarcity in predicting
  properties of energetic materials, a critical issue for accelerating materials discovery
  and screening. To overcome this limitation, the authors compile a multi-modal dataset
  containing both experimental and computational data for various properties of energetic
  materials, including detonation velocity, detonation pressure, heat of detonation,
  Gurney energy, impact sensitivity, and enthalpic properties.
---

# Multi-Task Multi-Fidelity Learning of Properties for Energetic Materials

## Quick Facts
- arXiv ID: 2408.14488
- Source URL: https://arxiv.org/abs/2408.14488
- Authors: Robert J. Appleton; Daniel Klinger; Brian H. Lee; Michael Taylor; Sohee Kim; Samuel Blankenship; Brian C. Barnes; Steven F. Son; Alejandro Strachan
- Reference count: 40
- Primary result: Multi-task neural networks outperform single-task models for predicting energetic material properties, with 30.2% reduction in RMSE for experimental detonation pressure

## Executive Summary
This work addresses the challenge of data scarcity in predicting properties of energetic materials by developing multi-task neural networks (MT-NNs) that can learn from multi-modal data containing both experimental and computational sources. The authors compile a comprehensive dataset of 643 unique energetic materials and demonstrate that MT-NNs significantly outperform traditional single-task models, particularly for properties with limited experimental data. The key innovation is the use of a selector vector that allows the model to develop general material descriptors from the entire dataset and use them to predict various properties simultaneously.

## Method Summary
The method involves compiling a multi-modal dataset of energetic materials containing both experimental and computational data for properties including detonation velocity, detonation pressure, heat of detonation, Gurney energy, impact sensitivity, and enthalpic properties. Multi-task neural networks are developed with a selector vector that indicates which property to predict, enabling the model to learn common material representations across all properties. The models use molecular descriptors derived from SMILES strings as features, and performance is evaluated through 5-fold cross-validation with hyperparameter optimization using HyperBand. The approach is compared against single-task random forests and dense neural networks as benchmarks.

## Key Results
- MT-NNs achieve 30.2% reduction in RMSE for experimental detonation pressure compared to best single-task models
- 13.9% reduction in RMSE for experimental heat of detonation using multi-task learning
- MT-NN trained on all properties simultaneously (MT-NN-all) is most accurate for several properties
- MT-NN using only molecular descriptors shows comparable performance to graph-based models for impact sensitivity prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task neural networks with selector vectors can learn common material descriptors from multi-modal data to improve predictions across properties.
- Mechanism: The MT-NN architecture incorporates a selector vector that indicates which output property to predict. This allows the model to develop general descriptors from the entire dataset and use them to predict various outputs simultaneously. The model learns common abstractions for each material from all available data.
- Core assumption: Properties of energetic materials share underlying features that can be learned jointly, and these shared features improve prediction accuracy for individual properties.
- Evidence anchors:
  - [abstract]: "multi-task neural networks can learn from multi-modal data and outperform single-task models trained for specific properties"
  - [section 3]: "The selector vector is a one-hot encoding of the possible outputs... the model will develop general descriptors of the problem at hand from the entirety of the data, and then use these descriptors to predict the various outputs"
  - [corpus]: No direct corpus evidence available for this specific mechanism
- Break condition: If properties are uncorrelated or share no meaningful features, the selector vector approach would not provide benefits and could even harm performance.

### Mechanism 2
- Claim: Multi-task learning provides greater improvements for properties with limited data availability.
- Mechanism: By training on the full dataset, the model can leverage information from abundant data sources (computational data) to improve predictions on scarce data sources (experimental data). The MT-NN uses correlations between properties to transfer knowledge.
- Core assumption: There are meaningful correlations between properties that can be exploited to transfer knowledge from well-represented properties to poorly-represented ones.
- Evidence anchors:
  - [abstract]: "the improvement is more significant for data-scarce properties"
  - [section 4]: "The properties that show the most significant improvement from multi-task learning are the experimental detonation pressure (ğ‘ƒ exp) (30.2% reduction in ğ‘…ğ‘€ğ‘†ğ¸Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì… compared to the best ST model) and experimental heat of detonation (ğ‘„ğ‘’ğ‘¥ exp) (13.9% reduction in ğ‘…ğ‘€ğ‘†ğ¸Ì…Ì…Ì…Ì…Ì…Ì…Ì…Ì… compared to the best ST model), where the data is extremely limited (< 70 datapoints)"
  - [section 2]: "We note the strong positive correlation between experimental and theoretical detonation properties and the strong negative correlation between the detonation properties and the impact sensitivity data"
- Break condition: If correlations between properties are weak or nonexistent, multi-task learning would not provide the expected benefits for data-scarce properties.

### Mechanism 3
- Claim: The choice of featurization scheme significantly impacts model performance, with molecular descriptors alone being sufficient for some properties.
- Mechanism: The paper uses a comprehensive set of molecular descriptors derived from SMILES strings, including electrotopological state vectors, functional group counts, bond descriptors, atomic counts, and oxygen balance. These descriptors capture both structural and chemical information necessary for property prediction.
- Core assumption: Simple molecular descriptors can capture the essential information needed to predict energetic material properties without requiring additional features like density.
- Evidence anchors:
  - [section 3]: "We use SMILES strings to represent the molecular structure of each material as a graph. From this graph, we can extract features using a combination of tools from the mmltoolkit python package, RDKit, and Mordred"
  - [section 4]: "models trained without density perform well on sensitivity and thermodynamic properties... The model can be very useful for high-level screening of new energetic materials because it allows for predictions of impact sensitivity from only the SMILES string and no other prior knowledge"
  - [section 4]: "The MT-NN trained with subset 2 which includes the molecular descriptors and the density as input features, was found to be noticeably less accurate. This indicates that though the density is a critical input feature for predicting the detonation properties of these materials it can hurt the predictability of the sensitivity"
- Break condition: If the molecular descriptors fail to capture critical information for certain properties, models using only these features would underperform compared to models with additional descriptors.

## Foundational Learning

- Concept: Multi-task learning
  - Why needed here: Energetic materials have multiple related properties (detonation characteristics, sensitivity, thermodynamic properties) where some properties have abundant data while others are scarce. Multi-task learning allows the model to learn shared representations across all properties.
  - Quick check question: What is the key difference between single-task and multi-task learning in the context of this paper?

- Concept: Multi-modal data integration
  - Why needed here: The dataset contains both experimental data (high-fidelity, scarce) and computational data (low-fidelity, abundant) for the same properties. Integrating these data sources allows the model to leverage abundant computational data to improve predictions on scarce experimental data.
  - Quick check question: How does the paper define "high-fidelity" and "low-fidelity" data in the context of energetic materials?

- Concept: Molecular featurization
  - Why needed here: The models need to represent molecular structures in a way that captures both structural and chemical information relevant to energetic material properties. The featurization scheme converts SMILES strings into numerical descriptors that can be used by machine learning models.
  - Quick check question: What are the five main categories of descriptors used in this work?

## Architecture Onboarding

- Component map: Input descriptors â†’ Selector vector â†’ Dense layers â†’ Output node
- Critical path: Featurization â†’ Model training with selector vector â†’ Property prediction
- Design tradeoffs:
  - Using density as input: Improves detonation property predictions but hurts sensitivity predictions
  - Model complexity: More complex models may overfit the limited experimental data
  - Feature selection: Including too many features may introduce noise, while too few may miss important information
- Failure signatures:
  - Poor performance on properties with abundant data suggests the model is not learning the shared representations correctly
  - Inconsistent performance across different property subsets indicates the selector vector mechanism may not be working properly
  - Overfitting to computational data while underperforming on experimental data suggests poor multi-fidelity learning
- First 3 experiments:
  1. Train a single-task neural network on experimental detonation velocity data only to establish baseline performance
  2. Train the multi-task neural network on all properties including experimental and computational detonation data to verify the selector vector mechanism works
  3. Compare single-task vs multi-task performance on experimental heat of detonation (data-scarce property) to validate the data-scarce property improvement hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multi-task neural networks compare when applied to different types of energetic materials beyond CHNOClF molecules, such as composite mixtures or materials containing metals?
- Basis in paper: [inferred] The authors explicitly state that their dataset does not contain composite mixtures or metals, suggesting a potential limitation in generalizability.
- Why unresolved: The paper does not explore the applicability of their multi-task neural network approach to other types of energetic materials, leaving open the question of how well the method would perform in these cases.
- What evidence would resolve it: Testing the multi-task neural network on a dataset containing composite mixtures or metals and comparing its performance to the current results would provide insight into the method's generalizability.

### Open Question 2
- Question: What is the impact of using different featurization schemes, such as graph-based representations, on the performance of multi-task neural networks for predicting properties of energetic materials?
- Basis in paper: [explicit] The authors mention that graph-based models, specifically Chemprop, were recently published and achieved comparable performance to their model, suggesting that alternative featurization schemes could be explored.
- Why unresolved: The paper focuses on using SMILES strings and molecular descriptors, but does not explore the potential benefits of using graph-based representations or other featurization schemes.
- What evidence would resolve it: Implementing and testing multi-task neural networks using graph-based representations or other featurization schemes on the same dataset and comparing their performance to the current results would provide insight into the impact of featurization on model performance.

### Open Question 3
- Question: How does the performance of multi-task neural networks change when applied to larger and more diverse datasets of energetic materials, potentially including more properties or data from different sources?
- Basis in paper: [inferred] The authors mention that their dataset is relatively small and limited to specific properties, suggesting that the performance of multi-task neural networks could be affected by the size and diversity of the dataset.
- Why unresolved: The paper does not explore the impact of dataset size and diversity on the performance of multi-task neural networks, leaving open the question of how well the method would scale to larger and more diverse datasets.
- What evidence would resolve it: Testing the multi-task neural network on larger and more diverse datasets of energetic materials and comparing its performance to the current results would provide insight into the scalability and robustness of the method.

## Limitations
- The dataset relies on existing computational sources that may contain systematic biases not accounted for in the analysis
- Molecular descriptor approach may miss complex structural relationships that graph-based methods could capture
- The method has not been validated on energetic materials beyond CHNOClF molecules, such as composites or metal-containing compounds

## Confidence
- **High confidence**: The core claim that multi-task neural networks outperform single-task models for energetic material property prediction is well-supported by systematic cross-validation results across multiple properties.
- **Medium confidence**: The mechanism by which selector vectors enable shared learning across properties is theoretically sound but not extensively validated through ablation studies or visualization of learned representations.
- **Medium confidence**: The claim that molecular descriptors alone are sufficient for impact sensitivity prediction is supported by empirical results but would benefit from comparison to more recent graph-based approaches on the same dataset.

## Next Checks
1. **Ablation study on selector vector**: Remove the selector vector mechanism and retrain the model to quantify the exact contribution of multi-task learning versus the base neural network architecture.

2. **Data quality validation**: Conduct a systematic analysis of the computational datasets used, including comparison of property distributions across different computational methods and identification of potential outliers or systematic biases.

3. **Feature importance analysis**: Perform permutation importance or SHAP analysis to understand which molecular descriptors contribute most to predictions for different properties, particularly focusing on why density inclusion hurts sensitivity predictions.