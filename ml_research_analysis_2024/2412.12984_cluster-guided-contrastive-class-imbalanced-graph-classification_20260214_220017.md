---
ver: rpa2
title: Cluster-guided Contrastive Class-imbalanced Graph Classification
arxiv_id: '2412.12984'
source_url: https://arxiv.org/abs/2412.12984
tags:
- graph
- learning
- class
- classes
- subclass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of class-imbalanced graph classification,\
  \ where standard graph neural networks are biased towards majority classes. To address\
  \ this, the authors propose C\xB3GNN, a framework that combines clustering and supervised\
  \ contrastive learning."
---

# Cluster-guided Contrastive Class-imbalanced Graph Classification

## Quick Facts
- arXiv ID: 2412.12984
- Source URL: https://arxiv.org/abs/2412.12984
- Reference count: 13
- This paper proposes C³GNN, achieving up to 47.54% accuracy on COIL-DEL with IF=20 for imbalanced graph classification.

## Executive Summary
This paper addresses class-imbalanced graph classification by proposing C³GNN, a framework that combines clustering and supervised contrastive learning. The method clusters majority-class graphs into subclasses comparable in size to minority classes, uses Mixup to enrich semantic diversity, and applies hierarchical contrastive learning to learn effective representations. Experiments on six real-world datasets demonstrate significant performance improvements over baseline methods, with accuracy gains of up to 47.54% on COIL-DEL with imbalance factor 20.

## Method Summary
C³GNN tackles class-imbalanced graph classification by first adaptively clustering graphs from each majority class into multiple subclasses, ensuring each subclass has a sample size comparable to the minority class. Within each subclass, Mixup interpolation generates synthetic samples to enrich semantic diversity. The framework then applies hierarchical supervised contrastive learning at both intra-subclass and inter-subclass levels to learn discriminative graph representations. The method uses GraphSAGE as the encoder, with cluster centers updated every 10 epochs during training.

## Key Results
- C³GNN achieves up to 47.54% accuracy on COIL-DEL with imbalance factor 20
- Outperforms baseline methods including GraphSAGE, CB loss, LACE loss, and GraphCL
- Demonstrates consistent improvements across six real-world graph datasets
- Shows effectiveness in balancing learning between majority and minority classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing majority classes into subclasses balances sample sizes and preserves hierarchical semantic substructures.
- Mechanism: Clustering each majority class into multiple subclasses with sizes comparable to the minority class ensures balanced treatment across all subclasses during training.
- Core assumption: Majority class graphs exhibit rich hierarchical substructures that can be meaningfully partitioned into semantically coherent subclasses.
- Evidence anchors:
  - [abstract]: "C³GNN clusters graphs from each majority class into multiple subclasses, with sizes comparable to the minority class, mitigating class imbalance."
  - [section]: "To capture hierarchical semantic substructures of the majority classes, C³GNN first adaptively clusters graphs of each majority class into multiple subclasses, ensuring that the sample sizes in each subclass are comparable to the minority class."
  - [corpus]: Weak. No direct corpus evidence supporting the hierarchical substructure assumption for majority classes.
- Break condition: If majority class graphs lack meaningful hierarchical substructures or clustering fails to produce semantically coherent subclasses.

### Mechanism 2
- Claim: Mixup interpolation enriches semantic diversity within subclasses and prevents representation collapse.
- Mechanism: Generating synthetic samples through Mixup interpolation within each subclass increases sample diversity and semantic richness, avoiding trivial solutions from sparse subclass samples.
- Core assumption: Sparse subclass samples lead to representation collapse, and Mixup effectively enriches semantic information without introducing noise.
- Evidence anchors:
  - [abstract]: "It also employs the Mixup technique to generate synthetic samples, enriching the semantic diversity of each subclass."
  - [section]: "We introduce the Mixup technique (Zhang et al. 2017) to synthesize new samples within subclasses, thereby increasing the diversity of samples and enriching their semantic structure."
  - [corpus]: Weak. No direct corpus evidence supporting Mixup's effectiveness specifically for class-imbalanced graph classification.
- Break condition: If Mixup interpolation creates semantically inconsistent samples or if the synthetic samples introduce more noise than benefit.

### Mechanism 3
- Claim: Hierarchical contrastive learning captures both intra-subclass and inter-subclass relationships, enabling effective representation learning.
- Mechanism: Supervised contrastive learning is applied at two levels: intra-subclass (encouraging similarity within subclasses) and inter-subclass (encouraging similarity across subclasses of the same class but different subclasses).
- Core assumption: Learning representations at both subclass and class levels provides complementary information that improves classification performance.
- Evidence anchors:
  - [abstract]: "Furthermore, supervised contrastive learning is used to hierarchically learn effective graph representations, enabling the model to thoroughly explore semantic substructures in majority classes while avoiding excessive focus on minority classes."
  - [section]: "We leverage supervised contrastive learning (Khosla et al. 2020) to effectively learn discriminative representations from both intra-subclass and inter-subclass perspectives."
  - [corpus]: Weak. No direct corpus evidence supporting the specific hierarchical contrastive learning approach for imbalanced graph classification.
- Break condition: If the contrastive learning objectives conflict or if the model fails to learn meaningful representations at both levels.

## Foundational Learning

- Graph Neural Networks:
  - Why needed here: GNNs are the primary mechanism for learning graph-level representations, which are essential for graph classification tasks.
  - Quick check question: How do GNNs aggregate information from neighboring nodes to create node embeddings?

- Contrastive Learning:
  - Why needed here: Contrastive learning helps the model learn discriminative representations by comparing positive and negative pairs, which is crucial for handling class imbalance.
  - Quick check question: What is the difference between supervised and unsupervised contrastive learning?

- Clustering Algorithms:
  - Why needed here: Clustering is used to partition majority classes into subclasses, which is a key step in balancing the class distribution and preserving semantic substructures.
  - Quick check question: How does k-means clustering work, and what are its limitations?

## Architecture Onboarding

- Component map: GraphSAGE encoder -> Adaptive clustering -> Mixup interpolation -> Hierarchical contrastive learning (intra-subclass and inter-subclass)

- Critical path:
  1. Extract graph-level representations using GNN encoder
  2. Apply adaptive clustering to partition majority classes into subclasses
  3. Generate synthetic samples using Mixup within each subclass
  4. Apply hierarchical contrastive learning to learn discriminative representations

- Design tradeoffs:
  - Tradeoff between subclass size and number of subclasses: Larger subclasses may retain more semantic information but reduce the number of subclasses, potentially limiting the benefits of hierarchical learning.
  - Choice of clustering algorithm: Different clustering algorithms may produce different subclass partitions, affecting the quality of learned representations.

- Failure signatures:
  - Poor performance on minority classes: Indicates insufficient focus on minority class samples during training.
  - Overfitting to majority classes: Suggests the model is not effectively learning from minority classes or the subclasses are not semantically meaningful.
  - Representation collapse: May indicate that Mixup interpolation is not effectively enriching semantic diversity within subclasses.

- First 3 experiments:
  1. Evaluate the impact of different clustering algorithms (e.g., k-means, DBSCAN) on subclass quality and model performance.
  2. Assess the effectiveness of Mixup interpolation by comparing models with and without Mixup on sample diversity and classification accuracy.
  3. Test the hierarchical contrastive learning approach by comparing models with only intra-subclass or only inter-subclass contrastive learning against the full hierarchical approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed C3GNN framework scale with graph size and complexity, particularly when dealing with large-scale graph datasets?
- Basis in paper: [inferred] The paper discusses the computational complexity of C3GNN, which is O(ND(|V| + B)), where N is the number of graphs, D is the embedding dimension, |V| is the average number of nodes in input graphs, and B is the batch size. However, the paper does not provide extensive experiments or analysis on how the framework performs with very large graphs or datasets.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis on the scalability of C3GNN with increasing graph size and complexity. This is an important aspect to consider for real-world applications where datasets can be very large.
- What evidence would resolve it: Conducting experiments with larger and more complex graph datasets, and analyzing the computational time and memory usage of C3GNN, would provide insights into its scalability. Additionally, comparing the performance of C3GNN with other methods on large-scale datasets would help in understanding its efficiency.

### Open Question 2
- Question: How does the performance of C3GNN vary with different types of graph augmentations, and what is the optimal combination of augmentations for different datasets?
- Basis in paper: [explicit] The paper mentions that graph-level representations of augmented graphs are extracted using a GNN-based encoder and discusses the use of stochastic graph augmentations T(·|G) to obtain correlated views for contrastive learning. However, it does not provide a detailed analysis of how different augmentations affect the performance or what the optimal combination is for different datasets.
- Why unresolved: The paper does not explore the impact of various graph augmentation techniques on the performance of C3GNN. Different augmentations might have varying effects on different datasets, and finding the optimal combination could further improve the framework's effectiveness.
- What evidence would resolve it: Conducting experiments with different combinations of graph augmentations and analyzing their impact on the performance of C3GNN across various datasets would provide insights into the optimal augmentation strategies. This could involve comparing the results of C3GNN with different augmentation techniques and identifying the most effective combinations for each dataset.

### Open Question 3
- Question: How does the proposed C3GNN framework handle the presence of noise and outliers in graph data, and what are the potential impacts on its performance?
- Basis in paper: [inferred] The paper does not explicitly discuss the robustness of C3GNN to noise and outliers in graph data. However, the presence of noise and outliers is a common challenge in real-world graph datasets, and it could potentially affect the performance of any graph-based learning method.
- Why unresolved: The paper does not provide any analysis or experiments on the robustness of C3GNN to noisy or outlier data. Understanding how the framework handles such scenarios is crucial for its practical applicability.
- What evidence would resolve it: Conducting experiments with graph datasets containing varying levels of noise and outliers, and analyzing the performance of C3GNN under these conditions, would provide insights into its robustness. Additionally, comparing the results with other methods that are known to be robust to noise would help in evaluating the effectiveness of C3GNN in such scenarios.

## Limitations

- Limited theoretical analysis of the hierarchical contrastive learning framework's effectiveness
- No direct validation of the assumption that majority classes contain meaningful hierarchical substructures
- Lack of experiments on very large-scale graph datasets to demonstrate scalability

## Confidence

- Mechanism 1 (Clustering for subclass balance): Medium - Empirical results support the approach, but the assumption of meaningful hierarchical substructures in majority classes is unverified
- Mechanism 2 (Mixup for semantic enrichment): Low - Limited theoretical or empirical support for Mixup's effectiveness on graph data in imbalanced settings
- Mechanism 3 (Hierarchical contrastive learning): Medium - The approach is novel but lacks ablation studies isolating the benefits of intra- vs inter-subclass contrastive learning

## Next Checks

1. Conduct ablation studies to isolate the contribution of intra-subclass vs inter-subclass contrastive learning components
2. Perform qualitative analysis of clustering results to verify that majority class subclasses are semantically coherent
3. Test Mixup interpolation on synthetic graph datasets with known semantic structures to verify it preserves meaningful relationships rather than introducing noise