---
ver: rpa2
title: Understanding Factual Recall in Transformers via Associative Memories
arxiv_id: '2412.06538'
source_url: https://arxiv.org/abs/2412.06538
tags:
- alpha
- beta
- number
- where
- associative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how transformers can efficiently store and
  retrieve factual information by viewing their weights as associative memories. The
  authors first prove that both linear and MLP associative memories can store a number
  of associations proportional to their parameter count when embeddings are random
  and non-orthogonal.
---

# Understanding Factual Recall in Transformers via Associative Memories

## Quick Facts
- arXiv ID: 2412.06538
- Source URL: https://arxiv.org/abs/2412.06538
- Reference count: 40
- Primary result: Single-layer transformers can store factual associations with parameters scaling linearly with dataset size

## Executive Summary
This paper analyzes how transformers store and retrieve factual information by viewing their weights as associative memories. The authors prove that both linear and MLP associative memories can store a number of associations proportional to their parameter count when embeddings are random and non-orthogonal. They introduce a synthetic factual recall task and show that a single-layer transformer can achieve 100% accuracy when either self-attention or MLP parameters scale linearly with dataset size, demonstrating a trade-off between these two storage mechanisms.

## Method Summary
The paper introduces a synthetic factual recall task where a transformer must extract subject-relation pairs from noisy contexts and map them to correct answers. A single-layer transformer with multi-head self-attention followed by an MLP is trained using online batch gradient descent with batch size 1024 on the population loss. The analysis focuses on random embeddings on a sphere and proves storage capacity bounds for both attention-based and MLP-based associative memory schemes.

## Key Results
- Linear associative memories can store O(d²) associations with d² parameters when embeddings are random and non-orthogonal
- MLP associative memories can store O(d³) associations with d² parameters using high-degree Hermite polynomial features
- Single-layer transformers achieve 100% accuracy on synthetic factual recall when parameters scale linearly with dataset size
- Transformers undergo sequential learning with an intermediate "hallucination" stage where they predict based only on relation tokens
- O(SR) parameters are necessary and sufficient for perfect accuracy on the factual recall task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can store factual associations using self-attention as associative memory
- Mechanism: Self-attention heads act as denoisers to isolate subject-relation tokens from noise, then use value matrices to store associations via outer products
- Core assumption: Embeddings are random and non-orthogonal, allowing superposition of associations
- Evidence anchors: [abstract] "shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity"; [section 4] "The key and query matrices of each self-attention head act as a denoiser, selecting the relevant subject and relation tokens"
- Break condition: If embeddings are orthogonal, storage capacity drops to d instead of d²

### Mechanism 2
- Claim: MLP layers can store factual associations using polynomial feature expansion
- Mechanism: MLP with random feature transformation maps subject-relation pairs to answers via high-degree Hermite polynomial features
- Core assumption: σ is a polynomial activation of sufficiently high degree
- Evidence anchors: [section 3] "MLP associative memory scheme has storage capacity which is (nearly) linear in the parameter count"; [section 4] "the MLP layer acts as an MLP associative memory, mapping φ(s) + φ(r) to φ(a∗(s, r))"
- Break condition: If activation is not polynomial or degree is too low, construction fails

### Mechanism 3
- Claim: Transformers undergo sequential learning with intermediate hallucination stage
- Mechanism: During training, model first learns to predict based only on relation token, ignoring subject token, before converging to correct prediction
- Core assumption: Number of subjects S is much greater than number of relations R
- Evidence anchors: [section 5] "the model undergoes a sequential learning dynamics... an intermediate 'hallucination' stage where the model outputs the conditional distribution for the answer based on only the relation"; [section 5] "For example, if S is the set of all countries and r is the relation 'capital,' then on the prompt 'What is the capital of France?' the model will output a random countries' capital"
- Break condition: If S ≈ R or S < R, intermediate stage exhibits different behavior

## Foundational Learning

- Concept: Associative memories and outer product storage
  - Why needed here: Core mechanism by which transformers store factual associations
  - Quick check question: What is the maximum number of associations a linear associative memory can store with d² parameters?

- Concept: Hermite polynomials and random feature expansion
  - Why needed here: Enables MLP to store more associations than linear case
  - Quick check question: Why do we need high-degree Hermite polynomials for the MLP construction?

- Concept: Gradient flow dynamics and sequential learning
  - Why needed here: Explains training behavior and intermediate hallucination stage
  - Quick check question: What causes the model to initially ignore the subject token during training?

## Architecture Onboarding

- Component map:
  Input sequence embedding -> Self-attention denoising and association mapping -> MLP transformation (if present) -> Output prediction via argmax

- Critical path:
  1. Input sequence embedding
  2. Self-attention denoising and association mapping
  3. MLP transformation (if present)
  4. Output prediction via argmax

- Design tradeoffs:
  - Attention vs MLP: Can trade off parameters between mechanisms
  - Embedding dimension d: Must scale with max(R,D) for attention-only, can be smaller for MLP
  - Head dimension dh: Must partition S ∪ R for attention-only

- Failure signatures:
  - Low accuracy despite sufficient parameters: Check embedding orthogonality
  - Slow convergence: Check learning rate and initialization scale
  - Hallucination persists: Check S ≫ R assumption

- First 3 experiments:
  1. Train attention-only transformer with varying d, H, dh on synthetic task
  2. Train attention+MLP transformer with varying m, d, H, dh on same task
  3. Train linear attention model and observe sequential learning dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do non-random embeddings affect the storage capacity of associative memories in transformers?
- Basis in paper: [inferred] The paper primarily focuses on random embeddings but mentions it would be interesting to understand the role of embeddings and whether there exists an optimal choice of (non-random) embeddings leading to more efficient constructions
- Why unresolved: The current theoretical framework assumes random embeddings, but transformers likely use learned, structured embeddings. Understanding how different embedding strategies affect capacity could reveal new optimization techniques
- What evidence would resolve it: Empirical studies comparing storage capacity using random vs. learned embeddings on factual recall tasks, or theoretical bounds for specific embedding distributions

### Open Question 2
- Question: Is there a fundamental scaling law lower bound for factual recall tasks in transformers?
- Basis in paper: [inferred] The paper presents lower bounds for associative memories and factual recall, but explicitly states it would be very interesting to understand the implication of results towards empirical LLM scaling laws and whether there exists a scaling law lower bound for the factual recall task
- Why unresolved: While the paper provides information-theoretic lower bounds, it does not establish a direct connection to empirical scaling laws observed in large language models
- What evidence would resolve it: Proofs showing that no transformer architecture can achieve better than a certain scaling relationship between parameters and factual recall performance, validated against real LLM training data

### Open Question 3
- Question: How do transformers distribute factual information between self-attention and MLP layers in practice?
- Basis in paper: [explicit] The paper concludes by stating it would be interesting to understand if larger models utilize similar associative memory constructions and if one can probe whether specific "facts" are stored in either the self-attention matrices or the MLP
- Why unresolved: The theoretical analysis shows transformers can trade off between attention and MLP parameters, but does not reveal how actual models distribute this information during training
- What evidence would resolve it: Mechanistic interpretability studies analyzing trained transformers to identify which components store specific facts, or experiments ablating attention vs. MLP parameters to measure impact on factual recall accuracy

## Limitations

- The theoretical analysis assumes random, non-orthogonal embeddings, but real-world transformers use learned embeddings with unknown orthogonality properties
- The sequential learning dynamics analysis relies on specific assumptions about the relative sizes of subject and relation sets (S ≫ R) that may not hold in practical scenarios
- The synthetic factual recall task may not fully capture the complexity of real factual recall tasks where subjects and relations have rich semantic structures

## Confidence

**High Confidence**: The storage capacity bounds for both linear and MLP associative memories (claims about O(d²) and O(d³) storage capacity respectively). These follow from well-established results in associative memory theory and the mathematical constructions are rigorous.

**Medium Confidence**: The main result showing single-layer transformers can achieve 100% accuracy on the synthetic task when parameters scale linearly with dataset size. While the theoretical construction is sound, the practical relevance depends on whether real transformers operate in this regime and whether the synthetic task captures real-world complexities.

**Medium Confidence**: The gradient flow dynamics analysis showing sequential learning with hallucination stage. The mathematical analysis is rigorous for the simplified linear attention model, but extending these conclusions to full softmax attention models involves additional assumptions.

## Next Checks

1. **Embedding Distribution Analysis**: Empirically measure the orthogonality properties of learned embeddings in trained transformers on real datasets to assess how far they deviate from the random, non-orthogonal assumption.

2. **S vs R Ratio Impact**: Systematically vary the ratio of subjects to relations in the synthetic task to determine how robust the sequential learning dynamics are to violations of the S ≫ R assumption.

3. **Real Task Transfer**: Test whether the parameter scaling laws derived from the synthetic task predict performance on real factual recall benchmarks like LAMA or entity linking tasks.