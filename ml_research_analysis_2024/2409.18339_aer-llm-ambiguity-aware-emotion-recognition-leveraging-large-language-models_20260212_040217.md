---
ver: rpa2
title: 'AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language Models'
arxiv_id: '2409.18339'
source_url: https://arxiv.org/abs/2409.18339
tags:
- emotion
- llms
- emotions
- recognition
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of recognizing ambiguous emotions
  in text using large language models (LLMs). It proposes zero-shot and few-shot prompting
  approaches, incorporating in-context learning by including past dialogue and speech
  features in textual format.
---

# AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language Models

## Quick Facts
- arXiv ID: 2409.18339
- Source URL: https://arxiv.org/abs/2409.18339
- Reference count: 27
- Key outcome: Few-shot prompting with context improves ambiguous emotion recognition by up to 49% in Bhattacharyya coefficient compared to zero-shot

## Executive Summary
This study addresses the challenge of recognizing ambiguous emotions in text using large language models (LLMs). The authors propose zero-shot and few-shot prompting approaches, incorporating in-context learning by including past dialogue and speech features in textual format. The method significantly improves emotion recognition performance, with few-shot prompting showing up to 49% improvement in Bhattacharyya coefficient compared to zero-shot prompting. Including context information proves highly beneficial, with optimal performance achieved using 10-20 previous utterances. The LLMs demonstrate greater effectiveness in recognizing less ambiguous emotions compared to highly ambiguous ones, paralleling human perceptual capabilities.

## Method Summary
The method uses zero-shot and few-shot prompting with Gemini-1.5-Flash to recognize ambiguous emotions by generating probability distributions over emotion labels. The approach incorporates past dialogue context (0-30 utterances) and speech features converted to textual format (88-dimensional eGeMAPS). Few-shot examples are included within prompts to guide the LLM's understanding. Performance is evaluated using uncertainty-centric metrics (JS divergence, Bhattacharyya coefficient, RÂ², ECE) and accuracy-centric metrics (accuracy, F1, UAR) across three datasets: MSP-Podcast, IEMOCAP, and GoEmotions.

## Key Results
- Few-shot prompting with context information achieves up to 49% improvement in Bhattacharyya coefficient compared to zero-shot prompting
- Optimal context window size is 10-20 utterances, with performance degrading for larger windows
- LLMs perform better on less ambiguous emotions, with effectiveness decreasing as emotional entropy increases
- Combining text and speech features improves performance over text-only input

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can recognize ambiguous emotions by leveraging in-context learning with few-shot examples.
- Mechanism: Few-shot prompting provides demonstration examples within the prompt, allowing LLMs to adapt their pre-trained knowledge to the specific task of ambiguous emotion recognition. This enables them to learn the mapping between utterances and emotion distributions rather than single labels.
- Core assumption: LLMs can generalize from few examples to recognize complex, multi-label emotional distributions.
- Evidence anchors:
  - [abstract] "We design zero-shot and few-shot prompting and incorporate past dialogue as context information for ambiguous emotion recognition."
  - [section] "With few-shot prompting, it demonstrates significant improvement, with approximately 25% reduction in JS, 49% increase in BC, and 31% increase in R2 for MSP-Podcast."
  - [corpus] Weak evidence - corpus neighbors focus on emotion recognition but not specifically on few-shot ambiguous emotion recognition with LLMs.

### Mechanism 2
- Claim: Including context information (past dialogue) significantly improves ambiguous emotion recognition.
- Mechanism: LLMs can leverage their long-range contextual understanding to decode emotions by analyzing conversational history. Emotions evolve smoothly within dynamic conversations, so considering past conversations provides a more comprehensive understanding of the emotional state over time.
- Core assumption: Emotional states are contextually dependent and can be better understood by examining preceding dialogue.
- Evidence anchors:
  - [abstract] "incorporating past dialogue and speech features in textual format" and "Including context information proves highly beneficial"
  - [section] "Fig. 2 shows the few-shot performance using text and speech with the increasing context windows from 0 to 30 in MSP-Podcast. Including context information proves significantly beneficial compared to that without contextual information."
  - [corpus] Weak evidence - corpus neighbors discuss emotion recognition but not specifically the impact of conversational context on ambiguous emotion recognition.

### Mechanism 3
- Claim: LLMs demonstrate greater effectiveness in recognizing less ambiguous emotions compared to highly ambiguous ones.
- Mechanism: LLMs can recognize less ambiguous emotions more effectively because they correspond to more concentrated probability distributions, which are easier to predict than highly ambiguous emotions with more dispersed distributions.
- Core assumption: LLMs can better handle simpler, less ambiguous emotional distributions than complex, highly ambiguous ones.
- Evidence anchors:
  - [abstract] "LLMs demonstrate a high degree of effectiveness in recognizing less ambiguous emotions and exhibit potential for identifying more ambiguous emotions, paralleling human perceptual capabilities."
  - [section] "As entropy increases, the medians (black lines) of JS rise, while BC and R2 decrease in general... The trend indicates that LLMs are more effective at recognizing less ambiguous emotions."
  - [corpus] Weak evidence - corpus neighbors discuss emotion ambiguity but not specifically the differential performance of LLMs on varying ambiguity levels.

## Foundational Learning

- Concept: Ambiguity-aware emotion representation
  - Why needed here: Traditional emotion recognition focuses on single labels, but human emotions are inherently ambiguous. Representing emotions as probability distributions over multiple labels is crucial for capturing this ambiguity.
  - Quick check question: Can you explain the difference between a single emotion label and an emotion distribution, and why distributions are more appropriate for ambiguous emotions?

- Concept: In-context learning
  - Why needed here: LLMs can adapt to new tasks by learning from examples provided within the prompt, without requiring fine-tuning. This is essential for leveraging LLMs for emotion recognition without extensive retraining.
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are the advantages and limitations of using in-context learning for emotion recognition?

- Concept: Speech feature integration
  - Why needed here: Humans express emotions through multiple cues, including both text and speech. Integrating speech features in textual format can enhance the LLM's understanding of emotions.
  - Quick check question: How can speech features be transformed into textual format for LLM processing, and what types of speech features are most relevant for emotion recognition?

## Architecture Onboarding

- Component map: Input (target utterance, past dialogue context, speech features) -> LLM (Gemini-1.5-Flash) -> Output (emotion probability distribution) -> Evaluation (uncertainty-centric and accuracy-centric metrics)

- Critical path: 1. Construct prompt with target utterance, context, and speech features 2. Send prompt to LLM (Gemini-1.5-Flash) 3. Receive emotion probability distribution 4. Evaluate performance using uncertainty-centric and accuracy-centric metrics

- Design tradeoffs:
  - Context window size: Larger windows provide more context but may introduce irrelevant information; optimal window size is 10-20 utterances
  - Few-shot examples: More examples improve performance but increase prompt length and cost; optimal number is 5-10 examples
  - Speech feature integration: Enhances performance but requires feature extraction and textual transformation; eGeMAPS features are used

- Failure signatures:
  - Poor performance on highly ambiguous emotions (high entropy)
  - Degradation in performance with very large context windows (>20 utterances)
  - Inconsistent emotion distributions across similar utterances

- First 3 experiments:
  1. Compare zero-shot vs. few-shot prompting performance on a small dataset
  2. Evaluate the impact of context window size on performance (test window sizes 0, 5, 10, 20, 30)
  3. Assess the contribution of speech features by comparing text-only vs. text+speech performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' emotion recognition capabilities compare when analyzing multi-party conversations versus dyadic interactions?
- Basis in paper: [inferred] The paper focuses on dyadic conversations and suggests LLMs can leverage context information, but doesn't explore multi-party scenarios where emotional ambiguity may be more complex.
- Why unresolved: The current study only uses datasets with two speakers (MSP-Podcast, IEMOCAP) and doesn't test performance in group conversations where multiple emotional threads and speaker dynamics could affect ambiguity.
- What evidence would resolve it: Direct comparison of LLM emotion recognition performance across dyadic and multi-party datasets, measuring how ambiguity levels and context window effectiveness change with more speakers.

### Open Question 2
- Question: What is the optimal balance between textual context and speech features for ambiguous emotion recognition across different emotional categories?
- Basis in paper: [explicit] The paper shows that combining text and speech features improves performance, but doesn't analyze which emotional categories benefit most from acoustic information versus contextual dialogue.
- Why unresolved: While the study demonstrates benefits of multimodal input, it treats all emotions uniformly and doesn't explore whether certain emotions (like anger vs. sadness) are better recognized through speech features versus textual context.
- What evidence would resolve it: Detailed ablation studies showing emotion-specific performance differences when using text-only, speech-only, and combined modalities, stratified by emotional category.

### Open Question 3
- Question: How does the length and structure of conversational context affect LLM performance on highly ambiguous emotions versus moderately ambiguous ones?
- Basis in paper: [inferred] The paper finds optimal context windows of 10-20 utterances and notes LLMs struggle more with highly ambiguous emotions, but doesn't examine whether different ambiguity levels require different context strategies.
- Why unresolved: The study uses fixed context windows across all ambiguity levels, without investigating whether highly ambiguous emotions might benefit from longer or differently structured context information compared to less ambiguous ones.
- What evidence would resolve it: Experiments varying context window lengths specifically for high-entropy (highly ambiguous) versus low-entropy utterances, measuring whether different strategies optimize performance for each ambiguity level.

## Limitations

- The study relies on few-shot examples that are not fully specified, requiring manual recreation and introducing variability in reproduction
- The integration of speech features into textual format lacks precise formatting details for the LLM prompt
- Performance on highly ambiguous emotions remains limited, with accuracy declining as emotional entropy increases

## Confidence

- **High Confidence:** The improvement in performance with few-shot prompting compared to zero-shot (up to 49% increase in BC) is well-supported by results across multiple datasets and evaluation metrics.
- **Medium Confidence:** The benefit of context windows (10-20 utterances optimal) is demonstrated, but the exact mechanism of why this range is optimal versus larger windows is not fully explained.
- **Medium Confidence:** The parallel between LLM performance on ambiguous emotions and human perceptual capabilities is suggested but not empirically validated through direct comparison with human judgment.

## Next Checks

1. **Reproduce few-shot performance variability:** Systematically vary the few-shot examples across multiple runs to quantify the impact of example selection on performance consistency.
2. **Test context window saturation point:** Evaluate performance with context windows beyond 30 utterances to determine if the observed benefits plateau or decline, and identify optimal window sizes for different datasets.
3. **Validate speech feature integration:** Compare performance using different methods of representing speech features in textual format (e.g., raw values vs. normalized or binned representations) to assess sensitivity to feature formatting.