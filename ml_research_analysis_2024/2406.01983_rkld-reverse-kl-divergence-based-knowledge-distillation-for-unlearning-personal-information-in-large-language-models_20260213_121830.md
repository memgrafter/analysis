---
ver: rpa2
title: 'RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal
  Information in Large Language Models'
arxiv_id: '2406.01983'
source_url: https://arxiv.org/abs/2406.01983
tags:
- unlearning
- forget
- rkld
- arxiv
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of unlearning personal information
  from large language models (LLMs) to comply with regulations like GDPR. The core
  method, RKLD, introduces a Reverse KL-Divergence-based Knowledge Distillation algorithm.
---

# RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models

## Quick Facts
- arXiv ID: 2406.01983
- Source URL: https://arxiv.org/abs/2406.01983
- Reference count: 14
- This paper introduces RKLD, a novel approach for unlearning personal information from LLMs using reverse KL-divergence-based knowledge distillation.

## Executive Summary
This paper addresses the challenge of unlearning personal information from large language models to comply with regulations like GDPR. The proposed method, RKLD, uses reverse KL-divergence-based knowledge distillation to effectively remove specific information while maintaining model utility. Experiments on the TOFU benchmark demonstrate that RKLD achieves significant forget quality while preserving the model's general capabilities, outperforming existing methods especially with larger forget sets.

## Method Summary
RKLD constructs an unlearning teacher model through continued training on a forget set, then distills knowledge to a student model using reverse KL divergence. The teacher model is created by training on the forget set to strengthen the influence of target tokens, then subtracting logits to identify which tokens need to be forgotten. The student model is then trained using reverse KL divergence to align with the teacher while effectively forgetting the specified personal information.

## Key Results
- RKLD achieves significant forget quality with KS-test p-values > 0.05 on TOFU benchmark
- Maintains high model utility across multiple datasets with ROUGE-L scores consistently above 0.3
- Outperforms existing unlearning methods, particularly with larger forget sets (5% and 10%)
- Ablation studies confirm reverse KL divergence is superior to forward KL for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reverse KL divergence is superior to forward KL divergence for distillation-based unlearning because it focuses on preventing the model from assigning high probabilities to tokens that should be forgotten.
- Mechanism: RKL penalizes the model more heavily when the student's probability is higher than the teacher's for tokens to be forgotten, encouraging the model to lower those probabilities while preserving the rest of the distribution.
- Core assumption: The unlearning teacher model correctly identifies which tokens need to be forgotten through the continued training and logits subtraction approach.
- Evidence anchors:
  - [abstract] "we find that the mathematical properties of reverse KL divergence are particularly effective for our unlearning objectives"
  - [section] "RKL emphasizes avoiding high probabilities in the original model that are absent in the teacher's distribution, aligning more closely with the goal of forgetting specific tokens while preserving the remaining distribution"
  - [corpus] Weak - no direct evidence about RKL vs FKL in unlearning context found in neighbors
- Break condition: If the unlearning teacher model incorrectly identifies tokens to forget, RKL would amplify this error by aggressively reducing probabilities for the wrong tokens.

### Mechanism 2
- Claim: The unlearning teacher model constructed through continued training on the forget set effectively guides the student model on which tokens to forget while preserving model utility.
- Mechanism: By subtracting the logits of the strengthened model (trained on forget set) from the original model, the unlearning teacher highlights tokens that have increased in probability due to the forget set, marking them for reduction.
- Core assumption: Continued training on the forget set causes relevant token logits to increase consistently, allowing their identification.
- Evidence anchors:
  - [section] "We identify tokens with consistently increased logits values, marking them as potentially influenced by the given forget target"
  - [section] "The unlearning teacher model reduces the token distribution that needs to be forgotten while also preserving the irrelevant token distributions"
  - [corpus] Weak - no direct evidence about this specific teacher construction method found in neighbors
- Break condition: If continued training causes unpredictable logit changes or the forget set contains tokens similar to retained data, the teacher model may incorrectly identify tokens to forget.

### Mechanism 3
- Claim: RKLD achieves significant forget quality while maintaining model utility by balancing forgetting objectives with preservation of general capabilities.
- Mechanism: The distillation process guided by the unlearning teacher ensures selective forgetting of personal information while the RKL loss prevents over-correction that would harm general model performance.
- Core assumption: The balance between forget quality and model utility can be achieved through this selective distillation approach without requiring the retain set.
- Evidence anchors:
  - [abstract] "Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments"
  - [section] "RKLD consistently maintains a relatively small performance decline" (from ablation study)
  - [corpus] Weak - no direct evidence about this specific balance mechanism found in neighbors
- Break condition: If the forget set is too large or contains information overlapping with general knowledge, maintaining both forget quality and model utility becomes impossible.

## Foundational Learning

- Concept: Kullback-Leibler (KL) Divergence
  - Why needed here: KL divergence measures the difference between two probability distributions, which is fundamental to understanding why reverse KL is used instead of forward KL in this distillation approach.
  - Quick check question: What is the mathematical difference between forward KL and reverse KL divergence?

- Concept: Knowledge Distillation
  - Why needed here: RKLD uses knowledge distillation as its core mechanism, where the unlearning teacher model guides the student model through probability distribution alignment.
  - Quick check question: In standard knowledge distillation, what is typically being transferred from teacher to student?

- Concept: Continued Training
  - Why needed here: The unlearning teacher is constructed through continued training on the forget set, which is essential for understanding how the teacher identifies tokens to forget.
  - Quick check question: What happens to token logits when a model is continued trained on data it has already seen?

## Architecture Onboarding

- Component map: Original model → Forget set continued training → Strengthened model → Logits subtraction → Unlearning teacher → Reverse KL distillation → Unlearned student model
- Critical path: The construction of the unlearning teacher (continued training + logits subtraction) is the most critical step, as it determines which tokens will be forgotten
- Design tradeoffs: Using RKL instead of FKL trades off some precision in probability alignment for better selective forgetting; not using the retain set simplifies the approach but may limit forget quality on very large forget sets
- Failure signatures: Poor forget quality (KS-test p-values ≤ 0.05), significant model utility degradation (ROUGE-L scores dropping below 0.3), or inconsistent results across seeds
- First 3 experiments:
  1. Verify the unlearning teacher construction by checking if logits of tokens in forget set increase after continued training
  2. Test RKL vs FKL distillation on a small forget set to confirm RKL's superiority for forgetting
  3. Measure forget quality and model utility on the TOFU benchmark with 1% forget set to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RKLD perform on real-world datasets with noisy and diverse data compared to controlled synthetic datasets?
- Basis in paper: [explicit] The paper acknowledges that TOFU benchmark datasets are controlled and synthetic, and states that the performance and robustness of RKLD in real-world scenarios remain to be fully tested and validated.
- Why unresolved: The paper only demonstrates RKLD's effectiveness on the TOFU benchmark, which uses synthetic data. Real-world datasets often contain noise, diverse patterns, and edge cases that may challenge the algorithm's assumptions.
- What evidence would resolve it: Experiments applying RKLD to real-world unlearning tasks with actual personal data, measuring both forget quality and model utility across diverse, noisy datasets.

### Open Question 2
- Question: What are the long-term effects of RKLD on model behavior, and do unlearned models experience concept drift or degradation over time?
- Basis in paper: [inferred] The paper mentions that despite avoiding certain metrics, current evaluation metrics may still fail to capture the full range of model behaviors post-unlearning, suggesting potential unknown long-term effects.
- Why unresolved: The experiments conducted are likely short-term, focusing on immediate forget quality and model utility. Long-term effects, such as gradual degradation of capabilities or unexpected behavior changes, are not addressed.
- What evidence would resolve it: Longitudinal studies tracking model performance, behavior, and potential concept drift over extended periods post-unlearning.

### Open Question 3
- Question: How does the choice of hyperparameter α (forgetting strength) affect the trade-off between forget quality and model utility in different scenarios?
- Basis in paper: [explicit] The paper mentions that α is a hyperparameter to control the forgetting strength in the unlearning teacher construction, but does not provide a sensitivity analysis of its impact.
- Why unresolved: The paper sets α = 8 but does not explore how different values of α affect the balance between forget quality and model utility, nor does it discuss optimal selection strategies for different use cases.
- What evidence would resolve it: Systematic experiments varying α across different settings and forget set sizes, analyzing the resulting trade-offs and identifying optimal ranges for different scenarios.

### Open Question 4
- Question: How does RKLD handle unlearning tasks involving multiple, overlapping forget sets where some personal information may be interconnected?
- Basis in paper: [inferred] The paper focuses on unlearning tasks with a single forget set, but does not address scenarios where multiple forget sets may contain overlapping or related personal information.
- Why unresolved: Real-world unlearning scenarios often involve complex relationships between data points, and the paper does not explore how RKLD handles such cases or whether it might inadvertently affect related information.
- What evidence would resolve it: Experiments testing RKLD on datasets with multiple, overlapping forget sets, measuring both targeted forget quality and unintended effects on related information.

## Limitations
- The effectiveness of RKLD heavily depends on the proper construction of the unlearning teacher model through continued training and logit subtraction.
- The method's generalization to other unlearning scenarios beyond TOFU benchmark remains unclear.
- The paper doesn't address potential biases introduced during the unlearning process or how the method scales to larger models and datasets.

## Confidence
- High confidence: RKLD achieves significant forget quality (KS-test p-values > 0.05) and maintains model utility on the TOFU benchmark
- Medium confidence: Reverse KL divergence is superior to forward KL for distillation-based unlearning
- Medium confidence: The unlearning teacher model construction through continued training effectively identifies tokens to forget

## Next Checks
1. Test RKLD on additional unlearning benchmarks beyond TOFU to verify generalization across different types of personal information and dataset sizes
2. Conduct ablation studies on the impact of different forget set sizes (1%, 5%, 10%) on both forget quality and model utility to identify optimal trade-offs
3. Evaluate RKLD's performance on larger language models (13B, 70B parameters) to assess scalability and whether the reverse KL advantage persists with model size