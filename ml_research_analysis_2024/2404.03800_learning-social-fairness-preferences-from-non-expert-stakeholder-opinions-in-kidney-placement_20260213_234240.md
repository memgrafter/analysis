---
ver: rpa2
title: Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in
  Kidney Placement
arxiv_id: '2404.03800'
source_url: https://arxiv.org/abs/2404.03800
tags:
- fairness
- kidney
- social
- participants
- survey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fairness of kidney placement algorithms
  by collecting feedback from non-expert participants on a crowdsourcing platform.
  A novel logit-based feedback model is proposed to capture ambiguous fairness preferences
  across group fairness notions.
---

# Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in Kidney Placement

## Quick Facts
- arXiv ID: 2404.03800
- Source URL: https://arxiv.org/abs/2404.03800
- Reference count: 21
- Primary result: Non-expert participants most prefer accuracy equality and predictive equality for kidney placement algorithms

## Executive Summary
This paper develops a method to learn social fairness preferences from non-expert stakeholders for kidney placement algorithms. The authors collect survey data from 75 participants rating fairness across six group fairness notions using a logit-based feedback model that captures ambiguous preferences. They then apply a projected gradient descent algorithm to learn social preference weights that minimize feedback regret. The results show that accuracy equality and predictive equality emerge as the most preferred fairness metrics, and the tested acceptance rate predictor is perceived as fair by participants.

## Method Summary
The study collects survey responses from non-expert participants on a crowdsourcing platform, where each participant rates the fairness of a kidney acceptance rate predictor (ARP) across six group fairness notions for three sensitive attributes. The authors propose a novel logit-based fairness feedback model that encodes Likert-scale responses and models ambiguous fairness preferences using a logit-normal distribution. They then develop a social fairness preference learning algorithm (SAFF) based on projected gradient descent to minimize social feedback regret and learn the collective preference weights over fairness notions. The method is validated through simulation experiments and analysis of the collected survey data.

## Key Results
- Accuracy equality (AE) and predictive equality (PE) are the most preferred group fairness notions across all three sensitive attributes (age, gender, race)
- The logit-based feedback model successfully captures non-expert participants' ambiguous fairness preferences
- The projected gradient descent algorithm effectively learns social fairness preferences from noisy survey feedback
- The tested acceptance rate predictor exhibits low bias according to the learned fairness preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The logit-based feedback model successfully captures non-expert participants' ambiguous fairness preferences across multiple group fairness notions.
- Mechanism: The model encodes participants' fairness feedback scores (1-7 Likert scale) by mapping them to intervals on the aggregated fairness evaluation space [-1,1], then applying a logit-normal distribution to model ambiguity in preference weighting. The temperature parameter λ controls sensitivity to utility differences.
- Core assumption: Participants' true intrinsic fairness evaluations follow a logit-normal distribution with mean equal to their weighted aggregation of fairness notions and known variance σ².
- Evidence anchors:
  - [abstract] "A novel logit-based fairness feedback model is proposed based on encoded Likert choices and ambiguous fairness preferences across group fairness notions."
  - [section] "the nth participant experiences a utility un,i as the probability of the true intrinsic fairness evaluation ψ to lie in a specific region Ri... Then, the fairness feedback score sn,m is modeled as the logit probability"
- Break condition: If the true preference distribution is not logit-normal or if participants' responses are not influenced by utility calculations across partitions, the model's accuracy would degrade significantly.

### Mechanism 2
- Claim: The projected gradient descent algorithm with efficient gradient computation can learn social fairness preferences from noisy feedback.
- Mechanism: The algorithm iteratively updates the social preference weight vector β using gradients computed via backpropagation through the feedback model, projecting onto the simplex to maintain valid probability distributions. The gradient combines derivatives of the social regret with respect to the feedback model and the fairness evaluations.
- Core assumption: The social feedback regret surface is smooth enough for gradient descent to converge to a meaningful local minimum.
- Evidence anchors:
  - [abstract] "A novel social fairness preference learning algorithm is proposed based on minimizing social feedback regret computed using a novel logit-based fairness feedback model."
  - [section] "The proposed algorithm SAFF is employed on both simulated data as well as survey responses... The social preference weight β∗ can be learned using Social Aggregation of Fairness Feedback (SAFF) Algorithm 1, which is developed using projected gradient descent."
- Break condition: If the feedback regret surface is highly non-convex or contains many local minima, the algorithm may converge to suboptimal preference weights that don't represent true social preferences.

### Mechanism 3
- Claim: Accuracy equality and predictive equality are the most preferred group fairness notions from public stakeholders' perspective in kidney placement.
- Mechanism: The survey results show that when learning social preferences, the weights assigned to accuracy equality (AE) and predictive equality (PE) are significantly higher than other fairness notions across all sensitive attributes, and the ARP exhibits low bias according to these metrics.
- Core assumption: The learned social preference weights accurately reflect the collective preferences of the non-expert participants.
- Evidence anchors:
  - [abstract] "The results show that accuracy equality and predictive equality are the most preferred group fairness notions"
  - [section] "Table 3 shows the estimated social preferences... Note that accuracy equality (AE) is the preferred group fairness notion across all three sensitive attributes... In the case of age and gender, predictive equality (PE) has the second highest preference"
- Break condition: If the learning algorithm fails to converge properly or if participant responses are systematically biased, the estimated preferences may not reflect true social preferences.

## Foundational Learning

- Concept: Group fairness notions in machine learning
  - Why needed here: The paper evaluates an acceptance rate predictor across multiple fairness metrics to understand which ones matter most to non-experts
  - Quick check question: What's the difference between equal opportunity and predictive equality, and why might one be preferred over the other in healthcare settings?

- Concept: Human feedback modeling and preference aggregation
  - Why needed here: The core innovation is translating Likert-scale feedback into utility-based preferences and aggregating them across participants
  - Quick check question: How does a logit-normal distribution differ from a standard normal distribution, and why is it appropriate for modeling preferences on a bounded scale?

- Concept: Projected gradient descent on probability simplices
  - Why needed here: The algorithm must learn preference weights that are valid probability distributions (non-negative, sum to 1)
  - Quick check question: What's the projection operator P doing in the SAFF algorithm, and why can't we just use unconstrained gradient descent?

## Architecture Onboarding

- Component map: STAR files → donor-recipient tuples → ARP predictions → survey interface → Likert questions → response collection → logit-based feedback model → social preference learning → preference analysis

- Critical path: Data preparation → ARP prediction generation → survey deployment → response collection → social preference learning → analysis of preferred fairness notions

- Design tradeoffs:
  - Using non-experts vs. medical experts: Broader representation vs. potentially less informed opinions
  - Likert scale discretization: Captures nuance vs. loses granularity in true preferences
  - Limited data tuples per participant: Reduces fatigue vs. may not capture full preference complexity

- Failure signatures:
  - Algorithm not converging: Poor initialization, learning rate issues, or highly noisy feedback
  - Unexpected preference weights: Survey design problems, demographic imbalances, or model specification errors
  - Low participant engagement: Survey length, complexity, or compensation issues

- First 3 experiments:
  1. Test logit-based feedback model on synthetic data with known preference distributions to verify learning accuracy
  2. Validate gradient computation by comparing analytical gradients to numerical approximations
  3. Run ablation study on different initialization strategies to identify robust approaches for real survey data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do non-expert stakeholders' fairness preferences change over time with repeated exposure to the ARP's predictions?
- Basis in paper: [explicit] The paper collects a single snapshot of fairness feedback from 75 participants on Prolific.
- Why unresolved: The study design only captures a one-time assessment, not tracking changes in perceptions.
- What evidence would resolve it: Longitudinal studies with repeated surveys over multiple time periods to measure shifts in fairness preferences.

### Open Question 2
- Question: How do the fairness perceptions of non-expert stakeholders compare to those of clinical experts when evaluating the same ARP?
- Basis in paper: [explicit] The paper explicitly contrasts non-expert and expert stakeholders, but does not collect data from clinical experts.
- Why unresolved: The study only surveys non-experts, leaving a gap in understanding how expert opinions differ.
- What evidence would resolve it: Conducting parallel surveys with clinical experts and comparing their fairness ratings to those of non-experts.

### Open Question 3
- Question: How sensitive are non-expert stakeholders' fairness preferences to the specific implementation details of the ARP, such as the choice of features or model architecture?
- Basis in paper: [inferred] The paper evaluates a single ARP implementation, but does not explore how changes to the model would affect perceptions.
- Why unresolved: The study does not test multiple variants of the ARP or systematically vary its components.
- What evidence would resolve it: Experimenting with different ARP configurations and measuring how non-expert feedback changes across these variants.

## Limitations
- Small sample size (75 participants) may not capture the full complexity of social fairness preferences
- Reliance on non-expert participants limits generalizability to real-world kidney allocation stakeholders
- Single dataset and ARP implementation constrain external validity of findings

## Confidence
**High Confidence:** The technical methodology for learning social preferences from feedback (gradient descent on logit-based models) is well-established and the implementation appears sound based on the described algorithm.

**Medium Confidence:** The survey design and data collection process appear methodologically sound, but the relatively small sample size and use of non-expert participants introduces uncertainty about the representativeness of the learned preferences.

**Low Confidence:** The generalizability of the specific finding that accuracy equality and predictive equality are most preferred group fairness notions beyond the specific context of kidney allocation and the particular ARP model used.

## Next Checks
1. **Sample Size Validation:** Conduct power analysis to determine the minimum number of participants needed to achieve stable estimates of social fairness preferences across different group fairness notions.

2. **Expert Comparison Study:** Run parallel surveys with medical experts and non-experts on the same kidney allocation scenarios to quantify the differences in fairness preferences and assess the validity of using non-expert feedback.

3. **Cross-Dataset Generalization:** Test the social preference learning framework on multiple kidney matching datasets and different prediction models to evaluate whether the same group fairness notions consistently emerge as most preferred across contexts.