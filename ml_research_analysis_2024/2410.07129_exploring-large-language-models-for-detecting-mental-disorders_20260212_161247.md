---
ver: rpa2
title: Exploring Large Language Models for Detecting Mental Disorders
arxiv_id: '2410.07129'
source_url: https://arxiv.org/abs/2410.07129
tags:
- shot
- mmlu
- depression
- vikhr
- gemma2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares traditional machine learning methods, encoder-based
  models, and large language models (LLMs) on the task of detecting depression and
  anxiety from Russian-language texts. Five datasets were used, including essays from
  clinically diagnosed patients and social media data labeled via questionnaires.
---

# Exploring Large Language Models for Detecting Mental Disorders

## Quick Facts
- arXiv ID: 2410.07129
- Source URL: https://arxiv.org/abs/2410.07129
- Reference count: 40
- Primary result: LLMs achieve highest F1 scores for detecting depression/anxiety in Russian texts, especially on small/noisy datasets

## Executive Summary
This study evaluates the performance of traditional machine learning, encoder-based models, and large language models (LLMs) for detecting depression and anxiety from Russian-language texts. The research compares five datasets including clinically diagnosed essays and questionnaire-labeled social media posts, finding that LLMs consistently outperform other approaches, particularly on small or noisy datasets. The study also examines model transfer between datasets and evaluates the quality of LLM-generated explanations by clinicians.

## Method Summary
The researchers trained traditional ML models using linguistic features and n-grams, encoder-based models (BERT/RoBERTa variants) through fine-tuning, and LLMs through zero-shot, few-shot prompting, and fine-tuning with LoRA. Five Russian-language datasets were used: Depression-Essays (clinically diagnosed), Depression-Social Media (questionnaire-labeled), and three anxiety datasets with varying sources. Models were evaluated using F1-macro and F1-pathology scores, with additional clinician assessment of LLM-generated explanations.

## Key Results
- LLMs achieved highest F1 scores, reaching 88.4% F1-macro on depression essays
- Linguistic features + AutoML models matched LLM performance on clinically confirmed data but lagged on noisy social media
- Zero-/few-shot prompting outperformed fine-tuning on very small or heterogeneous datasets
- Models trained on clinical essays failed to generalize to questionnaire-labeled social media texts
- LLM-generated explanations received poor clinician ratings (2.84/5 average) with common errors including confabulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs outperform traditional methods on small, noisy datasets because they generalize better from few examples and tolerate inconsistent formatting
- Mechanism: LLMs have been pretrained on diverse text genres, enabling them to infer task patterns without needing strict feature engineering or large labeled corpora
- Core assumption: The generalization ability from pretraining is retained when fine-tuning or using zero-/few-shot prompting on mental health text classification
- Evidence anchors:
  - "LLMs achieve the highest F1 scores, especially on small or noisy datasets."
  - "LLMs performed best on all five different mental health datasets... Even without fine-tuning, LLMs usually demonstrate relatively high performance."
- Break condition: If the dataset is too small for any meaningful pretraining transfer (e.g., fewer than ~50 examples), LLM performance may drop to random chance

### Mechanism 2
- Claim: Psycholinguistic feature + AutoML models can match LLM performance when trained on clinically validated depression essays
- Mechanism: Features extracted by the Smirnov et al. (2021) tool capture domain-specific linguistic markers (e.g., negative sentiment, low emotional tone) that correlate strongly with clinically diagnosed depression
- Core assumption: The linguistic markers used are robust across different essays from the same clinical population and not overfitted to the training set
- Evidence anchors:
  - "Psycholinguistic features and encoder-based models can achieve performance comparable to language models when trained on texts from individuals with clinically confirmed depression."
  - "The model trained on the linguistic features shows comparable results with 85.8% F1-macro... on the essay dataset (DE)."
- Break condition: If the clinical population changes significantly (e.g., different cultural background or symptom presentation), the same features may lose predictive power

### Mechanism 3
- Claim: Zero-/few-shot prompting outperforms fine-tuning on very small or heterogeneous datasets because the model's prior knowledge is more valuable than task-specific adaptation
- Mechanism: By leveraging prompts that frame the task as a binary classification (depression vs. healthy), the model can map its pretraining knowledge onto the new domain without the noise and overfitting risks of fine-tuning on limited data
- Core assumption: The prompt format and few-shot examples are sufficient to guide the model to the correct decision boundary without additional training
- Evidence anchors:
  - "For all other datasets, various prompting methods without fine-tuning significantly outperform SFT and LoRA."
  - "Models with LoRA, in general, perform better than encoder-based and AutoML-based models, but the best performance is achieved by 0-shot and 5-shot prompting."
- Break condition: If the task requires subtle, context-dependent reasoning not captured by the prompt or examples, prompting may yield unreliable results

## Foundational Learning

- Concept: Understanding the difference between zero-shot, few-shot, and fine-tuning settings
  - Why needed here: The paper evaluates all three, and results vary significantly across datasets; choosing the right setting depends on data size and quality
  - Quick check question: What is the key distinction between zero-shot and few-shot prompting?

- Concept: Role of linguistic features (psycholinguistic, TF-IDF) in text classification
  - Why needed here: The paper compares feature-based AutoML models against deep models; knowing what features represent helps interpret why they work on clinical data
  - Quick check question: Why might TF-IDF perform better on longer, coherent essays than on concatenated social media posts?

- Concept: Dataset labeling methodology and its impact on model generalization
  - Why needed here: Different datasets use clinical diagnosis, BDI scores, or HADS scores; models trained on one may not transfer to another
  - Quick check question: Why did models trained on DE fail to generalize to DSM?

## Architecture Onboarding

- Component map: Data ingestion → preprocessing (length capping, tokenization) → feature extraction (linguistic, encoder embeddings, LLM tokenization) → model training (AutoML, fine-tuning, prompting) → evaluation (F1-macro, F1-pathology) → clinician review (LLM explanation scoring)
- Critical path: Data → preprocessing → model choice (based on dataset size/noise) → evaluation → clinician feedback
- Design tradeoffs:
  - Using encoder-based models (BERT variants) vs. LLMs: trade-off between model size, training cost, and performance on noisy vs. clean data
  - Fine-tuning vs. prompting: fine-tuning requires more data and risks overfitting; prompting is cheaper but may be less precise
- Failure signatures:
  - Low F1-macro but high F1-healthy: model is biased toward majority class
  - High variance in F1 across seeds: dataset too small or model unstable
  - LLM explanations rated poorly by clinicians: explanations may contain confabulations or lack medical grounding
- First 3 experiments:
  1. Train AutoML on linguistic features for DE; evaluate F1-macro
  2. Fine-tune RuBERT on DE; compare to AutoML
  3. Apply best DE model (LLM) to DSM; observe transfer failure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-generated explanations for mental disorder detection be improved to meet clinical standards through additional training or prompting techniques?
- Basis in paper: [explicit] The paper evaluated LLM-generated explanations for depression detection and found them lacking, with an average score of 2.84/5 from clinicians
- Why unresolved: While the paper identified the shortcomings of current LLM explanations and proposed a categorization of errors, it did not explore potential solutions or training methods to address these issues
- What evidence would resolve it: Comparative experiments testing different prompting strategies, fine-tuning approaches with clinician feedback, or specialized training datasets designed to improve medical explanation quality

### Open Question 2
- Question: How does the performance of mental disorder detection models generalize across different cultural contexts and languages beyond Russian?
- Basis in paper: [explicit] The study was conducted exclusively on Russian-language datasets and acknowledged this as a limitation, noting that the methods are "language agnostic" but results need verification in other languages
- Why unresolved: The paper only examined Russian texts and made no comparative analysis with other languages or cultures
- What evidence would resolve it: Systematic testing of the same detection methods across multiple languages and cultural contexts, with careful control for dataset characteristics

### Open Question 3
- Question: What is the optimal balance between model complexity (LLMs vs traditional methods) for different types of mental health datasets and use cases?
- Basis in paper: [inferred] The study showed LLMs outperformed other methods on most datasets, but noted that psycholinguistic features and encoder models performed comparably on clinically confirmed data
- Why unresolved: While the paper compared various model types, it did not systematically explore which specific dataset characteristics should guide the choice between LLMs and simpler methods
- What evidence would resolve it: Detailed ablation studies examining model performance across controlled variations in dataset properties, combined with resource consumption analysis

## Limitations

- Results are limited to Russian-language texts and may not generalize to other languages or cultures
- LLM-generated explanations received poor clinician ratings, indicating reliability issues for clinical applications
- Models trained on clinical essays failed to transfer to questionnaire-labeled social media data, suggesting limited domain robustness

## Confidence

- Medium confidence in the central claim that LLMs outperform traditional and encoder-based models
  - Evidence from multiple datasets shows consistent LLM superiority
  - However, evaluation is limited to five Russian-language datasets with varying labeling methodologies
  - Poor clinician ratings of LLM explanations suggest potential reliability issues
  - Transfer learning results indicate limited domain robustness

## Next Checks

1. **Replication on independent dataset**: Apply the best-performing LLM model from this study to an external Russian-language mental health dataset not used in the original evaluation to assess generalization beyond the tested corpora

2. **Clinician validation study**: Conduct a blind evaluation where clinicians assess LLM-generated explanations against gold-standard clinical reasoning to quantify confabulation rates and identify systematic error patterns

3. **Ablation study on prompt engineering**: Systematically vary the few-shot examples and prompt structure to determine how sensitive LLM performance is to prompt design, particularly for the smallest datasets where zero-shot and few-shot methods showed the largest advantages