---
ver: rpa2
title: Prompt Learning on Temporal Interaction Graphs
arxiv_id: '2402.06326'
source_url: https://arxiv.org/abs/2402.06326
tags:
- prompt
- temporal
- node
- tprog
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TIGPrompt, a novel framework for temporal
  interaction graph (TIG) representation learning that bridges the temporal and semantic
  gaps in existing "pre-train, predict" paradigms. TIGPrompt employs a Temporal Prompt
  Generator (TProG) to produce temporally-aware, task-specific prompts that can be
  integrated with pre-trained TIG models.
---

# Prompt Learning on Temporal Interaction Graphs

## Quick Facts
- arXiv ID: 2402.06326
- Source URL: https://arxiv.org/abs/2402.06326
- Reference count: 40
- Key outcome: TIGPrompt significantly outperforms state-of-the-art baselines in link prediction and node classification tasks while requiring minimal additional computational resources

## Executive Summary
This paper introduces TIGPrompt, a novel framework for temporal interaction graph (TIG) representation learning that addresses temporal and semantic gaps in existing pre-train, predict paradigms. The framework employs a Temporal Prompt Generator (TProG) to produce task-specific, time-aware prompts that can be integrated with pre-trained TIG models. Three variants of TProG are proposed: Vanilla, Transformer, and Projection modes, each offering different levels of temporal expressiveness and personalization. Experiments on four benchmark datasets using five representative TIG models demonstrate significant performance improvements over state-of-the-art baselines.

## Method Summary
TIGPrompt bridges the temporal and semantic gaps in TIG representation learning by generating temporally-aware, task-specific prompts through the TProG component. The framework operates in two paradigms: "pre-train, prompt" where only prompts are tuned while keeping the pre-trained model frozen, and "pre-train, prompt-based fine-tune" where both the model and prompts are jointly optimized. The TProG generates prompts based on recent interaction history and time encoding, which are then fused with pre-trained node embeddings to create final representations for downstream tasks.

## Key Results
- TIGPrompt outperforms state-of-the-art baselines in both link prediction and node classification tasks
- The framework requires minimal additional computational resources compared to traditional fine-tuning
- The "pre-train, prompt-based fine-tune" paradigm offers greater flexibility and improved performance at the cost of additional computational resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TProG effectively bridges the temporal gap by generating time-aware prompts that adapt to evolving TIG data
- Mechanism: The Temporal Prompt Generator encodes recent interaction history and time encoding into prompts, ensuring they reflect the latest temporal dynamics of each node
- Core assumption: Temporal information from recent interactions is sufficient to capture the evolving nature of TIGs and can be encoded effectively into prompts
- Evidence anchors:
  - [abstract]: "These prompts stand out for their minimalistic design, relying solely on the fine-tuning of the prompt generator with very little supervision data."
  - [section]: "To generate temporal-aware prompt, we consider encoding the most relevant temporal information for each node. For a target node ð‘£, its most recent interactions provide valuable insights into its temporal information..."
  - [corpus]: Weak evidence - no direct mention of temporal gap bridging in corpus neighbors
- Break condition: If the recent interaction history does not adequately represent the node's current state or if the time encoding function fails to capture relevant temporal dynamics

### Mechanism 2
- Claim: TProG bridges the semantic gap by incorporating task-specific knowledge into prompts, allowing them to adapt to diverse downstream tasks
- Mechanism: The TProG is tuned with supervision from the specific downstream task, enabling the generated prompts to contain task-specific semantics that align with the learning and prediction capabilities of the pre-trained model
- Core assumption: Task-specific supervision can effectively guide the TProG to generate prompts that bridge the semantic gap between pretext and downstream tasks
- Evidence anchors:
  - [abstract]: "These prompts stand out for their minimalistic design, relying solely on the fine-tuning of the prompt generator with very little supervision data."
  - [section]: "To bridge the semantic gap between pretext and downstream tasks, the TProG component is tuned with a specific downstream task, facilitating adaptability to concrete downstream scenarios."
  - [corpus]: Weak evidence - no direct mention of semantic gap bridging in corpus neighbors
- Break condition: If the task-specific supervision is insufficient or noisy, leading to prompts that do not effectively bridge the semantic gap

### Mechanism 3
- Claim: The "pre-train, prompt-based fine-tune" paradigm enhances model adaptability by jointly optimizing the TIG model and prompts
- Mechanism: In this paradigm, both the TProG and the pre-trained TIG model are optimized concurrently, allowing them to reinforce each other and improve adaptability to new data and downstream tasks
- Core assumption: Joint optimization of the TIG model and prompts leads to better adaptation compared to only tuning the prompts while keeping the model frozen
- Evidence anchors:
  - [abstract]: "To cater to varying computational resource demands, we propose an extended 'pre-train, prompt-based fine-tune' paradigm, offering greater flexibility."
  - [section]: "The main difference between these two modes lies in whether the parameters of TIG model ð‘“Î˜ (Â·) is tuned during the prompt tuning stage. Therefore, for this paradigm, given prompt samples, both the prompts and the TIG model are optimized concurrently..."
  - [corpus]: Weak evidence - no direct mention of the "pre-train, prompt-based fine-tune" paradigm in corpus neighbors
- Break condition: If the additional computational resources required for joint optimization do not lead to significant performance improvements

## Foundational Learning

- Concept: Temporal Interaction Graphs (TIGs) and their representation learning
  - Why needed here: Understanding TIGs and their representation learning is crucial for grasping the problem TIGPrompt aims to solve and the design of the TProG component
  - Quick check question: What are the key challenges in learning representations for TIGs, and how do existing TIG models address them?

- Concept: Prompt learning and its application in NLP and graph domains
  - Why needed here: Familiarity with prompt learning is essential for understanding how TIGPrompt leverages this paradigm to bridge the temporal and semantic gaps in TIG representation learning
  - Quick check question: How does prompt learning differ from traditional fine-tuning, and what are its advantages in adapting pre-trained models to downstream tasks?

- Concept: Transformer architectures and their application in temporal modeling
  - Why needed here: Understanding Transformer architectures is important for comprehending the implementation of the Transformer TProG mode, which uses a Transformer to encode temporal neighbor tokens into prompts
  - Quick check question: How do Transformer architectures handle sequential data, and what modifications are needed to adapt them for temporal interaction graphs?

## Architecture Onboarding

- Component map:
  TIG model (pre-trained) -> Temporal Prompt Generator (TProG) -> Fusion function -> Predictor

- Critical path:
  1. Pre-train TIG model on pretext task (e.g., link prediction)
  2. Generate prompts using TProG based on recent interaction history and time encoding
  3. Fuse pre-trained node embeddings and prompts using the fusion function
  4. Use prompted node representations for downstream tasks

- Design tradeoffs:
  - TProG variants: Vanilla (simple, personalized), Transformer (expressive, captures temporal dynamics), Projection (balance between simplicity and expressiveness)
  - "Pre-train, prompt" vs. "Pre-train, prompt-based fine-tune": Lightweight vs. more flexible, but requires more computational resources

- Failure signatures:
  - Poor performance on temporal link prediction: Indicates insufficient temporal information in prompts or ineffective time encoding
  - Poor performance on node classification: Suggests semantic gap is not adequately bridged or prompts are not task-specific enough
  - High computational cost: May indicate overuse of complex TProG variants or unnecessary fine-tuning of the TIG model

- First 3 experiments:
  1. Compare performance of different TProG variants on a simple link prediction task
  2. Evaluate the impact of prompt tuning on downstream node classification performance
  3. Assess the tradeoff between "pre-train, prompt" and "pre-train, prompt-based fine-tune" paradigms in terms of performance and computational cost

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the methodology and results presented, several important questions arise:

1. How does the proposed temporal prompt generator compare in terms of computational efficiency and effectiveness with alternative temporal encoding methods, such as time-aware graph neural networks or temporal attention mechanisms?

2. Can the proposed TIGPrompt framework be extended to handle more complex temporal dynamics, such as long-term dependencies or irregular time intervals between interactions?

3. How does the performance of the proposed TIGPrompt framework vary across different types of TIG datasets, such as those with different interaction patterns, node attributes, or label distributions?

## Limitations

- The reliance on recent interaction history for temporal prompt generation may not fully capture long-term temporal patterns, particularly in datasets with irregular interaction patterns
- The semantic bridging mechanism depends heavily on the quality and quantity of task-specific supervision data, which could limit effectiveness when supervision is scarce
- The computational efficiency claims assume access to pre-trained TIG models, but the pre-training costs are not addressed in the current evaluation

## Confidence

- **High**: The core framework design and experimental methodology are well-specified and reproducible
- **Medium**: The claims about bridging temporal and semantic gaps are supported by experimental results but rely on specific assumptions about data quality
- **Low**: The generalization capabilities across diverse TIG datasets and tasks need further validation

## Next Checks

1. Test TProG performance on datasets with highly irregular interaction patterns to evaluate temporal encoding robustness
2. Evaluate framework performance with varying levels of task-specific supervision to quantify the semantic bridging mechanism's sensitivity
3. Compare computational costs of pre-training versus prompt-tuning approaches across different dataset scales