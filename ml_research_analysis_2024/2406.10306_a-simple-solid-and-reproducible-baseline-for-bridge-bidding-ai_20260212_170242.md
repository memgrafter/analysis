---
ver: rpa2
title: A Simple, Solid, and Reproducible Baseline for Bridge Bidding AI
arxiv_id: '2406.10306'
source_url: https://arxiv.org/abs/2406.10306
tags:
- bridge
- bidding
- wbridge5
- learning
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper tackles the challenge of creating an AI for contract
  bridge bidding, a game of imperfect information and multi-agent cooperation. The
  authors propose a surprisingly effective approach that combines existing methods:
  supervised learning pretraining on a human expert dataset, followed by reinforcement
  learning using Proximal Policy Optimization with fictitious self-play.'
---

# A Simple, Solid, and Reproducible Baseline for Bridge Bidding AI

## Quick Facts
- arXiv ID: 2406.10306
- Source URL: https://arxiv.org/abs/2406.10306
- Reference count: 33
- State-of-the-art performance: +1.24 IMPs per board against WBridge5

## Executive Summary
This paper addresses the challenge of creating an AI for contract bridge bidding, a game of imperfect information and multi-agent cooperation. The authors propose a simple yet effective approach combining supervised learning pretraining on human expert data with reinforcement learning using Proximal Policy Optimization and fictitious self-play. The resulting model achieves state-of-the-art performance, surpassing the previous best result by a significant margin. The approach is notable for its simplicity while achieving superior results, and the authors release their code and models as open-source software.

## Method Summary
The approach combines supervised learning pretraining on the OpenSpiel bridge dataset (12.8M state-action pairs) with reinforcement learning using Proximal Policy Optimization and fictitious self-play on the DDS dataset. The model architecture is a simple 4-layer MLP with 1024 neurons per layer. SL pretraining uses Adam optimizer (learning rate 1.0e-4, batch size 128, 40 epochs) to establish a strong initial policy. The RL phase employs PPO with fictitious self-play (8192 vectorized environments, rollout length 32, GAE λ 0.95, discount factor 1.0, clip ratio 0.2, value loss coefficient 0.5, entropy coefficient 1.0e-3, batch size 1024, Adam optimizer, learning rate 1.0e-6, 104 PPO update steps with 10 epochs each) to fine-tune the pretrained model.

## Key Results
- Achieves +1.24 IMPs per board against WBridge5, surpassing previous best of +0.85 IMPs per board
- Model trained with SL pretraining only already outperforms WBridge5 without RL enhancement
- Fictitious self-play consistently demonstrates ability to outperform predecessors, indicating stable training

## Why This Works (Mechanism)

### Mechanism 1
SL pretraining on human expert data provides a strong initial bidding policy. This gives the model a reasonable starting point that can already outperform WBridge5, reducing the burden on RL. The core assumption is that the SAYC dataset used for pretraining transfers well to good performance against WBridge5 despite being different from WBridge5's system.

### Mechanism 2
Fictitious self-play prevents policy cycling and enables stable RL learning. By sampling opponents uniformly from checkpoints, FSP prevents the agent from getting stuck in loops of repeated strategies against itself. This addresses the common problem of policy cycling in standard self-play.

### Mechanism 3
PPO is effective for cooperative multi-agent settings like bridge bidding. PPO's clipped objective prevents large policy updates that could destabilize learning in the cooperative environment. The algorithm's design features that make it suitable for single-agent RL also benefit this cooperative multi-agent scenario.

## Foundational Learning

- **Concept: Supervised Learning (SL) pretraining**
  - Why needed here: Provides a strong initial policy that can already outperform WBridge5, reducing the burden on RL
  - Quick check question: What is the purpose of SL pretraining in this bridge bidding approach?

- **Concept: Reinforcement Learning (RL) with Proximal Policy Optimization (PPO)**
  - Why needed here: Fine-tunes the pretrained model to further improve performance through self-play
  - Quick check question: Why is PPO chosen for the RL phase in this bridge bidding approach?

- **Concept: Fictitious Self-Play (FSP)**
  - Why needed here: Prevents policy cycling and enables stable RL learning by sampling opponents from checkpoints
  - Quick check question: How does FSP help in the RL phase of this bridge bidding approach?

## Architecture Onboarding

- **Component map:** Input (480-dimensional binary vector) → 4 MLP layers (1024 neurons, ReLU) → Policy head (38 actions) and Value head → SL pretraining → PPO RL with FSP → Evaluation against WBridge5
- **Critical path:** SL pretraining → PPO RL with FSP → Evaluation against WBridge5
- **Design tradeoffs:** Simple MLP architecture vs. more complex models specific to bridge; Using SAYC dataset for pretraining vs. WBridge5's system; FSP vs. standard self-play
- **Failure signatures:** Poor performance against WBridge5 after SL pretraining; Policy cycling during RL phase; Large variance in IMPs/board during evaluation
- **First 3 experiments:**
  1. Train model with SL pretraining only, evaluate against WBridge5
  2. Train model from scratch with RL and FSP, evaluate against WBridge5
  3. Compare standard self-play vs. FSP in RL phase

## Open Questions the Paper Calls Out

- What specific factors contribute to the performance discrepancy between this paper's model and Gong et al.'s claim of achieving strong results without SL pretraining?
- How would the proposed model perform when trained with WBridge5's native bidding system instead of the SAYC system used for pretraining?
- What are the specific limitations of WBridge5 as a benchmark, and how do they impact the validity of AI bridge bidding research?

## Limitations
- Evaluation focuses solely on IMPs per board against WBridge5 without exploring other metrics or opponents
- Lacks ablation studies to isolate contributions of individual components (SL pretraining, PPO, FSP)
- Simple 4-layer MLP architecture without exploration of more complex models specific to bridge bidding

## Confidence

- **High confidence** in the reported IMPs improvement (+1.24 vs +0.85) - this is directly measured against a known baseline
- **Medium confidence** in the mechanisms - while the components are described, the paper lacks rigorous ablation studies to isolate individual contributions
- **Low confidence** in generalizability - the model's performance on datasets different from training data or against other opponents is not explored

## Next Checks
1. Conduct ablation studies removing each component (SL pretraining, PPO, FSP) to quantify individual contributions to the final performance
2. Test the model against bridge bidding systems other than WBridge5 to assess generalization
3. Evaluate performance variance across different random seeds to establish statistical significance of the reported IMPs improvement