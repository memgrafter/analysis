---
ver: rpa2
title: Graph Neural Network Causal Explanation via Neural Causal Models
arxiv_id: '2407.09378'
source_url: https://arxiv.org/abs/2407.09378
tags:
- causal
- graph
- node
- neural
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CXGNN is the first GNN causal explainer using neural causal models
  to uncover the causal subgraph that drives predictions. The method builds a causal
  structure for a graph, trains a family of GNN-NCMs to estimate cause-effect relations,
  and selects the subgraph with highest node expressivity.
---

# Graph Neural Network Causal Explanation via Neural Causal Models

## Quick Facts
- **arXiv ID:** 2407.09378
- **Source URL:** https://arxiv.org/abs/2407.09378
- **Reference count:** 40
- **Primary result:** CXGNN is the first GNN causal explainer using neural causal models to uncover the causal subgraph that drives predictions, achieving up to 100% accuracy on some datasets.

## Executive Summary
CXGNN introduces a novel approach to explaining Graph Neural Network (GNN) predictions by leveraging neural causal models (NCMs). The method constructs a causal structure for each node, trains GNN-NCMs to estimate cause-effect relations, and identifies the most expressive causal subgraph. CXGNN significantly outperforms association-based and causality-inspired baselines on both synthetic and real-world datasets with groundtruth explanations.

## Method Summary
CXGNN builds causal structures centered on each node, constructs GNN-NCMs parameterized by neural networks, and trains them to approximate the underlying Structural Causal Model (SCM). The trained models compute interventional distributions via do-calculus to quantify causal effects. Node expressivity is used to select the most informative reference node, and its associated causal structure is returned as the causal explanatory subgraph. The method was evaluated on 6 synthetic and 2 real-world datasets, demonstrating superior performance in exact groundtruth matching.

## Key Results
- CXGNN achieves up to 100% accuracy on some datasets, while other methods achieve less than 50%.
- The method significantly outperforms association-based and causality-inspired baselines in exact groundtruth matching.
- Loss curves show that GNN-NCMs learn causal mechanisms more effectively for groundtruth nodes compared to non-groundtruth nodes.

## Why This Works (Mechanism)

### Mechanism 1
CXGNN uses neural causal models (NCMs) to estimate cause-effect relations in graphs, enabling the identification of the true causal subgraph. For each node, it builds a causal structure, constructs a GNN-NCM, and trains it to approximate the underlying SCM. The trained model computes interventional distributions via do-calculus, allowing quantification of causal effects between nodes.

### Mechanism 2
Node expressivity is used to select the reference node whose trained GNN-NCM yields the most informative causal explanation. After training, CXGNN calculates node expressivity as the expected value of predicted label probabilities over all possible outcomes. The node with the highest expressivity is chosen as the reference, and its associated causal structure is returned as the causal explanatory subgraph.

### Mechanism 3
Training GNN-NCMs on groundtruth nodes leads to lower loss compared to non-groundtruth nodes, indicating better learning of causal subgraphs. The loss curves show that during training, loss decreases stably for nodes in the groundtruth causal subgraph but remains higher for nodes not in it, suggesting the GNN-NCM architecture and training process are better suited to capture true causal relationships.

## Foundational Learning

- **Graph Neural Networks (GNNs) and message-passing:** CXGNN operates on graph structure and uses GNN embeddings as part of causal structure definition. Quick check: Can you explain how a GNN aggregates information from a node's neighbors in a single layer?
- **Structural Causal Models (SCMs) and do-calculus:** CXGNN builds SCMs for each node's causal structure and uses do-calculus to compute interventional distributions for causal effect estimation. Quick check: What is the difference between observational and interventional probabilities in an SCM?
- **Neural Causal Models (NCMs) and their relationship to SCMs:** CXGNN uses NCMs as a tractable approximation to SCMs, leveraging neural networks to learn causal mechanisms. Quick check: How does an NCM ensure that its inferences respect the causal dependencies captured by the underlying SCM?

## Architecture Onboarding

- **Component map:** Input graph G → Build causal structure for each node v → Construct GNN-NCM cM(G, θ) → Train GNN-NCM → Calculate node expressivity expv(cM(G, θ)) → Select node with highest expressivity → Return associated causal subgraph
- **Critical path:** Build causal structure → Construct and train GNN-NCM → Calculate node expressivity → Select node with highest expressivity → Return associated causal subgraph
- **Design tradeoffs:**
  - Computational cost: Training GNN-NCM for each node can be expensive for large graphs; parallelization can help
  - Expressivity vs. Overfitting: Deeper/wider NCMs may capture more complex causal mechanisms but risk overfitting to spurious correlations
  - Choice of reference node: Selecting node with highest expressivity assumes it's central to true causal subgraph; may fail if assumption is violated
- **Failure signatures:**
  - High loss for groundtruth nodes during GNN-NCM training (indicates poor learning of causal mechanisms)
  - Node expressivity scores are similar across all nodes (suggests no clear central causal node)
  - Estimated causal subgraph doesn't match groundtruth (indicates incorrect causal structure or training)
- **First 3 experiments:**
  1. Run CXGNN on small synthetic graph (e.g., BA+House) and verify estimated causal subgraph matches groundtruth "house" motif
  2. Compare loss curves for groundtruth vs. non-groundtruth nodes during GNN-NCM training on sample graph to confirm expected pattern
  3. Test CXGNN on real-world dataset (e.g., Benzene) and evaluate overlap between estimated causal subgraph and known benzene ring atoms

## Open Questions the Paper Calls Out

- How does CXGNN perform on large-scale graphs compared to existing explainers?
- How robust is CXGNN to adversarial attacks on the graph structure?
- How does the choice of the reference node affect the performance of CXGNN?

## Limitations
- Scalability concerns when training individual GNN-NCMs for each node in large graphs
- Sensitivity of node expressivity to hyperparameter choices
- Assumption that highest-expressivity node corresponds to true causal center may not always hold

## Confidence
- **Mechanism 1 (NCM estimation): High** - Strong theoretical grounding and clear experimental support
- **Mechanism 2 (Node expressivity selection): Medium** - Method is clearly defined but assumes expressivity directly maps to causal centrality, which may not always hold
- **Mechanism 3 (Loss curve distinction): Medium** - Observed pattern in experiments but limited to specific datasets; generalizability uncertain

## Next Checks
1. Test CXGNN on larger real-world graphs (e.g., social networks) to assess scalability and runtime
2. Perform ablation studies on node expressivity metric by comparing against alternative centrality measures (e.g., betweenness, eigenvector)
3. Validate CXGNN's robustness to noisy or incomplete causal structures by introducing edge perturbations in synthetic datasets