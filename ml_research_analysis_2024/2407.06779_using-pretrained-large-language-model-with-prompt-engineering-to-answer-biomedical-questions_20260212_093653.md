---
ver: rpa2
title: Using Pretrained Large Language Model with Prompt Engineering to Answer Biomedical
  Questions
arxiv_id: '2407.06779'
source_url: https://arxiv.org/abs/2407.06779
tags:
- question
- prompt
- system
- answer
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper describes a biomedical question-answering system that
  combines large language models with information retrieval from PubMed. The authors
  propose a two-stage pipeline: first, LLM-powered keyword extraction and query generation
  for document retrieval, followed by sentence transformer-based ranking for snippet
  selection.'
---

# Using Pretrained Large Language Model with Prompt Engineering to Answer Biomedical Questions

## Quick Facts
- arXiv ID: 2407.06779
- Source URL: https://arxiv.org/abs/2407.06779
- Reference count: 21
- Best MAP score: 0.14 for document retrieval

## Executive Summary
This paper presents a biomedical question-answering system that combines large language models with information retrieval from PubMed. The authors propose a two-stage pipeline: first, LLM-powered keyword extraction and query generation for document retrieval, followed by sentence transformer-based ranking for snippet selection. For question answering, they use in-context few-shot prompting with retrieved snippets as context, and apply post-processing techniques like malformed response detection and synonym grouping. The system achieves competitive results on the BioASQ 2024 Task 12b, with Mixtral 47B identified as the best-performing model.

## Method Summary
The system uses a two-stage pipeline for biomedical question answering. First, it extracts keywords from questions using biomedical NER models or LLMs, then constructs PubMed queries to retrieve documents. Documents are ranked using sentence transformers based on cosine similarity with the question. For question answering, the system uses in-context few-shot prompting with golden snippets as context, followed by post-processing steps including malformed response detection and resampling. The approach focuses on prompt engineering rather than fine-tuning, leveraging the few-shot examples to guide the LLM in generating answers in the desired format.

## Key Results
- Document retrieval achieved 0.14 MAP score
- Snippet retrieval achieved 0.05 MAP score
- Yes/no questions: 0.96 F1 score
- Factoid questions: 0.38 MRR score
- List questions: 0.50 F1 score
- Mixtral 47B identified as best-performing model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using in-context few-shot examples in prompts improves answer accuracy for biomedical questions.
- Mechanism: The LLM leverages the few-shot examples to understand the desired output format and reasoning pattern, reducing hallucinations and improving alignment with expected answer structure.
- Core assumption: The LLM has sufficient capacity to learn from few-shot examples and apply that knowledge to new questions.
- Evidence anchors:
  - [abstract] "We construct prompts with in-context few-shot examples and utilize post-processing techniques like resampling and malformed response detection."
  - [section] "The few-shot examples help LLM to generate answers in the desired format."
  - [corpus] Found 25 related papers with average neighbor FMR=0.515, indicating related work on LLM prompting for biomedical QA.
- Break condition: If the few-shot examples are poorly chosen or the context is too noisy, the LLM may fail to extract the correct reasoning pattern.

### Mechanism 2
- Claim: Using golden snippets as context improves answer accuracy compared to using retrieved snippets.
- Mechanism: Golden snippets contain verified, relevant information directly related to the question, providing accurate context that guides the LLM to generate correct answers.
- Core assumption: The golden snippets are indeed relevant and accurate for the given questions.
- Evidence anchors:
  - [abstract] "The context is crucial for generating high-quality answers and reducing model hallucinations."
  - [section] "Adding context, especially using 'correct' snippets as the context, to the prompt can greatly improve the QA answering accuracy for all types of questions."
  - [corpus] Weak evidence - no direct citation found for golden snippet effectiveness.
- Break condition: If the golden snippets contain outdated or incorrect information, or if they are not truly relevant to the question, the answer quality may degrade.

### Mechanism 3
- Claim: Resampling and malformed response detection improve overall system accuracy.
- Mechanism: By detecting and resampling responses that don't meet format requirements (e.g., missing "exact answer" field), the system ensures more consistent and correct output formats.
- Core assumption: The detection rules accurately identify malformed responses and that resampling produces better results.
- Evidence anchors:
  - [abstract] "We construct prompts with in-context few-shot examples and utilize post-processing techniques like resampling and malformed response detection."
  - [section] "When the answer does not follow the expected format, we resample the LLM output."
  - [corpus] Found related work on LLM robustness and response validation, supporting the general approach.
- Break condition: If the detection rules are too strict or too lenient, or if resampling doesn't improve the response, this mechanism may not add value.

## Foundational Learning

- Concept: Prompt Engineering with In-Context Learning
  - Why needed here: The system relies on guiding LLMs through examples rather than fine-tuning, making effective prompt design critical for performance.
  - Quick check question: What are the key components of an effective few-shot prompt for biomedical QA?

- Concept: Information Retrieval with Semantic Embeddings
  - Why needed here: The system uses sentence transformers to rank documents and snippets based on semantic similarity, requiring understanding of embedding-based retrieval.
  - Quick check question: How does cosine similarity between sentence embeddings determine document relevance?

- Concept: Post-Processing for Response Validation
  - Why needed here: The system implements rules to detect and resample malformed responses, requiring understanding of validation logic and error handling.
  - Quick check question: What are common signs of malformed LLM responses in structured output tasks?

## Architecture Onboarding

- Component map:
  Query Constructor (LLM/Keyword Extractor) → PubMed Search → Document Retriever
  Document Ranker (Sentence Transformer) → Snippet Extractor → Context Builder
  Question Answerer (LLM with Few-Shot Prompt) → Response Post-Processor → Final Answer

- Critical path:
  1. Keyword extraction from question
  2. PubMed query execution
  3. Document ranking and snippet selection
  4. Context formation for QA
  5. LLM response generation with validation

- Design tradeoffs:
  - Using golden snippets vs. retrieved snippets: accuracy vs. system autonomy
  - Number of snippets in context: comprehensiveness vs. token limits and noise
  - Model selection: performance vs. computational cost

- Failure signatures:
  - Low MAP scores: poor keyword extraction or ranking
  - Malformed answers: inadequate response validation or resampling
  - Hallucinations: insufficient or irrelevant context

- First 3 experiments:
  1. Compare keyword extraction performance between spaCy NER and LLM-based approaches on sample questions
  2. Test different context configurations (1 vs 3 snippets) on a subset of questions and measure accuracy impact
  3. Implement and test response validation rules on sample LLM outputs to ensure correct format detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the number of initially retrieved documents from PubMed beyond 30 affect the overall MAP score for document retrieval, and what is the optimal number of documents to retrieve?
- Basis in paper: [explicit] The authors mention that increasing the number of documents retrieved from 10 to 30 significantly improved the MAP score from 0.0497 to 0.1385 in batch 3. They also note that systems using BM25 models for retrieval typically fetch hundreds of documents initially, achieving the best scores.
- Why unresolved: The authors did not experiment with retrieving more than 30 documents due to the increased processing time required. They hypothesize that further increasing the number of documents could improve performance but did not test this.
- What evidence would resolve it: Conducting experiments with different numbers of initially retrieved documents (e.g., 50, 100, 200) and measuring the resulting MAP scores would provide insights into the optimal number of documents to retrieve for the best performance.

### Open Question 2
- Question: Does using different sentence embedding models or splitting documents into multiple parts and calculating similarity for each part improve the performance of the reranking stage in the information retrieval system?
- Basis in paper: [inferred] The authors mention using the all-MiniLM-L6-v2 sentence transformer for generating embeddings and ranking documents. They also note that they only use the first part of the document that fits the embedding model's input token size.
- Why unresolved: The authors did not experiment with different sentence embedding models or document splitting strategies to assess their impact on reranking performance.
- What evidence would resolve it: Experimenting with various sentence embedding models and document splitting approaches, then comparing the resulting MAP scores, would determine if these modifications improve the reranking stage's effectiveness.

### Open Question 3
- Question: How does fine-tuning the pretrained LLM model (specifically Mixtral47B) on the BioASQ dataset using Low Rank Adaptation (LoRA) compare to the current few-shot prompting approach in terms of question-answering performance?
- Basis in paper: [explicit] The authors mention that their current system relies on few-shot examples to guide the LLM in generating answers and have not utilized the potential of the BioASQ dataset. They suggest that fine-tuning the pretrained LLM model on the BioASQ dataset using LoRA could be a future improvement.
- Why unresolved: The authors did not experiment with fine-tuning the LLM model on the BioASQ dataset using LoRA, so there is no direct comparison with the current few-shot prompting approach.
- What evidence would resolve it: Fine-tuning the Mixtral47B model on the BioASQ dataset using LoRA and evaluating its performance on the question-answering tasks, then comparing the results with the current few-shot prompting approach, would provide insights into the effectiveness of fine-tuning versus prompting.

## Limitations

- The paper lacks detailed implementation specifications for critical components, particularly the few-shot examples used in prompting and exact criteria for malformed response detection.
- Document and snippet retrieval MAP scores (0.14 and 0.05) are relatively low, suggesting the information retrieval component may be a bottleneck that wasn't thoroughly investigated.
- Evaluation is limited to the BioASQ 2024 Task 12b dataset, restricting generalizability to other biomedical QA scenarios or different question types.

## Confidence

- High Confidence: The overall system architecture and its two-stage pipeline approach (document retrieval → snippet ranking → QA with context) is well-supported by the experimental results and aligns with established best practices in retrieval-augmented generation.
- Medium Confidence: The effectiveness of golden snippets as context is supported by performance improvements, but the paper doesn't provide ablation studies comparing different context configurations or investigate why retrieved snippets underperform.
- Medium Confidence: The claim that Mixtral 47B is the best-performing model is based on testing multiple models, but the comparison methodology and specific evaluation criteria aren't fully detailed.
- Low Confidence: The exact impact of individual post-processing techniques (resampling, malformed response detection, synonym grouping) on final performance isn't quantified through ablation studies.

## Next Checks

1. Implement ablation studies: Systematically disable each post-processing component (resampling, malformed response detection, synonym grouping) to quantify their individual contributions to the 0.50 F1 score for list questions and other metrics.

2. Test context sensitivity: Conduct controlled experiments varying the number and source of snippets (golden vs retrieved, 1 vs 3 vs 5 snippets) to determine the optimal context configuration and understand the performance gap between golden and retrieved snippets.

3. Benchmark retrieval improvements: Replace the current keyword extraction and document ranking pipeline with alternative approaches (e.g., BM25, learned sparse retrievers) to identify whether the low MAP scores stem from the retrieval methodology rather than the QA components.