---
ver: rpa2
title: 'AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML'
arxiv_id: '2410.02958'
source_url: https://arxiv.org/abs/2410.02958
tags:
- dataset
- data
- performance
- automl-agent
- automl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AutoML-Agent is a multi-agent LLM framework that automates the\
  \ full machine learning pipeline\u2014from data retrieval and preprocessing to model\
  \ selection, hyperparameter tuning, and deployment\u2014using natural language instructions.\
  \ It introduces a retrieval-augmented planning strategy with role-specific decomposition\
  \ and prompting-based execution to efficiently handle the complexity of end-to-end\
  \ AutoML."
---

# AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML

## Quick Facts
- arXiv ID: 2410.02958
- Source URL: https://arxiv.org/abs/2410.02958
- Reference count: 40
- Primary result: Multi-agent LLM framework automating full ML pipeline with 87.1% success rate

## Executive Summary
AutoML-Agent introduces a novel multi-agent LLM framework that automates the entire machine learning pipeline from data retrieval to deployment using natural language instructions. The system employs a retrieval-augmented planning strategy with role-specific decomposition and prompting-based execution to manage the complexity of end-to-end AutoML tasks. Through a multi-stage verification process, the framework ensures accurate implementation and alignment with user requirements. The approach demonstrates superior performance across seven downstream tasks spanning 14 datasets, significantly outperforming existing methods.

## Method Summary
The framework implements a multi-agent architecture where specialized agents handle different stages of the ML pipeline through coordinated planning and execution. The system leverages retrieval-augmented generation to access relevant documentation and examples during planning phases. Role-specific decomposition breaks down complex ML workflows into manageable subtasks, while prompting-based execution translates high-level instructions into concrete implementation steps. A multi-stage verification process validates each pipeline component before proceeding to the next stage, ensuring accuracy and completeness throughout the automated workflow.

## Key Results
- Achieves 87.1% success rate across seven downstream tasks on 14 datasets
- Obtains comprehensive score of 0.841 under constraint-aware settings
- Outperforms existing methods in automating full ML pipeline from data to deployment

## Why This Works (Mechanism)
The multi-agent architecture enables specialized handling of different ML pipeline stages while maintaining coordination through a central planning agent. Retrieval-augmented planning provides access to up-to-date documentation and best practices during decision-making, overcoming knowledge limitations of base LLMs. Role-specific decomposition transforms complex end-to-end tasks into manageable subtasks that can be handled by specialized agents with appropriate expertise. The multi-stage verification process catches errors early and ensures alignment with user requirements before proceeding to subsequent pipeline stages.

## Foundational Learning
- **Retrieval-augmented generation**: Needed for accessing current documentation and examples beyond base LLM knowledge cutoff. Quick check: Verify retrieval component can find relevant documentation for new ML libraries and frameworks.
- **Role-specific decomposition**: Required to break down complex ML workflows into manageable subtasks. Quick check: Ensure decomposition maintains logical flow and covers all pipeline stages.
- **Multi-stage verification**: Essential for catching errors early and ensuring alignment with requirements. Quick check: Validate that each verification stage catches different types of errors specific to that pipeline stage.
- **Prompt engineering for code generation**: Critical for translating natural language instructions into executable code. Quick check: Test generated code for correctness and efficiency across different ML tasks.
- **Agent coordination protocols**: Necessary for maintaining coherence across multiple specialized agents. Quick check: Verify agents can communicate effectively and resolve conflicts in planning and execution.

## Architecture Onboarding

**Component Map**: User Input -> Planning Agent -> Retrieval Module -> Specialized Agents (Data, Preprocessing, Model Selection, Hyperparameter Tuning, Deployment) -> Verification Module -> Output

**Critical Path**: Planning Agent orchestrates workflow → Retrieval Module provides context → Specialized Agents execute pipeline stages → Verification Module validates each stage → Final output generation

**Design Tradeoffs**: Multi-agent coordination overhead vs. specialized expertise; retrieval latency vs. up-to-date knowledge access; verification thoroughness vs. execution speed; natural language flexibility vs. precise specification requirements

**Failure Signatures**: Retrieval failures leading to outdated or incorrect documentation; decomposition errors causing incomplete pipeline coverage; verification gaps allowing erroneous implementations; agent communication breakdowns causing workflow inconsistencies

**First Experiments**:
1. Single pipeline stage automation (e.g., only model selection) to validate individual agent capabilities
2. Simple end-to-end workflow with known dataset to test coordination between agents
3. Stress test with ambiguous natural language instructions to evaluate planning robustness

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation framework may not fully represent real-world ML scenario diversity and complexity
- Performance metrics based on constraint-aware settings that may not reflect all practical deployments
- Limited analysis of failure cases and system weaknesses across different task types
- Natural language ambiguity may introduce interpretation errors not fully addressed in evaluation

## Confidence
- High confidence in technical feasibility and novelty of multi-agent architecture with role-specific decomposition and retrieval-augmented planning
- Medium confidence in reported performance metrics due to limited evaluation dataset and task scope
- Medium confidence in effectiveness of multi-stage verification process given limited implementation details and failure analysis

## Next Checks
1. Conduct extensive testing across diverse domains including healthcare, finance, and scientific research to evaluate generalizability
2. Perform ablation studies to quantify individual contributions of retrieval-augmented planning, role-specific decomposition, and verification process
3. Test system under varying computational constraints and time limitations to assess practical deployment viability and identify bottlenecks