---
ver: rpa2
title: 'GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models'
arxiv_id: '2407.21001'
source_url: https://arxiv.org/abs/2407.21001
tags:
- bias
- gender
- activity
- vlms
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Gender-Activity Binding (GAB) bias, where
  vision-language models (VLMs) incorrectly associate activities with stereotypical
  genders in complex scenes involving multiple people. To study this, the authors
  create the GAB dataset of ~5,500 AI-generated images depicting various gender-stereotyped
  activities in different contexts (single person vs.
---

# GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models

## Quick Facts
- arXiv ID: 2407.21001
- Source URL: https://arxiv.org/abs/2407.21001
- Reference count: 40
- Primary result: VLMs experience an average 13.2% accuracy drop on gender-activity binding tasks, particularly in two-person scenarios

## Executive Summary
This paper introduces the Gender-Activity Binding (GAB) bias in vision-language models (VLMs), where models incorrectly associate activities with stereotypical genders in complex scenes involving multiple people. The authors create a novel GAB dataset of ~5,500 AI-generated images depicting various gender-stereotyped activities in different contexts. They evaluate 12 prominent VLMs on text-to-image and image-to-text retrieval tasks, finding significant performance drops when encountering gender-activity binding bias. The bias primarily manifests in image-to-text retrieval, while text-to-image retrieval performance is nearly random. Supplementary experiments reveal the bias is more pronounced in text encoders than image encoders, and that VLMs' activity recognition is hindered by this binding bias.

## Method Summary
The authors construct the GAB dataset using DALL-E 3 to generate images depicting gender-stereotyped activities in single-person and two-person scenarios. They evaluate 12 pre-trained VLMs (CLIP variants, NegCLIP, Eva models, FLAVA, ALIGN, COCA, and AltCLIP) on zero-shot retrieval tasks using cosine similarity in shared embedding spaces. The evaluation includes text-to-image and image-to-text retrieval accuracy measurements, performance drop analysis when unexpected genders perform stereotypical activities, and supplementary experiments on text encoder bias and activity recognition capability.

## Key Results
- VLMs experience an average 13.2% accuracy drop when encountering gender-activity binding bias in two-person scenarios
- Text-to-image retrieval performance is nearly random (~50% accuracy), indicating VLMs struggle to bind gender and activity from text alone
- The bias is more pronounced in text encoders than image encoders, with the majority of activities showing expected gender preference in text encoder evaluation
- While VLMs can recognize activities to some extent, their performance is hindered by gender-activity binding bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAB bias arises from co-occurrence patterns in training data where certain activities are statistically more associated with one gender
- Mechanism: VLMs internalize sample selection bias during pretraining, causing them to preferentially link activities with stereotypical genders
- Core assumption: Training datasets reflect societal gender stereotypes through skewed activity-gender co-occurrence frequencies
- Break condition: If training data is balanced for activity-gender co-occurrences, or if model architecture explicitly disentangles activity and gender features

### Mechanism 2
- Claim: The bias is more pronounced in text encoders than image encoders
- Mechanism: Text embeddings are more strongly influenced by gender-activity associations because language inherently contains more explicit gender cues
- Core assumption: Language contains stronger and more explicit gender cues than visual representations
- Break condition: If image encoder is fine-tuned with gender-biased text supervision, or if text preprocessing removes gender cues

### Mechanism 3
- Claim: VLMs fail to bind gender and activity in text-to-image retrieval because they don't comprehend the properties of the given images
- Mechanism: In text-to-image retrieval, the model cannot use visual context to resolve ambiguity, so it relies solely on text embeddings which are gender-biased
- Core assumption: Text-to-image retrieval lacks the visual disambiguation cues available in image-to-text retrieval
- Break condition: If the model is trained with paired text-image supervision that explicitly encodes gender-activity relationships

## Foundational Learning

- Concept: Vision-Language Models (VLMs) and their shared embedding space
  - Why needed here: Understanding how VLMs encode and retrieve multimodal information is crucial to analyzing the GAB bias
  - Quick check question: What is the primary difference between image-to-text and text-to-image retrieval tasks in VLMs?

- Concept: Gender stereotypes and sample selection bias in training data
  - Why needed here: The GAB bias is rooted in how societal gender stereotypes are reflected in training data through co-occurrence patterns
  - Quick check question: How might skewed gender-activity co-occurrences in training data lead to biased predictions in VLMs?

- Concept: Masked Language Modeling (MLM) and its role in activity selection
  - Why needed here: The paper uses MLM to identify gender-biased activities by masking subjects and predicting gender probabilities
  - Quick check question: How does the use of MLM with RoBERTa help identify activities that are stereotypically associated with specific genders?

## Architecture Onboarding

- Component map: Text encoder -> Shared embedding space -> Image encoder; Activity recognition module evaluates model's understanding
- Critical path: Activity selection → Image generation (DALL-E 3) → Quality/diversity filtering → Model evaluation (text-to-image, image-to-text, activity recognition) → Bias analysis
- Design tradeoffs: Using AI-generated images ensures dataset availability but may introduce generation biases; focusing on two genders simplifies analysis but limits generalizability; evaluating 12 VLMs provides breadth but may miss nuances of individual model architectures
- Failure signatures: Random performance in text-to-image retrieval (~50% accuracy), significant performance drop when unexpected gender performs activity (13.2% average decline), bias in text encoder (majority of activities show expected gender preference)
- First 3 experiments:
  1. Evaluate a VLM on the E1 scenario (unexpected gender alone) and compare accuracy to U1 scenario (expected gender alone) to confirm baseline performance without bias
  2. Test the same VLM on E2 and U2 scenarios (both genders present) to measure the impact of GAB bias on image-to-text retrieval
  3. Analyze the text encoder separately by comparing embeddings of "a person is <doing activity>" with gendered versions to quantify text encoder bias

## Open Questions the Paper Calls Out

- How do VLMs perform on gender-activity binding tasks when evaluating more than two genders or non-binary gender representations?
- What is the relationship between VLM performance on gender-activity binding tasks and their performance on other forms of compositional reasoning?
- How does the gender-activity binding bias in VLMs compare to human biases in similar tasks?
- How do different training data distributions affect the strength and nature of gender-activity binding bias in VLMs?

## Limitations
- The findings are based on AI-generated images rather than real-world photographs, limiting generalizability to naturalistic scenarios
- The analysis is restricted to binary gender categories (male/female), excluding non-binary and other gender identities
- The mechanism explanations for text encoder bias are somewhat speculative and lack rigorous ablation studies
- The paper doesn't explore potential mitigation strategies or the effectiveness of bias correction techniques

## Confidence

- **High Confidence**: The empirical findings showing ~13.2% average accuracy drop in two-person scenarios and ~50% random performance in text-to-image retrieval are well-supported by experimental results
- **Medium Confidence**: The claim that GAB bias arises from training data co-occurrence patterns is plausible but not directly proven - it's inferred from bias manifestation rather than traced to specific training data characteristics
- **Low Confidence**: The assertion that text encoders are inherently more biased than image encoders due to language containing stronger gender cues is based on observed performance differences but lacks mechanistic explanation

## Next Checks

1. Replicate the core findings using real-world photographs from existing datasets to verify whether AI-generated images produce consistent bias patterns
2. Conduct ablation studies comparing VLMs trained on balanced versus stereotypical gender-activity co-occurrence datasets to establish causal links between training data and GAB bias
3. Test additional gender categories beyond binary male/female to assess whether observed bias patterns generalize to more inclusive gender representations