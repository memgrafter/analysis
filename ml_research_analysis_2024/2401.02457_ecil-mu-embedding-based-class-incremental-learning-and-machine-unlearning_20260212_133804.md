---
ver: rpa2
title: 'eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning'
arxiv_id: '2401.02457'
source_url: https://arxiv.org/abs/2401.02457
tags:
- unlearning
- classes
- class
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces eCIL-MU, a framework integrating class incremental
  learning (CIL) and machine unlearning (MU) using vector embeddings and databases.
  Instead of modifying models, it modifies data by mapping inputs to vectors stored
  in a vector database.
---

# eCIL-MU: Embedding based Class Incremental Learning and Machine Unlearning

## Quick Facts
- arXiv ID: 2401.02457
- Source URL: https://arxiv.org/abs/2401.02457
- Reference count: 0
- Primary result: Achieves up to ~278× speedup compared to retraining from scratch and restoring and resuming training for class incremental learning with machine unlearning.

## Executive Summary
eCIL-MU is a framework that integrates class incremental learning (CIL) and machine unlearning (MU) using vector embeddings and databases. Instead of modifying models, it modifies data by mapping inputs to vectors stored in a vector database. For CIL, it stores vectors in DB-CIL; for MU, it transfers vectors of classes to be unlearned from DB-CIL to DB-MU using cosine similarity and KNN matching. The overlap between CIL and MU tasks enables training acceleration. During inference, a vector filter identifies unlearned classes, and four output strategies are proposed. Experiments on CIFAR-10 and CIFAR-100 show that the shift-to-nearest-class strategy achieves non-destructive effects, with accuracy on remaining classes approaching that of CIL alone and lower accuracy on unlearned classes.

## Method Summary
eCIL-MU leverages vector embeddings and vector databases to enable data modification for both CIL and MU. The framework uses a pre-trained embedding model (ResNet-50) to map input data into high-dimensional vectors. These vectors are stored in two separate databases: DB-CIL for learned classes and DB-MU for unlearnable classes. During CIL, vectors are stored in DB-CIL and used to train the CIL-MU model. For MU, vectors associated with unlearnable classes are identified in DB-CIL using cosine similarity and KNN matching, then transferred to DB-MU. This transfer allows for asynchronous overlap between CIL and MU tasks, accelerating the overall process. During inference, a vector filter uses cosine similarity to identify potential unlearnable class data, and one of four output strategies is applied to handle predictions.

## Key Results
- Achieves non-destructive unlearning effects using the shift-to-nearest-class strategy during inference.
- Accuracy on remaining classes approaches that of CIL alone, with lower accuracy on unlearned classes.
- Achieves up to ~278× speedup compared to retraining from scratch and restoring and resuming training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vector embedding with KNN matching allows accurate identification and transfer of unlearnable classes from DB-CIL to DB-MU.
- Mechanism: The pre-trained embedding model Me maps input data into high-dimensional vectors. Cosine similarity is used to measure the proximity between vectors, and KNN (K=100) is applied to identify the most similar vectors belonging to the unlearnable classes. These vectors are then transferred from DB-CIL to DB-MU.
- Core assumption: Vectors from the same class are sufficiently close in the embedding space, and cosine similarity with KNN is effective in distinguishing between classes.
- Evidence anchors:
  - [abstract] "Utilizing an embedding technique, we map training data into vectors and utilize vector databases for vectors storage and unlearning. Specifically, during a CIL task, we store vectors in the DB-CIL; identify vectors linked to classes to be unlearned in DB-CIL and then transfer them to the unlearning database DB-MU during MU process."
  - [section] "Upon computing the cosine similarity, we utilize the K-nearest neighbor (KNN) to identify the class of unlearning data and select the most similar vectors v with same class. Subsequently, v is transferred from DB-CIL to DB-MU."
- Break condition: If the embedding model fails to create sufficiently discriminative vectors, KNN matching will not be effective, leading to incorrect class identification and transfer.

### Mechanism 2
- Claim: The "shift-to-nearest-class" strategy during inference mitigates errors introduced by vector filtering, improving accuracy on remaining classes and reducing accuracy on unlearnable classes.
- Mechanism: During inference, a vector filter using cosine similarity with a threshold identifies potential unlearnable class data. The "shift-to-nearest-class" strategy then reassigns misclassified data from the unlearnable class to the nearest correct class based on cosine similarity, improving overall prediction accuracy.
- Core assumption: Misclassified data from the unlearnable class are inherently closer to vectors of the remaining classes in the embedding space.
- Evidence anchors:
  - [abstract] "We apply four distinct output strategies for the unlearning during the inference phase, and find that using the strategy shift-to-nearest-class achieves non-destructive effects, while also delaying catastrophic forgetting."
  - [section] "Using Eq.5 for filtering introduces errors that result in certain vectors not being unlearned being incorrectly identified as unlearned vectors... We shift Cf to the nearest class to rectify these misclassified classes and produce accurate outputs."
- Break condition: If the threshold for vector filtering is not properly calibrated, the "shift-to-nearest-class" strategy may incorrectly reassign data, leading to decreased accuracy.

### Mechanism 3
- Claim: Overlapping CIL and MU tasks through vector database operations enables significant acceleration compared to retraining from scratch or restoring and resuming training.
- Mechanism: By storing vectors in DB-CIL and DB-MU, the framework allows for asynchronous transfer of unlearnable class vectors during CIL tasks. This overlap reduces the overall time required for unlearning compared to methods that require complete retraining or model restoration.
- Core assumption: The time required for vector embedding and database operations is less than the time required for retraining or model restoration.
- Evidence anchors:
  - [abstract] "Our approach exploits the overlap between CIL and MU tasks for acceleration. Experiments demonstrate the capability of achieving unlearning effectiveness and orders of magnitude (upto ∼ 278×) of acceleration."
  - [section] "As shown in Fig.2, eCIL-MU allows partial overlap between CIL and MU tasks. The acceleration effect is illustrated by the time taken for each strategy in Tab.1... it achieves an acceleration of up to 278.48 ×."
- Break condition: If the overhead of managing vector databases and performing asynchronous transfers outweighs the time saved by avoiding retraining, the acceleration benefit will be negated.

## Foundational Learning

- Concept: Class Incremental Learning (CIL)
  - Why needed here: CIL is the foundation for understanding how the model learns new classes over time while retaining knowledge of previously learned classes. The eCIL-MU framework integrates CIL with machine unlearning.
  - Quick check question: What is the primary challenge addressed by CIL, and how does it relate to the "stability-plasticity dilemma"?

- Concept: Machine Unlearning (MU)
  - Why needed here: MU is essential for understanding how the framework removes the influence of unlearnable classes from the model. The eCIL-MU framework modifies data rather than the model to achieve non-destructive unlearning.
  - Quick check question: How does the eCIL-MU framework differ from traditional MU methods in terms of model modification?

- Concept: Vector Embeddings and Vector Databases
  - Why needed here: Vector embeddings and vector databases are the core technologies enabling the framework's data modification approach. They allow for efficient storage, retrieval, and comparison of data representations.
  - Quick check question: What are the advantages of using vector embeddings and databases for CIL and MU compared to traditional model-based approaches?

## Architecture Onboarding

- Component map:
  - Embedding Model (Me) -> DB-CIL -> CIL-MU Model
  - DB-CIL -> Vector Filter -> Output Strategies
  - DB-CIL -> DB-MU (during MU task)

- Critical path:
  1. CIL task: Embed input data -> Store vectors in DB-CIL -> Train CIL-MU model.
  2. MU task: Identify unlearnable class vectors in DB-CIL -> Transfer to DB-MU -> Update CIL-MU model.
  3. Inference: Embed input data -> Filter using DB-MU -> Apply output strategy -> Predict class.

- Design tradeoffs:
  - Accuracy vs. Speed: The choice of output strategy (e.g., shift-to-nearest-class) impacts both accuracy and inference speed.
  - Storage vs. Performance: The size of DB-CIL and DB-MU affects storage requirements and retrieval performance.
  - Complexity vs. Flexibility: The framework's complexity allows for flexible integration of CIL and MU but may increase development and maintenance overhead.

- Failure signatures:
  - Low accuracy on remaining classes: Indicates issues with the embedding model, KNN matching, or output strategy.
  - High accuracy on unlearnable classes: Suggests problems with the vector filter or DB-MU management.
  - Slow inference: May be caused by inefficient vector filtering or large vector database sizes.

- First 3 experiments:
  1. Test the accuracy of KNN matching for identifying unlearnable class vectors in DB-CIL.
  2. Evaluate the performance of different output strategies (e.g., uniform random vs. shift-to-nearest-class) on inference accuracy.
  3. Measure the acceleration achieved by overlapping CIL and MU tasks compared to retraining from scratch.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out any open questions.

## Limitations
- Dependence on high-quality vector embeddings for effective unlearning.
- Accuracy of the "shift-to-nearest-class" strategy depends on proper threshold calibration for vector filtering.
- Acceleration gains assume vector database operations are significantly faster than retraining.

## Confidence
- **Mechanism 1**: Medium - Well-supported by evidence but effectiveness depends on embedding model quality.
- **Mechanism 2**: Medium - Supported by evidence but threshold sensitivity needs further investigation.
- **Mechanism 3**: Medium - Based on experimental results but overhead analysis is lacking.

## Next Checks
1. **Embedding Quality Assessment**: Evaluate the discriminative power of the embedding model by measuring the accuracy of KNN matching on a validation set with known class labels.
2. **Threshold Sensitivity Analysis**: Systematically vary the threshold for vector filtering and assess its impact on the accuracy of the "shift-to-nearest-class" strategy and overall unlearning effectiveness.
3. **Overhead Characterization**: Measure the time required for vector database operations (embedding, storage, retrieval, and transfer) and compare it to the time required for retraining from scratch to validate the claimed acceleration gains.