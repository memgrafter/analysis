---
ver: rpa2
title: The Mercurial Top-Level Ontology of Large Language Models
arxiv_id: '2405.01581'
source_url: https://arxiv.org/abs/2405.01581
tags:
- ontology
- chatgpt
- ontological
- entities
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper systematically analyzes the implicit ontological commitments
  in responses generated by large language models (LLMs), focusing on ChatGPT 3.5
  as a case study. The authors investigate how LLMs, despite having no explicit ontology,
  exhibit implicit ontological categorizations reflected in their generated texts.
---

# The Mercurial Top-Level Ontology of Large Language Models

## Quick Facts
- arXiv ID: 2405.01581
- Source URL: https://arxiv.org/abs/2405.01581
- Authors: Nele KÃ¶hler; Fabian Neuhaus
- Reference count: 29
- The paper systematically analyzes the implicit ontological commitments in responses generated by large language models (LLMs), focusing on ChatGPT 3.5 as a case study.

## Executive Summary
This paper presents a systematic analysis of the implicit ontological commitments embedded in LLM-generated texts, using ChatGPT 3.5 as a case study. The authors develop a methodology to extract and analyze top-level ontological categories from LLM responses, despite the lack of explicit ontology representation in the model. They demonstrate that LLMs exhibit implicit ontological categorizations through systematic patterns in their generated responses, revealing a taxonomy that shares similarities with existing top-level ontologies while facing unique challenges due to the mercurial nature of LLM outputs.

## Method Summary
The researchers used indirect prompting to elicit ontological categories from ChatGPT 3.5, collecting responses across multiple sessions and carefully designed questions. They systematically analyzed these responses to identify consistent ontological commitments, employing cross-validation with various prompts to ensure robustness. The methodology involved constructing a taxonomy hierarchy based on identified patterns and testing for inconsistencies and ontological overload. The resulting ontology is available as an OWL file at https://w3id.org/gptto/v1.0.0/.

## Key Results
- GPT's top-level ontology shares similarities with existing top-level ontologies like BFO, DOLCE, and UFO but faces unique challenges
- The study identifies a stable core of ontological commitments in ChatGPT despite its stochastic nature
- Ontological overload, ambiguity, and inconsistency emerge as fundamental challenges due to the LLM's contextual usage of terms rather than disambiguation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs produce text that reveals implicit ontological commitments through systematic patterns in their generated responses.
- Mechanism: The LLM processes millions of documents containing ontological distinctions, which are distilled into probabilistic patterns. When prompted, it generates text that reflects these patterns, revealing underlying ontological categories even without explicit ontology representation.
- Core assumption: The training corpus contains sufficient examples of ontological distinctions that the LLM can learn to reproduce them stochastically.
- Evidence anchors:
  - [abstract] "We investigate how LLMs, despite having no explicit ontology, exhibit implicit ontological categorizations that are reflected in the texts they generate."
  - [section 3] "LLMs are like stochastic parrots, which produce without any comprehension texts that are 'not grounded in communicative intent, any model of the world, or any model of the reader's state of mind'"
  - [corpus] Weak evidence - The corpus shows related work on LLMs and ontologies but no direct evidence of LLMs capturing implicit ontological commitments through text generation patterns.
- Break condition: If the training corpus lacks diverse ontological distinctions, or if the LLM's stochastic generation becomes too random to reveal consistent patterns.

### Mechanism 2
- Claim: Systematic analysis of LLM responses can extract a stable top-level ontology despite inherent inconsistencies.
- Mechanism: By repeatedly prompting with carefully designed questions and cross-validating responses across multiple sessions, researchers can identify consistent ontological commitments that form the basis of a systematic ontology.
- Core assumption: Despite stochastic variability, there exists a stable core of ontological commitments that appear consistently across different prompts and sessions.
- Evidence anchors:
  - [section 3] "We only included categories that were used consistently in different contexts. However, since ChatGPT generates text based on a stochastic process, it may produce texts that are inconsistent with the ontology we present in the next section."
  - [section 4.1] "There is a stable core of ontological commitments that are consistently made by ChatGPT (with few exceptions)."
  - [corpus] Moderate evidence - The corpus includes related work on using LLMs for ontology development, suggesting the feasibility of extracting systematic knowledge from LLM outputs.
- Break condition: If inconsistencies become too frequent or systematic patterns cannot be identified across multiple sessions.

### Mechanism 3
- Claim: LLM-generated ontologies differ fundamentally from traditional ontologies due to their mercurial nature and lack of disambiguation.
- Mechanism: Traditional ontologies resolve ambiguities through clear definitions and disjointness axioms, while LLMs learn to use terms contextually without disambiguation, leading to ontological overload and inconsistency.
- Core assumption: The difference between disambiguation in traditional ontologies and contextual usage in LLMs creates fundamental incompatibilities in how categories are defined and related.
- Evidence anchors:
  - [section 4.2] "This mercurial use of language is a great benefit for the task of generating natural language texts, but it is an obstacle for using LLMs for the task of creating ontologies."
  - [section 5] "LLMs do not learn to disambiguate terms, but rather to use terms appropriately in a given context. As we discussed, this results in a kind of 'ontological overload'"
  - [corpus] Strong evidence - The corpus shows related work on using LLMs for ontology development but also highlights the challenges of inconsistency and lack of clear definitions.
- Break condition: If LLMs could be trained or prompted to consistently disambiguate terms, reducing ontological overload and inconsistency.

## Foundational Learning

- Concept: Ontological Commitment
  - Why needed here: Understanding ontological commitment is crucial for interpreting what LLM-generated texts reveal about the categories and entities they implicitly acknowledge.
  - Quick check question: What is the difference between a text being ontologically committed to a category versus explicitly defining that category?

- Concept: Stochastic Text Generation
  - Why needed here: LLMs generate text probabilistically, which affects how consistent and reliable their ontological commitments are across different prompts and sessions.
  - Quick check question: How does the temperature setting in an LLM affect the consistency of its generated responses?

- Concept: Disambiguation vs Contextual Usage
  - Why needed here: Understanding the difference between how traditional ontologies and LLMs handle polysemous terms is key to understanding why LLM-generated ontologies have inherent inconsistencies.
  - Quick check question: Why would a traditional ontology introduce separate terms for "hammer-as-object" and "hammer-as-process" while an LLM might use "hammer" contextually for both?

## Architecture Onboarding

- Component map:
  Prompt Engineering Layer -> Response Analysis Pipeline -> Cross-validation Engine -> Ontology Construction Module -> Quality Control System

- Critical path:
  1. Design initial prompts to reveal ontological categories
  2. Collect responses across multiple sessions
  3. Identify consistent patterns and categories
  4. Cross-validate with alternative prompts
  5. Construct OWL taxonomy from validated patterns
  6. Test for inconsistencies and ontological overload
  7. Refine prompts and repeat if necessary

- Design tradeoffs:
  - Depth vs Breadth: Deeper ontological analysis may reveal more inconsistencies, while broader analysis may miss nuanced distinctions
  - Prompt Specificity vs Generality: Specific prompts may elicit clearer ontological commitments but may also prime responses
  - Cross-validation Depth vs Efficiency: More extensive cross-validation increases reliability but also computational and time costs

- Failure signatures:
  - High inconsistency rate across similar prompts
  - Difficulty identifying stable core of ontological commitments
  - Excessive ontological overload (same entity classified in disjoint categories)
  - Inability to construct coherent taxonomy due to mercurial usage of terms

- First 3 experiments:
  1. Test ontological commitment extraction with simple entity classification prompts (e.g., "What do a pen and a bird have in common?")
  2. Cross-validate consistency by asking the same ontological question with minor prompt variations
  3. Test for ontological overload by asking whether the same entity can belong to seemingly disjoint categories in different contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop systematic methods to cross-validate LLM classifications with various prompts to ensure robust and coherent ontology development?
- Basis in paper: [explicit] The authors emphasize the need for cross-validation of LLM classifications with various prompts to ensure robust and coherent ontology development.
- Why unresolved: The paper highlights the mercurial nature of LLM-generated texts, including ontological overload, ambiguity, and inconsistency, but does not provide a detailed methodology for cross-validation.
- What evidence would resolve it: Development and testing of systematic cross-validation methods that can be applied to LLM-generated ontologies to ensure consistency and coherence.

### Open Question 2
- Question: To what extent can the ontological distinctions made by LLMs be considered an approximation of the common sense ontology that underpins everyday discourse?
- Basis in paper: [inferred] The authors suggest that the ontological distinctions made by LLMs are a distillation of the ontological distinctions made by the authors of the millions of texts that the LLMs are trained on, which may be considered as an approximation of the common sense ontology.
- Why unresolved: The paper does not provide a detailed analysis of the extent to which LLM-generated ontologies reflect common sense ontology, nor does it compare LLM ontologies with empirical studies of common sense ontology.
- What evidence would resolve it: Comparative studies between LLM-generated ontologies and empirical studies of common sense ontology, including surveys and experiments to validate the alignment of LLM ontologies with human common sense.

### Open Question 3
- Question: How can the integration of LLM outputs with existing top-level ontologies like BFO, DOLCE, or UFO be achieved to ensure compatibility and coherence?
- Basis in paper: [explicit] The authors discuss the challenges of integrating LLM-generated ontologies with existing top-level ontologies due to significant differences in ontological categorization and structure.
- Why unresolved: The paper does not provide a detailed strategy for aligning LLM-generated ontologies with established top-level ontologies, nor does it propose a framework for ensuring compatibility.
- What evidence would resolve it: Development and testing of alignment frameworks and strategies that can be used to integrate LLM-generated ontologies with existing top-level ontologies, ensuring coherence and compatibility.

## Limitations

- The mercurial nature of LLM responses leads to ontological overload and inconsistency, preventing the same level of precision found in traditional ontologies
- The lack of clear disambiguation means categories may overlap in ways that violate logical consistency
- The stochastic nature of text generation means carefully designed prompts may produce varying results across sessions

## Confidence

- **High confidence**: The observation that LLMs exhibit implicit ontological commitments through their generated texts, and the basic methodology of cross-validating responses to identify consistent patterns
- **Medium confidence**: The specific taxonomy presented and the claim that GPT's top-level ontology shares similarities with existing top-level ontologies, as this requires more extensive validation across different prompt variations
- **Low confidence**: The completeness of the identified ontology and the assertion that no further systematic ontological commitments can be extracted, given the mercurial nature of LLM responses

## Next Checks

1. **Cross-domain consistency testing**: Prompt ChatGPT with ontological questions spanning multiple domains (physical objects, abstract concepts, social constructs) to verify if the same ontological commitments hold across different contexts and whether new categories emerge.

2. **Temporal stability assessment**: Repeat the ontological commitment extraction process across multiple sessions separated by significant time intervals to determine if the LLM's implicit ontology changes or evolves, and quantify the rate of such changes.

3. **Alternative LLM comparison**: Apply the same methodology to other LLM models (GPT-4, Claude, LLaMA) to determine whether the identified ontological commitments are model-specific or represent broader patterns across LLM architectures.