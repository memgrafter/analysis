---
ver: rpa2
title: Multi-Turn Interactions for Text-to-SQL with Large Language Models
arxiv_id: '2408.11062'
source_url: https://arxiv.org/abs/2408.11062
tags:
- table
- column
- text-to-sql
- language
- columns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of efficiently generating SQL
  queries from natural language questions using large language models (LLMs). The
  proposed framework, Interactive-T2S, introduces a multi-turn interaction approach
  where the LLM acts as an agent interacting with a database environment through four
  general tools: SearchColumn, SearchValue, FindShortestPath, and ExecuteSQL.'
---

# Multi-Turn Interactions for Text-to-SQL with Large Language Models

## Quick Facts
- arXiv ID: 2408.11062
- Source URL: https://arxiv.org/abs/2408.11062
- Authors: Guanming Xiong; Junwei Bao; Hongfei Jiang; Yang Song; Wen Zhao
- Reference count: 40
- One-line primary result: Achieves state-of-the-art performance on BIRD leaderboard using only two exemplars and multi-turn interaction with four general tools

## Executive Summary
This paper addresses the challenge of efficiently generating SQL queries from natural language questions using large language models (LLMs). The proposed Interactive-T2S framework introduces a multi-turn interaction approach where the LLM acts as an agent interacting with a database environment through four general tools: SearchColumn, SearchValue, FindShortestPath, and ExecuteSQL. This approach demonstrates exceptional performance on the Spider and BIRD datasets, achieving state-of-the-art results on the BIRD leaderboard without oracle knowledge, using only two exemplars for few-shot learning. The method addresses scalability issues with wide tables and provides a step-by-step, interpretable SQL generation process.

## Method Summary
The Interactive-T2S framework employs a LLM as an agent that interacts with a database environment through a multi-turn process. The agent uses four general tools: SearchColumn for locating relevant schema elements, SearchValue for finding cell values, FindShortestPath for identifying table join paths, and ExecuteSQL for executing queries. The LLM follows a thought-action-observation paradigm, making decisions at each turn based on previous observations. The framework uses only two exemplars for few-shot learning, with tool descriptions and interaction patterns providing structured scaffolding. The process continues for a maximum of 12 interaction rounds or until the agent signals completion, with the final SQL being the result of the last ExecuteSQL call.

## Key Results
- Achieves state-of-the-art performance on BIRD leaderboard without oracle knowledge
- Reduces token consumption by 36-22% compared to DIN-SQL (4.6k-4.7k tokens per case)
- Demonstrates competitive performance across Spider-DK, Syn, and Realistic datasets with only two exemplars

## Why This Works (Mechanism)

### Mechanism 1
Multi-turn interaction with specialized tools improves SQL accuracy by reducing reliance on model's implicit reasoning over large schema. Instead of feeding entire schema to LLM, tools (SearchColumn, SearchValue, FindShortestPath, ExecuteSQL) allow targeted, step-by-step exploration and validation. Core assumption: LLM can reliably decompose questions into actionable steps and use tool outputs effectively. Evidence: Interactive-T2S shows superior performance while using 36-22% fewer tokens than DIN-SQL. Break condition: If LLM cannot reliably decompose questions or misuses tool outputs, the step-by-step approach fails.

### Mechanism 2
Two exemplars are sufficient to guide LLM reasoning because tools and interaction logic provide structured scaffolding. Well-defined tool descriptions and a consistent interaction pattern (thought-action-observation) reduce the need for extensive demonstration examples. Core assumption: The interaction pattern and tool descriptions are sufficiently clear and general to cover diverse query types. Evidence: Interactive-T2S achieves SOTA results on BIRD with only two exemplars. Break condition: If queries require complex, unseen reasoning patterns not covered by the two exemplars and tool descriptions.

### Mechanism 3
Interactive-T2S generalizes better to variant datasets because it doesn't rely on exact schema-token matching or training data similarity. Dynamic tool-based exploration adapts to unseen schemas and paraphrases, unlike static exemplar selection or fixed few-shot methods. Core assumption: Tool outputs provide sufficient semantic grounding to handle schema variations and question paraphrases. Evidence: Interactive-T2S shows competitive performance across Spider-DK, Syn, and Realistic datasets. Break condition: If tool outputs are insufficient to disambiguate semantically similar schema elements across domains.

## Foundational Learning

- **Concept: Schema linking and its scalability challenges**
  - Why needed here: Understanding why traditional schema linking fails with wide tables is key to appreciating the tool-based approach.
  - Quick check question: What are the token cost and accuracy implications of feeding entire DB schema vs. using targeted tools?

- **Concept: In-context learning and few-shot prompting**
  - Why needed here: The method relies on minimal exemplars; understanding how LLMs generalize from few examples is crucial.
  - Quick check question: How do tool descriptions and interaction patterns compensate for limited exemplar diversity?

- **Concept: Graph-based reasoning for table joins**
  - Why needed here: FindShortestPath tool leverages schema as a graph; understanding this helps debug join-related errors.
  - Quick check question: How does modeling the schema as an undirected graph simplify multi-table joins compared to LLM-only reasoning?

## Architecture Onboarding

- **Component map**: Prompt constructor (instructions, exemplars, schema, question) -> LLM agent (thought-action loop) -> Tool suite (SearchColumn, SearchValue, FindShortestPath, ExecuteSQL) -> Database environment (SQLite interface) -> Evaluation pipeline (EM, EX metrics)

- **Critical path**: 1. Construct prompt with instructions, exemplars, schema, and question 2. LLM generates thought and action 3. Tool executes and returns observation 4. Repeat until Done or max rounds 5. Final SQL is result of last ExecuteSQL

- **Design tradeoffs**: Fewer exemplars (2) vs. more comprehensive demonstrations vs. Targeted tool-based exploration vs. full schema feeding (efficiency vs. potential missed context) vs. Step-by-step interaction vs. single-pass generation (interpretability vs. speed)

- **Failure signatures**: LLM fails to decompose question (stuck in thought loop) vs. Tool outputs are misinterpreted (wrong columns selected) vs. Execution errors not caught (invalid SQL) vs. Max rounds reached without Done (incomplete reasoning)

- **First 3 experiments**: 1. Run with a simple Spider-Dev query; verify tool outputs and SQL generation 2. Test with a BIRD query requiring cell value search; check SearchValue functionality 3. Evaluate on a Spider-Syn variant; observe generalization to paraphrased schema tokens

## Open Questions the Paper Calls Out

### Open Question 1
How can the Interactive-T2S framework be optimized to handle extremely large databases with thousands of tables and millions of rows while maintaining real-time response times? The paper discusses scalability challenges with wide tables but does not address performance with extremely large databases containing thousands of tables. This remains unresolved because the current implementation focuses on efficiency improvements for wide tables but does not explore the limits of database size or response time optimization for enterprise-scale databases.

### Open Question 2
Can the multi-turn interaction approach be adapted to work effectively with open-source LLMs that have smaller context windows and less reasoning capability than GPT-4? The paper explicitly states that there remains a considerable performance gap between open-source LLMs and closed-source LLMs like GPT-4, and only reports results with two open-source models. This remains unresolved because the framework relies heavily on the reasoning capabilities of large closed-source models, and the paper does not explore architectural modifications that could make it more accessible to open-source alternatives.

### Open Question 3
How can the framework be extended to support multiple SQL dialects and database systems beyond SQLite while maintaining the same level of accuracy and efficiency? While the paper demonstrates effectiveness on SQLite-based datasets, it does not address compatibility with other database systems like PostgreSQL, MySQL, or NoSQL databases, nor does it discuss handling different SQL dialects. This remains unresolved because the tool implementations and prompt design are tailored to SQLite's specific syntax and capabilities, and there is no discussion of abstraction layers that could handle dialect variations.

## Limitations
- Token cost remains substantial at 4.6-4.7k per case despite efficiency gains, raising scalability concerns
- Framework success critically depends on tool accuracy, with limited independent validation of tool reliability
- Generalization robustness with minimal exemplars is not fully validated across diverse, complex query types

## Confidence
- **High Confidence**: Superior performance on BIRD leaderboard and competitive results on Spider variants are well-supported by experimental results
- **Medium Confidence**: Two-exemplar sufficiency claim is supported but lacks extensive ablation studies
- **Low Confidence**: Performance on truly unseen, complex schemas and long-term reliability under diverse real-world conditions remain uncertain

## Next Checks
1. Conduct independent evaluation of each tool's accuracy (SearchColumn, FindShortestPath, etc.) on diverse schemas and queries to establish baseline reliability
2. Test framework's token consumption and response time when processing batches of 100+ queries across different complexity levels to assess real-world scalability
3. Perform ablation studies with varying numbers of exemplars (1, 2, 5, 10) to determine minimum exemplar requirement for maintaining performance