---
ver: rpa2
title: The Unreasonable Effectiveness Of Early Discarding After One Epoch In Neural
  Network Hyperparameter Optimization
arxiv_id: '2404.04111'
source_url: https://arxiv.org/abs/2404.04111
tags:
- learning
- performance
- early
- training
- discarding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the effectiveness of early discarding strategies
  in neural network hyperparameter optimization. It compares several advanced techniques
  against the simple baseline of training networks for a constant number of epochs.
---

# The Unreasonable Effectiveness Of Early Discarding After One Epoch In Neural Network Hyperparameter Optimization

## Quick Facts
- arXiv ID: 2404.04111
- Source URL: https://arxiv.org/abs/2404.04111
- Reference count: 34
- The 1-Epoch strategy often matches or exceeds sophisticated early discarding methods while being simpler to implement.

## Executive Summary
This paper investigates the effectiveness of early discarding strategies in neural network hyperparameter optimization. The authors compare several advanced techniques against the simple baseline of training networks for a constant number of epochs. Surprisingly, the i-Epoch approach (constant epochs) often dominates or matches the performance of more sophisticated methods while being simpler to implement. The study shows that one epoch can often be enough to identify promising models, as good models tend to stand out early in training. The authors recommend incorporating the i-Epoch strategy into future benchmark studies and emphasize the importance of multi-objective evaluation in early discarding research.

## Method Summary
The study evaluates hyperparameter optimization with early discarding strategies on fully connected deep neural networks using 4 regression datasets from HPOBench and 13 classification datasets with various characteristics. The methods tested include i-Epoch (constant epochs), SHA, LCE, and PFN. The evaluation uses random search with 200 iterations, with the top-3 models selected and trained to completion. Performance is measured using 1-R2 score and hypervolume indicator to assess trade-offs between training epochs and predictive performance. Learning curves are generated by training all hyperparameter configurations for 100 epochs to enable post-hoc analysis of early discarding decisions.

## Key Results
- The i-Epoch strategy achieved the highest average rank (1.125) across all benchmarks
- One epoch of training was often sufficient to identify promising models without sacrificing final predictive performance
- Advanced early discarding techniques (SHA, LCE, PFN) offered minimal added value compared to simple constant epoch strategies

## Why This Works (Mechanism)

### Mechanism 1
The 1-Epoch strategy works because high-quality models tend to separate from low-quality ones early in training, often after just one epoch. During the first epoch, the relative ordering of models based on performance metrics becomes predictive of their final ranking, allowing early discarding of poor models without sacrificing final predictive performance. This relies on the assumption that learning curves of different hyperparameter configurations do not cross significantly after the first epoch.

### Mechanism 2
Advanced early discarding techniques offer minimal advantage over constant epoch strategies because they either react too conservatively or make overconfident extrapolations. These techniques attempt to adaptively allocate resources based on early performance trends, but in practice, the added complexity does not translate into better Pareto frontiers compared to fixed epoch strategies. The uncertainty models used in LCE and PFN do not accurately capture the true variability in learning curves.

### Mechanism 3
The multi-objective nature of HPO with early discarding is better served by simple strategies like i-Epoch, which span a wider range of trade-offs between training epochs and predictive performance. By testing a range of fixed epoch values (1 to 100), i-Epoch naturally explores diverse points in the epochs-performance space, often covering or exceeding the Pareto frontiers of more complex methods.

## Foundational Learning

- **Hyperparameter Optimization (HPO)**
  - Why needed here: The paper's core contribution is about optimizing the selection of hyperparameters for neural networks more efficiently.
  - Quick check question: What is the difference between a hyperparameter and a model parameter in the context of neural networks?

- **Learning Curves**
  - Why needed here: Early discarding techniques rely on observing the performance of models over training epochs to make decisions.
  - Quick check question: How does the shape of a learning curve (e.g., convex vs. concave) affect the decision to stop training early?

- **Pareto Optimality and Pareto Front**
  - Why needed here: The evaluation of early discarding methods is framed as a multi-objective optimization problem where trade-offs between training epochs and predictive performance are assessed.
  - Quick check question: In a two-objective optimization problem, what does it mean for a solution to be Pareto optimal?

## Architecture Onboarding

- **Component map**: Random Search -> Early Discarding Module -> Model Evaluator -> Final Selection Module -> Top-3 Models Training -> Performance Evaluation
- **Critical path**: Propose hyperparameter configuration → Train for N epochs → Evaluate performance → Decide to continue or discard → Select top models → Train to completion
- **Design tradeoffs**: Simplicity vs. adaptivity in early discarding methods; computational cost vs. predictive performance; generalization across datasets vs. optimization for specific tasks
- **Failure signatures**: Overfitting to early noisy signals leading to discarding good models; underfitting leading to unnecessary training; failure to adapt to different learning curve shapes; poor performance on datasets where learning curves cross frequently
- **First 3 experiments**:
  1. Implement and compare the 1-Epoch strategy against a baseline of training all models to completion on a small dataset to verify the claim of unreasonable effectiveness.
  2. Vary the number of epochs in the i-Epoch strategy (e.g., 1, 5, 10, 50, 100) on multiple datasets to map out the Pareto frontier and confirm it spans diverse trade-offs.
  3. Implement and test SHA and LCE methods with different aggressiveness parameters to compare their Pareto frontiers against i-Epoch and identify any conditions where they offer added value.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of 1-Epoch vary when using different fidelity units (e.g., wall-clock time vs. epochs) in early discarding strategies? The study primarily focused on epochs as fidelity units, leaving the performance comparison with other units unexplored.

### Open Question 2
What is the impact of using batch-level performance data instead of epoch-level data in early discarding strategies? The study only considered epoch-level data, not exploring the potential benefits of batch-level data.

### Open Question 3
How does the performance of 1-Epoch change when applied to different types of neural network architectures (e.g., convolutional, recurrent) beyond fully connected networks? The effectiveness of 1-Epoch was only evaluated for fully connected networks.

## Limitations
- Findings are based on specific neural network architectures (fully connected deep networks) and may not generalize to more complex architectures like convolutional or transformer networks.
- The analysis focuses on regression and classification tasks but doesn't explore reinforcement learning or other problem domains.
- The random search methodology with 200 iterations may not capture the full variability of HPO scenarios, particularly for larger search spaces.

## Confidence
- **High Confidence**: The comparative performance of i-Epoch against advanced methods (SHA, LCE, PFN) is well-supported by the experimental results across multiple datasets.
- **Medium Confidence**: The claim that "one epoch is often enough" to identify promising models is supported but may depend on specific problem characteristics and network architectures.
- **Medium Confidence**: The recommendation to incorporate i-Epoch into benchmark studies is reasonable but should be validated across a broader range of HPO scenarios and architectures.

## Next Checks
1. **Architecture Generalization Test**: Replicate the study using convolutional neural networks on image datasets to verify if the 1-Epoch effectiveness holds for more complex architectures.
2. **Cross-Domain Validation**: Test the early discarding strategies on a reinforcement learning task to assess whether the findings generalize beyond supervised learning.
3. **Search Space Sensitivity Analysis**: Vary the size and complexity of the hyperparameter search space to determine if the relative effectiveness of i-Epoch changes with search dimensionality.