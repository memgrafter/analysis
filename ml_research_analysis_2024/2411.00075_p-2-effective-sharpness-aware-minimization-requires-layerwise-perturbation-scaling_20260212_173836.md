---
ver: rpa2
title: "\u03BCP$^2$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation\
  \ Scaling"
arxiv_id: '2411.00075'
source_url: https://arxiv.org/abs/2411.00075
tags:
- perturbation
- scaling
- learning
- layer
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the behavior of Sharpness Aware Minimization
  (SAM) in wide neural networks using the Tensor Programs framework. The authors show
  that standard SAM with global perturbation scaling effectively reduces to applying
  SAM only in the last layer in the infinite-width limit, even with optimal hyperparameters.
---

# μP$^2$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling

## Quick Facts
- arXiv ID: 2411.00075
- Source URL: https://arxiv.org/abs/2411.00075
- Authors: Moritz Haas; Jin Xu; Volkan Cevher; Leena Chennuru Vankadara
- Reference count: 40
- Key outcome: Standard SAM with global perturbation scaling effectively reduces to perturbing only the last layer in wide networks; µP2 solves this with layerwise perturbation scaling.

## Executive Summary
This paper analyzes the behavior of Sharpness Aware Minimization (SAM) in wide neural networks using the Tensor Programs framework. The authors show that standard SAM with global perturbation scaling effectively reduces to applying SAM only in the last layer in the infinite-width limit, even with optimal hyperparameters. To address this limitation, they introduce Maximal Update and Perturbation Parameterization (µP2), which ensures width-independent feature learning and effective perturbations in all layers through layerwise perturbation scaling. Experiments with MLPs, ResNets, and Vision Transformers demonstrate that µP2 achieves hyperparameter transfer of both learning rate and perturbation radius across model scales, and consistently improves generalization performance over standard parameterizations.

## Method Summary
The paper introduces µP2, a new parameterization that combines maximal update parameterization with layerwise perturbation scaling. For each layer, perturbations are scaled to match the spectral norm of updates, ensuring effective perturbations across all layers rather than just the output layer. The method implements SAM with these layerwise-scaled perturbations while maintaining the same optimization framework. The authors validate their approach on MLPs, ResNets, and Vision Transformers trained on CIFAR-10 and ImageNet1K, comparing performance against standard parameterizations.

## Key Results
- Standard SAM with global perturbation scaling collapses to perturbing only the last layer in the infinite-width limit
- µP2 achieves effective perturbations in all layers through layerwise perturbation scaling
- µP2 enables hyperparameter transfer of both learning rate and perturbation radius across model scales
- µP2 consistently improves generalization performance over standard parameterizations

## Why This Works (Mechanism)

### Mechanism 1
Standard SAM with global perturbation scaling effectively reduces to perturbing only the last layer in the infinite-width limit. In the Tensor Programs framework, the spectral norm of weight perturbations scales with layer width. For global scaling, perturbations in hidden layers vanish faster than updates, so their effect on the loss vanishes. Core assumption: Stability requires perturbations to be bounded; in µP, last-layer perturbations dominate the gradient norm.

### Mechanism 2
Layerwise perturbation scaling in µP2 ensures effective perturbations in every layer, enabling joint hyperparameter transfer of learning rate and perturbation radius. Matching perturbation and update spectral norms across layers maintains balanced gradient contributions, allowing the optimizer to jointly tune η and ρ without width-dependent shifts. Core assumption: Gradient-based perturbations are correlated with incoming activations, so they scale similarly to updates.

### Mechanism 3
Effective perturbations are necessary for SAM's generalization benefits; propagating perturbations from early layers without effective per-layer scaling does not inherit these benefits. SAM's inductive bias toward flat minima requires low spectral norm of the Hessian in each layer; effective perturbations directly influence this norm, whereas propagated perturbations do not. Core assumption: Generalization improvement requires explicit perturbation in each layer, not just propagation effects.

## Foundational Learning

- Concept: Tensor Programs and spectral scaling in infinite-width neural networks
  - Why needed here: The paper uses the Tensor Programs framework to rigorously characterize perturbation scaling; understanding spectral norms and scaling is essential to follow the proofs
  - Quick check question: In µP, how do the spectral norms of weights and updates scale with width for input, hidden, and output layers?

- Concept: Sharpness-aware minimization and its relationship to generalization
  - Why needed here: The paper motivates µP2 by showing standard SAM fails to perturb all layers effectively; understanding SAM's goal of flat minima is key to grasping the improvement
  - Quick check question: What is the role of the perturbation radius ρ in SAM, and why does its optimal value depend on width?

- Concept: Feature learning in infinite-width networks
  - Why needed here: µP2 is defined as the unique parameterization achieving feature learning and effective perturbations; knowing the conditions for feature learning clarifies why µP2 transfers hyperparameters
  - Quick check question: In the Tensor Programs framework, what condition on the maximal feature update scale r indicates that feature learning occurs in a layer?

## Architecture Onboarding

- Component map: Input -> MLP/ResNet/ViT layers -> Output layer -> Loss function -> Perturbation computation -> Weight update

- Critical path:
  1. Initialize weights according to parameterization (µP or SP)
  2. For each training step:
     a. Compute forward pass to get activations
     b. Compute gradients ∇W^l L(f(ξ; W+ε), y) with perturbed weights
     c. Normalize gradients to form perturbation ε^t
     d. Update weights with scaled gradients
  3. Track width-dependent scalings of updates and perturbations

- Design tradeoffs:
  - Global vs layerwise perturbation scaling: Global is simpler but ineffective in hidden layers; layerwise is more complex but achieves effective perturbations everywhere
  - Spectral norm vs Frobenius norm for gradient normalization: Spectral norm preserves correct scaling for perturbations; Frobenius norm may distort it
  - Weight initialization variance: Larger last-layer variance in SP vs zero in µP affects stability and feature learning

- Failure signatures:
  - Vanishing perturbations in hidden layers (spectral norm of ε^l much smaller than ||W^l||*)
  - Instability (exploding output function or activations)
  - Suboptimal generalization (no improvement over SGD)
  - Hyperparameter non-transfer (optimal η or ρ shifts with width)

- First 3 experiments:
  1. Train a 3-layer MLP in µP with global perturbation scaling ρ·n^{-1/2}; verify that only last-layer perturbations are effective
  2. Train the same MLP with µP2 layerwise scaling; verify effective perturbations in all layers and improved generalization
  3. Compare test accuracy vs width for both settings; check if optimal (η, ρ) transfers in µP2 but not in µP with global scaling

## Open Questions the Paper Calls Out

### Open Question 1
How do scaling laws change when considering joint limits of infinite width, depth, and training time with SAM? Basis in paper: The paper mentions that Tensor Program theory assumes constant batch size and training time, and does not make statements about generalization. It also states that developing theory to inform Pareto optimal trade-offs in a principled manner constitutes an important direction for future work. Why unresolved: Current Tensor Program theory does not account for the interplay between width, depth, and training time scaling. The paper acknowledges this limitation but does not provide theoretical results for this more complex scenario.

### Open Question 2
What is the exact mechanism by which SAM-ON achieves improved generalization through normalization layer perturbations? Basis in paper: The paper notes that applying SAM on only normalization layers (SAM-ON) often improves generalization further despite increasing sharpness, and that only SAM-ON and elementwise ASAM sufficiently perturb normalization layers in SP. However, the exact mechanism remains unclear. Why unresolved: While the paper observes that SAM-ON perturbs normalization layers effectively, it does not provide a theoretical explanation for why this specific perturbation pattern leads to improved generalization.

### Open Question 3
Under what conditions does standard parameterization (SP) with naive perturbation scaling achieve hyperparameter transfer after long training? Basis in paper: The paper observes that MLPs and ResNets in SP can sometimes display hyperparameter transfer in η and ρ after multi-epoch training to convergence, which contradicts the infinite-width theory from Yang and Hu (2021) that predicts output blowup under large learning rates. Why unresolved: The paper notes this transfer but does not provide a theoretical explanation for when and why SP can achieve hyperparameter transfer, especially since infinite-width theory predicts instability.

## Limitations

- The analysis assumes gradient-based perturbations are the only source of effective perturbations, which may not hold in all architectures
- The spectral scaling condition requires careful implementation and may not transfer seamlessly across all architectures
- Empirical validation is limited to specific architectures (MLPs, ResNets, ViTs) and datasets (CIFAR-10, ImageNet1K)

## Confidence

- Claim: Standard SAM collapses to last-layer perturbations in infinite-width limit -> High
- Claim: µP2 achieves effective perturbations in all layers -> Medium
- Claim: Propagated perturbations cannot inherit SAM's inductive bias -> Low

## Next Checks

1. Implement µP2 in architectures beyond MLPs, ResNets, and ViTs (e.g., recurrent networks or graph neural networks) to assess the generality of the spectral scaling condition

2. Investigate the role of non-gradient-based perturbations (e.g., random noise or adversarial attacks) in the context of SAM and µP2, to understand their potential impact on generalization

3. Conduct a systematic ablation study of the spectral scaling condition in µP2, varying the initialization, activation functions, and loss functions, to identify the key factors driving the observed improvements in generalization