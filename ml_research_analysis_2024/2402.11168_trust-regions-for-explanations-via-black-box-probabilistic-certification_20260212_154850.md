---
ver: rpa2
title: Trust Regions for Explanations via Black-Box Probabilistic Certification
arxiv_id: '2402.11168'
source_url: https://arxiv.org/abs/2402.11168
tags:
- unifi
- adapti
- unif
- region
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the problem of finding trust regions for
  local explanations of black-box ML models, where a trust region is the largest hypercube
  around an explained example within which the explanation maintains a specified fidelity.
  The authors propose three probabilistic certification strategies (uniform, uniform
  incremental, and adaptive incremental sampling) and prove finite-sample bounds on
  certification accuracy.
---

# Trust Regions for Explanations via Black-Box Probabilistic Certification

## Quick Facts
- arXiv ID: 2402.11168
- Source URL: https://arxiv.org/abs/2402.11168
- Reference count: 40
- One-line primary result: Three probabilistic certification strategies for finding trust regions of black-box ML explanations, proven to be more efficient than black-box optimization baselines while converging to similar trust region sizes.

## Executive Summary
This paper addresses the problem of finding trust regions for local explanations of black-box ML models. A trust region is defined as the largest hypercube around an explained example within which the explanation maintains a specified fidelity. The authors propose three probabilistic certification strategies (uniform, uniform incremental, and adaptive incremental sampling) and prove finite-sample bounds on certification accuracy. Experiments on synthetic and real data show the methods are significantly more efficient than black-box optimization baselines while converging to similar trust region sizes. The adaptive strategy is fastest in high dimensions, and the framework enables explanation comparison, reuse, and query savings.

## Method Summary
The method formalizes trust region estimation as a certification problem, where the goal is to find the largest hypercube around an example such that the explanation maintains a specified fidelity threshold. Three probabilistic certification strategies are proposed: uniform sampling, uniform incremental sampling with Gaussian perturbations, and adaptive incremental sampling focusing on promising prototypes. The methods use binary search to efficiently find the largest certified region, with theoretical guarantees on certification accuracy based on finite-sample bounds and extreme value theory. The framework is designed to work with any black-box model and explanation method, requiring only query access to the model and a quality metric for the explanation.

## Key Results
- The three certification strategies (uniform, uniform incremental, adaptive incremental) are significantly more efficient than black-box optimization baselines (ZO+) while converging to similar trust region sizes.
- The adaptive incremental sampling strategy is fastest in high dimensions, especially when the query budget is limited.
- Trust region half-widths vary across datasets and explanation methods, with some models having very small certified regions (e.g., HELOC) and others having larger regions (e.g., CIFAR10).
- The proposed methods enable explanation comparison, reuse, and query savings, making them practical for real-world applications.

## Why This Works (Mechanism)

### Mechanism 1
Uniform sampling (unif) guarantees certification with high probability if the region is truly certified, using finite-sample bounds that decay exponentially with query budget. The method samples Q examples uniformly in the target region and returns True only if all sampled examples meet the fidelity threshold θ. The probability of incorrect certification (i.e., certifying a violating region) is bounded by exp(-Q * F_i(f_w* + ε)), where F_i is the cdf of fidelities in that region. Core assumption: F_i(f_w* + ε) > 0, meaning there is non-zero probability of sampling an example with fidelity close to the true minimum.

### Mechanism 2
Adaptive incremental sampling (adaptI) improves efficiency by focusing queries near the minimum fidelity region, especially in high dimensions. The method iteratively samples prototypes uniformly, then adaptively samples more examples around the most promising prototypes (those yielding lowest minimum fidelity), halving the number of prototypes each iteration while doubling samples per prototype. Core assumption: The fidelity landscape is such that good prototypes (near low-fidelity regions) can be identified early, making further sampling around them productive.

### Mechanism 3
The outer binary search loop in Algorithm 1 efficiently finds the largest certified hypercube by doubling/halving the search range based on certification outcomes. The algorithm maintains lower/upper bounds (lb, ub) on the certified half-width and queries Certify(lb, ub, ...) to check if the region between them is certified. If certified, it doubles ub; if not, it halves ub. This converges to the maximum certified width. Core assumption: The certification predicate is monotonic in the sense that if a region is certified, all smaller regions are also certified.

## Foundational Learning

- Concept: Extreme Value Theory (EVT) for deriving asymptotic bounds on minimum fidelity estimation.
  - Why needed here: EVT provides cdf-free asymptotic bounds on the probability that the estimated minimum fidelity is close to the true minimum, which is useful when the cdf F_i is unknown or hard to estimate.
  - Quick check question: What is the asymptotic form of the cdf of the minimum of Q i.i.d. samples as Q → ∞, and how does it depend on the second-smallest sample?

- Concept: Randomized smoothing and its relation to query-based certification.
  - Why needed here: The paper's certification methods are similar to randomized smoothing techniques used for robustness certification, but instead of smoothing the model, they certify explanations directly.
  - Quick check question: How does the certification guarantee in randomized smoothing (e.g., Cohen et al., 2019) differ from the probabilistic certification of explanations in this work?

- Concept: Zeroth-order optimization (ZO) and its adaptation for adversarial attacks.
  - Why needed here: The paper compares its methods to a ZO-based baseline (ZO+) adapted from adversarial attack literature, highlighting the efficiency gains of the proposed methods.
  - Quick check question: In ZO optimization, how does the choice of query distribution (e.g., Gaussian vs. uniform) affect the convergence rate and accuracy of gradient estimation?

## Architecture Onboarding

- Component map:
  Ecertify -> Algorithm 1: Binary search for largest certified hypercube -> Algorithm 2: Region certification strategies (unif, unifI, adaptI) -> Support: EVT-based bounds, kernel density estimation for cdf approximation

- Critical path:
  1. Initialize Ecertify with problem parameters
  2. For each iteration Z:
     a. Call Certify(lb, ub, Q, θ, f, x0, s) with current bounds
     b. Update lb/ub based on certification outcome
  3. Return largest certified half-width w

- Design tradeoffs:
  - Query budget Q vs. certification accuracy: Higher Q gives tighter bounds but increases runtime
  - Strategy choice (unif/unifI/adaptI): unif is simplest but may be inefficient in high dimensions; adaptI is most efficient in high dimensions but more complex
  - Proxy for f_w* in bound estimation: Using ˆf_w* gives optimistic bounds, θ gives conservative bounds

- Failure signatures:
  - Algorithm returns -1: Even the example x0 fails the fidelity threshold θ
  - Estimated half-width w is much smaller than expected: The black-box model's fidelity drops rapidly with perturbations, or the explanation is unstable
  - Bounds are very loose or do not converge: The fidelity distribution is highly peaked or has zero mass near the minimum fidelity

- First 3 experiments:
  1. Synthetic data (d=1 to 10000, hyperplane explanation, varying Q): Verify convergence to true half-width and efficiency gains over ZO+
  2. Real data (HELOC, CIFAR10, ImageNet): Compare half-widths and runtimes across datasets and explanation methods (LIME vs. SHAP)
  3. Ablation study on strategy choice: Measure half-widths and runtimes for unif/unifI/adaptI on datasets of varying dimensionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed methods be adapted to find trust regions for non-linear explanations like RISE or contrastive explanations?
- Basis in paper: [explicit] Section 8 discusses applicability to other explanation types, including RISE and contrastive explanations.
- Why unresolved: The paper only demonstrates certification for RISE in a supplementary experiment, not in the main results. The general framework for handling arbitrary explanation types is not fully developed or validated.
- What evidence would resolve it: A comprehensive experimental evaluation showing the proposed methods can certify trust regions for a variety of non-linear explanation methods (e.g., RISE, contrastive explanations, exemplar-based methods) across different datasets and models.

### Open Question 2
- Question: How sensitive are the trust region estimates to the choice of sampling strategy (uniform, unifI, adaptI) and the number of queries (Q)?
- Basis in paper: [explicit] Section 5 provides theoretical analysis of the sampling strategies, and Section 7 shows experimental results comparing the strategies' performance in terms of half-width estimates and running times for different values of Q.
- Why unresolved: While the paper provides some insights into the strategies' relative performance, a more systematic investigation of their sensitivity to Q and the impact on trust region accuracy is needed.
- What evidence would resolve it: A detailed sensitivity analysis showing how the trust region estimates vary with Q for each strategy, and how this variation affects the probability of correct certification.

### Open Question 3
- Question: Can the proposed methods be extended to find trust regions in lower-dimensional manifolds instead of hypercubes in the input space?
- Basis in paper: [inferred] Section 8 mentions the possibility of adapting the methods to work on lower-dimensional manifolds, but does not provide concrete details or experimental results.
- Why unresolved: The paper does not explore this direction, and the technical challenges of extending the methods to manifolds are not addressed.
- What evidence would resolve it: A theoretical framework and experimental results demonstrating how the proposed methods can be adapted to find trust regions on manifolds, and a comparison of the resulting trust regions to those found in the input space.

## Limitations

- The framework assumes monotonicity of the fidelity metric with respect to perturbation size, which may not hold for all black-box models and explanation methods.
- The theoretical bounds rely on assumptions about the fidelity distribution (e.g., non-zero mass near the minimum) that may not be satisfied in practice.
- The paper doesn't address the computational cost of computing explanations for each query, which could be substantial for complex models like deep neural networks.

## Confidence

- High confidence: The efficiency gains of adaptive sampling over uniform sampling in high dimensions (supported by empirical results across multiple datasets).
- Medium confidence: The theoretical finite-sample bounds (depends on assumptions about fidelity distributions being reasonable).
- Low confidence: The binary search procedure always finding the true maximum certified width (relies on monotonicity assumption that may be violated).

## Next Checks

1. Test the certification methods on black-box models where fidelity is known to be non-monotonic with perturbation size to evaluate the binary search's robustness.
2. Measure the actual runtime of explanation computation (e.g., LIME, SHAP) for each query to assess the practical efficiency gains reported.
3. Evaluate the methods on a wider range of explanation types (beyond LIME and SHAP) to assess generalizability across explanation methods.