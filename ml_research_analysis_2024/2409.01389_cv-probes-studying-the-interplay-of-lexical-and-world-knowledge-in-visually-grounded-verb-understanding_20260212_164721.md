---
ver: rpa2
title: 'CV-Probes: Studying the interplay of lexical and world knowledge in visually
  grounded verb understanding'
arxiv_id: '2409.01389'
source_url: https://arxiv.org/abs/2409.01389
tags:
- arxiv
- verb
- captions
- images
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CV-Probes, a novel dataset designed to evaluate
  how well vision-language (VL) models ground verb phrases in context, particularly
  when interpreting actions that require both visual and social knowledge (e.g., "beg")
  versus those based on physical features alone (e.g., "sit"). CV-Probes includes
  image-caption pairs where verbs have visually similar counterparts, requiring models
  to distinguish between context-dependent and context-independent interpretations.
---

# CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding

## Quick Facts
- arXiv ID: 2409.01389
- Source URL: https://arxiv.org/abs/2409.01389
- Reference count: 20
- Models struggle with socially grounded verbs, achieving F1-scores of 0.6738 versus 0.8213 for physically grounded verbs

## Executive Summary
This study introduces CV-Probes, a novel dataset designed to evaluate how well vision-language (VL) models ground verb phrases in context, particularly when interpreting actions that require both visual and social knowledge (e.g., "beg") versus those based on physical features alone (e.g., "sit"). CV-Probes includes image-caption pairs where verbs have visually similar counterparts, requiring models to distinguish between context-dependent and context-independent interpretations. The dataset is built using ImSitu and FrameNet resources, with annotations validated by human judges. Results show that while VL models perform well on physically grounded verbs, they struggle significantly with socially grounded ones, often failing to leverage world knowledge appropriately. Analysis using MM-SHAP reveals that models frequently underutilize verb tokens in captions, instead relying more on other textual or visual cues. These findings highlight the need for improved methodologies in training and evaluating VL models to enhance their ability to integrate social and world knowledge for accurate grounding of verb phrases.

## Method Summary
The CV-Probes dataset was constructed by extracting images from ImSitu and identifying verb phrases that have both physically grounded (PG) and socially grounded (SG) interpretations using FrameNet. Captions were simplified to isolate verb contributions, and human judges validated the distinctions. Seven pretrained VLMs (LXMERT, ALBEF, BLIP, BLIP2, FLAVA, LLaVA-NeXT, GPT-4o) were evaluated on binary image-text matching tasks using matching probabilities. MM-SHAP analysis was applied to quantify token-level contributions, revealing modality reliance patterns. Performance was measured by F1-scores for SG/PG captions and non-matching pairs, with human judgments used for validation.

## Key Results
- VL models achieved F1-scores of 0.8213 for physically grounded verbs versus 0.6738 for socially grounded verbs
- MM-SHAP analysis showed BLIP relies heavily on textual modality (T-SHAP ≈ 89%) with minimal verb contributions for non-matching SG captions
- Models showed positive bias, often assigning high match probabilities even to non-matching SG captions with PG images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language models fail to ground socially grounded verb phrases because they rely too heavily on textual modality and underutilize visual cues.
- Mechanism: The models show high textual Shapley values (T-SHAP) and low verb contributions, especially for socially grounded verbs in non-matching contexts, indicating insufficient integration of visual information.
- Core assumption: High T-SHAP scores relative to visual Shapley values imply over-reliance on text rather than visual grounding.
- Evidence anchors:
  - [abstract] "Further analysis using explainable AI techniques shows that such models may not pay sufficient attention to the verb token in the captions."
  - [section] "For BLIP, verb attributions are high for match predictions (0.1294) and close to zero for non-match predictions (−0.0131) in the key condition..."
  - [corpus] Found 25 related papers; average neighbor FMR=0.49 suggests moderate relevance but no direct citations.
- Break condition: If models achieve balanced Shapley contributions across modalities or if visual grounding improves with architectural changes like dual-stream fusion.

### Mechanism 2
- Claim: Socially grounded verb understanding requires integration of social and world knowledge beyond literal visual features.
- Mechanism: Models cannot infer intentions or social context from visual cues alone, leading to poor performance on socially grounded verbs.
- Core assumption: Human validation confirms that socially grounded captions require reasoning about intentions not explicitly visible.
- Evidence anchors:
  - [abstract] "We show that VL models struggle to ground VPs that are strongly context-dependent."
  - [section] "Determining this involves reasoning about the woman’s intentions, the social significance of her actions, and other visual cues in the image."
  - [corpus] Weak corpus evidence; no direct studies on social knowledge grounding found.
- Break condition: If models achieve human-level performance on socially grounded verbs or if explicit social knowledge modules are integrated.

### Mechanism 3
- Claim: Architectural differences in vision-language models lead to varying abilities to ground verb phrases.
- Mechanism: Models with different visual backbones (e.g., ViT-g vs ViT-B/L) and fusion strategies show different performance patterns.
- Core assumption: Architectural choices like the number of query embeddings and pretraining objectives affect grounding ability.
- Evidence anchors:
  - [abstract] "We evaluate each image-caption pair separately... We do not explicitly compare matching and non-matching pairs..."
  - [section] "BLIP2 has 32 learnable query embeddings as input to the query transformer... BLIP2 differs from other models with a vision transformer visual backbone in that it uses ViT-g as an image encoder..."
  - [corpus] Moderate corpus relevance but no direct architectural comparisons found.
- Break condition: If models with similar architectures achieve consistent grounding performance or if architectural modifications eliminate performance gaps.

## Foundational Learning

- Concept: Vision-language model grounding
  - Why needed here: Understanding how VL models link visual features to linguistic meaning is essential for interpreting their verb grounding performance.
  - Quick check question: Can you explain the difference between physically grounded and socially grounded verb phrases?

- Concept: Shapley value attribution
  - Why needed here: Shapley values help quantify the contribution of individual tokens and modalities to model predictions, revealing grounding deficiencies.
  - Quick check question: How would you interpret a high textual Shapley value and low visual Shapley value in a grounding task?

- Concept: Image-text matching evaluation
  - Why needed here: The dataset uses image-text matching as the primary evaluation method, requiring understanding of how models compute matching probabilities.
  - Quick check question: What does it mean when a model assigns high probability to a non-matching image-caption pair?

## Architecture Onboarding

- Component map:
  Visual backbone -> Textual encoder -> Fusion mechanism -> Image-text matching head

- Critical path:
  1. Extract visual features from image
  2. Encode textual caption
  3. Fuse visual and textual representations
  4. Compute image-text matching probability

- Design tradeoffs:
  - Single-stream vs dual-stream architectures
  - Number of query embeddings vs computational cost
  - Frozen vs trainable components
  - Pretraining objectives (contrastive vs generative)

- Failure signatures:
  - High textual Shapley values with low visual contributions
  - Positive bias in non-matching cases
  - Poor performance on socially grounded verbs
  - Correlation between human and model predictions below 0.3

- First 3 experiments:
  1. Compare Shapley value distributions across modalities for physically vs socially grounded verbs
  2. Evaluate model performance when simplifying captions to isolate verb contributions
  3. Test different visual backbones (ViT-g vs ViT-B) on the same grounding task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to multimodal models would most effectively improve their ability to ground socially grounded verb phrases?
- Basis in paper: [explicit] The authors conclude that current VL models "have significant room for improvement in integrating social and world knowledge information in grounding verb phrases."
- Why unresolved: While the paper identifies the problem, it does not propose or test specific architectural solutions beyond noting that BLIP2's multiple query embeddings and pretraining approach yielded better results than other models.
- What evidence would resolve it: Comparative studies testing different architectural approaches (e.g., varying query embeddings, attention mechanisms, or pretraining objectives) on CV-Probes would demonstrate which modifications most improve grounding of socially grounded verbs.

### Open Question 2
- Question: How does the performance gap between socially and physically grounded verb understanding change across different model sizes and training datasets?
- Basis in paper: [inferred] The study tested multiple models but did not systematically vary model size or analyze how different pretraining datasets affect the ability to ground socially versus physically grounded verbs.
- Why unresolved: The paper compares different models but doesn't control for model size or pretraining data composition, making it unclear whether the observed gaps are inherent to the task or artifacts of specific training regimes.
- What evidence would resolve it: Experiments varying model size while holding other factors constant, or training models on datasets with systematically varied social versus physical content, would clarify the relationship between model characteristics and grounding performance.

### Open Question 3
- Question: To what extent does the reliance on text tokens over visual tokens in current VL models reflect architectural bias versus data bias in pretraining corpora?
- Basis in paper: [explicit] The MM-SHAP analysis revealed "BLIP relies heavily on the textual modality" and raised "the possibility of unimodal collapse," while noting the need to understand whether this reflects architecture or training data.
- Why unresolved: The paper identifies the phenomenon but cannot determine whether it stems from architectural choices (like the matching head design) or from the predominance of text-heavy examples in pretraining data.
- What evidence would resolve it: Ablation studies that modify architectural components while controlling for pretraining data, or training on balanced multimodal datasets, would distinguish between architectural and data-driven causes of modality imbalance.

## Limitations
- Conclusions are based on a relatively small dataset (1,016 captions) and limited model evaluation (7 VLMs)
- Inter-annotator agreement rates for caption simplification and validation are not reported
- The study does not explore alternative explanations for model failures, such as potential biases in the training data or limitations in the evaluation methodology

## Confidence
- **High confidence**: VL models show significant performance gaps between physically and socially grounded verb phrases (F1-scores: 0.8213 vs 0.6738)
- **Medium confidence**: Models underutilize verb tokens in captions, as evidenced by low verb attributions in MM-SHAP analysis, especially for socially grounded verbs
- **Low confidence**: The specific mechanisms by which socially grounded verbs require integration of social and world knowledge beyond visual features are not empirically validated

## Next Checks
1. Evaluate model performance on socially grounded verbs using alternative grounding tasks (e.g., action recognition, video understanding) to validate the dataset's findings across different modalities
2. Conduct ablation studies to isolate the impact of specific architectural components (e.g., visual backbone, fusion mechanism) on socially grounded verb grounding performance
3. Investigate the role of training data composition and biases by analyzing the distribution of physically and socially grounded verbs in popular vision-language pretraining datasets