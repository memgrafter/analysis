---
ver: rpa2
title: 'Just Propagate: Unifying Matrix Factorization, Network Embedding, and LightGCN
  for Link Prediction'
arxiv_id: '2410.21325'
source_url: https://arxiv.org/abs/2410.21325
tags:
- link
- prediction
- methods
- matrix
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for link prediction that
  encompasses matrix factorization, network embedding methods (DeepWalk, LINE), and
  graph neural networks (LightGCN). The authors show that the forward propagation
  and backward gradient descent in these methods can be equivalently represented as
  a representation propagation procedure using a common propagation kernel.
---

# Just Propagate: Unifying Matrix Factorization, Network Embedding, and LightGCN for Link Prediction

## Quick Facts
- arXiv ID: 2410.21325
- Source URL: https://arxiv.org/abs/2410.21325
- Reference count: 22
- Key outcome: Unified framework shows MF, DeepWalk, LINE, and LightGCN are equivalent forward propagation procedures using a common propagation kernel

## Executive Summary
This paper presents a unified framework that demonstrates how matrix factorization, network embedding methods (DeepWalk, LINE), and graph neural networks (LightGCN) can all be represented as a single representation propagation procedure. The authors show that forward propagation and backward gradient descent in these methods are equivalent to a representation propagation procedure using a common propagation kernel H(m+1). Through empirical analysis on recommendation datasets (Electronics and LastFM), the framework reveals that LightGCN outperforms matrix factorization and that controlling the fitting curve of the positive link kernel and properly normalizing the adjacency matrix are crucial for effective link prediction models.

## Method Summary
The unified framework represents all four methods as a representation propagation procedure where node representations X(m+1) = H(m+1)X(m) are updated through a common propagation kernel H(m+1) = c1I + c2Pa1,b1(˜A)[K(m+1)+ (A) - λK(m+1)- (B)]Pa1,b1(˜A). The kernel consists of three components: a high-order proximity matrix Pa1,b1(˜A), a positive link kernel K(m+1)+ (A), and a negative link kernel K(m+1)- (B). The framework uses binary cross-entropy loss with L2 regularization and negative sampling, with early stopping for training. Different methods are unified by varying the parameters in the propagation kernel and the normalization of the adjacency matrix.

## Key Results
- LightGCN outperforms matrix factorization on recommendation datasets (Electronics and LastFM)
- The propagation kernel, which controls how representations are propagated and whether higher-order neighbor information is utilized, significantly impacts performance
- Existing methods implicitly follow the balance theory of signed graphs when aggregating neighborhoods through negative sampling
- Properly controlling the fitting curve of the positive link kernel and normalizing the adjacency matrix are crucial for effective link prediction models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unified framework shows that all four methods (MF, DeepWalk, LINE, LightGCN) can be represented as a single representation propagation procedure.
- Mechanism: Forward propagation and backward gradient descent in these methods are equivalent to a representation propagation procedure using a common propagation kernel H(m+1).
- Core assumption: The propagation kernel H(m+1) = c1I + c2Pa1,b1(˜A)[K(m+1)+ (A) - λK(m+1)- (B)]Pa1,b1(˜A) captures the essential mechanics of all four methods.
- Evidence anchors:
  - [abstract] "the forward propagation and backward gradient descent in these methods can be equivalently represented as a representation propagation procedure using a common propagation kernel"
  - [section 3.1] "we find that they can be unified into a general framework that only performs forward propagation, i.e., even the back propagation for gradient descends is equivalent to the forward propagation"
  - [corpus] Weak - the corpus papers focus on different aspects of graph learning but don't directly address this unification claim
- Break condition: If the gradient descent updates cannot be expressed as a simple forward propagation kernel multiplication, the unification fails.

### Mechanism 2
- Claim: Propagation kernels that control how representations are propagated and utilize high-order neighbor information significantly impact performance.
- Mechanism: The parameters a1, b1 in Pa1,b1(˜A) and a2, b2 in K(m+1)+ (A) determine how much high-order proximity information is incorporated during each optimization step.
- Core assumption: Higher-order proximity information is beneficial for link prediction performance.
- Evidence anchors:
  - [abstract] "Propagation kernels, which determine how the representation is propagated and whether higher-order neighbor information can be effectively utilized, greatly affect the performance of different methods"
  - [section 3.2] "Different methods have diverse strategies in using the high-order information from neighborhoods"
  - [corpus] Weak - corpus papers mention high-order information but don't directly test this mechanism
- Break condition: If increasing high-order information beyond a certain point degrades performance or if different datasets show no consistent benefit from higher-order information.

### Mechanism 3
- Claim: The existing methods implicitly follow the balance theory of signed graphs when aggregating neighborhoods through negative sampling.
- Mechanism: By treating randomly drawn negative samples as negative edges, the propagation step satisfies the sociological assumption that "the enemy of my enemy is my friend."
- Core assumption: Sampled negative links correspond to real negative connections in the underlying graph.
- Evidence anchors:
  - [section 3.2] "the propagation step in Eq. (4) satisfies the balance theory, i.e., a sociological assumption that 'the enemy of my enemy is my friend'"
  - [abstract] "the existing approaches essentially follow the balance theory of signed graphs in aggregating neighborhoods"
  - [corpus] Missing - no corpus papers directly address signed graph balance theory in this context
- Break condition: If the negative sampling strategy changes significantly or if the underlying assumption about negative links being real connections is violated.

## Foundational Learning

- Concept: Matrix factorization for link prediction
  - Why needed here: Understanding how MF optimizes node representations directly using the loss function is crucial for seeing how it fits into the unified framework
  - Quick check question: How does matrix factorization differ from network embedding methods in terms of how they handle high-order proximity?

- Concept: Graph neural networks and message passing
  - Why needed here: LightGCN's propagation mechanism is central to understanding how the unified framework handles high-order information differently from traditional methods
  - Quick check question: What role does the normalized adjacency matrix ˜A play in controlling representation propagation in LightGCN?

- Concept: Negative sampling in implicit feedback
  - Why needed here: All methods in the framework use negative sampling, and understanding how this relates to the balance theory is key to the third mechanism
  - Quick check question: How does treating sampled negative links as negative edges affect the overall learning objective?

## Architecture Onboarding

- Component map:
  - Node representation matrix X
  - Propagation kernel H(m+1) with three components: high-order proximity matrix Pa1,b1(˜A), positive link kernel K(m+1)+ (A), negative link kernel K(m+1)- (B)
  - Loss function (binary cross-entropy with L2 regularization)
  - Normalization of adjacency matrix (D^(-1/2)AD^(-1/2) for LightGCN, D^(-1)A for others)

- Critical path: X(m+1) = H(m+1)X(m) → compute loss → update H(m+1) based on score matrices → repeat

- Design tradeoffs:
  - Choice of normalization matrix (D^(-1/2)AD^(-1/2) vs D^(-1)A) affects representation length control
  - Tradeoff between fitting training data quickly (MF) vs maintaining better generalization (LightGCN)
  - Balance between computational efficiency and incorporation of high-order information

- Failure signatures:
  - Rapid decrease in mean value of K+ (positive link kernel) may indicate overfitting
  - Unstable representation norms suggest poor normalization choice
  - Degradation in performance when increasing high-order information beyond certain levels

- First 3 experiments:
  1. Compare Frobenius norm evolution of X across methods to verify normalization effects
  2. Plot mean value of K+ vs optimization steps to analyze fitting behavior
  3. Test different values of a1, b1 in high-order proximity matrix to understand high-order information utilization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different propagation kernels specifically affect the performance of link prediction models?
- Basis in paper: [explicit] The paper mentions that "Propagation kernels, which determine how the representation is propagated and whether higher-order neighbor information can be effectively utilized, greatly affect the performance of different methods for the link prediction task."
- Why unresolved: The paper discusses the importance of propagation kernels but does not provide a detailed analysis of how different kernels specifically impact performance.
- What evidence would resolve it: Empirical studies comparing various propagation kernels and their effects on link prediction accuracy.

### Open Question 2
- Question: What are the implications of the balance theory of signed graphs on the aggregation of neighborhoods in link prediction methods?
- Basis in paper: [explicit] The paper states that "the existing approaches essentially follow the balance theory of signed graphs in aggregating neighborhoods."
- Why unresolved: While the paper mentions that methods follow the balance theory, it does not explore the deeper implications or potential improvements by leveraging this theory more explicitly.
- What evidence would resolve it: Theoretical analysis and experimental validation of how incorporating the balance theory more explicitly could enhance link prediction performance.

### Open Question 3
- Question: How does the fitting curve of the positive link kernel influence the effectiveness of link prediction models?
- Basis in paper: [explicit] The paper suggests that "we need to properly control the fitting curve to realize effective link prediction models."
- Why unresolved: The paper indicates the importance of controlling the fitting curve but does not provide a detailed mechanism or strategy for doing so.
- What evidence would resolve it: Studies that explore different strategies for controlling the fitting curve and their impact on model performance.

### Open Question 4
- Question: What role does the normalization of the adjacency matrix play in the success of link prediction methods?
- Basis in paper: [explicit] The paper notes that "properly choosing the normalization of adjacency matrix is crucial for the success of link prediction methods."
- Why unresolved: The paper highlights the importance of adjacency matrix normalization but does not explore the specific effects of different normalization techniques.
- What evidence would resolve it: Comparative analysis of different adjacency matrix normalization techniques and their effects on link prediction accuracy.

## Limitations
- Limited dataset scope: Only tested on two recommendation datasets (Electronics and LastFM)
- Lack of comparison against modern graph neural network baselines
- Reliance on the assumption that negative samples represent true negative edges without empirical validation

## Confidence
- Mathematical unification: High confidence in the core mathematical derivation
- Empirical validation: Medium confidence due to limited dataset scope
- Balance theory mechanism: Low confidence as it relies on untested assumptions about negative sampling

## Next Checks
1. **Generalization testing**: Validate the unified framework across diverse graph types (heterogeneous graphs, knowledge graphs, social networks) to test if the mechanisms hold beyond recommendation datasets.

2. **Negative sampling sensitivity**: Systematically vary negative sampling strategies (different sampling ratios, hard negative mining) to test the robustness of the balance theory mechanism and its impact on performance.

3. **High-order information threshold**: Conduct controlled experiments to identify the optimal high-order proximity level across different graph structures and determine when additional high-order information becomes detrimental rather than beneficial.