---
ver: rpa2
title: 'Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise Passage
  Re-Ranking with Cross-Encoders'
arxiv_id: '2404.06912'
source_url: https://arxiv.org/abs/2404.06912
tags:
- passage
- trec
- set-encoder
- passages
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Set-Encoder addresses the lack of permutation-invariant inter-passage
  attention in cross-encoder models for passage re-ranking. It introduces a novel
  inter-passage attention pattern where passages are processed in parallel with dedicated
  [INT] tokens that allow information exchange while maintaining permutation invariance.
---

# Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise Passage Re-Ranking with Cross-Encoders

## Quick Facts
- arXiv ID: 2404.06912
- Source URL: https://arxiv.org/abs/2404.06912
- Reference count: 40
- Key outcome: The Set-Encoder achieves effectiveness comparable to state-of-the-art listwise models while being 33-110 times faster with 6x less memory usage, particularly excelling at novelty-aware ranking.

## Executive Summary
The Set-Encoder introduces a novel permutation-invariant inter-passage attention mechanism for listwise passage re-ranking that addresses a critical limitation in cross-encoder models. By using dedicated [INT] tokens that enable information exchange between passages while maintaining permutation invariance, the model achieves state-of-the-art effectiveness comparable to listwise models but with significantly improved efficiency. The approach is particularly effective for novelty-aware ranking tasks where passage interactions become valuable, while maintaining strong performance on standard relevance ranking.

## Method Summary
The Set-Encoder uses a two-stage fine-tuning approach with ELECTRA BASE/LARGE checkpoints. First, it's fine-tuned using InfoNCE loss on MS MARCO with ColBERTv2 hard negatives for 20k steps. Second, it's fine-tuned using RankNet loss on Rank-DistiLLM for 3 epochs. The model processes passages in parallel using dedicated [INT] tokens that aggregate semantic information and enable inter-passage attention without encoding positional information. For novelty-aware ranking, duplicate-aware InfoNCE and novelty-aware RankNet variants are used. The model is evaluated on TREC Deep Learning 2019/2020 tracks, TIREx framework collections, and novelty tasks using Î±-nDCG@10.

## Key Results
- The Set-Encoder is 33-110 times faster and uses 6 times less memory than LLM-based listwise models
- Achieves effectiveness comparable to state-of-the-art listwise models on TREC Deep Learning and TIREx benchmarks
- Excels at novelty-aware ranking, demonstrating that passage interactions become valuable when ranking based on both relevance and novelty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Set-Encoder's permutation-invariant design allows effective re-ranking without requiring multiple permutations of input passages.
- Mechanism: By processing passages in parallel and using dedicated [INT] tokens with identical positional encodings, the model can model passage interactions without encoding passage order. This avoids the inefficiency and complexity of re-ranking multiple permutations as required by traditional listwise models.
- Core assumption: Positional encodings starting from zero for each sequence ensure no order information is encoded in the interaction tokens.
- Evidence anchors:
  - [abstract] "The Set-Encoder is as effective as state-of-the-art listwise models while being more efficient and invariant to input passage order permutations."
  - [section] "Our batched input encoding is permutation-invariant because each input sequence's positional encodings start from zero."
  - [corpus] Weak - the corpus doesn't provide direct evidence about the permutation-invariant mechanism's effectiveness.

### Mechanism 2
- Claim: The [INT] tokens enable lightweight information exchange between passages without the computational overhead of full concatenation.
- Mechanism: Each passage has a dedicated [INT] token that aggregates its semantic information, and all passages can attend to each other's [INT] tokens. This creates an efficient information-sharing mechanism that scales linearly rather than quadratically with passage count.
- Core assumption: The [INT] token can effectively capture and share semantic information about its passage.
- Evidence anchors:
  - [section] "Our intuition is that these [INT] tokens aggregate the semantic information from their sequence and can share it with all other sequences."
  - [section] "The Set-Encoder allows passage interactions solely through these [INT] tokens, making inter-passage attention computationally efficient."
  - [corpus] Weak - corpus doesn't provide specific evidence about the efficiency of the [INT] token mechanism.

### Mechanism 3
- Claim: The Set-Encoder can learn passage interactions when specifically trained for tasks requiring inter-passage reasoning, such as novelty-aware ranking.
- Mechanism: When fine-tuned with duplicate-aware InfoNCE or novelty-aware RankNet loss functions, the Set-Encoder learns to use the [INT] tokens for information sharing, improving performance on tasks where passage interactions are beneficial.
- Core assumption: The model can learn to use the [INT] tokens for information sharing when the training objective requires inter-passage reasoning.
- Evidence anchors:
  - [section] "Interestingly, when initially fine-tuned using duplicate-aware DA-InfoNCE...the Set-Encoder profits from inter-passage attention and is substantially more effective."
  - [section] "We find that the Set-Encoder fine-tuned with standard InfoNCE...is on par with a pointwise monoELECTRA model in novelty-aware ranking."
  - [corpus] Weak - corpus doesn't provide evidence about the learning of passage interactions.

## Foundational Learning

- Concept: Positional encoding in transformers
  - Why needed here: Understanding how positional encodings work is crucial to grasp why the Set-Encoder's approach achieves permutation invariance.
  - Quick check question: What happens to the positional encodings when passages are processed in parallel versus concatenated?

- Concept: Attention mechanism and self-attention
  - Why needed here: The Set-Encoder modifies the attention mechanism to allow inter-passage attention through [INT] tokens, so understanding attention is essential.
  - Quick check question: How does the modified attention function in the Set-Encoder differ from standard transformer attention?

- Concept: Contrastive learning objectives (InfoNCE)
  - Why needed here: The Set-Encoder uses InfoNCE loss for fine-tuning, which is important for understanding its training process.
  - Quick check question: What is the purpose of using InfoNCE loss in the context of passage re-ranking?

## Architecture Onboarding

- Component map: Query + multiple passages (each with [CLS], [INT], passage tokens) -> Transformer encoder with modified inter-passage attention -> Linear transformation on [CLS] tokens -> Relevance scores

- Critical path:
  1. Tokenize query and passages
  2. Construct input sequences with [CLS], [INT], and passage tokens
  3. Apply modified attention allowing inter-passage [INT] token interactions
  4. Generate contextualized embeddings
  5. Apply linear transformation to [CLS] tokens
  6. Output relevance scores

- Design tradeoffs:
  - Permutation invariance vs. potential loss of positional context
  - Computational efficiency vs. expressiveness of full concatenation
  - Dedicated [INT] tokens vs. using existing [CLS] tokens

- Failure signatures:
  - Similar performance to pointwise models (indicating [INT] tokens aren't being used)
  - Sensitivity to input passage order (indicating permutation invariance isn't working)
  - Degraded performance on novelty tasks (indicating poor inter-passage information sharing)

- First 3 experiments:
  1. Test permutation invariance by comparing performance across different passage orderings
  2. Compare [INT] token usage by ablating them and measuring performance drop
  3. Test efficiency by measuring inference time and memory usage vs. concatenated listwise models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the [CLS] token be adapted to effectively model passage interactions in cross-encoders, given that it is heavily pre-trained and less flexible than the [INT] token?
- Basis in paper: [explicit] The authors hypothesize that the [CLS] token is not suited to model passage interactions as it has been heavily pre-trained and thus is not flexible enough to learn new interactions in fine-tuning.
- Why unresolved: The paper only tests the [CLS] token in a specific ablation study and does not explore potential modifications or alternative approaches to make the [CLS] token more suitable for modeling interactions.
- What evidence would resolve it: Experiments comparing different modifications to the [CLS] token, such as additional fine-tuning steps or architectural changes, to determine if it can be adapted to model interactions effectively.

### Open Question 2
- Question: What are the specific characteristics of the TREC Deep Learning and TIREx scenarios that make passage interactions unnecessary for effective re-ranking?
- Basis in paper: [inferred] The authors note that passage interactions are not necessary in these scenarios, likely due to the independent nature of relevance judgments and the lack of consideration for inter-document related aspects such as novelty.
- Why unresolved: The paper does not provide a detailed analysis of the specific characteristics of these scenarios that make passage interactions less important.
- What evidence would resolve it: A comparative study of the TREC Deep Learning and TIREx scenarios with other scenarios that do benefit from passage interactions, highlighting the key differences in relevance judgment and evaluation setup.

### Open Question 3
- Question: How does the efficiency of the Set-Encoder compare to other permutation-invariant models, such as SetRank, in terms of both inference time and memory footprint?
- Basis in paper: [inferred] The authors mention that the Set-Encoder is more efficient than previous listwise models, but do not provide a direct comparison with other permutation-invariant models.
- Why unresolved: The paper focuses on comparing the Set-Encoder with non-permutation-invariant models, leaving the efficiency comparison with other permutation-invariant models unexplored.
- What evidence would resolve it: A comprehensive efficiency analysis comparing the Set-Encoder with other permutation-invariant models, such as SetRank, on the same benchmarks and hardware configurations.

## Limitations

- The permutation-invariant design may sacrifice some modeling power by not encoding passage order information
- Effectiveness gains are primarily demonstrated in novelty-aware ranking rather than standard relevance ranking
- Evaluation focuses heavily on TREC Deep Learning datasets with limited testing on other re-ranking benchmarks

## Confidence

- High confidence in the permutation-invariant mechanism design and computational efficiency claims
- Medium confidence in the effectiveness claims for novelty-aware ranking
- Low confidence in the generalizability of benefits to all re-ranking scenarios

## Next Checks

1. Test permutation invariance rigorously by measuring performance degradation across different passage orderings
2. Compare [INT] token effectiveness against simpler alternatives like using multiple [CLS] tokens
3. Evaluate the model's performance on non-novelty re-ranking tasks to assess generalizability of inter-passage attention benefits