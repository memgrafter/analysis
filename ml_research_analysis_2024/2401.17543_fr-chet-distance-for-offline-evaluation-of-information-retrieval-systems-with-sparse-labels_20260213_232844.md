---
ver: rpa2
title: "Fr\xE9chet Distance for Offline Evaluation of Information Retrieval Systems\
  \ with Sparse Labels"
arxiv_id: '2401.17543'
source_url: https://arxiv.org/abs/2401.17543
tags:
- distance
- echet
- relevant
- retrieval
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of evaluating information retrieval\
  \ (IR) systems when labeled data is sparse. The authors propose using the Fr\xE9\
  chet Distance (FD) to measure the similarity between the distributions of relevant\
  \ judged items and retrieved results, drawing inspiration from its successful application\
  \ in evaluating text-to-image generation systems."
---

# Fréchet Distance for Offline Evaluation of Information Retrieval Systems with Sparse Labels

## Quick Facts
- arXiv ID: 2401.17543
- Source URL: https://arxiv.org/abs/2401.17543
- Authors: Negar Arabzadeh; Charles L. A. Clarke
- Reference count: 18
- Primary result: FD effectively evaluates IR systems with sparse labels by measuring distribution similarity between relevant and retrieved items.

## Executive Summary
This paper proposes using Fréchet Distance (FD) to evaluate information retrieval systems when labeled data is sparse. The approach measures the similarity between distributions of relevant judged items and retrieved results using multivariate normal distributions fitted to their embeddings. Experiments on MS MARCO V1 and TREC Deep Learning Tracks demonstrate that FD correlates well with traditional IR metrics and remains robust even when evaluating unlabeled retrieved results. The method shows promise for real-world IR evaluation where obtaining comprehensive relevance judgments is expensive or impractical.

## Method Summary
The method involves retrieving top-k documents for each query using various IR models, generating embeddings for relevant judged items and retrieved documents using pre-trained language models (DistilBERT), fitting multivariate normal distributions to both sets of embeddings, and computing the Fréchet Distance between these distributions. The FD values are then compared against traditional IR metrics like MRR@10 and nDCG@10 to assess effectiveness. The approach leverages the property that FD can measure distribution similarity even when the two sets have different sizes, making it suitable for sparse label scenarios where the number of relevant items is much smaller than the number of retrieved items.

## Key Results
- FD shows high correlation with traditional IR metrics (MRR@10, nDCG@10) on both MS MARCO V1 and TREC Deep Learning datasets
- FD remains effective even when evaluating retrieved results without any relevance judgments
- Fine-tuning the embedding model on MS MARCO improves FD's correlation with traditional metrics
- FD can distinguish between different retrieval models even with as few as 40-50 queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FD effectively measures the similarity between distributions of retrieved and relevant items even when labels are sparse.
- Mechanism: FD computes the distance between two multivariate normal distributions fitted to embeddings of relevant judged items and retrieved results. A lower FD indicates higher similarity and better retrieval performance.
- Core assumption: The embeddings of relevant and retrieved items follow approximately normal distributions, and the FD captures meaningful similarity between these distributions.
- Evidence anchors: [abstract] "We propose leveraging the Fréchet Distance to measure the distance between the distributions of relevant judged items and retrieved results." [section 2.2] "To apply Fréchet Distance for assessing the quality of the IR system M, we measure FDMQ as follows..."

### Mechanism 2
- Claim: FD is robust to sparsity because it aggregates over the entire query set rather than relying on individual query judgments.
- Mechanism: By computing FD over all queries, the metric smooths out noise from sparse individual labels, providing a stable estimate of overall system performance.
- Core assumption: Aggregating over many queries compensates for sparse individual judgments, making the metric reliable even with limited per-query labels.
- Evidence anchors: [abstract] "Our experimental results on MS MARCO V1 dataset and TREC Deep Learning Tracks query sets demonstrate the effectiveness of the Fréchet Distance as a metric for evaluating IR systems, particularly in settings where a few labels are available."

### Mechanism 3
- Claim: FD can evaluate unlabeled retrieved results by comparing their embeddings to the distribution of relevant judged items.
- Mechanism: FD measures similarity between the distribution of embeddings of relevant judged items and the distribution of embeddings of top-k retrieved items, even when none are labeled as relevant.
- Core assumption: The embedding space captures semantic similarity such that unjudged relevant items have similar distributions to judged relevant items.
- Evidence anchors: [abstract] "we explore if we can quantify the quality of retrieved documents in an ad hoc retrieval system through Fréchet Distance." [section 6] "We measure the FD between one set consisting of the relevant judged items per query and the other set consisting of the top-k unjudged retrieved item for each query."

## Foundational Learning

- Concept: Multivariate normal distribution fitting
  - Why needed here: FD relies on fitting multivariate normal distributions to embeddings of relevant and retrieved items to compute the distance.
  - Quick check question: What are the parameters of a multivariate normal distribution and how are they estimated from data?

- Concept: Embedding spaces and feature extraction
  - Why needed here: FD requires meaningful embeddings of documents to compare distributions; the choice of embedding model affects FD's effectiveness.
  - Quick check question: How does a transformer-based model like DistilBERT generate embeddings for documents, and why might fine-tuning on MS MARCO improve them?

- Concept: Statistical correlation measures
  - Why needed here: The paper uses Kendall tau correlation to compare FD with traditional IR metrics like MRR and nDCG.
  - Quick check question: What does a negative Kendall tau correlation indicate about the relationship between FD and MRR?

## Architecture Onboarding

- Component map: Data pipeline -> Retrieval models -> Embedding models -> FD computation -> Evaluation metrics
- Critical path: 1. Retrieve top-k documents for each query using a retrieval model. 2. Generate embeddings for relevant judged items and retrieved documents. 3. Fit multivariate normal distributions to both sets of embeddings. 4. Compute FD between the two distributions. 5. Compare FD results with traditional metrics to assess effectiveness.
- Design tradeoffs: Embedding model choice: Fine-tuned models may better capture relevance but are more resource-intensive; general models are faster but may be less effective. Cut-off k: Higher k includes more documents but may introduce noise; lower k is more precise but may miss relevant items. Handling sparsity: FD aggregates over queries, which helps with sparsity but may mask poor performance on specific queries.
- Failure signatures: High FD values with good traditional metrics: Possible distribution fitting issues or embedding misalignment. Unstable FD across bootstrap samples: Insufficient query set size or extreme label sparsity. Low correlation with traditional metrics: Embedding model not capturing relevance or distributional assumptions violated.
- First 3 experiments: 1. Compute FD@10 for BM25 and ColBERT on MS MARCO dev set and compare with MRR@10 to verify FD reflects known performance differences. 2. Evaluate FD with 1, 5, and 10 relevant items per query on TREC DL datasets to test sensitivity to label sparsity. 3. Measure FD@10 on unlabeled retrieved results for various retrievers to confirm FD can assess without relevance judgments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of feature extraction method impact the effectiveness of Fréchet Distance in evaluating IR systems, and are there specific models or approaches that consistently outperform others across different datasets and retrieval tasks?
- Basis in paper: [explicit] The paper discusses the impact of document representation on FD's effectiveness, comparing a fine-tuned DistilBERT model with a general-purpose, unfine-tuned DistilBERT model.
- Why unresolved: The paper provides initial evidence that fine-tuning improves correlation with traditional metrics, but does not explore the full range of potential feature extraction methods or their performance across diverse retrieval tasks and datasets.
- What evidence would resolve it: Comprehensive experiments comparing FD performance using various feature extraction methods (e.g., different transformer models, sentence embeddings, handcrafted features) across multiple IR datasets and tasks, demonstrating consistent trends or identifying optimal approaches.

### Open Question 2
- Question: Can the Fréchet Distance be effectively adapted to evaluate generative IR systems that produce novel, potentially unseen content, and how does its performance compare to traditional metrics in these scenarios?
- Basis in paper: [inferred] The paper draws parallels between evaluating generated images in text-to-image generation tasks and assessing retrieved results in IR systems, suggesting the potential for FD to evaluate generative IR outputs.
- Why unresolved: While the paper demonstrates FD's effectiveness in evaluating retrieved results, it does not specifically address the unique challenges of assessing generative IR systems that produce novel content, which may not directly align with the ground truth.
- What evidence would resolve it: Experiments evaluating FD's performance in assessing the quality of generative IR systems, comparing its results with traditional metrics and human judgments, and analyzing its ability to capture the relevance and novelty of generated content.

### Open Question 3
- Question: How does the size and composition of the query set impact the stability and reliability of Fréchet Distance as an evaluation metric for IR systems, and what are the minimum requirements for achieving consistent and meaningful results?
- Basis in paper: [explicit] The paper mentions that FD can effectively distinguish between rankers even with a smaller number of queries (around 40-50) in the TREC DL datasets, contrasting with the large number of data points typically required for evaluating text-to-image generation tasks.
- Why unresolved: The paper does not provide a systematic analysis of how varying the number and characteristics of queries affects FD's performance, leaving uncertainty about the minimum requirements for reliable evaluation.
- What evidence would resolve it: A series of experiments systematically varying the size and composition of query sets in different IR datasets, analyzing the impact on FD's stability and correlation with traditional metrics, and identifying the minimum requirements for achieving consistent and meaningful results.

## Limitations

- The paper does not validate the assumption that document embeddings follow multivariate normal distributions
- Limited exploration of how extreme label sparsity affects FD's effectiveness
- No comparison with alternative distributional similarity metrics or non-parametric methods

## Confidence

- **High Confidence**: FD can compute meaningful distances between embedding distributions and correlates with traditional metrics on datasets with moderate label density.
- **Medium Confidence**: FD remains stable when evaluating unlabeled retrieved results and when aggregating over sparse per-query labels.
- **Low Confidence**: The distributional assumptions underlying FD are valid for IR embeddings, and FD is robust to extreme sparsity or non-Gaussian embedding distributions.

## Next Checks

1. Apply statistical tests (e.g., Shapiro-Wilk, Anderson-Darling) to verify that relevant and retrieved item embeddings follow multivariate normal distributions across different embedding models and datasets.

2. Systematically vary the number of relevant judgments per query (e.g., 1, 3, 5, 10) and measure FD's correlation with traditional metrics to identify the sparsity threshold where FD breaks down.

3. Replace multivariate normal fitting with kernel density estimation or other non-parametric methods and compare FD results to assess sensitivity to distributional assumptions.