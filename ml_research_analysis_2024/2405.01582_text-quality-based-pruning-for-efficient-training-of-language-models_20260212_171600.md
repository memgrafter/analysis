---
ver: rpa2
title: Text Quality-Based Pruning for Efficient Training of Language Models
arxiv_id: '2405.01582'
source_url: https://arxiv.org/abs/2405.01582
tags:
- text
- quality
- training
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel method for numerically evaluating text
  quality in large unlabelled NLP datasets using a model-agnostic approach. The method
  applies 14 heuristic-based filters to calculate a quality score for each text instance.
---

# Text Quality-Based Pruning for Efficient Training of Language Models

## Quick Facts
- arXiv ID: 2405.01582
- Source URL: https://arxiv.org/abs/2405.01582
- Reference count: 10
- Primary result: 0.9% accuracy improvement on OpenWebText and 0.8% on Wikipedia while using 40% and 20% less data respectively

## Executive Summary
This paper proposes a model-agnostic method for pruning low-quality text from unlabelled datasets to improve language model training efficiency. The approach uses 14 heuristic-based filters to compute quality scores for each text instance, enabling removal of low-quality samples while retaining high-quality ones. Experiments on OpenWebText and Wikipedia datasets demonstrate substantial gains in training effectiveness, with models achieving better accuracy while using less data and training faster.

## Method Summary
The method applies 14 heuristic-based filters to calculate quality scores for text instances in unlabelled datasets. These scores are used to prune the original dataset by removing low-quality text. The approach is completely model-agnostic and relies solely on the underlying data. The paper demonstrates the effectiveness of this pruning strategy on two datasets (OpenWebText and Wikipedia) across multiple language models, showing improvements in both training efficiency and model performance.

## Key Results
- 0.9% average accuracy improvement on OpenWebText while using 40% less data
- 0.8% average accuracy improvement on Wikipedia while using 20% less data
- 42% faster training on OpenWebText and 21% faster on Wikipedia

## Why This Works (Mechanism)

### Mechanism 1
- Low-quality text samples harm model performance more than they help
- Removing noisy or grammatically incorrect samples reduces confusion during training
- Evidence: 0.9% accuracy improvement on OpenWebText and 0.8% on Wikipedia with less data

### Mechanism 2
- Pruning low-quality data reduces training time without sacrificing model quality
- Fewer training samples mean fewer gradient updates and less computation
- Evidence: 42% faster training on OpenWebText and 21% faster on Wikipedia

### Mechanism 3
- Quality scores are stable across different models and datasets
- Heuristic-based scoring method is model-agnostic
- Evidence: Consistent improvements across multiple models (GPT2, GPT-Neo, Pythia, OPT) and datasets

## Foundational Learning

- Concept: Perplexity as a measure of language model fit
  - Why needed: Used to assign weights to filters and evaluate model performance post-pruning
  - Quick check: If validation perplexity increases after pruning, what does that imply about the quality of removed data?

- Concept: Heuristic-based data filtering
  - Why needed: 14 linguistic heuristics (e.g., capitalization, word repetition) compute quality scores
  - Quick check: If a filter consistently assigns high weights across datasets, what does that suggest about its effectiveness?

- Concept: Dataset pruning and its impact on generalization
  - Why needed: Method aims to remove low-quality samples while retaining enough diversity
  - Quick check: If downstream task accuracy drops after aggressive pruning, what might that indicate about the removed data?

## Architecture Onboarding

- Component map: Raw text dataset -> 14 heuristic filters -> Line-level quality scores -> Document-level scores -> Pruning -> LM training pipeline
- Critical path: 1) Apply 14 filters to compute indicator matrix 2) Calculate filter weights using perplexity differences 3) Compute line-level quality scores 4) Aggregate to document-level scores 5) Prune dataset based on percentile threshold 6) Train LM on pruned dataset
- Design tradeoffs: Heuristic simplicity vs. coverage, pruning threshold selection, fixed vs. adaptive filters
- Failure signatures: Perplexity spikes after pruning, downstream accuracy drops, training time does not decrease
- First 3 experiments: 1) Apply filters to small subset and inspect score distribution 2) Prune at 20%, 40%, 60%, 80% thresholds and check perplexity trends 3) Train small LM on pruned vs. unpruned data

## Open Questions the Paper Calls Out
- How does the effectiveness of the proposed method scale with larger language models (hundreds of billions of parameters)?
- How does the method perform on datasets significantly larger than those used in experiments?
- What is the impact of dataset pruning on the robustness, fairness, and interpretability of trained language models?

## Limitations
- Relies heavily on effectiveness of 14 heuristic filters across diverse domains and languages
- Pruning thresholds chosen without systematic exploration of optimal levels
- Only evaluated on English datasets (OpenWebText and Wikipedia)

## Confidence

**High Confidence**: Claim that pruning reduces training time is strongly supported by empirical results (42% and 21% faster training).

**Medium Confidence**: Accuracy improvements are statistically significant but modest (0.9% and 0.8%), suggesting incremental rather than transformative benefits.

**Low Confidence**: Stability of quality scores across different models and datasets is based on limited evidence from only two English datasets.

## Next Checks
1. Apply pruning method to diverse datasets (code repositories, scientific papers, multilingual corpora) to test cross-domain robustness
2. Design adaptive method to determine optimal pruning level for each dataset rather than using fixed percentages
3. Conduct systematic ablation study to determine individual contributions of each filter and identify critical ones for different dataset types