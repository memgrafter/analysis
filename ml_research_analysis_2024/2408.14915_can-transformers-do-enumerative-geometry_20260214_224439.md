---
ver: rpa2
title: Can Transformers Do Enumerative Geometry?
arxiv_id: '2408.14915'
source_url: https://arxiv.org/abs/2408.14915
tags:
- intersection
- numbers
- learning
- data
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors apply a Transformer-based architecture, called DynamicFormer,\
  \ to the problem of computing \u03C8-class intersection numbers on moduli spaces\
  \ of curves. These intersection numbers are fundamental invariants in algebraic\
  \ geometry and theoretical physics, and their computation is notoriously challenging\
  \ due to their recursive nature and extremely wide numerical range."
---

# Can Transformers Do Enumerative Geometry?

## Quick Facts
- arXiv ID: 2408.14915
- Source URL: https://arxiv.org/abs/2408.14915
- Authors: Baran Hashemi; Roderic G. Corominas; Alessandro Giacchetto
- Reference count: 14
- Key outcome: DynamicFormer achieves up to 0.999 R² on ψ-class intersection numbers up to genus 16, with uncertainty quantification via conformal prediction

## Executive Summary
This paper applies a Transformer-based architecture called DynamicFormer to compute ψ-class intersection numbers on moduli spaces of curves, a fundamental problem in enumerative geometry and theoretical physics. The authors address the challenge of extreme numerical ranges and recursive patterns by introducing a new activation function (DRA) and reformulating the problem as continuous optimization. Their model achieves high accuracy both in-distribution (genus ≤14) and out-of-distribution (genus 15-16), while also demonstrating the ability to implicitly learn mathematical structures like Virasoro constraints through abductive reasoning.

## Method Summary
The authors tackle the computation of ψ-class intersection numbers using a multi-modal Transformer architecture that processes both continuous tensor data (quantum Airy structure) and discrete partitions. They introduce the Dynamic Range Activator (DRA) to handle recursive patterns and extreme numerical ranges, incorporate uncertainty quantification through conformal prediction with dynamic sliding windows, and use register tokens ([DYN]) with talking modalities for contrastive learning between different input types. The model is trained on known data up to genus 13 and tested on higher genera to evaluate out-of-distribution generalization capabilities.

## Key Results
- Achieves up to 0.999 R² score on in-distribution test data (genus 14)
- Successfully predicts intersection numbers for out-of-distribution cases (genus 15-16) with high accuracy
- Uncertainty quantification shows reliable coverage rates through conformal prediction
- Model implicitly learns Virasoro constraints and extracts asymptotic parameters (A=2/3) from internal representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Range Activator (DRA) enables Transformers to capture recursive patterns and handle extreme numerical ranges in enumerative geometry.
- Mechanism: DRA combines linear, sinusoidal, and hyperbolic tangent components with learnable parameters to model both periodic fluctuations and rapid growth/decline, providing the flexibility needed for recursive functions like intersection numbers.
- Core assumption: The recursive nature of intersection numbers requires an activation function that can simultaneously model periodicity, rapid growth, and provide a stable linear baseline.
- Evidence anchors:
  - [abstract] "we propose the Dynamic Range Activator (DRA), a new activation function that enhances the Transformer's ability to model recursive patterns and handle severe heteroscedasticity"
  - [section] "DRA integrates both harmonic and hyperbolic components, as follows, DRA(x) := x + a sin²(x/b) + c cos(bx) + d tanh(bx), where a, b, c, d are learnable parameters"
  - [corpus] Weak - no direct corpus evidence for DRA's effectiveness in recursive pattern capture
- Break condition: If DRA fails to improve performance on recursive functions with factorial growth or extreme value ranges, or if simpler activation functions match or exceed its performance.

### Mechanism 2
- Claim: Multi-modal learning with register tokens ([DYN]) and talking modalities improves out-of-distribution generalization.
- Mechanism: [DYN] tokens act as global information registers that attend to all input tokens, facilitating contrastive learning between modalities (B tensor and partitions d) through canonical correlation analysis.
- Core assumption: Sharing and integrating information across different modalities (continuous tensor vs discrete sets) enhances the model's ability to generalize beyond training distributions.
- Evidence anchors:
  - [abstract] "we incorporate uncertainty quantification into our model's predictions using Conformal Prediction with a dynamic sliding window that is aware of the number of marked points"
  - [section] "By attending to all tokens in the input, [DYN] tokens, similar to register tokens, facilitate a soft contrastive learning update, enabling them to learn a global context for each sample"
  - [corpus] Weak - no direct corpus evidence for multi-modal learning with register tokens in Transformers
- Break condition: If adding [DYN] tokens and talking modalities does not improve OOD performance or if it degrades ID performance significantly.

### Mechanism 3
- Claim: Transformers can implicitly learn Virasoro constraints and extract asymptotic parameters through abductive reasoning.
- Mechanism: The model's internal embeddings capture mathematical structures (Virasoro constraints) and parameter values (exponential growth constant A=2/3) through pattern recognition in the training data, enabling abductive hypothesis testing.
- Core assumption: The recursive relationships in intersection numbers encode Virasoro constraints, and the asymptotic behavior encodes specific parameter values that can be extracted from learned representations.
- Evidence anchors:
  - [abstract] "Our interpretability analysis reveals that the network is implicitly modeling the Virasoro constraints in a purely data-driven manner"
  - [section] "Looking at the cosine similarity between samples in the embedding space of the model's internal activations, shown in figure 6, we observed some interesting recursive patterns. We hypothesize that these patterns stem from the Virasoro constraints between intersection numbers"
  - [corpus] Weak - no direct corpus evidence for Transformers learning abstract mathematical constraints implicitly
- Break condition: If the model cannot reproduce known mathematical relationships or fails to extract parameter values within uncertainty bounds when tested on new data.

## Foundational Learning

- Concept: Recursive functions and their computational complexity
  - Why needed here: Intersection numbers require O(g! · 2ⁿ) computations, and understanding recursion is essential for modeling their behavior
  - Quick check question: What is the computational complexity of computing intersection numbers using topological recursion, and why does this make the problem challenging for traditional methods?

- Concept: Conformal prediction and uncertainty quantification
  - Why needed here: The wide numerical range (10⁻⁴⁵ to 10⁴⁵) and heteroscedastic nature of intersection numbers require robust uncertainty estimation
  - Quick check question: How does conformal prediction with dynamic sliding windows address heteroscedasticity in the residuals of predicted intersection numbers?

- Concept: Abductive reasoning and hypothesis testing
  - Why needed here: The paper aims to extract mathematical insights and parameter values from the model's internal representations through systematic hypothesis testing
  - Quick check question: What is the difference between deductive and abductive reasoning in the context of mathematical knowledge discovery from AI models?

## Architecture Onboarding

- Component map: B tensor -> Transformer -> [DYN] tokens -> Talking modalities -> PNA pooling -> DRA -> MLP -> Prediction
- Critical path: B tensor → Transformer → [DYN] tokens → Talking modalities → PNA pooling → DRA → MLP → Prediction
  The flow of information from the quantum Airy structure through the multi-modal learning mechanism to the final prediction
- Design tradeoffs:
  - Complexity vs performance: Adding DRA, [DYN] tokens, and talking modalities increases model complexity but enables better recursive pattern capture and OOD generalization
  - Computational cost vs accuracy: Conformal prediction provides uncertainty but adds computational overhead
  - Modality integration vs simplicity: Multi-modal learning enables better generalization but requires careful coordination between different input types
- Failure signatures:
  - Poor recursive pattern capture: R² drops significantly for higher genera, especially in OOD settings
  - Uncertainty underestimation: Empirical coverage falls far below target (e.g., below 50% when targeting 90%)
  - Modality misalignment: Talking modalities loss doesn't converge or causes performance degradation
  - Activation saturation: DRA parameters become extreme or cause numerical instability
- First 3 experiments:
  1. Baseline comparison: Train DynamicFormer without DRA, [DYN] tokens, and talking modalities on g≤14, test on g=15,16 to establish baseline performance
  2. DRA ablation: Compare DRA against Snake, GLU, and ReLU activation functions on recursive function datasets to demonstrate DRA's advantage in capturing recursive patterns
  3. Modality integration test: Train with and without [DYN] tokens and talking modalities on a subset of data to quantify their impact on OOD generalization and uncertainty estimation

## Open Questions the Paper Calls Out
None

## Limitations
- Mathematical foundation: Claims about implicitly learning Virasoro constraints lack rigorous proof and remain correlative rather than causal
- Out-of-distribution generalization: Limited test samples for g=15-16 require more extensive validation across broader parameter ranges
- Uncertainty quantification: Extreme heteroscedasticity (10^45 orders of magnitude) may challenge conformal prediction's effectiveness

## Confidence
- High Confidence: Core architectural innovations and in-distribution performance metrics are well-specified and reproducible
- Medium Confidence: Claims about abductive knowledge discovery and Virasoro constraint learning require further validation
- Low Confidence: Asymptotic parameter extraction (A=2/3) needs more extensive validation and mathematical verification

## Next Checks
1. Conduct rigorous mathematical tests to establish whether learned embeddings truly encode Virasoro constraints or merely capture statistical correlations
2. Expand out-of-distribution evaluation to include genus values beyond g=16 and systematically vary marked points to assess true generalization capabilities
3. Perform comprehensive ablation studies comparing DRA against other recursive-specialized activation functions across multiple benchmarks to establish relative advantage