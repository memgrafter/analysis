---
ver: rpa2
title: 'SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement
  Learning Systems'
arxiv_id: '2402.03741'
source_url: https://arxiv.org/abs/2402.03741
tags:
- uni00000013
- uni00000011
- uni00000014
- uni00000015
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SUB-PLAY, the first adversarial policy attack
  framework for partially observable multi-agent competitive environments. The core
  idea is to construct multiple subgames based on observable victim agents, train
  separate subpolicies for each subgame, and combine them using transition dissemination
  to improve performance.
---

# SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems
## Quick Facts
- arXiv ID: 2402.03741
- Source URL: https://arxiv.org/abs/2402.03741
- Reference count: 40
- Primary result: SUB-PLAY reduces victim performance to 51.98%-59.07% of baseline across three types of partial observability limitations

## Executive Summary
This paper introduces SUB-PLAY, the first adversarial policy attack framework specifically designed for partially observable multi-agent competitive environments (POMARL). The framework addresses the challenge that existing adversarial policy methods fail to account for partial observability in multi-agent settings. By constructing multiple subgames based on observable victim agents and training separate subpolicies for each, SUB-PLAY effectively exploits the limited information available to attackers in these environments.

## Method Summary
SUB-PLAY operates by first identifying observable victim agents and constructing corresponding subgames, then training individual subpolicies for each subgame. The framework employs a transition dissemination mechanism to improve subpolicy performance by sharing state transitions across subgames. Finally, an ensemble approach combines these subpolicies into a unified adversarial policy. The method is evaluated against both distributed (MADDPG) and centralized (QMIX) MARL algorithms across two benchmark environments, demonstrating consistent performance improvements over the state-of-the-art Victim-play method.

## Key Results
- SUB-PLAY reduces victim performance to 51.98%-59.07% of baseline across three partial observability limitations
- Outperforms Victim-play by 27.16%-50.22% on average across all attack types
- Demonstrates effectiveness in both Predator-prey and World Communication environments
- Maintains performance across different MARL algorithms (MADDPG, QMIX)

## Why This Works (Mechanism)
SUB-PLAY works by exploiting the partial observability constraints in multi-agent environments through strategic subgame construction. By identifying observable victim agents and creating separate subgames for each, the framework can train specialized policies that target specific information gaps in the victim's perception. The transition dissemination mechanism further enhances performance by allowing subpolicies to share learned experiences across subgames, effectively compensating for the limited observational data available in each individual subgame. This approach is particularly effective because it directly addresses the core challenge of POMARL systems: the attacker's inability to observe all agents simultaneously.

## Foundational Learning
- **POMARL (Partially Observable Multi-Agent Reinforcement Learning)**: MARL where agents have limited or incomplete observations of the environment and other agents. Needed because real-world multi-agent systems often have communication constraints or sensor limitations.
- **Adversarial Policy Attacks**: Techniques where an attacker trains policies to deceive or manipulate victim agents in multi-agent environments. Needed to understand the security vulnerabilities in MARL systems.
- **Subgame Construction**: The process of creating smaller, manageable game scenarios from larger multi-agent environments based on observable agents. Needed because training directly on full POMARL environments is computationally prohibitive.
- **Transition Dissemination**: A mechanism for sharing state transitions across multiple subgames to improve learning efficiency. Needed because individual subgames may have limited data for effective training.
- **Ensemble Methods**: Techniques for combining multiple trained policies into a single, more robust policy. Needed to integrate the specialized subpolicies into a cohesive adversarial strategy.

## Architecture Onboarding
**Component Map**: Victim Observation -> Subgame Construction -> Subpolicy Training -> Transition Dissemination -> Ensemble Integration -> Adversarial Policy

**Critical Path**: The framework's effectiveness depends on accurate identification of observable victim agents, followed by precise subgame construction. The critical path flows from victim observation through subgame construction to subpolicy training, with transition dissemination and ensemble integration serving as performance-enhancing but non-critical components.

**Design Tradeoffs**: The primary tradeoff is between attack effectiveness and computational overhead. While training multiple subpolicies increases computational cost, it enables more targeted and effective attacks against partially observable victims. The framework also trades off attack generalizability for specificity to the victim's observability constraints.

**Failure Signatures**: SUB-PLAY may fail when victim agents have highly dynamic observability patterns, when observable agents change frequently during deployment, or when the computational overhead of training multiple subpolicies becomes prohibitive for large-scale systems.

**3 First Experiments**:
1. Test SUB-PLAY's performance in a simplified POMARL environment with known observability constraints
2. Compare subpolicy performance with and without transition dissemination in controlled settings
3. Evaluate ensemble method sensitivity by testing different combination strategies

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Restricted to partially observable competitive environments with fixed team structures
- Focuses on discrete-action spaces with relatively simple grid-world environments
- Does not thoroughly analyze computational overhead for larger systems
- Limited evaluation of performance in continuous control domains or complex real-world applications

## Confidence
**High Confidence**: Empirical results demonstrating SUB-PLAY's effectiveness against Victim-play and technical soundness of subgame construction approach.

**Medium Confidence**: Claim of being "the first" adversarial policy framework for POMARL systems and generalizability beyond evaluated environments and algorithms.

## Next Checks
1. Evaluate SUB-PLAY's performance in continuous control environments with high-dimensional observation spaces
2. Conduct ablation studies to quantify individual contributions of subgame construction, transition dissemination, and ensemble methods
3. Test framework's effectiveness when attackers have imperfect knowledge of victim team structures or dynamic team compositions