---
ver: rpa2
title: 'WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts'
arxiv_id: '2401.17703'
source_url: https://arxiv.org/abs/2401.17703
tags:
- reasoning
- answer
- instances
- step
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Tree-of-Experts (ToE), a novel prompting method
  to generate WSC+ instances, achieving 50% valid cases compared to 10% in recent
  methods. WSC+ is a dataset of 3,026 LLM-generated sentences with traditional, ambiguous,
  and offensive categories.
---

# WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts

## Quick Facts
- arXiv ID: 2401.17703
- Source URL: https://arxiv.org/abs/2401.17703
- Reference count: 40
- Primary result: Tree-of-Experts (ToE) achieves 50% valid WSC+ instances vs 10% for recent methods

## Executive Summary
This paper introduces WSC+, an enhanced Winograd Schema Challenge dataset with 3,026 LLM-generated sentences across traditional, ambiguous, and offensive categories. The authors propose Tree-of-Experts (ToE), a novel prompting method that structures collaborative reasoning across simulated experts to improve WSC instance generation quality. GPT-4 achieves 68.7% accuracy on WSC+, significantly below the human benchmark of 95.1%, while also revealing interesting patterns in generation-evaluation consistency where models don't always perform best on their own generated questions.

## Method Summary
The study combines prompt engineering with human verification to create WSC+. The Tree-of-Experts (ToE) method uses multiple LLM "experts" that iteratively build, critique, and refine reasoning steps to consensus. Various prompt templates (ToE, ToT, CoT, CoE, NT) are tested with different query types (WDQ, WIQ). Generated instances undergo human verification and filtering to ensure quality. The resulting dataset is then evaluated across multiple LLM architectures (GPT-4, GPT-3.5, Claude2) using the ToE prompt template, with analysis focusing on accuracy, validity rates, and generation-evaluation consistency.

## Key Results
- Tree-of-Experts (ToE) achieves 50% valid WSC+ instances versus 10% for recent methods
- GPT-4 achieves 68.7% accuracy on WSC+, significantly below human benchmark of 95.1%
- Generation-evaluation consistency reveals models may not always perform best on self-generated questions (GPT-3.5 achieves only 36.1% on self-made instances)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree-of-Experts improves WSC+ generation by structuring collaborative reasoning across simulated experts
- Mechanism: Multiple LLM "experts" iteratively build, critique, and refine reasoning steps to consensus, reducing surface-level pattern matching
- Core assumption: Simulated expert collaboration produces more rigorous reasoning than sequential prompting
- Evidence anchors:
  - [abstract] "Tree-of-Experts, a novel prompting method which enhances the generation of WSC instances (50% valid cases vs. 10% in recent methods)"
  - [section 2.3.1] "Tree-of-Experts (ToE): Analogous to ToT but in the context of CoE. Here, the experts collaboratively critique each reasoning phase, not just the conclusion, ensuring unanimous agreement at each step"
  - [corpus] Weak - neighbor papers do not discuss ToE specifically
- Break condition: If experts reach consensus too quickly without substantive critique, the method collapses to simple prompting

### Mechanism 2
- Claim: WSC+ reveals LLM overconfidence and bias through ambiguous and offensive categories
- Mechanism: By requiring "neither" answers for ambiguous questions and testing biased reasoning in offensive contexts, WSC+ exposes models' tendencies to commit to incorrect answers
- Core assumption: Models will exhibit overconfidence in ambiguous situations and reveal biases in controversial contexts
- Evidence anchors:
  - [abstract] "We extend the WSC framework by incorporating new 'ambiguous' and 'offensive' categories, providing a deeper insight into model overconfidence and bias"
  - [section 2.1] "ambiguous' and 'offensive' categories, providing a deeper insight into model overconfidence and bias"
  - [corpus] Weak - neighbor papers focus on other WSC variants but not bias detection through category extension
- Break condition: If models consistently refuse to answer offensive questions, the bias detection mechanism fails

### Mechanism 3
- Claim: Generation-evaluation consistency reveals reasoning quality differences
- Mechanism: Comparing model performance on self-generated vs. other-generated instances exposes gaps in reasoning faithfulness
- Core assumption: Models should perform equally well on questions they generate versus those from other models if reasoning is consistent
- Evidence anchors:
  - [abstract] "Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models"
  - [section 4.2] "Generation-Evaluation Consistency... GPT-3.5 shows its lowest performance on its self-made instances, achieving only 36.1%"
  - [corpus] Weak - no direct corpus support for this specific consistency analysis
- Break condition: If all models perform equally well regardless of generation source, the consistency measure loses diagnostic value

## Foundational Learning

- Concept: Pronominal coreference resolution
  - Why needed here: WSC+ fundamentally tests models' ability to resolve pronouns to correct antecedents
  - Quick check question: In "John went to the store with Bob, but he forgot his wallet," who does "he" most likely refer to and why?

- Concept: Prompt engineering strategies (Chain-of-Thought, Tree-of-Thoughts, Chain-of-Experts)
  - Why needed here: Different prompting methods significantly affect WSC+ generation quality
  - Quick check question: How does Tree-of-Experts differ from Chain-of-Thought in its approach to reasoning?

- Concept: Bias detection and measurement in NLP
  - Why needed here: WSC+ includes offensive categories specifically designed to reveal model biases
  - Quick check question: What distinguishes an offensive WSC+ instance from a traditional one in terms of expected model behavior?

## Architecture Onboarding

- Component map: LLM selection → Prompt template application → Instance generation → Human verification → Model evaluation → Error analysis
- Critical path: Prompt template → Generation → Verification → Evaluation
- Design tradeoffs: Human verification ensures quality but limits scale; automated generation enables scale but risks quality issues
- Failure signatures: Low validity rates indicate prompt/template issues; high bias leakage suggests safety mechanism inadequacies
- First 3 experiments:
  1. Generate 100 WSC+ instances using each prompt template (CoT, ToT, CoE, ToE) with WDQ queries
  2. Evaluate generated instances for validity and bias leakage
  3. Test model consistency by having each LLM evaluate its own generated instances versus others'

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could enable LLMs to better handle ambiguous WSC+ instances?
- Basis in paper: Explicit
- Why unresolved: The paper identifies ambiguity misinterpretation as a major challenge but does not explore potential solutions beyond prompt engineering.
- What evidence would resolve it: Experiments comparing LLM performance on ambiguous instances before and after targeted architectural changes (e.g., uncertainty quantification modules) or fine-tuning on ambiguity-specific datasets.

### Open Question 2
- Question: How does model size influence generation-evaluation consistency across different WSC+ categories?
- Basis in paper: Explicit
- Why unresolved: While the paper notes consistency issues, it does not systematically analyze how model scale affects this phenomenon.
- What evidence would resolve it: Comparative studies of small, medium, and large models' performance on self-generated versus externally-generated WSC+ instances across all categories.

### Open Question 3
- Question: What underlying mechanisms cause LLMs to exhibit improved performance on offensive WSC+ instances compared to ambiguous ones?
- Basis in paper: Explicit
- Why unresolved: The paper observes this performance discrepancy but does not investigate the cognitive or statistical reasons behind it.
- What evidence would resolve it: Analysis of attention patterns, token probabilities, or internal representations when models process offensive versus ambiguous instances with identical syntactic structures.

## Limitations

- The implementation details of Tree-of-Experts remain underspecified, making exact replication challenging
- Human verification ensures quality but limits scalability and may introduce unknown selection biases
- The counterintuitive generation-evaluation consistency findings lack robust theoretical grounding
- Claims about bias detection rely on subjective human classification criteria that are not fully specified

## Confidence

**High Confidence**: The overall framework for extending WSC with ambiguous and offensive categories is well-established and the reported human benchmark of 95.1% accuracy provides a solid reference point.

**Medium Confidence**: The 50% validity rate for Tree-of-Experts generation versus 10% for recent methods is promising but depends heavily on the specific implementation details that are not fully disclosed.

**Low Confidence**: The claim that ambiguous and offensive categories reveal specific patterns of overconfidence and bias relies on subjective human classification criteria that are not fully specified.

## Next Checks

1. **Implementation Replication**: Attempt to recreate the Tree-of-Experts prompting structure using the provided examples and document any deviations or ambiguities in the implementation that affect validity rates.

2. **Cross-Model Consistency**: Test whether the generation-evaluation consistency pattern holds when using different LLM architectures (beyond GPT-4, GPT-3.5, and Claude2) to determine if this is a general phenomenon or model-specific.

3. **Category Bias Analysis**: Conduct a detailed error analysis on the ambiguous and offensive categories to determine whether the observed model behavior patterns are consistent with known bias types or represent new failure modes in LLM reasoning.