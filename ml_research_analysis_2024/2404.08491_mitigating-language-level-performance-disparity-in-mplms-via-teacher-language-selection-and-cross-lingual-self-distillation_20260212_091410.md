---
ver: rpa2
title: Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language
  Selection and Cross-lingual Self-Distillation
arxiv_id: '2404.08491'
source_url: https://arxiv.org/abs/2404.08491
tags:
- languages
- performance
- alsace
- language
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ALSACE, a method that reduces language-level
  performance disparity in multilingual pretrained language models (mPLMs) by selecting
  appropriate teacher languages and performing cross-lingual self-distillation. The
  approach identifies reliable teacher languages for each task using pseudo-label
  consensus and average confidence scores, then transfers knowledge from these teachers
  to other languages via consistency loss on parallel sentence pairs.
---

# Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation

## Quick Facts
- arXiv ID: 2404.08491
- Source URL: https://arxiv.org/abs/2404.08491
- Authors: Haozhe Zhao; Zefan Cai; Shuzheng Si; Liang Chen; Yufeng He; Kaikai An; Baobao Chang
- Reference count: 19
- One-line primary result: ALSACE reduces cross-lingual performance gaps by 10-20% while maintaining competitive accuracy

## Executive Summary
This paper introduces ALSACE, a method that reduces language-level performance disparity in multilingual pretrained language models (mPLMs) by selecting appropriate teacher languages and performing cross-lingual self-distillation. The approach identifies reliable teacher languages for each task using pseudo-label consensus and average confidence scores, then transfers knowledge from these teachers to other languages via consistency loss on parallel sentence pairs. Experiments show ALSACE improves cross-lingual transferability across XLM-R and mT5 models on XNLI, PAWS-X, and XCOPA benchmarks, reducing performance gaps between high- and low-resource languages while maintaining competitive accuracy compared to state-of-the-art methods that use large parallel corpora. The method also enhances both language-agnostic and language-specific knowledge in mPLMs.

## Method Summary
ALSACE addresses language-level performance disparity in mPLMs through a two-stage approach. First, it fine-tunes on English training data, then generates pseudo-labels via majority voting across all languages. Teacher languages are selected based on average confidence scores on these pseudo-labels. Second, it applies cross-lingual self-distillation using consistency loss (KL divergence) between teacher and student languages on parallel sentence pairs, constructed using 500-shot unlabeled data generated via Supergen and machine translation. This approach eliminates the need for additional labeled multilingual data while effectively mitigating performance disparities across languages.

## Key Results
- ALSACE reduces cross-lingual performance gaps by 10-20% across XNLI, PAWS-X, and XCOPA benchmarks
- Improves language-specific knowledge probing accuracy on GeoMLAMA tasks
- Maintains competitive overall accuracy compared to methods using large parallel corpora
- Outperforms random teacher selection and direct multilingual fine-tuning baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teacher Language Selection improves distillation quality by matching culturally/linguistically proximate teachers to students
- Mechanism: The method computes pseudo-labels via majority voting across all languages, then measures each language's confidence on these pseudo-labels. Languages with higher average confidence become teachers, capturing the intuition that linguistically closer languages produce more reliable supervision signals
- Core assumption: Average confidence scores on pseudo-labels correlate with a language's suitability as a teacher for other languages in the task
- Evidence anchors:
  - [abstract] "we introduce Teacher Language Selection to choose reliable teacher languages for a specific task to supervise the student languages"
  - [section] "we employ a majority vote strategy to generate pseudo-labels derived from the consensus of the mPLMs' predictions across different languages... utilize the average confidence score... as the indicator to select the teacher languages"
  - [corpus] "TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models" suggests script differences affect cross-lingual transfer quality
- Break condition: If confidence scores don't correlate with actual teacher quality, random selection might perform equally well

### Mechanism 2
- Claim: Cross-lingual Self-Distillation transfers both language-agnostic and language-specific knowledge
- Mechanism: Consistency loss via KL divergence between teacher and student language predictions on parallel sentence pairs encourages the model to align distributions, effectively transferring knowledge encoded in the teacher language representations
- Core assumption: Parallel sentence pairs maintain semantic equivalence while preserving language-specific characteristics that can be transferred
- Evidence anchors:
  - [abstract] "we further propose Cross-Lingual Self-Distillation to leverage the knowledge from each selected teacher language to supervise other languages"
  - [section] "employs a consistency loss that encourages closer alignment between the model output distributions of each reliable teacher language and other languages"
  - [corpus] "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic Alignment for Low-Resource Languages" indicates mPLMs can capture language-specific temporal semantics
- Break condition: If teacher and student languages have fundamentally different knowledge distributions, consistency loss may not bridge the gap

### Mechanism 3
- Claim: The two-stage approach (selection + distillation) outperforms direct multilingual fine-tuning by preserving and transferring pre-trained knowledge
- Mechanism: Stage 1 identifies which languages have encoded relevant knowledge for the task; Stage 2 transfers this knowledge without requiring additional labeled multilingual data, preserving the pre-training stage's language-agnostic and language-specific knowledge
- Core assumption: Pre-training knowledge differences across languages create performance disparities that can be mitigated through targeted knowledge transfer
- Evidence anchors:
  - [abstract] "eliminating the need for additional labeled multilingual data" and "effectively mitigates language-level performance disparity"
  - [section] "we attempt to address language-level performance disparity from the knowledge disparity perspective" and "enables effective knowledge transfer between different languages instead of only transferring knowledge from high-resource to low-resource languages"
  - [corpus] "LangSAMP: Language-Script Aware Multilingual Pretraining" suggests language embeddings can encode important language-specific information
- Break condition: If pre-training knowledge is already uniformly distributed across languages, additional distillation provides minimal benefit

## Foundational Learning

- Concept: Kullback-Leibler divergence as a consistency loss
  - Why needed here: KL divergence measures the difference between probability distributions, making it suitable for encouraging teacher and student predictions to align during knowledge distillation
  - Quick check question: If P is the teacher distribution and Q is the student distribution, what does KL(P||Q) measure and how does minimizing it help knowledge transfer?

- Concept: Pseudo-label generation via majority voting
  - Why needed here: Majority voting across multiple languages creates more reliable pseudo-labels than relying on any single language, reducing noise in the teacher selection process
  - Quick check question: Why might majority voting produce more reliable pseudo-labels than using the highest-confidence prediction from a single language?

- Concept: Parallel corpus construction for cross-lingual tasks
  - Why needed here: Cross-lingual self-distillation requires aligned sentence pairs across languages to ensure the knowledge being transferred is semantically equivalent
  - Quick check question: What are the key requirements for parallel corpora used in cross-lingual self-distillation compared to standard machine translation tasks?

## Architecture Onboarding

- Component map: English fine-tuning -> Pseudo-label generation -> Teacher selection -> Parallel corpus construction -> Consistency loss application -> Model update
- Critical path: 1. Fine-tune on English training data 2. Generate pseudo-labels via majority voting 3. Compute confidence scores and select teachers 4. Construct parallel sentence pairs 5. Apply consistency loss between teachers and students 6. Evaluate cross-lingual performance
- Design tradeoffs:
  - Parallel data quantity vs. quality: 500-shot parallel data balances cost and effectiveness
  - Teacher selection threshold: too low includes poor teachers, too high excludes potentially useful ones
  - Consistency loss weight: needs balancing with task loss to avoid over-regularization
- Failure signatures:
  - Cross-lingual transfer gaps don't decrease: likely teacher selection or distillation configuration issue
  - Performance degrades on high-resource languages: may indicate over-regularization or poor teacher selection
  - Student languages don't improve: parallel data quality or consistency loss implementation may be problematic
- First 3 experiments:
  1. Baseline comparison: Run XNLI with XLM-R base model to establish baseline cross-lingual transfer gaps
  2. Teacher selection ablation: Compare ALSACE with random teacher selection to validate the selection mechanism
  3. Distillation strength sweep: Vary the weight of consistency loss to find optimal trade-off between preservation and transfer

## Open Questions the Paper Calls Out
The paper identifies three main open questions: 1) How does ALSACE perform when scaled to more than 15 languages and what is the relationship between the number of languages and performance gains? 2) What is the optimal strategy for selecting teacher languages when dealing with extremely low-resource languages like Kaixana or Ainu? 3) How can we develop a metric that better reflects performance equity across languages rather than just reducing cross-lingual transfer gaps?

## Limitations
- Limited to 15 languages in experiments, not representative of full mPLM coverage
- Relies on 500-shot parallel data which may not capture sufficient linguistic diversity for extremely low-resource languages
- Teacher selection mechanism's effectiveness depends on correlation between confidence scores and actual teacher quality, which is not empirically validated

## Confidence
- High confidence in overall framework design and experimental methodology
- Medium confidence in teacher selection mechanism's effectiveness
- Low confidence in generalizability to truly low-resource languages with minimal parallel data

## Next Checks
1. **Teacher Quality Correlation Analysis**: Conduct systematic experiments to measure the actual correlation between teacher selection confidence scores and downstream distillation performance across diverse language pairs
2. **Low-Resource Language Generalization**: Test ALSACE on languages with minimal parallel data availability (< 500 sentences) to evaluate effectiveness as parallel data becomes increasingly scarce
3. **Teacher Selection Ablation Study**: Compare ALSACE against multiple alternative teacher selection strategies including random selection, confidence-based selection without majority voting, and linguistically-motivated selection based on language family or script similarity