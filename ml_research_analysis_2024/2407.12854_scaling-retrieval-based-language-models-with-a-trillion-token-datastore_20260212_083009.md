---
ver: rpa2
title: Scaling Retrieval-Based Language Models with a Trillion-Token Datastore
arxiv_id: '2407.12854'
source_url: https://arxiv.org/abs/2407.12854
tags:
- datastore
- data
- documents
- scaling
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores scaling retrieval-based language models by
  increasing the size of the datastore used at inference time. The authors construct
  MASSIVE DS, a 1.4 trillion-token datastore spanning eight diverse domains, and design
  an efficient pipeline to study datastore scaling computationally.
---

# Scaling Retrieval-Based Language Models with a Trillion-Token Datastore

## Quick Facts
- arXiv ID: 2407.12854
- Source URL: https://arxiv.org/abs/2407.12854
- Reference count: 40
- Key outcome: Larger retrieval datastores consistently improve language modeling and downstream task performance

## Executive Summary
This paper investigates scaling retrieval-based language models by increasing datastore size, constructing MASSIVE DS, a 1.4 trillion-token datastore spanning eight diverse domains. The authors develop an efficient computational pipeline to study datastore scaling effects and demonstrate that larger datastores monotonically improve both language modeling perplexity and downstream task performance, particularly on knowledge-intensive question answering. Remarkably, smaller retrieval-augmented models can outperform larger LM-only models on these tasks. The study reveals that compute-optimal scaling curves show retrieval-based LMs achieve superior performance within the same training budget by offloading memorization to the datastore, establishing datastore size as a critical factor in language model efficiency and performance.

## Method Summary
The authors constructed MASSIVE DS, a 1.4 trillion-token datastore spanning eight diverse domains including books, news, scientific papers, and web data. They developed an efficient pipeline to study datastore scaling computationally by using smaller subsets of the full datastore for experiments. The study employed CONTRIEVER-MS-MARCO as the retriever and used HELM as the primary evaluation framework, conducting experiments across multiple model scales and datastore sizes while carefully addressing data contamination issues through rigorous filtering and detection methods.

## Key Results
- Increasing datastore size monotonically improves language modeling perplexity and downstream task performance
- Smaller retrieval-augmented models can outperform larger LM-only models on knowledge-intensive tasks
- Compute-optimal scaling curves show retrieval-based LMs achieve superior performance within the same training budget by offloading memorization to the datastore

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling the retrieval datastore improves both language modeling perplexity and downstream task performance.
- Mechanism: As datastore size increases, the retriever has a higher probability of finding relevant documents, which enhances the language model's ability to generate more accurate and coherent text. This effect is particularly pronounced for knowledge-intensive tasks where factual recall is crucial.
- Core assumption: The retriever can effectively identify relevant documents within the larger datastore, and the language model can effectively utilize the retrieved information.
- Evidence anchors:
  - [abstract] "increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks"
  - [section 4.2] "Retrieval is strictly beneficial for language modeling: the LM-only baselines show the highest perplexity across all models and evaluation datasets."

### Mechanism 2
- Claim: Using larger datastores can significantly improve model performance for the same training compute budget.
- Mechanism: Indexing a datastore is cheaper than training on the same amount of data. By offloading the memorization of factual knowledge to the datastore, retrieval-based LMs achieve better performance within the same training budget.
- Core assumption: The cost of indexing a datastore is significantly lower than the cost of training a model on the same amount of data.
- Evidence anchors:
  - [abstract] "By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget."
  - [section 4.3] "Since indexing a datastore is cheaper than training on the same amount of data, retrieval-based LMs enable better compute-optimal scaling trends."

### Mechanism 3
- Claim: Retrieval-based LMs are capable of automatically retrieving documents that are in-domain to the query.
- Mechanism: The retriever, trained on a diverse dataset, learns to identify and retrieve documents relevant to the query's domain, even when the datastore contains out-of-domain data. This allows retrieval-based LMs to benefit from larger, broader datastores.
- Core assumption: The retriever is trained on a sufficiently diverse dataset to learn domain-specific retrieval patterns.
- Evidence anchors:
  - [section 5.1] "we show that retrieval-based LMs are capable of automatically retrieving documents that are in-domain to the query, which allows them to reap the benefits of larger, broader datastores."

## Foundational Learning

- Concept: Retrieval-based language models (LM)
  - Why needed here: Understanding the core concept of retrieval-based LMs is essential to grasp how datastore scaling impacts their performance.
  - Quick check question: What is the key difference between a retrieval-based LM and a standard LM?

- Concept: Compute-optimal scaling
  - Why needed here: This concept is crucial for understanding how retrieval-based LMs can achieve better performance within the same training budget by utilizing larger datastores.
  - Quick check question: How does the cost of indexing a datastore compare to the cost of training a model on the same amount of data?

- Concept: Data contamination
  - Why needed here: Data contamination is a significant concern in retrieval-based LMs, and understanding how to mitigate it is crucial for evaluating their performance accurately.
  - Quick check question: What are some common methods for detecting and removing data contamination in retrieval-based LMs?

## Architecture Onboarding

- Component map:
  Retriever -> Datastore -> Language Model -> Data filtering

- Critical path:
  1. Index documents in the datastore.
  2. For a given query, retrieve relevant documents from the datastore.
  3. Concatenate the retrieved documents with the query and feed them to the language model.
  4. Generate text based on the concatenated input.

- Design tradeoffs:
  - Larger datastores offer better performance but require more storage and retrieval time.
  - More complex retrievers may improve retrieval quality but increase computational cost.
  - Stricter data filtering reduces contamination but may also remove useful information.

- Failure signatures:
  - Degraded language modeling perplexity despite increasing datastore size.
  - No improvement or decline in downstream task performance.
  - High latency or memory usage due to large datastore size.

- First 3 experiments:
  1. Evaluate language modeling perplexity on a held-out dataset with varying datastore sizes.
  2. Measure downstream task performance on a knowledge-intensive task with different datastore sizes.
  3. Compare the compute cost of indexing a datastore versus training a model on the same amount of data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of retriever architecture (e.g., dense vs. sparse) impact the scaling benefits observed with large datastores?
- Basis in paper: [explicit] The paper mentions ablating the retriever choice and finding that CONTRIEVER-MS-MARCO performs on par with or better than more recent larger retrievers, but does not explore the impact of different retriever architectures on scaling trends.
- Why unresolved: The paper only uses a single retriever (CONTRIEVER-MS-MARCO) in the full-scale scaling study due to computational constraints, limiting the generalizability of findings to other retriever architectures.
- What evidence would resolve it: Systematic comparison of scaling trends using different retriever architectures (dense, sparse, hybrid) on the same datastore and model configurations, measuring both retrieval quality and downstream task performance.

### Open Question 2
- Question: What is the optimal trade-off between datastore size and data quality filtering for maximizing performance gains?
- Basis in paper: [inferred] The paper explores data quality filtering (deduplication, decontamination, language/whitespace filters) but finds limited impact on performance, suggesting that the optimal balance between datastore size and data quality is unclear.
- Why unresolved: The paper only tests a limited set of quality filters adapted from DOLMA, and does not explore more computationally expensive but potentially higher-quality filters or different filtering strategies.
- What evidence would resolve it: Comprehensive ablation study comparing various combinations of data quality filters and datastore sizes, measuring their impact on both language modeling perplexity and downstream task performance across different model scales.

### Open Question 3
- Question: How do compute-optimal scaling trends change when considering inference cost alongside training cost?
- Basis in paper: [explicit] The paper acknowledges that inference cost increases with retrieval due to longer context length and additional search computation, but defers a study of inference-compute-optimal scaling to future work.
- Why unresolved: The current analysis focuses solely on training-time compute, which may not reflect the true cost-benefit trade-offs when retrieval is used in practice.
- What evidence would resolve it: Plot compute-optimal scaling curves that incorporate both training and inference costs, comparing retrieval-based LMs with LM-only models under different inference serving strategies (e.g., caching, pruning, quantization).

## Limitations
- Data contamination detection reliability is imperfect and may lead to overestimation of improvements
- Domain-specific generalizability beyond knowledge-intensive question answering is unclear
- Computational cost tradeoffs between datastore indexing, retrieval, and model inference need more detailed analysis

## Confidence

- **High Confidence**: The core finding that larger datastores improve language modeling perplexity is supported by extensive empirical evidence across multiple evaluation datasets and model sizes.
- **Medium Confidence**: The claim about compute-optimal scaling advantages requires careful interpretation due to varying deployment scenarios and infrastructure costs.
- **Medium Confidence**: The ability of retrieval-based LMs to automatically retrieve in-domain documents is demonstrated empirically but needs further validation across different domains.

## Next Checks
1. Test scaling laws on a broader range of downstream tasks beyond knowledge-intensive question answering, including creative writing, code generation, and reasoning tasks.
2. Conduct a comprehensive cost analysis comparing total system costs (datastore indexing + retrieval operations + model inference) against traditional pretraining approaches across different deployment scales.
3. Systematically test how different contamination detection methods and thresholds affect observed scaling trends by intentionally introducing controlled contamination at varying levels.