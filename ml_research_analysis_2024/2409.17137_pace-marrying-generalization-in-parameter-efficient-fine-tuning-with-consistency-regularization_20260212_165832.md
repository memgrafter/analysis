---
ver: rpa2
title: 'PACE: Marrying generalization in PArameter-efficient fine-tuning with Consistency
  rEgularization'
arxiv_id: '2409.17137'
source_url: https://arxiv.org/abs/2409.17137
tags:
- pace
- gradient
- generalization
- learning
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PACE improves parameter-efficient fine-tuning by reducing weight
  gradient norms and aligning fine-tuned models with pre-trained ones. It uses multiplicative
  noise on adapter features and consistency regularization to implicitly regularize
  gradients and retain pre-training knowledge.
---

# PACE: Marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization

## Quick Facts
- arXiv ID: 2409.17137
- Source URL: https://arxiv.org/abs/2409.17137
- Authors: Yao Ni; Shan Zhang; Piotr Koniusz
- Reference count: 40
- PACE improves PEFT by reducing gradient norms and aligning fine-tuned models with pre-trained ones using multiplicative noise and consistency regularization

## Executive Summary
PACE addresses the challenge of parameter-efficient fine-tuning (PEFT) by combining multiplicative noise perturbations with consistency regularization. The method implicitly regularizes gradients during training while maintaining alignment with pre-trained models, leading to improved generalization across vision and text tasks. By reducing gradient norms and preserving pre-training knowledge, PACE outperforms existing PEFT methods on benchmarks including VTAB-1k, FGVC, few-shot learning, domain adaptation, GLUE, and GSM-8K.

## Method Summary
PACE introduces consistency regularization to LoRA and VPT adapter-based PEFT methods by perturbing adapter features with multiplicative noise and enforcing output consistency across perturbations. This approach implicitly reduces first- and second-order gradients (improving generalization) while maintaining alignment with pre-trained models (preserving knowledge). The method requires tuning two hyperparameters: λ (regularization strength) and σ (perturbation magnitude), with performance demonstrated across vision adaptation tasks and text classification benchmarks.

## Key Results
- PACE surpasses existing PEFT methods on visual adaptation tasks including VTAB-1k, FGVC, few-shot learning, and domain adaptation
- PACE improves LoRA performance on text classification (GLUE) and mathematical reasoning (GSM-8K)
- Theoretical analysis confirms PACE benefits for generalization and knowledge retention through gradient regularization and model alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PACE reduces weight gradient norms during training, leading to improved generalization
- Mechanism: PACE perturbs adapter features with multiplicative noise and enforces consistency regularization across perturbations, implicitly penalizing first- and second-order gradients
- Core assumption: The consistency regularization loss can be approximated by terms involving gradient and Hessian of model output
- Evidence anchors: Abstract states PACE "implicitly regularizes gradients for enhanced generalization"; Theorem 2 shows consistency loss penalizes first- and second-order gradients
- Break condition: If multiplicative noise perturbation fails to span weight space or consistency loss becomes too dominant

### Mechanism 2
- Claim: PACE aligns fine-tuned model with pre-trained model, retaining knowledge from large-scale pre-training
- Mechanism: Minimizing consistency regularization loss implicitly reduces FP-distance (output distance between fine-tuned and pre-trained models), keeping model close to pre-trained one
- Core assumption: FP-distance can be upper-bounded by terms involving gradient and Hessian, which are reduced by consistency regularization
- Evidence anchors: Abstract states PACE "implicitly aligns the fine-tuned and pre-trained models to retain knowledge"; Theorem 3 establishes relationship between FP-distance and consistency loss
- Break condition: If pre-trained model knowledge is irrelevant to downstream task or consistency regularization is too weak

### Mechanism 3
- Claim: PACE outperforms other PEFT methods by combining gradient regularization and model alignment
- Mechanism: PACE addresses naive alignment limitations (doesn't guarantee gradient reduction, can cause gradient explosion) by using multiplicative noise and consistency regularization
- Core assumption: Combination of gradient regularization and model alignment is more effective than either approach alone
- Evidence anchors: Abstract states PACE "surpasses existing PEFT methods in visual adaptation tasks"; Section 3.4 explains how PACE addresses naive alignment limitations
- Break condition: If downstream task requires significant deviation from pre-trained model or consistency regularization is too strong

## Foundational Learning

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: PACE is a PEFT method improving generalization while retaining pre-trained knowledge
  - Quick check question: What are main challenges of traditional fine-tuning methods, and how does PEFT address them?

- Concept: Consistency Regularization
  - Why needed here: PACE uses consistency regularization to enforce invariance of model output to same input under varying perturbations
  - Quick check question: How does consistency regularization differ from other regularization techniques, and what are its key benefits?

- Concept: Multiplicative Noise Perturbation
  - Why needed here: PACE perturbs adapter features with multiplicative noise to create diverse perturbations for consistency regularization
  - Quick check question: Why is multiplicative noise preferred over additive noise in this context, and what are implications for perturbation space?

## Architecture Onboarding

- Component map: Pre-trained transformer model -> Adapter modules -> Multiplicative noise perturbation layer -> Consistency regularization loss -> Classification head
- Critical path: 1) Input processed through pre-trained transformer model 2) Adapter modules applied to transformer outputs 3) Multiplicative noise added to adapter outputs 4) Consistency regularization loss computed between perturbed and unperturbed outputs 5) Gradients backpropagated to update adapter parameters
- Design tradeoffs:
  - Perturbation strength (σ): Higher σ leads to stronger regularization but may slow convergence
  - Regularization strength (λ): Higher λ emphasizes consistency but may limit model flexibility
  - Adapter architecture: Different adapter designs may affect PACE effectiveness
- Failure signatures:
  - Gradient explosion: If λ is too high or perturbation too strong, gradient norms may become unstable
  - Over-regularization: If λ or σ is too high, model may become too constrained and fail to adapt
  - Under-regularization: If λ or σ is too low, benefits of PACE may be minimal
- First 3 experiments:
  1. Compare baseline PEFT method (e.g., LoRA) with PACE on simple image classification task (e.g., CIFAR-10)
  2. Analyze effect of different perturbation strengths (σ) on model performance and gradient norms
  3. Evaluate impact of varying regularization strengths (λ) on model accuracy and generalization

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, based on the methodology and results, several important questions arise regarding scalability, long-term effects on evolving datasets, and multi-modal data handling.

## Limitations

- Theoretical bounds vs practical gains: While mathematical justification exists for gradient regularization and model alignment, practical impact depends heavily on specific downstream tasks and pre-trained models
- Limited empirical scope: Paper evaluates primarily on visual adaptation tasks with only brief mentions of text classification and mathematical reasoning
- Hyperparameter sensitivity: PACE introduces two new hyperparameters requiring careful tuning, with performance sensitive to these choices

## Confidence

- High confidence: Core mechanism of PACE (multiplicative noise + consistency regularization) is technically sound and aligns with established regularization principles
- Medium confidence: PACE improves upon naive model alignment by simultaneously regularizing gradients and retaining pre-training knowledge
- Medium confidence: PACE's benefits generalize across different PEFT architectures and task domains, though more extensive validation is needed

## Next Checks

1. **Ablation on gradient norms**: Systematically measure first- and second-order gradients across different PEFT methods with and without PACE to verify claimed gradient regularization effects

2. **Cross-domain generalization test**: Evaluate PACE on broader range of tasks including natural language understanding, code generation, and specialized domains like medical imaging

3. **Runtime and memory efficiency analysis**: Compare wall-clock training time, memory usage, and convergence speed of PACE versus baseline PEFT methods across different batch sizes and sequence lengths