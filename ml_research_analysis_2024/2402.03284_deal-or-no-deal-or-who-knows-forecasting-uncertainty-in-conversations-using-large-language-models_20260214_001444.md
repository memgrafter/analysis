---
ver: rpa2
title: Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using
  Large Language Models
arxiv_id: '2402.03284'
source_url: https://arxiv.org/abs/2402.03284
tags:
- data
- language
- forecasts
- uncertainty
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task called FortUne Dial to evaluate
  language models' ability to represent uncertainty about future conversation outcomes.
  The key idea is to use uncertainty-aware metrics like Brier score and skill score
  that allow models to abstain from predicting on instances when they estimate high
  uncertainty.
---

# Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models

## Quick Facts
- arXiv ID: 2402.03284
- Source URL: https://arxiv.org/abs/2402.03284
- Reference count: 40
- Key outcome: Introduces FortUne Dial task to evaluate language models' ability to represent uncertainty about future conversation outcomes using uncertainty-aware metrics like Brier score and skill score.

## Executive Summary
This paper introduces a new task called FortUne Dial to evaluate language models' ability to represent uncertainty about future conversation outcomes. The key idea is to use uncertainty-aware metrics like Brier score and skill score that allow models to abstain from predicting on instances when they estimate high uncertainty. Two methods are proposed to represent uncertainty: implicit forecasts using token scores and direct forecasts using output tokens. Fine-tuning strategies including supervised learning and off-policy reinforcement learning are introduced to improve calibration of both representations. Experiments on eight negotiation datasets show that the proposed fine-tuning strategies can calibrate smaller open-source models to outperform pre-trained models 10x their size.

## Method Summary
The paper proposes two methods for representing uncertainty in conversation outcomes: implicit forecasts (using token scores) and direct forecasts (using output tokens). Fine-tuning strategies are developed to improve calibration of both representations. For implicit forecasts, supervised learning with prompt tuning is used, while for direct forecasts, an off-policy reinforcement learning approach is employed to preserve pre-trained models' predispositions to express uncertainty directly. A unified post-hoc correction technique is also proposed to improve calibration by estimating and scaling underlying latent scores. The methods are evaluated on eight negotiation datasets using uncertainty-aware metrics like Brier score and skill score.

## Key Results
- Two methods (implicit and direct forecasts) effectively represent uncertainty in conversation outcomes
- Fine-tuning strategies improve calibration, allowing smaller models to outperform larger pre-trained models by 10x
- GPT-4 can anticipate outcome certainty with up to 9% improvement over prior knowledge
- Uncertainty-aware metrics (Brier score, skill score) provide more nuanced evaluation than traditional accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1
Large language models can anticipate outcome certainty in conversations by representing uncertainty through either implicit forecasts (using token scores) or direct forecasts (using output tokens). The models generate probability estimates either by interpreting the likelihood from the full token distribution (implicit) or by directly outputting a probability in natural language (direct). Fine-tuning strategies like supervised learning and off-policy reinforcement learning improve calibration of these uncertainty representations.

### Mechanism 2
Post-hoc correction techniques can improve the calibration of both implicit and direct forecast representations by estimating and scaling the underlying latent scores. The correction technique estimates the latent score from the forecast probability, scales it using temperature and bias correction terms, and then converts it back to a calibrated probability.

### Mechanism 3
Uncertainty tuning of direct forecasts using off-policy reinforcement learning can preserve and capitalize on pre-trained models' predispositions to express uncertainty directly via output tokens. The direct forecast tuning is formulated as a Markov Decision Process where the reward is the negative proper scoring function of the forecast.

## Foundational Learning

- **Concept**: Calibration of probability estimates
  - Why needed here: The core task is to have the model output well-calibrated probability estimates for conversation outcomes. If the model is not calibrated, its probability estimates will not reflect the true likelihood of outcomes.
  - Quick check question: If a model is perfectly calibrated, what should be true about its probability estimates? (Answer: The observed frequency of an outcome should match the model's predicted probability for that outcome)

- **Concept**: Proper scoring rules
  - Why needed here: Proper scoring rules are used to evaluate and optimize the quality of probabilistic forecasts. They encourage the model to output well-calibrated and informative probability estimates.
  - Quick check question: What is the key property of a proper scoring rule that makes it useful for training calibrated models? (Answer: It is maximized when the predicted probability matches the true probability of the outcome)

- **Concept**: Domain generalization
  - Why needed here: The model is trained on a set of negotiation datasets but needs to generalize to new, unseen datasets. Domain generalization techniques are needed to ensure good performance on out-of-distribution data.
  - Quick check question: What is the main challenge in domain generalization that this work needs to address? (Answer: The model needs to learn features that are invariant across different domains and can generalize to new domains)

## Architecture Onboarding

- **Component map**: Language Model (LM) -> Prompting Strategy (implicit or direct) -> Fine-tuning Strategy (supervised or RL) -> Post-hoc Correction -> Evaluation Metrics (Brier score, skill score)

- **Critical path**: Input dialogue → Prompting strategy → LM generation → Probability extraction/parsing → Post-hoc correction (if used) → Final calibrated probability estimate

- **Design tradeoffs**: The main design tradeoffs are between implicit and direct forecasting, and between different fine-tuning strategies. Implicit forecasting may be more general but requires interpreting the token distribution, while direct forecasting may be more interpretable but requires the model to output probabilities in natural language.

- **Failure signatures**: If the model fails to generate calibrated probability estimates, it could be due to: 1) the prompting strategy not eliciting the right uncertainty representation, 2) the fine-tuning strategy not improving calibration, 3) the post-hoc correction technique not working well for the forecast distribution, or 4) the model not understanding the task or data.

- **First 3 experiments**:
  1. Test the prompting strategies (implicit vs. direct) on a small dataset to see which one elicits better uncertainty representations from the LM.
  2. Fine-tune the LM using supervised learning on a larger dataset and evaluate the calibration using proper scoring rules. Compare to the pre-trained LM.
  3. Apply the post-hoc correction technique to the fine-tuned LM and evaluate the improvement in calibration. Try different correction parameters to find the best ones.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the proposed "FortUne Dial" framework generalize to languages other than English, and what are the potential performance degradation issues? The paper mentions limitations regarding the generalization of findings to languages other than English, noting evidence that uncertainty representations may experience performance degradation when applied to other languages, especially low-resource ones.

- **Open Question 2**: How does the proposed framework differentiate between causation and correlation in language models' ability to anticipate social uncertainties in conversations? The paper acknowledges that there is no clear way to separate causation and correlation in the task, noting that language models may capitalize on "superficial" or "spurious" statistical correlations associated with outcomes.

- **Open Question 3**: What are the potential biases in the proposed framework, and how can they be mitigated to ensure fairness and robustness in critical applications? The paper mentions limitations related to biases in the training data, robustness to adversaries and natural token perturbations, and the need for ethical and safety considerations in critical applications.

## Limitations

- The evaluation is limited to eight negotiation corpora with binary outcomes, which may not represent the broader challenge of uncertainty representation in diverse conversational domains.
- The paper lacks complete specification of prompt templates and system prompts used across different models and datasets, making exact reproduction challenging.
- The effectiveness of the post-hoc correction technique across different model architectures and domains is not fully established.

## Confidence

- **High Confidence**: The claim that uncertainty-aware metrics (Brier score, skill score) provide more nuanced evaluation than traditional accuracy metrics is well-supported by the literature and the paper's experiments.
- **Medium Confidence**: The assertion that fine-tuning can calibrate smaller models to outperform larger pre-trained models by 10x is supported by the experimental results, though this comparison is limited to the specific negotiation datasets tested.
- **Medium Confidence**: The finding that GPT-4 can anticipate outcome certainty with up to 9% improvement over prior knowledge is supported by the data, but the generalizability of this result to other large language models is uncertain.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the uncertainty representation methods and fine-tuning strategies on non-negotiation conversational datasets (e.g., customer service, debate, or collaborative planning) to assess domain generalization beyond the eight negotiation corpora.

2. **Ablation Study of Prompt Templates**: Systematically vary the prompt templates and system prompts to quantify their impact on forecast quality and calibration, addressing the current underspecification of these critical implementation details.

3. **Calibration Robustness Analysis**: Test the post-hoc correction technique across different model families (e.g., decoder-only, encoder-decoder, OPT) and varying dataset sizes to establish the robustness and limitations of the calibration improvements.