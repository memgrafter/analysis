---
ver: rpa2
title: 'Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects'
arxiv_id: '2412.02803'
source_url: https://arxiv.org/abs/2412.02803
tags:
- adversarial
- images
- object
- m-ifgsm
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the underexplored vulnerability of 3D models
  to adversarial attacks, particularly in vision-language contexts. It introduces
  Masked Iterative Fast Gradient Sign Method (M-IFGSM), a novel adversarial attack
  that focuses perturbations on masked regions of 3D objects to degrade the performance
  of CLIP's zero-shot object detection.
---

# Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects

## Quick Facts
- arXiv ID: 2412.02803
- Source URL: https://arxiv.org/abs/2412.02803
- Reference count: 3
- M-IFGSM reduces CLIP's zero-shot object detection accuracy from 95.4% to 12.5% on training images and from 91.2% to 35.4% on test images

## Executive Summary
This paper introduces Masked Iterative Fast Gradient Sign Method (M-IFGSM), a novel adversarial attack targeting 3D Gaussian Splatting models by focusing perturbations on masked regions of objects. The method leverages segmentation masks to apply targeted adversarial noise that degrades CLIP's zero-shot object detection performance while remaining nearly imperceptible to human observers. Experiments on the CO3D dataset demonstrate significant vulnerabilities in 3D object detection systems, reducing top-1 accuracy dramatically across both training and test images.

## Method Summary
The paper proposes M-IFGSM, which generates adversarial perturbations by applying iterative FGSM updates only to masked regions of objects identified through segmentation. The process involves: (1) using SAM for object segmentation, (2) applying M-IFGSM to generate adversarial images with perturbations focused on masked regions, (3) reconstructing 3D models using Gaussian Splatting from the adversarial images, and (4) evaluating classification performance using CLIP. The method targets the 3D reconstruction process, demonstrating that adversarial noise from 2D images successfully transfers to 3D representations and degrades detection accuracy.

## Key Results
- M-IFGSM reduces CLIP's top-1 accuracy from 95.4% to 12.5% on training images
- Attack reduces accuracy from 91.2% to 35.4% on test images
- Adversarial perturbations remain nearly imperceptible to human observers while maintaining effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: M-IFGSM generates adversarial perturbations that remain nearly imperceptible to human observers while significantly degrading CLIP's classification accuracy
- Mechanism: The method applies perturbations only to masked regions of the object using element-wise multiplication with the segmentation mask, concentrating changes where they matter most for detection
- Core assumption: CLIP's object detection performance depends primarily on the visual appearance of the object itself rather than the entire scene context
- Evidence anchors:
  - [abstract]: "adversarial noise being nearly imperceptible to human observers"
  - [section]: "concentrating perturbations solely on the segmented object regions"
  - [corpus]: No direct evidence found in neighbors, but this aligns with standard adversarial attack literature
- Break condition: If CLIP's detection relies heavily on scene context or background features, or if the segmentation mask poorly captures the object boundaries

### Mechanism 2
- Claim: The iterative nature of M-IFGSM allows for more effective adversarial perturbations compared to single-step FGSM
- Mechanism: Multiple iterations with smaller step sizes enable the gradient to find more optimal perturbation directions that maximize the loss function
- Core assumption: The loss landscape is smooth enough that iterative updates can find better minima than single large steps
- Evidence anchors:
  - [section]: "Applying M-IFGSM to generate noise focused on masked regions" and "our experiments, we set the loss threshold τ to 20.00 for early stopping"
  - [corpus]: No direct evidence in neighbors, but iterative FGSM is well-established in adversarial attack literature
- Break condition: If the loss landscape is highly non-convex or if early stopping prevents sufficient iterations

### Mechanism 3
- Claim: The 3D Gaussian Splatting reconstruction process preserves and transfers adversarial perturbations from 2D images to the 3D model
- Mechanism: When training the Gaussian Splatting model on adversarially perturbed images, the optimization process learns Gaussian parameters that encode the adversarial features
- Core assumption: The reconstruction process is differentiable and can incorporate the adversarial features from training images into the 3D representation
- Evidence anchors:
  - [section]: "After generating adversarial images with M-IFGSM, we reconstruct 3D models using the Gaussian Splatting method" and "renders from training camera locations successfully transfer the adversarial noise into 3DGS models"
  - [corpus]: No direct evidence in neighbors, but this is a novel application of adversarial transfer to 3D reconstruction
- Break condition: If the 3D reconstruction process somehow filters out or averages away the adversarial features, or if the Gaussian Splatting optimization is not sensitive to the specific adversarial perturbations

## Foundational Learning

- Concept: Gradient-based adversarial attacks (FGSM, I-FGSM)
  - Why needed here: The paper builds on these established techniques and modifies them for masked regions
  - Quick check question: What is the key difference between standard FGSM and iterative FGSM in terms of perturbation generation?

- Concept: Vision-language models (CLIP architecture)
  - Why needed here: Understanding how CLIP processes images and generates predictions is crucial for targeting the attack
  - Quick check question: How does CLIP process input images before classification?

- Concept: 3D Gaussian Splatting reconstruction
  - Why needed here: The attack targets this specific 3D reconstruction technique and its vulnerabilities
  - Quick check question: What are the key parameters that 3D Gaussian Splatting optimizes during reconstruction?

## Architecture Onboarding

- Component map: Input images → SAM segmentation → M-IFGSM adversarial perturbation → 3D Gaussian Splatting reconstruction → CLIP evaluation
- Critical path: Segmentation → Adversarial perturbation → 3D reconstruction → Evaluation
- Design tradeoffs:
  - Masked vs. full-image perturbation: Masked is less detectable but may be less effective if CLIP uses background context
  - Iterative vs. single-step: Iterative is more effective but computationally more expensive
  - Number of training images: More images improve 3D reconstruction quality but increase computational cost
- Failure signatures:
  - High confidence on adversarial examples indicates poor attack effectiveness
  - Segmentation failures (incorrect masks) lead to perturbations in wrong regions
  - Poor 3D reconstruction quality prevents proper evaluation of transfer
- First 3 experiments:
  1. Generate adversarial perturbations on a single object with known mask and verify confidence drop
  2. Test segmentation accuracy on CO3D dataset objects to ensure reliable masks
  3. Validate that Gaussian Splatting can reconstruct from adversarial images by comparing rendered views to inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are current defenses against M-IFGSM attacks on 3D models in vision-language tasks?
- Basis in paper: [inferred] The paper demonstrates successful adversarial attacks but does not explore defensive mechanisms.
- Why unresolved: The authors focus solely on attack methodology and effectiveness, leaving defense research unexplored.
- What evidence would resolve it: Experimental results comparing model performance with and without various defensive techniques when subjected to M-IFGSM attacks.

### Open Question 2
- Question: Does M-IFGSM's effectiveness generalize to other 3D reconstruction methods beyond Gaussian Splatting?
- Basis in paper: [explicit] "This work represents a novel contribution to the adversarial machine learning field by bridging the gap between 2D adversarial attacks and their effects in 3D environments."
- Why unresolved: The paper only tests M-IFGSM on Gaussian Splatting, leaving uncertainty about its applicability to other 3D reconstruction techniques.
- What evidence would resolve it: Comparative studies applying M-IFGSM to multiple 3D reconstruction methods and measuring attack effectiveness across each.

### Open Question 3
- Question: How does the segmentation mask quality affect M-IFGSM's attack success rate in multi-object scenes?
- Basis in paper: [explicit] "The notable case of the couch dataset, where the segmentation process only perturbs one of two couches in the image, underscores a limitation in current masking techniques when dealing with multiple instances of the same class in a scene."
- Why unresolved: The paper identifies this limitation but doesn't systematically investigate how segmentation quality impacts attack effectiveness.
- What evidence would resolve it: Controlled experiments varying segmentation mask accuracy and completeness in multi-object scenes to quantify impact on attack success rates.

## Limitations
- The claim that adversarial perturbations are "nearly imperceptible to human observers" lacks quantitative validation through human perception studies or standardized perceptual metrics
- The effectiveness of M-IFGSM on real-world 3D objects with complex backgrounds and occlusions is not demonstrated, as experiments are limited to controlled CO3D dataset images
- The paper does not address potential defenses against such attacks or the transferability of perturbations to different CLIP model variants

## Confidence

- High confidence: The methodology for generating adversarial perturbations using M-IFGSM with segmentation masks is technically sound and follows established adversarial attack principles. The experimental setup with CO3D dataset and CLIP evaluation is clearly specified.
- Medium confidence: The claim that 3D Gaussian Splatting effectively transfers adversarial perturbations from 2D images to 3D representations is supported by experimental results but lacks detailed analysis of the reconstruction process and perturbation preservation mechanisms.
- Low confidence: The assertion that perturbations are "nearly imperceptible to human observers" lacks quantitative validation through human perception studies or standardized perceptual metrics.

## Next Checks

1. **Perceptual validation**: Conduct a user study with 20+ participants rating the visibility of adversarial perturbations on a 5-point scale, comparing original, adversarial, and perturbed images. Calculate average visibility scores and statistical significance to objectively validate the "nearly imperceptible" claim.

2. **Cross-model transferability**: Test whether perturbations generated against CLIP-ViT-B/16 transfer to other CLIP variants (CLIP-ViT-L/14, CLIP-ResNet50) and to different object detection models. Measure the attack success rate and confidence drops across multiple model architectures.

3. **Defense robustness evaluation**: Implement and evaluate standard adversarial defense techniques (adversarial training, input denoising, randomized smoothing) against M-IFGSM attacks. Measure the reduction in attack effectiveness and determine which defenses are most effective against masked-region adversarial perturbations.