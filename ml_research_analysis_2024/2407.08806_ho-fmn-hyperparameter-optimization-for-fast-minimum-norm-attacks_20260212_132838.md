---
ver: rpa2
title: 'HO-FMN: Hyperparameter Optimization for Fast Minimum-Norm Attacks'
arxiv_id: '2407.08806'
source_url: https://arxiv.org/abs/2407.08806
tags:
- attack
- attacks
- each
- robustness
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HO-FMN, a hyperparameter optimization framework
  for improving the Fast Minimum-Norm (FMN) attack. The key innovation is a modular
  reformulation of FMN that allows dynamic adjustment of loss functions, optimizers,
  and step-size schedulers, combined with Bayesian optimization to automatically find
  optimal hyperparameters for each configuration.
---

# HO-FMN: Hyperparameter Optimization for Fast Minimum-Norm Attacks

## Quick Facts
- arXiv ID: 2407.08806
- Source URL: https://arxiv.org/abs/2407.08806
- Reference count: 27
- Primary result: HO-FMN consistently finds smaller adversarial perturbations than baseline FMN and outperforms APGD in 11 out of 12 robust models tested

## Executive Summary
This paper introduces HO-FMN, a hyperparameter optimization framework that improves the Fast Minimum-Norm (FMN) attack through modular reformulation and Bayesian optimization. The method dynamically adjusts loss functions, optimizers, and step-size schedulers, automatically finding optimal hyperparameters for each configuration. HO-FMN provides complete robustness evaluation curves by computing minimum-norm adversarial examples, enabling more comprehensive assessment than fixed-budget attacks.

## Method Summary
HO-FMN reformulates FMN into a modular architecture where loss functions, optimizers, and step-size schedulers can be dynamically swapped. The framework creates multiple FMN configurations by combining different losses (LL, CE, DLR), optimizers (GD, Adam, AdaMax), and schedulers (CALR, RLRoP). Bayesian optimization then automatically finds optimal hyperparameters for each configuration-model pair by optimizing for the smallest median perturbation size. This approach enables reporting adversarial robustness as a function of perturbation budget rather than a single fixed-budget evaluation.

## Key Results
- HO-FMN consistently finds smaller adversarial perturbations than baseline FMN attack
- Outperforms APGD in 11 out of 12 robust models from CIFAR-10 and ImageNet datasets
- Computational overhead is approximately 4% additional runtime compared to base FMN attack
- DLR loss with GD optimizer and CALR scheduler performs consistently well across different models
- Provides complete robustness evaluation curves rather than single-point estimates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Modular reformulation of FMN enables dynamic adjustment of loss functions, optimizers, and schedulers without altering the core algorithmic structure.
- **Mechanism**: By parameterizing the loss L, optimizer u, and scheduler s as independent components in Algorithm 1, the framework can swap in different implementations while maintaining the same optimization loop. This allows experimentation with different loss landscapes and optimization dynamics.
- **Core assumption**: The modular components (loss, optimizer, scheduler) are interchangeable without breaking the mathematical convergence properties of the attack.
- **Evidence anchors**:
  - [abstract] "proposing a parametric variation of the well-known fast minimum-norm attack algorithm, whose loss, optimizer, step-size scheduler, and hyperparameters can be dynamically adjusted"
  - [section] "We modify the algorithm to work with any differentiable lossL" and "we generalize this step with a linear projection (proj) onto a unitary-sizedℓ p-ball"
  - [corpus] Weak evidence - related works focus on hyperparameter tuning in general but don't specifically address modular attack reformulation
- **Break condition**: If the interchangeability assumption fails, e.g., certain optimizer-scheduler pairs create instability or the projection step becomes ill-defined for specific losses.

### Mechanism 2
- **Claim**: Bayesian optimization efficiently finds optimal hyperparameters for each FMN configuration without exhaustive grid search.
- **Mechanism**: BO uses Gaussian Process Regression to model the relationship between hyperparameters and attack performance (median perturbation size), then uses acquisition functions like Noisy Expected Improvement to guide the search toward promising regions while balancing exploration and exploitation.
- **Core assumption**: The objective function (median perturbation size) is sufficiently smooth and can be approximated by a GP model, making BO more efficient than random or grid search.
- **Evidence anchors**:
  - [abstract] "leverage Bayesian optimization to perform a hyperparameter-optimization step that, for any given FMN configuration, automatically finds the best hyperparameters"
  - [section] "we employ the smallest median perturbation ˜∥δ∥as a criterion to find, for each configurationC 1, . . .CN ∈C, the best set of hyperparametersh ⋆"
  - [corpus] Moderate evidence - Bayesian optimization is well-established for hyperparameter tuning, though specific application to adversarial attacks is novel
- **Break condition**: If the objective landscape is highly non-smooth or discontinuous, BO may converge to suboptimal hyperparameters or require excessive trials.

### Mechanism 3
- **Claim**: Using minimum-norm adversarial examples enables construction of complete robustness evaluation curves rather than single-point estimates.
- **Mechanism**: FMN solves the optimization problem to find the smallest perturbation that causes misclassification, allowing computation of robust accuracy across multiple perturbation budgets from a single attack run. This contrasts with fixed-budget attacks that only provide accuracy at one epsilon value.
- **Core assumption**: Finding minimum-norm perturbations provides a more complete picture of model robustness than fixed-budget attacks at single epsilon values.
- **Evidence anchors**:
  - [abstract] "enables reporting adversarial robustness as a function of the perturbation budget, providing a more complete evaluation than that offered by fixed-budget attacks"
  - [section] "instead of having a scalar robustness evaluation associated with a predefined perturbation budgetϵfrom a single run, we can obtain an entire curve"
  - [corpus] Weak evidence - related works mention fixed-budget attacks but don't directly address minimum-norm approaches for curve construction
- **Break condition**: If the computational cost of finding minimum-norm solutions becomes prohibitive or if the curves don't provide additional practical insight beyond fixed-budget evaluations.

## Foundational Learning

- **Concept**: Bayesian Optimization and Gaussian Process Regression
  - Why needed here: BO is the core mechanism for efficiently searching the hyperparameter space without exhaustive grid search, and GP regression provides the probabilistic model that guides the search.
  - Quick check question: What is the role of the acquisition function in Bayesian optimization, and how does Noisy Expected Improvement differ from standard Expected Improvement?

- **Concept**: Modular Software Design and Component Interchangeability
  - Why needed here: The attack's modular reformulation relies on being able to swap different loss functions, optimizers, and schedulers while maintaining algorithmic correctness and performance.
  - Quick check question: What are the key interface requirements that must be satisfied by each modular component (loss, optimizer, scheduler) to ensure they can be interchanged in the FMN framework?

- **Concept**: Adversarial Robustness Evaluation Metrics
  - Why needed here: Understanding why minimum-norm attacks provide more complete evaluations than fixed-budget attacks requires knowledge of different robustness metrics and their interpretation.
  - Quick check question: How does the robust accuracy curve computed from minimum-norm perturbations differ in interpretation from the scalar robust accuracy at a fixed epsilon value?

## Architecture Onboarding

- **Component map**: Configuration generator -> Bayesian optimizer (trials -> GP updates -> new samples) -> Best configuration selection -> Final attack evaluation -> Robustness curve computation
- **Critical path**: Configuration generation → Bayesian optimization (trials → GP updates → new samples) → Best configuration selection → Final attack evaluation → Robustness curve computation
- **Design tradeoffs**:
  - Search space size vs. computational cost: More configurations and hyperparameters provide better coverage but increase optimization time
  - Exploration vs. exploitation in BO: Balancing finding new promising regions vs. refining known good solutions
  - Median vs. mean perturbation: Median reduces outlier impact but may miss some failure modes
- **Failure signatures**:
  - BO convergence to poor hyperparameters: Check if GP model is being updated correctly and if acquisition function is exploring sufficiently
  - Attack instability: Verify modular components are properly interfaced and gradients are flowing correctly
  - Excessive runtime: Profile individual components to identify bottlenecks in configuration generation or attack execution
- **First 3 experiments**:
  1. **Baseline validation**: Run original FMN (GD-CALR-LL) on a simple CIFAR-10 model to verify modular implementation matches original results
  2. **Configuration generation test**: Verify all expected configurations are generated correctly and that incompatible combinations are handled properly
  3. **BO sanity check**: Run BO with a synthetic objective function to verify the optimization framework is working before applying to actual attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational overhead scale with the number of configurations and trials in HO-FMN when applied to larger datasets or models?
- Basis in paper: [explicit] The paper reports that HO-FMN adds approximately 4% overhead to the base FMN attack runtime with 32 trials, but notes this is measured on CIFAR-10 and ImageNet with 4096 and 1000 samples respectively.
- Why unresolved: The paper doesn't explore how this overhead scales with larger datasets, more configurations, or more trials, which would be necessary for real-world deployment.
- What evidence would resolve it: Systematic experiments varying dataset size, number of configurations, and number of trials while measuring both optimization time and attack performance across different model sizes and complexities.

### Open Question 2
- Question: Can the modular approach be extended to other attack types beyond FMN, such as targeted attacks or non-gradient-based methods?
- Basis in paper: [inferred] The paper successfully extends FMN by making its loss function, optimizer, and scheduler modular and parametrizable, suggesting this architectural pattern could be generalizable.
- Why unresolved: The paper only demonstrates this approach on FMN and doesn't explore whether the modular, hyperparameter-optimized approach could benefit other attack families or threat models.
- What evidence would resolve it: Implementation and evaluation of the modular hyperparameter optimization approach on different attack types (targeted attacks, black-box attacks, decision-based attacks) and perturbation models.

### Open Question 3
- Question: What is the theoretical relationship between the search space complexity and the quality of the final attack configuration?
- Basis in paper: [explicit] The paper notes that adding more hyperparameters would make the search space bigger, resulting in longer optimization time, but doesn't explore the relationship between search space complexity and attack performance.
- Why unresolved: The paper mentions the trade-off between search space size and optimization time but doesn't investigate how different search space sizes or structures affect the quality of the discovered configurations or whether there are diminishing returns.
- What evidence would resolve it: Systematic experiments varying the number of parameters, their ranges, and search space structure while measuring both optimization time and final attack performance to establish optimal search space design principles.

## Limitations
- The modular reformulation assumes perfect interchangeability between losses, optimizers, and schedulers, but the paper doesn't rigorously test edge cases where certain combinations might fail or produce undefined behavior.
- The BO approach assumes smooth objective landscapes, but adversarial attack landscapes are often highly non-convex and noisy, and the paper doesn't analyze failure modes when BO converges to suboptimal hyperparameters.
- Computational overhead claims (4% increase) are based on specific hardware setups and may not generalize across different computing environments or model scales.

## Confidence

- **High**: Modular FMN reformulation works as described and enables component swapping (supported by working implementations and consistent improvements)
- **Medium**: Bayesian optimization effectively finds optimal hyperparameters across diverse model configurations (supported by empirical results but lacks theoretical guarantees for adversarial landscapes)
- **Low**: Minimum-norm approach provides substantially more informative robustness evaluation than fixed-budget methods (limited comparison to established evaluation protocols)

## Next Checks

1. **Robustness stress test**: Systematically test all (loss, optimizer, scheduler) combinations on a simple model to identify any incompatible or unstable configurations that weren't caught in the main experiments
2. **BO failure analysis**: Run HO-FMN with synthetic non-smooth objective functions to quantify how often BO fails to find good hyperparameters in adversarial attack landscapes
3. **Cross-platform verification**: Reproduce the 4% overhead claim on different hardware architectures and with varying batch sizes to verify computational cost claims generalize beyond the specific experimental setup