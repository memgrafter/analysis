---
ver: rpa2
title: Multi-Agent Diagnostics for Robustness via Illuminated Diversity
arxiv_id: '2401.13460'
source_url: https://arxiv.org/abs/2401.13460
tags:
- tizero
- adversarial
- reference
- madrid
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MADRID, a method for automatically generating
  diverse adversarial scenarios to expose strategic vulnerabilities in pre-trained
  multi-agent policies. MADRID leverages quality-diversity algorithms and regret estimation
  to systematically explore the space of adversarial settings.
---

# Multi-Agent Diagnostics for Robustness via Illuminated Diversity

## Quick Facts
- arXiv ID: 2401.13460
- Source URL: https://arxiv.org/abs/2401.13460
- Reference count: 40
- Pre-trained multi-agent policies can be systematically tested for strategic vulnerabilities using quality-diversity algorithms

## Executive Summary
This paper introduces MADRID, a method for automatically generating diverse adversarial scenarios to expose strategic vulnerabilities in pre-trained multi-agent policies. MADRID leverages quality-diversity algorithms and regret estimation to systematically explore the space of adversarial settings. When applied to the complex 11vs11 Google Research Football environment targeting the state-of-the-art TiZero policy, MADRID successfully identifies numerous tactical weaknesses, including poor positioning, misunderstanding of game rules like offsides, and unforced own goals. The method discovers that TiZero exhibits suboptimal decision-making in approximately 90% of generated adversarial levels, with reference policies often outperforming it despite being nominally weaker.

## Method Summary
MADRID uses MAP-Elites with CMA-ME for quality-diversity search to generate diverse adversarial levels in the Google Research Football environment. The method discretizes the feature space (ball position) into a grid and stores levels along with regret scores (performance gaps between reference and target policies). MADRID iteratively mutates levels using Gaussian noise, estimates regret by comparing reference and target policy performance, and replaces grid cells when higher regret is found. The approach uses 51 reference policies (48 TiZero checkpoints plus 3 heuristic bots) and runs for 5000 iterations to discover tactical vulnerabilities.

## Key Results
- TiZero exhibits suboptimal decision-making in approximately 90% of generated adversarial levels
- MADRID successfully identifies tactical weaknesses including offsides mistakes, unforced own goals, and poor positioning
- Reference policies often outperform TiZero despite being nominally weaker, demonstrating MADRID's effectiveness in revealing latent vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1
MADRID finds adversarial levels where TiZero's policy is outperformed by reference policies using MAP-Elites to evolve a diverse set of procedural levels, selecting and mutating those with high regret (gap between reference and target policy performance). A collection of reference policies estimates lower bounds on true regret, revealing where TiZero underperforms.

### Mechanism 2
MADRID discovers vulnerabilities such as offsides mistakes, unforced own goals, and suboptimal positioning by iterating over a discretized grid of game states (e.g., ball position) and reference policies, generating levels where TiZero exhibits specific tactical errors.

### Mechanism 3
MADRID's grid-based archive provides interpretability for discovered adversarial examples by storing specific level configurations and the reference policy that outperforms TiZero on it, allowing qualitative analysis of why TiZero fails.

## Foundational Learning

- Concept: Quality-Diversity (QD) algorithms
  - Why needed here: QD enables systematic exploration of diverse, high-performing solutions (adversarial levels) in a large space
  - Quick check question: What is the key difference between MAP-Elites and novelty search in QD?

- Concept: Regret estimation in multi-agent RL
  - Why needed here: Regret quantifies the suboptimality of TiZero's decisions compared to reference policies
  - Quick check question: How does MADRID estimate regret when the true optimal policy is unknown?

- Concept: Procedural level generation
  - Why needed here: Procedural generation allows systematic creation of diverse game scenarios for testing TiZero
  - Quick check question: Why does MADRID use Gaussian noise to mutate levels rather than generating them from scratch?

## Architecture Onboarding

- Component map: MAP-Elites archive -> Level generator -> Reference policies -> Regret estimator -> Experiment runner
- Critical path:
  1. Initialize MAP-Elites grid with random levels
  2. Sample a level and reference policy from the grid
  3. Mutate the level with Gaussian noise
  4. Estimate regret by comparing reference and target policy performance
  5. If regret is higher than current occupant, replace the cell
  6. Repeat for a fixed number of iterations

- Design tradeoffs:
  - Discretization granularity vs. computational cost
  - Number of reference policies vs. coverage of the behavior space
  - Mutation noise level vs. exploration vs. exploitation
  - Episode length vs. time to discover vulnerabilities

- Failure signatures:
  - Grid cells remain empty: Discretization too coarse or mutation too aggressive
  - Regret values plateau early: Reference policies not diverse enough or mutation too conservative
  - No interpretable patterns: Feature space not aligned with critical vulnerabilities

- First 3 experiments:
  1. Run MADRID with only one reference policy to confirm it can find adversarial levels
  2. Vary the discretization granularity and measure impact on regret and interpretability
  3. Test different mutation noise levels to find the optimal balance between exploration and exploitation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MADRID's performance scale with the complexity of the environment or number of agents in the multi-agent system?
- Basis in paper: The paper demonstrates MADRID on 11 vs 11 football but doesn't explore how performance changes with more agents or more complex environments
- Why unresolved: The paper only tests MADRID on one specific environment (GRF 11 vs 11) and doesn't provide analysis of how it would perform on more complex scenarios or with more agents
- What evidence would resolve it: Systematic experiments varying the number of agents and environment complexity, showing how regret discovery rates and computational requirements change

### Open Question 2
- Question: Can MADRID be adapted to generate adversarial scenarios that specifically target communication failures between agents?
- Basis in paper: The paper mentions that TiZero has communication-related issues (e.g., not passing to better positioned players) but doesn't explore whether MADRID can be explicitly designed to find such vulnerabilities
- Why unresolved: While MADRID identifies various tactical failures, the paper doesn't investigate whether it can be modified to specifically target and expose communication breakdowns between agents
- What evidence would resolve it: Experiments showing MADRID's ability to generate scenarios where communication failures are the primary cause of suboptimal behavior, with metrics specifically measuring coordination breakdowns

### Open Question 3
- Question: How transferable are the adversarial scenarios generated by MADRID to improving robustness in other multi-agent systems?
- Basis in paper: The paper mentions that discovered vulnerabilities could be used for fine-tuning to improve robustness, but doesn't investigate whether these specific adversarial scenarios transfer to improving other multi-agent systems
- Why unresolved: The paper focuses on finding vulnerabilities but doesn't test whether the same scenarios are useful for improving the robustness of different multi-agent systems beyond the target policy
- What evidence would resolve it: Experiments showing that adversarial scenarios generated for TiZero in GRF can be used to improve robustness in other multi-agent systems, either in different domains or different implementations of football

## Limitations

- The assumption that reference policies can reliably estimate regret bounds is not rigorously validated
- The discretization of the football field into a 10x8 grid may miss important tactical scenarios outside these cells
- The claim of superior interpretability compared to other adversarial testing methods lacks quantitative comparison

## Confidence

- High confidence: The core mechanism of using quality-diversity algorithms (MAP-Elites) to generate diverse adversarial levels is well-established and the implementation details are clearly specified
- Medium confidence: The claim that MADRID successfully identifies vulnerabilities in approximately 90% of generated adversarial levels is supported by experimental results
- Low confidence: The assertion that MADRID provides superior interpretability compared to other adversarial testing methods is based on qualitative observations rather than quantitative comparison

## Next Checks

1. Systematically vary the number and diversity of reference policies and measure the impact on regret estimation accuracy and vulnerability discovery rate
2. Test MADRID with different grid resolutions (finer and coarser than 10x8) and quantify how this affects both the number of discovered vulnerabilities and computational efficiency
3. Apply MADRID to a different multi-agent environment (e.g., simplified soccer or another team game) to verify that the method generalizes beyond the Google Research Football setting