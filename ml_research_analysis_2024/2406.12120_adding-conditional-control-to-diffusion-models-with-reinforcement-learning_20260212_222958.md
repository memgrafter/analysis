---
ver: rpa2
title: Adding Conditional Control to Diffusion Models with Reinforcement Learning
arxiv_id: '2406.12120'
source_url: https://arxiv.org/abs/2406.12120
tags:
- diffusion
- guidance
- pre-trained
- conditional
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CTRL, a reinforcement learning-based method
  for conditioning pre-trained diffusion models on additional controls. The key innovation
  is formulating conditional generation as an RL problem where the reward is the conditional
  log-likelihood of the additional control, and the policy corresponds to the denoising
  process.
---

# Adding Conditional Control to Diffusion Models with Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.12120
- Source URL: https://arxiv.org/abs/2406.12120
- Authors: Yulai Zhao; Masatoshi Uehara; Gabriele Scalia; Sunyuan Kung; Tommaso Biancalani; Tommaso Biancalani; Sergey Levine; Ehsan Hajiramezanali
- Reference count: 40
- Primary result: CTRL improves sample efficiency over classifier-free guidance by leveraging conditional independence between inputs and additional controls

## Executive Summary
This paper introduces CTRL, a reinforcement learning-based method for conditioning pre-trained diffusion models on additional controls. The key innovation is formulating conditional generation as an RL problem where the reward is the conditional log-likelihood of the additional control, and the policy corresponds to the denoising process. By leveraging conditional independence between inputs and controls, CTRL achieves sample efficiency improvements over existing approaches like classifier-free guidance while maintaining image quality.

## Method Summary
CTRL fine-tunes pre-trained diffusion models to condition on additional controls by formulating the problem as an MDP. The method introduces an augmented model architecture that combines the pre-trained model with new parameters for conditioning, then solves an RL problem to find the soft-optimal policy. The RL reward includes the conditional log-likelihood of the control signal, and the solution produces samples from the target conditional distribution. The approach requires only pairs of data and controls (not triplets), improving sample efficiency over classifier-free guidance.

## Key Results
- Achieves over 90% accuracy in challenging multi-task aesthetic score conditioning
- Outperforms baselines including reconstruction guidance, SMC, MPGD, and SVDD
- Maintains image quality while improving conditioning accuracy
- Demonstrates sample efficiency gains through conditional independence assumptions

## Why This Works (Mechanism)

### Mechanism 1: Sample Efficiency Through Conditional Independence
CTRL achieves sample efficiency by leveraging conditional independence to simplify classifier learning from offline data. Instead of modeling the full conditional distribution p(x|c,y) directly, CTRL only needs to learn p(y|x,c). This is simpler because y is often lower-dimensional than x, and when y⊥c|x, the classifier simplifies to p(y|x).

### Mechanism 2: RL Formulation Ensures Target Distribution Sampling
CTRL's RL formulation ensures the fine-tuned model samples from the target conditional distribution pγ(x|c,y). The RL problem maximizes a reward that includes the conditional log-likelihood of y given x,c, with a KL penalty against the pre-trained model. The soft-optimal policy from solving this RL problem produces samples from pγ(x|c,y).

### Mechanism 3: Augmented Model Architecture Without Full Retraining
CTRL's augmented model architecture enables fine-tuning without retraining the entire diffusion model. The augmented model g(t,c,y,x;ψ) combines the pre-trained model fpre(t,c,x;θpre) with new parameters ϕ added for conditioning on y. This allows incorporating additional controls while preserving the pre-trained knowledge.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and reinforcement learning
  - Why needed here: CTRL frames conditional generation as an MDP where the policy corresponds to the denoising process, and the reward is the conditional log-likelihood.
  - Quick check question: What are the state, action, and reward in CTRL's MDP formulation for diffusion model conditioning?

- Concept: Diffusion models and score matching
  - Why needed here: CTRL builds on pre-trained diffusion models, so understanding how they work (SDEs, denoising score matching) is essential for implementing the augmented model and fine-tuning process.
  - Quick check question: How does the drift term in a diffusion SDE relate to the score function of the data distribution?

- Concept: Conditional independence and its implications for generative modeling
  - Why needed here: CTRL leverages conditional independence to simplify dataset requirements and improve sample efficiency, which is crucial for understanding its advantages over baselines.
  - Quick check question: If Y⊥C|X, how does this simplify the joint distribution p(x,c,y)?

## Architecture Onboarding

- Component map: Pre-trained diffusion model -> Augmented model -> Classifier -> RL planner -> Inference module
- Critical path: 1. Train classifier p(y|x,c) on offline data 2. Construct augmented model g with fpre weights and new ϕ parameters 3. Solve RL problem to find soft-optimal policy (fine-tune ϕ) 4. Use fine-tuned model for conditional generation at inference
- Design tradeoffs: Adding more parameters to augmented model vs. computational cost and overfitting; Guidance strength γ: stronger guidance improves conditioning but may deviate from pre-trained model; Classifier-free guidance weight: balances between pre-trained model and additional conditioning during inference
- Failure signatures: Poor classification accuracy on validation set → Classifier training issues; No improvement in conditioning metrics during fine-tuning → RL optimization problems; Generated samples don't match conditioning → Augmented model architecture insufficient or guidance strength misconfigured
- First 3 experiments: 1. Verify classifier accuracy on held-out validation set before fine-tuning 2. Fine-tune augmented model with a small dataset and monitor conditioning metrics (e.g., classification accuracy) 3. Generate samples with different guidance strengths and evaluate both conditioning accuracy and image quality metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CTRL compare to classifier-free guidance when conditional independence assumptions (y ⊥ c|x) do not hold?
- Basis in paper: The paper demonstrates advantages of CTRL over classifier-free guidance when y ⊥ c|x holds, but does not explore cases where this assumption is violated.
- Why unresolved: The paper only validates CTRL in scenarios where conditional independence is present, leaving open questions about its performance in more general settings.
- What evidence would resolve it: Comparative experiments where y and c are conditionally dependent, measuring sample efficiency and accuracy against classifier-free guidance.

### Open Question 2
- Question: What is the impact of different fine-tuning strategies (e.g., LoRA vs full model fine-tuning) on CTRL's performance and computational efficiency?
- Basis in paper: The paper mentions using LoRA modules for efficiency but does not systematically compare different fine-tuning strategies.
- Why unresolved: The paper focuses on a specific implementation approach without exploring how alternative fine-tuning methods might affect results.
- What evidence would resolve it: Controlled experiments comparing CTRL performance and computational costs across different fine-tuning strategies.

### Open Question 3
- Question: How does the choice of guidance strength γ affect the trade-off between conditioning accuracy and image quality in CTRL?
- Basis in paper: The paper uses γ=10 in experiments but notes this is an open problem in diffusion model fine-tuning.
- Why unresolved: While the paper discusses the role of γ, it does not provide a systematic analysis of how different values affect the balance between conditioning accuracy and image quality.
- What evidence would resolve it: Ablation studies varying γ across a wide range, measuring both conditioning accuracy and image quality metrics.

## Limitations

- Relies on conditional independence assumption (y⊥c|x) which may not hold for many real-world control signals
- Sample efficiency claims lack quantitative comparisons with classifier-free guidance in terms of runtime or memory usage
- Augmented model architecture effectiveness for complex, high-dimensional controls remains unclear

## Confidence

- High Confidence: The core RL formulation connecting diffusion models to conditional generation is theoretically sound, with clear connections to existing classifier guidance approaches (Theorem 1 and Lemma 2). The experimental methodology and evaluation metrics are well-defined.
- Medium Confidence: The empirical results demonstrating improved sample efficiency and conditioning performance are convincing within the tested scenarios. However, the generalizability across different types of controls and the robustness when conditional independence assumptions are violated remain uncertain.
- Low Confidence: Claims about computational efficiency compared to classifier-free guidance lack quantitative comparisons. The paper asserts improved sample efficiency but doesn't provide runtime or memory usage comparisons with existing approaches.

## Next Checks

1. **Conditional Independence Robustness Test**: Systematically evaluate CTRL's performance when the conditional independence assumption is violated by introducing correlated controls. Compare sample efficiency and conditioning accuracy against classifier-free guidance under varying degrees of correlation.

2. **RL Optimization Stability Analysis**: Conduct hyperparameter sensitivity analysis for the RL fine-tuning process, including learning rate, batch size, and KL penalty weight. Report training curves showing convergence behavior and KL divergence between fine-tuned and pre-trained models.

3. **Architecture Scalability Validation**: Test the augmented model architecture with progressively more complex controls (higher dimensionality, non-linear relationships). Compare performance against alternatives like full fine-tuning or more sophisticated conditioning architectures.