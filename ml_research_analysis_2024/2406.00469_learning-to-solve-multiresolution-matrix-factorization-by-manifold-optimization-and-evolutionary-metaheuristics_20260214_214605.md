---
ver: rpa2
title: Learning to Solve Multiresolution Matrix Factorization by Manifold Optimization
  and Evolutionary Metaheuristics
arxiv_id: '2406.00469'
source_url: https://arxiv.org/abs/2406.00469
tags:
- matrix
- graph
- learning
- wavelet
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of finding high-quality factorizations
  for Multiresolution Matrix Factorization (MMF), which is useful for modeling hierarchical
  structures in graphs. Existing greedy methods are brittle and often yield suboptimal
  factorizations.
---

# Learning to Solve Multiresolution Matrix Factorization by Manifold Optimization and Evolutionary Metaheuristics

## Quick Facts
- arXiv ID: 2406.00469
- Source URL: https://arxiv.org/abs/2406.00469
- Authors: Truong Son Hy; Thieu Khang; Risi Kondor
- Reference count: 40
- Primary result: Learnable MMF using manifold optimization and evolutionary metaheuristics outperforms greedy methods and produces wavelet bases that improve molecular graph classification and node classification accuracy.

## Executive Summary
This paper addresses the challenge of finding high-quality factorizations for Multiresolution Matrix Factorization (MMF), which is useful for modeling hierarchical structures in graphs. Existing greedy methods are brittle and often yield suboptimal factorizations. The authors propose a "learnable" version of MMF that optimizes the factorization using metaheuristics, specifically evolutionary algorithms and directed evolution, along with Stiefel manifold optimization through backpropagation. This approach outperforms existing greedy methods and provides a wavelet basis that better reflects the structure of the underlying matrix or graph. The resulting wavelet basis is used to construct Wavelet Neural Networks (WNNs) that achieve state-of-the-art results on molecular graph classification and node classification on citation graphs. The learnable MMF algorithm also demonstrates superior performance in matrix factorization tasks compared to the original greedy algorithm and the Nyström method.

## Method Summary
The paper presents a learnable MMF algorithm that combines Stiefel manifold optimization with evolutionary and directed evolution metaheuristics. The method treats MMF as a joint optimization problem over nested index sets and rotation matrices, using gradient descent on the Stiefel manifold to preserve orthogonality constraints. Evolutionary algorithms (EA) and directed evolution (DE) search the combinatorial space of wavelet index sets, with fitness evaluated through initial Stiefel optimization. The approach produces sparse, localized wavelet bases that improve spectral graph convolution efficiency and accuracy. These wavelets are integrated into WNNs for molecular and citation graph classification tasks, demonstrating superior performance compared to greedy MMF and Fourier-based methods.

## Key Results
- Learnable MMF with K=8,16 rotations outperforms greedy MMF with K=2 on matrix factorization tasks, achieving lower Frobenius error with the same number of active columns
- WNNs using learnable MMF wavelets achieve state-of-the-art classification accuracy on molecular graphs (MUTAG, PTC, PROTEINS, NCI1) and citation graphs (Cora, Citeseer)
- MMF wavelets are significantly sparser than Fourier bases (e.g., 19.23% vs 99.71% nonzeros on MUTAG) while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
The learnable MMF outperforms greedy methods because it optimizes all rotations simultaneously via Stiefel manifold gradient descent, avoiding local minima traps. By treating the rotation matrices as points on the Stiefel manifold and using Cayley transforms to follow the gradient while preserving orthogonality, the algorithm jointly adjusts all Uℓ matrices to minimize the residual Frobenius norm. Core assumption: The MMF objective surface is sufficiently smooth in the Stiefel manifold neighborhood of good solutions, so gradient-based descent finds better local minima than sequential greedy updates. Evidence anchors: [abstract] "optimizes the factorization using metaheuristics, specifically evolutionary algorithms and directed evolution, along with Stiefel manifold optimization through backpropagating errors" [section 4] Formal definition of the orthogonal constraint optimization and the gradient formula ∇F(X) = G − XGᵀX. Break Condition: If the residual error landscape is too rugged or contains many sharp local minima, gradient descent may still get stuck, reducing advantage over heuristics.

### Mechanism 2
Evolutionary and directed evolution metaheuristics efficiently explore the combinatorial space of nested index sets {Sℓ}, which is infeasible to solve exactly. The algorithm encodes a candidate solution as an ordered set of wavelet indices. Fitness is the initial Frobenius error from the Stiefel optimizer; evolution operators (crossover, mutation) generate new index sets, and selection drives convergence toward low-error factorizations. Core assumption: The mapping from index sets to factorization quality is smooth enough that small changes in the set (mutation) produce incremental changes in fitness, enabling evolutionary search to climb toward good solutions. Evidence anchors: [section 5.2] "Evolutionary algorithms ... iteratively improves a population of candidate solutions by applying operations such as selection, crossover, and mutation." [section 5.3] "Directed evolution ... generated through iterative cycles of genetic diversification and library screening or selection." Break Condition: If the fitness landscape is highly discontinuous (e.g., small set changes cause large error jumps), evolution may fail to converge or take excessive generations.

### Mechanism 3
Using sparse MMF wavelets instead of dense Laplacian eigenvectors enables localized, computationally efficient graph convolutions. The MMF decomposition yields wavelet bases that are sparse and localized in both vertex and frequency domains. This allows graph signal transforms via sparse matrix multiplication, preserving locality in spectral filtering layers. Core assumption: Graph signals of interest have multiscale structure that sparse wavelets capture better than global Fourier bases, improving both accuracy and speed. Evidence anchors: [abstract] "wavelet basis that better reflects the structure of the underlying matrix or graph" [section 6.1] "the wavelets are generally localized in both vertex domain and frequency" [section 7.2] Table 3 showing sparsity percentages (e.g., 19.23% nonzeros vs 99.71% for Fourier on MUTAG). Break Condition: If graph signals are inherently global or the graph lacks clear multiscale structure, the advantage of sparse wavelets diminishes and dense Fourier bases may perform similarly.

## Foundational Learning

- Concept: Stiefel manifold geometry and Cayley transform for orthogonal constraints
  - Why needed here: The MMF rotation matrices must remain orthogonal during optimization; naive gradient descent would break this constraint.
  - Quick check question: What is the tangent space condition for matrices X on the Stiefel manifold V_p(Rⁿ)?

- Concept: Evolutionary algorithm design (selection, crossover, mutation) and fitness evaluation
  - Why needed here: The nested index set selection is a combinatorial optimization problem; metaheuristics are required to search this space efficiently.
  - Quick check question: In the crossover operator described, why must common elements between parents be preserved before applying one-point crossover?

- Concept: Multiresolution analysis and wavelet basis construction
  - Why needed here: Understanding how MMF wavelet bases decompose graph signals into approximation and detail subspaces is key to designing the spectral graph convolution layer.
  - Quick check question: In MMF, what does the set Sℓ represent in terms of the active rows/columns at level ℓ?

## Architecture Onboarding

- Component map: Normalized graph Laplacian A -> Metaheuristic (EA or DE) generating ordered wavelet index sets -> Stiefel manifold gradient descent optimizing all Uℓ simultaneously -> Wavelet basis W (concatenated father and mother wavelets) -> Wavelet neural network layers using W for spectral convolution

- Critical path:
  1. Generate candidate index set via metaheuristic
  2. Run Stiefel optimizer to get initial error (fitness)
  3. Select best candidate, then run full Stiefel optimization to convergence
  4. Build W and train WNN on task

- Design tradeoffs:
  - Higher population size in EA improves solution quality but increases runtime significantly (see Table 1)
  - Larger rotation size K improves expressiveness but increases optimization dimensionality
  - Early stopping of Stiefel optimization during metaheuristic search trades accuracy for speed

- Failure signatures:
  - Fitness plateaus early → EA/DE stuck in local optima or landscape too rugged
  - Gradient descent diverges or cycles → step size or Armijo-Wolfe parameters mis-tuned
  - WNN performance matches or underperforms GFT-based models → wavelet sparsity insufficient for given graph structure

- First 3 experiments:
  1. Run EA and DE on Karate Club matrix (N=34) with K=2 and K=4; compare runtime vs. greedy baseline and final Frobenius error.
  2. For Cora citation graph, generate wavelet bases with K=8, 16, 32; measure sparsity and downstream node classification accuracy.
  3. On MUTAG dataset, train WNN with learnable MMF wavelets vs. fixed Fourier basis; report accuracy and inference time per graph.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the learnable MMF perform on larger graphs with more complex hierarchical structures compared to the greedy algorithms?
- Basis in paper: [explicit] The paper states that the learnable MMF consistently outperforms the original greedy algorithm and the Nyström baseline given the same number of active columns, dL.
- Why unresolved: The paper only evaluates the learnable MMF on three datasets with relatively small sizes. The performance on larger graphs with more complex hierarchical structures remains unknown.
- What evidence would resolve it: Evaluating the learnable MMF on larger graphs with varying levels of complexity and comparing its performance to greedy algorithms.

### Open Question 2
- Question: How does the choice of the rotation matrix size K affect the performance of the learnable MMF algorithm?
- Basis in paper: [explicit] The paper mentions that the learnable MMF uses larger rotations (K=8, 16) compared to the original greedy MMF (K=2), which can yield more expressive factorizations and better approximations.
- Why unresolved: The paper does not explore the impact of different rotation matrix sizes on the performance of the learnable MMF algorithm. The optimal choice of K remains unclear.
- What evidence would resolve it: Conducting experiments with different rotation matrix sizes (K) and comparing the performance of the learnable MMF algorithm.

### Open Question 3
- Question: How does the learnable MMF algorithm scale with increasing matrix size and resolution levels L?
- Basis in paper: [explicit] The paper states that the learnable MMF algorithm is more computationally intensive than greedy methods, but it produces higher quality factorizations and a wavelet basis that better reflects the structure of the underlying matrix or graph.
- Why unresolved: The paper does not provide a detailed analysis of the computational complexity of the learnable MMF algorithm or its scalability with increasing matrix size and resolution levels L.
- What evidence would resolve it: Analyzing the computational complexity of the learnable MMF algorithm and conducting experiments with varying matrix sizes and resolution levels L to assess its scalability.

## Limitations
- Computational complexity is significantly higher than greedy methods due to metaheuristic search and full Stiefel optimization
- Performance on larger graphs with complex hierarchical structures remains untested
- Limited ablation studies to isolate the impact of learnable MMF wavelets versus other architectural changes

## Confidence
- Core claim of learnable MMF's superiority: Medium
- Architectural design (WNN with sparse wavelets) for citation graphs: High
- Architectural design (WNN with sparse wavelets) for molecular graphs: Low

## Next Checks
1. Run ablation on Cora: replace learnable MMF wavelets with random orthogonal wavelets and fixed Fourier basis; measure accuracy and inference time to isolate MMF's contribution.
2. Test EA robustness on Karate Club: vary crossover operator (with/without duplicate preservation), population size, and mutation rate; record fitness convergence curves.
3. Scale up molecular graph experiments: include baselines using standard GNNs (GCN, GAT) and WNNs with Nyström wavelets; report accuracy, sparsity, and runtime per graph.