---
ver: rpa2
title: 'The Indoor-Training Effect: unexpected gains from distribution shifts in the
  transition function'
arxiv_id: '2401.15856'
source_url: https://arxiv.org/abs/2401.15856
tags:
- grid
- reward
- mean
- episodes
- pong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Indoor-Training Effect reveals that training reinforcement
  learning agents on noise-free environments can sometimes lead to better performance
  when tested on noisy environments, compared to training and testing on the same
  noisy environment. The study introduces Noise Injection, a method to generate controlled
  variations of Markov Decision Processes (MDPs) by adding Gaussian noise to transition
  probabilities.
---

# The Indoor-Training Effect: unexpected gains from distribution shifts in the transition function

## Quick Facts
- arXiv ID: 2401.15856
- Source URL: https://arxiv.org/abs/2401.15856
- Authors: Serena Bono; Spandan Madan; Ishaan Grover; Mao Yasueda; Cynthia Breazeal; Hanspeter Pfister; Gabriel Kreiman
- Reference count: 40
- One-line primary result: Training RL agents on noise-free environments can outperform training and testing on the same noisy environment

## Executive Summary
The Indoor-Training Effect reveals a counterintuitive phenomenon in reinforcement learning where agents trained on noise-free environments and tested on noisy ones outperform agents trained and tested on the same noisy environment. The study introduces a method called Noise Injection that generates controlled variations of Markov Decision Processes by adding Gaussian noise to transition probabilities. Experiments across 60 variations of ATARI games demonstrate that this effect extends beyond noise variations to semantic game modifications. Analysis shows that performance differences correlate with exploration patterns, where agents trained on noise-free environments explore fundamental state-action pairs more thoroughly, leading to better generalization.

## Method Summary
The research introduces Noise Injection to create controlled variations of MDPs by adding Gaussian noise to transition probabilities. The study tests two exploration strategies (Boltzmann and epsilon-greedy) across 60 variations of ATARI games (PacMan, Pong, Breakout) with both noise variations and semantic modifications. Two agent types are compared: Generalization agents trained on noise-free environments and tested on noisy ones, versus Learnability agents trained and tested on the same noisy environment. The experiments use Q-learning and SARSA algorithms, training for 1,000 episodes with 500 agents averaged, and evaluate performance using mean reward curves and Area Under Curve metrics while analyzing state-action pair exploration patterns.

## Key Results
- Generalization agents (trained on noise-free, tested on noisy) outperform Learnability agents (trained and tested on noisy) across 60 ATARI game variations
- Performance gains correlate with differences in state-action pair exploration patterns between agent types
- The effect extends beyond noise variations to semantic game modifications like TeleportingGhost and FollowingPaddle
- Analysis reveals that noise-free training enables more thorough exploration of fundamental state-action pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training in noise-free environments leads to better exploration of fundamental state-action pairs that generalize to noisy environments.
- **Mechanism:** Noise-free environments allow agents to focus on mastering core dynamics without distraction from noise-induced non-standard transitions. This results in policies that explore essential state-action pairs more thoroughly.
- **Core assumption:** Noise-free environments provide cleaner signal-to-noise ratio for learning fundamental transitions.
- **Evidence anchors:**
  - [abstract] "Analysis of exploration patterns reveals that the performance gap correlates with differences in state-action pair exploration between agents."
  - [section] "Lδ agents outperformed GT agents, as expected from the literature, when GT agents fail to explore the same State-Action pairs as the Lδ agents."
  - [corpus] Weak correlation - corpus focuses on robustness to noise but not the specific Indoor-Training Effect mechanism.
- **Break condition:** If noise in testing environment is so high that non-standard transitions dominate the state space, the benefit of noise-free training diminishes.

### Mechanism 2
- **Claim:** Noise-free training creates more conservative policies that are less likely to overfit to specific noisy patterns.
- **Mechanism:** Training without noise prevents agents from learning spurious correlations between noise patterns and rewards. When tested in noisy environments, these conservative policies are more robust.
- **Core assumption:** Noise during training can create misleading reward signals that don't generalize.
- **Evidence anchors:**
  - [abstract] "training on the noise-free environment and tested on the noisy δ-environments... compared to training and testing on the same δ-environments."
  - [section] "agents can perform better when trained on the noise-free environment and tested on the noisy δ-environments"
  - [corpus] No direct evidence - corpus discusses robustness to noise but not the counterintuitive benefit of noise-free training.
- **Break condition:** If the testing environment's noise characteristics are predictable and consistent, training with similar noise might be beneficial.

### Mechanism 3
- **Claim:** The Indoor-Training Effect is strongest when there's moderate divergence in exploration patterns between training and testing environments.
- **Mechanism:** When agents trained on noise-free environments explore different state-action pairs than those trained on noisy environments, they discover complementary strategies that perform better in testing.
- **Core assumption:** Different exploration patterns lead to discovering different optimal policies.
- **Evidence anchors:**
  - [abstract] "Analysis of exploration patterns reveals that the performance gap correlates with differences in state-action pair exploration between agents."
  - [section] "the performance gap between agents is indeed correlated with their exploration patterns under different transition probabilities"
  - [corpus] Weak evidence - corpus discusses exploration but not the specific correlation with Indoor-Training Effect.
- **Break condition:** If exploration patterns are too divergent, agents may fail to learn the fundamental dynamics needed for the task.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs)
  - Why needed here: The Indoor-Training Effect operates on MDPs with modified transition functions. Understanding MDP structure is essential to grasp how noise injection creates δ-environments.
  - Quick check question: In an MDP, what four components define the environment structure?

- **Concept:** Exploration vs Exploitation tradeoff
  - Why needed here: The Indoor-Training Effect analysis focuses on exploration patterns. Understanding this tradeoff explains why different training conditions lead to different exploration strategies.
  - Quick check question: What exploration strategy would you use if you wanted to thoroughly map out all state-action pairs in a new environment?

- **Concept:** Distribution shifts and generalization
  - Why needed here: The Indoor-Training Effect is fundamentally about how policies trained on one distribution (noise-free) generalize to another (noisy). This concept is central to understanding the phenomenon.
  - Quick check question: What's the difference between interpolation and extrapolation in the context of domain generalization?

## Architecture Onboarding

- **Component map:** MDP Generator → Agent Trainer (noise-free) → Agent Trainer (noisy) → Evaluator → Analysis Module
- **Critical path:** MDP Generator creates original and δ-environments → Agent Trainer implements Q-Learning/SARSA with exploration strategies → Evaluator measures performance and explores state-action pairs → Analysis Module correlates exploration patterns with performance
- **Design tradeoffs:**
  - Noise level selection: Too little noise may not demonstrate the effect; too much may make learning impossible
  - Exploration strategy choice: Boltzmann exploration vs epsilon-greedy affects state-action pair coverage
  - MDP complexity: Simpler MDPs may not capture the full phenomenon; too complex may obscure results
- **Failure signatures:**
  - No performance difference between agents: May indicate insufficient noise level or exploration strategy not capturing state-action differences
  - Generalization agent performs worse: Could indicate that noise-free training missed critical dynamics
  - Inconsistent results across runs: May suggest need for more training episodes or better hyperparameter tuning
- **First 3 experiments:**
  1. Implement Noise Injection on a simple grid-world MDP and verify that adding Gaussian noise creates δ-environments with modified transition probabilities
  2. Train two agents (noise-free vs noisy) on the same MDP and confirm conventional wisdom that training and testing on the same environment yields better performance
  3. Apply Indoor-Training Effect setup: train one agent on noise-free MDP, another on noisy MDP, test both on noisy MDP, and measure performance difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific neural mechanisms or representations allow agents trained on noise-free environments to better handle noisy test environments compared to agents trained on the same noisy environment?
- Basis in paper: [explicit] The paper notes this is an unexplained phenomenon and mentions analyzing exploration patterns as a first step
- Why unresolved: The study identifies correlation between exploration patterns and performance but doesn't explain the underlying mechanism
- What evidence would resolve it: Detailed analysis of neural network activations or Q-value representations showing why noise-free training leads to more robust policies

### Open Question 2
- Question: Does the Indoor-Training Effect extend to deep reinforcement learning algorithms like PPO or A3C, beyond the Q-learning and SARSA methods tested?
- Basis in paper: [inferred] The paper explicitly notes this as a limitation and only tests classical RL methods
- Why unresolved: The authors state they haven't tested deep RL approaches yet
- What evidence would resolve it: Experiments with modern deep RL algorithms showing similar or different patterns of generalization

### Open Question 3
- Question: How does the magnitude and type of noise in the transition function affect the performance gap between generalization and learnability agents?
- Basis in paper: [explicit] The paper tests only two noise levels (σ=0.1 and σ=0.5) and uses Gaussian noise
- Why unresolved: Limited exploration of the noise parameter space and noise distribution types
- What evidence would resolve it: Systematic study varying noise magnitude and distribution types to map the relationship between noise characteristics and generalization benefits

## Limitations

- The study only tests classical RL methods (Q-learning and SARSA) and hasn't explored deep RL algorithms like PPO or A3C
- Limited exploration of noise parameter space, testing only two noise levels (σ=0.1 and σ=0.5) with Gaussian distributions
- The underlying mechanism explaining why noise-free training leads to better exploration remains unclear beyond observed correlations

## Confidence

- **High confidence:** The empirical observation that Generalization agents outperform Learnability agents in controlled experiments
- **Medium confidence:** The correlation between exploration patterns and performance gaps
- **Low confidence:** The precise mechanism explaining why noise-free training leads to better exploration of fundamental state-action pairs

## Next Checks

1. **Mechanism Validation:** Design an experiment that explicitly controls for exploration patterns - train agents with identical exploration strategies but different noise conditions, then measure whether the performance gap persists.

2. **Noise Sensitivity Analysis:** Systematically vary noise levels from very low to very high to identify the threshold where the Indoor-Training Effect breaks down, establishing the conditions under which the phenomenon operates.

3. **Generalization Test:** Apply the Indoor-Training Effect to a completely different domain (e.g., robotics simulation or continuous control tasks) to verify whether the phenomenon extends beyond discrete ATARI games.