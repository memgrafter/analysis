---
ver: rpa2
title: 'The Use of Multimodal Large Language Models to Detect Objects from Thermal
  Images: Transportation Applications'
arxiv_id: '2406.13898'
source_url: https://arxiv.org/abs/2406.13898
tags:
- thermal
- images
- multimodal
- image
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated Multimodal Large Language Models (MLLMs) for
  object detection in thermal and RGB images for transportation applications. Using
  a thermal ADAS dataset, GPT-4 and Gemini were tested on zero-shot in-context learning
  to detect and classify objects like pedestrians, bikes, cars, and motorcycles.
---

# The Use of Multimodal Large Language Models to Detect Objects from Thermal Images: Transportation Applications

## Quick Facts
- **arXiv ID**: 2406.13898
- **Source URL**: https://arxiv.org/abs/2406.13898
- **Reference count**: 0
- **Primary result**: MLLMs show moderate generalization across thermal and RGB modalities for object detection, with highest accuracy for cars and lowest for motorcycles

## Executive Summary
This study evaluates Multimodal Large Language Models (MLLMs) for object detection in thermal and RGB images within transportation contexts. Using a thermal ADAS dataset, GPT-4 and Gemini were tested on zero-shot in-context learning to detect and classify objects like pedestrians, bikes, cars, and motorcycles. The research demonstrates that MLLMs can effectively process thermal images and achieve moderate generalization capabilities, though performance varies significantly across object categories and imaging conditions. The models show improved accuracy when analyzing paired RGB-thermal images from the same scene, highlighting the potential of multimodal integration in advanced imaging automation for Intelligent Transportation Systems and autonomous driving applications.

## Method Summary
The study employed zero-shot in-context learning to test GPT-4 and Gemini on a thermal ADAS dataset for object detection tasks. The evaluation used both thermal-only and paired RGB-thermal images from the same or different scenes. Object categories included pedestrians, bikes, cars, and motorcycles. Performance metrics included Mean Absolute Percentage Error (MAPE) for detection accuracy and F1 scores for classification performance. The experimental design systematically compared single-modality versus multimodal inputs and examined consistency across different imaging conditions and object sizes.

## Key Results
- Detection accuracy was highest for cars (F1 up to 0.85 for paired images) and lowest for motorcycles (MAPE up to 96.15% for GPT-4)
- Pedestrian detection MAPE ranged from 70.39% (GPT-4) to 81.48% (Gemini)
- Models performed better with paired RGB-thermal images from the same scene compared to single modality inputs
- Multimodal integration improved object detection in complex environments, though scene consistency remained challenging

## Why This Works (Mechanism)
MLLMs leverage their inherent multimodal training capabilities to process thermal and RGB images through parallel feature extraction pathways. The models can identify patterns and correlations across different imaging modalities, allowing them to cross-reference information when both thermal and RGB data are available. This multimodal reasoning capability enables the models to compensate for limitations in one modality using information from another, particularly useful in transportation scenarios where lighting conditions and environmental factors affect image quality differently across modalities.

## Foundational Learning
- **Zero-shot in-context learning**: Why needed - Enables testing without model fine-tuning; Quick check - Verify prompt engineering effectiveness across object categories
- **Thermal imaging principles**: Why needed - Understanding heat signatures vs visible light detection; Quick check - Compare thermal vs RGB feature extraction capabilities
- **Multimodal fusion techniques**: Why needed - Integrating information from different imaging sources; Quick check - Measure performance gains from paired vs single modality inputs
- **ADAS dataset characteristics**: Why needed - Understanding real-world transportation scenarios; Quick check - Validate dataset coverage of diverse environmental conditions
- **Object detection metrics**: Why needed - Quantifying model performance across categories; Quick check - Compare MAPE and F1 score consistency across experiments

## Architecture Onboarding

**Component Map**: Thermal/RGB Image Input -> Feature Extraction -> Cross-modal Fusion -> Object Classification -> Detection Output

**Critical Path**: Image preprocessing → Feature extraction → Multimodal fusion → Classification → Post-processing

**Design Tradeoffs**: The study prioritized zero-shot learning over fine-tuning, sacrificing some accuracy for broader applicability. Using paired images from the same scene improves performance but may not reflect real-world scenarios where RGB and thermal data aren't always perfectly aligned.

**Failure Signatures**: Highest errors occur with small objects (motorcycles), across-scene modality inconsistencies, and single-modality inputs. MAPE values exceeding 80% indicate significant detection challenges.

**First Experiments**:
1. Test single-modality thermal vs RGB performance on identical scenes to establish baseline differences
2. Evaluate cross-scene paired image performance to quantify consistency requirements
3. Measure object size correlation with detection accuracy across all categories

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance degradation for smaller objects like motorcycles (MAPE up to 96.15%)
- Cross-scene consistency challenges when RGB and thermal images are not from identical scenes
- Current accuracy levels insufficient for safety-critical transportation applications
- Limited evaluation of diverse environmental conditions and lighting scenarios

## Confidence
- **Model performance generalization**: Medium - Results show consistent patterns but with notable category-specific variations
- **Multimodal integration benefits**: High - Clear performance improvements with paired same-scene images
- **Safety application readiness**: Low - Current accuracy levels, particularly for motorcycles, are inadequate for safety-critical systems
- **Zero-shot learning effectiveness**: Medium - Demonstrates capability but with room for improvement through fine-tuning

## Next Checks
1. Validate model performance across a broader range of environmental conditions and weather scenarios
2. Test fine-tuning approaches to improve accuracy for small object categories like motorcycles
3. Evaluate real-time processing capabilities and computational efficiency for autonomous driving applications