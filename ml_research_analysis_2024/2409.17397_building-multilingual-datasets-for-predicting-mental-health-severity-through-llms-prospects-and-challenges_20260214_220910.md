---
ver: rpa2
title: 'Building Multilingual Datasets for Predicting Mental Health Severity through
  LLMs: Prospects and Challenges'
arxiv_id: '2409.17397'
source_url: https://arxiv.org/abs/2409.17397
tags:
- health
- mental
- languages
- language
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study created a multilingual dataset for mental health severity
  prediction by translating English social media posts into six languages (Greek,
  Turkish, French, Portuguese, German, Finnish) using LLMs. GPT-3.5 and Llama3.1 were
  then used to predict depression and suicide severity levels across languages.
---

# Building Multilingual Datasets for Predicting Mental Health Severity through LLMs: Prospects and Challenges

## Quick Facts
- arXiv ID: 2409.17397
- Source URL: https://arxiv.org/abs/2409.17397
- Reference count: 12
- Primary result: Multilingual mental health severity prediction using LLM-translated datasets shows language-dependent performance, with F1 scores ranging from 0.15-0.67 across languages and severity levels.

## Executive Summary
This study investigates the feasibility of using large language models to predict mental health severity levels across multiple languages by translating English social media posts. The researchers created a multilingual dataset by translating English posts into six languages (Greek, Turkish, French, Portuguese, German, Finnish) using GPT-3.5, then used GPT-3.5, GPT-4o-mini, and Llama3.1 to predict depression and suicide severity levels. The approach demonstrates that LLM-based translation enables cross-lingual mental health assessment, though performance varies significantly across languages, with Greek showing the best results (F1=0.66) and Turkish the worst. The study highlights both the potential for cost-effective multilingual mental health support and the risks of relying solely on automated diagnosis.

## Method Summary
The researchers created multilingual mental health datasets by translating English social media posts (from Reddit and Twitter) into six languages using GPT-3.5-turbo with temperature=0. They then applied zero-shot and one-shot learning approaches, prompting LLMs (GPT-3.5-turbo, GPT-4o-mini, and Llama3.1) to classify severity levels for depression and suicide risk. Performance was evaluated using Precision, Recall, and F1 scores across languages, comparing predictions to ground truth labels. The DEP-SEVERITY dataset (depression severity) and SUI-TWI dataset (suicide risk) were used, with severity levels ranging from 0-3 for depression and 0-2 for suicide risk.

## Key Results
- GPT-4o-mini outperformed GPT-3.5 across all languages, achieving F1=0.27 vs 0.17 on DEP-SEVERITY in English
- Greek showed best overall performance (F1=0.66), while Turkish showed worst (F1=0.17-0.30)
- Language performance varied significantly: English (F1=0.25-0.44), French (F1=0.23-0.45), German (F1=0.15-0.67), Portuguese (F1=0.22-0.47), Finnish (F1=0.16-0.34)
- Suicide dataset showed similar patterns with lower overall performance (F1=0.17-0.34 across languages)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based translation followed by classification preserves enough semantic content to allow cross-lingual mental health severity prediction.
- Mechanism: Translation transfers the linguistic indicators of mental health conditions from English to target languages, enabling the same classification model to operate on non-English data.
- Core assumption: Semantic meaning and mental health indicators are preserved through machine translation, despite potential losses.
- Evidence anchors:
  - [abstract] states the dataset is "automatically translated from English using an LLM" and "enables a comprehensive evaluation of LLM performance in detecting mental health conditions."
  - [section] shows results where models predict severity across languages, though performance varies.
  - [corpus] includes "Large Language Models for Mental Health: A Multilingual Evaluation" with similar findings.
- Break condition: If translation introduces semantic drift or cultural context is lost, model performance would degrade sharply in target languages.

### Mechanism 2
- Claim: Performance variability across languages reflects language-specific model coverage and cultural-linguistic nuances.
- Mechanism: Models trained or fine-tuned primarily on high-resource languages (e.g., English, French, German) perform better when applied to those languages, while low-resource languages (e.g., Turkish) show worse performance.
- Core assumption: Model effectiveness is tied to the quantity and quality of mental health data in each language during training.
- Evidence anchors:
  - [abstract] notes "considerable variability in performance across languages" and "language-specific nuances and mental health data coverage can effect the accuracy."
  - [section] reports best performance in Greek (F1=0.66) and worst in Turkish, with Turkish performance dropping the most in the suicide dataset.
  - [corpus] cites "Resources for turkish natural language processing: A critical survey" explaining limited Turkish resources.
- Break condition: If mental health datasets become equally available across languages, performance gaps should narrow.

### Mechanism 3
- Claim: Stronger LLM variants (GPT-4o-mini vs GPT-3.5) improve cross-lingual performance, indicating model capacity is a limiting factor.
- Mechanism: Larger or more capable models can better handle translation noise and subtle linguistic cues across languages.
- Core assumption: Model architecture and scale directly influence ability to generalize across linguistic contexts.
- Evidence anchors:
  - [section] shows GPT-4o-mini achieves F1=0.27 vs GPT-3.5's 0.17 on DEP-SEVERITY in English, and similarly better results in target languages.
  - [section] states "we observe performance gains... with stronger models."
  - [corpus] includes "Large Language Models for Mental Health: A Multilingual Evaluation" exploring different model capabilities.
- Break condition: If translation quality is the dominant bottleneck, upgrading the model alone may not yield proportional gains.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The study depends on transferring mental health classification performance from English to other languages via translation.
  - Quick check question: What are the main challenges of applying NLP models trained on one language to another without direct training data?

- Concept: Mental health symptom detection from text
  - Why needed here: The core task is detecting depression and suicide severity from social media posts.
  - Quick check question: What linguistic cues do models typically look for to identify depression or suicidal ideation in text?

- Concept: Dataset balancing and class imbalance
  - Why needed here: DEP-SEVERITY is highly imbalanced, with most posts in the minimal severity class, affecting model training and evaluation.
  - Quick check question: How does class imbalance in a dataset impact the performance metrics of a classification model?

## Architecture Onboarding

- Component map:
  English social media posts -> LLM translation to target languages -> LLM classification of severity levels -> Performance evaluation metrics

- Critical path:
  1. Load English dataset
  2. Translate each post to six target languages using LLM
  3. For each language, prompt LLM to classify severity
  4. Compare predictions to ground truth
  5. Compute and report metrics

- Design tradeoffs:
  - Cost vs. accuracy: Using 0-shot learning minimizes cost but may reduce accuracy compared to fine-tuning.
  - Translation vs. direct modeling: Translating is cheaper than collecting native-language data but risks semantic loss.
  - Model choice: Stronger models (GPT-4o-mini) improve performance but increase API costs.

- Failure signatures:
  - Consistently low F1 across all classes in a language suggests translation quality issues or model unfamiliarity with that language.
  - High variance in F1 across classes in a language may indicate imbalanced class representation or ambiguous linguistic cues.
  - Degradation when moving from 0-shot to 1-shot could signal sensitivity to prompt format or example quality.

- First 3 experiments:
  1. Run DEP-SEVERITY classification in English only (no translation) to establish baseline.
  2. Translate a small subset of posts to one target language and classify to check translation impact.
  3. Compare GPT-3.5 vs GPT-4o-mini on the same translated dataset to measure model capacity effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary across different languages for mental health severity prediction, and what specific linguistic or cultural factors contribute to these variations?
- Basis in paper: [explicit] The study observes considerable variability in performance across languages, with some languages showing better results than English.
- Why unresolved: While the study identifies variability, it does not delve into the specific linguistic or cultural factors that contribute to these differences. The reasons behind why certain languages perform better or worse remain unexplored.
- What evidence would resolve it: Detailed linguistic analysis and cultural studies comparing the nuances and context of mental health expressions in each language could provide insights into the performance variations.

### Open Question 2
- Question: What are the potential risks and ethical considerations of using LLMs for mental health diagnosis across different languages, especially in terms of misdiagnosis and cultural sensitivity?
- Basis in paper: [explicit] The study emphasizes the risks of relying exclusively on LLMs in medical settings due to potential misdiagnoses and highlights the need for caution.
- Why unresolved: The study acknowledges the risks but does not explore the ethical implications or propose frameworks for mitigating these risks in multilingual contexts.
- What evidence would resolve it: Research into the ethical frameworks and guidelines for LLM use in mental health, along with case studies of misdiagnosis incidents, could inform best practices.

### Open Question 3
- Question: How can LLMs be improved to better handle low-resource languages in mental health applications, and what strategies can be employed to enhance their performance?
- Basis in paper: [inferred] The study mentions the challenge of low-resource languages and the need for innovative approaches in transfer learning to leverage sparse resources.
- Why unresolved: The paper suggests the need for improvement but does not propose specific strategies or methods to enhance LLM performance for low-resource languages.
- What evidence would resolve it: Experiments with different transfer learning techniques, data augmentation strategies, and multilingual model architectures could demonstrate improvements in low-resource language performance.

## Limitations

- Translation quality introduces uncertainty about semantic preservation across languages, particularly for low-resource languages like Turkish
- 0-shot learning approach may underperform compared to fine-tuned models for nuanced mental health classification tasks
- Absence of clinical validation means automated severity predictions cannot be used for actual diagnosis or treatment decisions

## Confidence

**High confidence**: The methodology for creating multilingual datasets through LLM translation is clearly specified and reproducible. The observed performance patterns across languages (Greek best, Turkish worst) align with expectations based on language resource availability and model coverage.

**Medium confidence**: The translation quality and its impact on mental health severity detection remains uncertain. While the approach works for some languages, the variable performance suggests that semantic meaning may not be fully preserved across all translations.

**Low confidence**: The clinical validity of LLM-predicted severity levels is not established. The study demonstrates technical feasibility but cannot verify whether predicted severity levels correspond to actual clinical conditions across different cultural contexts.

## Next Checks

1. **Translation quality validation**: Manually inspect a random sample of 50 translated posts from each language, comparing them to original English posts to assess semantic preservation and identify translation failures that could impact classification accuracy.

2. **Clinical expert review**: Have mental health professionals evaluate model predictions for 100 randomly selected cases across languages to determine if predicted severity levels align with clinical assessment, focusing on cases where automated predictions differ from ground truth.

3. **Cross-cultural symptom validation**: Analyze whether linguistic indicators of depression and suicide risk identified by the model are culturally appropriate across target languages, identifying potential false positives/negatives due to cultural differences in symptom expression.