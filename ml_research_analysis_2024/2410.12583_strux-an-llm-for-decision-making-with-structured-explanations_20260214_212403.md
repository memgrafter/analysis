---
ver: rpa2
title: 'STRUX: An LLM for Decision-Making with Structured Explanations'
arxiv_id: '2410.12583'
source_url: https://arxiv.org/abs/2410.12583
tags:
- facts
- decision
- fact
- decisions
- stock
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STRUX, a framework for improving LLM decision-making
  through structured explanations. The method involves distilling lengthy information
  into concise fact tables, using self-reflection to categorize key facts as favorable
  or adverse, and fine-tuning LLMs to prioritize these facts for decision optimization.
---

# STRUX: An LLM for Decision-Making with Structured Explanations

## Quick Facts
- arXiv ID: 2410.12583
- Source URL: https://arxiv.org/abs/2410.12583
- Reference count: 19
- Primary result: 25.55% accuracy on stock investment decisions, outperforming DeLLMa (22.35%) and direct prompting (20.27%)

## Executive Summary
This paper introduces STRUX, a framework for improving LLM decision-making through structured explanations. The method involves distilling lengthy information into concise fact tables, using self-reflection to categorize key facts as favorable or adverse, and fine-tuning LLMs to prioritize these facts for decision optimization. Evaluated on stock investment decisions based on earnings call transcripts, STRUX achieves 25.55% accuracy, outperforming strong baselines including DeLLMa (22.35%) and direct prompting methods (20.27%). The framework enhances decision transparency by structuring explanations with supporting facts and their assigned strengths, representing a meaningful advancement in practical LLM decision-making applications.

## Method Summary
STRUX is a multi-stage framework that enhances LLM decision-making through structured explanations. It first extracts relevant facts from earnings call transcripts using GPT-4o-mini, then employs self-reflection to create training instances by having the model identify flaws in its reasoning. The system fine-tunes Llama3-8b using SFT on demonstrations and RL with PPO using pairwise comparisons between correct and incorrect decisions. The structured explanations present facts in a table format with "+" or "-" symbols indicating their impact strength on the final decision, improving transparency and interpretability.

## Key Results
- 25.55% accuracy on stock investment decisions, outperforming DeLLMa (22.35%) and direct prompting (20.27%)
- Self-reflection rounds improve accuracy progressively, with 4 rounds showing optimal performance
- Structured explanations with strength ratings enhance decision transparency compared to free-text explanations

## Why This Works (Mechanism)

### Mechanism 1
Self-reflection improves decision accuracy through iterative refinement. The system uses GPT-4o-mini to analyze its own mistakes, identify flaws in fact selection and strength assignment, and generate alternative decisions without seeing the correct answer. This creates a training signal for fine-tuning. The core assumption is that the LLM can identify its own reasoning flaws and generate meaningfully different decisions when instructed to avoid previous ones.

### Mechanism 2
Structured explanations with strength ratings improve decision transparency. By organizing supporting facts into a table with "+" or "-" symbols indicating impact strength, the system creates interpretable decision pathways that humans can review and modify. The core assumption is that users can understand and evaluate decisions better when facts are presented with explicit strength indicators rather than free text.

### Mechanism 3
Fine-tuning with demonstration and comparison data improves decision prioritization. The system uses SFT on correct decisions and RL with a reward model trained on pairwise comparisons between correct and incorrect decisions, teaching the LLM to prioritize relevant facts. The core assumption is that the reward model can effectively distinguish between good and bad decisions, and the policy optimization can learn from these comparisons.

## Foundational Learning

- Concept: Decision theory under uncertainty
  - Why needed here: The system must weigh multiple factors with different impacts to make rational investment decisions
  - Quick check question: How would you assign utilities to different stock outcomes and calculate expected utility given uncertain future performance?

- Concept: Reinforcement learning with proximal policy optimization
  - Why needed here: The system uses PPO to optimize the policy for making better decisions based on rewards from the reward model
  - Quick check question: What role does the KL penalty term play in preventing the policy from diverging too far from the original model?

- Concept: Fact summarization and extraction
  - Why needed here: The system must distill lengthy earnings call transcripts into concise, relevant facts for decision-making
  - Quick check question: What criteria would you use to determine which facts from a transcript are most relevant for predicting stock price movement?

## Architecture Onboarding

- Component map: Fact extraction module (gpt-4o-mini) -> Self-reflection engine (gpt-4o-mini) -> SFT fine-tuning pipeline (Llama3-8b) -> Reward model training (linear layer on SFT embeddings) -> PPO reinforcement learning loop -> Decision generation interface

- Critical path: Fact extraction → Self-reflection → SFT fine-tuning → Reward model training → PPO fine-tuning → Decision generation

- Design tradeoffs:
  - Using gpt-4o-mini for fact extraction vs. open-source models (better quality vs. cost/compute)
  - Number of self-reflection iterations (more iterations improve accuracy but increase compute cost)
  - Fact selection range (6-10 facts optimal, but system could adapt based on transcript length)
  - Reward model complexity (simple linear vs. more complex architectures)

- Failure signatures:
  - Fact extraction produces irrelevant or incorrect facts → decisions become unreliable
  - Self-reflection gets stuck in loops → no improvement in decision quality
  - Reward model learns spurious correlations → PPO optimizes for wrong signals
  - PPO policy diverges too far → decisions become inconsistent or nonsensical

- First 3 experiments:
  1. Run fact extraction on sample transcripts and verify the extracted facts are relevant and accurate
  2. Test self-reflection on known incorrect decisions and verify it generates different decisions
  3. Evaluate the base Llama3 model on a small test set before any fine-tuning to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
How can the fact extraction process be improved to better capture negative financial factors that might influence stock prices? The current method relies on LLMs to summarize earnings call transcripts, which may inherently lean towards positive language used by company executives, potentially missing important negative factors.

### Open Question 2
What is the optimal number of self-reflection iterations needed to improve decision-making accuracy without causing decision instability? While the paper uses 4 rounds of reflection, it doesn't determine if this is optimal or if there's a point of diminishing returns where additional reflection might harm decision stability.

### Open Question 3
How can STRUX be adapted to handle real-time market data and external factors beyond earnings call transcripts? The current implementation focuses solely on textual information from earnings call transcripts and excludes acoustic features or real-time market data.

## Limitations
- Evaluation focuses exclusively on stock investment decisions, limiting generalizability to other domains
- 25.55% accuracy, while outperforming baselines, remains relatively low for financial decision-making applications
- Relies on GPT-4o-mini for fact extraction and self-reflection, creating potential single points of failure

## Confidence
- **High confidence** in the core methodology: The structured approach follows established patterns in LLM optimization with clearly specified implementation details
- **Medium confidence** in the performance claims: While experimental results show improvement, absolute accuracy remains low and test set size is modest
- **Medium confidence** in the self-reflection mechanism: Innovative concept but effectiveness depends heavily on reflection quality and consistency

## Next Checks
1. **Cross-domain transferability test**: Apply the STRUX framework to a different decision-making task (e.g., medical diagnosis from clinical notes) to verify whether performance gains generalize beyond financial contexts
2. **Ablation study on self-reflection**: Systematically vary the number of self-reflection iterations and measure marginal improvement in decision accuracy to determine if computational overhead is justified
3. **Human evaluation of transparency**: Conduct user studies where financial analysts review structured explanations with strength ratings versus free-text explanations to verify improved interpretability and trust