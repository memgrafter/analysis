---
ver: rpa2
title: 'LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal
  Demonstrations'
arxiv_id: '2412.01441'
source_url: https://arxiv.org/abs/2412.01441
tags:
- 'false'
- 'true'
- gemini
- actions
- flash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a comprehensive benchmark for evaluating the
  multimodal in-context imitation learning capabilities of frontier language models
  across long contexts (up to 1 million tokens). The authors investigate whether modern
  models can learn to act in dynamic environments by generalizing from large numbers
  of expert demonstrations, testing across six interactive decision-making tasks including
  chess, tic-tac-toe, Atari games, grid navigation, crosswords, and cheetah control.
---

# LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations

## Quick Facts
- **arXiv ID**: 2412.01441
- **Source URL**: https://arxiv.org/abs/2412.01441
- **Reference count**: 40
- **Primary result**: Most frontier models fail to reach expert performance on challenging tasks despite up to 512 demonstration episodes

## Executive Summary
This paper introduces LMAct, a benchmark for evaluating multimodal in-context imitation learning capabilities of frontier language models across six interactive decision-making tasks with contexts up to 1 million tokens. The authors test whether modern models can learn to act in dynamic environments by generalizing from large numbers of expert demonstrations, finding that while models consistently outperform random baselines, they struggle to translate factual knowledge into effective decision-making even with extensive in-context learning support. The benchmark reveals a persistent "knowing-doing gap" where current models fail to match expert performance on complex tasks like Atari, chess, and DM Control despite optimizing prompts and using hundreds of demonstrations.

## Method Summary
The benchmark evaluates models on six interactive environments (tic-tac-toe, chess, Atari Phoenix, grid navigation, crosswords, cheetah control) using zero-shot preamble + expert demonstrations + separator + evaluation trajectory prompt structure. Models receive demonstrations ranging from 0 to 512 episodes, with observations encoded as either text (ASCII, FEN/PGN) or images (RGB). Each model-task combination is evaluated across 100 episodes with different initial conditions, measuring average cumulative reward. The study tests various prompt engineering strategies including chain-of-thought prompting and display of legal actions, while investigating how performance varies with observation modality, context length, and number of demonstrations.

## Key Results
- Most models fail to reach expert performance on challenging tasks like Atari, chess, and DM Control even with 512 demonstration episodes
- Models consistently outperform random baselines but show little improvement beyond 1-2 demonstrations on complex tasks
- Some models show steady improvement with more demonstrations on easier tasks like grid navigation and tic-tac-toe, but this trend is not universal
- Performance is largely independent of the number of demonstrations presented on most tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models can learn algorithmic patterns through in-context imitation learning when demonstrations are formatted correctly
- Mechanism: Pretrained transformers recognize sequential decision-making patterns and can reproduce them when demonstrations follow consistent formatting with clear observation-action pairs
- Core assumption: Models have sufficient capacity to attend to and reproduce patterns across hundreds of demonstration steps within context
- Evidence anchors: [abstract] "the ability to learn new patterns and imitate algorithms in context"; [section 2.3] "We may, however, show the available legal actions... depending on whether it is beneficial per model and task"

### Mechanism 2
- Claim: Multimodal state representations enable better in-context learning than text-only representations
- Mechanism: Visual information provides richer context that helps models understand task state beyond what text descriptions can capture
- Core assumption: Models can effectively process and integrate multimodal inputs within their context windows
- Evidence anchors: [abstract] "We investigate the effect of encoding observations as text or images"; [section 3.2] Models perform differently across observation formats

### Mechanism 3
- Claim: Chain-of-thought prompting enhances decision-making performance by encouraging explicit reasoning
- Mechanism: Models benefit from structured reasoning steps that break down decision-making into manageable sub-problems
- Core assumption: Models possess sufficient reasoning capability that can be activated through appropriate prompting
- Evidence anchors: [abstract] "We investigate the effect of... chain-of-thought prompting"; [section 2.3] "we may use a chain-of-thought... style prompt"

## Foundational Learning

- **Concept**: Attention mechanisms and context window limitations
  - Why needed here: Understanding why models struggle with long contexts (up to 1M tokens) and how they attend to demonstrations
  - Quick check question: If a model has 200K token context and each demonstration episode is 100 steps with 10 tokens per step, how many full episodes can it fit?

- **Concept**: Multimodal token processing and cross-modal integration
  - Why needed here: Models process text and images differently - images consume fixed tokens regardless of content while text scales with length
  - Quick check question: Why does Phoenix (image-only) pose greater context challenges than tic-tac-toe (text-only) even with similar episode lengths?

- **Concept**: In-context learning as meta-learning
  - Why needed here: Understanding how demonstrations serve as conditioning information that enables few-shot generalization
  - Quick check question: What's the difference between learning from 512 demonstrations versus learning from 1 demonstration repeated 512 times?

## Architecture Onboarding

- **Component map**: Environment simulators → Prompt builder → Model API → Evaluation pipeline → Results aggregator
- **Critical path**: Environment → Generate demonstration episodes → Build prompt with demonstrations → Call model → Parse action → Apply to environment → Collect reward
- **Design tradeoffs**: Text vs image observations (token efficiency vs richness), legal actions shown vs inferred (safety vs model capability), chain-of-thought vs direct action (reasoning vs efficiency)
- **Failure signatures**: Illegal actions (format mismatch), repeated actions (stuck behavior), context overflow (truncated demonstrations), poor performance despite demonstrations (inadequate pattern recognition)
- **First 3 experiments**:
  1. Grid world navigation with ASCII observations and 1 demonstration episode - should show basic in-context learning capability
  2. Tic-tac-toe with RGB observations and varying numbers of demonstrations - tests multimodal integration
  3. Phoenix with text observations (if available) vs images - isolates effect of modality on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the performance degradation observed with increasing numbers of demonstration episodes (in-context interference) in certain tasks like cheetah control and grid world navigation with ASCII observations?
- Basis in paper: [inferred] The authors observe that models' performance deteriorates with increasing numbers of expert demonstrations in cheetah run and grid world navigation with ASCII observations, which they refer to as "in-context interference."
- Why unresolved: The authors conduct initial investigations into this phenomenon but conclude that a definitive answer would require thorough investigation, making it an interesting direction for future work.

### Open Question 2
- Question: Would specialized models designed for long-context tasks (rather than general-purpose frontier models) show significantly better performance on in-context imitation learning?
- Basis in paper: [explicit] The authors note that "it would be interesting to investigate how specialized models that were specifically developed for long-context tasks... would fare on our benchmark."
- Why unresolved: The benchmark evaluates general-purpose frontier models with long context windows, but the authors explicitly suggest comparing against specialized long-context models as future work.

### Open Question 3
- Question: How would reward-conditioned or in-context reinforcement learning approaches compare to pure imitation learning on these interactive decision-making tasks?
- Basis in paper: [explicit] The authors state "it seems plausible that pretraining or finetuning with data from interactive decision-making tasks, and, in particular, in-context imitation of an expert policy, would be quite effective" and mention that "all our tasks could easily be extended by providing additional reward observations."
- Why unresolved: The current benchmark focuses on imitation learning without rewards, but the authors suggest that reward information might be beneficial and that comparing against other methods would be valuable.

## Limitations

- Benchmark focuses on narrow, discrete-action environments that may not capture real-world task complexity
- Results depend heavily on specific prompt engineering choices made by authors
- Evaluation doesn't distinguish between failures of pattern recognition versus failures of reasoning within learned patterns

## Confidence

- **High confidence**: Claims about consistent underperformance on complex tasks despite extensive demonstrations
- **Medium confidence**: Claims about effectiveness of specific prompting strategies (chain-of-thought, legal action display)
- **Low confidence**: Claims about optimal demonstration formatting and presentation

## Next Checks

1. Test whether repeated demonstrations (1 episode × 512 vs 512 unique episodes) produce different performance to isolate pattern recognition from attention effects
2. Evaluate whether fine-tuning models on demonstration data bridges the knowing-doing gap compared to in-context learning alone
3. Assess whether breaking long contexts into smaller chunks with context summaries improves performance on memory-intensive tasks