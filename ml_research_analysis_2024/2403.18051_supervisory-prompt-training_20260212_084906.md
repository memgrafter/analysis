---
ver: rpa2
title: Supervisory Prompt Training
arxiv_id: '2403.18051'
source_url: https://arxiv.org/abs/2403.18051
tags:
- prompt
- generator
- corrector
- questions
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Supervisory Prompt Training (SPT), a novel
  framework that automates the generation of highly effective prompts for large language
  models (LLMs) using a dual LLM system. In SPT, one LLM (the generator) performs
  a task while another LLM (the corrector) provides feedback and generates improved
  prompts.
---

# Supervisory Prompt Training

## Quick Facts
- arXiv ID: 2403.18051
- Source URL: https://arxiv.org/abs/2403.18051
- Reference count: 4
- Primary result: GPT-4 accuracy on GSM8K improved from 65.8% to 94.1% using automated prompt refinement

## Executive Summary
This paper introduces Supervisory Prompt Training (SPT), a framework that automates the generation of highly effective prompts for large language models using a dual LLM system. In SPT, one LLM (the generator) performs a task while another LLM (the corrector) provides feedback and generates improved prompts. Both LLMs collaboratively and continuously improve their prompts over time. The method introduces impact scores to measure sentence-level effectiveness of prompts and was tested on four benchmarks designed to measure hallucinations in LLMs.

## Method Summary
SPT employs two separate LLMs: a generator that performs the target task and a corrector that analyzes the generator's mistakes to improve prompts. The corrector receives the generator's mistakes along with the current prompt and generates candidate meta-prompts. These candidates are tested on the mistake examples to select the best-performing one, which becomes the new prompt for the generator. This iterative process continues with both models refining their outputs. The method also introduces impact scores that measure how much each sentence in a prompt contributes to accuracy, helping guide prompt improvements.

## Key Results
- GPT-4 accuracy on GSM8K improved from 65.8% to 94.1% (28.3% increase)
- Significant reduction in hallucinations across multiple benchmarks
- Demonstrated effectiveness on TruthfulQA, GSM8K, MMLU, and MedQA tasks
- Automated prompt refinement achieved without model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous dual-LLM feedback loop improves generator prompts over time
- Mechanism: Generator LLM attempts task, corrector analyzes mistakes, generates improved prompts; both iterate and refine
- Core assumption: Corrector can reliably analyze generator's errors and propose effective prompt improvements without access to generator's internal parameters
- Evidence anchors:
  - [abstract] "both the generator and corrector collaboratively and continuously improve their prompts over time"
  - [section 3.2] "The corrector is tasked to generate n candidate meta-prompts... that could help the generator avoid these mistakes"
  - [corpus] Weak evidence; only related papers on soft prompt tuning, not dual-LLM feedback
- Break condition: If corrector's feedback quality degrades or becomes repetitive, prompt improvements stall or reverse

### Mechanism 2
- Claim: Impact scores provide measurable feedback for sentence-level prompt effectiveness
- Mechanism: Each sentence in generator's prompt is assigned an accuracy delta; scores are passed to corrector to guide future prompt improvements
- Core assumption: Sentence-level accuracy changes are attributable to individual prompt sentences and measurable by re-running the generator
- Evidence anchors:
  - [section 3.4.2] "The impact score is a measure of how much adding a sentence to pi increases the training accuracy of the generator"
  - [section 4.4] "The impact scores are added at the end of each sentence in pi... and then, passed into the corrector"
  - [corpus] No direct evidence; this is a novel contribution
- Break condition: If sentences interact in complex, non-additive ways, impact scores may misrepresent true contribution

### Mechanism 3
- Claim: Corrector's meta-prompt improvement (SPT-pc) leads to better generator feedback over time
- Mechanism: Corrector analyzes its own failures (when p* and p0 both fail same questions), updates its meta-prompt to avoid those patterns
- Core assumption: Corrector can introspect its own meta-prompt deficiencies and self-correct based on past mistakes
- Evidence anchors:
  - [section 3.3] "the corrector criticizes and improves itself, by giving itself enough information about how to make better c*ᵢ"
  - [section 5.1] "The corrector's meta-prompt considers the complexities of the questions and pushes the corrector to identify them"
  - [corpus] No direct evidence; self-improvement of corrector meta-prompt is unique to this work
- Break condition: If corrector cannot identify root causes of its own mistakes, self-improvement loop fails

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Enables corrector to provide detailed step-by-step analysis of generator's errors for better prompt refinement
  - Quick check question: Can you describe how chain-of-thought differs from direct answer generation in prompting?

- Concept: Reinforcement learning concepts (policy and reward)
  - Why needed here: Underlies iterative prompt improvement where corrector acts as a policy optimizing generator's performance
  - Quick check question: What would serve as the "reward" signal in the SPT framework?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: SPT builds on and automates prompt engineering techniques for task-specific LLM performance
  - Quick check question: How does few-shot prompting differ from zero-shot in terms of prompt structure?

## Architecture Onboarding

- Component map:
  - Generator (G) -> Corrector (C) -> Impact Score Tracker -> Evaluation Pipeline

- Critical path:
  1. G runs on Dtrain → collects mistakes (mpi)
  2. C receives mpi + current prompt → generates candidate prompts
  3. Test candidates on mpi → select best as p*
  4. Evaluate p* on Dtest
  5. If using SPT-pc, C updates its own meta-prompt based on failures

- Design tradeoffs:
  - Computational cost: Multiple LLM calls per iteration vs. potential accuracy gains
  - Data efficiency: Small training sets may limit corrector's ability to generalize improvements
  - Prompt length: Iterative additions may create unwieldy prompts over time

- Failure signatures:
  - Accuracy plateaus despite iterations
  - Impact scores become uniformly low or negative
  - Corrector generates identical or near-identical prompt suggestions
  - Performance degrades on test set (overfitting)

- First 3 experiments:
  1. Baseline: Run G on Dtest with empty prompt, measure accuracy
  2. Single iteration: Run SPT-p once, compare improvement over baseline
  3. Multi-iteration: Run SPT-p for 3-5 iterations, track accuracy progression and prompt evolution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Supervisory Prompt Training (SPT) vary across different types of hallucinations (e.g., non-factual information vs. nonsensical information)?
- Basis in paper: [inferred] The paper focuses on non-factual outputs but mentions that hallucinations can also include nonsensical information
- Why unresolved: The paper only evaluates SPT on benchmarks designed to measure non-factual outputs (TruthfulQA, GSM8K, MMLU, MedQA)
- What evidence would resolve it: Testing SPT on benchmarks specifically designed to measure nonsensical information and comparing the performance across different types of hallucinations

### Open Question 2
- Question: What is the impact of the number of training examples on the effectiveness of SPT?
- Basis in paper: [explicit] The paper mentions that the small ratio of training to testing data in MMLU may have contributed to the lack of significant improvements
- Why unresolved: The paper only tests SPT on a fixed number of training examples for each benchmark
- What evidence would resolve it: Conducting experiments with varying numbers of training examples for each benchmark and analyzing the relationship between training data size and SPT performance

### Open Question 3
- Question: How does the choice of corrector model affect the performance of SPT?
- Basis in paper: [explicit] The paper uses GPT-4 as the corrector for all experiments but mentions that different correctors could be explored in future work
- Why unresolved: The paper only uses one corrector model (GPT-4) and does not compare its performance with other models
- What evidence would resolve it: Testing SPT with different corrector models (e.g., GPT-3.5, Llama2) and comparing their performance in terms of accuracy and prompt quality

## Limitations

- Computational cost of multiple LLM calls per iteration not adequately characterized
- Dependence on having access to two separate LLM instances creates deployment constraints
- Evaluation focused primarily on mathematical reasoning tasks and hallucination detection, limited testing on other domains

## Confidence

- **High Confidence**: The core mechanism of using a corrector LLM to analyze generator mistakes and suggest prompt improvements is well-supported by the experimental results, particularly the GSM8K accuracy gains
- **Medium Confidence**: The impact score methodology for measuring sentence-level effectiveness is theoretically sound but lacks external validation across different task types and LLM architectures
- **Low Confidence**: The generalizability of SPT across diverse domains, the computational cost-benefit tradeoff at scale, and the long-term stability of iterative prompt refinement remain largely unexplored

## Next Checks

1. **Cost-Benefit Analysis at Scale**: Run SPT for 10+ iterations on a medium-sized dataset (e.g., 1000 examples) and measure both accuracy improvements and total computational costs, comparing against simpler prompt engineering approaches and traditional fine-tuning to establish when SPT becomes cost-prohibitive

2. **Cross-Model Generalization Test**: Take the final optimized prompts generated by SPT on GPT-4 and evaluate them on different LLM architectures (Claude, Llama, Mistral) to determine whether the improvements are model-specific or transfer across systems

3. **Domain Transfer Experiment**: Apply the same SPT framework to non-mathematical tasks like text summarization (CNN/DailyMail) or code generation (HumanEval) to test whether the dual-LLM feedback mechanism generalizes beyond mathematical reasoning and hallucination detection