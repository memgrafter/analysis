---
ver: rpa2
title: The Impact of Differential Feature Under-reporting on Algorithmic Fairness
arxiv_id: '2401.08788'
source_url: https://arxiv.org/abs/2401.08788
tags:
- under-reporting
- feature
- data
- group
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the impact of differential feature under-reporting
  on algorithmic fairness. The authors propose a statistical model where administrative
  data is more complete for individuals relying on public services, leading to biased
  predictions.
---

# The Impact of Differential Feature Under-reporting on Algorithmic Fairness

## Quick Facts
- arXiv ID: 2401.08788
- Source URL: https://arxiv.org/abs/2401.08788
- Authors: Nil-Jana Akpinar; Zachary C. Lipton; Alexandra Chouldechova
- Reference count: 40
- Primary result: Differential feature under-reporting leads to increased disparities in algorithmic fairness, with proposed methods successfully reducing unfairness

## Executive Summary
This paper investigates how differential feature under-reporting in administrative data affects algorithmic fairness. The authors demonstrate that when administrative data is more complete for individuals relying on public services, it creates systematic bias in predictions. Standard missing data methods fail to address this specific form of bias, necessitating new approaches. The paper proposes augmented loss estimation and optimal prediction imputation methods that show promise in reducing unfairness in real-world datasets.

## Method Summary
The authors develop a statistical model where administrative data completeness varies based on public service usage. They propose two novel methods: augmented loss estimation, which modifies the loss function to account for under-reporting patterns, and optimal prediction imputation, which strategically fills missing values to improve fairness. These methods are evaluated against standard missing data techniques on multiple real-world datasets, demonstrating their effectiveness in mitigating bias caused by differential under-reporting.

## Key Results
- Standard missing data methods fail to mitigate bias from differential feature under-reporting
- Under-reporting typically leads to increasing disparities in selection rates
- Proposed augmented loss estimation and optimal prediction imputation methods successfully reduce unfairness
- Experimental results validate the theoretical framework on real-world datasets

## Why This Works (Mechanism)
The differential under-reporting creates systematic bias because the missingness is not random but correlated with the outcome variable through public service usage. The proposed methods work by explicitly modeling this non-random missingness pattern and adjusting predictions accordingly, rather than treating missing values as random.

## Foundational Learning

1. Differential Feature Under-reporting
   - Why needed: Understanding how administrative data completeness varies across populations is crucial for identifying sources of algorithmic bias
   - Quick check: Can you identify scenarios where administrative data might be systematically more complete for certain groups?

2. Missing Data Mechanisms
   - Why needed: Distinguishing between Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR) is essential for selecting appropriate treatment methods
   - Quick check: How would you classify differential under-reporting in terms of these mechanisms?

3. Fairness Metrics
   - Why needed: Measuring disparate impact and other fairness metrics is necessary to quantify the effect of bias on different groups
   - Quick check: What fairness metrics would you use to evaluate algorithmic decisions across protected groups?

## Architecture Onboarding

Component Map: Administrative Data -> Feature Extraction -> Differential Under-reporting Detection -> Fairness Assessment -> Prediction Model -> Fairness-Aware Adjustment

Critical Path: The pipeline processes administrative data, identifies under-reporting patterns, assesses fairness impacts, and applies corrections through the proposed methods before final predictions.

Design Tradeoffs: The methods balance computational complexity with fairness improvements. Optimal prediction imputation offers better fairness but at higher computational cost compared to augmented loss estimation.

Failure Signatures: When standard missing data methods are applied without accounting for differential under-reporting, disparities in selection rates will increase, particularly for groups relying more on public services.

First Experiments:
1. Apply the proposed methods to a publicly available administrative dataset with known completeness disparities
2. Compare fairness metrics before and after applying augmented loss estimation
3. Benchmark computational runtime of optimal prediction imputation against standard imputation methods

## Open Questions the Paper Calls Out
None

## Limitations
- Assumption that administrative data is uniformly more complete for public service users may not hold across all contexts
- Theoretical framework based on specific patterns of differential under-reporting may not generalize to all scenarios
- Computational complexity of optimal prediction imputation may limit scalability to very large datasets

## Confidence
High confidence that differential feature under-reporting leads to increased disparities in selection rates.
Medium confidence that standard missing data methods fail to address this specific form of bias.
Medium confidence in the effectiveness of proposed augmented loss estimation and optimal prediction imputation methods.

## Next Checks
1. Test the proposed methods on additional real-world datasets from different domains (e.g., healthcare, education, employment) to assess generalizability.
2. Conduct a scalability analysis to determine the computational limits of optimal prediction imputation on large-scale datasets.
3. Perform ablation studies to isolate the individual contributions of augmented loss estimation and optimal prediction imputation to overall fairness improvements.