---
ver: rpa2
title: 'Generative Verifiers: Reward Modeling as Next-Token Prediction'
arxiv_id: '2408.15240'
source_url: https://arxiv.org/abs/2408.15240
tags:
- verification
- answer
- step
- verifiers
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes generative verifiers (GenRM) for LLM-based verification
  that use next-token prediction instead of discriminative classification. GenRM predicts
  solution correctness via token probabilities (e.g., "Yes"/"No") and can incorporate
  chain-of-thought reasoning with majority voting for improved accuracy.
---

# Generative Verifiers: Reward Modeling as Next-Token Prediction

## Quick Facts
- arXiv ID: 2408.15240
- Source URL: https://arxiv.org/abs/2408.15240
- Reference count: 40
- The paper proposes generative verifiers (GenRM) that use next-token prediction instead of discriminative classification, achieving strong performance on algorithmic and math tasks with chain-of-thought reasoning and majority voting.

## Executive Summary
This paper introduces Generative Verifiers (GenRM), a novel approach to LLM-based verification that leverages next-token prediction instead of traditional discriminative classification. By predicting solution correctness through token probabilities (e.g., "Yes"/"No"), GenRM can incorporate chain-of-thought reasoning with majority voting for improved accuracy. The unified training framework that combines generation and verification tasks further enhances performance. Experiments demonstrate that GenRM-CoT significantly outperforms discriminative reward models, DPO verifiers, LLM-as-a-Judge, and self-consistency on algorithmic and math tasks, particularly in Best-of-N settings.

## Method Summary
The method trains an LLM to verify solution correctness by predicting "Yes" or "No" tokens through next-token prediction, using cross-entropy loss. For CoT reasoning, the model generates a rationale before predicting correctness. Unified training combines verification with correct solution generation. At inference, majority voting across multiple CoT samples improves accuracy. The approach uses synthetic verification rationales generated by LLMs with reference guidance for training data.

## Key Results
- GenRM-CoT achieves 45.3% success rate on algorithmic tasks (vs 5% baseline)
- GSM8K performance improves from 73% to 93.4% with GenRM-CoT
- MATH performance increases from 28% to 44.6% with GenRM-CoT
- Best-of-N performance significantly outperforms discriminative RMs, DPO verifiers, and LLM-as-a-Judge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative verifiers can outperform discriminative verifiers by leveraging text generation capabilities for reasoning.
- Mechanism: GenRM uses next-token prediction to represent solution correctness as the probability of "Yes" or "No" tokens, enabling CoT reasoning and majority voting.
- Core assumption: The LLM's generation capabilities are sufficient to produce meaningful verification rationales that improve accuracy.
- Evidence anchors:
  - [abstract]: "GenRM-CoT outperforms discriminative RMs, DPO verifiers, and LLM-as-a-Judge, resulting in large performance gains"
  - [section]: "GenRM naturally supports CoT reasoning... it can be trained to reason explicitly by generating a verbalized rationale before predicting correctness"
  - [corpus]: FMR=0.5897 for related "Generative Reward Models" paper
- Break condition: If generated rationales are too noisy or contain errors that mislead the verification process.

### Mechanism 2
- Claim: Unifying solution generation with verification improves both tasks through positive transfer.
- Mechanism: Joint training on verification and correct solution generation (equation 5) allows the model to learn from both what constitutes a correct solution and how to generate one.
- Core assumption: The skills required for generating correct solutions overlap sufficiently with those needed for verification.
- Evidence anchors:
  - [section]: "This unified training can improve verifier and generation performance via positive transfer"
  - [section]: "Incorporating CoT verification data into the generator's training mix leads to better solution generation performance"
  - [corpus]: FMR=0.5532 for "Rewarding Creativity" paper on RL with generative reward models
- Break condition: If the generation data dominates training and causes the model to prioritize generation over accurate verification.

### Mechanism 3
- Claim: Scaling inference-time compute through majority voting improves verification accuracy.
- Mechanism: Sampling multiple CoT rationales and averaging "Yes" token probabilities (equation 7) reduces the impact of individual reasoning errors.
- Core assumption: Different CoT paths will have varying correctness probabilities, and averaging will improve reliability.
- Evidence anchors:
  - [section]: "Since individual verification rationales from CoT verifiers can have reasoning errors, majority voting can mitigate the impact"
  - [section]: "GenRM-CoT can leverage additional inference-time compute to improve its accuracy"
  - [corpus]: FMR=0.5155 for "GenPRM" paper on scaling test-time compute via generative reasoning
- Break condition: If the model consistently produces the same erroneous reasoning across multiple samples.

## Foundational Learning

- Concept: Next-token prediction and cross-entropy loss
  - Why needed here: Forms the basis of how GenRM is trained and how it produces scores
  - Quick check question: What is the difference between training a discriminative verifier and a generative verifier in terms of loss function?

- Concept: Chain-of-thought reasoning
  - Why needed here: Enables the model to explicitly reason about solution correctness rather than making direct judgments
  - Quick check question: How does CoT reasoning help detect subtle errors that direct verification might miss?

- Concept: Majority voting and ensemble methods
  - Why needed here: Allows the model to use inference-time compute to improve verification accuracy
  - Quick check question: Why might sampling multiple verification rationales and averaging their scores be more reliable than a single pass?

## Architecture Onboarding

- Component map:
  Problem statement + candidate solution -> LLM fine-tuned with next-token prediction -> "Yes"/"No" token probability as correctness score -> (Optional: CoT generation step) -> (Optional: Majority voting across multiple CoT samples)

- Critical path:
  1. Receive problem and solution
  2. Generate verification CoT (if using GenRM-CoT)
  3. Compute probability of "Yes" token
  4. Use score for Best-of-N selection

- Design tradeoffs:
  - Direct GenRM vs GenRM-CoT: Simpler but potentially less accurate vs more complex but better at catching errors
  - Unified training vs separate: Potential for positive transfer but risk of interference
  - Majority voting count: More samples = better accuracy but higher inference cost

- Failure signatures:
  - Low RM accuracy despite good Best-of-N performance: Model is ranking solutions well but not accurately classifying individual solutions
  - Degraded performance with too much generation data: Loss balance needs adjustment
  - Majority voting provides minimal improvement: Model may not be generating diverse enough rationales

- First 3 experiments:
  1. Train GenRM on algorithmic tasks with oracle verification data to establish baseline performance
  2. Compare GenRM vs discriminative RM on GSM8K with same model architecture
  3. Test impact of majority voting (K=1, 4, 16, 32) on GenRM-CoT performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content, several questions arise regarding cross-domain reasoning, temperature effects, scaling with reasoning complexity, optimal data ratios, and subjective task evaluation.

## Limitations
- Synthetic data quality concerns: Heavy reliance on LLM-generated verification rationales without detailed quality control mechanisms
- Verification accuracy vs ranking: Strong Best-of-N performance but relatively lower absolute verification accuracy
- Limited scaling evidence: Claims about scaling benefits beyond tested 9B parameters remain unverified

## Confidence
- High Confidence (8/10): Core mechanism of next-token prediction for verification is well-established and empirically validated
- Medium Confidence (6/10): CoT enhancement and majority voting improvements supported by evidence but rely on assumptions about rationale quality
- Low Confidence (4/10): Unified training benefits based on limited evidence and may vary with different task types

## Next Checks
1. Test GenRM-CoT on held-out datasets from different domains (e.g., code generation or scientific reasoning) to verify generality beyond algorithmic and math tasks.
2. Conduct human evaluation of synthetic verification rationales to assess quality, diversity, and identify systematic errors.
3. Systematically test model performance at 70B+ parameters to identify diminishing returns and validate scaling claims for frontier model sizes.