---
ver: rpa2
title: Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy
arxiv_id: '2410.17234'
source_url: https://arxiv.org/abs/2410.17234
tags:
- semantic
- entropy
- question
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the problem of reducing hallucinations in Large\
  \ Language Models (LLMs) by training them to abstain from answering questions beyond\
  \ their capabilities. While prior fine-tuning approaches rely on ground-truth labels\
  \ or are limited to short-form responses, this paper proposes using semantic entropy\u2014\
  an uncertainty measure derived from introspection into the model\u2014that does\
  \ not require external labels."
---

# Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy

## Quick Facts
- arXiv ID: 2410.17234
- Source URL: https://arxiv.org/abs/2410.17234
- Reference count: 38
- Primary result: Semantic entropy fine-tuning reduces hallucinations by up to 30.1% for long-form and 8.7% for short-form generations

## Executive Summary
This paper addresses the challenge of reducing hallucinations in Large Language Models by training them to abstain from answering questions beyond their capabilities. The key innovation is using semantic entropy—an uncertainty measure derived from introspection into the model—rather than relying on ground-truth labels or raw token entropy. By computing entropy over the semantic space of model generations using bidirectional entailment, the method is robust to lexical and syntactical variations. Experiments across four datasets and two answering settings show that semantic entropy-based fine-tuning matches or outperforms existing approaches, achieving significant reductions in hallucination rates while maintaining engagement.

## Method Summary
The method generates M high-temperature responses per question, clusters them into semantic equivalence classes using bidirectional entailment, and computes semantic entropy over these classes. Questions are partitioned at a threshold τ into high-entropy (abstain) and low-entropy (answer) sets. The model is fine-tuned with LoRA (rank 8) using modified labels—standard responses for low-entropy questions and "I don't know" for high-entropy questions. The accuracy-engagement distance (AED) metric evaluates performance by combining accuracy and engagement to quantify hallucination severity.

## Key Results
- Semantic entropy achieves up to 30.1% reduction in hallucination rates for long-form generations compared to prior methods
- For short-form generations, semantic entropy reduces hallucinations by up to 8.7% over existing approaches
- Models fine-tuned with semantic entropy match or outperform R-Tuning and R-Tuning-U baselines across all datasets
- AED scores for semantic entropy fine-tuning are significantly lower than the original model and other fine-tuning methods

## Why This Works (Mechanism)

### Mechanism 1
Semantic entropy measures uncertainty more robustly than raw token entropy by grouping semantically equivalent responses. The method clusters high-temperature model responses into semantic equivalence classes using bidirectional entailment, then computes entropy over these classes rather than individual token sequences. This approach assumes that two responses with different token sequences but same semantic meaning indicate low uncertainty.

### Mechanism 2
Fine-tuning on semantic entropy thresholded data improves abstention accuracy while maintaining engagement. The model learns to associate high semantic entropy with questions requiring abstention, effectively learning uncertainty boundaries without ground-truth labels. This mechanism assumes semantic entropy correlates with model uncertainty in a way that generalizes across domains.

### Mechanism 3
Accuracy-Engagement Distance (AED) provides better evaluation than accuracy alone by penalizing low engagement. AED computes normalized Euclidean distance from ideal point (max accuracy, max engagement) using incorrect/correct counts. This assumes a model that answers fewer questions correctly is worse than one answering more questions correctly, even at same accuracy rate.

## Foundational Learning

- Concept: Entropy and information theory
  - Why needed here: Semantic entropy computation relies on understanding entropy over probability distributions
  - Quick check question: What's the difference between Shannon entropy and conditional entropy in the context of language models?

- Concept: Semantic equivalence and entailment
  - Why needed here: Semantic clustering depends on correctly identifying when responses mean the same thing
  - Quick check question: How does bidirectional entailment differ from unidirectional entailment in practice?

- Concept: Supervised fine-tuning with modified labels
  - Why needed here: The method requires understanding how to train models with abstention labels
  - Quick check question: What happens to cross-entropy loss when the ground-truth label is "I don't know"?

## Architecture Onboarding

- Component map: High-temperature response generator → Semantic clustering engine → Entropy calculator → Threshold splitter → LoRA fine-tuner
- Critical path: 1) Generate M high-temperature responses per question 2) Cluster responses into semantic equivalence classes using entailment model 3) Compute semantic entropy for each question 4) Partition dataset at threshold τ 5) Fine-tune with modified labels 6) Evaluate using AED metric
- Design tradeoffs: Entailment model choice (DeBERTa vs Llama), number of high-temperature samples M, threshold τ selection, LoRA rank
- Failure signatures: Semantic entropy values cluster near threshold, AED improves on training set but not validation set, standard responses get misclassified as uncertain
- First 3 experiments: 1) Verify semantic entropy computation on synthetic data with known equivalence classes 2) Test entailment model accuracy on paraphrased question-answer pairs 3) Run ablation study: compare raw token entropy vs semantic entropy on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of semantic entropy-based fine-tuning compare when using other types of semantic similarity models beyond DeBERTa-NLI and Llama-3-70B-Instruct? The paper only experiments with two semantic similarity models, and other models could potentially yield different results.

### Open Question 2
What is the impact of using different LoRA rank values (r) on the effectiveness of semantic entropy-based fine-tuning? The paper uses a relatively low LoRA rank (r=8) due to resource constraints, and higher or lower ranks could potentially improve or degrade performance.

### Open Question 3
How does the semantic entropy-based fine-tuning method scale to longer-form generations, such as paragraphs or biographies? The paper focuses on short-form and sentence-length responses, but longer-form generations introduce more complexity and potential for hallucinations.

### Open Question 4
Can semantic entropy be used as a fine-tuning method to calibrate language models beyond hallucination reduction? The paper suggests this possibility but does not explore this application.

### Open Question 5
How does the performance of semantic entropy-based fine-tuning generalize across different model architectures and sizes? The paper uses Llama-3-8B-Instruct as the base model, and other architectures and model sizes may yield different results.

## Limitations

- Semantic clustering reliability depends critically on the accuracy of semantic equivalence detection, which may fail on paraphrased responses
- Computational overhead is significant due to generating multiple high-temperature responses and entailment model inference for each question
- Threshold sensitivity may lead to unstable performance across different dataset splits and threshold choices

## Confidence

- High Confidence: Semantic entropy outperforms raw token entropy in measuring uncertainty
- Medium Confidence: Semantic entropy enables effective fine-tuning without ground-truth labels
- Low Confidence: AED metric comprehensively captures the trade-off between accuracy and engagement across all scenarios

## Next Checks

1. Evaluate the semantic clustering accuracy on a held-out test set of manually annotated paraphrased responses to quantify how often semantically equivalent responses are correctly grouped
2. Apply the fine-tuned models to datasets from completely different domains to assess whether semantic entropy patterns transfer effectively
3. Perform k-fold cross-validation on the training data to measure how much the optimal threshold τ varies across different data splits