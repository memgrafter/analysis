---
ver: rpa2
title: 'Approximate Nearest Neighbour Search on Dynamic Datasets: An Investigation'
arxiv_id: '2404.19284'
source_url: https://arxiv.org/abs/2404.19284
tags:
- search
- methods
- datasets
- index
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work empirically evaluates 5 popular approximate nearest neighbour
  (ANN) methods for dynamic datasets that change over time. Unlike traditional ANN
  evaluations on static datasets, this study considers the computational costs of
  updating index structures, as well as the rate and size of updates.
---

# Approximate Nearest Neighbour Search on Dynamic Datasets: An Investigation

## Quick Facts
- arXiv ID: 2404.19284
- Source URL: https://arxiv.org/abs/2404.19284
- Authors: Ben Harwood; Amir Dezfouli; Iadine Chades; Conrad Sanderson
- Reference count: 24
- Primary result: k-d trees performs worse than baseline exhaustive search on dynamic datasets; HNSWG achieves consistent speedup across wide recall range for online data collection

## Executive Summary
This paper investigates the performance of five popular approximate nearest neighbor (ANN) methods on dynamic datasets that change over time through addition and update events. Unlike traditional ANN evaluations that focus on static datasets, this study considers the computational costs of updating index structures and how different update rates and sizes affect performance. Through experiments on two dynamic datasets modeled after SIFT1M and DEEP1B, the authors demonstrate that standard parameter tuning approaches for static search problems do not translate well to dynamic scenarios, and that methods like k-d trees can perform worse than simple exhaustive search when faced with frequent updates.

## Method Summary
The study evaluates five ANN methods (ScaNN, IVFPQ, k-d trees, ANNOY, and HNSWG) on dynamic datasets created from SIFT1M and DEEP1B. The authors conduct experiments with both standard parameter tuning (based on static search recommendations) and expanded parameter exploration specifically designed for dynamic scenarios. They measure performance using speedup over brute-force search across various recall rates, while controlling for event rates, search rates, and batch sizes. Two types of dynamic datasets are created: online data collection (ODC) using addition events, and online feature learning (OFL) using update events to model converging samples.

## Key Results
- k-d trees performs worse than baseline exhaustive search method on dynamic datasets
- HNSWG achieves consistent speedup over baseline across wide range of recall rates for online data collection
- ScaNN is faster than baseline for recall rates below 75% on online feature learning datasets
- Expanded parameter exploration significantly improves performance for all methods compared to static-tuned parameters
- Increasing update event batch size results in faster search times due to improved index quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Index structures that do not account for dynamic updates can degrade over time, leading to slower searches and lower recall.
- Mechanism: When the underlying dataset changes, pre-computed index structures become less accurate. k-d trees require full reconstruction for each change, making them inefficient for dynamic datasets, while HNSWG and ScaNN allow incremental updates but still incur overhead.
- Core assumption: Frequency and magnitude of dataset changes are significant enough to affect index quality.
- Evidence anchors:
  - [abstract] "Traditional evaluation approaches do not consider the computational costs of updating the index structure, as well as the rate and size of index updates."
  - [section IV.B] "On both datasets, the results first show that the best practice approaches for parameter tuning on static search problems do not directly translate to dynamic search problems, resulting in the performance of many methods being well below the baseline."

### Mechanism 2
- Claim: Expanded parameter exploration tuned for dynamic datasets significantly improves ANN method performance compared to static-tuned parameters.
- Mechanism: Static-tuned parameters are optimized for one-time index construction scenarios. Dynamic scenarios require parameters that balance index construction time, update time, and search speed.
- Core assumption: There exists a parameter configuration that balances construction, update, and search costs effectively for dynamic datasets.
- Evidence anchors:
  - [abstract] "The results indicate that the often used k-d trees method is not suitable on dynamic datasets as it is slower than a straightforward baseline exhaustive search method."
  - [section IV.B] "Expanding the parameter exploration in terms of range and resolution, in order to better accommodate the needs of dynamic search problems, resulted in notably improved performance for all methods."

### Mechanism 3
- Claim: Batching update events reduces per-event computational cost and can improve search performance on dynamic datasets.
- Mechanism: Processing update events in larger batches allows for more global optimizations to the index structure rather than local changes, leading to more stable and efficient indexes.
- Core assumption: Larger batches of updates provide more global context for index optimization.
- Evidence anchors:
  - [section IV.C] "The results also show that increasing the update event batch size results in faster search times, which is consistent with the stance that using more data for index updates can increase the quality of the index."

## Foundational Learning

- Concept: Approximate Nearest Neighbor (ANN) Search
  - Why needed here: ANN search is the core technique being evaluated for dynamic datasets. Understanding trade-offs is essential for interpreting results.
  - Quick check question: What is the primary trade-off in ANN search, and how does it differ from exact nearest neighbor search?

- Concept: Index Structures for ANN
  - Why needed here: Different ANN methods use different index structures (quantization, tree, graph), each with strengths and weaknesses for dynamic datasets.
  - Quick check question: What are the main types of index structures used in ANN search, and how do they handle dynamic updates?

- Concept: Dynamic Datasets and Update Events
  - Why needed here: The paper focuses on how dataset changes affect ANN performance. Understanding update impact is key to interpreting experimental results.
  - Quick check question: What are the two types of update events discussed in the paper, and how do they differ in their impact on the index structure?

## Architecture Onboarding

- Component map:
  - Data Sources (SIFT1M/OFL, DEEP1B/ODC) -> ANN Methods (ScaNN, IVFPQ, k-d trees, ANNOY, HNSWG) -> Evaluation Metrics (Speedup, Recall, Event processing time) -> Experiment Control (Event rate, Search rate, Batch sizes) -> Parameter Tuning (Static-tuned, Dynamic-tuned)

- Critical path:
  1. Initialize dataset with initial samples
  2. Process update/addition events (batched or individual)
  3. Update index structure for each ANN method
  4. Perform search queries
  5. Measure performance metrics (speedup, recall, event processing time)
  6. Repeat steps 2-5 for different parameter configurations

- Design tradeoffs:
  - Index Construction Time vs. Update Time: Methods with fast construction may have slow updates, while methods with slow construction may have faster updates
  - Search Speed vs. Recall: Methods optimized for speed may sacrifice recall, while methods optimized for recall may be slower
  - Memory Usage vs. Performance: Some methods require more memory for better performance

- Failure signatures:
  - Degraded Recall: Index structure becomes less accurate over time, leading to lower recall
  - Slow Search: Frequent updates or large dataset changes cause search times to increase
  - High Event Processing Time: Index updates consume too much computational resources, slowing down the overall system

- First 3 experiments:
  1. Compare performance of all ANN methods using static-tuned parameters on ODC dataset. Measure speedup and recall.
  2. Repeat experiment 1 using dynamic-tuned parameters. Compare results to identify impact of parameter tuning.
  3. Vary event rate on OFL dataset and measure effect on performance of HNSWG. Identify threshold where method becomes less effective.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms by which update events degrade the index quality more severely than addition events, and can these mechanisms be quantified or mitigated?
- Basis in paper: [inferred] from observation that update events affect all samples in an index, while addition events have more local impact
- Why unresolved: The paper conjectures that update events are more disruptive but does not provide detailed analysis of underlying mechanisms or potential mitigation strategies
- What evidence would resolve it: Comparative studies isolating effects of update vs. addition events on index quality metrics, and experiments testing index reconstruction or selective updating strategies

### Open Question 2
- Question: How do different batching strategies (event and search batch sizes) interact with varying event rates to influence long-term stability and performance of ANN methods on dynamic datasets?
- Basis in paper: [explicit] from experiments showing larger update event batch sizes result in faster search times, while higher event rates tend to slow search
- Why unresolved: While paper demonstrates impact of batching and rates separately, it does not explore their combined effects or identify optimal strategies
- What evidence would resolve it: Systematic experiments varying both batch sizes and event rates simultaneously, measuring index quality and search performance over extended periods

### Open Question 3
- Question: To what extent can temporal dependence between consecutive samples be exploited to improve search efficiency in dynamic datasets, and which ANN methods benefit most from such exploitation?
- Basis in paper: [explicit] authors suggest exploiting mutual information between consecutive search batches may provide large performance gains
- Why unresolved: Paper identifies this as potential avenue for future research but does not investigate how temporal dependence could be modeled or integrated into ANN search algorithms
- What evidence would resolve it: Implementation and evaluation of ANN methods that incorporate temporal context, comparing their performance against standard methods on datasets with strong temporal coherence

## Limitations

- Methodology for modeling update events in OFL dataset (converging samples) is not fully specified, making it difficult to assess how representative these dynamics are of real-world scenarios
- Parameter ranges for expanded exploration are not provided, limiting reproducibility and understanding of which specific configurations drove performance improvements
- Study uses only two datasets (SIFT1M and DEEP1B) with specific characteristics, which may not generalize to all dynamic dataset types

## Confidence

- High confidence: k-d trees performs worse than baseline exhaustive search on dynamic datasets, supported by direct experimental evidence
- Medium confidence: Best practice static parameters do not translate to dynamic scenarios, based on limited dataset diversity
- Medium confidence: Expanded parameter exploration significantly improves performance, as specific parameter ranges that succeeded are not disclosed

## Next Checks

1. Replicate the experiments with additional diverse dynamic datasets (e.g., time-series, streaming sensor data) to test generalizability
2. Conduct ablation studies to isolate the impact of different update event types (addition vs. update) on each ANN method's performance
3. Implement and test the identified successful parameter configurations from expanded exploration on a new, independent dynamic dataset to verify robustness