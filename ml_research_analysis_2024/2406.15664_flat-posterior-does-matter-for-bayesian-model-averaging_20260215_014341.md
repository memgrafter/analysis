---
ver: rpa2
title: Flat Posterior Does Matter For Bayesian Model Averaging
arxiv_id: '2406.15664'
source_url: https://arxiv.org/abs/2406.15664
tags:
- sa-bma
- learning
- flatness
- bayesian
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key issue in Bayesian neural networks
  (BNNs): most existing methods fail to find flat minima in the loss landscape, which
  limits their generalization performance and the effectiveness of Bayesian Model
  Averaging (BMA). The authors show that without considering posterior flatness, BMA
  can even degrade performance.'
---

# Flat Posterior Does Matter For Bayesian Model Averaging

## Quick Facts
- arXiv ID: 2406.15664
- Source URL: https://arxiv.org/abs/2406.15664
- Reference count: 40
- Most existing BNN methods fail to find flat posterior distributions, limiting BMA effectiveness

## Executive Summary
This paper identifies a fundamental issue in Bayesian neural networks: most existing methods fail to find flat minima in the loss landscape, which limits their generalization performance and the effectiveness of Bayesian Model Averaging (BMA). The authors show that without considering posterior flatness, BMA can even degrade performance. To address this, they propose Sharpness-Aware Bayesian Model Averaging (SA-BMA), a novel training objective that explicitly encourages flat posteriors in a principled Bayesian manner. SA-BMA computes an adversarial posterior within a KL divergence ball and updates the variational parameters using gradients from this perturbed posterior. The method is shown to be a generalization of existing sharpness-aware optimizers like SAM and Fisher SAM.

## Method Summary
SA-BMA extends sharpness-aware optimization to the Bayesian setting by computing an adversarial posterior within a KL divergence ball around the current posterior. The method perturbs the variational parameters and optimizes them using gradients from this perturbed posterior, explicitly encouraging flat regions in the posterior distribution. The authors also introduce a Bayesian transfer learning scheme that selectively optimizes specific components (e.g., normalization and classifier layers) to ensure compatibility with large-scale pre-trained models while maintaining computational efficiency. This approach is evaluated on few-shot classification and distribution shift settings, demonstrating improved generalization performance compared to existing BNN methods.

## Key Results
- SA-BMA successfully captures flat posteriors, achieving the lowest Hessian eigenvalue values compared to all other baselines
- Bayesian Model Averaging with SA-BMA improves calibration (lower ECE) and robustness on distribution shift datasets
- The method demonstrates strong performance on few-shot classification tasks with 10-shot per class settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BNNs trained without explicit flatness-seeking often fail to find flat posterior distributions, which limits generalization performance and makes BMA less effective.
- Mechanism: Existing BNN training methods (VI, SWAG, etc.) optimize the loss directly without considering the geometry of the posterior distribution in parameter space. This leads to sharp minima where the posterior concentrates around narrow regions, reducing the benefits of Bayesian Model Averaging.
- Core assumption: Flat minima in the loss landscape correlate with better generalization performance in neural networks.
- Evidence anchors:
  - [abstract] "most approximate Bayesian inference methods fail to yield a flat posterior"
  - [section 3.1] "BNNs trained with SGD often exhibit higher lambda1 compared to DNN trained with SGD"
  - [corpus] "Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking"

### Mechanism 2
- Claim: The sharpness of individual models in BMA affects the sharpness of the averaged model, potentially degrading performance.
- Mechanism: When averaging models sampled from a sharp posterior, the eigenvalues of the averaged Hessian can be bounded below by a combination of individual model Hessians. Sharp models pull the averaged model toward sharper regions, reducing generalization benefits.
- Core assumption: The eigenvalues of the averaged model Hessian can be bounded by a function of individual model Hessian eigenvalues (Weyl's inequality).
- Evidence anchors:
  - [section 3.2] "Theorem 1 implies that the flatness of averaged model reflects the flatness of model samples"
  - [abstract] "we show that (1) most approximate Bayesian inference methods fail to yield a flat posterior and (2) BMA predictions, without considering posterior flatness, are less effective at improving generalization"
  - [corpus] "Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling"

### Mechanism 3
- Claim: SA-BMA finds flatter posteriors by explicitly optimizing for flatness in parameter space using KL divergence between perturbed posteriors.
- Mechanism: SA-BMA computes an adversarial posterior within a KL divergence ball around the current posterior, then updates using gradients from this perturbed posterior. This directly encourages flat regions in the posterior distribution.
- Core assumption: Optimizing the geometry of the posterior distribution in parameter space (rather than just the loss) can find flatter minima that generalize better.
- Evidence anchors:
  - [section 4.1] "SA-BMA calculates the divergence between posteriors in the parameter space, aligning with the nature of BNNs"
  - [abstract] "we propose Sharpness-Aware Bayesian Model Averaging (SA-BMA), a novel training objective that explicitly encourages flat posteriors"
  - [section 5.3] "SA-BMA has the lowest value compared to all other baselines, which can be interpreted as our model being the flattest"

## Foundational Learning

- Concept: Bayesian Neural Networks and Posterior Distribution
  - Why needed here: The paper is about improving BNNs, so understanding how they represent uncertainty through posterior distributions is fundamental.
  - Quick check question: What is the key difference between how BNNs and DNNs represent model parameters?

- Concept: Sharpness and Flatness in Loss Landscapes
  - Why needed here: The core contribution is about finding flat posteriors, so understanding what sharpness means and how it's measured is essential.
  - Quick check question: How is flatness typically measured in neural network loss landscapes?

- Concept: Bayesian Model Averaging (BMA)
  - Why needed here: The paper discusses how BMA performance is affected by posterior flatness, so understanding the ensemble mechanism is important.
  - Quick check question: What is the mathematical formulation of Bayesian Model Averaging in BNNs?

## Architecture Onboarding

- Component map:
  - Pre-trained Model (DNN or BNN) -> BNN Framework (SWAG or VI) -> SA-BMA Optimizer -> Bayesian Transfer Learning Scheme -> Loss Function

- Critical path:
  1. Load pre-trained model (DNN or BNN)
  2. Convert to BNN using SWAG or VI
  3. Train using SA-BMA optimizer with selective layer training
  4. Sample from posterior for BMA predictions

- Design tradeoffs:
  - Full vs. selective training: Training only normalization and classifier layers reduces computation but may miss important flat regions in other layers
  - Perturbation size γ: Too small may not find flat regions, too large may destabilize training
  - Low-rank approximation: Reduces FIM computation cost but may miss some curvature information

- Failure signatures:
  - Poor calibration (high ECE) despite good accuracy suggests flat posterior not properly captured
  - NLL degradation with more averaged models indicates sharpness problem in BMA
  - Computational instability during training suggests γ parameter issues

- First 3 experiments:
  1. Compare λ1 values of SA-BMA vs. standard BNN training on CIFAR-10 to verify flatness improvement
  2. Test BMA performance with SA-BMA vs. baseline when varying number of averaged models
  3. Evaluate robustness on CIFAR-10C/CIFAR-100C to verify distribution shift performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the FIM be computed for the entire model parameter space instead of just the trainable parameters to achieve even better performance improvements?
- Basis in paper: [explicit] The paper mentions that calculating the FIM in weight space rather than output space makes it intractable to compute the FIM for the entire model parameter, and obtaining the FIM for the entire model weight space could lead to much more powerful performance improvements.
- Why unresolved: Computing the FIM for the entire model parameter space is computationally expensive and may not be feasible for large-scale models.
- What evidence would resolve it: Experiments demonstrating improved performance by computing the FIM for the entire model parameter space, or theoretical proofs showing the benefits of this approach.

### Open Question 2
- Question: How does the performance of SA-BMA compare to other methods when training from scratch without pre-trained models?
- Basis in paper: [inferred] The paper assumes the existence of pre-trained models, and it mentions that in situations where pre-trained weights are not available, performance improvements may be somewhat limited.
- Why unresolved: The paper does not provide experiments or analysis on training from scratch without pre-trained models.
- What evidence would resolve it: Experiments comparing the performance of SA-BMA with other methods when training from scratch on various datasets.

### Open Question 3
- Question: What is the impact of the choice of low-rank component K on the performance of SA-BMA?
- Basis in paper: [explicit] The paper mentions that the covariance is approximated by combining diagonal covariance and a low-rank matrix L with low-rank component K, but it does not provide a detailed analysis of the impact of K on performance.
- Why unresolved: The paper does not conduct experiments or provide theoretical analysis on the effect of different values of K on the performance of SA-BMA.
- What evidence would resolve it: Experiments demonstrating the performance of SA-BMA with different values of K, or theoretical analysis showing the relationship between K and performance.

## Limitations
- The theoretical analysis relies on assumptions about the relationship between posterior flatness and generalization that may not hold universally across all network architectures
- The computational complexity of SA-BMA, particularly the FIM approximation and perturbation computation, may limit its applicability to very large-scale models
- The selective training scheme assumes the availability of batch normalization layers, which may not be present in all pre-trained models

## Confidence
- High Confidence: The empirical results showing improved calibration (ECE) and robustness on distribution shift datasets are well-supported and consistent across multiple experiments
- Medium Confidence: The theoretical connection between posterior flatness and BMA performance is plausible but relies on assumptions about the behavior of averaged Hessian eigenvalues that may not always hold in practice
- Medium Confidence: The claim that most existing BNN methods fail to find flat posteriors is supported by the analysis but could benefit from more extensive comparison across different BNN training methodologies

## Next Checks
1. Test SA-BMA on larger-scale pre-trained models (e.g., ViT, CLIP) to evaluate scalability and verify if the selective training scheme remains effective without batch normalization layers
2. Conduct ablation studies on the perturbation magnitude γ and low-rank approximation rank to understand their impact on both performance and computational efficiency
3. Evaluate SA-BMA's performance on out-of-distribution datasets that differ more substantially from CIFAR (e.g., natural language or medical imaging tasks) to assess generalizability beyond image classification