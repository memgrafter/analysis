---
ver: rpa2
title: Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable
  Prototypes
arxiv_id: '2404.02830'
source_url: https://arxiv.org/abs/2404.02830
tags:
- prototypes
- class
- prototype
- fracture
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of interpretable vertebral fracture
  grading, which is crucial for diagnosing osteoporosis. The proposed method, ProtoVerse,
  is an interpretable-by-design deep learning model that learns human-understandable
  prototypes for different fracture grades.
---

# Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes

## Quick Facts
- arXiv ID: 2404.02830
- Source URL: https://arxiv.org/abs/2404.02830
- Reference count: 16
- Primary result: ProtoVerse model with diversity-promoting loss and median-weighted cross-entropy outperforms baseline methods in interpretable vertebral fracture grading on VerSe'19 dataset.

## Executive Summary
This work introduces ProtoVerse, an interpretable-by-design deep learning model for vertebral fracture grading that learns human-understandable prototypes for different fracture grades. The model addresses key challenges in medical image interpretation by introducing a novel diversity-promoting loss to prevent prototype repetition and median-weighted cross-entropy to handle class imbalance in the small, imbalanced VerSe'19 dataset. Clinical validation by expert radiologists confirms that the learnt prototypes are both visually interpretable and clinically relevant, demonstrating the potential of interpretable-by-design models for enhancing trust in medical diagnosis.

## Method Summary
The ProtoVerse model is an interpretable-by-design deep learning architecture that learns human-understandable prototypes for vertebral fracture grading. It uses a feature extractor backbone (VGG11), prototype layer, and classification layer with similarity-based scoring. The model employs a diversity-promoting loss to prevent prototype repetition within classes and median-weighted cross-entropy to handle class imbalance. Prototypes are visualized by retrieving the closest training patches, enabling human interpretability. The model is trained on the VerSe'19 dataset of CT scans and evaluated on classification accuracy and clinical interpretability by expert radiologists.

## Key Results
- ProtoVerse outperforms existing prototype-based methods and post-hoc interpretability techniques in classification accuracy and visual interpretability
- Diversity-promoting loss successfully prevents prototype repetitions in small, imbalanced datasets
- Expert radiologist validation confirms clinical relevance and visual similarity of learnt prototypes to actual fracture patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototype diversity loss prevents feature collapse by explicitly minimizing cosine similarity among prototypes within the same class.
- Mechanism: The loss term $\lambda_{div}L_{div} = \frac{2}{cm(m-1)} \sum_{j=1}^{c} \sum_{k,l=1,m,k\neq l} (p_j^k \top \times p_j^l)^2$ encourages the model to learn distinct, semantically varied parts for each fracture grade rather than repeating similar features.
- Core assumption: Small, imbalanced datasets lack sufficient variation for the model to learn diverse prototypes without explicit diversity constraints.
- Evidence anchors:
  - [abstract] "Specifically, we introduce a novel diversity-promoting loss to mitigate prototype repetitions in small datasets with intricate semantics."
  - [section 4.1.1] "Diversity loss prevents the collapsing of multiple prototypes of the same class into identical ones."
  - [corpus] Weak evidence - related works do not explicitly mention diversity-promoting losses in prototype-based medical image models.
- Break condition: If the dataset becomes large and balanced, the diversity loss may be redundant as natural variation will suffice.

### Mechanism 2
- Claim: Median-weighted cross-entropy (MWCE) mitigates class imbalance by adjusting loss weights based on median sample counts.
- Mechanism: Class weights are computed as $w_j = \frac{\text{median}(\{d_l\}_{l=1}^c)}{\sum_{l=1,c,l\neq j} d_l}$, giving more influence to underrepresented fracture classes during training.
- Core assumption: Standard CE loss overweights the dominant healthy class, leading to poor fracture class performance despite high overall accuracy.
- Evidence anchors:
  - [abstract] "Further, we mitigate huge class imbalances with the help of differential weighting in the classification loss."
  - [section 4.1.2] "We adopt differential weighting in the CE loss... median-weighted cross-entropy (MWCE) in the classification loss."
  - [corpus] Weak evidence - no explicit mention of median-weighted CE in related prototype-based medical works.
- Break condition: If class distribution changes significantly, the median-based weighting may no longer be optimal.

### Mechanism 3
- Claim: Visual prototypes retrieved from training patches provide human-interpretable explanations by mapping latent prototype vectors back to input image regions.
- Mechanism: For each prototype $p_j^k$, the closest training patch $z$ minimizing $\|z - p_j^k\|^2$ is stored and displayed during inference to show which image part the prototype represents.
- Core assumption: Radiologists can validate prototype relevance by matching retrieved patches to actual fracture patterns in test samples.
- Evidence anchors:
  - [abstract] "Expert radiologists validated the visual interpretability of our results, showing clinical applicability."
  - [section 5.5] "We prepared a study including all test samples... asked the radiologists to rate based on... Prototype Relevance? Visual Similarity?"
  - [corpus] Moderate evidence - related interpretable medical models use prototype retrieval, but clinical validation details are sparse.
- Break condition: If prototype patches are too abstract or semantically disconnected from fracture regions, radiologists will not find them interpretable.

## Foundational Learning

- Concept: ProtoPNet architecture and its interpretable-by-design nature
  - Why needed here: Understanding how prototypes are learned and visualized is critical for extending or debugging the model.
  - Quick check question: What is the difference between a prototype vector and its corresponding prototype patch?
- Concept: Cross-entropy loss and class imbalance handling
  - Why needed here: Median-weighted CE is central to the model's performance on imbalanced fracture grading data.
  - Quick check question: How does median weighting differ from inverse-frequency weighting in addressing imbalance?
- Concept: Medical image semantic interpretation
  - Why needed here: Prototypes must capture clinically relevant fracture features, not just visual similarity.
  - Quick check question: Why might a prototype focusing on posterior vertebral elements be more relevant for high-grade fractures?

## Architecture Onboarding

- Component map:
  - Input → CNN backbone → prototype layer → similarity heatmap → max-pooled scores → weighted sum → classification
- Critical path: Input → CNN backbone → prototype layer → similarity heatmap → max-pooled scores → weighted sum → classification
- Design tradeoffs:
  - Backbone choice (VGG11 best in ablation) vs. deeper networks
  - Number of prototypes per class (3 optimal) vs. model capacity
  - Diversity loss coefficient (0.3 optimal) vs. cluster loss strength
- Failure signatures:
  - Repetitive prototypes → check diversity loss and dataset size
  - Poor fracture class accuracy → verify MWCE weighting and class balance
  - Low clinical relevance → inspect prototype patches and radiologist feedback
- First 3 experiments:
  1. Train with and without diversity loss to confirm its effect on prototype repetition.
  2. Compare MWCE vs. INS/ISNS weighting strategies on class-averaged accuracy.
  3. Vary the number of prototypes per class to find the sweet spot before overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed diversity loss function impact the visual interpretability of prototypes compared to other potential diversity-promoting approaches?
- Basis in paper: [explicit] The paper introduces a novel diversity-promoting loss to mitigate prototype repetitions in small, imbalanced datasets and shows its effectiveness compared to the baseline ProtoPNet method.
- Why unresolved: While the paper demonstrates the effectiveness of the diversity loss, it does not compare it against other potential diversity-promoting approaches or provide a detailed ablation study on the impact of different diversity loss formulations on visual interpretability.
- What evidence would resolve it: A comprehensive comparison of the proposed diversity loss against alternative diversity-promoting methods, along with an ablation study on the impact of different diversity loss formulations on the visual interpretability of prototypes.

### Open Question 2
- Question: How does the performance of the proposed method vary with the number of prototypes per class, and what is the optimal number of prototypes for achieving the best balance between interpretability and classification accuracy?
- Basis in paper: [explicit] The paper mentions an ablation study on the number of prototypes per class but does not provide detailed results or insights into the impact of varying the number of prototypes on the trade-off between interpretability and classification accuracy.
- Why unresolved: The paper does not provide a comprehensive analysis of the performance variation with the number of prototypes per class, leaving the optimal number of prototypes for achieving the best balance between interpretability and classification accuracy unclear.
- What evidence would resolve it: A detailed ablation study on the impact of varying the number of prototypes per class on both interpretability and classification accuracy, including insights into the optimal number of prototypes for achieving the best balance between the two.

### Open Question 3
- Question: How does the proposed method handle inter-rater variability in fracture grading, and what strategies can be employed to improve the learning of prototypes in the presence of such variability?
- Basis in paper: [explicit] The paper mentions that the proposed method may struggle to learn prototypes for lower fracture grades due to inter-rater variability and suggests exploring explicit modeling of inter-rater variability in future work.
- Why unresolved: While the paper acknowledges the challenge of inter-rater variability, it does not provide a detailed analysis of how the proposed method handles this variability or propose concrete strategies for improving prototype learning in its presence.
- What evidence would resolve it: A comprehensive analysis of the proposed method's performance in the presence of inter-rater variability, along with proposed strategies for improving prototype learning by explicitly modeling inter-rater variability during training.

## Limitations

- Clinical validation relies on subjective ratings rather than quantitative metrics for prototype interpretability
- Effectiveness of diversity loss lacks quantitative ablation studies demonstrating the extent of improvement
- Median-weighted cross-entropy's effectiveness shown through overall accuracy but lacks detailed class-level improvements

## Confidence

- **High Confidence**: ProtoVerse's superior classification performance compared to baseline methods, as evidenced by multiple metrics (accuracy, F1) and controlled ablation studies.
- **Medium Confidence**: The effectiveness of the diversity-promoting loss and MWCE weighting, based on experimental results but lacking detailed ablations.
- **Medium Confidence**: Clinical validation results, given the subjective nature of radiologist ratings and limited sample size.

## Next Checks

1. Conduct a quantitative ablation study measuring prototype similarity metrics (e.g., intra-class cosine similarity) with and without diversity loss to confirm its effect on preventing feature collapse.

2. Perform a detailed class-wise breakdown of performance with MWCE vs. standard CE to identify which fracture grades benefit most from median weighting.

3. Implement a blinded study with more radiologists rating a larger set of prototypes against ground-truth fracture patterns to strengthen clinical validation claims.