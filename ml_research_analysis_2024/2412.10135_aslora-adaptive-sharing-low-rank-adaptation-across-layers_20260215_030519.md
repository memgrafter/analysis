---
ver: rpa2
title: 'ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers'
arxiv_id: '2412.10135'
source_url: https://arxiv.org/abs/2412.10135
tags:
- sharing
- layers
- across
- aslora
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ASLoRA, a parameter-efficient fine-tuning method
  that combines global sharing with partial adaptive sharing. The key idea is to share
  the low-rank matrix A across all layers while adaptively merging matrix B during
  training to reduce parameters and improve performance.
---

# ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers

## Quick Facts
- arXiv ID: 2412.10135
- Source URL: https://arxiv.org/abs/2412.10135
- Authors: Junyan Hu; Xue Xiao; Mengqi Zhang; Yao Chen; Zhaochun Ren; Zhumin Chen; Pengjie Ren
- Reference count: 21
- Primary result: Achieves GLUE performance comparable to LoRA while using only 24% of parameters

## Executive Summary
ASLoRA introduces a parameter-efficient fine-tuning method that combines global sharing of low-rank matrix A across all layers with adaptive merging of matrix B during training. The approach reduces the number of parameters while maintaining or improving performance compared to existing methods like LoRA and adapter tuning. By adaptively merging similar B matrices based on L2 norm similarity, ASLoRA achieves significant parameter reduction without sacrificing model quality on both GLUE benchmark tasks and instruction tuning datasets.

## Method Summary
ASLoRA implements a three-stage training process for parameter-efficient fine-tuning of large language models. The method shares matrix A globally across all layers while initially keeping B matrices independent, then adaptively merges B matrices based on similarity during training, and finally optimizes the merged architecture. This approach combines the benefits of global parameter sharing with layer-specific adaptation, achieving better performance with fewer parameters than traditional LoRA methods. The adaptive merging is controlled by a scheduler that determines when and how B matrices should be combined based on their pairwise similarity.

## Key Results
- On GLUE benchmark, ASLoRA uses only 24% of LoRA's parameters while achieving comparable or better performance
- For instruction tuning, ASLoRA uses 26% of the parameters required by other efficient fine-tuning methods while outperforming them on multiple datasets
- The method demonstrates effective parameter reduction through adaptive merging without significant performance degradation

## Why This Works (Mechanism)

### Mechanism 1
Adaptive sharing of matrix B across layers reduces redundancy while maintaining layer-specific expressiveness. The model starts with full sharing of A and independent B matrices per layer, then merges B matrices based on similarity during training. Core assumption: B matrices in adjacent layers tend to be similar, allowing meaningful merging without significant performance loss. Break condition: If B matrices across layers are highly dissimilar, merging would destroy unique layer information and degrade performance.

### Mechanism 2
Sharing matrix A globally provides consistent initialization for similarity calculations of B matrices. Since A is shared across all layers, each layer's B matrix starts from the same initial value and propagates through identical A, eliminating initialization bias in similarity measurements. Core assumption: Uniform A initialization ensures that observed differences in B matrices reflect true layer-specific needs rather than initialization artifacts. Break condition: If A initialization significantly affects B evolution patterns, the similarity-based merging could be misleading.

### Mechanism 3
The three-stage training process (shared training → adaptive merging → final optimization) ensures both efficient learning and stable convergence. Initial shared training learns global patterns with full B independence, adaptive merging reduces redundancy based on learned similarities, and final optimization stabilizes the merged model. Core assumption: Learning meaningful B patterns requires initial independence before merging can be effective. Break condition: If merging occurs too early or too aggressively, the model may not have learned sufficient layer-specific patterns before reduction.

## Foundational Learning

- **Concept: Low-rank matrix decomposition (BA factorization)**
  - Why needed here: ASLoRA builds directly on LoRA's approach of approximating weight updates with low-rank matrices
  - Quick check question: What are the dimensions of matrices B and A in LoRA when the weight matrix has shape (d×d) and rank r?

- **Concept: Parameter-efficient fine-tuning strategies**
  - Why needed here: ASLoRA is compared against multiple PEFT methods like adapters, prefix tuning, and LoRA variants
  - Quick check question: How does parameter sharing across layers affect both memory efficiency and model expressiveness?

- **Concept: Similarity metrics for parameter comparison**
  - Why needed here: Adaptive merging relies on L2 norm similarity calculations between B matrices
  - Quick check question: Why might L2 norm be preferred over other distance metrics for comparing weight matrices?

## Architecture Onboarding

- **Component map:** Global A matrix (shared across all layers) -> Layer-specific B matrices (initially independent, then adaptively merged) -> Similarity calculation module (L2 norm based) -> Merging scheduler (controls when and how B matrices merge) -> Final optimization phase (stabilizes merged architecture)

- **Critical path:** Training → Similarity Calculation → Merging Decision → Parameter Update → Evaluation

- **Design tradeoffs:**
  - More merges → fewer parameters but risk of losing layer-specific information
  - Fewer merges → more parameters but better preservation of layer uniqueness
  - Merge timing → early merging may be premature, late merging may miss optimization opportunities

- **Failure signatures:**
  - Performance degradation after merging → merging criteria too aggressive
  - No parameter reduction → merging never triggered or criteria too strict
  - Training instability → merge frequency or timing inappropriate

- **First 3 experiments:**
  1. Implement basic LoRA with shared A and independent B matrices to verify foundation
  2. Add similarity calculation and verify L2 norm produces reasonable similarity scores across layers
  3. Implement single merge operation and measure impact on parameters and performance

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal merge interval strategy for different types of NLP tasks? The paper currently uses a fixed merge interval of 10 steps but acknowledges this may not be optimal for all tasks. Empirical comparison of different merge interval strategies (fixed, adaptive, task-specific) across various NLP benchmarks showing performance differences would resolve this question.

### Open Question 2
How does ASLoRA's performance scale with model size and depth? The paper only tests on RoBERTa-base (12 layers) and LLaMA-2-7B, but mentions that "as the number of model layers increases, the amount of parameters that can be reduced also increases." Performance and parameter efficiency results from applying ASLoRA to larger models like LLaMA-3 70B or deeper architectures like 24-layer transformers would resolve this question.

### Open Question 3
What is the impact of merging order on ASLoRA's performance? The paper states "We merge the two layers with the lowest similarity" but doesn't explore alternative merging strategies. Comparative results showing performance differences between different merging orders (e.g., random order, most dissimilar first, hierarchical merging) would resolve this question.

## Limitations
- The adaptive merging mechanism lacks rigorous validation of when similarity-based merging is beneficial versus destructive
- Absence of ablation studies examining the impact of merge timing, merge frequency, and rank selection on both parameter efficiency and model performance
- Weak citation support (0 citations for related papers) suggests limited foundation in existing literature

## Confidence

- **High Confidence**: The basic mechanism of sharing matrix A globally while keeping B matrices independent initially is straightforward and well-supported. The experimental setup and evaluation methodology appear sound.

- **Medium Confidence**: The adaptive merging strategy shows promise but lacks rigorous validation. While the paper demonstrates improved parameter efficiency and competitive performance, the generalizability of the merging criteria across different scenarios remains uncertain.

- **Low Confidence**: Claims about why the method works at a theoretical level (e.g., "eliminating initialization bias" and "preserving layer-specific expressiveness") are primarily intuitive rather than empirically validated. The paper does not provide systematic analysis of when and why merging decisions are beneficial or harmful.

## Next Checks

1. **Merge Stability Analysis**: Conduct experiments varying the merge timing (starting steps) and merge frequency across multiple random seeds to determine the sensitivity of performance to these hyperparameters. Measure both parameter reduction and performance stability.

2. **Similarity Metric Validation**: Systematically compare different similarity metrics (L2 norm, cosine similarity, correlation) for merging decisions and evaluate their impact on final model performance and parameter efficiency. Include qualitative analysis of what the similarity scores actually represent in the learned parameter space.

3. **Cross-Architecture Generalization**: Test ASLoRA on architectures beyond RoBERTa-base and LLaMA-2-7B (e.g., BERT, GPT variants, smaller models) to evaluate whether the adaptive sharing benefits transfer across different model families and whether the same hyperparameter settings remain effective.