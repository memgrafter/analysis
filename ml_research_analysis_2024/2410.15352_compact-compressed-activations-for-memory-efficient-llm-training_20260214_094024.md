---
ver: rpa2
title: 'CompAct: Compressed Activations for Memory-Efficient LLM Training'
arxiv_id: '2410.15352'
source_url: https://arxiv.org/abs/2410.15352
tags:
- memory
- compact
- training
- projection
- galore
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CompAct reduces peak GPU memory by 25-30% for pretraining and\
  \ 50% for fine-tuning large language models. It compresses linear layer activations\
  \ using random projections, lowering the memory cost of the computation graph\u2014\
  the largest component of training memory."
---

# CompAct: Compressed Activations for Memory-Efficient LLM Training

## Quick Facts
- **arXiv ID:** 2410.15350
- **Source URL:** https://arxiv.org/abs/2410.15350
- **Reference count:** 18
- **Primary result:** CompAct reduces peak GPU memory by 25-30% for pretraining and 50% for fine-tuning large language models using random projections.

## Executive Summary
CompAct introduces a memory-efficient training technique for large language models by compressing activations in linear layers using random projections. Unlike prior methods that only reduce optimizer overheads, CompAct targets the compute graph itself—the largest memory component during training. The method stores low-rank compressed activations during the forward pass and uses them for gradient computation in the backward pass, achieving significant memory savings without substantial performance degradation. For LLaMA-350M, CompAct reduces peak memory from ~40 GB to ~34.7 GB, and for RoBERTa-base, it cuts peak memory in half.

## Method Summary
CompAct compresses linear layer activations using Gaussian random projection matrices sampled on-the-fly during the forward pass. Instead of storing full activation tensors, it stores compressed versions in a lower-dimensional subspace. During the backward pass, gradients are computed from these compressed activations and then projected back to full rank for weight updates. The method maintains optimizer states (first and second moment estimates) in the compressed subspace, updating them in reduced space before projecting back for weight updates. CompAct is applied to all attention and MLP blocks except output projection in attention, using ranks of n/2, n/4, or n/8 relative to the original dimension.

## Key Results
- CompAct achieves 25-30% memory reduction for pretraining and 50% for fine-tuning across various model sizes.
- For LLaMA-350M, peak memory decreases from ~40 GB to ~34.7 GB (13% reduction).
- On RoBERTa-base, CompAct cuts peak memory in half compared to baseline training.
- The method outperforms GaLore, achieving superior memory savings without SVD overhead.

## Why This Works (Mechanism)

### Mechanism 1
CompAct reduces peak GPU memory by compressing the largest component of allocated memory during training—the model's compute graph—rather than just optimizer states or parameters. By storing low-rank, compressed activations during the forward pass instead of full activation tensors, CompAct reduces the size of gradients and optimizer states as well, since gradients computed from compressed activations are also low-rank. Core assumption: The compressed activations contain sufficient information to compute gradients that, when projected back to full rank, yield weight updates comparable to training with uncompressed activations.

### Mechanism 2
Random projections maintain approximation quality while avoiding SVD overhead. CompAct uses Gaussian random matrices sampled on-the-fly during the forward pass, eliminating the need to store or compute optimal projection matrices. This reduces memory overhead while still preserving the top singular values of the activation matrices with high probability. Core assumption: Random Gaussian matrices preserve the norm and essential structure of the original activations sufficiently for effective gradient computation and model convergence.

### Mechanism 3
CompAct's memory savings scale with batch size because activations grow with batch size while optimizer states do not. CompAct compresses activations (which depend on batch size) rather than just optimizer states (which are independent of batch size). Therefore, the memory reduction becomes more significant as batch size increases. Core assumption: The activation memory is the dominant memory component that scales with batch size in LLM training.

## Foundational Learning

- **Concept:** Low-rank matrix approximation and random projection theory
  - **Why needed here:** CompAct relies on projecting high-dimensional activation matrices into lower-dimensional subspaces while preserving essential information for gradient computation.
  - **Quick check question:** What property of Gaussian random matrices ensures they preserve norms of vectors they project?

- **Concept:** Backpropagation and gradient computation
  - **Why needed here:** Understanding how gradients flow through compressed activations and how weight updates are computed from these gradients is critical for implementing CompAct correctly.
  - **Quick check question:** How does compressing activations during the forward pass affect the computation of gradients with respect to weights?

- **Concept:** Optimizer state management (Adam optimizer)
  - **Why needed here:** CompAct maintains optimizer states (first and second moment estimates) in the compressed subspace, requiring understanding of how Adam operates on reduced-rank gradients.
  - **Quick check question:** What modifications are needed to Adam's update rule when working with compressed gradients?

## Architecture Onboarding

- **Component map:** Input tensor → Linear layer → Random projection → Compressed activation stored; Compressed activation + output gradient → Compressed gradient computed → Projected back to full rank → Full gradient used for weight update; Optimizer maintains moments in compressed space → Updates performed in compressed space → Projection back to full space for weight update

- **Critical path:** Forward pass (compression) → Backward pass (gradient computation from compressed activations) → Optimizer update (in compressed space) → Weight update (projection back to full space)

- **Design tradeoffs:**
  - Rank selection: Lower rank provides greater memory savings but may degrade model performance; higher rank preserves performance but offers less memory reduction
  - Projection type: Gaussian random matrices are computationally efficient but may introduce some approximation error; structured projections might offer better approximation at higher computational cost
  - Update frequency: More frequent projection updates may improve approximation quality but increase computational overhead

- **Failure signatures:**
  - Training instability or divergence: Likely indicates projection matrix is not preserving sufficient information
  - Degraded model performance: May indicate rank is too low or projection matrix quality is insufficient
  - Minimal memory savings: Could indicate incorrect implementation of compression or activation checkpointing interfering with compression

- **First 3 experiments:**
  1. Implement CompAct with rank = n/2 on a small LLaMA model (60M parameters) and verify memory reduction vs baseline
  2. Compare perplexity and training stability across different ranks (n/2, n/4, n/8) on the same small model
  3. Measure throughput impact of CompAct vs baseline and vs GaLore with and without activation checkpointing on the small model

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical justification for using random projections over learned projections in CompAct? The paper references Theorem 2 from Meier and Nakatsukasa (2024), which shows that random sketching preserves top singular values with high probability. While the theorem provides probabilistic guarantees, it doesn't directly address the practical trade-offs between random and learned projections in the specific context of LLM training with CompAct's architecture.

### Open Question 2
How does CompAct's performance scale with model size beyond LLaMA-65B? The authors explicitly state "We expect CompAct's savings to scale even higher for larger models" and show increasing memory savings with model size from 60M to 65B parameters. Without empirical validation on larger models, the scalability claim remains theoretical.

### Open Question 3
What is the impact of applying CompAct to non-linear activation layers (beyond linear layers)? The authors mention that Yang et al. (2024) focus on "saving activation memory generated by nonlinear functions and normalization layers, whereas our work focuses on the activations generated by linear layers." The paper explicitly states this as a limitation and potential area for improvement.

## Limitations

- The paper doesn't provide comprehensive throughput measurements, leaving uncertainty about computational overhead relative to memory savings.
- Evaluation focuses primarily on LLaMA-based models, with limited evidence for effectiveness on other transformer architectures.
- The optimal rank selection depends on the intrinsic dimensionality of activations for specific models, which isn't characterized.

## Confidence

**High Confidence:**
- CompAct reduces peak GPU memory by compressing the compute graph rather than just optimizer states
- Random projections maintain approximation quality while avoiding SVD overhead
- Memory savings scale with batch size because activations grow with batch size

**Medium Confidence:**
- CompAct is composable with existing memory-saving techniques like activation checkpointing
- The method generalizes to all transformer architectures beyond LLaMA-based models
- Performance degradation is minimal across all tested configurations

**Low Confidence:**
- CompAct will maintain performance with sequence lengths beyond 256 tokens
- The method is equally effective for encoder-only models like BERT
- Memory savings will remain proportional as models scale to trillion parameters

## Next Checks

1. **Throughput benchmarking:** Measure training throughput (samples/second) for CompAct vs baseline and vs GaLore across different model sizes and batch sizes to quantify computational overhead introduced by random projections.

2. **Architecture generalization test:** Implement CompAct on BERT-base and GPT-2-medium architectures, training on their respective standard benchmarks (GLUE for BERT, LAMBADA for GPT-2) to assess architectural generalization.

3. **Long-sequence scaling analysis:** Evaluate CompAct with sequence lengths of 512, 1024, and 2048 tokens on LLaMA-350M to determine if the random projection approximation degrades with longer sequences and identify the maximum effective sequence length for each rank setting.