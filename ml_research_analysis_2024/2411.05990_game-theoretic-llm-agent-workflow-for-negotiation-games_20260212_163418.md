---
ver: rpa2
title: 'Game-theoretic LLM: Agent Workflow for Negotiation Games'
arxiv_id: '2411.05990'
source_url: https://arxiv.org/abs/2411.05990
tags:
- game
- workflow
- negotiation
- agents
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the strategic decision-making of large language\
  \ models (LLMs) in game-theoretic settings and identifies that LLMs frequently fail\
  \ to act rationally, especially in complex games or under negotiation. To address\
  \ this, the authors propose game-theoretic workflows that guide LLMs\u2019 reasoning\
  \ using established methods like backward induction, best response analysis, and\
  \ Bayesian belief updating."
---

# Game-theoretic LLM: Agent Workflow for Negotiation Games

## Quick Facts
- arXiv ID: 2411.05990
- Source URL: https://arxiv.org/abs/2411.05990
- Reference count: 40
- Key outcome: Game-theoretic workflows significantly improve LLM rationality in negotiation games by guiding strategic reasoning through backward induction, best response analysis, and Bayesian belief updating

## Executive Summary
This paper addresses the fundamental problem that large language models frequently fail to act rationally in strategic, game-theoretic settings, particularly during negotiations. The authors propose structured workflows that guide LLMs through established game-theoretic reasoning methods like backward induction and best response analysis. These workflows significantly improve LLM performance across both complete-information games (increasing agreement rates, Pareto optimality, and envy-freeness) and incomplete-information scenarios (improving negotiation outcomes through Bayesian belief updating). The study reveals a meta-game-theoretic challenge: whether to adopt such workflows itself becomes a strategic decision that depends on the specific LLM and opponent characteristics.

## Method Summary
The authors evaluate LLM strategic decision-making using classic complete-information games (Prisoner's Dilemma, Stag Hunt, Battle of the Sexes, etc.) and the Deal or No Deal incomplete-information game. They implement game-theoretic workflows based on backward induction for sequential games, best response analysis for simultaneous games, and Bayesian belief updating for incomplete-information scenarios. The workflows are tested on state-of-the-art LLMs (Claude-3.5 Sonnet, Claude-3 Opus, GPT-4o, o1) with and without workflow guidance. Performance metrics include agreement rate, Pareto optimality, envy-freeness, Nash equilibrium achievement, and total reward. The study also explores the strategic decision of whether to adopt workflows through a meta-game-theoretic analysis.

## Key Results
- LLMs without workflows frequently fail to achieve Nash equilibrium or Pareto optimal outcomes in strategic games
- Game-theoretic workflows significantly improve LLM performance, increasing agreement rates, Pareto optimality, and envy-freeness in negotiations
- The rationality of workflow adoption itself becomes a strategic decision, with some LLM models benefiting more than others depending on the game context
- In incomplete-information games, Bayesian belief updating workflows help LLMs handle uncertainty about opponent valuations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail in strategic reasoning due to incomplete backward induction in sequential games
- Mechanism: Without workflow, LLMs skip recursive reasoning steps and miss optimal end-state payoffs
- Core assumption: Backward induction requires explicit traversal of game tree from terminal nodes to root
- Evidence anchors:
  - [section] "We adopt the traditional game-theoretic method: backward induction... The backward induction starts from terminal nodes."
  - [section] "The workflow directs the LLM to employ backward induction by systematically traversing the game decision tree from the terminal nodes back to the initial node."
- Break condition: If the LLM's reasoning process cannot model the full game tree or the workflow does not enforce backward traversal, rationality fails

### Mechanism 2
- Claim: Negotiation without workflow shifts LLMs away from Nash Equilibrium toward non-equilibrium Pareto optimal strategies
- Mechanism: During multi-round negotiation, LLMs are influenced by persuasive language and trust assumptions, leading to decisions that deviate from rational game-theoretic predictions
- Core assumption: LLMs treat negotiation dialogue as credible input that can override strategic reasoning
- Evidence anchors:
  - [abstract] "LLMs frequently fail to act rationally... especially in complex games or under negotiation."
  - [section] "In such cases, negotiation may lead agents away from the rational strategy predicted by game theory."
- Break condition: If the LLM can distinguish negotiation dialogue from strategic payoff reasoning, or if trust assumptions are calibrated, the deviation may be reduced

### Mechanism 3
- Claim: Workflow improves LLM rationality by integrating structured game-theoretic reasoning into decision-making
- Mechanism: The workflow enforces best-response analysis and Bayesian belief updating, constraining LLM outputs to follow established game-theoretic principles
- Core assumption: Explicit algorithmic guidance can override default LLM reasoning and enforce rational strategies
- Evidence anchors:
  - [abstract] "workflows significantly improve LLM performance, increasing agreement rates, Pareto optimality, and envy-freeness in negotiations."
  - [section] "These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices."
- Break condition: If the LLM ignores workflow constraints or if the workflow is too rigid for the specific game structure, performance may not improve

## Foundational Learning

- Concept: Nash Equilibrium
  - Why needed here: Core evaluation metric for rationality; workflows aim to guide LLMs toward Nash Equilibrium
  - Quick check question: What condition must hold for a strategy profile to be a Nash Equilibrium?

- Concept: Backward Induction
  - Why needed here: Required for solving sequential games; workflow uses this method explicitly
  - Quick check question: In what order does backward induction solve a game tree?

- Concept: Bayesian Belief Updating
  - Why needed here: Used in incomplete-information games to refine beliefs about opponent valuations during negotiation
  - Quick check question: How does Bayesian updating change a belief distribution after observing opponent behavior?

## Architecture Onboarding

- Component map:
  - Game specification → LLM agent (with or without workflow) → Strategy computation → Negotiation phase → Final action selection
  - Workflow modules: Best-response analysis, backward induction, Bayesian update, fairness constraints

- Critical path:
  1. Parse game rules and payoff matrix
  2. Apply workflow (if enabled) to compute optimal strategy
  3. Execute strategy, possibly engaging in negotiation
  4. Update beliefs and re-optimize if needed

- Design tradeoffs:
  - Workflow vs. autonomy: Workflows improve rationality but may reduce flexibility
  - Trust calibration: Too much trust in negotiation leads to exploitation; too little blocks Pareto improvements
  - Computational cost: Workflows add overhead but enable more robust decisions

- Failure signatures:
  - Deviation from Nash Equilibrium without workflow
  - Exploitation by non-workflow opponents
  - Inability to handle payoff matrix perturbations

- First 3 experiments:
  1. Run Prisoner's Dilemma with and without workflow; compare action distributions
  2. Vary payoff matrix slightly (invariant perturbation); observe LLM consistency
  3. Test negotiation robustness: workflow vs. non-workflow opponent in Deal-or-No-Deal scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the adoption of game-theoretic workflows become the dominant strategy for LLM agents in incomplete-information games?
- Basis in paper: [explicit] Section 5.6.1 discusses payoff matrices showing workflow adoption is dominant strategy for some models but not others
- Why unresolved: The rationality of workflow adoption depends on specific LLM characteristics and opponent behavior, requiring further empirical analysis across different game types and agent capabilities
- What evidence would resolve it: Systematic experiments comparing workflow vs non-workflow performance across multiple LLM models and game scenarios, identifying the boundary conditions where workflow adoption becomes rational

### Open Question 2
- Question: How can LLMs develop more robust strategies that maintain rationality under negotiation while being resistant to exploitation?
- Basis in paper: [explicit] Section 6.5 shows engineered prompts temporarily mitigate negative negotiation effects, but influence diminishes with more rounds
- Why unresolved: Current LLMs show vulnerability to persuasion during negotiation despite being able to compute rational strategies when isolated, suggesting need for deeper integration of strategic reasoning with communication
- What evidence would resolve it: Development of LLM architectures or training methods that maintain computed Nash equilibria during negotiation while still achieving fair outcomes

### Open Question 3
- Question: What mechanisms enable LLMs to detect and adapt to indistinguishable valuation sets in incomplete-information games?
- Basis in paper: [explicit] Section 5.5.2 shows Opus achieves high performance despite imprecise valuation estimation due to indistinguishable sets
- Why unresolved: While indistinguishable valuations explain some successful negotiations, understanding how LLMs can explicitly identify and leverage these sets remains unclear
- What evidence would resolve it: Algorithms or workflows that help LLMs recognize when different valuation profiles lead to same optimal allocations, and use this knowledge to simplify belief updating during negotiation

### Open Question 4
- Question: How does the order of negotiation messages affect LLM decision-making in multi-round games?
- Basis in paper: [explicit] Section 6.6 shows order of negotiation initiation does not significantly affect outcomes in Battle of the Sexes
- Why unresolved: While initial message order may not matter, the specific sequence of negotiation messages could still influence LLM rationality, requiring deeper analysis of how message order interacts with LLM reasoning
- What evidence would resolve it: Experiments systematically varying message order while controlling for other factors, measuring changes in final decisions and rationality metrics across different game types

## Limitations

- Lack of detailed implementation specifications for game-theoretic workflows, particularly Bayesian belief updating algorithm
- Evaluation focused on a small set of classic games that may not capture full complexity of real-world strategic interactions
- Performance variations across different LLM models not fully explored across model families
- Limited analysis of how message order and negotiation dynamics affect LLM rationality

## Confidence

- **High**: The core observation that LLMs frequently fail to act rationally in strategic games without workflow guidance is well-supported by multiple game types and metrics
- **Medium**: The effectiveness of game-theoretic workflows in improving LLM performance is demonstrated but relies on specific implementations that are not fully disclosed
- **Medium**: The identification of negotiation as a source of rationality breakdown is compelling but requires further validation with more diverse negotiation scenarios

## Next Checks

1. Implement the game-theoretic workflows using publicly available game specifications and test with additional LLM models to verify reproducibility of the performance improvements
2. Conduct ablation studies to determine which specific components of the workflow (backward induction, best response, Bayesian updating) contribute most to performance gains
3. Design experiments with more complex and realistic strategic scenarios to test the scalability and robustness of the workflow approach beyond the classic game set