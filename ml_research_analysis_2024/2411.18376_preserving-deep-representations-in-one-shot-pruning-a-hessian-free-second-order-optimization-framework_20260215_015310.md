---
ver: rpa2
title: 'Preserving Deep Representations In One-Shot Pruning: A Hessian-Free Second-Order
  Optimization Framework'
arxiv_id: '2411.18376'
source_url: https://arxiv.org/abs/2411.18376
tags:
- pruning
- layer
- sparsity
- snows
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SNOWS, a one-shot post-training pruning framework
  that optimizes a more global reconstruction objective compared to existing layer-wise
  methods. The key innovation is using Hessian-free optimization to efficiently compute
  exact Newton descent steps without explicitly forming the full Hessian matrix, enabling
  scalable pruning of large vision models.
---

# Preserving Deep Representations In One-Shot Pruning: A Hessian-Free Second-Order Optimization Framework

## Quick Facts
- arXiv ID: 2411.18376
- Source URL: https://arxiv.org/abs/2411.18376
- Authors: Ryan Lucas; Rahul Mazumder
- Reference count: 40
- Primary result: SNOWS achieves state-of-the-art pruning results, including ResNet50 to 1:4 sparsity with only 1.5% accuracy drop on CIFAR-100

## Executive Summary
This paper introduces SNOWS, a one-shot post-training pruning framework that optimizes a global reconstruction objective using Hessian-free second-order optimization. The key innovation is extending layer-wise reconstruction to account for nonlinear activations deep in the network (K-step reconstruction), which better preserves learned representations during pruning. By leveraging Hessian-free optimization, SNOWS efficiently computes exact Newton descent steps without forming the full Hessian matrix, enabling scalable pruning of large vision models. The method significantly outperforms existing layer-wise approaches across various benchmarks and architectures.

## Method Summary
SNOWS is a one-shot post-training pruning framework that optimizes a K-step reconstruction objective to preserve deep network representations. It uses Hessian-free optimization with conjugate gradient to efficiently compute Newton descent steps without explicitly forming the full Hessian matrix. The method exploits sparsity patterns to reduce computational complexity and works with existing mask selection methods. SNOWS is applied layer-wise in a cascading fashion, with each layer's pruning accounting for subsequent nonlinear activations up to K steps deep.

## Key Results
- ResNet50 pruned to 1:4 sparsity on CIFAR-100 with only 1.5% accuracy drop (vs 7.6% for prior methods)
- ViT-L/16 pruned to 2:4 sparsity achieving 81.01% accuracy
- SNOWS consistently outperforms layer-wise OBS, SparseGPT, and MP across multiple architectures and datasets
- The method shows particular effectiveness at preserving attention patterns in vision transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SNOWS optimizes a more global reconstruction objective that accounts for nonlinear activations deep in the network, leading to better preservation of learned representations compared to layer-wise least squares reconstruction methods.
- Mechanism: By extending the reconstruction objective to include nonlinear activations up to K steps deep in the network, SNOWS creates a better proxy for the network loss function. This accounts for how pruning at one layer affects subsequent layer representations.
- Core assumption: The K-step reconstruction loss provides a more accurate approximation of the network's true loss function than layer-wise linear reconstruction alone.
- Evidence anchors:
  - [abstract] "This objective accounts for nonlinear activations deep in the network to obtain a better proxy for the network loss."
  - [section 3] "While more tractable than global approaches, the layer-wise formulation in Eqn (2) focuses solely on preserving the linear activations at a particular layer without considering the impact on subsequent layers' representations."
- Break condition: If K is set too large relative to available computational resources, the optimization becomes intractable and may not converge properly.

### Mechanism 2
- Claim: Hessian-free optimization enables scalable computation of exact Newton descent steps without explicitly forming or storing the full Hessian matrix.
- Mechanism: SNOWS uses finite differences to compute Hessian-vector products (H·δ) instead of the full Hessian, allowing efficient second-order optimization even for large networks.
- Core assumption: The Hessian-vector product can be computed efficiently enough to make Hessian-free optimization practical for large-scale networks.
- Evidence anchors:
  - [abstract] "A key innovation of our framework is the use of Hessian-free optimization to compute exact Newton descent steps without needing to compute or store the full Hessian matrix."
  - [section 4] "Hessian-free optimization is a second-order optimization method that avoids explicit computation of the Hessian matrix by exploiting the fact that while the Hessian matrix H is expensive to compute, the so-called Hessian product H · δ can be computed as..."
- Break condition: If the network architecture includes operations that are not twice-differentiable, the Hessian-vector product computation would fail.

### Mechanism 3
- Claim: SNOWS exploits sparsity in the Hessian to reduce computational complexity and memory requirements.
- Mechanism: The sparsity pattern of the Hessian matches the sparsity pattern of the mask Z, allowing SNOWS to work only with the active weights rather than the full parameter space.
- Core assumption: The mask-induced sparsity structure in the Hessian can be effectively exploited to reduce the dimensionality of the optimization problem.
- Evidence anchors:
  - [section 4] "The matrix HZ ∈ Rm×m is the Hessian restricted to the active weights, where m represents the number of weights with Zi ̸= 0. Correspondingly, the gradient ∇L(cW ℓ Z) ∈ Rm includes only the components associated with these active weights."
  - [section 4] "We exploit this sparsity in HZ to reduce the dimensionality of the system required to minimize Eqn (7)."
- Break condition: If the sparsity pattern becomes too dense (e.g., near the beginning of pruning), the computational savings diminish significantly.

## Foundational Learning

- Concept: Second-order optimization methods
  - Why needed here: SNOWS uses second-order information to more accurately approximate the loss landscape and make better pruning decisions than first-order methods.
  - Quick check question: What is the key advantage of second-order methods over first-order methods like SGD in optimization problems?

- Concept: Conjugate gradient method
  - Why needed here: SNOWS uses a customized conjugate gradient method to solve the reduced linear system efficiently without forming the full Hessian.
  - Quick check question: How does the conjugate gradient method solve linear systems iteratively without requiring matrix inversion?

- Concept: K-step reconstruction loss
  - Why needed here: This is the core innovation that allows SNOWS to account for deeper network representations when pruning, rather than just local layer-wise reconstruction.
  - Quick check question: What is the difference between K=0 and K=1 in the SNOWS reconstruction loss, and why does this matter for pruning quality?

## Architecture Onboarding

- Component map: K-step reconstruction computation → Hessian-vector product calculation → conjugate gradient solve → weight update loop
- Critical path: The critical path is the K-step reconstruction computation → Hessian-vector product calculation → conjugate gradient solve → weight update loop. This must be efficient for each layer to make the overall pruning process scalable.
- Design tradeoffs: The main tradeoff is between K (how many layers deep to consider in reconstruction) and computational cost. Larger K gives better pruning quality but requires more computation per layer.
- Failure signatures: If SNOWS fails to converge, it typically shows as (1) the conjugate gradient solver not converging within the iteration limit, (2) numerical instability in the Hessian-vector product computation, or (3) the reconstruction loss not decreasing despite iterations.
- First 3 experiments:
  1. Implement and validate the K-step reconstruction loss computation for a single layer with K=1 on a small network.
  2. Test the Hessian-vector product computation using finite differences and verify it matches analytical gradients.
  3. Run the full SNOWS algorithm on ResNet20 with K=3 and compare accuracy to layer-wise OBS baseline.

## Open Questions the Paper Calls Out

- The paper does not explicitly call out open questions, but based on the limitations section and discussion, several questions emerge:
  - How does SNOWS scale to extremely large models (100B+ parameters)?
  - What is the optimal relationship between K and network depth?
  - Can SNOWS be effectively adapted for structured pruning patterns beyond N:M sparsity?

## Limitations

- SNOWS is designed for N:M and unstructured sparsity patterns, with structured pruning remaining an open direction
- The framework assumes twice-differentiable operations, which may limit applicability to certain network architectures
- Computational complexity increases with larger K values, creating a tradeoff between pruning quality and efficiency

## Confidence

- **High confidence** in the core technical innovation: Hessian-free optimization for pruning is well-established in the literature and the implementation details appear sound.
- **Medium confidence** in the empirical claims: The results are comprehensive and show consistent improvements across multiple architectures, but some comparisons could be strengthened with additional baseline methods.
- **Medium confidence** in the mechanism claims: The connection between K-step reconstruction and better preservation of representations is plausible and supported by experiments, but the exact relationship between K values and pruning quality across different architectures could be more thoroughly analyzed.

## Next Checks

1. **Scalability verification**: Test SNOWS on a model larger than ViT-L/16 (e.g., ViT-Huge) to verify that the Hessian-free approach maintains its computational advantages as model size increases.

2. **K-value sensitivity**: Conduct a more systematic ablation study of K values across all architectures to better understand the relationship between K and pruning quality, particularly for the transition between CNN and ViT behavior.

3. **Generalization test**: Apply SNOWS to a non-vision domain (e.g., NLP or speech recognition) to verify that the K-step reconstruction and Hessian-free optimization approach generalizes beyond the vision tasks demonstrated.