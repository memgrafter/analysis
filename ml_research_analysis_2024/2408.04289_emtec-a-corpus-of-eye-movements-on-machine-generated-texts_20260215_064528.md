---
ver: rpa2
title: 'EMTeC: A Corpus of Eye Movements on Machine-Generated Texts'
arxiv_id: '2408.04289'
source_url: https://arxiv.org/abs/2408.04289
tags:
- reading
- https
- text
- language
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EMTeC is a naturalistic eye-tracking corpus of 107 native English
  speakers reading machine-generated texts produced by three large language models
  using five decoding strategies across six text types. The corpus includes raw gaze
  data (2000 Hz), fixation sequences, and word-level reading measures at all preprocessing
  stages, plus uncorrected and vertically drift-corrected versions.
---

# EMTeC: A Corpus of Eye Movements on Machine-Generated Texts

## Quick Facts
- arXiv ID: 2408.04289
- Source URL: https://arxiv.org/abs/2408.04289
- Reference count: 40
- Key outcome: Eye-tracking corpus of 107 native English speakers reading machine-generated texts across 14 conditions (3 LLMs × 5 decoding strategies) with raw gaze data, reading measures, and model internals.

## Executive Summary
EMTeC is a naturalistic eye-tracking corpus containing data from 107 native English speakers reading machine-generated texts produced by three large language models using five decoding strategies across six text types. The corpus includes raw gaze data at 2000 Hz sampling rate, fixation sequences, word-level reading measures at multiple preprocessing stages, and both uncorrected and manually drift-corrected versions. It also provides comprehensive model internals (transition scores, attention scores, hidden states) and linguistic annotations including surprisal estimates, POS/dependency tags, and readability metrics. The resource enables psycholinguistic analysis of machine-generated text processing, evaluation of decoding strategies, development of drift-correction algorithms, and cognitive interpretability studies of language models.

## Method Summary
The corpus was created through an eye-tracking experiment where participants read texts generated by three LLMs (Phi-2, Mistral 7B Instruct, WizardLM 13B) using five decoding strategies (greedy, beam, sampling, top-k, top-p) across six text types. Raw gaze data was collected at 2000 Hz and processed through multiple stages including fixation extraction, manual drift correction, and computation of reading measures. The dataset combines behavioral data with model internals and linguistic annotations, providing both corrected and uncorrected versions for validation purposes.

## Key Results
- Reading measures show expected psycholinguistic effects (word length, frequency, surprisal) across all model/decoding combinations
- Minimal variability observed between different decoding strategies and model conditions
- Corpus provides comprehensive data at multiple preprocessing stages for algorithm development and validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The corpus enables analysis of how different decoding strategies affect reading behavior by providing texts generated with five decoding strategies across three LLMs.
- Mechanism: By systematically varying decoding strategies while controlling for model and text type, researchers can isolate the effect of decoding choices on fixation patterns and reading times.
- Core assumption: Decoding strategy differences are reflected in observable eye movement patterns and not confounded by other factors.
- Evidence anchors:
  - [abstract] "The texts are generated by three large language models using five different decoding strategies"
  - [section 2.2.2] "Likelihood-Maximization Strategies" and "Stochastic Strategies" describe decoding differences
  - [corpus] Corpus provides all 14 conditions (3 models × 5 strategies + 1 beam search per model) for each text type
- Break condition: If decoding strategy effects are smaller than individual reading variability or if participants detect machine-generation cues.

### Mechanism 2
- Claim: Manual fixation correction addresses vertical drift without introducing bias by preserving sequential reading patterns.
- Mechanism: Human annotators map vertically drifted fixations to correct areas of interest based on reading flow, maintaining temporal order while correcting spatial errors.
- Core assumption: Reading has inherent sequential structure that makes drift patterns recognizable and correctable by humans.
- Evidence anchors:
  - [section 6.3] "It is usually easy for a human annotator experienced with eye movements in reading to recognize the vertical drift"
  - [section 6.3] Examples of vertical drift in Figure 8 and Appendix C show systematic patterns
  - [corpus] Provides both corrected and uncorrected versions for validation
- Break condition: If drift patterns become too complex or if sequential reading assumptions break down (e.g., non-linear reading).

### Mechanism 3
- Claim: Eye movement data at multiple preprocessing stages enables development of new gaze event detection algorithms.
- Mechanism: Raw coordinate data allows researchers to test alternative algorithms for fixation/saccade detection without being constrained by the Engbert and Kliegl (2003) method.
- Core assumption: Different preprocessing algorithms may reveal different aspects of reading behavior.
- Evidence anchors:
  - [section 6.1] "The data files written by the eye-tracker are non-human readable edf files" and provide raw coordinate data
  - [section 6.2] Describes microsaccade detection algorithm used, implying alternatives exist
  - [corpus] Raw data at 2000 Hz sampling rate available for algorithm development
- Break condition: If raw data quality is insufficient for reliable alternative detection or if algorithm development requires different data characteristics.

## Foundational Learning

- Concept: Eye movement terminology (fixations, saccades, first-pass reading time)
  - Why needed here: Understanding the basic metrics used in analyses and their interpretation
  - Quick check question: What's the difference between first-fixation duration and first-pass reading time?

- Concept: Decoding strategies (greedy search, beam search, top-k, top-p, ancestral sampling)
  - Why needed here: To understand how text generation affects reading behavior
  - Quick check question: Which decoding strategy maximizes probability at each step?

- Concept: Bayesian hierarchical modeling
  - Why needed here: To understand how psycholinguistic effects are estimated in the analyses
  - Quick check question: What's the purpose of random intercepts for subjects in the model?

## Architecture Onboarding

- Component map: Raw EDF -> ASCII conversion -> CSV processing -> Fixation detection -> Manual drift correction -> Reading measure computation -> Statistical analysis
- Critical path: Raw data acquisition -> Fixation extraction -> Manual correction -> Reading measure computation
- Design tradeoffs: Manual correction provides accuracy but introduces potential bias; automated drift correction is faster but less reliable
- Failure signatures: High proportion of fixations mapped to wrong areas of interest; inconsistent reading times across conditions; drift patterns that cannot be corrected
- First 3 experiments:
  1. Compare fixation detection algorithms on the raw data to see if alternatives perform better than the current method
  2. Analyze reading time distributions across the 14 conditions to identify any systematic differences
  3. Test the manual correction process by having multiple annotators correct the same data and measuring inter-rater reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does vertical drift correction significantly impact the validity of eye-tracking analyses across different text types and decoding strategies?
- Basis in paper: [explicit] The paper discusses manual fixation correction for vertical drift and provides both corrected and uncorrected data versions.
- Why unresolved: The paper doesn't present comparative analyses of results using corrected vs uncorrected data to quantify the impact of this correction.
- What evidence would resolve it: Direct comparison of key findings (e.g., psycholinguistic effects, decoding strategy impacts) using both corrected and uncorrected datasets to measure differences in outcomes.

### Open Question 2
- Question: Which decoding strategies produce machine-generated texts that are most cognitively aligned with human reading patterns?
- Basis in paper: [explicit] The paper states "The texts are machine-generated by LLMs of different sizes and different families using five of the most common decoding strategies" and discusses potential use cases for investigating cognitive alignment.
- Why unresolved: While the paper presents descriptive statistics of reading measures across decoding strategies, it doesn't conduct formal analyses comparing cognitive alignment metrics between strategies.
- What evidence would resolve it: Statistical comparisons of reading time patterns, fixation distributions, and surprisal correlations across decoding strategies to identify which most closely matches human reading behavior.

### Open Question 3
- Question: How does the predictive power of surprisal estimates from different language models vary when computed on machine-generated vs naturally occurring texts?
- Basis in paper: [explicit] The paper notes that "the predictive power of surprisal on human reading times has been shown to differ depending on the language model from which it has been extracted" and provides surprisal estimates from multiple models.
- Why unresolved: The paper doesn't compare surprisal predictive power between machine-generated stimuli and naturalistic corpora or across the different language models used for surprisal estimation.
- What evidence would resolve it: Correlation analyses between surprisal estimates from each language model and actual reading times, compared across machine-generated and naturalistic text datasets.

## Limitations
- Minimal variability observed between different decoding strategies and model conditions may limit detection of systematic effects
- Manual drift correction introduces potential human bias that cannot be quantified without inter-rater reliability measures
- Limited generalizability to non-native English speakers due to exclusive focus on native English participants

## Confidence
**High Confidence**: The technical infrastructure for data collection and preprocessing is sound, as evidenced by the systematic approach to recording raw gaze data at 2000 Hz, implementing multiple preprocessing stages, and providing comprehensive model internals and linguistic annotations.

**Medium Confidence**: The claim that reading measures show expected psycholinguistic effects (word length, frequency, surprisal) across all conditions is supported by the data but requires careful interpretation given the minimal variability between conditions.

**Low Confidence**: The assertion that this corpus will enable meaningful analysis of decoding strategy effects on reading behavior is questionable given the observed minimal variability in reading measures and the potential for individual differences to overwhelm systematic effects.

## Next Checks
1. **Inter-rater reliability assessment**: Have multiple annotators independently perform manual drift correction on the same subset of data and calculate agreement rates to quantify the reliability of this critical preprocessing step.

2. **Statistical power analysis**: Conduct formal power analysis on the reading time data to determine the minimum effect size detectable given the current sample size and experimental design, and calculate the number of additional participants needed to reliably detect smaller decoding strategy effects.

3. **Decoding strategy manipulation check**: Design a follow-up experiment where participants explicitly judge text naturalness or coherence across different decoding strategies, to verify that the chosen decoding parameters actually produce perceptibly different text qualities that could affect reading behavior.