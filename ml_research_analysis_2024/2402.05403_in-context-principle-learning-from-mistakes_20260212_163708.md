---
ver: rpa2
title: In-Context Principle Learning from Mistakes
arxiv_id: '2402.05403'
source_url: https://arxiv.org/abs/2402.05403
tags:
- leap
- should
- principles
- level
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Learning Principles (LEAP) improves LLM performance by intentionally
  inducing mistakes on given examples, reflecting on those mistakes to generate explicit
  task-specific principles, and then using those principles to guide responses on
  unseen examples. Across multiple reasoning benchmarks including DROP, HotpotQA,
  GSM8K, MATH, and Big-Bench Hard, LEAP improves over standard few-shot prompting
  by up to 7.5% accuracy using GPT-4.
---

# In-Context Principle Learning from Mistakes

## Quick Facts
- arXiv ID: 2402.05403
- Source URL: https://arxiv.org/abs/2402.05403
- Reference count: 40
- Improves LLM performance by up to 7.5% accuracy using Learning Principles (LEAP) without additional examples

## Executive Summary
LEAP is a method that improves in-context learning by inducing mistakes on few-shot examples, reflecting on those mistakes to generate explicit task-specific principles, and then using those principles during inference. The approach works with strong models like GPT-4, GPT-3.5-turbo, and Claude-2.1 across multiple reasoning benchmarks including DROP, HotpotQA, GSM8K, MATH, and Big-Bench Hard. Critically, LEAP requires no additional examples beyond standard few-shot settings while achieving up to 7.5% accuracy improvement.

## Method Summary
LEAP follows a three-step process: first, it intentionally generates incorrect responses on few-shot examples using high temperature sampling; second, the model reflects on these mistakes by contrasting them with correct answers to extract explicit task-specific principles; third, these principles are appended to the few-shot examples during inference on unseen test questions. The method creates both low-level principles (specific to each mistake) and high-level principles (aggregated and more abstract) to guide reasoning.

## Key Results
- Improves DROP benchmark accuracy by up to 7.5% using GPT-4
- Achieves 3.3% improvement on HotpotQA with the same approach
- Works with multiple strong models (GPT-4, GPT-3.5-turbo, Claude-2.1) across diverse reasoning tasks
- Requires no additional examples beyond standard few-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LEAP improves performance by inducing the model to make mistakes on given examples, then generating explicit principles from those mistakes to guide future responses.
- Mechanism: The model first generates incorrect responses using high temperature sampling, then reflects on these mistakes by contrasting them with correct answers to extract explicit task-specific principles. These principles are then used during inference on unseen examples.
- Core assumption: LLMs can effectively learn from their own mistakes when provided with correct answers and prompted to reflect on discrepancies.
- Evidence anchors:
  - [abstract] "First, we intentionally induce the model to make mistakes on these few examples; then the model itself reflects on these mistakes, and learn explicit task-specific 'principles' from them without any human supervision"
  - [section 3] "We identify incorrect solutions by comparing each ˆaj i with the ground-truth answer ai (which is given as part of the task)"
- Break condition: If the LLM cannot effectively distinguish between correct and incorrect reasoning patterns, or if the principles extracted are too vague to be actionable.

### Mechanism 2
- Claim: LEAP creates a two-level principle structure (low-level and high-level) to improve generalization from mistakes to unseen examples.
- Mechanism: After generating mistakes, the model first creates low-level principles specific to each mistake, then condenses these into high-level principles that are more abstract and example-agnostic.
- Core assumption: High-level principles derived from low-level mistakes will generalize better to unseen examples than the low-level principles alone.
- Evidence anchors:
  - [section 3] "Subsequently, we use the LLM to condense the low-level principles into approximately 5 key bullet points, thus creating high-level principles"
  - [section 4] "In most tasks and base models, both LEAP LOW-LEVEL and LEAP HIGH-LEVEL improve over the Few-shot COT baseline"
- Break condition: If the high-level principles become too abstract and lose their practical applicability, or if the condensation process loses important task-specific details.

### Mechanism 3
- Claim: LEAP requires no additional examples beyond standard few-shot settings while improving performance by leveraging the LLM's reflection capabilities.
- Mechanism: The same few examples used in standard few-shot prompting are used to generate mistakes and principles, then these principles are appended to the prompt during inference.
- Core assumption: The LLM's emergent ability to follow instructions and explain mistakes (as demonstrated in related work) enables effective self-reflection without additional training data.
- Evidence anchors:
  - [abstract] "Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings"
  - [section 4] "Importantly, in each benchmark, the exact same few-shot examples were used across all evaluated approaches, including the baseline and LEAP"
- Break condition: If the LLM's reflection capabilities are insufficient for the task complexity, or if the few examples provided don't contain enough diversity to generate useful principles.

## Foundational Learning

- Concept: In-context learning (ICL) and few-shot prompting
  - Why needed here: LEAP builds directly on ICL by adding the principle learning step while maintaining the same number of examples
  - Quick check question: What is the maximum number of examples typically used in few-shot prompting according to the paper?

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: LEAP is applied on top of few-shot CoT, generating mistaken CoT responses to create more informative mistakes
  - Quick check question: How does the paper modify the standard CoT approach when applying LEAP?

- Concept: Temperature sampling in LLM generation
  - Why needed here: High temperature sampling is used to intentionally generate diverse mistakes from the few-shot examples
  - Quick check question: What temperature value does the paper use when generating mistakes, and why is this important?

## Architecture Onboarding

- Component map:
  Mistake Generation -> Low-Level Principle Extraction -> High-Level Principle Condensation -> Inference Enhancement

- Critical path:
  1. Generate mistakes on few-shot examples (one-time per task)
  2. Create low-level principles from each mistake
  3. Condense to high-level principles
  4. Append principles to few-shot examples for inference on test set

- Design tradeoffs:
  - Using same LLM for all steps vs. specialized models for different components
  - Low-level vs. high-level principles (specificity vs. generalizability)
  - Number of mistakes generated per example (15 in paper) vs. computational cost

- Failure signatures:
  - Principles are too verbose or similar to each other (seen with Gemini Pro on HotpotQA)
  - Open-source models fail to leverage learned principles despite generating them correctly
  - Performance degrades when principles are too abstract and lose task-specific guidance

- First 3 experiments:
  1. Replicate baseline few-shot CoT on a simple reasoning task (e.g., GSM8K) to establish performance baseline
  2. Apply LEAP LOW-LEVEL on same task and compare improvement
  3. Apply LEAP HIGH-LEVEL and compare both performance and principle quality with LOW-LEVEL version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do open-source models like Llama-2-chat-70B fail to effectively utilize principles learned through LEAP, while proprietary models like GPT-4 and GPT-3.5-turbo show significant improvements?
- Basis in paper: Explicit - Section 4.3 and Table 3 show that Llama-2-chat-70B does not improve with LEAP while proprietary models do.
- Why unresolved: The paper identifies that open-source models may lack sufficient instruction-following and reflection capabilities, but doesn't explore what specific architectural or training differences cause this limitation or how these capabilities could be enhanced in open-source models.
- What evidence would resolve it: Comparative analysis of instruction-following capabilities between open-source and proprietary models, experiments testing different prompting strategies with open-source models, or ablation studies identifying which components of LEAP are most challenging for open-source models.

### Open Question 2
- Question: What is the optimal number of examples needed for LEAP to generate effective principles, and how does this vary across different reasoning task complexities?
- Basis in paper: Inferred - The paper uses 3 examples consistently across benchmarks but doesn't explore whether this is optimal or how it varies by task complexity.
- Why unresolved: While the paper demonstrates LEAP works with 3 examples, it doesn't investigate whether more or fewer examples would be more effective for different types of reasoning tasks or whether the optimal number depends on task complexity.
- What evidence would resolve it: Systematic experiments varying the number of examples (1-10) across different task types and measuring the quality of generated principles and downstream performance, or theoretical analysis of how principle complexity scales with the number of training examples.

### Open Question 3
- Question: How do the low-level and high-level principles generated by LEAP differ in their effectiveness across various reasoning tasks, and what determines which type is more beneficial?
- Basis in paper: Explicit - Section 4 and Tables 1-2 show that sometimes LEAP LOW-LEVEL performs better and sometimes LEAP HIGH-LEVEL performs better, with no clear pattern identified.
- Why unresolved: The paper observes inconsistent performance between low-level and high-level principles but doesn't provide a framework for predicting when each type would be more effective or analyze the structural differences in the principles that lead to better performance.
- What evidence would resolve it: Detailed analysis of the content and structure of principles that lead to improvements versus those that don't, correlation between principle characteristics (specificity, abstraction level, actionability) and task types, or experiments directly comparing the two approaches on the same tasks with larger sample sizes.

## Limitations

- Open-source models show inconsistent improvement despite generating valid principles, suggesting brittleness in instruction-following capabilities
- The computational overhead of generating 15 mistakes per example is not thoroughly discussed and could be prohibitive for resource-constrained applications
- The underlying cognitive mechanism of why mistake reflection leads to better principles remains unclear and may not generalize to all reasoning tasks

## Confidence

**High Confidence (Likelihood > 80%):**
- The basic methodology of LEAP (mistake generation → principle extraction → inference) is reproducible and works as described on strong models like GPT-4
- The improvement over standard few-shot CoT prompting is real and measurable across multiple benchmarks
- LEAP requires no additional examples beyond standard few-shot settings

**Medium Confidence (Likelihood 50-80%):**
- The high-level principles consistently outperform low-level principles across all tasks and models (inconsistent results with open-source models)
- The 7.5% accuracy improvement on DROP represents typical performance gains (this appears to be the maximum rather than typical)
- The method generalizes well to new reasoning tasks beyond those tested

**Low Confidence (Likelihood < 50%):**
- The principles extracted are truly "task-specific" rather than generic reasoning patterns
- The mistake generation process (temperature sampling) is optimal and robust across all domains
- Open-source models can benefit equally from LEAP despite current limitations

## Next Checks

1. **Cross-Model Consistency Test**: Apply LEAP to multiple open-source models (Llama-2, Mistral, etc.) on the same benchmarks to quantify the gap between commercial and open-source model performance, focusing on whether principle extraction works but application fails.

2. **Principle Quality Analysis**: Manually evaluate a sample of extracted principles for each benchmark to determine: (a) whether they are truly task-specific vs. generic, (b) if high-level principles are meaningfully more abstract than low-level ones, and (c) correlation between principle quality and downstream performance.

3. **Computational Overhead Measurement**: Benchmark the full LEAP pipeline including mistake generation (15 samples × examples) to quantify wall-clock time and token costs compared to standard few-shot CoT, particularly for deployment scenarios with strict latency requirements.