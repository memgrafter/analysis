---
ver: rpa2
title: Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine Translation
arxiv_id: '2410.20941'
source_url: https://arxiv.org/abs/2410.20941
tags:
- translation
- fluency
- text
- evaluation
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the ability of instruction-tuned large language
  models (LLMs) to perform document-level machine translation (docMT) without specialized
  fine-tuning. It compares translating entire documents in a single pass to translating
  individual sentences separately and then concatenating them.
---

# Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine Translation

## Quick Facts
- arXiv ID: 2410.20941
- Source URL: https://arxiv.org/abs/2410.20941
- Reference count: 40
- Primary result: Instruction-tuned LLMs can perform document-level MT without fine-tuning, with GPT-4 judging achieving ~95% human agreement

## Executive Summary
This work investigates the capability of instruction-tuned large language models to perform document-level machine translation without specialized fine-tuning. The study compares translating entire documents in a single pass versus translating individual sentences separately and concatenating them. Results demonstrate that document-level translation produces better fluency and fewer content errors despite BLEU scores often favoring the sentence-based approach. The paper introduces an LLM-as-a-judge evaluation framework using GPT-4 to assess translation quality across multiple dimensions including fluency, content errors, lexical cohesion, and grammatical cohesion, addressing BLEU's limitations in capturing discourse-level phenomena.

## Method Summary
The study evaluates instruction-tuned LLMs (Vicuna-7B/13B/16K, Mistral-instruct-7B) on the WMT22 test set across four translation directions (zh-en, en-zh, de-en, en-de). Two translation approaches are compared: DOC (translating entire documents in one pass) and ST[k] (translating k-sentence chunks separately then concatenating). BLEU scores are calculated for traditional evaluation, while GPT-4 is used to assess translation quality through targeted prompts evaluating fluency, content errors, lexical cohesion, and grammatical cohesion. Human agreement with GPT-4 evaluations is measured to validate the LLM-as-a-judge paradigm.

## Key Results
- Document-level translation (DOC) consistently outperforms sentence-based translation (ST3) in fluency and content errors
- BLEU scores show poor correlation with discourse-level phenomena metrics and can be misleading for docMT evaluation
- GPT-4-as-a-judge achieves approximately 95% agreement with human evaluations
- Instruction-tuned LLMs can effectively leverage document context for translation without specialized fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating entire documents in a single pass improves fluency and reduces content errors
- Mechanism: LLMs leverage inter-sentence dependencies and document-level context when processing entire documents
- Core assumption: Instruction-tuning provides sufficient understanding of document-level discourse
- Evidence: DOC outperforms ST3 in fluency and content errors across multiple models
- Break condition: Document length exceeding LLM's context window

### Mechanism 2
- Claim: BLEU scores are unreliable for docMT evaluation
- Mechanism: BLEU's n-gram matching approach cannot assess global document coherence
- Core assumption: Discourse-level phenomena are essential but not reflected in n-gram statistics
- Evidence: Poor correlation between BLEU and discourse-level metrics
- Break condition: BLEU used with additional discourse-level metrics

### Mechanism 3
- Claim: GPT-4 evaluation provides reliable multi-dimensional assessment
- Mechanism: Targeted prompts assess fluency, content errors, lexical cohesion, and grammatical cohesion separately
- Core assumption: GPT-4's language understanding enables effective multi-dimensional evaluation
- Evidence: ~95% human agreement with GPT-4 evaluations
- Break condition: Inconsistent results across different prompt formulations

## Foundational Learning

- Concept: Document-level machine translation (docMT)
  - Why needed here: Distinguishes sentence-level from document-level translation challenges
  - Quick check: What are the key differences between sentence-level and document-level translation?

- Concept: Evaluation metrics for translation quality
  - Why needed here: Explains why BLEU alone is insufficient for docMT
  - Quick check: How do BLEU, fluency, content errors, and cohesion differ in what they measure?

- Concept: Instruction-tuned large language models
  - Why needed here: Explains the model capabilities being tested
  - Quick check: What is instruction-tuning and how does it differ from fine-tuning?

## Architecture Onboarding

- Component map: WMT22 test set -> Translation models (Vicuna, Mistral) -> DOC/ST[k] approaches -> BLEU scores + GPT-4 evaluation -> Multi-dimensional analysis
- Critical path: Prepare data -> Translate using DOC/ST[k] -> Calculate BLEU -> GPT-4 evaluation -> Analyze correlations -> Validate with human agreement
- Design tradeoffs: Nuanced evaluation (GPT-4) vs. computational cost, document coherence vs. potential error accumulation, context window limitations
- Failure signatures: BLEU-accuracy misalignment, low human agreement with GPT-4, performance degradation at context limits, inconsistent translation directions
- First 3 experiments: 1) Compare DOC vs ST3 using BLEU and GPT-4 on small subset, 2) Test BLEU correlation with discourse metrics, 3) Validate GPT-4 reliability with human judgments

## Open Questions the Paper Calls Out

- Low-resource languages: How do instruction-tuned LLMs perform in docMT for low-resource language pairs compared to high-resource ones? The paper evaluates only high-resource languages and acknowledges this limitation.
- Larger models: Do larger LLMs (beyond 7B/13B tested) continue to show improved docMT performance and maintain the observed BLEU vs discourse-level evaluation trends?
- Context length: Does the poor BLEU correlation persist when translating texts that exceed the model's context length? The study avoids documents exceeding 2048 tokens.

## Limitations

- Context window dependency: Findings constrained by tested models' context window sizes (4K-16K tokens)
- Limited linguistic diversity: Evaluation limited to four high-resource language pairs from WMT22
- Prompt sensitivity: GPT-4 evaluation reliability may depend on specific prompt formulations

## Confidence

- High Confidence: Document-level translation superiority in fluency and content errors
- Medium Confidence: BLEU scores provide misleading results for docMT evaluation
- Medium Confidence: LLM-as-a-judge paradigm with ~95% human agreement

## Next Checks

- Test DOC vs ST3 comparison across broader range of language pairs including low-resource languages
- Systematically vary GPT-4 evaluation prompts to measure sensitivity to formulation
- Evaluate translations using additional traditional discourse-level metrics to triangulate findings