---
ver: rpa2
title: Large Language Models Empowered Personalized Web Agents
arxiv_id: '2410.17236'
source_url: https://arxiv.org/abs/2410.17236
tags:
- user
- personalized
- memory
- function
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of LLM-empowered personalized Web
  agents, which integrate user-specific data (profiles and historical behaviors) to
  enhance instruction comprehension and action execution. To support this, the authors
  construct the first comprehensive benchmark, PersonalWAB, featuring 1,000 users
  with diverse profiles and behaviors across three tasks (search, recommendation,
  review), Web functions, and two evaluation paradigms.
---

# Large Language Models Empowered Personalized Web Agents

## Quick Facts
- arXiv ID: 2410.17236
- Source URL: https://arxiv.org/abs/2410.17236
- Reference count: 40
- One-line primary result: PUMA framework outperforms existing methods on PersonalWAB benchmark for personalized Web agents

## Executive Summary
This paper introduces the task of LLM-empowered personalized Web agents that integrate user-specific data to enhance instruction comprehension and action execution. The authors construct PersonalWAB, the first comprehensive benchmark featuring 1,000 users with diverse profiles and behaviors across search, recommendation, and review tasks. They propose PUMA, a novel framework that retrieves relevant user behaviors via task-specific memory and aligns LLMs for personalized action execution using fine-tuning and preference optimization.

## Method Summary
The PUMA framework constructs a memory bank storing long-term user behaviors and retrieves relevant entries using cosine similarity with current instructions. It employs heuristic methods to generate pseudo-labels for Web function parameters, using these for supervised fine-tuning. Direct preference optimization then optimizes the model based on pair-wise comparisons of function parameters generated through diverse sampling strategies. The framework is evaluated on the PersonalWAB benchmark across single-turn and multi-turn evaluation tracks.

## Key Results
- PUMA consistently outperforms existing methods on PersonalWAB benchmark
- Framework demonstrates improved alignment with personalized instructions and preferences
- Maintains efficiency while achieving superior performance across search, recommendation, and review tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific memory retrieval improves function accuracy by filtering relevant user behaviors for each task
- Mechanism: The framework stores long-term user behaviors in a memory bank and retrieves only the most relevant entries using cosine similarity with the current instruction. For each task type (search, recommendation, review), it extracts targeted features from the retrieved memory, ensuring the LLM focuses on contextually appropriate information when generating function parameters
- Core assumption: User behaviors relevant to the current instruction can be identified through semantic similarity and that these filtered behaviors will improve decision-making
- Evidence anchors:
  - [abstract]: "PUMA utilizes a memory bank with a task-specific retrieval strategy to filter relevant historical Web behaviors."
  - [section]: "When the user ð‘¢ provides an instruction ð‘– and the Web function ð‘“ is determined, we first retrieve the top ð¾ memory entries by computing the cosine similarity between the instruction ð‘– and each memory ð‘š ð‘— in the bank ð‘€."
  - [corpus]: Weak - The related papers mention memory mechanisms but don't specifically validate task-specific retrieval for Web agents
- Break condition: If the semantic similarity measure fails to capture task relevance, or if user behaviors are too sparse to provide meaningful context

### Mechanism 2
- Claim: Heuristic fine-tuning with pseudo-labels enables the model to generate reasonable function parameters despite vast parameter space
- Mechanism: The framework constructs pseudo-labels for each Web function using heuristic methods - ChatGPT for search queries, recent product ASINs for recommendations, and actual review text for reviews. These pseudo-labels are used in supervised fine-tuning to teach the LLM to generate plausible parameters
- Core assumption: Heuristic methods can generate high-quality pseudo-labels that approximate optimal function parameters
- Evidence anchors:
  - [abstract]: "PUMA designs several heuristic strategies to construct pseudo-label parameters for supervised fine-tuning (SFT)."
  - [section]: "The labels are the Web function parameters, constructed using heuristic methods tailored to each Web function."
  - [corpus]: Weak - Related papers mention memory mechanisms but don't specifically address pseudo-label construction for function parameter generation
- Break condition: If heuristic methods generate poor quality pseudo-labels that mislead the model, or if the function parameter space is too complex for heuristic approximation

### Mechanism 3
- Claim: Direct preference optimization (DPO) with diverse parameter sampling aligns the model with personalized user preferences
- Mechanism: After SFT, the framework generates diverse parameter candidates using high-temperature sampling and beam search. It evaluates these candidates based on result accuracy, then uses the best and worst performers to create pair-wise preference data for DPO. This optimizes the model to generate parameters similar to high-reward examples while avoiding low-reward ones
- Core assumption: Pair-wise preference optimization based on result accuracy can effectively align the model with user preferences
- Evidence anchors:
  - [abstract]: "PUMA then aligns LLMs for personalized action execution through fine-tuning and direct preference optimization."
  - [section]: "We then apply DPO to optimize the fine-tuned model ðœ‹ref by encouraging it to generate function parameters similar to ð‘b ð‘– and discouraging it from generating function parameters similar to ð‘w ð‘–."
  - [corpus]: Weak - While related papers mention personalization, they don't specifically validate DPO for Web agent parameter optimization
- Break condition: If the evaluation metric doesn't accurately reflect user preferences, or if the sampling process fails to generate sufficiently diverse candidates

## Foundational Learning

- Concept: Semantic similarity measures for information retrieval
  - Why needed here: The framework relies on cosine similarity between instructions and memory entries to retrieve relevant user behaviors
  - Quick check question: How would you implement cosine similarity calculation between text instructions and memory entries using sentence transformers?

- Concept: Supervised fine-tuning with pseudo-labels
  - Why needed here: The framework uses heuristic methods to generate pseudo-labels for function parameters, requiring SFT to train the model on these constructed examples
  - Quick check question: What are the key differences between training on real labels versus pseudo-labels, and how might this affect model performance?

- Concept: Direct preference optimization (DPO)
  - Why needed here: The framework uses DPO to optimize the model based on pair-wise comparisons of function parameters, requiring understanding of preference learning techniques
  - Quick check question: How does DPO differ from traditional reinforcement learning approaches, and what are its advantages for this application?

## Architecture Onboarding

- Component map: Memory Bank -> Task-specific Retrieval -> SFT Module -> DPO Module -> LLM Core
- Critical path: Instruction â†’ Task-specific Retrieval â†’ SFT â†’ DPO â†’ Function Parameter Generation
- Design tradeoffs:
  - Memory length vs. computational efficiency (256 vs. 768 tokens)
  - Heuristic quality vs. training time for pseudo-label generation
  - Temperature in sampling vs. diversity of parameter candidates
- Failure signatures:
  - Low function accuracy: Memory retrieval not capturing relevant information
  - Poor result accuracy: SFT or DPO not effectively learning from pseudo-labels
  - Slow response times: Memory bank too large or inefficient retrieval
- First 3 experiments:
  1. Test memory retrieval with different similarity thresholds to find optimal balance between recall and precision
  2. Compare different heuristic methods for pseudo-label generation to identify most effective approach
  3. Evaluate impact of temperature settings on parameter diversity and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework handle evolving user preferences over time?
- Basis in paper: [inferred] The paper mentions that user profiles are generated based on historical behaviors, but does not address how the framework adapts to changes in user preferences over time
- Why unresolved: The paper focuses on constructing profiles from historical data and using them for current tasks, but does not discuss mechanisms for updating profiles or adapting to shifts in user preferences
- What evidence would resolve it: A detailed description of how the framework updates user profiles dynamically, including any mechanisms for detecting and incorporating changes in user preferences, would clarify this aspect

### Open Question 2
- Question: What are the limitations of the task-specific memory retrieval strategy in handling diverse user behaviors?
- Basis in paper: [explicit] The paper describes a task-specific memory retrieval strategy but does not discuss its limitations in handling diverse or complex user behaviors
- Why unresolved: While the strategy is designed to extract relevant information, the paper does not explore scenarios where user behaviors might be too diverse or complex for the retrieval strategy to effectively capture all relevant information
- What evidence would resolve it: Empirical studies or examples demonstrating the strategy's performance across a wide range of user behaviors, including edge cases, would provide insights into its limitations

### Open Question 3
- Question: How does the framework ensure the diversity of function parameters in the Direct Preference Optimization (DPO) process?
- Basis in paper: [explicit] The paper mentions generating diverse function parameters using high-temperature sampling and beam search, but does not detail how diversity is maintained in the DPO process
- Why unresolved: While the initial sampling aims for diversity, the DPO process might converge towards specific parameter sets, potentially reducing diversity
- What evidence would resolve it: A detailed explanation of how the DPO process maintains or enhances parameter diversity, possibly through additional sampling or diversity-preserving techniques, would clarify this aspect

### Open Question 4
- Question: How does the framework handle ambiguous user instructions?
- Basis in paper: [inferred] The paper does not explicitly address how the framework deals with ambiguous user instructions, which could be a common scenario in real-world applications
- Why unresolved: Ambiguous instructions could lead to incorrect function calls or parameter generation, but the paper does not discuss strategies for handling such cases
- What evidence would resolve it: Examples or case studies showing how the framework interprets and resolves ambiguous instructions would provide clarity on this issue

### Open Question 5
- Question: What are the computational costs associated with the proposed framework?
- Basis in paper: [explicit] The paper mentions efficiency gains in task completion time but does not provide a detailed analysis of the overall computational costs
- Why unresolved: While the framework shows improvements in task completion time, the computational resources required for memory retrieval, parameter generation, and optimization are not discussed
- What evidence would resolve it: A comprehensive analysis of the computational resources (e.g., memory, processing time) required by each component of the framework would provide a clearer picture of its efficiency

## Limitations

- The effectiveness of heuristic pseudo-label generation for function parameters lacks ablation studies showing how different methods affect performance
- Memory retrieval mechanism assumes semantic similarity captures task relevance, which may not hold for complex user behaviors or instructions with implicit preferences
- Computational costs and resource requirements are not comprehensively analyzed despite efficiency claims

## Confidence

- Mechanism 1 (task-specific retrieval): Medium - The retrieval strategy is clearly defined but relies heavily on cosine similarity which may not capture nuanced task relevance
- Mechanism 2 (SFT with pseudo-labels): Low-Medium - While the approach is well-specified, the heuristic quality varies significantly across different Web functions
- Mechanism 3 (DPO alignment): Medium-High - The pair-wise optimization framework is standard, though its effectiveness depends heavily on the quality of parameter sampling

## Next Checks

1. Conduct ablation studies comparing different heuristic methods for pseudo-label generation to identify which approaches yield the most robust function parameters
2. Test the memory retrieval mechanism with varying similarity thresholds and evaluate precision-recall tradeoffs for task-specific information extraction
3. Evaluate model performance when applying different sampling strategies (temperature, beam search) during the DPO phase to determine optimal diversity vs. accuracy tradeoffs