---
ver: rpa2
title: 'Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese'
arxiv_id: '2404.07824'
source_url: https://arxiv.org/abs/2404.07824
tags:
- japanese
- evaluation
- vlms
- heron
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Japanese Heron-Bench, a novel benchmark for
  evaluating Vision Language Models (VLMs) in Japanese. The authors address the gap
  in VLM evaluation for non-English languages by creating a benchmark with image-question-answer
  pairs tailored to Japanese context.
---

# Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese

## Quick Facts
- arXiv ID: 2404.07824
- Source URL: https://arxiv.org/abs/2404.07824
- Reference count: 39
- Introduces Japanese Heron-Bench, a novel benchmark for evaluating Vision Language Models (VLMs) in Japanese

## Executive Summary
The paper introduces Japanese Heron-Bench, a novel benchmark designed to evaluate Vision Language Models (VLMs) in Japanese. The authors address the gap in VLM evaluation for non-English languages by creating a benchmark with image-question-answer pairs tailored to Japanese cultural contexts. They also present a baseline Japanese VLM trained using Japanese visual instruction tuning datasets. The Japanese Heron-Bench reveals strengths and limitations of the proposed VLM across various ability dimensions, and the study compares the performance of the baseline model with strong closed models like GPT-4V, highlighting the capability gap between them. The benchmark dataset and training code are made publicly available to facilitate further developments in Japanese VLM research.

## Method Summary
The authors developed Japanese Heron-Bench using 21 Japanese-specific images with 102 questions across categories like anime, food, culture, and landmarks. They trained a baseline Japanese VLM using visual instruction tuning with approximately 620K Japanese image-text pairs, leveraging the language capabilities of a Japanese LLM through an adapter architecture. The benchmark uses GPT-4 API to generate reference answers and scores both reference and VLM answers on a 10-point scale, with the final VLM score being the ratio of its average score to GPT-4's average score. The baseline model architecture consists of OpenAI's CLIP Large Patch 14 as image encoder, StabilityAI's japanese-stablelm-base-alpha-7b as Japanese LLM, and a single linear adapter layer connecting them.

## Key Results
- Japanese Heron-Bench reveals model capabilities through comparative scoring against GPT-4 model answers
- Visual instruction tuning with Japanese datasets enables effective Japanese VLM development
- Cultural context specificity in the benchmark reveals capability gaps between models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Japanese Heron-Bench reveals model capabilities through comparative scoring against GPT-4 model answers.
- Mechanism: The benchmark uses GPT-4 API to generate reference answers for image-question pairs, then scores both the reference and VLM answers on a 10-point scale. The final VLM score is the ratio of its average score to GPT-4's average score.
- Core assumption: GPT-4's answers serve as a reliable reference standard for Japanese visual reasoning tasks.
- Evidence anchors:
  - [abstract] "The Japanese Heron-Bench consists of a variety of image-question answer pairs tailored to the Japanese context. Using this dataset, we can effectively analyze the abilities of VLMs to understand visual scenes and answer questions in the Japanese context."
  - [section] "The obtained answers, the GPT-4's answers, and the contexts (ground truth) are then evaluated by using GPT-4 API. The GPT-4 API is instructed to assign scores out of 10 to both the GPT-4's answers and the VLM's answers based on the context and to provide explanations for the scores."
- Break condition: If GPT-4's Japanese language performance is insufficient for the cultural context, the scoring becomes unreliable.

### Mechanism 2
- Claim: Visual instruction tuning with Japanese datasets enables effective Japanese VLM development.
- Mechanism: The baseline Japanese VLM is trained using visual instruction tuning with approximately 620K Japanese image-text pairs, leveraging the language capabilities of a Japanese LLM through an adapter architecture.
- Core assumption: Japanese visual instruction tuning can transfer knowledge effectively from English-trained models.
- Evidence anchors:
  - [abstract] "we present a baseline Japanese VLM that has been trained with Japanese visual instruction tuning datasets."
  - [section] "For the model training, we adopted the visual instruction tuning method proposed for developing LLaVA-1.6. The dataset consists of approximately 558K samples used for pre-training the adapter, and about 665K image-text pair samples used for instruction tuning when the LLM and adapter parameters were unfrozen during training."
- Break condition: If the Japanese instruction tuning dataset lacks diversity or is too small, performance improvements may be limited.

### Mechanism 3
- Claim: Cultural context specificity in the benchmark reveals capability gaps between models.
- Mechanism: The benchmark uses 21 Japanese-specific images with 102 questions across categories like anime, food, culture, and landmarks, revealing which models perform well in culturally-specific contexts versus generic visual reasoning.
- Core assumption: Japanese cultural context provides meaningful differentiation between model capabilities.
- Evidence anchors:
  - [abstract] "The Japanese Heron-Bench consists of a variety of image-question answer pairs tailored to the Japanese context."
  - [section] "The final evaluation dataset consists of 102 questions. Furthermore, each image is assigned one of seven subcategories: anime, art, culture, food, landscape, landmark, and transportation."
- Break condition: If models can perform well without understanding Japanese cultural context, the benchmark fails to differentiate capabilities.

## Foundational Learning

- Concept: Vision Language Models (VLMs) combine image encoders with language models to perform multimodal reasoning.
  - Why needed here: Understanding the architecture is crucial for interpreting why Japanese Heron-Bench is necessary.
  - Quick check question: What component connects the image encoder to the language model in typical VLM architectures?

- Concept: Visual instruction tuning methodology for VLMs.
  - Why needed here: The baseline model uses this technique, making it essential to understand how it works.
  - Quick check question: How does visual instruction tuning differ from standard pretraining in VLMs?

- Concept: Cross-attention mechanisms in multimodal models.
  - Why needed here: Many VLM architectures use cross-attention to align visual and textual features.
  - Quick check question: What role does cross-attention play in connecting visual features to language model tokens?

## Architecture Onboarding

- Component map: OpenAI's CLIP Large Patch 14 (336) → Linear adapter → StabilityAI's japanese-stablelm-base-alpha-7b
- Critical path: Image → CLIP encoder → Linear adapter → Japanese LLM → Answer generation
- Design tradeoffs: Using a single linear adapter layer simplifies architecture but may limit fine-grained visual reasoning compared to more complex adapter designs.
- Failure signatures: Low scores on culturally-specific questions indicate adapter or Japanese LLM limitations; high scores on generic questions suggest image encoder capabilities are adequate.
- First 3 experiments:
  1. Evaluate baseline model on Japanese Heron-Bench to establish performance baseline
  2. Test adapter ablation by removing adapter layer and observing performance degradation
  3. Compare scores across subcategories to identify specific capability strengths and weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Japanese Heron-Bench compare to existing Japanese VLM evaluation benchmarks like JA-VG-VQA-500 and JA-VLM-Bench-In-the-Wild?
- Basis in paper: The paper mentions these benchmarks but does not provide a direct comparison of their effectiveness or suitability for evaluating Japanese VLMs.
- Why unresolved: The paper only states that Heron-Bench serves as a "valuable new option" but does not quantify how it differs from or improves upon existing benchmarks.
- What evidence would resolve it: A comparative analysis of evaluation results across all three benchmarks, highlighting differences in scoring, question types, and overall effectiveness in measuring Japanese VLM capabilities.

### Open Question 2
- Question: What are the specific limitations of using GPT-4 for scoring Japanese VLM answers, given its reportedly inferior Japanese language performance compared to English?
- Basis in paper: The paper acknowledges this limitation but does not provide empirical data on how it affects scoring accuracy or reliability.
- Why unresolved: The authors mention the issue but do not quantify its impact on the benchmark's validity or suggest alternative scoring methods.
- What evidence would resolve it: A study comparing GPT-4's Japanese scoring consistency with human raters or other Japanese language models, along with an analysis of scoring discrepancies for different types of questions.

### Open Question 3
- Question: How does the Japanese VLM baseline model's performance on Heron-Bench correlate with its real-world application capabilities in Japanese cultural contexts?
- Basis in paper: The paper introduces a baseline model but does not evaluate its practical utility beyond the benchmark.
- Why unresolved: The authors focus on benchmark performance but do not address how well the model would perform in actual Japanese cultural or linguistic scenarios.
- What evidence would resolve it: User studies or real-world tests of the model's performance in Japanese cultural contexts, comparing its benchmark scores with its practical effectiveness in tasks like cultural artifact analysis or Japanese language assistance.

## Limitations
- Benchmark relies heavily on GPT-4's performance as a reference for scoring, introducing a circular evaluation problem
- Japanese visual instruction tuning dataset (approximately 620K samples) is relatively small compared to English VLM training datasets
- Baseline model uses a single linear adapter layer, representing a simplified approach compared to more sophisticated adapter architectures

## Confidence

- **High Confidence**: The creation of a Japanese-specific VLM benchmark addresses a genuine gap in the field, and the methodology for benchmark construction (using GPT-4 API for scoring) is clearly described and reproducible.
- **Medium Confidence**: The baseline model training methodology is well-documented, but the limited scale of Japanese training data and the use of a single adapter layer suggest that the model's performance ceiling may be constrained.
- **Low Confidence**: The paper claims the benchmark reveals "strengths and limitations" of VLMs, but without comparison to a broader range of baseline models or human evaluation, the benchmark's sensitivity to model differences remains uncertain.

## Next Checks
1. Conduct human evaluation of GPT-4's answers on culturally-specific Japanese questions to verify they represent accurate and culturally-appropriate responses before using them as the benchmark reference.
2. Analyze the Japanese visual instruction tuning dataset for diversity across cultural contexts, image types, and question styles to assess whether the 620K samples provide sufficient coverage for robust VLM training.
3. Evaluate additional VLMs (both Japanese and non-Japanese) on the Japanese Heron-Bench to determine whether the benchmark effectively differentiates between models with varying capabilities in Japanese visual reasoning.