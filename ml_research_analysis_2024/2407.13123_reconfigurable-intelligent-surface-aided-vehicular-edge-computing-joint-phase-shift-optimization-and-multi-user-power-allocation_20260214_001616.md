---
ver: rpa2
title: 'Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint Phase-shift
  Optimization and Multi-User Power Allocation'
arxiv_id: '2407.13123'
source_url: https://arxiv.org/abs/2407.13123
tags:
- power
- offloading
- ieee
- phase-shift
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of optimizing vehicular edge computing
  (VEC) in environments with obstacles by introducing reconfigurable intelligent surfaces
  (RIS). The authors propose a novel deep reinforcement learning (DRL) framework that
  jointly optimizes RIS phase-shift and multi-user power allocation.
---

# Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint Phase-shift Optimization and Multi-User Power Allocation

## Quick Facts
- arXiv ID: 2407.13123
- Source URL: https://arxiv.org/abs/2407.13123
- Reference count: 40
- Primary result: Proposed SARL-MARL framework reduces power consumption by 14.48% compared to NO-RIS and 90.18% compared to fully centralized DDPG at 4 Mbps task arrival rate

## Executive Summary
This paper addresses the challenge of optimizing vehicular edge computing (VEC) in environments with obstacles by introducing reconfigurable intelligent surfaces (RIS). The authors propose a novel deep reinforcement learning (DRL) framework that jointly optimizes RIS phase-shift and multi-user power allocation. By decomposing the problem into two sub-problems and using a two-stage approach with DDPG for RIS phase-shift optimization followed by a modified MADDPG algorithm for vehicle user power allocation, the framework achieves significant improvements in power consumption and buffer length reduction compared to traditional approaches.

## Method Summary
The paper proposes a two-stage deep reinforcement learning framework for RIS-aided VEC systems. Stage 1 uses a centralized DDPG algorithm to optimize RIS phase-shift coefficients to maximize VU information transmission rates. Stage 2 employs a modified MADDPG algorithm with twin-delayed critics for VU power allocation, using the trained RIS model from Stage 1. The system jointly minimizes power consumption and buffer length across all vehicles through a dual-reward structure that balances local and global objectives.

## Key Results
- Proposed SARL-MARL framework achieves 14.48% power reduction compared to NO-RIS configuration at 4 Mbps task arrival rate
- Framework shows 90.18% improvement over fully centralized DDPG scheme in power consumption
- Significant buffer length reduction achieved across various task arrival rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The joint DDPG + modified MADDPG framework separates RIS phase-shift optimization from vehicle power allocation, allowing each problem to be solved in a more tractable way.
- Mechanism: RIS phase-shift coefficients are first optimized using a centralized DDPG agent that maximizes VU information transmission rate; the trained model is then fixed and used as input to the MADDPG-based power allocation stage. This avoids the complexity of joint optimization of different agent types (RIS vs vehicles).
- Core assumption: The RIS phase-shift problem can be effectively decoupled from power allocation because RIS is a passive device whose coefficients affect all vehicles' channels simultaneously.
- Evidence anchors:
  - [abstract] "We decompose the optimization problem into two sub-problems, firstly, RIS is used as an auxiliary tool which aims to maximize the information transmission rate of the VUs by adjusting the phase-shift coefficients..."
  - [section] "Since the vehicle and RIS are different types of agents, so in order to solve P1, we first solve the RIS phase-shift optimal problem during vehicle offloading by fixing the vehicle offloading power..."
  - [corpus] Limited; most related works do not explicitly decompose RIS and vehicle agent problems.
- Break condition: If the RIS channel conditions change too rapidly for the pre-trained model to remain valid, the decoupling assumption fails.

### Mechanism 2
- Claim: Using twin-delayed critics (TD3-style) in the global critic of MADDPG reduces overestimation bias and improves stability of multi-agent power allocation.
- Mechanism: The modified MADDPG replaces the single global critic with two twin critics and takes the minimum value during updates, following TD3's approach to mitigate overestimation. This stabilizes learning in a cooperative multi-agent setting.
- Core assumption: Overestimation bias in value estimation is a significant cause of instability in multi-agent RL, and twin critics can reduce it.
- Evidence anchors:
  - [section] "to improve the MADDPG algorithm, we employ twin-delay deterministic strategy gradient to replace the global critic."
  - [section] "the modified policy gradient can be written as... Global Critic... and Local Critic..."
  - [corpus] Weak; no direct citation to TD3 in related papers, though some RIS/MEC works mention RL without explicit TD3-style critic modifications.
- Break condition: If the state and action spaces are too large or non-stationary, the twin critic approach may still struggle to converge.

### Mechanism 3
- Claim: The local reward encourages individual VU power efficiency while the global reward enforces cooperation, balancing individual and system-wide objectives.
- Mechanism: Each VU receives a local reward based on its own power consumption and buffer length, and a global reward that is the average of all local rewards. This dual-reward structure guides agents toward Pareto-optimal solutions.
- Core assumption: Multi-agent RL can effectively learn a policy that balances local and global rewards when both are explicitly provided.
- Evidence anchors:
  - [section] "the local and global rewards of the kth VU at time slot t are rk,l = -[w1(pk,o(t) + pk,l(t)) + w2qk(t)]" and "rg = 1/K Σk∈K rk,l."
  - [section] "each agent operates with its own actor and critic networks for decision-making and action evaluation."
  - [corpus] Weak; related works mention multi-agent RL but rarely detail dual-reward designs.
- Break condition: If the weight tuning (w1, w2) is suboptimal, agents may over-prioritize local or global goals, hurting overall performance.

## Foundational Learning

### Concept: Deep Deterministic Policy Gradient (DDPG) algorithm
- Why needed here: DDPG is used to optimize continuous RIS phase-shift coefficients, which is a high-dimensional, non-convex problem unsuitable for discrete RL methods.
- Quick check question: In DDPG, what two neural networks are trained and what are their roles?

### Concept: Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm
- Why needed here: MADDPG allows each VU to learn its own power allocation policy while observing others' actions, enabling decentralized decision-making with centralized training.
- Quick check question: How does MADDPG differ from standard DDPG in handling multiple agents?

### Concept: Reconfigurable Intelligent Surface (RIS) channel modeling
- Why needed here: Accurate modeling of RIS-assisted channels (Rician fading, LoS components) is critical for realistic simulation and reward computation.
- Quick check question: What are the key components of the RIS-VU-BS channel gain expression used in this paper?

## Architecture Onboarding

### Component map
Centralized DDPG agent (BS) -> RIS phase-shift coefficients -> Modified MADDPG framework (K VU agents) -> Power allocation (offloading + local execution) -> Shared experience replay buffer for MADDPG -> Twin global critics + individual local critics per VU -> RIS phase-shift model loaded into VEC simulator for training and testing

### Critical path
1. Train DDPG to obtain RIS phase-shift model
2. Load RIS model, run MADDPG training for VU power allocation
3. Evaluate with testing episodes using both trained models

### Design tradeoffs
- Centralized RIS training vs. distributed VU training balances complexity and scalability
- Fixed RIS model during VU training may lag behind fast channel changes
- Twin critics add computational overhead but improve stability

### Failure signatures
- RIS model convergence stalls or oscillates -> check exploration noise and learning rates
- VU power allocation fails to reduce buffer length -> check reward weights and critic losses
- Overall reward plateaus early -> check replay buffer size and batch sampling

### First 3 experiments
1. Run DDPG training with varying numbers of RIS elements; observe reward trends
2. Compare VU power allocation rewards between MADDPG and standard DDPG
3. Evaluate end-to-end performance (power, buffer) at multiple task arrival rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed SARL-MARL framework scale with an increasing number of VUs and RIS elements?
- Basis in paper: [inferred] The paper mentions that the framework combines SARL and MARL to handle multiple VUs, but does not provide detailed scalability analysis with varying numbers of agents and RIS elements.
- Why unresolved: The paper focuses on a specific configuration (8 VUs, 36 RIS elements) and does not explore the performance limits or scaling behavior with larger numbers of agents or RIS elements.
- What evidence would resolve it: Experimental results showing the performance of the framework with different numbers of VUs and RIS elements, including analysis of computational complexity and convergence behavior.

### Open Question 2
- Question: How does the proposed framework perform under dynamic and unpredictable channel conditions compared to the tested quasi-static scenarios?
- Basis in paper: [inferred] The paper assumes quasi-static channel conditions, which may not reflect real-world scenarios with rapid and unpredictable changes.
- Why unresolved: The paper does not evaluate the robustness of the framework under highly dynamic channel conditions or sudden changes in the environment.
- What evidence would resolve it: Simulations or real-world tests demonstrating the framework's performance under varying and unpredictable channel conditions, including metrics like adaptation speed and stability.

### Open Question 3
- Question: What is the impact of RIS hardware limitations, such as discrete phase-shift values and imperfect reflection, on the overall system performance?
- Basis in paper: [explicit] The paper acknowledges hardware constraints on RIS phase-shift values but does not extensively analyze their impact on system performance.
- Why unresolved: While the paper mentions these limitations, it does not quantify how they affect the optimization results or compare them to ideal scenarios.
- What evidence would resolve it: Comparative analysis of system performance with ideal versus realistic RIS hardware constraints, including metrics like achievable rate and power consumption.

### Open Question 4
- Question: How does the proposed framework compare to other advanced optimization techniques, such as model-based methods or alternative RL algorithms, in terms of efficiency and scalability?
- Basis in paper: [inferred] The paper compares the proposed framework to DDPG, TD3, and some stochastic schemes but does not explore other optimization methods or RL algorithms.
- Why unresolved: The paper does not provide a comprehensive comparison with other state-of-the-art techniques that could offer different trade-offs in performance or computational efficiency.
- What evidence would resolve it: Experimental results comparing the proposed framework to other optimization techniques and RL algorithms, including analysis of computational complexity and performance metrics.

## Limitations
- RIS Channel Assumptions: Effectiveness relies on Rician fading channels with sufficient LoS components, which may not hold in all urban scenarios
- Training Overhead: Two-stage training process may not adapt quickly to dynamic channel conditions, potentially limiting real-time performance
- Hyperparameter Sensitivity: Framework's performance appears sensitive to reward weight tuning (w1, w2) and network hyperparameters

## Confidence
- **High Confidence**: The core algorithmic approach (DDPG for RIS, MADDPG for VU power) is technically sound and the reward structure is well-defined
- **Medium Confidence**: The simulation results showing performance improvements over baselines, though the exact magnitude may vary with different system parameters
- **Low Confidence**: Claims about real-world deployment readiness, particularly regarding training time and adaptability to rapidly changing environments

## Next Checks
1. **Ablation Study on RIS Elements**: Systematically vary N (number of RIS elements) and measure the impact on both RIS phase-shift optimization quality and end-to-end performance to understand scalability limits
2. **Reward Weight Sensitivity Analysis**: Conduct experiments with different combinations of w1 and w2 in the reward function to identify optimal settings and understand the trade-offs between power consumption and buffer length
3. **Real-Time Adaptation Test**: Implement a mechanism to periodically re-train or fine-tune the RIS phase-shift model during VU power allocation to evaluate performance under dynamic channel conditions compared to the fixed-model approach