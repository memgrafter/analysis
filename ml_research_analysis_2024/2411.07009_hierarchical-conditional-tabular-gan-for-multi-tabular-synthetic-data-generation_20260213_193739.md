---
ver: rpa2
title: Hierarchical Conditional Tabular GAN for Multi-Tabular Synthetic Data Generation
arxiv_id: '2411.07009'
source_url: https://arxiv.org/abs/2411.07009
tags:
- data
- synthetic
- table
- hctgan
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HCTGAN, a novel algorithm for generating synthetic
  multi-tabular relational data using hierarchical conditional GANs. The method extends
  CTGAN by conditioning child table generators on parent table data through Gaussian
  noise vectors, enabling information transfer between related tables.
---

# Hierarchical Conditional Tabular GAN for Multi-Tabular Synthetic Data Generation

## Quick Facts
- arXiv ID: 2411.07009
- Source URL: https://arxiv.org/abs/2411.07009
- Authors: Wilhelm Ågren; Victorio Úbeda Sosa
- Reference count: 40
- HCTGAN achieves slightly lower data quality scores than HMA1 but guarantees referential integrity and generates data faster

## Executive Summary
This paper introduces HCTGAN, a novel algorithm for generating synthetic multi-tabular relational data using hierarchical conditional GANs. The method extends CTGAN by conditioning child table generators on parent table data through Gaussian noise vectors, enabling information transfer between related tables while maintaining referential integrity. HCTGAN is evaluated on three datasets and compared against HMA1, demonstrating comparable data quality with guaranteed referential integrity and improved sampling speed. The authors conclude HCTGAN is suitable for large-scale synthetic data generation in complex multi-tabular datasets where speed and referential integrity are prioritized over marginal quality improvements.

## Method Summary
HCTGAN extends CTGAN by introducing hierarchical conditioning for multi-tabular data generation. The algorithm trains parent table generators first, then conditions child table generators on parent table data through Gaussian noise vectors. The training uses mode-specific normalization with variational Gaussian mixture models, conditional generation with training-by-sampling, and Wasserstein loss with gradient penalty. The sampling algorithm generates data hierarchically while ensuring referential integrity by carefully managing foreign key relationships. The method maintains database structure and guarantees that generated foreign keys always reference valid primary keys.

## Key Results
- HCTGAN achieves slightly lower data quality scores than HMA1 but guarantees referential integrity (RI=1.0 across all datasets)
- HCTGAN generates data faster than HMA1, particularly for larger numbers of rows
- HCTGAN produces better range coverage and more novel synthetic rows compared to HMA1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HCTGAN achieves referential integrity while maintaining data quality through hierarchical conditional generation.
- Mechanism: The algorithm conditions child table generators on parent table data via Gaussian noise vectors, ensuring foreign keys reference valid primary keys. This hierarchical conditioning transfers relational information while preserving database structure.
- Core assumption: Gaussian noise vectors can effectively capture and transfer parent table information to child generators without exposing real data.
- Evidence anchors:
  - [abstract] "conditioning child table generators on parent table data through Gaussian noise vectors, enabling information transfer between related tables"
  - [section 3.2] "zi is a concatenation of column-wise Gaussian noise for each column in the parent tables"
- Break condition: If the parent-child relationship complexity exceeds the Gaussian noise representation capacity, referential integrity may be compromised.

### Mechanism 2
- Claim: Mode-specific normalization and conditional generation improve synthetic data quality for complex distributions.
- Mechanism: The algorithm uses variational Gaussian mixture models to estimate modes for continuous columns, then normalizes values based on sampled modes. This handles class imbalance and complicated column distributions effectively.
- Core assumption: Complex column distributions can be adequately modeled using mode-specific normalization with Gaussian mixtures.
- Evidence anchors:
  - [section 3.1] "Mode-specific normalization is a technique used to model columns with complicated distributions"
  - [section 3.1] "The number of modes mi for each continuous column Ci is estimated with a variational Gaussian mixture model"
- Break condition: When column distributions are too complex or multimodal for the Gaussian mixture approximation to capture accurately.

### Mechanism 3
- Claim: Wasserstein loss with gradient penalty prevents mode collapse and improves training stability.
- Mechanism: The algorithm uses WGAN modifications with Wasserstein metric in the loss function and gradient penalty, which improves gradient behavior and simplifies generator optimization.
- Core assumption: WGAN modifications provide more stable training than traditional GAN objectives for tabular data generation.
- Evidence anchors:
  - [section 3.1] "The WGAN extension improves gradient behavior and simplifies the optimization process of the generator"
  - [section 3.1] "The generator and critic networks in the CTGAN model aim to respectively minimize and maximize the Wasserstein GAN loss"
- Break condition: If the gradient penalty hyperparameter λ is not properly tuned, training stability may be compromised.

## Foundational Learning

- Mode-specific normalization
  - Why needed here: Handles complex, multimodal distributions in continuous columns that simple normalization cannot capture
  - Quick check question: How does mode-specific normalization differ from standard normalization techniques?

- Conditional generation with training-by-sampling
  - Why needed here: Ensures the generator learns the real conditional distribution by conditioning on categorical values with log-frequency sampling
  - Quick check question: Why is log-frequency distribution used for sampling conditional vectors instead of uniform distribution?

- Wasserstein GAN with gradient penalty
  - Why needed here: Provides more stable training and prevents mode collapse compared to traditional GAN objectives
  - Quick check question: What problem does the gradient penalty solve in WGAN training?

## Architecture Onboarding

- Component map:
  - Parent table generators -> Child table generators (conditioned on parent Gaussian noise) -> Sampling algorithm with referential integrity checks

- Critical path:
  1. Initialize parent table generators
  2. Train parent generators with WGAN loss
  3. For each child table, condition generator on parent noise vectors
  4. Train child generators with parent conditioning
  5. Sample data hierarchically ensuring referential integrity

- Design tradeoffs:
  - Speed vs quality: HCTGAN sacrifices some data quality metrics for faster sampling and guaranteed referential integrity
  - Complexity vs scalability: Hierarchical conditioning adds complexity but enables scaling to deep multi-tabular datasets
  - Privacy vs performance: Gaussian noise conditioning provides privacy but may limit information transfer compared to direct conditioning

- Failure signatures:
  - Poor data quality metrics: May indicate insufficient training epochs or suboptimal hyperparameters
  - Referential integrity violations: Could signal issues with the sampling algorithm or parent-child relationship modeling
  - Training instability: Might result from improper gradient penalty weight or learning rate issues

- First 3 experiments:
  1. Train HCTGAN on a simple two-table dataset and verify referential integrity in generated data
  2. Compare data quality metrics (KSComplement, CorrelationSimilarity) against HMA1 on the same dataset
  3. Test sampling speed for varying numbers of rows and measure performance degradation with dataset depth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of parent column distribution affect the quality of generated synthetic data?
- Basis in paper: [explicit] "For future work we propose to investigate the effect of the chosen parent column distribution which the child tables condition their generator on."
- Why unresolved: The paper suggests estimating parent column distributions similarly to mode-specific normalization in CTGAN, but does not implement or test this approach.
- What evidence would resolve it: Experimental comparison between HCTGAN with current noise conditioning versus HCTGAN with estimated parent column distributions, measuring quality metrics like KSComplement and CardinalityShapeSimilarity.

### Open Question 2
- Question: What is the impact of conditioning on parent information for relational learning in HCTGAN?
- Basis in paper: [explicit] "Furthermore, performing an ablation study on the conditioning of parent information would be insightful to determine its effect."
- Why unresolved: The paper introduces parent conditioning as a key innovation but does not provide ablation results comparing with unconditioned models.
- What evidence would resolve it: Ablation study comparing HCTGAN with and without parent conditioning on the same datasets, measuring metrics like CorrelationSimilarity and NewRowSynthesis.

### Open Question 3
- Question: How does HCTGAN perform on larger, more complex multi-tabular datasets beyond the three evaluated?
- Basis in paper: [inferred] The paper notes that "Given that the HMA1 model scales poorly for datasets with many relational tables, the amount of datasets available for comparison is limited" and suggests future work on larger datasets.
- Why unresolved: The evaluation is limited to three relatively small datasets (University_v1, Hepatitis_std_v1, Pyrimidine_v1) due to HMA1's scalability limitations.
- What evidence would resolve it: Evaluation of HCTGAN on larger, deeper multi-tabular datasets with more complex relationships, measuring both quality metrics and sampling efficiency compared to baseline models.

## Limitations
- Performance degradation on University_v1 suggests HCTGAN may struggle with certain data characteristics
- Comparison limited to only three datasets, baseline results may not represent state-of-the-art
- Gaussian noise conditioning may not capture all complex parent-child relationships effectively
- No exploration of HCTGAN performance with deeper hierarchies beyond 2-3 levels

## Confidence
- High Confidence: HCTGAN guarantees referential integrity through hierarchical generation (validated by consistent RI=1.0 across all datasets)
- Medium Confidence: HCTGAN achieves comparable data quality metrics to HMA1 (results show mixed performance across different metrics)
- Medium Confidence: HCTGAN provides faster sampling than HMA1 (speed metrics are reported but methodology details are limited)

## Next Checks
1. Stress Test Referential Integrity: Generate 100,000+ rows across all three datasets and systematically verify that every foreign key references an existing primary key, including edge cases where parent tables have minimal data.

2. Deep Hierarchy Evaluation: Implement HCTGAN on a 4-5 level hierarchical dataset (beyond the 2-3 levels tested) to evaluate scalability and identify potential failure modes in deeper relationships.

3. Privacy-Performance Tradeoff Analysis: Conduct controlled experiments varying the dimensionality and distribution of Gaussian noise vectors to quantify the privacy-performance tradeoff, measuring how noise reduction affects both data quality and information leakage.