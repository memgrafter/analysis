---
ver: rpa2
title: 'Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space'
arxiv_id: '2406.19370'
source_url: https://arxiv.org/abs/2406.19370
tags:
- concept
- learning
- arxiv
- color
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for analyzing concept learning
  dynamics in generative models, proposing that concepts are learned in a structured
  order determined by "concept signal" - the sensitivity of the data-generating process
  to changes in concept values. The authors develop synthetic toy datasets with controllable
  concept signals and demonstrate that learning speed for individual concepts is directly
  proportional to their concept signal strength.
---

# Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space

## Quick Facts
- arXiv ID: 2406.19370
- Source URL: https://arxiv.org/abs/2406.19370
- Reference count: 40
- Primary result: Generative models learn concepts in order determined by "concept signal," with sudden emergence of hidden capabilities where models can manipulate concepts before being able to demonstrate this through naive input prompting.

## Executive Summary
This paper introduces a framework for analyzing concept learning dynamics in generative models, proposing that concepts are learned in a structured order determined by "concept signal" - the sensitivity of the data-generating process to changes in concept values. The authors develop synthetic toy datasets with controllable concept signals and demonstrate that learning speed for individual concepts is directly proportional to their concept signal strength. They identify a two-phase learning process: (P1) acquisition of latent capability to manipulate concepts via hidden representations, and (P2) alignment of input space with these representations. Critically, they observe sudden transitions in learning dynamics that correspond to emergence of hidden capabilities - where models possess concept manipulation abilities before these can be elicited through naive input prompting. This phenomenon is validated across multiple prompting protocols including linear latent interventions and overprompting. The framework is further applied to study underspecification effects, showing that correlated concepts in conditioning information delay and hinder out-of-distribution generalization. Results generalize to realistic data like CelebA, revealing that models can learn to manipulate concepts without being able to demonstrate this capability through standard prompting methods.

## Method Summary
The authors create synthetic 2D object datasets with controllable concepts (shape, size, color, background color) and train variational diffusion models to generate these objects. They measure learning dynamics by tracking how well the model can manipulate individual concepts over training, using classifier probes to evaluate concept manipulation ability. The key innovation is the concept space framework that allows analysis of learning at the granularity of individual concepts rather than aggregate model performance. They test alternative prompting protocols including linear latent interventions (directly manipulating model representations) and overprompting (providing excessive conditioning information) to reveal hidden capabilities that cannot be elicited through standard input prompting.

## Key Results
- Concept signal strength directly controls learning speed for individual concepts in generative models
- Models exhibit sudden transitions in learning dynamics corresponding to emergence of hidden capabilities
- Underspecification in conditioning information delays and hinders out-of-distribution generalization
- Alternative prompting protocols can elicit hidden capabilities before they're accessible through naive input prompting
- Results generalize from synthetic data to realistic datasets like CelebA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Concept signal strength directly controls the learning speed for individual concepts in generative models.
- **Mechanism**: The concept signal measures the sensitivity of the data-generating process to changes in concept values (∂G(z)/∂zi). Higher sensitivity means greater gradient signal for the model to learn that concept, resulting in faster acquisition of both the latent capability and the input-space alignment for manipulating that concept.
- **Core assumption**: The learning objective is factorized at the granularity of concepts, so each concept's contribution to the loss is independent and proportional to its concept signal.
- **Evidence anchors**:
  - [abstract] "the speed at which a concept is learned, and hence the order of concept learning, is controlled by properties of the data we term concept signal"
  - [section 4.1] "we observe that concept signal dictates the speed at which individual concepts are learned"
  - [corpus] Weak - no direct corpus support for this specific mechanism
- **Break condition**: If the learning objective couples concepts (non-factorized loss), or if optimization dynamics create bottlenecks that decouple learning speed from concept signal strength.

### Mechanism 2
- **Claim**: Hidden capabilities emerge suddenly when models learn latent manipulation abilities before being able to demonstrate them through naive input prompting.
- **Mechanism**: During training, models first develop internal representations that allow manipulation of concepts (Phase P1), but cannot yet align these representations with input space operations. At a critical point, both capabilities emerge simultaneously, creating a sudden transition in learning dynamics visible as sharp turns in concept space trajectories.
- **Core assumption**: The model's latent space contains linear or near-linear representations of concepts that can be manipulated independently of input conditioning.
- **Evidence anchors**:
  - [abstract] "models possess latent capabilities that emerge suddenly and consistently during training, though a model might not exhibit these capabilities under naive input prompting"
  - [section 4.4] "we observe that alternative protocols for prompting the model can consistently elicit the desired outputs much earlier than input prompting"
  - [corpus] Moderate - related work on "emergence of hidden capabilities" in language models provides some support
- **Break condition**: If concept representations are highly non-linear or entangled, preventing independent manipulation, or if the critical transition point is gradual rather than sudden.

### Mechanism 3
- **Claim**: Underspecification in conditioning information delays and hinders OOD generalization by creating correlations between concepts.
- **Mechanism**: When conditioning information masks or correlates concepts (e.g., "red strawberry" where color and object are correlated), the model learns joint representations rather than disentangled ones. This delays the emergence of hidden capabilities and biases OOD generalization toward training distributions.
- **Core assumption**: The model can learn to disentangle concepts when given sufficient, well-specified conditioning, but correlations in training data impede this process.
- **Evidence anchors**:
  - [abstract] "correlated concepts in conditioning information delay and hinder out-of-distribution generalization"
  - [section 5] "as the percentage of masked prompts increases, the speed of learning a concept decreases"
  - [corpus] Moderate - related work on "underspecification" in machine learning provides supporting context
- **Break condition**: If the model has strong inductive biases toward disentanglement regardless of training data correlations, or if the correlation structure is learnable as a joint distribution rather than requiring disentanglement.

## Foundational Learning

- **Concept**: Concept Space Framework
  - Why needed here: Provides the coordinate system for analyzing learning dynamics at the granularity of individual concepts rather than aggregate model performance
  - Quick check question: Can you define what a "concept class" is and how it differs from a single concept in this framework?

- **Concept**: Latent Interventions vs Input Prompting
  - Why needed here: Distinguishes between manipulating model representations directly versus using training-conditioned inputs, revealing hidden capabilities
  - Quick check question: What's the key difference between linear latent intervention and overprompting protocols?

- **Concept**: Factorized Learning Objectives
  - Why needed here: Underlies the assumption that concept signal can independently control learning speed for each concept
  - Quick check question: Why is the assumption of factorized objectives critical for the concept signal mechanism to work?

## Architecture Onboarding

- **Component map**: Data generation process -> conditioning embedding MLP -> sinusoidal timestep embedding -> U-Net with ResNet blocks and self-attention -> noise prediction -> sample generation -> classifier probe evaluation

- **Critical path**: Data generation → conditioning embedding → U-Net forward pass → noise prediction → sample generation → classifier probe evaluation. The conditioning embedding is particularly critical as it bridges concept space and model representations.

- **Design tradeoffs**: Using synthetic toy datasets provides control over concept signals but limits generalizability. The choice of diffusion models enables gradient-based interventions but adds sampling complexity. Classifier probes provide automatic evaluation but may not perfectly capture human perceptual concepts.

- **Failure signatures**: Slow or no OOD generalization indicates concept memorization rather than disentanglement. Asymmetric learning trajectories suggest imbalanced concept signals. Inability to elicit capabilities through latent interventions suggests failure of the hidden capability emergence mechanism.

- **First 3 experiments**:
  1. Train baseline model on balanced concept signals (equal color/size separation) and verify symmetric learning trajectories
  2. Vary concept signal strength for one concept while keeping others constant, measuring learning speed changes
  3. Implement latent intervention protocol on intermediate checkpoint to test for hidden capabilities before input-space alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sudden transition phenomenon observed in concept learning dynamics generalize to larger, more complex generative models trained on naturalistic data?
- Basis in paper: [explicit] The paper hypothesizes that hidden capabilities emerge suddenly and consistently during training, but acknowledges that findings are mainly drawn from synthetic data and that further studies are needed to investigate how these findings apply to real data in general.
- Why unresolved: The paper only provides preliminary validation on CelebA dataset, which is relatively simple compared to complex real-world data. The experiments were conducted on synthetic 2D objects with controllable concept signals, and while some results generalize to CelebA, the complexity and diversity of real-world data may reveal different dynamics.
- What evidence would resolve it: Systematic experiments on diverse, complex datasets (e.g., ImageNet, LAION, or large-scale video datasets) using state-of-the-art generative models (e.g., diffusion models, GANs, or autoregressive models) to observe if sudden transitions in concept learning dynamics persist and what factors influence their occurrence.

### Open Question 2
- Question: What are the computational and theoretical limits of the concept space framework in terms of scalability to high-dimensional concept spaces and complex data distributions?
- Basis in paper: [inferred] The paper demonstrates the framework on synthetic data with 2-3 concepts and provides preliminary results on CelebA with 2 concepts. The authors mention that real-world concepts are often hierarchical or relational, which would require more complex concept space representations.
- Why unresolved: The framework's effectiveness in capturing and analyzing learning dynamics in high-dimensional spaces with complex concept interactions remains unexplored. The paper doesn't address computational complexity or theoretical guarantees for the framework's applicability to more complex scenarios.
- What evidence would resolve it: Formal analysis of computational complexity for concept space construction and trajectory analysis, experiments on datasets with many concepts (e.g., ConceptNet or large-scale multimodal datasets), and development of efficient algorithms for high-dimensional concept space analysis.

### Open Question 3
- Question: How does the concept signal framework relate to information-theoretic measures of concept importance and what are the implications for model training and architecture design?
- Basis in paper: [explicit] The paper introduces concept signal as a measure of concept importance but doesn't explore its relationship to established information-theoretic measures like mutual information, information bottleneck, or Fisher information. The authors suggest concept signal could inform better training schemes.
- Why unresolved: The paper doesn't establish formal connections between concept signal and information-theoretic quantities that could provide theoretical grounding for the framework. The implications for model architecture design (e.g., attention mechanisms, feature hierarchies) that could optimize concept learning are unexplored.
- What evidence would resolve it: Mathematical derivation connecting concept signal to information-theoretic measures, experiments comparing concept learning dynamics under different training objectives (e.g., information bottleneck vs. standard loss), and architectural modifications that explicitly optimize concept signal during training.

## Limitations
- The concept signal mechanism assumes factorized learning objectives, but real-world generative models often have coupled losses that may decouple learning speed from concept signal strength
- The sudden emergence of hidden capabilities relies on linear or near-linear concept representations, which may not hold for more complex, entangled concepts
- Underspecification effects are demonstrated in synthetic data, but generalization to real-world datasets with more complex correlation structures remains uncertain

## Confidence
- Concept signal directly controls learning speed: **Medium** - Strong evidence from synthetic data but limited real-world validation
- Hidden capabilities emerge suddenly through latent manipulation: **Low** - Novel phenomenon with limited external validation beyond synthetic examples
- Underspecification delays OOD generalization: **Medium** - Supported by synthetic experiments but needs broader dataset validation

## Next Checks
1. **Test non-linear concept representations**: Create synthetic datasets with intentionally non-linear concept relationships and measure if hidden capabilities still emerge suddenly or if transitions become gradual
2. **Validate on real-world correlated concepts**: Apply the framework to datasets like CLEVR with known concept correlations to verify that underspecification effects persist beyond synthetic 2D objects
3. **Examine coupling effects**: Design experiments with explicitly coupled loss functions (e.g., joint KL divergence terms) to test whether learning speed remains proportional to concept signal under non-factorized objectives