---
ver: rpa2
title: 'ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation
  with an Astronomy Case Study'
arxiv_id: '2412.14436'
source_url: https://arxiv.org/abs/2412.14436
tags:
- astronomy
- dataset
- domain-specific
- these
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of creating high-quality, domain-specific
  datasets for training specialized large language models, particularly in fields
  like astronomy that require deep expertise. ORBIT introduces a two-stage filtering
  pipeline that combines embedding-based similarity matching with BERT-based educational
  value assessment to efficiently curate massive datasets from noisy web sources.
---

# ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation with an Astronomy Case Study
## Quick Facts
- arXiv ID: 2412.14436
- Source URL: https://arxiv.org/abs/2412.14436
- Authors: Eric Modesitt; Ke Yang; Spencer Hulsey; Chengxiang Zhai; Volodymyr Kindratenko
- Reference count: 40
- Primary result: ORBIT filtered 10B tokens from FineWeb-Edu, improving astronomy LLM performance from 69% to 76% MMLU accuracy

## Executive Summary
ORBIT addresses the critical challenge of creating high-quality, domain-specific datasets for training specialized large language models, particularly in fields like astronomy that require deep expertise. The method introduces a two-stage filtering pipeline that combines embedding-based similarity matching with BERT-based educational value assessment to efficiently curate massive datasets from noisy web sources. Applying ORBIT to astronomy, law, and medicine, the authors refined FineWeb-Edu into a 10B-token astronomy dataset, achieving significant quality improvements over unfiltered baselines. Fine-tuning LLaMA-3-8B on this data improved MMLU astronomy accuracy from 69% to 76% and outperformed state-of-the-art models like AstroLLaMA on astronomy benchmarks, with GPT-4o preferring Orbit-LLaMA in 73% of cases.

## Method Summary
ORBIT employs a two-stage filtering pipeline that first uses embedding-based similarity matching to identify content relevant to the target domain, then applies BERT-based educational value assessment to evaluate content quality and relevance. The method starts by creating a seed dataset from trusted sources and uses it to build domain-specific embedding models. These models score candidate documents for relevance, followed by a BERT-based classifier that evaluates educational value. The pipeline was applied to curate a 10B-token astronomy dataset from FineWeb-Edu, demonstrating significant improvements in domain-specific LLM performance across multiple benchmarks.

## Key Results
- Improved MMLU astronomy accuracy from 69% to 76% after fine-tuning LLaMA-3-8B on ORBIT-curated dataset
- GPT-4o preferred Orbit-LLaMA over baselines in 73% of cases on astronomy tasks
- Outperformed AstroLLaMA and other state-of-the-art models on astronomy benchmarks
- Validated generalizability across law and medicine domains with similar quality improvements

## Why This Works (Mechanism)
ORBIT works by combining semantic similarity matching with educational quality assessment to filter noisy web data. The embedding-based similarity stage identifies domain-relevant content using cosine similarity scores between document embeddings and domain-specific seed embeddings. The BERT-based educational assessment then evaluates whether identified content has genuine educational value rather than being superficial or irrelevant. This two-stage approach efficiently removes noise while preserving high-quality educational content, addressing the challenge that domain experts are scarce and expensive for manual dataset curation.

## Foundational Learning
**Embedding-based similarity matching**: Uses semantic embeddings to identify domain-relevant content from massive web corpora. Needed because keyword matching misses nuanced domain content. Quick check: Verify embedding model captures domain-specific terminology and concepts.

**BERT-based educational assessment**: Fine-tuned BERT model evaluates content for educational value beyond mere relevance. Needed because relevance alone doesn't ensure quality learning material. Quick check: Test classifier on known good/bad educational examples.

**Two-stage filtering pipeline**: Sequential application of relevance matching followed by quality assessment. Needed to balance computational efficiency with curation quality. Quick check: Measure precision/recall at each stage.

**Domain expertise verification**: Ensures curators have appropriate credentials and publication history. Needed because domain adaptation requires deep subject matter knowledge. Quick check: Cross-validate expertise claims with independent sources.

## Architecture Onboarding
**Component map**: Web corpus -> Embedding similarity filter -> BERT educational filter -> Curated dataset -> LLM fine-tuning -> Domain-specific model

**Critical path**: The two-stage filtering pipeline is the core innovation - embedding similarity scoring followed by BERT educational assessment. This path determines both quality and computational efficiency of the curation process.

**Design tradeoffs**: Speed vs quality in filtering stages, computational cost vs curation accuracy, domain specificity vs generalizability. The method prioritizes quality over speed, accepting higher computational costs for better dataset curation.

**Failure signatures**: Low precision in embedding similarity (including irrelevant content), BERT classifier overfitting to training data, expertise verification missing qualified contributors, or computational costs exceeding dataset value.

**First experiments**: 1) Test embedding similarity on small seed dataset to verify domain capture, 2) Validate BERT educational classifier on labeled examples, 3) Run end-to-end pipeline on 1% of corpus to measure computational requirements.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Evaluation relies heavily on synthetic benchmarks (MMLU, GalaxyZoo, AstroDocs) which may not capture real-world astronomy research tasks
- GPT-4o preference study uses single human annotator, limiting statistical confidence
- Astronomy expertise verification depends on author credentials and publication history, which may not fully capture practical domain knowledge
- Two-stage filtering pipeline may inadvertently remove nuanced but valuable content that doesn't fit clean keyword patterns

## Confidence
- **High confidence**: Technical implementation of filtering pipeline, embedding-based similarity matching, and BERT-based educational assessment are well-documented and reproducible
- **Medium confidence**: Domain adaptation results showing MMLU improvement from 69% to 76% are methodologically sound but may not generalize to other specialized domains
- **Low confidence**: Claims about cost-effectiveness and scalability are based on single 10B-token experiment without comparison to alternative approaches

## Next Checks
1. Replicate ORBIT on a domain with different linguistic characteristics (e.g., medical or legal texts) to test generalizability beyond astronomy's technical vocabulary and structured format

2. Conduct multi-annotator evaluation of GPT-4o preferences using statistical significance testing to validate qualitative performance claims

3. Test ORBIT's performance with progressively larger datasets (100B+ tokens) to assess whether quality improvements scale linearly or plateau, and compare computational costs against baseline random sampling approaches