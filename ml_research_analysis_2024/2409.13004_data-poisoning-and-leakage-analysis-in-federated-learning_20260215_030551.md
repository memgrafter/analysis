---
ver: rpa2
title: Data Poisoning and Leakage Analysis in Federated Learning
arxiv_id: '2409.13004'
source_url: https://arxiv.org/abs/2409.13004
tags:
- data
- training
- learning
- federated
- poisoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This chapter analyzes two primary threats in federated learning:
  training data privacy intrusion and training data poisoning. The authors demonstrate
  that attackers can reconstruct private training data by analyzing gradient updates
  shared during federated learning, and can manipulate training data to cause targeted
  misclassification in the trained model.'
---

# Data Poisoning and Leakage Analysis in Federated Learning

## Quick Facts
- arXiv ID: 2409.13004
- Source URL: https://arxiv.org/abs/2409.13004
- Reference count: 40
- Primary result: Dynamic model perturbation strategies can effectively protect against both training data leakage and poisoning attacks in federated learning while maintaining model accuracy

## Executive Summary
This chapter analyzes two critical threats in federated learning: training data privacy intrusion through gradient leakage and training data poisoning attacks. The authors demonstrate that attackers can reconstruct private training data by analyzing gradient updates shared during federated learning, and can manipulate training data to cause targeted misclassification in the trained model. To address these threats, the authors propose dynamic model perturbation strategies, including dynamic differential privacy noise that adapts to gradient patterns during training, showing improved resilience to gradient leakage attacks while maintaining model accuracy.

## Method Summary
The authors implement federated learning with local SGD on multiple datasets (MNIST, Fashion-MNIST, CIFAR10, LFW) using standard CNN architectures. They evaluate training data leakage attacks using reconstruction algorithms on gradients at different training rounds, measuring reconstruction quality (MSE) and privacy spending (ε) under fixed vs dynamic differential privacy noise. For poisoning mitigation, they implement PCA-based detection of poisoned gradients and compare poisoning resilience between fixed noise injection and dynamic differential privacy noise with decaying scale, measuring attack success rates and model accuracy on victim/non-victim classes.

## Key Results
- Training data leakage is most effective at early training rounds when gradient magnitudes are larger
- Dynamic differential privacy noise injection outperforms constant noise injection for both privacy protection and poisoning resilience
- Data poisoning attacks are more effective in later training rounds due to catastrophic forgetting characteristics
- The proposed dynamic perturbation strategies effectively balance privacy protection, poisoning resilience, and model performance across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data leakage in federated learning is most effective during early training rounds when gradient magnitudes are larger.
- Mechanism: As federated learning progresses, gradients naturally shrink toward zero due to convergence. Early gradients contain more information about the training data, making reconstruction attacks more successful.
- Core assumption: The magnitude of gradients correlates with information content available for reconstruction attacks.
- Evidence anchors: [abstract] "training data leakage is most effective at early training rounds"; [section] "Our second observation is that the stolen gradients at earlier training rounds of federated learning are more informative"

### Mechanism 2
- Claim: Dynamic differential privacy noise injection outperforms constant noise injection for both privacy protection and poisoning resilience.
- Mechanism: Early in training, gradients are larger and require more noise for privacy protection. Later, smaller gradients need less noise to avoid harming model accuracy. Dynamic adjustment balances these competing needs.
- Core assumption: Gradient magnitudes follow a predictable decay pattern that can be leveraged for adaptive noise injection.
- Evidence anchors: [abstract] "dynamic model perturbation strategies outperform constant noise injection"; [section] "we demonstrated the feasibility of best balancing privacy protection, poisoning resilience, and model performance with dynamic model perturbation"

### Mechanism 3
- Claim: Data poisoning attacks are more effective when performed in later training rounds due to catastrophic forgetting characteristics.
- Mechanism: Later-stage poisoning exploits the model's reduced ability to "forget" earlier training patterns, making the poisoning effects more persistent and harder to mitigate through subsequent clean training.
- Core assumption: Deep learning models exhibit catastrophic forgetting that can be exploited for persistent poisoning effects.
- Evidence anchors: [section] "the poisoning attacks are more effective at the later stage of training compared to only performing poisoning in the early stage"; [section] "We attribute the phenomenon to the catastrophic forgetting characteristics of deep learning models"

## Foundational Learning

- Concept: Federated learning architecture and gradient-based optimization
  - Why needed here: Understanding how gradients are computed, shared, and aggregated is fundamental to analyzing both privacy leakage and poisoning attacks
  - Quick check question: How do FedSGD and FedAvg differ in their gradient aggregation approaches?

- Concept: Differential privacy fundamentals and sensitivity analysis
  - Why needed here: The chapter heavily relies on differential privacy noise injection for both privacy protection and poisoning mitigation
  - Quick check question: What is the relationship between clipping bound, sensitivity, and noise scale in differential privacy?

- Concept: Backpropagation and gradient computation mechanics
  - Why needed here: Training data leakage attacks work by reconstructing private data from gradients, requiring understanding of how gradients encode information about training examples
  - Quick check question: How does the gradient of a single training example differ from the aggregated gradient of a batch?

## Architecture Onboarding

- Component map: Client-side local training -> Gradient computation -> Optional noise injection -> Server-side gradient aggregation -> Model update -> Repeat until convergence

- Critical path: 1. Clients receive global model 2. Clients compute gradients on local data 3. Clients optionally apply noise/sanitization 4. Clients send gradients to server 5. Server aggregates gradients (with outlier detection) 6. Server updates global model 7. Repeat until convergence

- Design tradeoffs: Privacy vs accuracy (more noise protects privacy but harms accuracy); Defense strength vs computational overhead (stronger defenses may slow training); Early vs late training protection (different strategies needed at different training phases)

- Failure signatures: High MSE between reconstructed and original data indicates successful leakage attack; Sudden accuracy drops on specific classes suggest poisoning attacks; Model divergence or slow convergence may indicate excessive noise injection

- First 3 experiments: 1. Baseline federated learning without defenses on MNIST/Fashion-MNIST 2. Training data leakage attack on early vs late training rounds 3. Poisoning attack effectiveness at different percentages of malicious clients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of training data poisoning attacks vary with the percentage of malicious clients in federated learning systems?
- Basis in paper: [explicit] The paper states "it is critical and yet challenging to determine the proper amount of model perturbation to use at different rounds of federated learning" and mentions that model perturbation methods for poisoning mitigation must assume "that the percentage of malicious clients is small [ 45, 55]"
- Why unresolved: The paper acknowledges this is a critical challenge but does not provide specific quantitative thresholds or analysis of how poisoning effectiveness scales with different percentages of malicious clients beyond noting that effects are more pronounced with larger percentages.
- What evidence would resolve it: Empirical studies measuring poisoning effectiveness across varying percentages of malicious clients (e.g., 5%, 10%, 25%, 50%) with different datasets and model architectures would clarify this relationship.

### Open Question 2
- Question: What are the optimal dynamic differential privacy parameters for simultaneously maximizing privacy protection and model performance in federated learning?
- Basis in paper: [explicit] The paper discusses dynamic differential privacy noise but notes "determining noise scale .σtwill need to take the following three factors into consideration" without providing a complete optimization framework.
- Why unresolved: The paper demonstrates the concept of dynamic DP noise but does not provide a comprehensive framework for determining optimal parameters across different federated learning scenarios.
- What evidence would resolve it: Comparative studies evaluating different dynamic DP parameter configurations across multiple datasets, model architectures, and federated learning settings would establish best practices.

### Open Question 3
- Question: How can federated learning systems effectively balance the trade-off between model accuracy and resilience to both training data leakage and poisoning attacks simultaneously?
- Basis in paper: [explicit] The paper states "We argue that the security protection techniques for federated learning should bear the above analysis and observations into consideration when determining the right amount of noises to be used by the model perturbation" and discusses the challenge of balancing these competing objectives.
- Why unresolved: The paper demonstrates that dynamic model perturbation can address both threats but does not provide a comprehensive framework for balancing these trade-offs in practice.
- What evidence would resolve it: Systematic evaluation of different model perturbation strategies across multiple threat scenarios, datasets, and performance metrics would establish guidelines for optimal trade-off decisions.

## Limitations

- The dynamic differential privacy mechanism lacks precise implementation details, particularly regarding the decay function parameters and sensitivity calculations
- The empirical evidence for gradient reconstruction timing advantages is limited to synthetic gradient leakage experiments without real-world validation
- The catastrophic forgetting mechanism linking poisoning effectiveness to training stage remains theoretically grounded but lacks comprehensive empirical validation across diverse model architectures

## Confidence

- High Confidence: The existence of gradient leakage vulnerabilities and basic poisoning attack mechanisms in federated learning
- Medium Confidence: The effectiveness of dynamic noise injection compared to constant noise approaches, based on controlled experimental results
- Low Confidence: The catastrophic forgetting explanation for timing-dependent poisoning effectiveness, due to limited theoretical and empirical support

## Next Checks

1. **Timing Analysis Replication**: Reproduce gradient reconstruction experiments across multiple training stages with varying gradient clipping thresholds to validate the timing advantage hypothesis.

2. **Cross-Architecture Poisoning Study**: Test poisoning effectiveness timing across different neural network architectures (CNNs, Transformers) to determine if catastrophic forgetting is the primary mechanism.

3. **Dynamic Noise Parameter Sensitivity**: Systematically vary dynamic noise decay parameters to establish robustness boundaries and identify optimal parameter ranges for different dataset characteristics.