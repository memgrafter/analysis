---
ver: rpa2
title: 'Wolf2Pack: The AutoFusion Framework for Dynamic Parameter Fusion'
arxiv_id: '2410.05746'
source_url: https://arxiv.org/abs/2410.05746
tags:
- fusion
- autofusion
- permutation
- parameter
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoFusion addresses the problem of fusing model parameters trained
  on different tasks without shared pre-trained checkpoints. The core idea is to learn
  parameter permutations via an end-to-end, unsupervised approach that aligns neurons
  with similar functions and preserves diversity for multi-task capability.
---

# Wolf2Pack: The AutoFusion Framework for Dynamic Parameter Fusion

## Quick Facts
- arXiv ID: 2410.05746
- Source URL: https://arxiv.org/abs/2410.05746
- Authors: Bowen Tian; Songning Lai; Yutao Yue
- Reference count: 40
- Primary result: AutoFusion achieves up to 75.97% average accuracy on MNIST, Fashion-MNIST, and KMNIST through dynamic parameter fusion without shared pre-trained checkpoints

## Executive Summary
AutoFusion addresses the challenge of fusing model parameters trained on different tasks without shared pre-trained checkpoints. The framework learns parameter permutations via an end-to-end, unsupervised approach that aligns neurons with similar functions while preserving diversity for multi-task capability. Using a differentiable Sinkhorn operator to approximate permutation matrices, AutoFusion optimizes through two losses: Lalign for aligning similar parameters and Lretain for preserving task-specific features via pseudo-label supervision. The method significantly outperforms existing parameter fusion approaches on multiple datasets and architectures.

## Method Summary
AutoFusion fuses two models with identical architecture but trained on different tasks by learning parameter permutations without requiring shared pre-trained checkpoints. The core mechanism uses a differentiable Sinkhorn operator to convert discrete permutation matrices into a form suitable for gradient descent optimization. Two loss functions guide the learning process: Lalign minimizes the difference between aligned parameters to preserve shared representations, while Lretain uses pseudo-label supervision to ensure retained diversity for multi-task performance. The framework was evaluated on MNIST, Fashion-MNIST, and CIFAR-10 datasets using MLP and CNN architectures.

## Key Results
- On MNIST with CNN architecture, AutoFusion achieved 65.62% joint accuracy versus ZipIt's 52.00%, representing a 13.23% improvement
- AutoFusion demonstrated strong performance in fusing models trained on datasets with different distributions, achieving up to 75.97% average accuracy across MNIST, Fashion-MNIST, and KMNIST
- The method showed consistent improvement over baselines including Weight Interpolation, Git Re-Basin, and ZipIt across multiple experimental conditions

## Why This Works (Mechanism)

### Mechanism 1
The differentiable Sinkhorn operator allows gradient-based learning of parameter permutations without discrete optimization. AutoFusion approximates the permutation matrix using the Sinkhorn operator, which iteratively normalizes rows and columns of an exponentiated similarity matrix until convergence. This produces a soft permutation matrix that is differentiable and can be optimized via gradient descent. The entropy-regularized optimal transport problem has a smooth solution that can be approximated by Sinkhorn iterations.

### Mechanism 2
The two-loss optimization (Lalign + Lretain) enables simultaneous alignment of shared functionality and preservation of task-specific features. Lalign minimizes the difference between aligned parameters to preserve shared representations, while Lretain uses pseudo-label supervision to ensure retained diversity for multi-task performance. These losses are combined and normalized to prevent domination by either objective. The mutual constraints between the two distinct optimization objectives facilitate learning more valuable permutations.

### Mechanism 3
End-to-end learning of permutations without shared pre-trained checkpoints enables multi-task model fusion in the absence of common initialization. By learning permutations directly from data rather than relying on pre-trained similarity, AutoFusion can align parameters that perform similar functions across models trained on disjoint tasks, enabling effective fusion without requiring shared initialization. Parameter permutation invariance in neural networks means that functionally similar neurons may not be in the same positions across differently trained models, and this can be learned without pre-training.

## Foundational Learning

- Concept: Permutation invariance in neural networks
  - Why needed here: Understanding that neuron order doesn't affect model output is crucial for grasping why parameter permutation can be used for model fusion
  - Quick check question: Why can we permute neurons within a neural network layer without changing the network's output?

- Concept: Optimal transport and the Sinkhorn algorithm
  - Why needed here: The Sinkhorn operator is the mathematical foundation that enables differentiable permutation learning
  - Quick check question: What problem does the Sinkhorn algorithm solve, and how does entropy regularization help make it differentiable?

- Concept: Multi-task learning and catastrophic forgetting
  - Why needed here: Understanding how models can learn multiple tasks without forgetting previous ones is essential for grasping the motivation behind parameter fusion
  - Quick check question: What is catastrophic forgetting, and why is it a challenge in multi-task learning?

## Architecture Onboarding

- Component map: Two trained models -> Similarity matrix computation -> Sinkhorn iteration -> Permutation matrix update -> Parameter fusion -> Evaluation
- Critical path: Data sampling → similarity matrix computation → Sinkhorn iteration → permutation matrix update → parameter fusion → evaluation
- Design tradeoffs:
  - Memory vs accuracy: Using soft permutations vs hard discrete permutations
  - Labeled vs unlabeled data: Pseudo-label approach vs requiring true labels
  - Complexity vs performance: Sinkhorn iterations vs simpler alignment methods
- Failure signatures:
  - Poor performance on either task suggests misalignment or over-alignment
  - High variance across runs indicates instability in permutation learning
  - Low joint accuracy suggests failure to preserve diversity
- First 3 experiments:
  1. Test on MNIST with MLP architecture using 2-class vs 8-class split to verify basic functionality
  2. Compare with Weight Interpolation baseline on same data to establish improvement
  3. Vary the number of Sinkhorn iterations to find optimal tradeoff between accuracy and computation

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of AutoFusion scale with the depth of the neural network architecture, and is there an optimal number of layers beyond which the method becomes less effective? The paper mentions an exploration showing accuracy improves with depth up to 6 layers but decreases at 8 layers due to overfitting, but experiments only tested up to 8 hidden layers.

### Open Question 2
Can AutoFusion be extended to fuse models with different architectures or shared pre-trained checkpoints, and what modifications would be necessary? The paper explicitly states that AutoFusion is designed for models with the same architecture and without shared pre-trained checkpoints, acknowledging this as a limitation.

### Open Question 3
How sensitive is AutoFusion to the choice of hyperparameters such as the pseudo-label threshold (ζ) and the number of Sinkhorn iterations, and what are the best practices for tuning them? While the paper provides some guidance through ablation studies, it does not establish a systematic approach for hyperparameter selection across diverse datasets and model architectures.

## Limitations

- Limited empirical scope: Experiments only cover MNIST, Fashion-MNIST, and CIFAR-10 datasets with MLP and CNN architectures
- Architecture constraint: Method requires identical model architectures for fusion
- Pseudo-label reliability: Lretain loss depends on model confidence for pseudo-label generation
- Computational overhead: Sinkhorn iterations add computational cost during fusion

## Confidence

- **High confidence**: The core mechanism of using differentiable Sinkhorn operators for parameter permutation learning is mathematically sound
- **Medium confidence**: The two-loss optimization approach (Lalign + Lretain) is novel and shows strong empirical results
- **Low confidence**: Claims about performance on datasets with different distributions need more rigorous validation

## Next Checks

1. Test AutoFusion on models with different but related architectures (e.g., ResNet vs DenseNet) to evaluate the method's robustness beyond identical architectures
2. Apply AutoFusion to fuse models trained on genuinely different tasks (e.g., object detection and language modeling) rather than different subsets of similar datasets
3. Evaluate the method's performance and computational overhead when fusing models with significantly more parameters (e.g., modern deep networks with 10M+ parameters) to assess practical deployment viability