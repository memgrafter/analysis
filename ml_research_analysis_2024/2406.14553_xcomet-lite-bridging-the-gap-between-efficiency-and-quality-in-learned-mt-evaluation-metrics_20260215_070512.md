---
ver: rpa2
title: 'xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT
  Evaluation Metrics'
arxiv_id: '2406.14553'
source_url: https://arxiv.org/abs/2406.14553
tags:
- quality
- quantization
- metrics
- pruning
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates compression techniques to create efficient
  alternatives to large-scale learned MT evaluation metrics like xCOMET, which has
  up to 10.7B parameters. Three methods are explored: knowledge distillation, quantization,
  and pruning.'
---

# xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics

## Quick Facts
- arXiv ID: 2406.14553
- Source URL: https://arxiv.org/abs/2406.14553
- Authors: Daniil Larionov, Mikhail Seleznyov, Vasiliy Viskov, Alexander Panchenko, Steffen Eger
- Reference count: 40
- Primary result: xCOMET-lite achieves state-of-the-art quality among metrics with <600M parameters while using 50% fewer parameters than previous best models

## Executive Summary
This paper addresses the efficiency bottleneck of large-scale learned MT evaluation metrics like xCOMET, which can have up to 10.7B parameters. The authors propose xCOMET-lite, a compressed version that retains 92.1% of xCOMET-XXL's quality while using only 2.6% of its parameters. Through a novel data collection pipeline creating 14M training examples, the study demonstrates that knowledge distillation is the most effective compression technique, with quantization and pruning providing complementary benefits. The resulting xCOMET-lite outperforms smaller metrics like COMET-22 and BLEURT-20 by 6.4% on the WMT22 metrics challenge dataset.

## Method Summary
The study explores three compression techniques for MT evaluation metrics: knowledge distillation, quantization, and layer pruning. A novel data collection pipeline generates 14M pseudo-labeled examples using the xCOMET-XXL teacher model. The distilled student model uses mDeBERTa v3 base instead of XLM-RoBERTa-XL. Quantization experiments use GPTQ, LLM.int8(), and QLoRA methods. Layer pruning removes up to 25% of layers with parameter-efficient fine-tuning. The methods are evaluated on WMT22 metrics challenge data, measuring Kendall-τ correlation, inference speed, and memory consumption.

## Key Results
- xCOMET-lite retains 92.1% of xCOMET-XXL quality with only 278M parameters (2.6% of original)
- Quantization with GPTQ 3-bit reduces memory usage by 54% with no quality loss
- Pruning up to 25% of layers improves speed with minimal quality impact
- xCOMET-lite outperforms smaller metrics like COMET-22 and BLEURT-20 by 6.4% on WMT22 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation transfers evaluation capability from xCOMET-XXL to a smaller model while preserving most quality.
- Mechanism: The large teacher model generates pseudo-labels (segment-level scores and error spans) for a large dataset of translation examples. The smaller student model is trained on these pseudo-labels, learning the evaluation behavior without needing internal states.
- Core assumption: Teacher outputs contain sufficient information to train a capable student model.
- Evidence anchors: Abstract states xCOMET-lite retains 92.1% quality with 2.6% parameters; section 3 describes pseudo-label generation.
- Break condition: If teacher outputs are too noisy or dataset lacks diversity, distilled model may fail to capture evaluation capability.

### Mechanism 2
- Claim: Quantization reduces memory consumption and improves speed with minimal quality loss.
- Mechanism: Quantization techniques reduce precision of model parameters and activations, allowing models to fit in less memory and potentially run faster.
- Core assumption: Model performance is robust to reduced precision for MT evaluation calculations.
- Evidence anchors: Abstract mentions compression up to three times with no quality degradation; section 4.2 describes GPTQ quantization experiments.
- Break condition: If quantization is too aggressive (e.g., 2-bit) or model contains outlier features sensitive to precision reduction, quality degrades significantly.

### Mechanism 3
- Claim: Pruning layers can improve inference speed and reduce memory usage with minimal quality impact.
- Mechanism: The pruning technique removes least significant layers based on heuristic that deeper layers are less critical, followed by parameter-efficient fine-tuning.
- Core assumption: Later layers are less critical for maintaining evaluation quality, and fine-tuning compensates for information loss.
- Evidence anchors: Abstract states pruning up to 25% improves speed with minimal quality impact; section 3 describes layer pruning technique.
- Break condition: If too many layers are pruned or fine-tuning is insufficient, quality degrades substantially.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Understanding how student learns from teacher's outputs without internal states is crucial for implementing distillation approach.
  - Quick check question: What information does teacher provide to student during distillation, and how is this information used to train student?

- Concept: Quantization Techniques
  - Why needed here: Different quantization methods have different trade-offs between memory reduction, speed improvement, and quality preservation.
  - Quick check question: How do GPTQ, LLM.int8(), and QLoRA differ in reducing precision, and what are implications for model quality and efficiency?

- Concept: Model Pruning
  - Why needed here: Pruning can significantly reduce model size and improve speed, but risks quality degradation.
  - Quick check question: What heuristic selects layers for pruning, and how does fine-tuning after pruning mitigate quality loss?

## Architecture Onboarding

- Component map: Input text triplets (source, hypothesis, reference) -> Encoder (XLM-RoBERTa-XL/XXL or mDeBERTa) -> Regression head (segment-level quality scoring) -> Tagging head (token-level error span detection) -> Output scores and error spans
- Critical path: The critical path for MT evaluation is: input text triplets → encoder → regression and tagging heads → output scores and error spans. Efficiency improvements primarily affect encoder and heads.
- Design tradeoffs: Main tradeoff is between model efficiency (memory usage, speed) and evaluation quality (correlation with human judgment). Distillation offers best quality-efficiency tradeoff.
- Failure signatures: Quality degradation, memory leaks, or unexpected slowdowns may indicate issues with compression techniques (aggressive quantization, insufficient fine-tuning after pruning, poor distillation dataset quality).
- First 3 experiments:
  1. Apply GPTQ 8-bit quantization to xCOMET-XL and measure impact on quality (Kendall correlation) and memory usage.
  2. Prune 8 layers from xCOMET-XL, fine-tune the model, and evaluate impact on quality and speed.
  3. Train xCOMET-lite on the distillation dataset and compare its quality to original xCOMET models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of compression techniques vary across different translation language pairs, particularly for low-resource languages?
- Basis in paper: [explicit] Research focuses on high-resource languages and performance on low-resource languages remains unexplored.
- Why unresolved: Paper does not conduct experiments on low-resource language pairs.
- What evidence would resolve it: Conducting experiments with compression techniques on low-resource language pairs and comparing results to high-resource languages.

### Open Question 2
- Question: What is impact of using distilled models like xCOMET-lite in high-stakes applications such as filtering datasets or evaluating machine translation systems in sensitive domains?
- Basis in paper: [explicit] Paper mentions potential risks of using distilled models in high-stakes applications due to slightly lower accuracy.
- Why unresolved: Paper does not provide empirical evidence or specific examples of consequences in high-stakes scenarios.
- What evidence would resolve it: Conducting case studies or experiments in sensitive domains to evaluate performance and potential risks.

### Open Question 3
- Question: How do compressed models perform in terms of robustness to adversarial attacks or input perturbations?
- Basis in paper: [inferred] Paper focuses on efficiency and quality trade-offs but does not explicitly address robustness to adversarial attacks or input perturbations.
- Why unresolved: Robustness to adversarial attacks is critical but not investigated in the paper.
- What evidence would resolve it: Evaluating compressed models' performance on adversarial attack benchmarks or testing resilience to input perturbations.

## Limitations
- Distillation approach requires access to teacher model for generating pseudo-labels, which may not be feasible in all deployment scenarios
- Layer pruning shows limited compatibility with quantization methods, with incompatibility only explored with GPTQ quantization
- 14M distillation dataset, while large, may not fully capture diversity of translation quality across all language pairs and domains

## Confidence

- High Confidence: Distillation results showing xCOMET-lite retaining 92.1% quality with 2.6% parameters are well-supported by experimental evidence and align with established knowledge distillation principles.
- Medium Confidence: Quantization results showing 54% memory reduction with 3-bit GPTQ maintaining quality are supported by experiments, but interaction between quantization and pruning requires further investigation.
- Medium Confidence: Pruning results showing speed improvements with minimal quality loss are empirically demonstrated, but generalization across different model architectures remains untested.

## Next Checks
1. Test compatibility of layer pruning with QLoRA and LLM.int8() quantization methods to determine if incompatibility with GPTQ is universal or method-specific.
2. Evaluate xCOMET-lite's performance on low-resource language pairs and specialized domains not represented in WMT22 metrics challenge dataset.
3. Conduct ablation studies on distillation dataset size to determine minimum number of examples needed to achieve comparable quality to full 14M dataset.