---
ver: rpa2
title: Eliciting Uncertainty in Chain-of-Thought to Mitigate Bias against Forecasting
  Harmful User Behaviors
arxiv_id: '2410.14744'
source_url: https://arxiv.org/abs/2410.14744
tags:
- bias
- language
- uncertainty
- forecasting
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the use of uncertainty estimation to mitigate
  biases in large language models (LLMs) for conversation forecasting tasks, particularly
  for predicting harmful behaviors in social media moderation. The authors investigate
  three key questions: how forecasting accuracy changes when models represent uncertainty,
  how bias changes with uncertainty representation, and how uncertainty can be used
  to reduce bias without extensive training data.'
---

# Eliciting Uncertainty in Chain-of-Thought to Mitigate Bias against Forecasting Harmful User Behaviors

## Quick Facts
- arXiv ID: 2410.14744
- Source URL: https://arxiv.org/abs/2410.14744
- Reference count: 11
- Authors: Anthony Sicilia; Malihe Alikhani
- One-line primary result: Uncertainty elicitation in chain-of-thought reasoning reduces bias in conversation forecasting tasks while maintaining or improving accuracy

## Executive Summary
This paper investigates how eliciting uncertainty in language models affects their ability to forecast harmful behaviors in social media conversations. The authors find that asking models to express confidence on a Likert scale during chain-of-thought reasoning reduces statistical bias against predicting personal attacks, particularly for models with higher initial bias. They also demonstrate that post-hoc scaling of uncertainty estimates can further mitigate bias without requiring expensive fine-tuning. The study provides evidence that uncertainty-aware inference is a promising approach for improving fairness in conversation forecasting tasks.

## Method Summary
The authors test five open-source language models (Llama 3.1 8B/70B, Mistral 7B, Mixtral 8x22B, Qwen2 72B) on two conversation forecasting datasets using traditional chain-of-thought prompting versus an uncertainty-aware approach where models express confidence on a Likert scale. They evaluate forecasting accuracy, F1 score, and statistical bias (systematic error in predictions). The uncertainty-aware method forces models to consider likelihood of outcomes rather than making binary predictions. Additionally, they apply post-hoc scaling to uncertainty estimates to further mitigate bias without extensive training data.

## Key Results
- Uncertainty-aware inference improves forecasting accuracy for some models (particularly Llama 3.1 8B)
- Statistical bias against predicting harmful outcomes decreases when models represent uncertainty
- Post-hoc scaling of uncertainty estimates can further reduce bias without impacting accuracy
- Models with higher initial bias show greater bias reduction when uncertainty is considered
- Topic-specific analysis reveals varying levels of bias across different conversation themes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Eliciting uncertainty in chain-of-thought reasoning reduces bias against forecasting harmful outcomes
- Mechanism: The uncertainty elicitation prompt changes the model's reasoning process by forcing it to consider the likelihood of outcomes rather than making binary predictions, which helps overcome alignment-induced bias against predicting harmful events
- Core assumption: Language models have learned statistical patterns from human-generated text that can be leveraged when explicitly asked to reason about uncertainty
- Evidence anchors:
  - [abstract] "we ask three primary research questions: 1) how does LLM forecasting accuracy change when we ask models to represent their uncertainty; 2) how does LLM bias change when we ask models to represent their uncertainty"
  - [section] "We hypothesize language models may benefit from utilizing similar patterns of reasoning, having learned these (statistical) patterns from the human-generated text on which they are trained"
  - [corpus] Weak - the paper doesn't provide direct evidence about the training data composition that would support this mechanism
- Break condition: If the model has been heavily fine-tuned on instruction data that specifically avoids discussing harmful outcomes, the learned patterns may not exist to leverage

### Mechanism 2
- Claim: Post-hoc scaling of uncertainty estimates can further mitigate bias without impacting accuracy
- Mechanism: The scaling parameters (τ and β) systematically adjust the latent scores to correct for systematic under-prediction of personal attacks, effectively shifting the decision boundary
- Core assumption: The model's uncertainty estimates contain systematic biases that can be corrected with a simple transformation learned from a small validation set
- Evidence anchors:
  - [section] "Rather than data- and compute-expensive fine-tuning of model weights, we suggest post-hoc forecast scaling, which is a variant of Platt Scaling"
  - [section] "the parameter β acts to remove systematic biases from the latent score ˆZ in Eq. (1)"
  - [corpus] Weak - no corpus-level evidence about why this specific mathematical transformation would work across different datasets
- Break condition: If the bias is not systematic (i.e., varies unpredictably across examples), the scaling parameters cannot effectively correct it

### Mechanism 3
- Claim: Models with higher initial bias show greater bias reduction when uncertainty is considered
- Mechanism: The chain-of-thought reasoning triggered by uncertainty elicitation allows the model to access and correct for its own biases during the reasoning process
- Core assumption: The model has some internal representation of its uncertainty that can be accessed through prompting, and this representation contains information about the model's confidence in its predictions
- Evidence anchors:
  - [section] "The Llama 3.1 series are the only models that do not show any benefit" and "we find results are largely consistent with those reported for accuracy"
  - [section] "Reductions in bias generally correlate with improved accuracy (an apparent quadratic relationship)"
  - [corpus] Weak - the paper doesn't provide evidence about the relationship between model architecture and bias susceptibility
- Break condition: If the model's reasoning is not influenced by the uncertainty prompt (i.e., the prompt doesn't change the token generation process), no bias reduction will occur

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: The paper builds on this technique as the baseline for conversation forecasting and then modifies it to include uncertainty representation
  - Quick check question: What is the primary difference between traditional chain-of-thought prompting and the uncertainty-aware approach used in this paper?

- Concept: Statistical bias vs social bias
  - Why needed here: The paper distinguishes between statistical bias (systematic error in predictions) and social bias (prejudice against certain groups), which is crucial for understanding the research questions
  - Quick check question: How does the statistical bias measure E[ ˆO − O] differ from common measures of social bias in language models?

- Concept: Post-hoc calibration techniques
  - Why needed here: The paper uses Platt Scaling as a basis for its bias mitigation approach, which requires understanding how calibration works
  - Quick check question: What is the purpose of the parameter β in the post-hoc scaling equation, and how does it relate to bias correction?

## Architecture Onboarding

- Component map: Prompt template -> Language model inference -> Output parsing -> (Optional) Post-hoc scaling -> Evaluation metrics
- Critical path: Prompt → LM inference → Parse output → (Optional) Scaling → Evaluate metrics
- Design tradeoffs: Using uncertainty elicitation adds complexity to the prompt but may improve bias without requiring model retraining; post-hoc scaling requires a small validation set but avoids expensive fine-tuning
- Failure signatures: If uncertainty elicitation doesn't change the model's predictions, the prompt template may not be effective; if scaling doesn't reduce bias, the model's uncertainty estimates may not contain useful information for correction
- First 3 experiments:
  1. Compare traditional CoT vs uncertainty-aware CoT on a small validation set to verify the prompt modification works
  2. Test post-hoc scaling on the validation set to ensure the scaling parameters can be learned effectively
  3. Evaluate the full pipeline on held-out test data to measure bias reduction and accuracy impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does AI alignment fundamentally bias language models against predicting harmful outcomes, or are other factors (like pre-training data or fine-tuning methods) more significant?
- Basis in paper: [explicit] The paper states "we speculate this is a result of alignment mechanisms biasing language models against predicting harmful outcomes" but acknowledges this cannot be confirmed without access to training data.
- Why unresolved: The authors note they cannot definitively determine the root cause of observed biases due to lack of transparency in training data and methods, particularly for instruction-tuned models.
- What evidence would resolve it: Access to training data and methods for alignment mechanisms, along with experiments comparing aligned vs unaligned models on the same forecasting tasks.

### Open Question 2
- Question: How does the performance of uncertainty-aware forecasting change as model size increases beyond the tested range (8B-72B parameters)?
- Basis in paper: [inferred] The paper notes that Llama 3.1 8B and 70B models showed little to no benefit from uncertainty-aware prompting, suggesting a potential saturation effect at higher performance levels.
- Why unresolved: The study only tested models up to 72B parameters, leaving open whether even larger models would continue to show diminishing returns from uncertainty elicitation.
- What evidence would resolve it: Testing uncertainty-aware forecasting on frontier models (e.g., 400B+ parameter models) to determine if performance plateaus or if new patterns emerge.

### Open Question 3
- Question: How do uncertainty-aware forecasting methods perform on non-English datasets or across different cultural contexts?
- Basis in paper: [inferred] The study focuses exclusively on English-language datasets (Wikipedia and Reddit), raising questions about generalizability to other languages and cultural contexts where harmful behavior definitions may differ.
- Why unresolved: The research was limited to English-language social media moderation, and the paper does not address cross-cultural variations in conversation forecasting or harmful behavior definitions.
- What evidence would resolve it: Testing the same uncertainty-aware methods on multilingual datasets and in different cultural contexts to evaluate performance consistency and cultural sensitivity.

## Limitations

- Evaluation scope limited to 5 open-source models on two English-language datasets with only 200 conversations total
- Post-hoc scaling approach requires a small validation set, which may not be available in real-world deployment scenarios
- Statistical bias metric captures systematic prediction errors but may not fully represent social or demographic biases

## Confidence

- **High Confidence**: The core finding that uncertainty-aware inference improves forecasting accuracy for some models (particularly Llama 3.1 8B) is well-supported by the experimental results
- **Medium Confidence**: The claim that post-hoc scaling effectively mitigates bias without impacting accuracy is supported by the data but relies on assumptions about generalizability
- **Low Confidence**: The generalizability of these findings to other model architectures, datasets, or conversation forecasting tasks remains uncertain

## Next Checks

1. **Cross-dataset validation**: Test the uncertainty elicitation approach on additional conversation forecasting datasets with different conversation types and domains to assess generalizability beyond wiki and reddit data.

2. **Bias type analysis**: Conduct a detailed analysis of which specific types of bias (e.g., demographic, topical, linguistic) are most affected by uncertainty elicitation to understand the mechanism's scope and limitations.

3. **Real-world deployment simulation**: Evaluate the approach in a simulated real-world moderation scenario where models must process conversations in real-time and the validation set assumption is relaxed, to assess practical applicability.