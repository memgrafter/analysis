---
ver: rpa2
title: Rule Based Learning with Dynamic (Graph) Neural Networks
arxiv_id: '2406.09954'
source_url: https://arxiv.org/abs/2406.09954
tags:
- graph
- rule
- neural
- layers
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces rule-based layers, a new type of dynamic
  neural network layer that integrates expert knowledge or additional information
  into the learning process. The core idea is to define rule functions that dynamically
  arrange learnable parameters in weight matrices and bias vectors depending on the
  input samples.
---

# Rule Based Learning with Dynamic (Graph) Neural Networks

## Quick Facts
- arXiv ID: 2406.09954
- Source URL: https://arxiv.org/abs/2406.09954
- Reference count: 40
- Key outcome: Rule-based graph neural networks integrate expert knowledge through dynamic rule functions, achieving competitive performance with state-of-the-art graph classifiers while offering a framework for incorporating domain expertise

## Executive Summary
This paper introduces rule-based layers, a novel type of dynamic neural network layer that integrates expert knowledge or additional information into the learning process. The core innovation is defining rule functions that dynamically arrange learnable parameters in weight matrices and bias vectors depending on the input samples. The authors prove that rule-based layers generalize classical feed-forward layers such as fully connected and convolutional layers. As a concrete application, they present rule-based graph neural networks (RuleGNNs) that overcome limitations of ordinary graph neural networks. Experiments show that RuleGNNs are competitive with state-of-the-art graph classifiers using simple rules based on Weisfeiler-Leman labeling and pattern counting.

## Method Summary
The paper proposes RuleGNNs that combine Weisfeiler-Leman layers, pattern counting layers, and aggregation layers with tanh activation. The core mechanism uses rule functions to dynamically arrange learnable parameters based on node labels and pairwise properties. The architecture processes graphs through multiple message-passing layers where weights are determined by rules rather than being fixed. The models are trained using Adam optimizer with learning rate 0.05, cross entropy loss, batch size 128, and 50 epochs for real-world datasets or 200 epochs for synthetic datasets. Performance is evaluated using 10-fold cross validation with mean and standard deviation of test accuracy reported.

## Key Results
- RuleGNNs achieve competitive accuracy with state-of-the-art graph classifiers on standard benchmarks (NCI1, NCI109, Mutagenicity, DHFR, IMDB-BINARY, IMDB-MULTI)
- RuleGNNs successfully integrate expert knowledge on synthetic datasets, outperforming ordinary graph neural networks
- The framework generalizes classical feed-forward layers including fully connected and convolutional layers

## Why This Works (Mechanism)
Rule-based layers work by dynamically computing weight matrices based on input-dependent rules rather than using fixed weights. This allows the model to adapt its computation graph to the specific characteristics of each input sample. The rule functions map node pairs to learnable parameter indices, creating sparse weight matrices that capture structural relationships in the data. This dynamic computation enables integration of expert knowledge through carefully designed rules while maintaining the flexibility of neural networks.

## Foundational Learning
- **Graph Neural Networks**: Why needed - To understand the baseline architecture RuleGNNs extend; Quick check - Verify understanding of message passing and aggregation in GNNs
- **Weisfeiler-Leman Test**: Why needed - Forms the basis for many node labeling rules; Quick check - Confirm understanding of WL test and its role in graph isomorphism
- **Pattern Counting**: Why needed - Provides structural features for rule functions; Quick check - Understand how cycles, cliques, and other patterns are counted in graphs
- **Dynamic Weight Computation**: Why needed - Core mechanism of rule-based layers; Quick check - Verify how rules map node pairs to parameter indices
- **Graph Classification Metrics**: Why needed - To evaluate model performance correctly; Quick check - Confirm understanding of cross-validation and accuracy reporting
- **Sparse Matrix Operations**: Why needed - Rule-based layers create sparse weight matrices; Quick check - Verify efficiency of sparse operations in implementation

## Architecture Onboarding

Component Map: Graph -> Preprocessing (WL/Pattern Labels) -> RuleGNN Layers (WL + Pattern Counting + Aggregation) -> Classification

Critical Path: Preprocessing generates node labels and pairwise properties -> Rule functions map these to weight indices -> Dynamic weight matrices computed -> Message passing through layers -> Global pooling -> Classification

Design Tradeoffs: Fixed vs dynamic weights (flexibility vs computational overhead), sparse vs dense weight matrices (efficiency vs expressiveness), rule complexity (performance vs interpretability)

Failure Signatures: Poor performance on datasets requiring specific structural patterns, high preprocessing time for dense graphs, suboptimal results when rules don't match data characteristics

Three First Experiments:
1. Implement minimal RuleGNN with single WL layer on EvenOddRings to verify basic functionality
2. Compare RuleGNN with standard GNN on NCI1 dataset to validate competitive performance
3. Test RuleGNN with pattern counting only on synthetic dataset to evaluate expert knowledge integration

## Open Questions the Paper Calls Out
1. Can we automatically learn the best rule functions for a given graph classification task during training? The authors state "it is an interesting question if it is possible to automatically learn a rule that fits the data or captures the expert knowledge in the best way." This remains unresolved as the paper only demonstrates manually designed rule functions.

2. Can rule-based graph neural networks be extended to handle multi-dimensional node features and edge labels effectively? The authors note their "experimental results are restricted to graphs that do not have multidimensional node features" and "we have not considered edge features in our rules."

3. What is the trade-off between the performance of RuleGNNs and the sparsity of weight matrices for large and dense graphs? The authors mention that "for dense graphs the number of positions can be quadratic in the number of nodes" and acknowledge the need to find a trade-off.

## Limitations
- Exact implementation details of rule functions RW and Rb are not fully specified, creating uncertainty in faithful reproduction
- Specific pattern sets used for pattern counting layers are not completely detailed, affecting reproducibility on synthetic datasets
- Performance on real-world datasets may depend on optimal rule selection and hyperparameter tuning for specific datasets

## Confidence

High confidence: The theoretical framework of RuleGNNs and their ability to generalize classical layers is well-established and mathematically proven.

Medium confidence: The experimental results on real-world datasets are reproducible given the described methodology, but optimal performance may depend on precise rule function implementations.

Low confidence: The performance claims on synthetic datasets heavily depend on the specific expert knowledge encoded, which is not fully detailed in the paper.

## Next Checks
1. Implement a minimal RuleGNN prototype using the paper's framework and validate on a simple synthetic dataset (e.g., EvenOddRings) to confirm the basic functionality.

2. Compare the exact rule function implementations with the authors' code (if available) to ensure faithful reproduction of the RW and Rb mappings.

3. Conduct ablation studies on real-world datasets to determine the impact of different rule functions and aggregation strategies on performance.