---
ver: rpa2
title: Efficient Sample-Specific Encoder Perturbations
arxiv_id: '2405.01601'
source_url: https://arxiv.org/abs/2405.01601
tags:
- performance
- systems
- encoder
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to improve the performance
  of frozen encoder-decoder foundation models by perturbing the encoder outputs on
  a sample-by-sample basis using gradients from a small proxy network trained to predict
  a desired metric. The proxy network is trained to approximate a sequence-level evaluation
  metric, such as COMET for machine translation or word error rate for speech recognition.
---

# Efficient Sample-Specific Encoder Perturbations

## Quick Facts
- **arXiv ID**: 2405.01601
- **Source URL**: https://arxiv.org/abs/2405.01601
- **Reference count**: 15
- **Key outcome**: This paper proposes a novel approach to improve the performance of frozen encoder-decoder foundation models by perturbing the encoder outputs on a sample-by-sample basis using gradients from a small proxy network trained to predict a desired metric.

## Executive Summary
This paper introduces a novel approach to improve frozen encoder-decoder foundation models by applying sample-specific perturbations to encoder outputs. The method trains a small proxy network to approximate a sequence-level evaluation metric (like COMET for machine translation or WER for speech recognition) from encoder outputs. During inference, gradients of this proxy with respect to the encoder outputs are used to find perturbations that improve the decoder's performance according to the target metric. Experiments demonstrate consistent improvements across different model sizes and tasks, with the approach being particularly effective for smaller, less robust models.

## Method Summary
The approach involves training a lightweight Non-Autoregressive Proxy (NAP) network on top of the encoder to approximate a sequence-level metric. During inference, the gradient of the NAP with respect to the encoder outputs is computed and used to find a perturbation vector. This perturbation is scaled by a hyperparameter α and added to the encoder output before decoding. The method is applied to both machine translation (using Flan-T5 on OPUS-100) and speech recognition (using Whisper on AMI and LibriSpeech datasets), showing consistent improvements in COMET scores and WER reduction respectively.

## Key Results
- Consistent improvements in COMET scores for machine translation across various beam sizes and model sizes
- Significant WER reduction in speech recognition experiments using Whisper models
- Smaller models benefit more from perturbations than larger, more robust models
- Proxies trained on one translation direction can generalize to improve performance on related directions
- The approach is lightweight and efficient, requiring only a small proxy network and sample-specific gradient computation

## Why This Works (Mechanism)

### Mechanism 1
Gradient-based encoder perturbation allows the frozen decoder to generate better outputs by nudging encoder representations toward values that improve the proxy's score estimate. A small proxy network is trained to approximate a sequence-level metric from encoder outputs. During inference, the gradient of the proxy with respect to the encoder outputs is used to find a perturbation vector. This perturbation is added to the encoder output before decoding, biasing the decoder toward sequences that the proxy scores higher. The core assumption is that the proxy's gradient is a useful signal for improving the true metric, even though the proxy is only an approximation and the decoder is frozen.

### Mechanism 2
The proxy network generalizes across related data splits, enabling reuse of proxies trained on one translation direction for another direction. NAPs trained on a source-target pair (e.g., en→de) retain sufficient correlation with the target metric on related pairs (e.g., de→en, en→es), allowing gradient perturbations derived from one proxy to improve decoding in a different but related setting. The core assumption is that the proxy's internal representations capture metric-relevant features that transfer across language pairs or domains, even if trained on a single split.

### Mechanism 3
The method works better on smaller, less robust models because there is more room for improvement via encoder tweaks. Smaller Flan-T5 variants (e.g., Small) benefit more from encoder perturbations than larger ones (e.g., Large) because their decoder outputs are less robust to input variations, so even small changes in encoder representation can shift decoding toward better sequences. The core assumption is that larger models have learned more robust representations and decoding strategies, so encoder perturbations have diminishing returns.

## Foundational Learning

- **Concept**: Gradient-based optimization of differentiable proxies
  - **Why needed here**: The method relies on computing gradients of a proxy network with respect to encoder outputs to find useful perturbations; understanding how gradients propagate through MLPs and how they can be used for sample-specific optimization is critical.
  - **Quick check question**: How do you compute and apply gradients of a small proxy network with respect to the encoder's output embeddings?

- **Concept**: Sequence-level metrics and their non-differentiability
  - **Why needed here**: The target metrics (COMET, WER) are sequence-level and not directly differentiable, so a differentiable proxy is needed to approximate them and allow gradient-based perturbation.
  - **Quick check question**: Why can't we directly backpropagate through a BLEU or WER score to update encoder weights?

- **Concept**: Teacher-forcing vs. inference-time decoding
  - **Why needed here**: The method addresses the exposure bias and metric mismatch problems that arise when training with teacher-forcing but evaluating with sequence-level metrics at inference time.
  - **Quick check question**: What is exposure bias, and how does it differ between teacher-forcing training and inference-time decoding?

## Architecture Onboarding

- **Component map**: Input sequence -> Encoder -> Encoder outputs -> Proxy -> Metric score estimate -> Gradient computation -> Perturbation (scaled by α) -> Perturbed encoder outputs -> Frozen decoder -> Final output

- **Critical path**: 1. Input sequence → Encoder → Encoder outputs 2. Encoder outputs → Proxy → Metric score estimate 3. Gradient of proxy w.r.t. encoder outputs 4. Perturbation (scaled by α) added to encoder outputs 5. Perturbed encoder outputs → Frozen decoder → Final output

- **Design tradeoffs**:
  - Proxy size vs. correlation with true metric: larger proxies may predict better but add more compute
  - Perturbation magnitude (α): too small → no effect; too large → degradation
  - Proxy generalization: proxies trained on one domain may not transfer well to others
  - Frozen decoder constraint: only encoder modifications are allowed, limiting the approach to encoder-decoder models

- **Failure signatures**:
  - No improvement (or degradation) in metric after applying perturbations
  - Proxy correlation with target metric is low or unstable across domains
  - Optimal α varies wildly across samples or datasets
  - Method works on small models but not on larger, more robust ones

- **First 3 experiments**:
  1. Train a proxy on COMET scores from Flan-T5 greedy decodings on OPUS-100 en→de; evaluate proxy correlation on a held-out set.
  2. Apply encoder perturbations using proxy gradients with various α values; measure COMET improvement on en→de validation set.
  3. Test if the same proxy improves en→es and de→en decoding; compare performance gains across translation directions.

## Open Questions the Paper Calls Out

The paper acknowledges that the approach is limited to encoder-decoder systems and suggests future work on modal-specific encoders in multimodal models. It also mentions that other adaptation methods like prompt tuning and LoRA could be explored for comparison. The authors note that while the method is shown to work on COMET scores and WER, it could potentially be applied to other attributes and domains, though this generalization is not explored in depth.

## Limitations

- The method is limited to encoder-decoder architectures where the decoder can be frozen, excluding many modern architectures
- Exact architecture details of the NAP networks are not fully specified, making exact reproduction challenging
- The approach's effectiveness may vary significantly across language pairs and domains due to proxy generalization limitations
- Performance appears sensitive to the perturbation magnitude (α), requiring additional hyperparameter tuning

## Confidence

**High Confidence**:
- The basic mechanism of using proxy gradients to perturb encoder outputs works as described
- The method provides consistent improvements across the tested beam sizes and model sizes
- Smaller models benefit more from the perturbation approach than larger, more robust models

**Medium Confidence**:
- The claim that proxies trained on one domain can generalize to improve performance on related domains
- The assertion that the method is lightweight and efficient in practice
- The claim of generalizability to other attributes and domains beyond the tested ones

**Low Confidence**:
- The long-term stability and robustness of the perturbed encoder representations
- The method's effectiveness on very large models or models with different architectural patterns
- The approach's performance on languages or domains significantly different from the training data

## Next Checks

1. **Proxy Architecture Sensitivity**: Systematically vary the NAP architecture (number of layers, hidden dimensions, activation functions) to determine how sensitive the method is to proxy network design choices. This will help establish the robustness of the approach to implementation details.

2. **Cross-Domain Generalization Test**: Train proxies on one domain (e.g., news translation) and test their effectiveness on a significantly different domain (e.g., conversational speech recognition or biomedical text). This will validate the claimed generalization capability and identify the limits of proxy transferability.

3. **Large-Scale Model Evaluation**: Apply the method to the largest available Flan-T5 model (XXL) and compare the relative improvements to smaller variants. This will test the claim that the method works best on smaller models and establish whether there's a threshold beyond which the approach becomes ineffective.