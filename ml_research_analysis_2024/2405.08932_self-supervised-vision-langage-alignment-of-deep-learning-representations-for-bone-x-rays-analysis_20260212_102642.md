---
ver: rpa2
title: Self-supervised vision-langage alignment of deep learning representations for
  bone X-rays analysis
arxiv_id: '2405.08932'
source_url: https://arxiv.org/abs/2405.08932
tags:
- text
- images
- learning
- pretraining
- bone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-supervised vision-language pretraining
  approach using bone X-rays paired with French medical reports. A practical pipeline
  is introduced for anonymizing and processing the French reports.
---

# Self-supervised vision-langage alignment of deep learning representations for bone X-rays analysis

## Quick Facts
- arXiv ID: 2405.08932
- Source URL: https://arxiv.org/abs/2405.08932
- Reference count: 40
- Proposes self-supervised vision-language pretraining for bone X-rays using French medical reports

## Executive Summary
This paper introduces a self-supervised vision-language pretraining approach for bone X-ray analysis that leverages French medical reports to shape the embedding space. The method involves aligning visual and textual embeddings derived from deep encoders, resulting in an image encoder capable of handling downstream tasks like osteoarthritis quantification, bone age estimation, fracture detection, and anomaly detection. The approach demonstrates competitive performance compared to alternatives requiring more human annotations and represents the first study to integrate French reports for shaping bone X-ray representations.

## Method Summary
The method involves a contrastive learning framework that aligns visual and textual embeddings using CLIP loss. It processes anonymized X-ray images paired with French medical reports through a data pipeline involving DICOM anonymization, PDF text extraction, and French pseudonymization. Multiple text encoders (XLMR, MLUKE, DrBERT) and ViT image encoders at different resolutions (224x224, 336x336, 448x448) are evaluated. The pretraining aligns image and text representations in a shared 512-dimensional space, which is then adapted for downstream tasks through linear layers or fine-tuning.

## Key Results
- Achieves competitive performance on downstream tasks compared to models requiring more human annotations
- Demonstrates that increasing resolution during pretraining from 224x224 to 336x336 positively impacts performance
- Shows that multilingual text encoders with UMLS self-alignment can compensate for smaller French-only corpora

## Why This Works (Mechanism)

### Mechanism 1
- Claim: French reports can shape embedding space for bone X-rays despite lower pretraining data than English models
- Mechanism: Multi-lingual encoder (MLUKE/XLMR) trained on diverse languages captures cross-lingual patterns; UMLS self-alignment aligns biomedical concepts across languages, compensating for smaller French-only corpus
- Core assumption: UMLS synonym alignment transfers adequately between English and French biomedical concepts
- Evidence anchors: [abstract] "Our work stands as the first study to integrate French reports to shape the embedding space devoted to bone X-Rays representations"; [section 2.2] "Methods like Self-alignment pretraining (Sap) [...] showing promising performance on tasks such as Biomedical Entity Linking"

### Mechanism 2
- Claim: Higher resolution pretraining (336x336) improves downstream performance via finer-grained feature learning
- Mechanism: Interpolation of position embeddings preserves semantic token relationships while allowing larger input patches, enabling ViT to capture more detailed anatomical structures
- Core assumption: Resolution increase without architectural changes retains pretraining benefits and doesn't cause overfitting
- Evidence anchors: [section 3.1.3] "Unlike FlexiVit, the resizing is performed once to initialize a ViT B24 336 and a ViT B32 448 models"; [section 4.2.2] "The increase of resolution during the vision-language pretraining from 224x224 to 336x336 has a positive impact on the performances"

### Mechanism 3
- Claim: Self-supervised contrastive alignment learns transferable representations without paired labels
- Mechanism: CLIP-style loss pulls image and text embeddings of same study together while pushing unrelated pairs apart, creating a shared multi-modal space where anatomical and pathological concepts align
- Core assumption: One image per study sufficiently represents multi-image studies for pretraining alignment
- Evidence anchors: [section 3.1.1] "A study denotes the outcomes of a radiological examination. Hence, it is specific to one patient and to one visit to the hospital"; [section 4.1] "It revealed that, the benefits of using multiple images pretraining were not clearly evident"

## Foundational Learning

- Concept: Multi-modal representation learning (vision + language)
  - Why needed here: Bone X-rays and French reports form paired data requiring joint embedding space for zero-shot tasks
  - Quick check question: Can you explain how a shared embedding space enables zero-shot classification without task-specific training?

- Concept: Self-supervised contrastive learning
  - Why needed here: Avoids expensive manual annotations while leveraging large unpaired image-report datasets
  - Quick check question: What's the difference between instance discrimination and global contrastive alignment in this context?

- Concept: Vision Transformer architecture and resolution scaling
  - Why needed here: ViT allows flexible resolution changes via position embedding interpolation, critical for capturing fine bone details
  - Quick check question: How does increasing patch size versus interpolating position embeddings affect computational complexity differently?

## Architecture Onboarding

- Component map: DICOM anonymization → PDF text extraction → French pseudonymization → image-text pairing → VLP pretraining → resolution scaling → downstream evaluation
- Critical path: Data preprocessing → VLP pretraining → resolution scaling → downstream evaluation
- Design tradeoffs:
  - Single image per study vs all images: Simplicity and computational efficiency vs potential information loss
  - Resolution scaling via interpolation vs patch size: Better performance vs computational cost
  - Multilingual vs French-only text encoder: More pretraining data vs language specificity
- Failure signatures:
  - Poor zero-shot performance: Embedding space not well-aligned for target concepts
  - Resolution scaling degradation: Position embedding interpolation issues or batch size too small
  - Downstream underperformance: Insufficient pretraining data or concept alignment failure
- First 3 experiments:
  1. Verify French pseudonymization preserves clinical meaning while removing PHI using DEDUCE adaptation
  2. Test single vs multiple image sampling impact on pretraining loss convergence
  3. Compare 224x224 vs 336x336 resolution pretraining on small downstream classification task with frozen encoder

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed vision-language pretraining approach compare to other existing methods for self-supervised learning on bone X-rays, such as masked image modeling or contrastive learning without text supervision?
- Basis in paper: [inferred] The paper focuses on vision-language pretraining and compares its performance to models trained on ImageNet or from scratch. However, it does not directly compare to other self-supervised learning methods for bone X-rays without text supervision.
- Why unresolved: The paper does not provide a direct comparison to other self-supervised learning methods for bone X-rays without text supervision.
- What evidence would resolve it: A direct comparison of the proposed vision-language pretraining approach to other self-supervised learning methods for bone X-rays without text supervision, such as masked image modeling or contrastive learning, on the same downstream tasks.

### Open Question 2
- Question: How does the choice of text encoder (e.g., Dr BERT, MLUKE, XLMR) impact the performance of the vision-language pretraining approach on different downstream tasks?
- Basis in paper: [explicit] The paper explores different text encoders (Dr BERT, MLUKE, XLMR) and their impact on performance.
- Why unresolved: While the paper provides results for different text encoders, it does not provide a comprehensive analysis of how the choice of text encoder impacts performance across different downstream tasks.
- What evidence would resolve it: A detailed analysis of the impact of different text encoders on the performance of the vision-language pretraining approach across various downstream tasks, including classification, regression, and zero-shot tasks.

### Open Question 3
- Question: How does the resolution of the image encoder (e.g., 224x224, 336x336, 448x448) impact the performance of the vision-language pretraining approach on different downstream tasks?
- Basis in paper: [explicit] The paper explores different image encoder resolutions (224x224, 336x336, 448x448) and their impact on performance.
- Why unresolved: While the paper provides results for different image encoder resolutions, it does not provide a comprehensive analysis of how the resolution impacts performance across different downstream tasks.
- What evidence would resolve it: A detailed analysis of the impact of different image encoder resolutions on the performance of the vision-language pretraining approach across various downstream tasks, including classification, regression, and zero-shot tasks.

## Limitations

- Limited empirical validation of UMLS self-alignment effectiveness for cross-lingual biomedical concept transfer between English and French
- Insufficient ablation studies comparing single-image vs multi-image study sampling strategies for pretraining
- Lack of comprehensive analysis on how text encoder choice and image resolution impact performance across different downstream tasks

## Confidence

- French reports shaping embedding space: Low
- Resolution scaling benefits: Medium
- Self-supervised contrastive learning effectiveness: High
- Zero-shot task performance: Medium

## Next Checks

1. Conduct systematic ablation studies comparing UMLS self-alignment performance against French-only pretraining to quantify the actual benefit of cross-lingual concept transfer.

2. Perform sensitivity analysis on position embedding interpolation at higher resolutions (336x336, 448x448) to identify potential artifacts and determine optimal batch sizes.

3. Design experiments testing multi-image vs single-image study sampling strategies on downstream tasks that specifically require multi-view information, such as fracture detection in complex anatomical regions.