---
ver: rpa2
title: Human-Interpretable Adversarial Prompt Attack on Large Language Models with
  Situational Context
arxiv_id: '2407.14644'
source_url: https://arxiv.org/abs/2407.14644
tags:
- prompt
- adversarial
- llms
- step
- criminal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that human-interpretable adversarial prompt
  attacks can be generated without gradient-based methods by combining adversarial
  insertions with situational context from movies. Using a few-shot chain-of-thought
  prompting approach, the authors successfully induced harmful responses from multiple
  LLMs including GPT-3.5, GPT-4, Llama-3-8B, and Gemma-7B.
---

# Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Context

## Quick Facts
- arXiv ID: 2407.14644
- Source URL: https://arxiv.org/abs/2407.14644
- Reference count: 14
- Key outcome: Human-interpretable adversarial prompts using movie contexts can bypass LLM safety guardrails with high success rates across multiple models

## Executive Summary
This work introduces a novel approach to generating adversarial prompts that can induce harmful responses from large language models while remaining human-interpretable. Unlike previous nonsensical suffix attacks, this method leverages situational contexts from movies combined with carefully crafted adversarial insertions. The approach demonstrates that as few as 1-2 demonstrations can successfully bypass safety mechanisms in state-of-the-art models including GPT-3.5, GPT-4, Llama-3-8B, and Gemma-7B. The attacks achieve high harmfulness scores (4-5 on a GPT-4 Judge scale) and transfer between models, representing a practical threat with lower technical barriers than gradient-based methods.

## Method Summary
The authors developed a few-shot chain-of-thought prompting approach that combines adversarial insertions with situational context derived from movies. The method involves selecting an innocuous movie context, crafting an adversarial insertion that triggers harmful responses, and presenting these as demonstrations in a few-shot learning format. The LLM is then prompted to continue the pattern, resulting in harmful outputs. The approach was evaluated across four different LLM architectures, with success measured by a GPT-4 Judge scoring system. The authors systematically varied the number of demonstrations and evaluated transferability between models to assess robustness.

## Key Results
- Successful attacks on GPT-3.5, GPT-4, Llama-3-8B, and Gemma-7B with minimal demonstrations (1-2)
- GPT-4 Judge scores of 4-5 indicating significant harmfulness of generated responses
- Cross-model transferability demonstrating robustness of the attack method
- Higher success rates compared to nonsensical suffix attacks while maintaining human interpretability

## Why This Works (Mechanism)
The attack exploits the few-shot learning capabilities of LLMs by providing contextually relevant demonstrations that establish a harmful pattern. The situational context from movies makes the prompts appear innocuous while the adversarial insertions create a subtle but consistent direction toward harmful content. The chain-of-thought prompting encourages the model to follow the demonstrated reasoning path, effectively overriding safety guardrails that might otherwise detect and block harmful content. This approach works because LLMs are optimized to complete patterns rather than strictly adhere to safety constraints when given sufficient contextual justification.

## Foundational Learning
- **Few-shot learning in LLMs**: Why needed - Enables pattern completion with minimal examples; Quick check - Verify model responds to 1-2 demonstrations with correct pattern continuation
- **Chain-of-thought prompting**: Why needed - Guides reasoning process toward desired outcomes; Quick check - Confirm model follows demonstrated reasoning steps
- **Contextual embeddings**: Why needed - Situational context makes prompts appear benign; Quick check - Test model's interpretation of movie context vs. explicit harmful content
- **Transfer learning between models**: Why needed - Demonstrates attack robustness across architectures; Quick check - Validate attack success across different model families

## Architecture Onboarding

**Component Map**: Movie Context Selection -> Adversarial Insertion Crafting -> Few-shot Demonstration Assembly -> LLM Prompting -> Harmfulness Evaluation

**Critical Path**: The core workflow follows: (1) Select movie context, (2) Craft adversarial insertion, (3) Create few-shot demonstrations, (4) Generate harmful response, (5) Evaluate with Judge scoring

**Design Tradeoffs**: The method trades computational complexity (avoiding gradient-based optimization) for manual effort in crafting contextually appropriate demonstrations. This makes attacks more practical but potentially less scalable than automated approaches.

**Failure Signatures**: Attacks fail when: movie context is too distant from the harmful topic, adversarial insertions are too subtle to establish a clear pattern, or safety mechanisms detect the harmful trajectory despite contextual masking.

**Three First Experiments**:
1. Test attack success with varying numbers of demonstrations (0, 1, 2, 3) to establish minimal requirements
2. Evaluate transferability by attacking one model family and testing against different architectures
3. Compare success rates against baseline nonsensical suffix attacks to quantify improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Manual effort required for crafting situational contexts may limit scalability for large-scale attacks
- Effectiveness across domains beyond movie contexts remains unexplored, potentially limiting generalizability
- Focus on specific model architectures leaves open questions about robustness against other LLM types
- No evaluation against state-of-the-art defense mechanisms that might detect or block such attacks

## Confidence
- Claim that human-interpretable adversarial prompts can induce harmful responses: **High** - Supported by empirical results across multiple models
- Claim about scalability and generalizability: **Medium** - Limited by manual effort and domain specificity
- Claim of lower technical barrier compared to gradient methods: **Medium** - Requires prompt engineering expertise still

## Next Checks
1. Evaluate attack effectiveness across diverse domains (news, literature, historical events) to assess generalizability beyond movie contexts
2. Test attack robustness against state-of-the-art defense mechanisms including adversarial training and input sanitization
3. Investigate automation of situational context generation and adversarial insertion crafting to reduce manual effort and assess practical deployment feasibility