---
ver: rpa2
title: 'MLEM: Generative and Contrastive Learning as Distinct Modalities for Event
  Sequences'
arxiv_id: '2401.15935'
source_url: https://arxiv.org/abs/2401.15935
tags:
- generative
- contrastive
- event
- learning
- mlem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores self-supervised learning for event sequences,
  a key modality in applications like banking, e-commerce, and healthcare. While existing
  SSL methods excel in domains like images and text, they often struggle with event
  sequences due to their unique characteristics, including multiple features, categorical
  and numerical values, non-uniform temporal spacing, and the importance of temporal
  proximity.
---

# MLEM: Generative and Contrastive Learning as Distinct Modalities for Event Sequences

## Quick Facts
- arXiv ID: 2401.15935
- Source URL: https://arxiv.org/abs/2401.15935
- Reference count: 23
- Key outcome: MLEM outperforms existing self-supervised learning methods for event sequences by combining generative and contrastive approaches

## Executive Summary
This study addresses the challenge of self-supervised learning for event sequences, which are critical in domains like banking, e-commerce, and healthcare. Existing SSL methods designed for images and text struggle with event sequences due to their unique characteristics including multiple features, categorical and numerical values, non-uniform temporal spacing, and the importance of temporal proximity. Through comprehensive comparative analysis, the authors find that neither generative nor contrastive approaches consistently outperform the other, leading to the development of MLEM (Multimodal-Learning Event Model). MLEM treats generative and contrastive learning as distinct yet complementary modalities and aligns their embeddings, achieving superior performance across multiple datasets and metrics including downstream task performance, embedding quality, and robustness to data perturbations.

## Method Summary
The MLEM approach combines generative and contrastive learning for event sequences through a unified framework. A GRU-based encoder produces sequence embeddings that feed both a generative transformer decoder for sequence reconstruction and a frozen pre-trained contrastive encoder for subsequence discrimination. The key innovation is an alignment loss that pulls embeddings from the same sequence across modalities closer while maintaining separation between sequences. The model is trained with fixed hyperparameters (learning rate 1e-3, weight decay 3e-3, batch size 128) for 100 epochs on small datasets or 40 epochs on large datasets. Evaluation includes linear/nonlinear probing on downstream tasks, anisotropy and intrinsic dimension computation, and robustness testing via event shuffling and dropout.

## Key Results
- MLEM consistently achieves superior performance across all five evaluated datasets (ABank, Age, TaoBao, PhysioNet 2012, Pendulum)
- The model demonstrates improved embedding quality with lower anisotropy and more favorable intrinsic dimension characteristics
- MLEM shows better robustness to data perturbations compared to individual generative or contrastive approaches
- Neither generative nor contrastive methods alone consistently outperform the other across all metrics and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative and contrastive approaches encode complementary aspects of event sequences, with generative modeling capturing temporal dynamics and contrastive learning focusing on structural similarities.
- Mechanism: The generative encoder learns to reconstruct entire sequences, forcing it to encode temporal order and event dependencies. The contrastive encoder learns to distinguish between subsequences from the same sequence versus different sequences, capturing local temporal patterns and event relationships.
- Core assumption: Event sequences contain both global temporal structure and local event relationships that can be captured by different self-supervised approaches.
- Evidence anchors:
  - [abstract]: "We find that neither the contrastive nor generative method is superior" - indicates complementary strengths
  - [section 3.2]: "The decoder autoregressively reconstructs the entire sequence" - shows generative modeling captures temporal dynamics
  - [corpus]: Weak - no direct evidence found

### Mechanism 2
- Claim: MLEM's alignment loss effectively combines generative and contrastive embeddings by pulling embeddings from the same sequence closer while pushing embeddings from different sequences apart.
- Mechanism: The alignment loss uses a contrastive objective to align embeddings from the same sequence across modalities while maintaining separation between sequences. This creates a unified embedding space that captures both generative reconstruction capability and contrastive structural awareness.
- Core assumption: Embeddings from the same sequence should be similar regardless of whether they come from the generative or contrastive encoder.
- Evidence anchors:
  - [abstract]: "MLEM treats contrastive learning and generative modeling as distinct yet complementary modalities, aligning their embeddings" - directly states the alignment mechanism
  - [section 3.4]: Formal definition of Lalign shows the mathematical mechanism for alignment
  - [corpus]: Weak - no direct evidence found

### Mechanism 3
- Claim: The combined generative and contrastive approach improves downstream task performance by creating embeddings that capture both event content and temporal relationships.
- Mechanism: By training with both generative reconstruction loss and contrastive alignment loss, the model learns embeddings that preserve sequence-level information for reconstruction while also maintaining local temporal patterns for discrimination tasks.
- Core assumption: Downstream tasks benefit from embeddings that capture both global sequence information and local temporal patterns.
- Evidence anchors:
  - [abstract]: "combining contrastive and generative approaches into one procedure with MLEM achieves superior performance across multiple metrics" - shows improved downstream performance
  - [section 5.1]: "MLEM consistently achieves the most favorable results across all datasets" - empirical evidence of superior performance
  - [corpus]: Weak - no direct evidence found

## Foundational Learning

- Concept: Self-supervised learning for event sequences
  - Why needed here: Event sequences have unique characteristics (multiple features, categorical and numerical values, non-uniform temporal spacing) that make direct application of image/text SSL methods ineffective
  - Quick check question: Why can't we directly apply contrastive learning methods designed for images to event sequences?

- Concept: Generative modeling for sequence reconstruction
  - Why needed here: Event sequences require modeling of temporal dependencies and multiple feature types simultaneously, which generative approaches can handle through autoregressive reconstruction
  - Quick check question: How does the autoregressive reconstruction objective force the encoder to capture temporal relationships?

- Concept: Contrastive learning for representation learning
  - Why needed here: Contrastive methods can learn to distinguish between sequences and capture local temporal patterns without requiring explicit labels
  - Quick check question: What is the role of positive and negative pairs in contrastive learning for event sequences?

## Architecture Onboarding

- Component map: Input → GRU Encoder → (Generative Decoder + Contrastive Encoder) → Alignment Loss → Combined Embeddings
- Critical path: The GRU encoder is the central component that feeds both the generative decoder and contrastive encoder. The alignment loss is the key innovation that combines the two approaches.
- Design tradeoffs:
  - Using pre-trained contrastive encoder vs training from scratch: Pre-training reduces computation but may limit flexibility
  - Alignment strength (β parameter): Higher alignment may improve combined embeddings but could harm individual modality performance
  - Embedding dimension: Higher dimensions capture more information but increase computational cost and risk overfitting
- Failure signatures:
  - Poor reconstruction performance indicates generative branch is not learning temporal dependencies
  - Contrastive loss not decreasing suggests issues with positive/negative pair sampling or encoder capacity
  - Alignment loss divergence indicates conflicts between generative and contrastive objectives
  - Downstream performance worse than individual methods suggests alignment is harming rather than helping
- First 3 experiments:
  1. Train generative model alone on a simple dataset (e.g., Pendulum) and evaluate reconstruction quality to verify temporal modeling capability
  2. Train contrastive model alone on the same dataset and check if it can distinguish between sequences with different properties
  3. Combine both models with alignment loss and verify that alignment loss decreases while both individual losses remain reasonable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MLEM model's sensitivity to dropout at 0.5 probability indicate that it is more susceptible to noise or outliers in the data compared to other methods?
- Basis in paper: [explicit] The paper mentions that at a dropout rate of 0.5, the generative modeling embeddings not only resist the decline but actually surpass their original performance, while MLEM's performance slightly decreases.
- Why unresolved: The paper suggests this effect shows high sensitivity to the presence of noisy or out-of-distribution events but states that further work is required to draw reliable conclusions.
- What evidence would resolve it: Additional experiments varying the type and amount of noise or outliers in the data, and comparing the performance of MLEM and other methods under these conditions.

### Open Question 2
- Question: Is there a correlation between anisotropy or intrinsic dimension and downstream performance in the domain of event sequences?
- Basis in paper: [inferred] The paper states that unlike findings in other domains, no correlation was observed between anisotropy or intrinsic dimension and downstream performance in their study.
- Why unresolved: The paper's findings differ from previous studies, and the authors suggest that further investigation is needed to understand the relationship between these metrics and performance in the event sequence domain.
- What evidence would resolve it: Conducting experiments with a wider variety of datasets and model architectures, and analyzing the correlation between anisotropy, intrinsic dimension, and downstream performance across these different settings.

### Open Question 3
- Question: How does the sequential nature of event sequence data affect the performance of current self-supervised models, and can models be improved to better utilize this sequential information?
- Basis in paper: [explicit] The paper discusses the unique characteristics of event sequences, such as non-uniform temporal spacing and the importance of temporal proximity, and suggests that current models may be employing a 'Bag-of-Words' approach rather than considering the sequential nature of the data.
- Why unresolved: The paper identifies this as an area for improvement but does not provide a definitive solution or demonstrate how models can be enhanced to better capture sequential information.
- What evidence would resolve it: Developing and evaluating new models or techniques specifically designed to leverage the sequential nature of event sequence data, and comparing their performance to existing methods on tasks that heavily rely on temporal relationships.

## Limitations
- The evaluation focuses on five specific datasets that may not represent the full diversity of event sequence applications
- The experimental setup doesn't fully explore boundary conditions where one approach might dominate the other
- The alignment mechanism relies on several hyperparameters whose sensitivity to different data characteristics is not thoroughly explored

## Confidence

**High Confidence**: The core observation that generative and contrastive approaches capture complementary information in event sequences is well-supported by empirical results across multiple datasets and evaluation metrics.

**Medium Confidence**: The claim that MLEM consistently outperforms existing methods is supported by the presented results, but performance gains vary across datasets and tasks.

**Low Confidence**: The assertion that the alignment loss effectively combines the two modalities without introducing conflicts requires further validation.

## Next Checks
1. Conduct ablation studies removing the alignment loss component to quantify the specific contribution of multimodal alignment versus individual generative and contrastive capabilities
2. Systematically vary the alignment strength (β) and other key hyperparameters across a wider range of values to understand their impact on different types of event sequence data
3. Test MLEM on additional event sequence datasets from domains not represented in the current study (e.g., sensor data, network traffic logs) to assess generalizability beyond the five evaluated datasets