---
ver: rpa2
title: Model Selection with a Shapelet-based Distance Measure for Multi-source Transfer
  Learning in Time Series Classification
arxiv_id: '2409.20005'
source_url: https://arxiv.org/abs/2409.20005
tags:
- learning
- transfer
- time
- datasets
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for selecting and utilizing
  multiple datasets for transfer learning in time series classification. The approach
  combines multiple datasets as a single source dataset for pre-training neural networks,
  enhancing the effectiveness of transfer learning.
---

# Model Selection with a Shapelet-based Distance Measure for Multi-source Transfer Learning in Time Series Classification

## Quick Facts
- arXiv ID: 2409.20005
- Source URL: https://arxiv.org/abs/2409.20005
- Reference count: 40
- Primary result: Proposed method achieves 80.25% average test accuracy on 128 UCR time series datasets

## Executive Summary
This paper introduces a novel method for selecting and utilizing multiple datasets for transfer learning in time series classification. The approach combines multiple datasets as a single source dataset for pre-training neural networks, enhancing the effectiveness of transfer learning. To select appropriate datasets, the method measures transferability based on shapelet discovery, which is more efficient than traditional methods requiring extensive pre-training for each possible architecture.

## Method Summary
The proposed method addresses data scarcity in time series classification by combining multiple source datasets into a single "super dataset" for pre-training. Shapelet-based distance measures (Average Shapelet and Minimum Shapelet distances) are used to identify and select the most relevant source datasets without requiring pre-trained models. The method uses a temporal CNN with a VGG-like architecture (3 conv blocks, global average pooling) for both pre-training and fine-tuning phases. Source datasets are balanced through oversampling to ensure equal contribution to the transfer learning process.

## Key Results
- Achieves 80.25% average test accuracy across 128 UCR time series datasets
- Outperforms other comparative methods in time series classification
- Demonstrates computational efficiency compared to traditional transferability estimation methods
- Shows effectiveness of shapelet-based distance measures for source dataset selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source transfer learning improves time series classification by increasing the diversity and volume of training data for pre-training.
- Mechanism: Combining multiple source datasets into a single "super dataset" allows the model to learn a more general set of features before fine-tuning on the target dataset. This addresses the data scarcity problem common in time series tasks.
- Core assumption: The combined dataset preserves the beneficial features of individual sources while mitigating the risk of negative transfer through balanced oversampling.
- Evidence anchors:
  - [abstract] "Specifically, our method combines multiple datasets as one source dataset for pre-training neural networks."
  - [section] "To balance SMulti, oversampling is performed while preserving the class ratios. This is done due to the discrepancy in the size of possible source datasets; it ensures that every dataset has an equal contribution to the transfer learning."

### Mechanism 2
- Claim: Shapelet-based distance measures enable effective source dataset selection without requiring pre-trained models.
- Mechanism: Discriminative shapelets are extracted from each dataset using Matrix Profile, and the similarity between shapelet sets is used to rank datasets for transfer learning. This avoids the computational cost of training models for each potential source.
- Core assumption: Datasets with similar discriminative shapelets contain similar underlying patterns, making them good candidates for transfer learning.
- Evidence anchors:
  - [abstract] "For selecting multiple sources, our method measures the transferability of datasets based on shapelet discovery for effective source selection."
  - [section] "We propose two shapelet distance measure schemes, Average Shapelet, and Minimum Shapelet distances."

### Mechanism 3
- Claim: The proposed method is computationally efficient compared to traditional transferability estimation methods.
- Mechanism: Shapelet similarity-based source selection requires only dataset processing (O(n² + w)) rather than training multiple models (O(i · n²)), making it scalable to large numbers of potential source datasets.
- Core assumption: The computational savings from avoiding model training outweighs any potential loss in selection accuracy compared to model-based methods.
- Evidence anchors:
  - [section] "Regarding computational time, shapelet similarity-based source selection has a huge benefit compared to other transferability estimation metrics."

## Foundational Learning

- Concept: Time series classification and the challenges of data scarcity
  - Why needed here: Understanding why transfer learning is particularly valuable for time series tasks
  - Quick check question: Why is transfer learning more challenging for time series than for image recognition?

- Concept: Shapelet discovery and Matrix Profile
  - Why needed here: These are the core techniques for measuring dataset similarity without pre-training models
  - Quick check question: How does Matrix Profile identify discriminative shapelets efficiently?

- Concept: Transfer learning fundamentals and negative transfer
  - Why needed here: To understand the problem being solved and why source selection matters
  - Quick check question: What is negative transfer and how does the multi-source approach help mitigate it?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Resampling and balancing multiple source datasets
  - Shapelet extraction: Matrix Profile-based discovery of discriminative subsequences
  - Distance computation: Average and minimum shapelet distance measures
  - Model architecture: 1D CNN with VGG-like structure (3 conv blocks, global average pooling)
  - Training pipeline: Multi-source pre-training followed by target fine-tuning

- Critical path: Source selection → Multi-source pre-training → Target fine-tuning
- Design tradeoffs:
  - Fixed shapelet size (15) vs. adaptive sizing: Simplicity vs. potential loss of discriminative power for some datasets
  - Fixed pre-training iterations (10,000) vs. adaptive: Fair comparison vs. potential under/over-training
  - Average vs. minimum shapelet distance: Comprehensive vs. focused similarity measurement

- Failure signatures:
  - Poor performance on datasets with noisy or subtle patterns
  - Diminishing returns when combining too many source datasets
  - Computational bottleneck in shapelet extraction for very long time series

- First 3 experiments:
  1. Verify shapelet extraction works on a simple two-class dataset by visualizing the discovered shapelets
  2. Test the distance computation between two known similar and two known dissimilar datasets
  3. Run the full pipeline on a small subset of UCR datasets (e.g., 5) to verify the end-to-end functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of shapelet size affect the transferability measure's accuracy?
- Basis in paper: [inferred] The paper mentions using a fixed shapelet size of 15, but does not explore varying this parameter.
- Why unresolved: The optimal shapelet size may vary depending on the dataset and task.
- What evidence would resolve it: Experiments comparing transferability measure accuracy across different shapelet sizes.

### Open Question 2
- Question: Can the proposed method be extended to non-temporal neural network architectures?
- Basis in paper: [explicit] The paper states "While the experimental results use temporal CNNs, there is no theoretical limitation on the type of neural network used."
- Why unresolved: The paper only demonstrates the method with temporal CNNs, leaving other architectures unexplored.
- What evidence would resolve it: Experiments applying the method to various non-temporal neural network architectures.

### Open Question 3
- Question: How does the proposed method perform with real-world noisy data compared to clean datasets?
- Basis in paper: [explicit] The paper notes that datasets with more noise showed worse performance.
- Why unresolved: The paper only evaluates on UCR Archive datasets, which may not represent real-world noise levels.
- What evidence would resolve it: Experiments on real-world datasets with varying levels of noise.

### Open Question 4
- Question: Is there an optimal number of source datasets beyond which adding more does not improve performance?
- Basis in paper: [inferred] The paper mentions diminishing returns with the number of datasets but does not specify an optimal number.
- Why unresolved: The paper sets a fixed number of pre-training iterations, which may mask the true effect of adding more datasets.
- What evidence would resolve it: Experiments varying the number of source datasets and pre-training iterations.

## Limitations

- Limited empirical validation scope: While tested on 128 datasets from the UCR archive, the paper lacks ablation studies to isolate the impact of shapelet-based selection versus other components.
- Fixed hyperparameters without sensitivity analysis: The shapelet size (15) and pre-training iterations (10,000) are treated as constants without exploring their impact on performance.
- Computational complexity concerns: The actual runtime comparison with model-based methods is not provided, raising questions about scalability for very large datasets.

## Confidence

- High confidence: The core mechanism of combining multiple source datasets for pre-training is well-established in transfer learning literature.
- Medium confidence: The shapelet-based distance measure for dataset selection is theoretically sound, but empirical evidence for its superiority is limited.
- Low confidence: The claim that the proposed method "increases the performance of temporal convolutional neural networks" lacks statistical significance testing and comparison to strong baselines on individual datasets.

## Next Checks

1. **Ablation study on source selection**: Compare the proposed shapelet-based selection against random selection and model-based transferability metrics on a subset of 10-15 UCR datasets with statistical significance testing.

2. **Hyperparameter sensitivity analysis**: Systematically vary shapelet size (10, 15, 20, 25) and pre-training iterations (5K, 10K, 15K) to determine their impact on classification accuracy and identify optimal configurations.

3. **Computational benchmarking**: Measure and compare the wall-clock time for shapelet-based selection versus model-based transferability estimation across datasets of varying sizes to validate the claimed efficiency benefits.