---
ver: rpa2
title: The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations
  in Large Language Models
arxiv_id: '2404.05904'
source_url: https://arxiv.org/abs/2404.05904
tags:
- tasks
- language
- llms
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Hallucinations Leaderboard, an open initiative
  to quantitatively measure and compare the tendency of Large Language Models (LLMs)
  to produce hallucinations - outputs that do not align with factual reality or the
  input context. The leaderboard uses a comprehensive set of benchmarks focusing on
  different aspects of hallucinations, such as factuality and faithfulness, across
  various tasks including question-answering, summarisation, and reading comprehension.
---

# The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2404.05904
- Source URL: https://arxiv.org/abs/2404.05904
- Reference count: 26
- This paper introduces the Hallucinations Leaderboard, an open initiative to quantitatively measure and compare the tendency of Large Language Models (LLMs) to produce hallucinations.

## Executive Summary
This paper presents the Hallucinations Leaderboard, a comprehensive framework for measuring and comparing hallucination tendencies in Large Language Models (LLMs) across multiple tasks and model families. The leaderboard evaluates models using standardized benchmarks focusing on factuality and faithfulness in tasks like question-answering, summarization, and reading comprehension. The analysis reveals important patterns in how different models handle hallucinations, showing that instruction fine-tuning improves faithfulness but not consistently factuality, and that larger models generally show greater improvements in factuality. The leaderboard serves as a valuable tool for researchers and practitioners to assess and compare LLM reliability.

## Method Summary
The Hallucinations Leaderboard evaluates 20 open-source LLMs across 15 tasks using zero- or few-shot in-context learning through the EleutherAI Language Model Evaluation Harness. The framework categorizes tasks into factuality (Closed-book QA, Fact-Checking, Hallucination Detection) and faithfulness (Summarization, Reading Comprehension, Instruction Following, Hallucination Detection). Two scores are computed: factuality score and faithfulness score, representing overall performance on each hallucination type. Results are displayed in heatmaps and organized using hierarchical clustering based on Ward variance minimization linkage method and Euclidean distance.

## Key Results
- Instruction fine-tuned models show higher faithfulness scores but mixed factuality scores, suggesting a tradeoff between instruction adherence and factual accuracy.
- Increasing model size generally improves both faithfulness and factuality, with factuality showing more substantial improvements.
- Evaluation results are highly sensitive to prompt variations, with tasks relying on factual knowledge showing less sensitivity than those dependent on logical reasoning.
- The leaderboard reveals significant variances in hallucination tendencies across different models and tasks, providing actionable insights for model selection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The leaderboard provides a systematic framework to quantify hallucination tendencies in LLMs across multiple tasks and model families.
- Mechanism: By evaluating models using a standardized set of benchmarks covering factuality and faithfulness in tasks like QA, summarization, and reading comprehension, the leaderboard enables consistent comparison and identification of hallucination patterns.
- Core assumption: The selected tasks and metrics reliably capture hallucination phenomena, and the zero- or few-shot evaluation settings are appropriate for measuring hallucinations without model adaptation.
- Evidence anchors:
  - [abstract] "The leaderboard uses a comprehensive set of benchmarks focusing on different aspects of hallucinations, such as factuality and faithfulness, across various tasks, including question-answering, summarisation, and reading comprehension."
  - [section 2] "We distinguish two scenarios for LLM hallucinations... factuality, i.e. whether LLMs generate factually correct content according to world knowledge... and faithfulness, i.e., whether an LLM generation adheres to the given source of information."
- Break condition: If the task benchmarks do not adequately capture hallucination behavior or if evaluation settings (zero/few-shot) fail to reflect real-world usage, the systematic comparison would break down.

### Mechanism 2
- Claim: Instruction fine-tuning improves faithfulness to input context but does not consistently improve factuality.
- Mechanism: Fine-tuning models on instruction datasets enhances their ability to follow specific instructions and retain fidelity to provided context, which improves faithfulness scores, but this does not necessarily translate to improved generation of factually accurate information, as seen in the mixed factuality score results.
- Core assumption: The instruction fine-tuning process prioritizes instruction adherence over knowledge recall or factuality.
- Evidence anchors:
  - [section 3.1] "instruction fine tuned models achieve higher Faithfulness scores than their base counterparts... Unlike Faithfulness, Factuality scores across the models show a trend of either marginal improvement or, in some cases, a decline."
  - [section 3.1] "This pattern suggests a trade-off between Faithfulness and Factuality in instruction fine-tuning."
- Break condition: If instruction fine-tuning is modified to explicitly target both faithfulness and factuality, or if factuality is prioritized during training, this tradeoff may not hold.

### Mechanism 3
- Claim: Increasing model size generally improves both faithfulness and factuality, with larger gains in factuality.
- Mechanism: As model size increases, the parametric knowledge accumulated during training becomes more extensive, reducing dependence on context and improving factuality, while also enhancing overall generation quality and faithfulness.
- Core assumption: Larger models have more parameters to store factual knowledge and can better generalize from training data.
- Evidence anchors:
  - [section 3.2] "While an increase in model size generally enhances both Faithfulness and Factuality, it is noteworthy that Factuality tends to exhibit more substantial improvements compared to Faithfulness."
  - [section 3.2] "This suggests that as model size increases, the accumulation of parametric knowledge during the training phrase becomes more extensive, leading to a reduced dependence on context."
- Break condition: If increasing model size does not lead to proportional gains in parametric knowledge or if context becomes more critical for certain tasks, the improvements in factuality and faithfulness may plateau or reverse.

## Foundational Learning

- Concept: Zero-shot and few-shot in-context learning
  - Why needed here: The leaderboard evaluates models without training, relying on their ability to perform tasks based on instructions and examples provided in the prompt, which is essential for measuring inherent hallucination tendencies.
  - Quick check question: Can a model perform a task based solely on instructions and a few examples provided in the prompt, without any parameter updates?

- Concept: Hallucination types (factuality vs. faithfulness)
  - Why needed here: Understanding the distinction between hallucinations that contradict world knowledge (factuality) and those that deviate from the input context (faithfulness) is crucial for interpreting the leaderboard results and identifying the root causes of hallucinations.
  - Quick check question: Is a generated summary that contradicts the input document an example of factuality hallucination or faithfulness hallucination?

- Concept: Evaluation metrics (accuracy, ROUGE-L, EM)
  - Why needed here: Familiarity with the metrics used to assess model performance on different tasks (e.g., accuracy for QA, ROUGE-L for summarization, EM for reading comprehension) is necessary to understand and compare the results across the leaderboard.
  - Quick check question: What metric is used to evaluate the factual accuracy of a generated summary compared to the reference summary?

## Architecture Onboarding

- Component map: Tasks (Factuality and Faithfulness categories) -> LLMs (pre-trained, fine-tuned, instruction-tuned) -> EleutherAI Evaluation Harness -> Results (heatmaps, scores)
- Critical path: Select tasks and models → Define prompt templates → Run evaluation using harness → Collect and normalize results → Generate heatmap and scores
- Design tradeoffs: Zero/few-shot evaluation measures inherent capabilities but may not reflect fine-tuned performance; focus on open-source models enables reproducibility but limits scope; standardized tasks ensure comparability but may not capture all hallucination aspects
- Failure signatures: Inconsistent results across tasks or models may indicate evaluation setup issues; low scores across all models on a task may suggest task difficulty or metric mismatch; high variance in results across prompt variations may indicate sensitivity to prompt engineering
- First 3 experiments:
  1. Run evaluation for a small set of tasks and models using default settings to verify basic functionality and identify immediate issues
  2. Generate multiple prompt variations for a few tasks and compare results to assess sensitivity to prompt engineering
  3. Analyze results for a specific task or model in detail to understand hallucination types and causes using generated outputs and metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sensitivity of evaluation results to prompt variations differ between tasks that rely on factual knowledge versus those that depend on logical reasoning or linguistic features?
- Basis in paper: [inferred] from the analysis of prompt template robustness in Appendix A and the discussion on task dependencies
- Why unresolved: The paper mentions that tasks relying on factual knowledge (Natural Questions, TruthfulQA) show minor sensitivity to prompt variations, while tasks dependent on logical reasoning or linguistic features might be more sensitive. However, the analysis is limited to a small set of tasks and prompts, and additional generated instructions could lead to more noticeable distortions in task performance.
- What evidence would resolve it: Conduct a comprehensive study on prompt robustness across a wider range of tasks, including those that rely on logical reasoning or linguistic features, using a larger set of prompt variations and multiple models.

### Open Question 2
- Question: What is the impact of instruction fine-tuning on the trade-off between faithfulness and factuality in Large Language Models (LLMs)?
- Basis in paper: [explicit] from the discussion in Section 3.1 on the impact of instruction fine-tuning on hallucinations
- Why unresolved: The paper shows that instruction fine-tuned models achieve higher faithfulness scores but do not consistently improve factuality scores. This suggests a trade-off between faithfulness and factuality, but the underlying reasons and mechanisms for this trade-off are not fully explored.
- What evidence would resolve it: Investigate the internal representations and mechanisms of instruction fine-tuned models to understand how they balance faithfulness and factuality. Conduct ablation studies to isolate the effects of instruction fine-tuning on these aspects.

### Open Question 3
- Question: How does the model size influence the accumulation of parametric knowledge and its effect on factuality and faithfulness hallucinations?
- Basis in paper: [explicit] from the analysis in Section 3.2 on the impact of model size on hallucinations
- Why unresolved: The paper observes that larger models generally enhance both factuality and faithfulness, with factuality showing more substantial improvements. However, the exact relationship between model size, parametric knowledge accumulation, and hallucination tendencies is not fully elucidated.
- What evidence would resolve it: Conduct a detailed analysis of the knowledge stored in models of different sizes and how this knowledge is utilized during generation. Use techniques like probing and interpretability methods to understand the impact of parametric knowledge on hallucinations.

## Limitations
- The evaluation relies on zero- or few-shot in-context learning, which may not fully capture model behavior under fine-tuned conditions
- Results are highly sensitive to prompt formulation, but exact prompt templates are not provided, making exact reproduction challenging
- The correlation between faithfulness and factuality metrics is not always consistent across tasks, suggesting potential limitations in how these concepts are operationalized
- The focus on open-source models limits generalizability to proprietary models that may have different hallucination profiles

## Confidence

**High Confidence:** The systematic framework for quantifying hallucinations across multiple tasks and model families is well-established. The leaderboard successfully demonstrates variances in hallucination tendencies across different LLMs and tasks, providing a useful tool for researchers and practitioners.

**Medium Confidence:** The mechanism showing that instruction fine-tuning improves faithfulness but not necessarily factuality is supported by the data, but the tradeoff pattern may vary depending on specific training procedures and datasets used. The observation that larger models show greater improvements in factuality than faithfulness is supported but requires further validation across more model families.

**Low Confidence:** The exact impact of prompt sensitivity on leaderboard results cannot be fully assessed without access to the specific prompt templates used. The generalizability of findings to proprietary models and real-world deployment scenarios remains uncertain.

## Next Checks

1. **Prompt Sensitivity Analysis**: Conduct systematic experiments varying prompt formulations across all tasks to quantify the impact of prompt engineering on hallucination scores. This should include testing different instruction wordings, few-shot example selections, and prompt formats to establish the robustness of the leaderboard rankings.

2. **Correlation Validation**: Perform detailed correlation analysis between faithfulness and factuality metrics across all tasks to identify where these measures diverge and investigate the root causes. This should include qualitative analysis of model outputs to understand the nature of discrepancies.

3. **Fine-tuning Impact Study**: Evaluate a subset of models both in their zero-shot/few-shot form and after fine-tuning on instruction datasets to quantify the actual impact of fine-tuning on hallucination tendencies. This would help validate the observed tradeoffs and provide guidance on when fine-tuning is most beneficial for reducing hallucinations.