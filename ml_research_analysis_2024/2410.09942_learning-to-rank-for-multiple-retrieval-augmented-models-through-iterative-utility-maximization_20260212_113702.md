---
ver: rpa2
title: Learning to Rank for Multiple Retrieval-Augmented Models through Iterative
  Utility Maximization
arxiv_id: '2410.09942'
source_url: https://arxiv.org/abs/2410.09942
tags:
- search
- feedback
- agents
- retrieval
- engine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of designing a unified search
  engine to serve multiple retrieval-augmented generation (RAG) agents, each with
  different tasks, large language models, and RAG strategies. The authors propose
  an iterative approach using an expectation-maximization algorithm to optimize the
  search engine based on feedback from RAG agents, both in offline and online settings.
---

# Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization

## Quick Facts
- arXiv ID: 2410.09942
- Source URL: https://arxiv.org/abs/2410.09942
- Reference count: 40
- Key outcome: IUM improves average performance across 18 RAG models from 60.85 to 61.59

## Executive Summary
This paper addresses the challenge of designing a unified search engine to serve multiple retrieval-augmented generation (RAG) agents with different tasks, large language models, and RAG strategies. The authors propose an iterative approach using an expectation-maximization algorithm to optimize the search engine based on feedback from RAG agents, both in offline and online settings. The method, called Iterative Utility Maximization (IUM), aims to maximize each agent's utility function by personalizing retrieval results. Experiments on datasets from the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrate that IUM significantly outperforms baselines across 18 RAG models.

## Method Summary
The method uses an iterative expectation-maximization algorithm to optimize a search engine for multiple RAG agents. In the offline phase, the search engine retrieves documents for training queries from all agents, collects per-document feedback, and updates the reranker parameters using an EM framework. In the online phase, the search engine serves queries in batches, collects feedback from individual agents, and updates parameters per agent. The reranker uses a cross-encoder architecture that takes as input the query, document, and agent identifiers (task ID and model ID) to personalize retrieval results for each agent's specific information needs.

## Key Results
- IUM achieves average performance of 61.59 compared to 60.85 for baselines across 18 RAG models
- Personalization via task and model IDs improves retrieval effectiveness for agents with similar tasks but different LLMs
- Online IUM with batch feedback collection (batch size b) improves serving-time performance while balancing update frequency and noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative feedback from multiple RAG agents improves retrieval ranking for each agent's specific needs.
- Mechanism: The EM algorithm alternates between collecting feedback (E-step) and optimizing the retrieval model (M-step). In each iteration, the search engine retrieves documents for each agent's training queries, collects per-document feedback, and uses this to refine the ranking model. This process progressively personalizes retrieval by adapting to each agent's utility function.
- Core assumption: Feedback from agents is informative and can guide optimization; agents provide feedback on a per-document basis that correlates with utility.
- Evidence anchors:
  - [abstract] "This feedback is then used to iteratively optimize the search engine using an expectation-maximization algorithm, with the goal of maximizing each agent's utility function."
  - [section] "In each iteration, the search engine applies the parameters optimized from the previous feedback round to retrieve new documents tailored to the specific information needs of each RAG agent."
- Break condition: Feedback quality degrades (e.g., becomes noisy or uninformative), causing the model to overfit to irrelevant signals rather than improving personalization.

### Mechanism 2
- Claim: Personalization via task ID and model ID enables the search engine to distinguish between agents with similar tasks but different LLMs.
- Mechanism: The search engine incorporates task ID (tid) and model ID (mid) as input features to the reranking model. This allows the model to learn distinct preferences for each agent, including both general task preferences and specific LLM-based preferences. Without these IDs, the model defaults to a one-size-fits-all approach that fails to adapt to individual agents.
- Core assumption: Agents with the same task but different LLMs have different information needs that can be captured by these IDs.
- Evidence anchors:
  - [section] "Each agent provides its underlying LLM architecture and the task it is performing, represented by a task ID (tid) and a model ID (mid)."
  - [section] "The search engine uses these identifiers as input and, when optimized with feedback from each user, learns both the specific preferences of individual users (presence of tid and mid together in input) and the general information needs associated with each task (tid) and architecture (mid)."
- Break condition: If task IDs and model IDs are not representative or if agents' preferences cannot be captured by these discrete categories, personalization will fail.

### Mechanism 3
- Claim: Online IUM with batch feedback collection improves serving-time performance by balancing update frequency and noise.
- Mechanism: During serving, the search engine processes queries in batches (size b), collects feedback, and updates parameters after each batch. This approach avoids the noise of per-query updates while still adapting to agents' preferences during serving. The batch size hyperparameter b controls the trade-off between responsiveness and stability.
- Core assumption: Feedback is sufficiently stable within a batch to enable meaningful parameter updates.
- Evidence anchors:
  - [section] "We adopt a middle-ground approach and apply the same method as described in Section 4.1 to optimize the model over several iterations. We define an iteration as serving b consecutive queries and receiving the corresponding feedback."
  - [section] "While the optimal batch size might vary depending on the agent, the general trend suggests that increasing the batch size improves performance up to a certain point, beyond which performance starts to decline."
- Break condition: If feedback varies too much between queries within a batch, updates will be unstable and degrade performance.

## Foundational Learning

- Concept: Expectation-Maximization (EM) algorithm
  - Why needed here: The iterative optimization framework relies on EM to alternate between collecting feedback (E-step) and optimizing the retrieval model (M-step).
  - Quick check question: What are the two main steps in the EM algorithm, and how do they apply to optimizing the search engine?

- Concept: Pointwise ranking objective
  - Why needed here: The paper converts the list-wise ranking problem into a pointwise classification problem by treating feedback as relevance labels and optimizing document-level probabilities.
  - Quick check question: How does the pointwise ranking objective differ from list-wise ranking, and why is it used here?

- Concept: Kullback-Leibler divergence
  - Why needed here: KL divergence measures the difference between the variational distribution q and the true posterior, forming the basis of the ELBO objective.
  - Quick check question: What role does KL divergence play in the ELBO formulation, and why is it important for the optimization?

## Architecture Onboarding

- Component map:
  BM25 (first-stage retriever) → Cross-encoder reranker → RAG agents → Feedback collection → EM optimization
  Personalization inputs: task ID, model ID
  Optimization phases: Offline IUM (training) and Online IUM (serving)

- Critical path:
  1. Retrieve 100 documents using BM25
  2. Rerank using cross-encoder with task/model IDs
  3. Serve to RAG agents
  4. Collect per-document feedback
  5. Update reranker parameters via EM
  6. Repeat for iterations (offline) or batches (online)

- Design tradeoffs:
  - Batch size b in online learning: smaller batches allow faster adaptation but increase noise; larger batches provide stability but reduce responsiveness
  - Personalization granularity: task-level vs. agent-level personalization affects model complexity and generalization
  - Feedback quality: noisy feedback can degrade optimization; mechanisms to filter or weight feedback may be needed

- Failure signatures:
  - Performance degradation with additional iterations (indicates overfitting to feedback)
  - Inconsistent feedback across agents (suggests personalization is not working)
  - High variance in batch updates (indicates batch size is too small or feedback is too noisy)

- First 3 experiments:
  1. Verify EM algorithm implementation by running a single iteration and checking that feedback collection and parameter updates occur as expected
  2. Test personalization by training with and without task/model IDs and comparing retrieval similarity between agents
  3. Validate online learning by serving queries in batches and monitoring parameter stability and performance improvement across batches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Iterative Utility Maximization (IUM) scale with the number of RAG agents it serves simultaneously?
- Basis in paper: [explicit] The paper mentions serving multiple RAG agents but doesn't explore performance degradation or scalability limits as the number of agents increases.
- Why unresolved: The experiments used a fixed set of 18 RAG agents without testing how the system performs with larger numbers of diverse agents.
- What evidence would resolve it: Systematic experiments varying the number of RAG agents from 1 to 100+ while measuring average performance, training time, and memory usage would reveal scalability limits.

### Open Question 2
- Question: Can IUM effectively handle RAG agents with conflicting information needs that cannot be simultaneously optimized?
- Basis in paper: [inferred] The paper assumes all agents can be optimized through a single search engine, but doesn't address potential conflicts when agents require mutually exclusive retrieval strategies.
- Why unresolved: The experiments used complementary datasets and tasks, avoiding scenarios where one agent's optimal retrieval would harm another's performance.
- What evidence would resolve it: Testing IUM with agents having contradictory information needs (e.g., one agent requiring highly specific documents while another needs broad overviews) would reveal whether the framework can handle such conflicts.

### Open Question 3
- Question: How sensitive is IUM to the quality and quantity of initial feedback collected from RAG agents?
- Basis in paper: [explicit] The paper mentions that using untrained retrieval models for initial feedback collection is suboptimal, but doesn't systematically study the impact of feedback quality on final performance.
- Why unresolved: While the paper notes this limitation, it doesn't provide quantitative analysis of how different initial feedback scenarios affect convergence and final performance.
- What evidence would resolve it: Experiments varying initial feedback quality (e.g., simulated poor vs. good initial retrieval) and quantity (different amounts of initial feedback) while measuring impact on convergence speed and final performance would reveal sensitivity to initialization conditions.

## Limitations
- The paper doesn't specify the exact mechanism for converting RAG agent utility outputs into per-document feedback scores
- All experiments use the same BM25 + BERT reranker pipeline, limiting generalizability to other retrieval architectures
- The paper doesn't provide systematic analysis of optimal batch sizes across different agent types or task complexities

## Confidence
- High confidence: The core EM-based iterative optimization framework is well-specified and theoretically sound
- Medium confidence: The personalization mechanism using task and model IDs is plausible given experimental results
- Low confidence: The practical effectiveness of online IUM in real-world scenarios with noisy, heterogeneous feedback

## Next Checks
1. **Feedback granularity validation**: Implement a controlled experiment where feedback is collected at different granularities (per-query vs. per-document) to determine the optimal feedback structure for the EM algorithm.
2. **Architecture generalization test**: Apply IUM to a different retrieval pipeline (e.g., dense retriever + cross-encoder) to verify the method's effectiveness across architectures beyond BM25 + BERT.
3. **Online learning stress test**: Simulate a multi-agent serving environment with varying query loads and feedback quality to identify failure modes in the online IUM approach, particularly around batch size selection and feedback noise handling.