---
ver: rpa2
title: 'GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal
  LLMs'
arxiv_id: '2406.13246'
source_url: https://arxiv.org/abs/2406.13246
tags:
- spatial
- arxiv
- reasoning
- what
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the What'sUp dataset to create GSR-BENCH, a benchmark
  for evaluating grounded spatial reasoning in multimodal large language models (MLLMs).
  The dataset is enriched with bounding box annotations, segmentation masks, and depth
  information for more rigorous assessment.
---

# GSR-BENCH: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs

## Quick Facts
- arXiv ID: 2406.13246
- Source URL: https://arxiv.org/abs/2406.13246
- Authors: Navid Rajabi; Jana Kosecka
- Reference count: 36
- Primary result: MLLMs significantly outperform VLMs in spatial reasoning with best model achieving 86.1% accuracy

## Executive Summary
This work extends the What'sUp dataset to create GSR-BENCH, a comprehensive benchmark for evaluating grounded spatial reasoning in multimodal large language models. The benchmark enriches the original dataset with bounding box annotations, segmentation masks, and depth information, enabling more rigorous assessment of both spatial reasoning and object grounding capabilities. The study evaluates 27 models, including 18 vision-language models and 9 multimodal LLMs, using multiple prompting strategies across 4,958 image instances.

The experimental results reveal a strong positive correlation between model size and spatial reasoning accuracy, with MLLMs significantly outperforming VLMs. The best-performing model achieved 86.1% accuracy, while depth information proved particularly helpful for disambiguating "behind" and "in front of" relationships. The study also identified challenges with small object grounding and observed saturation in reasoning capabilities at larger model scales, highlighting important considerations for future spatial reasoning research.

## Method Summary
The GSR-BENCH methodology extends the What'sUp dataset with GroundingDINO for bounding box detection, SAM for segmentation masks, and ZoeDepth for depth estimation. The benchmark evaluates 27 models (18 VLMs and 9 MLLMs) across 4,958 image instances using template-based generation and multiple-choice prompting strategies. Spatial reasoning accuracy is measured via CircularEval (requiring correct answers across all four prompt permutations), while grounding accuracy uses IoU ≥ 0.5 between predicted and ground-truth bounding boxes. Depth-augmented prompting is specifically applied to behind/in front of cases to leverage depth information for disambiguation.

## Key Results
- MLLMs significantly outperform VLMs in spatial reasoning, with the best model achieving 86.1% accuracy
- Positive correlation between model size and reasoning accuracy, with saturation observed at larger scales (QWEN-1.5-110B)
- Depth information effectively disambiguates "behind" and "in front of" relationships but can introduce bias when multiple relationships hold
- Template-based generation with CircularEval reduces prompt ordering bias compared to standard multiple-choice prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatial reasoning performance scales with model size due to increased parameter capacity enabling richer spatial representations
- Mechanism: Larger models can learn more complex spatial relationships and maintain better object grounding through higher dimensional feature spaces
- Core assumption: Model architecture remains consistent while scaling parameters
- Evidence anchors:
  - [abstract] "positive correlation between model size and reasoning accuracy"
  - [section] "positive correlation, even stronger in grounding, between scaling the LLM size & visual resolution, and the overall accuracy"
  - [corpus] "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding" (TangramPuzzle paper)
- Break condition: Saturation observed in QWEN-1.5-110B suggests scaling alone eventually yields diminishing returns without architectural improvements

### Mechanism 2
- Claim: Depth information improves spatial reasoning accuracy for "behind" and "in front of" relationships
- Mechanism: Depth cues disambiguate relative positioning when 2D visual features are ambiguous, particularly for objects on tabletop scenes
- Core assumption: Depth estimation quality is sufficient to provide meaningful spatial constraints
- Evidence anchors:
  - [abstract] "knowledge of scene depth helps to disambiguate certain relationships (e.g., 'in front of' or 'behind')"
  - [section] "incorporating depth could make the model's decision biased towards the in front of preposition"
  - [corpus] "We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning" (SAVVY paper shows spatial reasoning extends to depth)
- Break condition: When multiple spatial relationships hold simultaneously, depth hints may introduce bias rather than clarity

### Mechanism 3
- Claim: Template-based generation with CircularEval reduces bias compared to standard multiple-choice prompting
- Mechanism: CircularEval requires correct answers across all permutations, preventing models from exploiting positional biases in option ordering
- Core assumption: Model behavior is consistent across prompt permutations
- Evidence anchors:
  - [abstract] "We ran each prompt with 4 different permutations so as to vary the position of the answer among the choices"
  - [section] "This bias and sensitivity turned out to be even more detrimental in smaller models"
  - [corpus] "Strengthened symbol binding makes large language models reliable multiple-choice selectors" (shows prompt sensitivity issues exist)
- Break condition: When models develop systematic biases independent of prompt ordering, or when semantic content varies significantly across permutations

## Foundational Learning

- Concept: Object detection and segmentation fundamentals
  - Why needed here: GSR-BENCH requires accurate bounding box and mask annotations for grounding evaluation
  - Quick check question: Can you explain the difference between object detection and instance segmentation, and why both are needed for spatial reasoning?

- Concept: Spatial relationship primitives and compositional reasoning
  - Why needed here: The benchmark evaluates specific spatial prepositions (on, under, behind, etc.) that require compositional understanding
  - Quick check question: How would you represent the spatial relationship "A is to the left of B" using coordinate geometry, and what ambiguities might arise?

- Concept: Depth estimation and metric space interpretation
  - Why needed here: Depth information is used to disambiguate front/behind relationships in the augmented benchmark
  - Quick check question: Given depth values for two objects, how do you determine which is "in front of" the other, considering camera perspective?

## Architecture Onboarding

- Component map:
  Image preprocessing pipeline → Object detector (GroundingDINO) → Segmenter (SAM) → Depth estimator (ZoeDepth) → Annotation storage
  Prompt generator → Model inference → Evaluation metrics (IoU, CircularEval accuracy) → Results aggregation

- Critical path:
  Input image → Object detection → Bounding box generation → Prompt creation → Model inference → Answer evaluation → Grounding accuracy calculation

- Design tradeoffs:
  High-resolution images improve spatial reasoning but increase computational cost and latency
  Complex prompting strategies improve accuracy but reduce prompt engineering efficiency
  Multiple model evaluations provide comprehensive benchmarking but require significant computational resources

- Failure signatures:
  Low IoU scores indicate poor grounding capability
  High variance across prompt permutations suggests sensitivity to input formatting
  Performance degradation on small objects indicates limitations in spatial resolution handling

- First 3 experiments:
  1. Test grounding accuracy on Subset A with LLaVA-NeXT-8B using default prompt format
  2. Evaluate depth-augmented prompting on behind/in front of cases in Subset B
  3. Compare CircularEval vs VanillaEval across all model sizes to quantify bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise scaling relationship between model size and spatial reasoning performance beyond the observed plateau in QWEN-1.5-110B?
- Basis in paper: [explicit] The paper notes a plateau in reasoning performance for QWEN-1.5-110B, the largest tested model, while grounding continues to improve with scaling.
- Why unresolved: The study only tested up to 110B parameters and observed diminishing returns in reasoning capabilities, suggesting potential saturation that needs further exploration.
- What evidence would resolve it: Testing models significantly larger than 110B parameters (e.g., 200B+) to determine if the plateau persists or if reasoning performance eventually scales again with size.

### Open Question 2
- Question: How do different training/instruction-tuning methodologies affect spatial reasoning performance independent of model size?
- Basis in paper: [explicit] The paper notes that various models with different training methods (LLaVA, LLaVA-NeXT, Intern-VL) show performance differences that cannot be fully explained by parameter count alone.
- Why unresolved: The study acknowledges confounding factors between architecture, training methodology, and release date, but does not isolate the impact of training approaches on spatial reasoning capabilities.
- What evidence would resolve it: Controlled experiments training identical architectures with different instruction-tuning approaches on the same spatial reasoning datasets to measure performance differences.

### Open Question 3
- Question: What is the optimal prompting strategy for spatial reasoning that balances accuracy with robustness to option ordering?
- Basis in paper: [explicit] The paper found significant sensitivity to choice ordering in MC prompting, especially in smaller models, leading them to adopt template-based generation with CircularEval.
- Why unresolved: While the study identified this sensitivity issue, it did not systematically explore alternative prompting strategies or their trade-offs in terms of accuracy, robustness, and computational efficiency.
- What evidence would resolve it: Comparative analysis of multiple prompting strategies (including chain-of-thought, few-shot examples, and different answer formats) across various model sizes to identify optimal approaches.

## Limitations
- Dataset bias concerns: The What'sUp dataset extension may inherit inherent biases from its original construction, potentially limiting generalizability
- Depth estimation quality: Reliance on ZoeDepth means accuracy depends on depth estimation quality, which could systematically bias results
- Model diversity limitations: The evaluation includes 27 models but doesn't explore intermediate architectures or alternative training paradigms

## Confidence
**High Confidence**: The positive correlation between model size and spatial reasoning accuracy is well-supported by empirical results across multiple model families and size ranges.

**Medium Confidence**: The effectiveness of depth-augmented prompting for disambiguating spatial relationships is supported by experimental results but may be influenced by depth estimation quality.

**Low Confidence**: The claim that template-based generation with CircularEval significantly reduces bias compared to standard multiple-choice prompting is based on observed performance differences, but underlying mechanisms are not fully explored.

## Next Checks
- Validation Check 1: Replicate depth-augmented prompting experiments using ground-truth depth information rather than estimated depth to isolate depth quality impact
- Validation Check 2: Conduct ablation studies on prompt permutations by systematically varying the number of permutations and analyzing variance in model responses
- Validation Check 3: Extend benchmarking to include models with alternative architectural designs (region-based approaches or explicit spatial reasoning modules) to determine if observed size-scaling trends hold across different paradigms