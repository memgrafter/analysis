---
ver: rpa2
title: Exploring Forgetting in Large Language Model Pre-Training
arxiv_id: '2410.17018'
source_url: https://arxiv.org/abs/2410.17018
tags:
- forgetting
- pre-training
- replay
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores catastrophic forgetting during pre-training\
  \ of large language models, a phenomenon that has been widely observed but never\
  \ systematically studied. The authors identify key limitations of traditional metrics\
  \ like perplexity and M(f) in capturing forgetting and introduce new entity-focused\
  \ metrics\u2014Min and Mex\u2014that better reflect knowledge retention and degradation."
---

# Exploring Forgetting in Large Language Model Pre-Training

## Quick Facts
- **arXiv ID**: 2410.17018
- **Source URL**: https://arxiv.org/abs/2410.17018
- **Reference count**: 28
- **Primary result**: Memory replay strategies effectively mitigate catastrophic forgetting during LLM pre-training, with entity-focused metrics revealing knowledge degradation masked by traditional metrics.

## Executive Summary
This paper systematically investigates catastrophic forgetting during large language model pre-training, a phenomenon that has been widely observed but never thoroughly studied. The authors identify key limitations of traditional metrics like perplexity and M(f) in capturing forgetting and introduce new entity-focused metrics—Min and Mex—that better reflect knowledge retention and degradation. Through experiments on GPT-2 models, they demonstrate that simple memory replay strategies can effectively mitigate forgetting, with periodic, high-intensity replay methods showing particular promise. The study also reveals that forgetting curves in LLMs resemble human learning patterns, with higher initial learning intensity leading to better long-term retention.

## Method Summary
The research employs GPT-2 models (0.1B and 1.5B parameters) pre-trained on datasets including SlimPajama, OpenWebText, and English Wikipedia. The authors introduce new entity-focused metrics (Min, Mex, PPLent, M(f)ent) alongside traditional perplexity to evaluate forgetting. Memory replay strategies are implemented using BM25-based similarity retrieval and Intensive Focused Stochasticity with periodic, high-intensity replay. The methodology involves segmenting training data to observe forgetting patterns, implementing replay mechanisms during pre-training, and evaluating performance on entity-related tasks and benchmark datasets like Hellaswag, MMLU, and Winograd.

## Key Results
- Traditional metrics like perplexity mask true forgetting during pre-training, while entity-focused metrics (Min, Mex) reveal knowledge degradation
- Memory replay strategies effectively mitigate forgetting, with periodic, high-intensity replay showing the most promise
- Forgetting curves in LLMs resemble human learning patterns, where higher initial learning intensity leads to better long-term retention
- Replay methods improve performance on entity-related tasks, with Intensive Focused Stochasticity outperforming standard pre-training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Traditional metrics like perplexity and M(f) fail to capture forgetting during pre-training because they average over all tokens, masking losses for rare entities.
- **Mechanism**: Entity-focused metrics (Min, Mex) directly evaluate the model's ability to recall specific factual details, making forgetting more visible.
- **Core assumption**: Forgetting is most noticeable for rare, entity-related information rather than common tokens.
- **Evidence anchors**:
  - [abstract] "questioning traditional metrics such as perplexity (PPL) and introducing new metrics to better detect entity memory retention"
  - [section 3.2] "Takeaway 1: PPL and M(f) metrics potentially mask true forgetting, as their bias towards easy-to-remember elements can underestimate the model's memory decline across dataset shifts."
  - [corpus] Weak: No direct corpus evidence for this mechanism; relies on internal experiment findings.
- **Break condition**: If entity-related metrics also show stable performance over training, the assumption about rare token sensitivity would be invalid.

### Mechanism 2
- **Claim**: Memory replay methods can mitigate forgetting by periodically re-exposing the model to previously seen samples.
- **Mechanism**: Storing and replaying samples with high similarity or containing entities helps reinforce their retention.
- **Core assumption**: Re-learning previously seen data prevents degradation of knowledge related to those samples.
- **Evidence anchors**:
  - [abstract] "simple memory replay strategies can effectively mitigate forgetting, with periodic, high-intensity replay methods showing particular promise."
  - [section 5.3] "Our memory replay methods show potential in alleviating forgetting in the pre-training phase, while a gap persists relative to the upper bound, signifying the necessity for further research."
  - [corpus] Moderate: Related work on memory replay exists but specific application to pre-training forgetting is novel here.
- **Break condition**: If replay increases training cost disproportionately without improving retention metrics, the method becomes impractical.

### Mechanism 3
- **Claim**: Higher initial learning intensity leads to better long-term retention, mirroring human forgetting curves.
- **Mechanism**: Intensive, short-term learning creates stronger memory traces that decay more slowly during subsequent training.
- **Core assumption**: The model's learning dynamics follow patterns similar to human memory, where effortful initial learning improves durability.
- **Evidence anchors**:
  - [abstract] "forgetting curves in LLMs resemble human learning patterns, with higher initial learning intensity leading to better long-term retention."
  - [section 6.2.1] "a significant decline is still observed even when the dataset used for subsequent training is identical... higher initial learning intensity results in better performance across various metrics"
  - [corpus] Moderate: Human forgetting curve literature exists but direct comparison to LLMs is novel.
- **Break condition**: If increased initial intensity does not translate to improved long-term retention metrics, the human analogy breaks down.

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why models lose previously learned information is essential for interpreting the paper's findings.
  - Quick check question: What is the primary cause of catastrophic forgetting in neural networks during sequential training?

- **Concept**: Perplexity as a language model metric
  - Why needed here: The paper critiques traditional perplexity for failing to detect forgetting, requiring understanding of how it works.
  - Quick check question: How does perplexity measure model performance, and why might it mask forgetting for common tokens?

- **Concept**: Entity recognition and factual knowledge in LLMs
  - Why needed here: The paper focuses on entity-related forgetting, requiring understanding of how LLMs store and retrieve factual information.
  - Quick check question: What distinguishes entity-related information from other types of knowledge in language models?

## Architecture Onboarding

- **Component map**: Data pipeline → Model training → Metric evaluation → Replay mechanism
- **Critical path**: Pre-training → Entity-focused metric evaluation → Memory replay implementation → Forgetting curve analysis
- **Design tradeoffs**: Simple replay vs. computational cost; entity-focused metrics vs. general applicability
- **Failure signatures**: Stable perplexity despite knowledge loss; recovery of simple metrics during training
- **First 3 experiments**:
  1. Replicate perplexity analysis on segmented training data to observe stability
  2. Implement Min and Mex metrics on entity-rich samples
  3. Test basic memory replay with similarity-based sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of memory replay strategies scale with model size and pre-training data volume?
- Basis in paper: [inferred] The paper acknowledges computational limitations, noting that experiments were conducted on GPT-2 (0.1B) rather than larger models like GPT-2 XL (1.5B) due to resource constraints. It also mentions that scaling laws suggest smaller model findings may inform larger-scale experiments.
- Why unresolved: The study was constrained to smaller models and datasets due to computational costs, leaving uncertainty about how these findings generalize to larger models and more extensive pre-training data.
- What evidence would resolve it: Experiments demonstrating the performance of memory replay strategies across a range of model sizes (e.g., 1B, 10B, 100B parameters) and diverse, large-scale datasets would clarify scalability and effectiveness.

### Open Question 2
- Question: What is the optimal balance between replay frequency and intensity to minimize forgetting without compromising generalization to downstream tasks?
- Basis in paper: [explicit] The paper explores different replay strategies, including frequency (every 100 steps) and intensity (number of epochs per replay batch), and notes that periodic, high-intensity replay can mitigate forgetting. However, it also raises concerns about potential trade-offs affecting generalizability.
- Why unresolved: While the paper shows that replay can reduce forgetting, it does not systematically investigate the trade-off between replay parameters and the model's ability to perform well on diverse downstream tasks.
- What evidence would resolve it: Controlled experiments varying replay frequency and intensity, combined with comprehensive evaluations on a wide range of downstream tasks, would identify the optimal balance for both forgetting mitigation and task performance.

### Open Question 3
- Question: How do entity-focused metrics like Mex and Min compare to traditional metrics in detecting forgetting in task-specific fine-tuning scenarios?
- Basis in paper: [explicit] The paper introduces Mex and Min as more sensitive metrics for detecting forgetting in pre-training, particularly for entity-related information, and contrasts them with traditional metrics like PPL and M(f) that may underestimate forgetting.
- Why unresolved: The paper focuses on pre-training forgetting but does not explore whether these entity-focused metrics are also more effective in detecting forgetting during task-specific fine-tuning, where traditional metrics like task-specific accuracy are typically used.
- What evidence would resolve it: Comparative studies applying Mex and Min alongside traditional task-specific metrics during fine-tuning on various tasks would reveal their relative effectiveness in detecting forgetting in that context.

## Limitations

- Experimental scope limited to small models (0.1B parameters) due to computational constraints, raising questions about scalability
- Memory replay implementations lack optimization for practical deployment at scale
- Focus primarily on factual entity knowledge may overlook other forms of knowledge affected by forgetting

## Confidence

- **High confidence**: The observation that traditional metrics like perplexity mask forgetting during pre-training is well-supported by the experimental data showing stable perplexity despite knowledge loss.
- **Medium confidence**: The effectiveness of memory replay strategies in mitigating forgetting is demonstrated, but the gap to upper-bound performance suggests these methods are not yet fully optimized.
- **Low confidence**: The claim about forgetting curves resembling human learning patterns is based on limited experiments and requires more extensive validation across different model architectures and training scenarios.

## Next Checks

1. **Scale validation**: Replicate the core experiments using a 7B or 13B parameter model to assess whether the forgetting patterns and replay effectiveness scale with model size.
2. **Generalization test**: Apply the memory replay methods to non-entity knowledge (reasoning, style, task-specific capabilities) to determine if the approach generalizes beyond factual recall.
3. **Overhead measurement**: Quantify the computational and memory costs of the proposed replay strategies to establish practical feasibility thresholds for production-scale pre-training.