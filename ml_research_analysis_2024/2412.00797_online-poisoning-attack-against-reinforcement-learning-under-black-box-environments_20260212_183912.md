---
ver: rpa2
title: Online Poisoning Attack Against Reinforcement Learning under Black-box Environments
arxiv_id: '2412.00797'
source_url: https://arxiv.org/abs/2412.00797
tags:
- poisoning
- agent
- attacker
- transition
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of conducting poisoning attacks
  against reinforcement learning agents in black-box environments, where the attacker
  lacks knowledge of environment dynamics. The authors propose an online attack scheme
  capable of manipulating both reward functions and state transitions to steer the
  agent toward a malicious policy.
---

# Online Poisoning Attack Against Reinforcement Learning under Black-box Environments

## Quick Facts
- arXiv ID: 2412.00797
- Source URL: https://arxiv.org/abs/2412.00797
- Reference count: 3
- One-line primary result: Online poisoning attack scheme that manipulates both rewards and state transitions to steer RL agents toward malicious policies in black-box environments

## Executive Summary
This paper presents an online poisoning attack against reinforcement learning agents in black-box environments where the attacker lacks knowledge of environment dynamics. The authors propose a novel attack scheme that manipulates both reward functions and state transitions to steer the agent toward a malicious policy. The poisoning task is formalized as a constrained optimization problem and solved using a stochastic gradient descent algorithm with sample-based gradient approximations. A penalty-based method combined with bilevel reformulation is employed to transform the problem into an unconstrained form while avoiding the double-sampling issue. The approach is validated in a maze environment, demonstrating successful manipulation of the agent's learned policy.

## Method Summary
The authors propose an online poisoning algorithm that manipulates both rewards and transitioned states to steer RL agents toward malicious policies in black-box environments. The method formalizes poisoning as a constrained optimization problem that minimizes deviation from the original MDP while ensuring the agent follows a target policy. The problem is solved using stochastic gradient descent with sample-based gradient approximations, employing a penalty-based method combined with bilevel reformulation to avoid double-sampling issues. The attacker intercepts transition data and replaces rewards and states based on knowledge of reachable sets, enabling real-time policy manipulation during agent-environment interaction.

## Key Results
- Successfully demonstrated online poisoning attack in maze environment
- Agent's policy was manipulated to follow a target path through reward and transition poisoning
- Bilevel optimization approach effectively avoided double-sampling issues

## Why This Works (Mechanism)

### Mechanism 1
The attacker can steer the RL agent toward a malicious policy without knowing environment dynamics by manipulating both rewards and transitioned states. The attacker intercepts transition data ⟨s, a, rs,a, s′⟩ and replaces the reward rs,a with a poisoned reward ¯rs,a and the transitioned state s′ with a poisoned state ¯s′ drawn from the reachable set S′s,a. This shifts the Bellman equation equilibrium and induces a target Q-value function. Core assumption: The attacker knows the reachable set S′s,a for each state-action pair, which can be learned online or through prior interaction.

### Mechanism 2
The bilevel optimization formulation avoids the double-sampling issue that plagues penalty-based methods when approximating gradients in black-box environments. By separating the lower-level problem (finding the poisoned reward ¯r that satisfies the Bellman constraint) from the upper-level problem (optimizing over ¯Q and δ), the method computes gradients without requiring independent samples for transition probability estimation. Core assumption: The lower-level problem has a unique solution given fixed δ and ¯Q, enabling the use of implicit function theorem techniques.

### Mechanism 3
The stochastic gradient descent algorithm with sample-based gradient approximations enables efficient online execution of the poisoning attack during agent-environment interaction. The attacker updates its poisoning strategy (¯r, ¯Q, δ) at each iteration using gradients computed from a batch of transition data collected during agent-environment interaction, without requiring full environment resets. Core assumption: Sufficient transition samples are collected to provide reliable gradient estimates.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and Bellman equations
  - Why needed here: The poisoning attack fundamentally manipulates the Bellman equation to induce target Q-values, requiring understanding of MDP structure and value function updates.
  - Quick check question: What is the Bellman equation for Q-values and how does it relate to state transitions and rewards?

- **Concept**: Bilevel optimization and implicit function theorem
  - Why needed here: The attack formulation converts constrained optimization into bilevel form to avoid double-sampling, requiring knowledge of hierarchical optimization and gradient computation techniques.
  - Quick check question: How does the implicit function theorem enable gradient computation in bilevel optimization when the lower-level solution depends on upper-level variables?

- **Concept**: Stochastic gradient descent and sample-based approximation
  - Why needed here: The online attack requires computing gradients from finite samples rather than full expectations, necessitating understanding of SGD convergence and variance reduction.
  - Quick check question: What are the key differences between exact gradient descent and stochastic gradient descent in terms of convergence properties and sample requirements?

## Architecture Onboarding

- **Component map**: Attacker (manipulates rewards and states) -> RL Agent (updates policy using poisoned data) -> Environment (generates transitions) -> Transition Data Pipeline (feeds poisoned data back to agent)
- **Critical path**: Agent sends action -> Environment generates transition -> Attacker intercepts and poisons -> Agent updates policy -> Agent sends next action
- **Design tradeoffs**: The reachable set assumption enables transition manipulation but may be difficult to learn; bilevel optimization avoids double-sampling but increases computational complexity; online SGD enables real-time attacks but suffers from gradient variance
- **Failure signatures**: Agent policy diverges from target policy despite poisoning; poisoning parameters fail to converge; agent detects anomalies in transition data patterns
- **First 3 experiments**:
  1. Implement the reachable set estimation mechanism and verify it can accurately learn transition possibilities through interaction
  2. Test the transition manipulation scheme (Equation 3) in a simple environment to confirm it can shift transition probabilities as intended
  3. Validate the bilevel optimization implementation on a small-scale problem with known gradients to ensure correct gradient computation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the abstract or conclusion sections.

## Limitations
- The reachable set assumption represents a significant limitation - if this cannot be accurately learned in practice, the transition manipulation mechanism becomes ineffective
- The bilevel optimization approach, while theoretically sound, may suffer from computational overhead that makes real-time online execution challenging in complex environments
- The gradient approximation method relies on sufficient sampling of transition data, which may not be guaranteed in all black-box settings

## Confidence

- **High confidence**: The basic poisoning mechanism (reward manipulation) is well-established in RL literature and the mathematical formulation of the constrained optimization problem is sound.
- **Medium confidence**: The transition manipulation mechanism works as described given the reachable set assumption, but practical estimation of reachable sets in unknown environments remains challenging.
- **Low confidence**: The bilevel optimization approach effectively avoids double-sampling issues in practice, as this requires specific conditions on problem structure that may not hold in all RL poisoning scenarios.

## Next Checks

1. **Reachable Set Estimation**: Implement and test the reachable set learning mechanism in a controlled environment to verify it can accurately learn transition possibilities through interaction without prior knowledge.

2. **Transition Manipulation Effectiveness**: Create a simple MDP where the reachable set is known, then validate that the transition manipulation scheme can actually shift transition probabilities toward the poisoned states as intended by Equation 3.

3. **Bilevel Optimization Stability**: Run the full poisoning algorithm on a small-scale problem with analytically computable gradients to verify that the bilevel reformulation correctly computes gradients and converges to the intended solution without numerical instability.