---
ver: rpa2
title: 'Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language
  Model'
arxiv_id: '2411.04496'
source_url: https://arxiv.org/abs/2411.04496
tags:
- speaker
- skill
- social
- arxiv
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "skill-of-mind," which involves
  interpreting social contexts and selecting appropriate conversational skills for
  more engaging dialogue responses. The authors construct M ULTIFACETED SKILL -OF-M
  IND, a dataset of ~100K conversations annotated with skill-of-mind information including
  social context, explanations, and conversational skills across diverse scenarios.
---

# Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model

## Quick Facts
- arXiv ID: 2411.04496
- Source URL: https://arxiv.org/abs/2411.04496
- Reference count: 22
- One-line primary result: Thanos significantly enhances LLM-based conversational agents' response quality and promotes prosocial behavior through skill-of-mind infusion

## Executive Summary
This paper introduces Thanos, a family of skill-of-mind-infused LLMs (1B, 3B, 8B) designed to improve conversational agents by incorporating social context awareness and appropriate skill selection. The authors construct M ULTIFACETED SKILL -OF-M IND, a dataset of ~100K conversations annotated with skill-of-mind information including social context, explanations, and conversational skills across diverse scenarios. When integrated with existing LLM-based conversational agents, Thanos demonstrates significant improvements in response quality, naturalness, engagingness, and prosocial behavior as validated through human evaluations.

## Method Summary
The method involves constructing a large-scale dataset of conversations annotated with social context information and conversational skills, then fine-tuning LLaMA-3 models using a Chain-of-Thought approach to generate explanations and skills sequentially. The Thanos models are integrated with existing LLM-based agents by using the skill-of-mind generation as augmented input, guiding the response generation process. The approach uses LoRA and FSDP for efficient fine-tuning across multiple model sizes (1B, 3B, 8B).

## Key Results
- Thanos models successfully demonstrate skill-of-mind inference and exhibit strong generalizability across various dialogue scenarios
- Integration with existing LLM-based agents significantly improves response quality metrics in human evaluations
- Thanos promotes prosocial behavior and shows potential for safety detection in dialogue interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skill-of-mind inference acts as a guided prompt for LLM-based conversational agents, improving response quality by narrowing down the response space
- Mechanism: The skill-of-mind process involves interpreting the dialogue context, planning the most appropriate conversational skill, and generating an explanation. This explanation-skill pair is then provided as augmented input to the LLM, which uses this guidance to generate a more contextually appropriate response
- Core assumption: LLMs benefit from explicit guidance in complex social interactions, reducing the one-to-many problem and improving response relevance
- Evidence anchors:
  - [abstract]: "planning appropriate conversational skills, as humans do, is challenging due to the complexity of social dialogue"
  - [section 2.2]: "By introducing guidance based on the concept of skill-of-mind, LLM-based conversational agents can more effectively navigate social interactions"
  - [corpus]: Weak. No direct citations to skill-of-mind as a guiding mechanism, but related work on instruction tuning and prompting exists
- Break condition: If the LLM ignores or misinterprets the skill-of-mind guidance, or if the guidance itself is incorrect or irrelevant to the dialogue context

### Mechanism 2
- Claim: Thanos models, trained on a large, diverse dataset of skill-of-mind annotations, generalize well to unseen dialogue scenarios and improve response quality across various tasks
- Mechanism: Thanos models are fine-tuned on Multifaceted Skill-of-Mind, learning to predict conversational skills and generate explanations for a wide range of dialogue contexts. This training enables them to infer appropriate skills in new situations, acting as a socially-aware layer for existing LLM-based agents
- Core assumption: A diverse training dataset covering multiple conversational skills and social contexts allows the model to generalize to new scenarios
- Evidence anchors:
  - [abstract]: "Using this dataset, we introduce a new family of skill-of-mind-infused LLMs, named Thanos... With extensive experiments, these models successfully demonstrate the skill-of-mind process and exhibit strong generalizability"
  - [section 4.2]: "THANOS performs well in out-of-domain settings... Compared to baselines, THANOS outperforms on the BST, which was not used during its training"
  - [corpus]: Weak. No direct citations to Thanos model performance, but related work on fine-tuning LLMs for specific tasks exists
- Break condition: If the training dataset is not sufficiently diverse or if the model overfits to specific patterns in the training data, leading to poor generalization

### Mechanism 3
- Claim: Incorporating skill-of-mind into LLM-based agents promotes prosocial behavior and improves safety in dialogue interactions
- Mechanism: The skill-of-mind process, guided by a taxonomy of conversational skills including ethics, helpfulness, and harmlessness, encourages the LLM to generate responses that are more considerate and less harmful. This acts as a safety mechanism, filtering out potentially problematic content
- Core assumption: Explicit training on prosocial skills can influence the LLM's behavior and lead to more positive interactions
- Evidence anchors:
  - [abstract]: "we show that Thanos significantly enhances the quality of responses generated by LLM-based conversational agents and promotes prosocial behavior in human evaluations"
  - [section 4.2]: "In the PROSOCIAL DIALOGUE setting, our model achieves significant performance, indicating its potential for safety detection"
  - [corpus]: Weak. No direct citations to prosocial behavior promotion, but related work on safety and ethics in AI exists
- Break condition: If the prosocial skills are not well-defined or if the model prioritizes other skills over prosocial ones in certain contexts

## Foundational Learning

- **Concept**: Social context information (demographics, persona, relationships, etc.) influences conversational skill selection
  - Why needed here: Skill-of-mind relies on understanding the interlocutor and the current situation to choose the most appropriate skill
  - Quick check question: How would the choice of conversational skill differ in a conversation with a close friend versus a professional colleague?

- **Concept**: Hierarchical taxonomy of conversational skills (Interpersonal, Memory & Knowledge Management, Cognitive & Problem-Solving, Communication & Listening, Task-Oriented)
  - Why needed here: The taxonomy provides a structured framework for organizing and selecting from a wide range of conversational skills
  - Quick check question: Can you give an example of a conversational skill that would fall under each of the five main categories?

- **Concept**: Chain-of-Thought fine-tuning approach for sequential generation of explanation and skill
  - Why needed here: This approach allows the model to first generate a rationale for the skill choice, followed by the skill itself, mimicking human reasoning
  - Quick check question: What are the potential benefits and drawbacks of using a Chain-of-Thought approach compared to generating the skill directly?

## Architecture Onboarding

- **Component map**: Thanos model (fine-tuned LLM) -> Skill-of-mind generation (explanation + skill) -> Augmented input to LLM-based agent -> Response generation
- **Critical path**: Thanos model inference (for skill-of-mind) -> LLM-based agent response generation (with augmented input)
- **Design tradeoffs**: Larger Thanos models may provide better performance but increase computational cost; smaller models may be more efficient but sacrifice some accuracy
- **Failure signatures**: Incorrect skill predictions, irrelevant or nonsensical explanations, failure to improve response quality compared to baseline agents
- **First 3 experiments**:
  1. Evaluate Thanos model performance on skill classification and explanation generation tasks using the Multifaceted Skill-of-Mind test set
  2. Integrate Thanos with a baseline LLM-based agent and compare response quality on a held-out dialogue dataset
  3. Conduct human evaluations to assess the naturalness, engagingness, and overall quality of responses generated by the Thanos-enhanced agent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the skill-of-mind concept generalize to languages other than English, given that the current implementation relies heavily on English-language datasets and GPT-4's capabilities?
- Basis in paper: Explicit
- Why unresolved: The paper focuses exclusively on English dialogue datasets and uses GPT-4 (gpt-4-turbo) for annotation and generation. The skill taxonomy and social context templates are all developed in English, with no mention of cross-lingual validation or adaptation strategies
- What evidence would resolve it: Experiments demonstrating equivalent skill-of-mind performance across multiple languages using translated datasets, or a framework for adapting the skill taxonomy to different linguistic and cultural contexts

### Open Question 2
- Question: What is the computational overhead of integrating Thanos into real-time conversational agents, and how does it affect response latency compared to direct LLM generation?
- Basis in paper: Inferred
- Why unresolved: While the paper mentions potential latency improvements by focusing on a single skill-expert agent versus multiple parallel agents, it doesn't provide empirical measurements of the actual computational cost or latency impact of the Thanos skill-of-mind generation step
- What evidence would resolve it: Benchmark studies comparing response times for LLM-only generation versus LLM + Thanos integration across different hardware configurations and model sizes

### Open Question 3
- Question: How does the performance of Thanos change when trained on more diverse or larger-scale datasets beyond the 100K conversations used in this study?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges this as a limitation in the "Extending the Generalizability of Skill-of-Mind" section, noting that additional experiments in varied dialogue scenarios would be needed to verify extensive generalization capabilities
- What evidence would resolve it: Performance comparisons of Thanos models trained on incrementally larger and more diverse datasets, showing scaling trends in skill classification accuracy and explanation quality across different domains

## Limitations

- The skill-of-mind dataset relies heavily on GPT-4 for annotation, raising concerns about potential model bias and the reliability of automatically generated labels
- Human evaluation sample sizes are relatively small (25-30 per condition), which may limit the statistical power of the findings
- The paper lacks thorough error analysis of model failures or systematic investigation of edge cases where the skill-of-mind approach might break down

## Confidence

**High Confidence**: The core methodology of fine-tuning LLMs on annotated dialogue data for skill-of-mind inference is technically sound and well-documented. The architecture and training procedures are clearly specified, and the results demonstrate measurable improvements over baseline models in multiple evaluation settings.

**Medium Confidence**: The claim that skill-of-mind significantly improves response quality and promotes prosocial behavior is supported by the experimental results, but the magnitude of improvement and its generalizability across diverse real-world scenarios remain uncertain. The reliance on automated annotation and the limited diversity of evaluation datasets contribute to this medium confidence rating.

**Low Confidence**: The paper's claims about the model's ability to generalize to completely unseen dialogue scenarios and its effectiveness as a safety mechanism for promoting prosocial behavior are not fully substantiated. The evaluation of prosocial behavior is particularly limited, with no systematic analysis of how the model handles nuanced or challenging social situations.

## Next Checks

1. **Statistical Power Analysis**: Conduct power analysis on human evaluation results to determine if the current sample sizes are sufficient to detect meaningful differences between conditions. Consider increasing sample sizes for key comparisons if statistical power is inadequate.

2. **Bias Audit**: Perform a systematic analysis of potential biases in the GPT-4 annotated dataset by comparing model predictions with human annotations on a stratified sample of dialogues. Investigate whether certain demographic groups or conversation types are systematically misclassified.

3. **Robustness Testing**: Design adversarial test cases that specifically target the skill-of-mind inference mechanism, including scenarios with ambiguous social contexts, conflicting conversational goals, or deliberately misleading dialogue history. Evaluate model performance on these edge cases to identify failure modes and potential improvements.