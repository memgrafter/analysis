---
ver: rpa2
title: Adversarial Attacks on Large Language Models in Medicine
arxiv_id: '2406.12259'
source_url: https://arxiv.org/abs/2406.12259
tags:
- adversarial
- llms
- medical
- data
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that both prompt-based and fine-tuning-based
  adversarial attacks can effectively manipulate large language models (LLMs) in medical
  applications, leading to harmful recommendations such as discouraging COVID-19 vaccination,
  suggesting dangerous drug combinations, and recommending unnecessary diagnostic
  tests. The attacks were tested on both proprietary (GPT-3.5-turbo, GPT-4) and open-source
  (Llama2-7b) models using real-world patient data from MIMIC-III and PMC-Patients
  datasets.
---

# Adversarial Attacks on Large Language Models in Medicine

## Quick Facts
- arXiv ID: 2406.12259
- Source URL: https://arxiv.org/abs/2406.12259
- Authors: Yifan Yang; Qiao Jin; Furong Huang; Zhiyong Lu
- Reference count: 22
- Primary result: Adversarial attacks can manipulate medical LLMs to generate harmful recommendations like discouraging vaccination and suggesting dangerous drug combinations

## Executive Summary
This study demonstrates that large language models in medical applications are vulnerable to both prompt-based and fine-tuning-based adversarial attacks. Using real-world patient data from MIMIC-III and PMC-Patients datasets, the researchers successfully manipulated proprietary models (GPT-3.5-turbo, GPT-4) and open-source models (Llama2-7b) to produce harmful medical recommendations. The attacks achieved significant success rates without substantially degrading overall model performance on standard benchmarks, highlighting a critical security vulnerability in healthcare AI systems.

The research identifies measurable shifts in fine-tuned model weights, particularly in LoRA (Low Rank Adapters) weight norms, which could serve as a detection mechanism for adversarial manipulation. However, the study emphasizes that while the experimental results are concerning, they were conducted in controlled settings rather than deployed clinical systems, and the findings may not generalize to all healthcare contexts or languages.

## Method Summary
The researchers conducted systematic adversarial attacks on medical LLMs using two primary approaches: prompt-based attacks that manipulated input queries and fine-tuning-based attacks that injected adversarial examples during model training. They tested multiple model architectures including GPT-3.5-turbo, GPT-4, and Llama2-7b using patient data from MIMIC-III and PMC-Patients datasets. The attacks targeted medical decision-making scenarios, attempting to generate harmful recommendations such as discouraging COVID-19 vaccination, suggesting dangerous drug combinations, and recommending unnecessary diagnostic tests. Model performance was evaluated using standard medical benchmarks to assess whether adversarial training impacted overall capability.

## Key Results
- Adversarial attacks achieved high success rates in manipulating medical LLMs to generate harmful recommendations across multiple model architectures
- Integration of adversarial data during fine-tuning did not significantly degrade overall model performance on standard medical benchmarks
- Measurable shifts in LoRA weight norms were observed in fine-tuned models, suggesting a potential detection mechanism for adversarial manipulation
- Real-world patient data from MIMIC-III and PMC-Patients datasets were successfully used to demonstrate vulnerabilities in both proprietary and open-source models

## Why This Works (Mechanism)
Adversarial attacks succeed in medical LLMs because these models, despite their sophisticated training on medical literature, lack robust defenses against carefully crafted input manipulations. The attacks exploit the models' pattern-matching capabilities by presenting inputs that trigger undesirable responses while maintaining surface-level plausibility. Fine-tuning-based attacks are particularly effective because they can subtly shift the model's decision boundaries during training, embedding vulnerabilities that persist during inference. The observed weight norm shifts in LoRA adapters occur because adversarial training modifies the low-rank representations that capture task-specific knowledge, creating detectable patterns in the model's learned parameters.

## Foundational Learning

**Medical LLMs and Clinical Decision Support**: Why needed - Understanding how LLMs are deployed in healthcare settings to provide diagnostic and treatment recommendations; Quick check - Review clinical workflows where LLMs assist with patient triage, medication recommendations, and diagnostic suggestions.

**Adversarial Attack Types**: Why needed - Different attack vectors (prompt injection vs. data poisoning) require distinct defense strategies; Quick check - Compare effectiveness of black-box vs. white-box attack methodologies on medical AI systems.

**Model Fine-tuning and LoRA Adapters**: Why needed - Fine-tuning modifies model behavior for specific tasks, creating attack surfaces; Quick check - Analyze how LoRA weight modifications correlate with changes in model output distributions across clinical scenarios.

**Medical Benchmark Evaluation**: Why needed - Standard metrics may not capture subtle harmful outputs in healthcare contexts; Quick check - Assess whether traditional accuracy metrics adequately detect dangerous medical recommendations.

**Patient Data Privacy and Security**: Why needed - Medical data usage raises ethical concerns beyond technical vulnerabilities; Quick check - Evaluate compliance with healthcare data regulations when testing adversarial scenarios.

## Architecture Onboarding

**Component Map**: Patient Data -> Data Preprocessing -> Model Architecture (LLM + LoRA) -> Adversarial Attack Vector -> Output Generation -> Performance Evaluation

**Critical Path**: The attack pipeline flows from data preparation through model inference, with the adversarial vector injection point being critical for successful manipulation. The LoRA fine-tuning stage represents the most vulnerable component where permanent behavioral modifications can be embedded.

**Design Tradeoffs**: The study reveals a fundamental tension between model performance and security - defensive measures against adversarial attacks may impact the model's ability to provide accurate medical recommendations, while maintaining high performance leaves systems vulnerable to manipulation.

**Failure Signatures**: Successful adversarial attacks produce detectable weight norm shifts in LoRA adapters, generate outputs that deviate from standard medical guidelines, and maintain high performance on conventional benchmarks despite harmful behavior modifications.

**3 First Experiments**:
1. Test LoRA weight norm detection across different attack intensities to establish sensitivity thresholds
2. Evaluate model responses to the same medical queries before and after adversarial fine-tuning to identify behavioral drift
3. Assess whether defensive fine-tuning with clean medical data can mitigate adversarial vulnerabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Findings demonstrated in controlled experimental settings rather than deployed clinical systems
- Evaluation focused on English-language medical data, limiting generalizability to other languages and healthcare systems
- Study tested only three specific model families, not smaller clinical AI systems or domain-specific medical models

## Confidence

**High confidence**: Core finding that adversarial attacks can manipulate medical LLMs to generate harmful recommendations is well-supported by systematic testing across multiple attack types and model architectures.

**Medium confidence**: LoRA weight norm shifts serving as a detection mechanism is promising but requires further validation across diverse clinical tasks and attack patterns.

**Medium confidence**: Assertion that adversarial training does not significantly impact overall benchmark performance needs more extensive testing across a broader range of medical evaluation datasets.

## Next Checks

1. Test the proposed LoRA weight norm detection method against a broader range of adversarial attack patterns, including more subtle manipulation techniques that might evade simple weight-based detection.

2. Evaluate the effectiveness of the identified adversarial vulnerabilities in a realistic clinical workflow simulation where multiple safety checks and human oversight are present.

3. Assess whether the observed model behavior changes under adversarial attack persist across different patient demographics and clinical contexts to identify potential biases or differential vulnerabilities.