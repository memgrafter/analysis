---
ver: rpa2
title: 'Retrieval-Augmented Generation for Domain-Specific Question Answering: A Case
  Study on Pittsburgh and CMU'
arxiv_id: '2411.13691'
source_url: https://arxiv.org/abs/2411.13691
tags:
- data
- answer
- documents
- questions
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Retrieval-Augmented Generation (RAG) system
  for answering domain-specific questions about Pittsburgh and Carnegie Mellon University
  (CMU). The authors extracted over 1,800 subpages from relevant websites and created
  a hybrid annotation process combining manual and Mistral-generated question-answer
  pairs, achieving an inter-annotator agreement score of 0.7625.
---

# Retrieval-Augmented Generation for Domain-Specific Question Answering: A Case Study on Pittsburgh and CMU

## Quick Facts
- arXiv ID: 2411.13691
- Source URL: https://arxiv.org/abs/2411.13691
- Reference count: 3
- Primary result: RAG system achieved F1 score of 42.21% compared to 5.45% baseline

## Executive Summary
This paper presents a Retrieval-Augmented Generation (RAG) system for answering domain-specific questions about Pittsburgh and Carnegie Mellon University (CMU). The authors developed a hybrid annotation process combining manual and Mistral-generated question-answer pairs, achieving an inter-annotator agreement score of 0.7625. Their RAG framework integrates BM25 and FAISS retrievers with a reranker for improved document retrieval accuracy. Experimental results demonstrate significant performance improvements over non-RAG baselines, particularly for time-sensitive and complex queries.

## Method Summary
The authors created a RAG system for Pittsburgh/CMU domain question answering through a three-phase approach. First, they extracted over 1,800 subpages from relevant websites using Selenium and BeautifulSoup with BFS strategy. Second, they generated 165 manual and 1,302 Mistral-generated QA pairs with few-shot learning, achieving an IAA score of 0.7625. Third, they built a RAG pipeline using BM25+FAISS retrievers, a sentence-transformers reranker, and Mistral 7B as the generator, evaluating performance on their curated test set.

## Key Results
- F1 score improved from 5.45% (baseline) to 42.21% with RAG system
- Recall achieved 56.18% on the test set
- RAG system demonstrated clear advantage for time-sensitive questions
- IAA score of 0.7625 indicates moderate annotation consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid retriever (BM25 + FAISS) with reranker improves retrieval accuracy by balancing keyword-based precision and vector-based recall.
- Mechanism: BM25 ensures high precision for exact keyword matches, while FAISS captures semantic similarity. The reranker module filters and orders results to remove noise, enhancing the final retrieved documents' relevance.
- Core assumption: The combination of exact match (BM25) and semantic similarity (FAISS) covers both surface and deep document relevance.
- Evidence anchors:
  - [abstract] "Our RAG framework integrates BM25 and FAISS retrievers, enhanced with a reranker for improved document retrieval accuracy."
  - [section] "We choose to combine both BM25 and FAISS retrievers... BM25 helps compensate for this by improving precision."
  - [corpus] Weak or missing: No direct citations or comparisons to single-retriever setups in the corpus.

### Mechanism 2
- Claim: Few-shot learning with manually curated QA pairs significantly boosts the model's ability to generate precise answers.
- Mechanism: The model is first trained on a small set of manually written QA pairs, then applies that knowledge to generate new pairs from scraped content. This stabilizes output quality and reduces hallucinations.
- Core assumption: A small, diverse manually annotated set is enough to guide the model's generalization to the domain.
- Evidence anchors:
  - [abstract] "We employed a hybrid annotation process, combining manual and Mistral-generated question-answer pairs..."
  - [section] "We used Mistral with few-shot learning... allowed the model to generalize from the small number of examples..."
  - [corpus] Weak or missing: No direct comparison of few-shot vs. no-few-shot performance in the corpus.

### Mechanism 3
- Claim: The retrieval-augmented approach particularly benefits time-sensitive and complex queries by grounding responses in retrieved documents.
- Mechanism: For time-sensitive questions, the retriever finds up-to-date or event-specific documents. For complex questions, it retrieves multiple supporting documents, allowing the model to synthesize multi-step answers.
- Core assumption: The retrieved documents contain the necessary context for accurate answers, and the model can effectively leverage them.
- Evidence anchors:
  - [abstract] "Experimental results show that the RAG system significantly outperforms a non-RAG baseline, particularly in time-sensitive and complex queries..."
  - [section] "We categorized the test set into two main types: time-sensitive and non-time-sensitive questions... the RAG model demonstrated a clear advantage in answering time-sensitive questions..."
  - [corpus] Weak or missing: No explicit breakdown of complex vs. simple queries in the corpus.

## Foundational Learning

- Concept: Inter-Annotator Agreement (IAA)
  - Why needed here: To measure the quality and consistency of the annotated QA dataset, especially when combining manual and model-generated pairs.
  - Quick check question: What IAA score was achieved in this study, and what does it indicate about annotation quality?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: To understand how external document retrieval can enhance a language model's answers beyond its pre-trained knowledge.
  - Quick check question: What are the two main components of a RAG system, and how do they interact?

- Concept: BM25 and FAISS retrieval methods
  - Why needed here: To grasp how keyword-based and semantic similarity searches complement each other in the retriever ensemble.
  - Quick check question: What is the primary difference between BM25 and FAISS in terms of document matching?

## Architecture Onboarding

- Component map: Data Extraction (Selenium + BeautifulSoup) -> Data Annotation (Manual + Mistral) -> Vector Database (Chunking + Embedding) -> Retriever (BM25 + FAISS + Reranker) -> Generator (Mistral 7B) -> Answer
- Critical path: Data Extraction → Vector Database Creation → Retriever (BM25+FAISS+Reranker) → Generator (Mistral + few-shot) → Answer
- Design tradeoffs:
  - Precision vs. Recall: BM25 favors precision, FAISS favors recall; ensemble balances both.
  - Latency vs. Accuracy: Reranker improves accuracy but adds latency.
  - Manual vs. Automatic Annotation: Manual ensures quality, automatic ensures scale.
- Failure signatures:
  - Low IAA score → Inconsistent or low-quality annotations.
  - Poor F1/EM scores → Retrieval or generation issues; check retriever relevance and prompt quality.
  - High latency → Oversized retriever ensemble or inefficient reranking.
- First 3 experiments:
  1. Compare single retriever (BM25 vs. FAISS) vs. ensemble on a small QA set to measure precision/recall trade-offs.
  2. Test with and without reranker to quantify its impact on retrieval quality and latency.
  3. Vary chunk size and overlap to find optimal balance between information loss and retrieval granularity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the IAA score of 0.7625 translate to actual answer accuracy in real-world applications?
- Basis in paper: [explicit] The authors report an IAA score of 0.7625 but do not provide a direct correlation between IAA and real-world accuracy
- Why unresolved: The IAA score measures annotator agreement but doesn't necessarily reflect how well the system performs in practical scenarios
- What evidence would resolve it: A study comparing IAA scores with actual system performance metrics across different domains and annotation methods

### Open Question 2
- Question: What is the optimal chunk size and overlap for document chunking in RAG systems?
- Basis in paper: [explicit] The authors use a chunk size of 1000 with 200 overlap but note this was based on experimentation without providing justification
- Why unresolved: The optimal chunking strategy depends on document type, query complexity, and model architecture, yet there's no established methodology
- What evidence would resolve it: Systematic experiments varying chunk sizes and overlaps across different document types and query patterns

### Open Question 3
- Question: How does the RAG system's performance scale with larger document collections?
- Basis in paper: [inferred] The authors only tested on ~1,800 subpages but claim their system could handle domain-specific questions
- Why unresolved: Performance characteristics may change significantly as the document collection grows, affecting retrieval accuracy and computational efficiency
- What evidence would resolve it: Benchmarking the system with progressively larger document collections while monitoring performance metrics

## Limitations
- Relatively small test set size (165 questions) and lack of large-scale ablation study to isolate component contributions
- IAA score of 0.7625 indicates moderate agreement between annotators, suggesting potential inconsistencies in ground truth
- Lack of comparative analysis against state-of-the-art RAG systems on similar domains
- Performance metrics reported only on curated dataset without external validation

## Confidence

- **High Confidence**: The hybrid retriever architecture (BM25 + FAISS) provides measurable improvements in document retrieval accuracy. The experimental methodology and metrics are clearly defined and reproducible.
- **Medium Confidence**: The few-shot learning approach with Mistral significantly improves answer generation quality. The claim about RAG's particular advantage for time-sensitive queries is supported but requires further validation.
- **Low Confidence**: The relative contribution of each component (retriever, reranker, generator) to the overall performance gain. The generalizability of results to other domain-specific applications.

## Next Checks

1. **Ablation Study**: Systematically disable each component (BM25, FAISS, reranker) individually and measure their contribution to F1 score to quantify the marginal benefit of the hybrid approach.

2. **Cross-Domain Testing**: Apply the same RAG framework to a different domain (e.g., healthcare or legal documents) with a similar annotation pipeline to test generalizability of the approach.

3. **Error Analysis**: Categorize incorrect predictions by error type (retrieval failure, generation hallucination, insufficient context) and analyze whether time-sensitive queries indeed have higher error rates than non-time-sensitive ones.