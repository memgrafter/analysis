---
ver: rpa2
title: Explaining Predictive Uncertainty by Exposing Second-Order Effects
arxiv_id: '2401.17441'
source_url: https://arxiv.org/abs/2401.17441
tags:
- uncertainty
- features
- explanation
- data
- predictive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for explaining predictive uncertainty
  in machine learning models by identifying second-order effects involving individual
  features and their interactions. The authors show that predictive uncertainty can
  be expressed as a linear combination of prediction products, which can then be attributed
  to pairs of input features using a covariance-based approach over first-order explanations
  from ensemble models.
---

# Explaining Predictive Uncertainty by Exposing Second-Order Effects

## Quick Facts
- arXiv ID: 2401.17441
- Source URL: https://arxiv.org/abs/2401.17441
- Reference count: 40
- Authors: Florian Bley; Sebastian Lapuschkin; Wojciech Samek; Grégoire Montavon
- One-line primary result: Introduces a method for explaining predictive uncertainty in ML models by identifying second-order effects through covariance computation over first-order explanations from ensemble models.

## Executive Summary
This paper addresses the challenge of explaining predictive uncertainty in machine learning models by identifying second-order effects involving individual features and their interactions. The authors propose a framework that expresses predictive uncertainty as a linear combination of prediction products, which can then be attributed to pairs of input features using a covariance-based approach over first-order explanations from ensemble models. The method, implemented as CovLRP and CovGI, is evaluated on multiple regression datasets and shown to outperform traditional first-order explanation techniques in terms of explanation accuracy measured by area under the flipping curve (AUFC).

## Method Summary
The proposed method explains predictive uncertainty by computing a covariance over first-order explanations (e.g., LRP, GI) for each model in an ensemble. The predictive variance is expressed as a linear combination of prediction products, allowing the attribution of uncertainty to feature interactions through the outer product of individual model attributions. This covariance-based approach generalizes existing attribution techniques, converting them into powerful second-order uncertainty explainers that capture feature interactions relevant to uncertainty.

## Key Results
- CovLRP and CovGI outperform traditional first-order explanation techniques like LRP, GI, and Shapley Values in terms of explanation accuracy measured by AUFC.
- The method successfully identifies underrepresented features in image datasets (CelebA) that, when added to the training set, reduce model uncertainty.
- CovLRP effectively captures feature interactions contributing to electricity price volatility, demonstrating practical applications in time series analysis.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The variance of ensemble predictions can be decomposed into a linear combination of prediction products.
- Mechanism: The authors express predictive uncertainty (variance over ensemble outputs) as a sum of pairwise products of model predictions weighted by coefficients derived from ensemble structure. This allows attributing uncertainty to second-order feature interactions.
- Core assumption: The ensemble's variance can be represented exactly as a linear combination of products of individual model outputs.
- Evidence anchors:
  - [abstract] "predictive uncertainty can be expressed as a linear combination of prediction products"
  - [section 3] "we observe that the predictive variance stated in Eq. (1) can be rewritten as a linear combination of prediction products, i.e. s² = ∑ₘ,ₘ′ bₘ,ₘ′·yₘyₘ′"
- Break condition: If the ensemble variance cannot be expressed as such a linear combination (e.g., non-linear variance structures or non-additive ensemble models), the decomposition fails.

### Mechanism 2
- Claim: Second-order explanations can be computed as a covariance over first-order explanations from each ensemble member.
- Mechanism: The authors show that attributing the product of two model outputs to input features can be done by taking the outer product of their individual first-order attributions. Aggregating these outer products over the ensemble yields a covariance matrix representing feature-feature contributions to uncertainty.
- Core assumption: The outer product of first-order explanations provides a valid second-order attribution for the product of predictions.
- Evidence anchors:
  - [abstract] "Computationally, our method reduces to a simple covariance computation over a collection of first-order explanations"
  - [section 3] "E(s²; xxᵀ) = Covₘ(E(ym;x))"
- Break condition: If the first-order attribution method doesn't preserve relevant properties (e.g., conservation, irrelevance preservation), the covariance-based second-order attribution may produce incorrect results.

### Mechanism 3
- Claim: The method generalizes existing attribution techniques by converting them into uncertainty explainers through covariance computation.
- Mechanism: By applying the covariance framework to existing methods like LRP or Gradient×Input, the authors create new uncertainty-specific explainers (CovLRP, CovGI) that capture feature interactions relevant to uncertainty.
- Core assumption: Any first-order attribution method can be plugged into the covariance framework to produce valid second-order uncertainty explanations.
- Evidence anchors:
  - [abstract] "our method is generally applicable, allowing for turning common attribution techniques (LRP, Gradient×Input, etc.) into powerful second-order uncertainty explainers"
  - [section 3] "our framework lets the user choose which first-order explanation method to use inside Eq. (5)"
- Break condition: If the underlying attribution method is not suitable for the model architecture or produces unreliable attributions, the resulting uncertainty explainer will be flawed.

## Foundational Learning

- Concept: Ensemble variance decomposition
  - Why needed here: Understanding how predictive uncertainty (model variance) can be expressed as a function of individual model predictions is fundamental to the proposed method.
  - Quick check question: Can you derive the expression for variance of ensemble predictions as a linear combination of prediction products?

- Concept: Covariance-based attribution
  - Why needed here: The method relies on computing covariance over first-order explanations to capture feature interactions contributing to uncertainty.
  - Quick check question: How does computing covariance over multiple explanations differ from simply averaging them?

- Concept: First-order attribution methods (LRP, Gradient×Input)
  - Why needed here: The proposed method builds upon existing first-order attribution techniques to create second-order uncertainty explainers.
  - Quick check question: What are the key properties (conservation, irrelevance preservation) that a good first-order attribution method should satisfy?

## Architecture Onboarding

- Component map: Ensemble models -> First-order attribution module -> Covariance computation module -> Summarization layer
- Critical path:
  1. Generate predictions from ensemble models
  2. Compute first-order attributions for each prediction
  3. Calculate covariance over the first-order attributions
  4. Summarize the d×d explanation matrix into d-dimensional heatmaps
- Design tradeoffs:
  - Choice of first-order attribution method: LRP offers robustness but requires computational graph, while Gradient×Input is simpler but potentially less stable
  - Explanation summarization: Diagonal terms provide global attribution but miss interactions, while marginalization includes interactions but may be noisier
  - Ensemble size: Larger ensembles provide more stable covariance estimates but increase computation time
- Failure signatures:
  - Explanations dominated by noise: Indicates insufficient ensemble size or unstable first-order attributions
  - All-zero explanations: Suggests either all features are irrelevant or the attribution method is not capturing relevant patterns
  - Unexpectedly high uncertainty attribution to irrelevant features: Points to issues with the first-order attribution method or model architecture
- First 3 experiments:
  1. Apply CovLRP with diagonal summarization to a simple ensemble of linear models on UCI datasets and compare with first-order baselines using AUFC
  2. Test CovGI on a deep ensemble with MC dropout and evaluate explanation quality on high-uncertainty test samples
  3. Use CovLRP to analyze uncertainty in an image classification ensemble before and after fine-tuning on underrepresented features (similar to CelebA use case)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several potential open questions emerge:

### Open Question 1
- Question: How do second-order explanations (like CovLRP) perform on high-dimensional data compared to first-order methods?
- Basis in paper: [inferred] The paper demonstrates superior performance of CovLRP on tabular datasets with moderate dimensions, but does not explore high-dimensional data like images or text.
- Why unresolved: High-dimensional data introduces curse of dimensionality, potentially amplifying noise in covariance estimates and making second-order effects harder to attribute reliably.
- What evidence would resolve it: A systematic evaluation of CovLRP vs. first-order methods on diverse high-dimensional datasets, measuring explanation accuracy and computational efficiency.

### Open Question 2
- Question: Can the proposed framework be extended to explain uncertainty in non-ensemble models like Bayesian neural networks?
- Basis in paper: [explicit] The paper focuses on ensemble models and mentions potential extension to Mixture Density Networks but does not provide details.
- Why unresolved: Bayesian neural networks have different uncertainty representations (e.g., posterior distributions) that may require modifications to the second-order attribution framework.
- What evidence would resolve it: A derivation and implementation of the covariance-based approach for Bayesian neural networks, validated on benchmark datasets.

### Open Question 3
- Question: What is the impact of the choice of first-order attribution method on the quality of second-order uncertainty explanations?
- Basis in paper: [explicit] The paper uses LRP and GI as examples but does not systematically compare different first-order methods within the Cov framework.
- Why unresolved: Different attribution methods have varying properties (e.g., sensitivity to noise, interpretability) that could affect the robustness and faithfulness of the resulting second-order explanations.
- What evidence would resolve it: A comprehensive study comparing CovLRP, CovGI, and other Cov variants on multiple datasets, using quantitative metrics like AUFC and qualitative assessments.

## Limitations
- The method's performance depends heavily on the quality of first-order attribution methods, which can be sensitive to model architecture and implementation details.
- The strong assumption that ensemble variance can be expressed as a linear combination of prediction products may not hold for non-additive ensemble methods or complex model architectures.
- The computational cost of computing explanations for large ensembles could be prohibitive for real-time applications.

## Confidence
- **High confidence**: The mathematical framework for decomposing ensemble variance and the covariance-based attribution method are well-founded and rigorously derived.
- **Medium confidence**: The empirical evaluation shows promising results, but the comparison is limited to specific datasets and baselines. The method's generalizability across different domains and model architectures requires further validation.
- **Medium confidence**: The use cases (underrepresented features in CelebA, electricity price volatility) demonstrate practical applications but lack quantitative evaluation of the explanations' impact on model improvement or decision-making.

## Next Checks
1. Test the method's robustness across diverse model architectures (CNNs, transformers) and ensemble types (bagging, boosting) to assess generalizability beyond MLPs with MC dropout.

2. Conduct ablation studies to quantify the contribution of second-order effects versus first-order explanations in reducing predictive uncertainty, establishing when the added complexity is justified.

3. Implement a user study with domain experts to evaluate whether the second-order explanations provided by CovLRP/CovGI lead to actionable insights for uncertainty reduction, moving beyond AUFC metrics to real-world impact assessment.