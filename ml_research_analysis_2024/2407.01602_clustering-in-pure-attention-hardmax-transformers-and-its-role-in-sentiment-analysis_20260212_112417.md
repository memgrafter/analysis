---
ver: rpa2
title: Clustering in pure-attention hardmax transformers and its role in sentiment
  analysis
arxiv_id: '2407.01602'
source_url: https://arxiv.org/abs/2407.01602
tags:
- tokens
- such
- which
- token
- leaders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors analyze transformers with hardmax self-attention and
  normalization sublayers as the number of layers tends to infinity, viewing them
  as discrete-time dynamical systems. They show that the transformer inputs asymptotically
  converge to a clustered equilibrium determined by special points called leaders.
---

# Clustering in pure-attention hardmax transformers and its role in sentiment analysis

## Quick Facts
- arXiv ID: 2407.01602
- Source URL: https://arxiv.org/abs/2407.01602
- Authors: Albert Alcalde; Giovanni Fantuzzi; Enrique Zuazua
- Reference count: 37
- The authors analyze transformers with hardmax self-attention and normalization sublayers as the number of layers tends to infinity, viewing them as discrete-time dynamical systems. They show that the transformer inputs asymptotically converge to a clustered equilibrium determined by special points called leaders. Leveraging this theoretical understanding, they build an interpretable transformer model for sentiment analysis that effectively captures 'context' by clustering meaningless words around leader words carrying the most meaning. Their numerical results confirm empirically that the clustering entailed by their transformer acts as a mechanism to provide 'context', filtering out words that carry little meaning by clustering them near the most meaningful words.

## Executive Summary
This paper provides a theoretical analysis of transformers with hardmax self-attention and normalization sublayers, viewing them as discrete-time dynamical systems. The authors prove that under certain conditions, the transformer inputs converge asymptotically to a clustered equilibrium determined by special points called leaders. This theoretical understanding is leveraged to build an interpretable transformer model for sentiment analysis that captures context by clustering meaningless words around leader words carrying the most meaning.

## Method Summary
The authors analyze transformers with hardmax self-attention and normalization sublayers as the number of layers tends to infinity. They view these transformers as discrete-time dynamical systems where token values evolve layer by layer. The method involves proving convergence to clustered equilibria using geometric interpretation of the self-attention mechanism based on hyperplane separation. For sentiment analysis, they build a transformer model with three components: an encoder that maps words to token vectors and selects leaders, transformer layers that update tokens using hardmax attention, and a decoder that projects final token values to sentiment prediction. The model is trained using PyTorch with Adam optimizer on the IMDb movie review dataset.

## Key Results
- The authors prove that hardmax transformers with normalization sublayers converge to clustered equilibria as the number of layers approaches infinity.
- They characterize the cluster points as either special tokens called leaders or particular convex combinations thereof.
- Their interpretable transformer model for sentiment analysis effectively captures context by clustering meaningless words around leader words carrying the most meaning, achieving competitive accuracy on the IMDb dataset.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hardmax self-attention creates discrete clustering dynamics that converge to a finite set of cluster points.
- Mechanism: The self-attention sublayer selects a single leader token for each token to follow based on the largest projection along a fixed direction (A z_i). This deterministic selection causes tokens to aggregate into clusters that shrink toward a convex polytope.
- Core assumption: The matrix A is symmetric and positive definite, ensuring the bilinear form is an inner product and enabling geometric interpretation.
- Evidence anchors:
  - [abstract]: "viewing such transformers as discrete-time dynamical systems... thanks to a geometric interpretation of the self-attention mechanism based on hyperplane separation"
  - [section 2.2]: "token zi is attracted to the tokens with the largest orthogonal projection in the direction of Az_i"
  - [corpus]: Weak evidence - only related to "hardmax dynamics" in neighbor papers, no direct clustering proofs.
- Break condition: If A is not symmetric/positive definite, the geometric interpretation fails and clustering is not guaranteed.

### Mechanism 2
- Claim: The normalization sublayer ensures boundedness of all tokens, enabling convergence to a compact set.
- Mechanism: The normalization rescales all tokens by a constant factor (1/(1+α)), preventing explosion and ensuring all token values remain within the convex hull of initial values.
- Core assumption: The step-size parameter α is fixed and positive, and initial tokens are distinct and nonzero.
- Evidence anchors:
  - [section 3.3]: "the convex hull of the token values zk_1,...,zk_n does not increase in time... all tokens remain in the (compact) closed convex hull of the initial token values"
  - [section 4.1]: "Proposition 4.3 (The set S is attracting): For every token zi, zk_i → S as k → ∞"
  - [corpus]: No direct evidence about normalization's role in convergence.
- Break condition: If normalization is removed or replaced with unbounded scaling, tokens may diverge.

### Mechanism 3
- Claim: Leaders emerge as tokens that are never followed by any other token, and they determine the vertices of the limiting polytope.
- Mechanism: A token becomes a leader when Ci(Z^k) = {i} for some k, meaning it is the unique token maximizing the projection for itself. Leaders remain fixed once identified and converge to distinct vertices of the limiting polytope K.
- Core assumption: Initial tokens are distinct and nonzero, ensuring no collisions occur during dynamics.
- Evidence anchors:
  - [abstract]: "cluster points are either special tokens that we call leaders, or particular convex combinations thereof"
  - [section 5.1]: "Lemma 5.1: The set of leaders L is not empty... If zi ∈ L and k ∈ N such that Ci(Z^k) = {i}, then zk_i = z^ℓ_i for all times ℓ ≥ k"
  - [corpus]: Weak evidence - only mentions "leaders" in neighbor paper titles, no proof details.
- Break condition: If initial tokens are not distinct or some are zero, the leader structure may change or additional fixed points may appear.

## Foundational Learning

- Concept: Convex hull and polytope geometry
  - Why needed here: The analysis relies on showing that tokens remain within shrinking convex hulls that converge to a polytope, and cluster points lie on the boundary.
  - Quick check question: If you have tokens at points (0,0), (1,0), and (0,1), what is their convex hull and is it a polytope?

- Concept: Discrete-time dynamical systems
  - Why needed here: The transformer is modeled as a discrete-time dynamical system where token values evolve layer by layer, and convergence analysis requires understanding attractors and equilibria.
  - Quick check question: What distinguishes a fixed point from an attracting fixed point in a dynamical system?

- Concept: Hardmax selection and its continuity properties
  - Why needed here: The hardmax function is discontinuous, which creates technical challenges in proving convergence and prevents tokens from oscillating between clusters indefinitely.
  - Quick check question: Why does using softmax instead of hardmax complicate the clustering analysis?

## Architecture Onboarding

- Component map: Encoder -> K transformer layers -> Decoder -> Loss computation -> Parameter update
- Critical path: Encoder → K transformer layers → Decoder → Loss computation → Parameter update
- Design tradeoffs:
  - Hardmax vs softmax: Hardmax enables exact clustering analysis but breaks differentiability for training
  - Fixed A vs trainable A: Fixed A enables theoretical analysis but may limit model expressiveness
  - Single-head vs multi-head: Single-head enables geometric interpretation but may be less powerful than multi-head
- Failure signatures:
  - No leaders emerge: Initial tokens may not be distinct or normalization may be incorrect
  - Tokens don't converge: Step-size α may be too large or A may not be positive definite
  - Clustering is unstable: Implementation may accidentally use softmax instead of hardmax
- First 3 experiments:
  1. Verify that for distinct initial tokens with A=I and small α, tokens converge to a finite set
  2. Test that leaders remain fixed once identified and correspond to vertices of the limiting polytope
  3. Confirm that removing normalization causes token values to grow unboundedly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can clustering be proven for pure-attention hardmax transformers with non-symmetric or non-positive definite attention matrices?
- Basis in paper: [explicit] The authors note that their analysis relies on the bilinear form being an inner product, which is not the case when the attention matrix is not symmetric or positive definite. They state this is important because symmetry and positive definiteness usually fail for real-life transformers using low-rank matrices for training.
- Why unresolved: The proof techniques used by the authors heavily depend on the properties of inner products. When the attention matrix is not symmetric or positive definite, these properties no longer hold, requiring new mathematical tools and approaches.
- What evidence would resolve it: A rigorous proof showing that clustering still occurs when the attention matrix is non-symmetric or non-positive definite, or a counterexample demonstrating that clustering fails in these cases.

### Open Question 2
- Question: Does the clustering mechanism of self-attention improve the performance of regular feed-forward networks in transformers?
- Basis in paper: [explicit] The authors suggest that a promising direction of study is to understand to what extent the clustering mechanism of self-attention improves the performance of regular feed-forward networks, noting that feed-forward sublayers are necessary for sufficient approximation power in real transformers.
- Why unresolved: While the authors provide theoretical analysis of pure-attention transformers, they do not directly compare the performance of transformers with and without feed-forward sublayers. The interaction between self-attention and feed-forward components in real transformers remains unclear.
- What evidence would resolve it: Empirical studies comparing the performance of transformers with different configurations (pure-attention, with feed-forward sublayers, with different numbers of heads) on various tasks, or theoretical analysis showing the specific benefits of self-attention's clustering mechanism.

### Open Question 3
- Question: How does the asymptotic behavior of transformers change when using multi-head attention instead of single-head attention?
- Basis in paper: [explicit] The authors note that real transformers use multi-head attention, where each head can have different attention and value matrices, but they only provide analysis for the single-head case. They state it is not clear how the asymptotic behavior changes when different heads interact.
- Why unresolved: The mathematical analysis becomes significantly more complex with multiple heads, as each head can potentially contribute differently to the overall attention mechanism. The authors' proof techniques for single-head attention do not directly extend to the multi-head case.
- What evidence would resolve it: A rigorous mathematical analysis of the asymptotic behavior of transformers with multi-head attention, showing how the interaction between different heads affects clustering and other dynamical properties, or empirical studies demonstrating the differences in behavior between single-head and multi-head transformers.

## Limitations
- The theoretical analysis assumes a simplified model with a fixed symmetric positive-definite matrix A, single-head attention, and a specific normalization scheme.
- The theoretical convergence results are asymptotic (as the number of layers approaches infinity), and the practical implications for finite-depth transformers remain unclear.
- The claim that clustering provides a "mechanism to filter out words that carry little meaning" is primarily supported by qualitative observations rather than rigorous analysis.

## Confidence

- **High Confidence**: The geometric interpretation of hardmax self-attention as selecting tokens with maximal projection along a fixed direction is mathematically sound and well-established in the literature. The boundedness of token values under the normalization scheme is also rigorously proven.

- **Medium Confidence**: The convergence to clustered equilibria and the characterization of leaders as cluster points is theoretically justified, but the practical relevance depends on whether these asymptotic properties manifest in finite-depth transformers used in practice. The empirical validation on sentiment analysis provides some evidence but is limited in scope.

- **Low Confidence**: The claim that clustering provides a "mechanism to filter out words that carry little meaning" is primarily supported by qualitative observations rather than rigorous analysis. The connection between asymptotic clustering behavior and practical sentiment classification performance requires further investigation.

## Next Checks

1. **Finite-depth behavior**: Empirically test whether clustering and leader emergence occur in transformers with realistic numbers of layers (e.g., 6-12 layers) rather than only in the infinite-layer limit. Measure how quickly tokens converge to clusters and whether the limiting polytope structure is preserved.

2. **Robustness to architectural variations**: Test whether clustering behavior persists when using multi-head attention, learned attention matrices, or different normalization schemes. Compare clustering patterns and classification accuracy across these variations to assess the importance of the specific architecture analyzed.

3. **Ablation study on clustering**: Quantitatively measure the impact of clustering on sentiment classification by comparing the proposed model against variants that use softmax attention (which may not cluster) or random attention (which should not cluster). Measure both accuracy and interpretability metrics to assess whether clustering provides benefits beyond standard transformer performance.