---
ver: rpa2
title: ADBA:Approximation Decision Boundary Approach for Black-Box Adversarial Attacks
arxiv_id: '2406.04998'
source_url: https://arxiv.org/abs/2406.04998
tags:
- attacks
- attack
- queries
- decision
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Approximation Decision Boundary Approach
  (ADBA) for query-efficient decision-based black-box adversarial attacks. Unlike
  existing methods that use binary search to precisely identify decision boundaries
  with many queries, ADBA compares perturbation directions using an Approximation
  Decision Boundary (ADB), avoiding costly exact search.
---

# ADBA:Approximation Decision Boundary Approach for Black-Box Adversarial Attacks

## Quick Facts
- arXiv ID: 2406.04998
- Source URL: https://arxiv.org/abs/2406.04998
- Reference count: 40
- Key outcome: ADBA achieves >99% attack success rates with 60-70% fewer queries than RayS on six ImageNet models

## Executive Summary
This paper introduces ADBA (Approximation Decision Boundary Approach), a query-efficient method for decision-based black-box adversarial attacks. Unlike existing methods that use binary search to precisely locate decision boundaries, ADBA employs an approximation decision boundary (ADB) to compare perturbation directions with significantly fewer queries. The approach demonstrates that using the median value of the decision boundary distribution as ADB can effectively differentiate perturbation directions with only four queries on average, achieving high attack success rates while substantially reducing query counts.

## Method Summary
ADBA addresses the query inefficiency problem in black-box adversarial attacks by replacing exact decision boundary identification with an approximation approach. The method leverages statistical analysis of decision boundary distributions, showing that the median value can serve as an effective ADB for comparing perturbation directions. This eliminates the need for costly binary searches while maintaining attack effectiveness. The approach is designed to work across different models and datasets without requiring re-estimation of distribution parameters, making it both efficient and broadly applicable.

## Key Results
- Achieves >99% attack success rates across six ImageNet models (VGG19, ResNet50, Inception-V3, ViT-B32, DenseNet161, EfficientNet-B0)
- Reduces average query count by 60-70% compared to RayS baseline
- Maintains effectiveness without re-estimating distribution parameters across different models
- Demonstrates consistent performance across diverse architectures including CNNs and vision transformers

## Why This Works (Mechanism)
ADBA works by exploiting the statistical properties of decision boundaries in classification models. Instead of precisely locating the exact boundary through binary search, it uses an approximation based on the median of the decision boundary distribution. This approximation is sufficient to reliably compare the effectiveness of different perturbation directions. The key insight is that exact boundary identification is computationally expensive and often unnecessary - the median value provides enough information to make effective attack decisions while dramatically reducing the number of queries required.

## Foundational Learning

**Decision-based black-box attacks**: Why needed - understanding the attack scenario where only final predictions are accessible; Quick check - verify attacker can only query model outputs without gradient information

**Query efficiency metrics**: Why needed - quantifying attack performance beyond success rate; Quick check - ensure proper comparison of query counts across methods

**Decision boundary statistics**: Why needed - understanding how boundaries are distributed across the input space; Quick check - validate median-based approximation captures sufficient boundary information

## Architecture Onboarding

**Component map**: Input image -> ADBA perturbation generation -> Median-based ADB comparison -> Query model -> Update perturbation -> Repeat until success

**Critical path**: The core algorithm flow where perturbation direction is evaluated using ADB approximation, queries are sent to the model, and the attack vector is updated based on responses

**Design tradeoffs**: Accuracy vs. query efficiency - ADBA sacrifices precise boundary location for significant query reduction; Generalization vs. optimization - uses median distribution that works across models without re-estimation

**Failure signatures**: High query counts despite successful attacks, inconsistent performance across different image types, failure to converge on certain model architectures

**3 first experiments**: 1) Test ADB effectiveness on synthetic decision boundaries with known distributions; 2) Validate query reduction on simple linear classifiers; 3) Compare ADB-based vs. binary search performance on small neural networks

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations

The assumption that decision boundary distribution remains stable across different models and datasets may not hold universally. The experiments were conducted primarily on ImageNet, limiting generalizability to other datasets or problem domains. The paper focuses comparison mainly on RayS, lacking comprehensive benchmarking against other state-of-the-art attack methods.

## Confidence

**High Confidence**: Experimental results on six diverse ImageNet models show consistent improvement over RayS in query efficiency while maintaining high attack success rates.

**Medium Confidence**: The generalizability of the approach across different datasets and model types beyond the tested ImageNet models.

**Low Confidence**: The claim that decision boundary distribution stability eliminates the need for re-estimation across all scenarios, which is critical to the method's practicality.

## Next Checks

1. **Cross-Dataset Validation**: Test ADBA on non-ImageNet datasets (e.g., CIFAR-10, MNIST, or medical imaging datasets) to verify the stability of the decision boundary distribution assumption across different data domains.

2. **Architecture-Specific Analysis**: Evaluate ADBA on additional model architectures not covered in the original experiments, particularly focusing on vision transformers with different sizes and attention mechanisms.

3. **Comparative Benchmarking**: Conduct comprehensive benchmarking against additional state-of-the-art black-box attack methods (e.g., Sign-OPT, GeoDA, SimBA) on the same ImageNet models to provide a more complete picture of ADBA's relative performance.