---
ver: rpa2
title: A Comparative Study on Multi-task Uncertainty Quantification in Semantic Segmentation
  and Monocular Depth Estimation
arxiv_id: '2405.17097'
source_url: https://arxiv.org/abs/2405.17097
tags:
- uncertainty
- deep
- semantic
- segmentation
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates uncertainty quantification methods for joint
  semantic segmentation and monocular depth estimation. The authors compare Deep Ensembles,
  Deep Sub-Ensembles, and Monte Carlo Dropout when applied to single-task and multi-task
  models.
---

# A Comparative Study on Multi-task Uncertainty Quantification in Semantic Segmentation and Monocular Depth Estimation

## Quick Facts
- arXiv ID: 2405.17097
- Source URL: https://arxiv.org/abs/2405.17097
- Authors: Steven Landgraf; Markus Hillemann; Theodor Kapler; Markus Ulrich
- Reference count: 13
- One-line primary result: Deep Ensembles provide superior uncertainty quality but high computational cost; multi-task learning improves semantic segmentation uncertainty quality.

## Executive Summary
This paper evaluates uncertainty quantification methods for joint semantic segmentation and monocular depth estimation tasks. The authors compare Deep Ensembles, Deep Sub-Ensembles, and Monte Carlo Dropout across single-task and multi-task models. Deep Ensembles achieve the best performance in terms of prediction accuracy and uncertainty quality, though at high computational cost. Multi-task learning improves uncertainty quality for semantic segmentation compared to single-task models. The study finds that the median uncertainty threshold is a robust default for classifying certain versus uncertain predictions.

## Method Summary
The study implements three baseline models (SegFormer, DepthFormer, SegDepthFormer) combined with three uncertainty quantification methods (Deep Ensembles, Deep Sub-Ensembles, Monte Carlo Dropout with 20% and 50% dropout ratios). The SegDepthFormer multi-task model is derived from SegFormer for joint semantic segmentation and monocular depth estimation. Evaluation is performed on Cityscapes dataset using metrics including mIoU, RMSE, ECE, uncertainty quality measures (p(acc/cer), p(inacc/unc), PA, vPU), and inference time.

## Key Results
- Deep Ensembles achieve superior prediction performance and uncertainty quality but incur high computational cost (667.51ms inference time)
- Multi-task learning improves semantic segmentation uncertainty quality compared to single-task models
- Median uncertainty threshold provides robust classification of certain versus uncertain predictions
- Deep Sub-Ensembles offer a good balance between efficiency and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep Ensembles achieve superior uncertainty quality through averaging over multiple model predictions
- Mechanism: Multiple independently trained models capture different modes of the data distribution, and their disagreement reflects model uncertainty
- Core assumption: Ensemble members are sufficiently diverse to capture different aspects of the data distribution
- Evidence anchors: Abstract states Deep Ensembles "stand out as the preferred choice, particularly in out-of-domain scenarios" and section confirms they deliver "best prediction performance and mostly superior uncertainty quality"
- Break condition: If ensemble members are too similar (same initialization seeds), diversity assumption fails and uncertainty estimates become unreliable

### Mechanism 2
- Claim: Multi-task learning improves semantic segmentation uncertainty quality through shared representations
- Mechanism: Depth estimation provides geometric constraints that help segmentation distinguish between ambiguous regions, leading to more confident predictions where geometry is clear
- Core assumption: The two tasks have complementary information that can be jointly exploited through shared representations
- Evidence anchors: Abstract notes "multi-task learning improves uncertainty quality for semantic segmentation compared to single-task models" and section shows SegDepthFormer has "greater uncertainty quality for semantic segmentation task"
- Break condition: If tasks are negatively correlated or compete for capacity, shared representations may degrade both tasks' performance

### Mechanism 3
- Claim: Median uncertainty threshold provides robust classification by adapting to uncertainty score distribution
- Mechanism: The median splits the uncertainty distribution into equal halves, providing natural separation between "certain" and "uncertain" predictions that is less sensitive to outliers than fixed thresholds
- Core assumption: Uncertainty distribution has sufficient spread to make median meaningful
- Evidence anchors: Abstract finds "median uncertainty threshold is found to be a robust default for classifying certain versus uncertain predictions"
- Break condition: If uncertainty scores are highly concentrated or multimodal, median may not provide meaningful separation

## Foundational Learning

- Concept: Predictive entropy for classification uncertainty
  - Why needed here: Semantic segmentation requires uncertainty quantification per pixel, and entropy provides principled measure based on class probabilities
  - Quick check question: How does entropy change when a model is uncertain between two classes versus uncertain across all classes?

- Concept: Variance-based uncertainty for regression
  - Why needed here: Monocular depth estimation is regression task requiring uncertainty measures based on depth prediction variance
  - Quick check question: What's the difference between using variance of predictions versus variance of depth values?

- Concept: Calibration metrics (ECE - Expected Calibration Error)
  - Why needed here: Evaluating whether predicted uncertainties match actual accuracy requires understanding calibration concepts
  - Quick check question: What does it mean for a model to be well-calibrated in terms of predicted probabilities vs actual accuracy?

## Architecture Onboarding

- Component map: Three main model variants (SegFormer, DepthFormer, SegDepthFormer) × three uncertainty methods (MCD, DSE, DE) = nine experimental configurations
- Critical path: Baseline model training → uncertainty method integration → evaluation pipeline (mIoU, RMSE, ECE, p(acc/cer), p(inacc/unc), PA vPU)
- Design tradeoffs: Deep Ensembles provide best performance but highest computational cost (667.51ms inference time vs 17.90ms baseline); MCD degrades prediction performance with 50% dropout; DSE offers middle ground
- Failure signatures: Poor calibration (high ECE), high p(inacc/unc) indicating uncertain predictions are often inaccurate, or inference time exceeding real-time requirements
- First 3 experiments:
  1. Baseline SegFormer training on Cityscapes to establish mIoU reference
  2. MCD with 20% dropout integration on SegDepthFormer to verify uncertainty quality improvements
  3. DSE implementation to compare efficiency vs DE performance trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Deep Ensembles performance scale with ensemble size in multi-task learning settings?
- Basis in paper: Authors note Deep Ensembles achieve best performance but suffer from high computational cost
- Why unresolved: Paper does not provide analysis of how ensemble size affects performance versus computational efficiency
- What evidence would resolve it: Systematic study varying number of ensemble members and measuring both performance metrics and computational costs

### Open Question 2
- Question: Can other uncertainty quantification methods beyond MCD, DSE, and DE be effectively applied to multi-task learning?
- Basis in paper: Authors evaluated three methods and suggest further exploration
- Why unresolved: Paper only compares three methods and acknowledges other methods exist
- What evidence would resolve it: Implementation and evaluation of additional uncertainty quantification methods on same multi-task framework

### Open Question 3
- Question: How do uncertainty quantification methods perform on out-of-distribution data in multi-task learning scenarios?
- Basis in paper: Authors state Deep Ensembles "stand out as the preferred choice, particularly in out-of-domain scenarios"
- Why unresolved: While mentioned, paper does not provide detailed analysis or specific experiments on out-of-distribution performance
- What evidence would resolve it: Comprehensive testing on diverse out-of-distribution datasets and comparison of uncertainty quantification methods' ability to detect distributional shift

### Open Question 4
- Question: What is the impact of uncertainty quantification on real-time applications where computational resources are constrained?
- Basis in paper: Paper notes high computational cost of Deep Ensembles and mentions inference times for different methods
- Why unresolved: Paper does not explore trade-offs between uncertainty quality and real-time performance constraints
- What evidence would resolve it: Evaluation of uncertainty quantification methods under strict latency constraints and analysis of performance-uncertainty quality trade-off

## Limitations
- Findings based on synthetic out-of-domain experiments using Gaussian noise rather than real-world domain shift scenarios
- Mechanism for multi-task learning improving semantic segmentation uncertainty quality lacks theoretical explanation
- Median uncertainty threshold recommendation based on single observation without comparative analysis against alternative thresholding strategies

## Confidence
- Deep Ensembles performance claims: **Medium** - well-supported by experimental results but limited to synthetic out-of-domain testing
- Multi-task learning benefits: **Low-Medium** - experimental results show improvement but mechanism lacks theoretical explanation
- Median threshold recommendation: **Low** - based on single observation without comparative analysis

## Next Checks
1. Test Deep Ensembles on real-world domain shift datasets (e.g., Cityscapes to KITTI) to verify synthetic noise experiments generalize to actual out-of-domain scenarios
2. Conduct ablation studies isolating shared representation effects from other multi-task benefits to validate the proposed mechanism
3. Compare median threshold performance against learned thresholding methods and fixed percentile thresholds across multiple uncertainty distributions