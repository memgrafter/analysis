---
ver: rpa2
title: 'Backdooring Outlier Detection Methods: A Novel Attack Approach'
arxiv_id: '2412.05010'
source_url: https://arxiv.org/abs/2412.05010
tags:
- backdoor
- attack
- detection
- samples
- outlier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces BATOD, the first backdoor attack specifically\
  \ designed for outlier detection tasks. Unlike previous backdoor attacks that target\
  \ closed-set classification boundaries, BATOD creates two types of triggers\u2014\
  In-triggers to misclassify outliers as inliers, and Out-triggers to misclassify\
  \ inliers as outliers."
---

# Backdooring Outlier Detection Methods: A Novel Attack Approach

## Quick Facts
- arXiv ID: 2412.05010
- Source URL: https://arxiv.org/abs/2412.05010
- Reference count: 40
- Primary result: First backdoor attack specifically designed for outlier detection tasks using dual-trigger mechanism

## Executive Summary
This paper introduces BATOD, a novel backdoor attack targeting outlier detection systems. Unlike traditional backdoor attacks that focus on closed-set classification boundaries, BATOD creates two distinct types of triggers - In-triggers to misclassify outliers as inliers and Out-triggers to misclassify inliers as outliers. The attack synthesizes artificial outliers using negative transformations and generates triggers via a surrogate model to manipulate the Maximum Softmax Probability (MSP) of the victim model. Experiments demonstrate BATOD's superior performance compared to existing attacks, achieving significantly lower Poisoned-AUC scores while maintaining high Benign-AUC scores across multiple datasets.

## Method Summary
BATOD employs a two-stage trigger generation approach for backdoor attacks on outlier detection. First, a surrogate model is trained on a subset of inlier data. Synthetic outliers are then generated through negative transformations (elastic transforms, rotations, etc.) of inlier samples. In-triggers are created using PGD attacks to make outliers appear as inliers, while out-triggers use DeepFool to make inliers appear as outliers. The victim model is trained on poisoned data containing these triggers, resulting in a model that misclassifies data at inference time when triggers are present. The attack specifically targets the MSP scoring mechanism used in outlier detection.

## Key Results
- BATOD achieves significantly lower Poisoned-AUC scores (e.g., 2.3% vs 55.3% for previous attacks)
- Maintains high Benign-AUC scores while degrading open-set performance
- Effective against multiple defense mechanisms
- Preserves clean classification accuracy while compromising outlier detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** BATOD achieves significantly lower Poisoned-AUC scores than previous attacks by specifically targeting the open-set boundary rather than closed-set boundaries.
- **Mechanism:** BATOD creates two trigger types - In-triggers that misclassify outliers as inliers and Out-triggers that misclassify inliers as outliers. These triggers are designed to manipulate the Maximum Softmax Probability (MSP) of the victim model, exploiting the boundary between closed and open sets.
- **Core assumption:** The MSP scoring method in outlier detection is vulnerable to targeted perturbations that can shift classification confidence across the open-set boundary.
- **Evidence anchors:**
  - [abstract]: "BATOD achieves significantly lower Poisoned-AUC scores (e.g., 2.3% vs 55.3% for previous attacks)"
  - [section]: "We propose developing two types of triggers: in-triggers and out-triggers. In-triggers are designed to mispredict outliers as inliers, while out-triggers convert inliers to outliers during inference."
- **Break condition:** If MSP is not the primary scoring method used, or if the model's confidence calibration is too robust to be manipulated by these triggers.

### Mechanism 2
- **Claim:** The synthetic outlier generation through negative transformations effectively creates realistic outlier samples for training the triggers.
- **Mechanism:** BATOD uses hard augmentations (negative transformations like rotation and blur) to create artificial outliers from inlier samples, then applies triggers to these synthesized outliers to train the model to misclassify them as inliers.
- **Core assumption:** Negative transformations create data distributions that sufficiently deviate from the original inlier distribution to serve as effective outlier proxies.
- **Evidence anchors:**
  - [section]: "Inspired by this distribution deviation, these negative transformations are instrumental in generating artificial examples that represent outliers."
  - [abstract]: "The attack synthesizes artificial outliers using negative transformations and generates triggers via a surrogate model"
- **Break condition:** If the negative transformations don't create sufficiently distinct distributions, or if the model can easily distinguish synthetic from real outliers.

### Mechanism 3
- **Claim:** The two-stage trigger generation (in-triggers and out-triggers) specifically targets both directions of the open-set boundary manipulation.
- **Mechanism:** In-triggers are generated using PGD attacks to make outliers appear as inliers, while out-triggers use DeepFool to make inliers appear as outliers. This bidirectional approach ensures comprehensive boundary manipulation.
- **Core assumption:** Different attack methods (PGD for in-triggers, DeepFool for out-triggers) are needed because the perturbation goals are opposite (increase vs decrease confidence).
- **Evidence anchors:**
  - [section]: "In-triggers: These triggers are designed to make outliers appear as inliers... Out-triggers: These triggers are designed to make inliers be detected as outliers"
  - [abstract]: "BATOD creates two types of triggers—In-triggers to misclassify outliers as inliers, and Out-triggers to misclassify inliers as outliers"
- **Break condition:** If the perturbation budgets are too constrained to achieve the desired confidence shifts, or if the surrogate model's knowledge is insufficient to generate effective triggers.

## Foundational Learning

- **Concept:** Maximum Softmax Probability (MSP) for outlier detection
  - Why needed here: BATOD specifically targets the MSP scoring mechanism to manipulate outlier detection boundaries
  - Quick check question: How does MSP distinguish between inliers and outliers in a neural network's output?

- **Concept:** Negative transformations and data augmentation
  - Why needed here: BATOD uses negative transformations to synthesize artificial outliers from inlier data
  - Quick check question: What distinguishes negative transformations from positive transformations in data augmentation?

- **Concept:** Projected Gradient Descent (PGD) and DeepFool attacks
  - Why needed here: BATOD employs these specific attack methods to generate in-triggers and out-triggers respectively
  - Quick check question: What are the key differences between PGD and DeepFool in terms of their optimization objectives?

## Architecture Onboarding

- **Component map:** Surrogate model (fs) -> Trigger generator -> Discriminator (H) -> Victim model (fθ)
- **Critical path:** Surrogate model → Trigger generation → Poisoning training data → Train victim model → Inference time trigger application
- **Design tradeoffs:**
  - Using a surrogate model reduces dependence on the victim model but may generate suboptimal triggers
  - The two-trigger approach provides comprehensive coverage but increases complexity
  - Synthetic outlier generation enables training but may not perfectly represent real outliers
- **Failure signatures:**
  - High Benign-AUC but also high Poisoned-AUC indicates ineffective trigger generation
  - Low attack success rate on synthesized outliers suggests poor trigger design
  - Model detects synthetic outliers despite trigger application indicates transformation insufficiency
- **First 3 experiments:**
  1. Test trigger generation on a simple surrogate model with synthetic outliers to verify basic functionality
  2. Evaluate the effectiveness of in-triggers vs out-triggers separately on a validation set
  3. Measure the impact of different negative transformation intensities on outlier synthesis quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is BATOD against frequency-based defenses that specifically target high-frequency components of triggers?
- Basis in paper: [explicit] The paper discusses BATOD's resilience against frequency-based defenses, noting that it incorporates a sophisticated two-pronged technique involving noise generation and Gaussian blur filtering to enhance resistance.
- Why unresolved: The paper does not provide empirical evidence or detailed analysis of BATOD's performance against specific frequency-based defenses, leaving a gap in understanding its effectiveness in this area.
- What evidence would resolve it: Conducting experiments to test BATOD's performance against various frequency-based defenses, such as those that analyze or suppress high-frequency components, would provide concrete evidence of its resilience.

### Open Question 2
- Question: What are the potential limitations of BATOD when applied to datasets with a large number of classes, and how does it affect the attack's scalability?
- Basis in paper: [inferred] The paper mentions a limitation related to the number of classes K and the number of samples |Da|, suggesting that the attack's scalability could be affected as the number of classes increases.
- Why unresolved: The paper does not explore or quantify the impact of increasing class numbers on BATOD's performance, leaving questions about its scalability in more complex datasets.
- What evidence would resolve it: Conducting experiments with datasets of varying class sizes and analyzing the impact on BATOD's effectiveness and efficiency would provide insights into its scalability limitations.

### Open Question 3
- Question: How does BATOD perform in real-world scenarios where data distributions are non-stationary and evolve over time?
- Basis in paper: [inferred] The paper focuses on controlled experimental settings and does not address the dynamic nature of real-world data distributions, which could affect the attack's long-term effectiveness.
- Why unresolved: There is a lack of discussion or evidence on how BATOD adapts to changing data distributions, which is critical for understanding its practical applicability.
- What evidence would resolve it: Implementing BATOD in environments with evolving data distributions and monitoring its performance over time would provide evidence of its adaptability and effectiveness in real-world scenarios.

## Limitations
- Lack of detailed implementation specifics for critical hyperparameters
- Assumes availability of outlier samples for training, which may not hold in many real-world scenarios
- Effectiveness of synthetic outliers as proxies for real outliers remains an empirical question

## Confidence

- **High Confidence**: The two-trigger framework design (In-triggers and Out-triggers) is well-motivated and theoretically sound, with clear experimental evidence showing significant Poisoned-AUC reduction compared to baseline attacks.
- **Medium Confidence**: The synthetic outlier generation through negative transformations is logically sound but relies on assumptions about distribution deviation that may not generalize to all datasets or outlier types.
- **Low Confidence**: The specific hyperparameter choices and implementation details required for faithful reproduction are not fully specified, making exact replication challenging without additional experimentation.

## Next Checks

1. **Synthetic Outlier Quality Validation**: Conduct t-SNE visualization and statistical distance measures (e.g., Wasserstein distance) between original inliers, synthetic outliers, and real outliers to quantify how well negative transformations create representative outlier distributions.

2. **Trigger Robustness Testing**: Evaluate trigger effectiveness across different perturbation budgets and transformation intensities, measuring the trade-off between attack success rate and trigger detectability using PSNR, SSIM, and LPIPS metrics.

3. **Defense Mechanism Resilience**: Test BATOD against adaptive defenses specifically designed for outlier detection, including outlier-aware trigger detection and robust training techniques, to assess the attack's practical vulnerability in defensive environments.