---
ver: rpa2
title: 'FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask
  Evaluation'
arxiv_id: '2408.13585'
source_url: https://arxiv.org/abs/2408.13585
tags:
- translation
- sign
- they
- language
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLEURS-ASL, a high-quality extension of the
  FLORES/FLEURS machine translation benchmarks to American Sign Language (ASL). The
  dataset was translated by 5 Certified Deaf Interpreters with extensive preparation
  time, ensuring professional-level quality.
---

# FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation

## Quick Facts
- arXiv ID: 2408.13585
- Source URL: https://arxiv.org/abs/2408.13585
- Reference count: 40
- Primary result: 3.7 BLEU / 37.2 BLEURT on sentence-level translation with unified modeling approach

## Executive Summary
This paper introduces FLEURS-ASL, extending the FLORES/FLEURS machine translation benchmarks to American Sign Language. The dataset was translated by 5 Certified Deaf Interpreters with 5-6 hours preparation time per hour of content, ensuring professional-level quality across 200+ text languages and 102 speech languages. The paper presents a unified modeling approach that incorporates timestamp tokens, previous text tokens, and a 34-second context window, achieving strong performance on sentence-level translation while enabling discourse-level translation and other tasks. Human baselines are provided (13.0 BLEU / 64.6 BLEURT), and multimodal frontier models are evaluated, finding they demonstrate virtually no understanding of ASL.

## Method Summary
The method builds on T5v1.1-Base encoder-decoder architecture, using MediaPipe Holistic landmarks (85 3D points) as visual input instead of raw video. The model trains on random video clips from YouTube-ASL data rather than strictly aligned captions, enabling flexible inference across multiple tasks through control tokens (timestamp tokens, previous text context, and task-specific tokens). A 34-second context window allows for discourse-level translation. The unified model is trained on a multitask mixture including caption alignment, untimed translation, and timed translation, with specific sub-mixture weights (0.04 for caption alignment vs 0.96 for translation).

## Key Results
- Unified modeling approach achieves 3.7 BLEU / 37.2 BLEURT on sentence-level translation
- Human baselines established at 13.0 BLEU / 64.6 BLEURT for sentence-level translation
- Frontier models (GPT-4o, Claude 3 Opus, Gemini 1.5 Pro) show virtually no understanding of ASL
- Model supports multiple tasks including sentence-level, discourse-level, timed translation, and caption alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified multitask modeling with timestamp tokens and context windows enables flexible inference across multiple sign language tasks
- Mechanism: The model trains on random video clips with caption tracks rather than strictly aligned captions, allowing it to handle arbitrary clipping at inference time through autoregressive chunks
- Core assumption: Training on misaligned but well-aligned caption tracks enables better generalization to arbitrary clip boundaries than training on strictly aligned sentence-level clips
- Evidence anchors: [abstract] "builds upon YouTube-ASL data and uses random video clips rather than strictly aligned captions, allowing for flexible inference and supporting multiple translation tasks" - [section 4.2.1] "our training examples contain caption tracks as supervision rather than individual captions, and the timing of these tracks is used to adapt to random video clip boundaries, rather than determine the boundaries of those clips" - [corpus] Weak - related papers mention "Scaling Sign Language Translation" and "YouTube-SL-25" but don't specifically address the random clip methodology
- Break condition: If caption misalignment becomes too severe relative to the context window, the model will decohere from the input track during autoregression

### Mechanism 2
- Claim: Using Certified Deaf Interpreters with extensive preparation time produces higher quality translations than live interpretation
- Mechanism: CDIs with 5-6 hours preparation per 1 hour of content can research and plan translations to achieve professional-level quality, unlike live interpreters who must translate on the fly
- Core assumption: The additional preparation time allows interpreters to handle complex content domains and produce more natural, culturally-aware translations
- Evidence anchors: [abstract] "translated by 5 Certified Deaf Interpreters with extensive preparation time, ensuring professional-level quality" - [section 3.1] "interpreters were told the project's expectations for the amount of preparation and asked to sign as naturally as possible" - [corpus] Weak - corpus doesn't contain specific evidence about interpreter preparation methods, only mentions "Scaling Sign Language Translation" and "Enhanced Sign Language Translation using LLMs"
- Break condition: If preparation time is reduced or interpreters lack domain expertise, translation quality will degrade significantly

### Mechanism 3
- Claim: MediaPipe Holistic landmarks provide sufficient visual features for sign language translation compared to raw video
- Mechanism: Linearly projected 85 3D points from MediaPipe Holistic skeletons capture essential signing information while being computationally efficient for T5 encoder integration
- Core assumption: The key linguistic information in sign language is captured by hand positions, facial expressions, and body movements that MediaPipe Holistic tracks
- Evidence anchors: [section 4.2.1] "linearly projects 85 3D points from MediaPipe Holistic skeletons into the encoder of T5v1.1-Base" - [section 6] "modeling poses of the kind produced by MediaPipe Holistic is also a fundamentally flawed approach in general because it does not capture semantically relevant features like the tongue" - [corpus] Weak - corpus mentions "Large Sign Language Models: Toward 3D American Sign Language Translation" but doesn't provide direct evidence about MediaPipe Holistic's sufficiency
- Break condition: If critical linguistic features (like tongue position or finger spelling details) are not captured by MediaPipe Holistic, translation quality will be severely impacted

## Foundational Learning

- Concept: MediaPipe Holistic landmark extraction
  - Why needed here: The model uses 85 3D points from MediaPipe Holistic as visual input instead of raw video frames, requiring understanding of what these landmarks represent and how they're extracted
  - Quick check question: What are the three main components tracked by MediaPipe Holistic and how many landmarks does each provide?

- Concept: T5 encoder-decoder architecture
  - Why needed here: The model builds on T5v1.1-Base, so understanding how text tokens flow through the encoder and decoder is essential for modifying the architecture
  - Quick check question: In T5's encoder-decoder setup, where do timestamp tokens get processed and how do they influence the decoder's output?

- Concept: Random clip sampling methodology
  - Why needed here: The model trains on random crops of videos rather than strictly aligned captions, requiring understanding of how to maintain caption alignment information while allowing for flexible inference
  - Quick check question: How does the model handle captions that span across clip boundaries when training on random video segments?

## Architecture Onboarding

- Component map: Video -> MediaPipe landmarks -> T5 encoder -> decoder with context -> text output
- Critical path: Video -> MediaPipe landmarks -> T5 encoder -> decoder with context -> text output
  - Bottlenecks: Landmark extraction speed, context window size (512 tokens), autoregressive inference for long videos
- Design tradeoffs:
  - Raw video vs. MediaPipe landmarks: Computational efficiency vs. potential loss of semantic information
  - Strict vs. random clip alignment: Training stability vs. inference flexibility
  - Context window size: More context vs. memory/compute constraints
- Failure signatures:
  - Low BLEU scores with high BLEURT scores: Model understands content but struggles with exact phrasing
  - Decoherence during autoregression: Context window too small or caption misalignment too severe
  - Poor performance on caption alignment: Model prioritizes translation over alignment accuracy
- First 3 experiments:
  1. Baseline comparison: Train model with strict caption alignment vs. random clips to measure flexibility benefits
  2. Context ablation: Test model performance with 17-second vs. 34-second context windows to find optimal tradeoff
  3. Landmark ablation: Compare performance using MediaPipe landmarks vs. raw video features to quantify information loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of unified modeling approaches compare to specialized models across different sign language tasks?
- Basis in paper: [explicit] The paper introduces a unified modeling approach that "meets or exceeds the performance of sentence-level baselines while supporting a multitude of new tasks" but also shows "poor performance on caption alignment"
- Why unresolved: The paper provides limited quantitative comparison between the unified model and specialized models across the full range of FLEURS-ASL tasks
- What evidence would resolve it: Comprehensive benchmarking of the unified model against specialized models on all FLEURS-ASL tasks (sentence-level translation, discourse-level translation, caption alignment, etc.)

### Open Question 2
- Question: What is the impact of context window size on sign language translation quality?
- Basis in paper: [explicit] The unified model uses "a 34-second context window" and shows improved performance with "additional context"
- Why unresolved: The paper does not systematically explore how different context window sizes affect translation quality across tasks
- What evidence would resolve it: Experiments varying context window sizes (e.g., 17s, 34s, 68s) and measuring performance across FLEURS-ASL tasks

### Open Question 3
- Question: How well do current multimodal models generalize to sign languages beyond American Sign Language?
- Basis in paper: [explicit] The paper evaluates frontier models on ASL and finds "virtually no understanding of ASL" but only tests on one sign language
- Why unresolved: The evaluation is limited to American Sign Language, and it's unclear if performance would differ for other sign languages
- What evidence would resolve it: Testing the same frontier models on FLEURS-ASL extensions for other sign languages (e.g., British Sign Language, Japanese Sign Language)

## Limitations

- MediaPipe Holistic landmarks are "fundamentally flawed" for capturing all semantically relevant features of sign language, particularly tongue position and fine finger spelling details
- Random clip sampling methodology implementation details are not fully specified, making faithful reproduction difficult
- Multimodal frontier models show virtually no understanding of ASL, but evaluation methodology is not detailed

## Confidence

**High Confidence:** The dataset creation methodology using Certified Deaf Interpreters with extensive preparation time (claims about professional quality, interpreter preparation benefits). **Medium Confidence:** The unified modeling approach's ability to handle multiple tasks through timestamp tokens and context windows (claims about flexible inference, multitask capabilities). **Low Confidence:** The sufficiency of MediaPipe Holistic landmarks for sign language translation (claims about landmark adequacy, computational efficiency tradeoffs).

## Next Checks

1. **Landmark Sufficiency Test:** Evaluate translation quality using MediaPipe Holistic landmarks versus raw video frames on a subset of FLEURS-ASL to quantify information loss from using projected landmarks rather than full video.

2. **Random Clip Sampling Validation:** Implement and test the random clip sampling methodology to verify that caption alignment information is properly maintained when training on arbitrary video segments rather than strictly aligned captions.

3. **Context Window Ablation Study:** Systematically evaluate model performance with varying context window sizes (17s, 34s, 68s) to determine the optimal tradeoff between translation quality and computational constraints, particularly for discourse-level translation tasks.