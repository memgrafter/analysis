---
ver: rpa2
title: Practical Performative Policy Learning with Strategic Agents
arxiv_id: '2412.01344'
source_url: https://arxiv.org/abs/2412.01344
tags:
- policy
- learning
- performative
- gradient
- strategic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of practical performative policy
  learning where strategic agents respond to deployed policies by adjusting their
  features, causing distribution shifts. The authors propose a method that relaxes
  strong parametric assumptions on both agent utility and macro-level data distributions
  by leveraging bounded rationality and exploiting structure in the feature space.
---

# Practical Performative Policy Learning with Strategic Agents

## Quick Facts
- arXiv ID: 2412.01344
- Source URL: https://arxiv.org/abs/2412.01344
- Reference count: 40
- Primary result: Novel method for performative policy learning with strategic agents using feature space partitioning and RKHS-based convergence guarantees

## Executive Summary
This paper addresses the challenge of performative policy learning where strategic agents adjust their features in response to deployed policies, causing distribution shifts. The authors propose a method that relaxes strong parametric assumptions by leveraging bounded rationality and exploiting structure in the feature space. The approach partitions features into manipulatable and fixed components, constructs an evaluation vector as a mediator, and uses a differentiable behavior model to bypass micro-level decision processes. The Strategic Policy Gradient algorithm employs batch feedback for improved sample efficiency in high-dimensional settings.

## Method Summary
The method partitions features into manipulatable (discrete) and fixed components, constructs an evaluation vector [πθ((w,v))]w∈U as a mediator summarizing policy impact, and uses a differentiable behavior model hγ to estimate p(u|ζ(v,πθ)). The Strategic Policy Gradient algorithm uses batch feedback rather than bandit feedback, making it more sample-efficient for high-dimensional settings. The approach achieves convergence guarantees through RKHS realizability assumptions, avoiding parametric assumptions on both agent utility and macro-level data distributions.

## Key Results
- Policy values reach 5.26 (synthetic) and 3.52×10⁵ (semi-synthetic) versus 2.92 and 1.74×10⁵ for best baselines
- Superior performance demonstrated on both synthetic data and semi-synthetic loan application dataset (307,508 records)
- Successfully incentivizes agents to improve features while maintaining convergence stability
- RKHS-based convergence analysis provides theoretical guarantees without parametric assumptions

## Why This Works (Mechanism)

### Mechanism 1
The partition of features into manipulatable (discrete) and fixed components enables effective dimensionality reduction in the distribution map D(πθ). By evaluating the policy function πθ across all manipulatable feature values while keeping fixed features constant, the method creates an evaluation vector [πθ((w,v))]w∈U that serves as a mediator summarizing the impact of the policy on agent behavior. This works because agents' decision-making depends only on the evaluation vector ζ(v,πθ) rather than the full policy function, and the manipulatable feature space U is discrete and low-dimensional.

### Mechanism 2
The behavior model hγ directly estimating p(u|ζ(v,πθ)) bypasses the need to model complex micro-level decision processes. Instead of modeling agent utility maximization or decision processes, the method trains a classifier hγ to predict the observed manipulatable feature u given the evaluation vector ζ(v,πθ), effectively learning the distribution of outcomes directly. This works because the true conditional distribution p(u|ζ(v,πθ)) can be accurately approximated by a differentiable classifier, and this approximation lies in a common RKHS with the true distribution.

### Mechanism 3
The Strategic Policy Gradient algorithm achieves convergence through RKHS-based analysis rather than parametric assumptions on the distribution map. By assuming the true and estimated distributions lie in a common RKHS, the method bounds the gap between their gradients, enabling convergence guarantees through standard convex optimization techniques. This works because the kernel gradient is uniformly bounded, the feature map of RKHS is uniformly bounded, and the performative policy value is l-smooth and concave in θ.

## Foundational Learning

- Concept: Causal inference and structural causal models
  - Why needed here: The method relies on understanding causal mechanisms, particularly how policy interventions (πθ) affect agent behavior through mediators (evaluation vectors), and how to identify these causal effects without parametric assumptions.
  - Quick check question: Can you explain why the evaluation vector ζ(v,πθ) serves as a valid mediator in the causal path from policy to data distribution?

- Concept: Reproducing Kernel Hilbert Spaces (RKHS) and function approximation theory
  - Why needed here: The convergence proof relies on RKHS properties, specifically that functions and their gradients in RKHS can be bounded using kernel properties, and that approximation errors in RKHS translate to gradient estimation errors.
  - Quick check question: Why does the RKHS realizability assumption (both true and estimated distributions in common RKHS) enable gradient-based convergence analysis?

- Concept: Policy gradient methods and REINFORCE estimators
  - Why needed here: The algorithm uses policy gradient optimization with a REINFORCE-type estimator that includes the gradient of the log probability of observed agent behavior, requiring understanding of how to estimate gradients in reinforcement learning contexts.
  - Quick check question: How does the REINFORCE estimator structure enable the algorithm to use batch feedback rather than bandit feedback?

## Architecture Onboarding

- Component map: Data generation -> Warm-up policy updates -> Behavior model training -> Strategic policy gradient optimization -> Convergence
- Critical path: Data generation → Warm-up policy updates → Behavior model training → Strategic policy gradient optimization → Convergence
- Design tradeoffs:
  - Discrete vs continuous manipulatable features: Discrete enables practical classification but loses precision
  - Kernel choice for RKHS: Gaussian/Laplacian kernels provide bounded gradients but may have computational costs
  - Network architecture: 3-layer MLPs balance expressiveness and overfitting risk
  - Optimizer choice: Adagrad avoids momentum issues with shifting data distributions
- Failure signatures:
  - Behavior model training loss plateaus early: Indicates insufficient variation in evaluation vectors or model capacity issues
  - Policy value plateaus at suboptimal level: May indicate poor CATE estimation or RKHS approximation failure
  - Oscillations in policy updates: Suggests learning rate too high or insufficient exploration in early phases
- First 3 experiments:
  1. Test basic functionality with synthetic data (U=5, V=20) using best-response mechanism and verify convergence to higher policy value than baseline methods
  2. Validate RKHS assumption by comparing behavior model performance using Gaussian process classifier vs MLP on same synthetic data
  3. Test discretization sensitivity by varying |U| from 2 to 10 on synthetic data with M=15 true levels and measuring impact on convergence and policy value

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of discretization granularity on the accuracy of the estimated performative gradient when the true cardinality of manipulatable features M is significantly larger than the discretized level |U|? The paper examines M=15 and M=50 cases but doesn't test much larger values.

### Open Question 2
How does the performance of the strategic policy gradient algorithm degrade when agents have heterogeneous resistance to manipulation (some agents are "always-takers" or "never-takers")? The paper assumes uniform resistance levels but doesn't test heterogeneous patterns.

### Open Question 3
What is the theoretical relationship between the sample size per epoch n and the convergence rate of the strategic policy gradient algorithm? The paper establishes convergence guarantees but only provides O notation for estimation error.

### Open Question 4
How sensitive is the strategic policy gradient algorithm to violations of Assumption 2 (the causal mechanism assumption) in practical applications? The paper states violating this leads to identification issues but doesn't test realistic violation scenarios.

### Open Question 5
Can the behavior model hγ be effectively replaced by alternative function approximators beyond neural networks and Gaussian processes, particularly in extremely high-dimensional settings? The paper demonstrates both work but doesn't explore other architectures.

## Limitations

- RKHS realizability assumption may be restrictive and not hold in practical settings where true and estimated distributions lie in different function spaces
- Discrete partitioning of manipulatable features loses information compared to continuous feature spaces and may create suboptimal incentives
- Convergence guarantees rely on specific smoothness and concavity conditions that may not be satisfied in all performative settings

## Confidence

- Core mechanism using evaluation vectors as mediators: High confidence
- RKHS-based convergence analysis: Medium confidence (relies on strong theoretical assumptions)
- Discrete feature partition approach generalizability: Low confidence (may not work for continuous features or complex causal structures)

## Next Checks

1. Test RKHS assumption validity by systematically comparing performance of Gaussian process classifiers versus MLP behavior models across different kernel choices and data distributions, measuring the gap in gradient estimates.
2. Evaluate continuous feature performance by implementing a continuous version of the manipulatable feature space (e.g., using quantile binning with variable width) and comparing policy values and convergence stability to the discrete baseline.
3. Validate causal assumptions by conducting ablation studies where the evaluation vector ζ is systematically corrupted or modified, measuring impact on policy performance to confirm its role as an effective mediator.