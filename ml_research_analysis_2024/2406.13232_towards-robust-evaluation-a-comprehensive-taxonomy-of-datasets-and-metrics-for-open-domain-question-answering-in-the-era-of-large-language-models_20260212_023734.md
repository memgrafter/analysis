---
ver: rpa2
title: 'Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics
  for Open Domain Question Answering in the Era of Large Language Models'
arxiv_id: '2406.13232'
source_url: https://arxiv.org/abs/2406.13232
tags:
- questions
- evaluation
- question
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive taxonomy of datasets and evaluation
  metrics for Open Domain Question Answering (ODQA) in the era of large language models.
  It categorizes 52 datasets across textual and multimodal modalities, introducing
  a novel taxonomy based on question types and modalities.
---

# Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models

## Quick Facts
- arXiv ID: 2406.13232
- Source URL: https://arxiv.org/abs/2406.13232
- Authors: Akchay Srivastava; Atif Memon
- Reference count: 0
- This paper provides a comprehensive taxonomy of datasets and evaluation metrics for Open Domain Question Answering (ODQA) in the era of large language models.

## Executive Summary
This paper presents a comprehensive taxonomy of datasets and evaluation metrics for Open Domain Question Answering (ODQA), focusing on the challenges and opportunities presented by large language models. The study reviews 52 datasets and 20 evaluation techniques across textual and multimodal modalities, introducing a novel categorization based on question types and modalities. The research identifies key limitations in current ODQA evaluation approaches, particularly for complex question types, multimodal data, and LLM hallucination detection, while proposing directions for future research.

## Method Summary
The study employs a systematic literature review methodology to analyze 52 ODQA datasets and 20 evaluation metrics. The approach involves categorizing datasets by modality (textual vs. multimodal) and question type (short-form, long-form, ambiguous, multi-hop, conversational, cross-lingual, time-sensitive, paraphrased, counterfactual), while organizing evaluation metrics into human-based, lexical, semantic, and LLM-based categories. The research identifies gaps in current evaluation approaches and proposes directions for future development.

## Key Results
- Introduced a novel taxonomy categorizing 52 ODQA datasets based on modality and question types
- Organized 20 evaluation metrics into human-based, lexical, semantic, and LLM-based approaches
- Identified Wikipedia as the dominant knowledge source in ODQA datasets
- Highlighted critical gaps in current evaluation methods, particularly for hallucination detection and multimodal ODQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The taxonomy enables better dataset selection by categorizing ODQA data based on modality and question difficulty, improving benchmarking precision.
- Mechanism: Modality-aware categorization separates datasets into textual and multimodal, while difficulty-based subcategorization (short-form, long-form, ambiguous, multi-hop, conversational, cross-lingual, time-sensitive, paraphrased, counterfactual) allows targeted evaluation of specific QA capabilities.
- Core assumption: Different ODQA challenges require distinct evaluation strategies; grouping datasets by these characteristics improves assessment granularity.
- Evidence anchors:
  - [abstract] "Our study presents a thorough examination of the current landscape of ODQA benchmarking by reviewing 52 datasets and 20 evaluation techniques across textual and multimodal modalities."
  - [section] "Our analysis divides the datasets into two fundamental categories: textual and multimodal... we propose a subcategorization of textual datasets based on the question types."
- Break condition: If evaluation needs don't align with dataset difficulty or modality distinctions, the taxonomy becomes unnecessarily complex.

### Mechanism 2
- Claim: Semantic and LLM-based evaluation metrics better capture answer quality than purely lexical metrics for complex question types.
- Mechanism: Semantic metrics (BERTScore, BLEURT, MoverScore) use contextual embeddings to assess meaning similarity, while LLM-based evaluation (GPTScore, G-Eval) leverages reasoning capabilities to judge answer quality beyond word overlap.
- Core assumption: Modern QA systems require evaluation that captures semantic equivalence and reasoning ability, not just lexical matching.
- Evidence anchors:
  - [section] "Semantic evaluation leverages semantic representations to capture the meaning of an answer by focusing on semantic similarity rather than word overlap."
  - [section] "LLMs demonstrate exceptional promise in evaluating the accuracy of predicted answers, owing to their remarkable capabilities in language processing and reasoning."
- Break condition: If LLM-based evaluation proves unreliable due to position/verbosity biases or limited reasoning capacity, simpler metrics may be preferable.

### Mechanism 3
- Claim: The review identifies critical gaps in current ODQA evaluation, enabling targeted future research.
- Mechanism: Systematic analysis of datasets and metrics reveals limitations in current evaluation approaches, particularly for complex question types, multimodal data, and LLM hallucination detection.
- Core assumption: Understanding current limitations guides more effective research directions than working without systematic gap analysis.
- Evidence anchors:
  - [section] "Our thorough review... reveals several key findings and illuminates promising directions for future research endeavors."
  - [section] "We anticipate continued research focus in this area" regarding automatic metrics that reflect human judgment.
- Break condition: If identified gaps don't align with actual research needs or if community consensus differs on priority areas.

## Foundational Learning

- Concept: Modality distinction in QA systems
  - Why needed here: Understanding how text, images, tables, and video differ as knowledge sources is crucial for selecting appropriate datasets and evaluation metrics
  - Quick check question: What are the key differences between evaluating textual vs. multimodal ODQA systems?

- Concept: Evaluation metric types and their trade-offs
  - Why needed here: Different metrics capture different aspects of answer quality; choosing the right combination requires understanding their strengths and limitations
  - Quick check question: When would you prefer semantic similarity metrics over lexical matching metrics for ODQA evaluation?

- Concept: Large language model capabilities and limitations
  - Why needed here: LLM-based evaluation leverages emergent capabilities but has known biases; understanding these is essential for effective implementation
  - Quick check question: What are the primary limitations of using LLMs as automated judges for ODQA systems?

## Architecture Onboarding

- Component map: Dataset selection → Model training → Answer generation → Evaluation pipeline (lexical/semantic/LLM metrics) → Gap analysis
- Critical path: Dataset → Question type identification → Appropriate metric selection → Performance assessment → Iterative improvement
- Design tradeoffs: Comprehensive evaluation vs. computational efficiency; automatic vs. human evaluation; single vs. multiple metrics
- Failure signatures: Poor metric selection leading to misleading results; computational bottlenecks in evaluation; misalignment between dataset characteristics and evaluation approach
- First 3 experiments:
  1. Implement basic evaluation pipeline using Exact Match and F1 on a short-form dataset
  2. Add semantic metric comparison (BERTScore vs. BLEURT) on the same dataset
  3. Test LLM-based evaluation (GPTScore) on a dataset with reference answers to compare with traditional metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automatic evaluation metrics be developed to effectively detect and quantify hallucinations in LLM-generated answers for ODQA?
- Basis in paper: [explicit] The paper explicitly identifies hallucination detection as a critical challenge for LLM-based ODQA systems and notes that current evaluation methods are inadequate for this purpose.
- Why unresolved: Existing evaluation metrics focus on semantic similarity and fluency but lack mechanisms to verify factual correctness against external knowledge sources.
- What evidence would resolve it: Development and validation of evaluation metrics that can accurately identify hallucinated content by cross-referencing generated answers with reliable knowledge sources, along with human evaluation studies demonstrating their effectiveness.

### Open Question 2
- Question: What are the key design principles for creating ODQA benchmarks that effectively evaluate systems in hybrid public-private data settings while preserving privacy?
- Basis in paper: [explicit] The paper identifies the lack of benchmarks for ODQA in public-private data settings as a significant limitation for developing privacy-preserving conversational agents.
- Why unresolved: Current datasets and evaluation methods are primarily designed for either fully public or fully private data, with limited exploration of hybrid scenarios.
- What evidence would resolve it: Creation of benchmark datasets and evaluation frameworks that simulate real-world hybrid data environments, along with empirical studies demonstrating their utility in advancing privacy-preserving ODQA research.

### Open Question 3
- Question: How can multimodal ODQA datasets be expanded to better support languages beyond English, particularly for non-textual modalities like images and videos?
- Basis in paper: [inferred] The paper notes the limitation of current multimodal datasets in supporting languages beyond English, particularly for non-textual modalities, which constrains the global applicability of ODQA systems.
- Why unresolved: Existing multimodal datasets are predominantly English-centric, with limited resources for other languages, especially in image and video domains.
- What evidence would resolve it: Development of large-scale, multilingual multimodal datasets with diverse question types and modalities, validated through cross-lingual evaluation studies demonstrating their effectiveness in improving ODQA performance across languages.

## Limitations
- The analysis is primarily based on existing literature review rather than empirical validation of the proposed taxonomy's effectiveness in practice.
- The relative effectiveness of semantic and LLM-based metrics compared to lexical metrics requires empirical validation across diverse datasets.
- The prioritization of identified research gaps may be influenced by the specific papers reviewed and might not reflect the full landscape of ODQA research needs.

## Confidence

**Dataset taxonomy claims (High confidence)**: The categorization of 52 datasets into modality and question type subcategories is well-supported by the literature and provides a clear framework for understanding current ODQA benchmarks.

**Metric taxonomy claims (Medium confidence)**: While the organization of evaluation metrics into lexical, semantic, and LLM-based categories is methodologically sound, the relative effectiveness of these approaches for different question types requires empirical validation.

**Gap analysis claims (Medium confidence)**: The identification of research gaps, particularly around hallucination detection and multimodal evaluation, is reasonable based on the systematic review, though prioritization may be influenced by review scope.

## Next Checks
1. Conduct empirical comparison of lexical, semantic, and LLM-based metrics across datasets with varying question complexity to validate the proposed evaluation framework's effectiveness
2. Implement the taxonomy for dataset selection in a practical ODQA system and measure whether it improves benchmarking precision and model development
3. Test the hallucination detection recommendations on state-of-the-art LLMs to assess the practical utility of the proposed evaluation approaches for real-world deployment