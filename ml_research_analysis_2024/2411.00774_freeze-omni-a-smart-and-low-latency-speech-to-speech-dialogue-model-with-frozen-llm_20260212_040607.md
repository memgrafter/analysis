---
ver: rpa2
title: 'Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with
  Frozen LLM'
arxiv_id: '2411.00774'
source_url: https://arxiv.org/abs/2411.00774
tags:
- speech
- decoder
- text
- freeze-omni
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Freeze-Omni is a speech-to-speech dialogue model that maintains
  the intelligence of a frozen LLM while enabling low-latency streaming interaction.
  It uses a three-stage training strategy to align speech input and output modalities
  without fine-tuning the LLM backbone, addressing the catastrophic forgetting problem.
---

# Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM

## Quick Facts
- arXiv ID: 2411.00774
- Source URL: https://arxiv.org/abs/2411.00774
- Reference count: 29
- Primary result: Enables speech-to-speech dialogue with frozen LLM backbone, achieving 72% accuracy on LlaMA-Questions and 53.88% on Trivia QA with ~1.2s latency

## Executive Summary
Freeze-Omni is a speech-to-speech dialogue model that preserves the intelligence of a frozen large language model (LLM) while enabling low-latency streaming interaction. It employs a three-stage training strategy to align speech input and output modalities without fine-tuning the LLM backbone, thereby addressing catastrophic forgetting. The system uses a chunk-wise streaming encoder and adapter for speech input, and a token-based decoder with non-autoregressive (NAR) and autoregressive (AR) components for speech output. Duplex dialogue is enabled through chunk-wise state prediction, allowing the model to process and respond to speech in real-time.

## Method Summary
The model operates in three stages: first, aligning the speech input encoder with the frozen LLM through chunk-wise streaming and adapter training; second, aligning the speech output decoder with the frozen LLM using NAR and AR components; and third, enabling duplex dialogue through chunk-wise state prediction. This approach ensures that the LLM's performance is preserved while extending its capabilities to speech-to-speech interaction. The system achieves low-latency streaming by processing speech in chunks and predicting dialogue states incrementally.

## Key Results
- Spoken question-answering accuracy: 72% on LlaMA-Questions, 53.88% on Trivia QA
- Latency: approximately 1.2 seconds in realistic scenarios
- Maintains LLM intelligence while enabling speech-to-speech dialogue

## Why This Works (Mechanism)
The model's success hinges on its three-stage training strategy, which bridges the gap between speech and text modalities without degrading the frozen LLM's performance. By using chunk-wise streaming for speech input and a hybrid NAR-AR decoder for speech output, the system processes and generates speech in real-time while preserving the LLM's intelligence. Chunk-wise state prediction enables duplex dialogue by allowing the model to handle overlapping speech and rapid turn changes.

## Foundational Learning
- **Chunk-wise streaming**: Why needed: Enables low-latency processing of speech input. Quick check: Measure latency reduction when processing speech in chunks vs. full utterances.
- **Non-autoregressive (NAR) decoding**: Why needed: Speeds up speech output generation. Quick check: Compare NAR vs. AR decoding latency and quality.
- **Autoregressive (AR) decoding**: Why needed: Ensures coherent speech output generation. Quick check: Evaluate coherence and fluency of AR-generated speech.
- **Duplex dialogue**: Why needed: Enables real-time, bidirectional conversation. Quick check: Test turn-taking latency and overlap handling in multi-turn dialogues.
- **Catastrophic forgetting**: Why needed: Prevents degradation of frozen LLM performance. Quick check: Compare LLM performance before and after fine-tuning.

## Architecture Onboarding
- **Component map**: Speech input -> Chunk-wise streaming encoder -> Adapter -> Frozen LLM -> NAR-AR decoder -> Speech output
- **Critical path**: Speech input -> Chunk-wise streaming encoder -> Adapter -> Frozen LLM -> NAR-AR decoder -> Speech output
- **Design tradeoffs**: Balances latency and quality by using NAR for speed and AR for coherence.
- **Failure signatures**: Degradation in LLM performance, increased latency, or incoherent speech output.
- **First experiments**:
  1. Test latency reduction with chunk-wise streaming vs. full utterance processing.
  2. Compare NAR vs. AR decoding for speed and quality.
  3. Evaluate duplex dialogue handling in overlapping speech scenarios.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation of robustness to diverse speech conditions (e.g., accents, background noise).
- Narrow focus on spoken question-answering, lacking evidence of open-domain dialogue handling.
- Claims of duplex dialogue capability are supported only by mechanism description, not systematic evaluation.

## Confidence
- Speech-text modality alignment without LLM degradation: **Medium**
- Low-latency streaming performance in realistic scenarios: **Medium**
- Duplex dialogue capability through chunk-wise state prediction: **Low**

## Next Checks
1. Test model robustness across diverse speech conditions (different speakers, accents, background noise levels) and report performance variance.
2. Conduct head-to-head latency comparison with a non-frozen LLM baseline under identical streaming conditions and define "realistic" operationally.
3. Evaluate duplex dialogue behavior in multi-turn conversations with overlapping speech or rapid turn changes to verify practical usability.