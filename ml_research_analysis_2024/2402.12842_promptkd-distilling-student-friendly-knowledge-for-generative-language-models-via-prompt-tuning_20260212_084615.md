---
ver: rpa2
title: 'PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models
  via Prompt Tuning'
arxiv_id: '2402.12842'
source_url: https://arxiv.org/abs/2402.12842
tags:
- teacher
- prompt
- promptkd
- knowledge
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PromptKD, a knowledge distillation method for
  generative language models using prompt tuning to distill student-friendly knowledge.
  The method addresses the challenge of applying knowledge distillation to large language
  models by using soft prompts to efficiently modify the teacher model without full
  fine-tuning.
---

# PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning

## Quick Facts
- arXiv ID: 2402.12842
- Source URL: https://arxiv.org/abs/2402.12842
- Authors: Gyeongman Kim; Doohyuk Jang; Eunho Yang
- Reference count: 40
- This paper proposes PromptKD, a knowledge distillation method for generative language models using prompt tuning to distill student-friendly knowledge

## Executive Summary
This paper addresses the challenge of applying knowledge distillation to large language models by introducing PromptKD, a method that uses soft prompts to efficiently modify the teacher model without full fine-tuning. The approach alleviates exposure bias by using student-generated responses as pseudo-targets during training, creating a more realistic training distribution that matches inference conditions. Experiments on instruction-following datasets demonstrate that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts.

## Method Summary
PromptKD distills student-friendly knowledge by first generating pseudo-targets using the student model's responses, then using prompt tuning to modify the teacher's output distribution to match the student's style while maintaining quality. The method employs a three-step training process: pseudo-target generation, prompt tuning for adaptive teaching, and student-friendly knowledge distillation. A regularization loss stabilizes the prompt tuning process by preventing early divergence from the original teacher. The student is trained using reverse KL divergence to match the teacher's distribution, and the approach is evaluated on instruction-following tasks using ROUGE-L and GPT-4 feedback scores.

## Key Results
- Achieves state-of-the-art performance on instruction-following datasets while adding only 0.0007% of the teacher's parameters as prompts
- Effectively alleviates exposure bias throughout training, as indicated by analysis of training and inference distributions
- Outperforms existing knowledge distillation methods for generative language models on multiple benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PromptKD alleviates exposure bias by using student-generated responses as pseudo-targets during training
- Mechanism: The method generates responses using the student model, which are then used as inputs to both the teacher (with prompt) and student during training. This creates a training distribution that matches the inference distribution where the model must generate from its own outputs.
- Core assumption: The student-generated responses, while potentially lower quality than ground truth, provide a more realistic training signal that reduces the gap between training and inference distributions
- Evidence anchors:
  - [abstract] "Analysis indicates that the method effectively alleviates exposure bias throughout training, leading to improved performance"
  - [section 3.1] "PromptKD uses the response y generated by the student for the prompt tuning and knowledge distillation processes, treating it as the pseudo-target. This approach addresses exposure bias"
  - [corpus] Weak - The corpus neighbors don't directly address exposure bias mechanisms, though related works on distillation and LLMs exist
- Break condition: If the student model is too weak initially, the pseudo-targets could be so poor that they mislead the teacher and degrade the distillation process

### Mechanism 2
- Claim: Student-friendly knowledge is extracted by modifying the teacher's output distribution through prompt tuning
- Mechanism: The prompt is trained to make the teacher generate responses similar to the student's distribution while maintaining generation performance. This creates a teacher that "speaks the student's language" without sacrificing quality.
- Core assumption: A teacher model can be modified to produce outputs that are easier for the student to learn from while maintaining the core semantic content of the responses
- Evidence anchors:
  - [abstract] "PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance"
  - [section 3.3] "The updated prompt is utilized as a trigger to extract student-friendly knowledge from the teacher and distill it to the student"
  - [corpus] Weak - The corpus neighbors discuss related topics like prompt tuning and distillation but don't specifically address student-friendly knowledge extraction
- Break condition: If the prompt tuning causes the teacher to deviate too far from its original knowledge distribution, the distilled knowledge could become incorrect or misleading

### Mechanism 3
- Claim: Regularization loss stabilizes the prompt tuning process by preventing early divergence from the original teacher
- Mechanism: An additional loss term encourages the teacher with prompt to stay close to the teacher without prompt during early training stages, preventing instability from prompt effects
- Core assumption: The prompt can cause significant instability in the teacher's output distribution early in training, requiring stabilization through regularization
- Evidence anchors:
  - [section 3.2] "To address this issue, we initialize the prompt with text embedding and devise an additional regularization loss Lreg to ensure that the teacher model distribution remains similar whether the prompt is used or not"
  - [section 3.2] "However, during the early stages of training, the influence of the prompt may cause significant deviations or inaccuracies in the teacher model distribution, leading to unstable learning"
  - [corpus] Weak - The corpus doesn't provide specific evidence about regularization in prompt tuning for knowledge distillation
- Break condition: If the regularization coefficient decays too quickly or is too strong, it might prevent the prompt from effectively modifying the teacher to produce student-friendly knowledge

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The paper builds on KD principles but applies them to generative models where traditional KD methods don't work well
  - Quick check question: What is the key difference between knowledge distillation for classification tasks versus generative tasks?

- Concept: Exposure Bias
  - Why needed here: Understanding exposure bias is crucial for grasping why using student-generated responses as pseudo-targets is beneficial
  - Quick check question: How does exposure bias manifest differently in autoregressive generation compared to other sequence modeling tasks?

- Concept: Prompt Tuning
  - Why needed here: The method relies on prompt tuning as a parameter-efficient way to modify the teacher model
  - Quick check question: How does prompt tuning differ from full fine-tuning in terms of parameter efficiency and training dynamics?

## Architecture Onboarding

- Component map:
  Teacher model (fixed, pre-trained) -> Soft prompts (trainable, prepended to teacher input) -> Student model (trainable, initialized from SFT checkpoint) -> Pseudo-target generator (student model during training) -> Loss functions: KD loss, regularization loss, student loss

- Critical path:
  1. Sample request x from data distribution
  2. Generate response y from student model qθ(y|x)
  3. Update prompt P using Lprompt (KD loss + regularization loss)
  4. Update student θ using Lstudent (reverse KL divergence)
  5. Repeat until convergence

- Design tradeoffs:
  - Using student-generated responses vs ground truth for training: Student responses better match inference distribution but may be lower quality
  - Prompt tuning vs full fine-tuning: More parameter-efficient but potentially less expressive modification capability
  - Reverse KL vs forward KL: Mode-seeking behavior vs mode-covering, affects which student behaviors are reinforced

- Failure signatures:
  - High exposure bias metrics despite training: Student and teacher distributions diverge significantly when using student-generated vs teacher-generated pseudo-targets
  - Student performance worse than teacher: Prompt tuning may have degraded the teacher's knowledge representation
  - Unstable training: Regularization loss may not be properly balancing prompt modification and stability

- First 3 experiments:
  1. Measure exposure bias with and without PromptKD to confirm the method's effectiveness
  2. Compare prompt tuning vs LoRA for modifying the teacher to validate the parameter-efficient approach
  3. Test different prompt initialization methods (random, padding, text) to optimize prompt tuning stability and performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on ROUGE-L and GPT-4 feedback scores, which may not fully capture instruction-following response quality
- Method's effectiveness is demonstrated primarily on instruction-following datasets, with uncertainty about generalization to other generative tasks
- Computational overhead of the three-step training process and potential instability from prompt tuning are not thoroughly characterized

## Confidence
**High Confidence**: The mechanism of using student-generated responses as pseudo-targets to alleviate exposure bias is well-supported by theoretical framework and experimental evidence showing reduced exposure bias metrics.

**Medium Confidence**: The claim that prompt tuning efficiently extracts student-friendly knowledge while maintaining teacher performance is supported by results but depends on proper regularization and prompt initialization.

**Low Confidence**: The assertion that PromptKD achieves state-of-the-art performance is based on comparisons with specific baselines on limited datasets, without comprehensive ablation studies or comparisons against all relevant distillation methods.

## Next Checks
1. **Exposure Bias Validation**: Implement the ExAccErr metric and measure exposure bias with and without PromptKD across different generation lengths to verify the method's effectiveness in reducing distribution mismatch between training and inference.

2. **Prompt Tuning Stability**: Conduct experiments comparing different prompt initialization methods (random, padding, text) and regularization schedules to identify optimal configurations that prevent early divergence while allowing effective student-friendly knowledge extraction.

3. **Generalization Testing**: Apply PromptKD to non-instruction-following tasks (such as summarization or dialogue generation) to validate whether the method generalizes beyond the demonstrated datasets and maintains performance improvements across diverse generative tasks.