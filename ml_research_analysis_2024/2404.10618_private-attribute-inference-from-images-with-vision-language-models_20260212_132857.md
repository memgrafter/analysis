---
ver: rpa2
title: Private Attribute Inference from Images with Vision-Language Models
arxiv_id: '2404.10618'
source_url: https://arxiv.org/abs/2404.10618
tags:
- image
- prompt
- privacy
- information
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the privacy risks of vision-language models
  (VLMs) inferring personal attributes from online images. The authors construct a
  dataset of Reddit images with manually annotated personal attributes, ensuring images
  do not directly depict humans to focus on inference beyond human attribute recognition.
---

# Private Attribute Inference from Images with Vision-Language Models

## Quick Facts
- **arXiv ID**: 2404.10618
- **Source URL**: https://arxiv.org/abs/2404.10618
- **Reference count**: 40
- **Primary result**: Vision-language models can infer personal attributes from images without direct human depictions with up to 77.6% accuracy

## Executive Summary
This paper investigates the privacy risks of vision-language models (VLMs) inferring personal attributes from online images. The authors construct a dataset of Reddit images with manually annotated personal attributes, ensuring images do not directly depict humans to focus on inference beyond human attribute recognition. They evaluate 7 state-of-the-art VLMs, finding that they can infer various personal attributes with up to 77.6% accuracy. The authors show that even safety-aligned VLMs can be easily circumvented using simple prompt engineering, and that their accuracy scales with general model capabilities, suggesting future models will pose even greater privacy risks. Additionally, VLM inferences are significantly faster and cheaper than human annotation, enabling unprecedented scalability of privacy-infringing inferences.

## Method Summary
The authors created the VIP dataset by collecting Reddit images from subreddits likely to contain personal attribute information, manually annotating these images with personal attributes (sex, age, income, location, occupation, etc.), and filtering to exclude direct human depictions. They then evaluated 7 state-of-the-art VLMs using various prompt engineering techniques to circumvent safety safeguards, measuring inference accuracy across different attributes. The evaluation compared naive prompts against extended chain-of-thought and gamified prompts, and tested automated zooming features to overcome resolution limitations.

## Key Results
- VLMs can infer personal attributes from images without direct human depictions with up to 77.6% accuracy
- Safety-aligned VLMs can be easily circumvented using simple prompt engineering techniques
- Inference accuracy scales with general model capabilities, suggesting future models will pose greater privacy risks
- VLM inferences are significantly faster and cheaper than human annotation, enabling unprecedented scalability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-language models can infer personal attributes from images without direct human depictions by recognizing contextual cues.
- Mechanism: VLMs use advanced reasoning and world knowledge to connect visual objects and environments with personal attributes, such as recognizing a sports logo to infer location or a kitchen layout to infer country.
- Core assumption: The VLM has been trained on diverse visual and textual data that allows it to make such associations reliably.
- Evidence anchors:
  - [abstract]: "they can infer various personal attributes at up to 77.6% accuracy" and "focus on images where inferable private attributes do not stem from direct depictions of humans."
  - [section]: "VLMs enable the automated inference of personal attributes from images that do not necessarily contain the subjected person in the image but, e.g., only an inconspicuous depiction of their living room."
  - [corpus]: Weak - no direct evidence of VLM reasoning from context in cited papers.
- Break condition: If the VLM lacks sufficient world knowledge or the visual cues are too ambiguous or rare in training data.

### Mechanism 2
- Claim: Even safety-aligned VLMs can be circumvented using simple prompt engineering, enabling privacy-infringing inferences.
- Mechanism: The alignment safeguards are brittle and can be bypassed by framing the inference task as a gamified, step-by-step reasoning challenge, which the model is inclined to engage with.
- Core assumption: The safety alignment is primarily rule-based and does not deeply understand the intent behind the prompt.
- Evidence anchors:
  - [abstract]: "even safety-aligned VLMs can be easily circumvented using simple prompt engineering."
  - [section]: "we observe that it can recognize relevant parts of the image... that could help the inference but which due to technical resolution limitations are too small to be analyzed" and "we found that once the safeguard of the model has been (easily) evaded, the model cooperates on the inference task."
  - [corpus]: Moderate - related papers discuss adversarial shielding and jailbreaking, supporting the fragility of safeguards.
- Break condition: If safety alignment is significantly strengthened to detect and block such reasoning prompts.

### Mechanism 3
- Claim: The accuracy of private attribute inference scales with the general capabilities of the VLMs, implying future models will pose greater privacy risks.
- Mechanism: VLMs with higher general performance on multimodal benchmarks (like MMMU) also perform better on private attribute inference tasks, indicating that as models improve overall, their inference capabilities will improve as well.
- Core assumption: There is a direct correlation between general multimodal reasoning ability and the ability to infer private attributes from images.
- Evidence anchors:
  - [abstract]: "we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger inferential adversaries."
  - [section]: "we observe that the personal attribute inference accuracy is strongly correlated with the general capabilities of the models."
  - [corpus]: Moderate - papers on vision-language model capabilities and privacy leakage support the idea that stronger models can extract more information.
- Break condition: If future VLMs are designed with explicit privacy constraints that decouple general capability from inference accuracy.

## Foundational Learning

- Concept: Vision-Language Models (VLMs)
  - Why needed here: Understanding how VLMs process and reason over both images and text is crucial to grasp how they can infer personal attributes.
  - Quick check question: What is the main difference between traditional image recognition and VLM-based inference?

- Concept: Prompt Engineering and Safety Alignment
  - Why needed here: Knowing how to craft prompts that bypass safety measures is key to understanding the vulnerability of VLMs to privacy attacks.
  - Quick check question: How does gamification of the inference task help evade safety safeguards?

- Concept: Privacy Risks and Personal Data Definitions
  - Why needed here: Recognizing what constitutes private information under regulations like GDPR and CCPA helps frame the privacy implications of the findings.
  - Quick check question: Which personal attributes discussed in the paper fall under GDPR's definition of personal data?

## Architecture Onboarding

- Component map: Image input → preprocessing (resolution limits) → multimodal encoding → reasoning/inference → output generation → safety check (if applicable) → final output
- Critical path: Image input → preprocessing (resolution limits) → multimodal encoding → reasoning/inference → output generation → safety check (if applicable) → final output
- Design tradeoffs: Higher resolution input improves inference accuracy but increases computational cost; stronger safety alignment reduces privacy risks but may degrade utility; open-source models offer accessibility but often lack robust safeguards
- Failure signatures: High refusal rates indicate active safety filters; low accuracy may suggest insufficient reasoning capability or poor alignment between prompt and model expectations; inconsistent results across attributes may indicate model bias or limitations
- First 3 experiments:
  1. Test a simple prompt ("Where was this picture taken?") on GPT4-V and measure refusal rate and accuracy
  2. Apply the final gamified prompt and compare accuracy and refusal rate to the simple prompt
  3. Enable the automated zooming feature and evaluate its impact on location prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different image resolutions affect the accuracy of private attribute inference in VLMs?
- Basis in paper: [inferred] The paper mentions that VLMs are limited in input resolution and struggle to process small details, but does not systematically investigate the impact of resolution on inference accuracy.
- Why unresolved: The paper only briefly touches on resolution limitations in the context of automated zooming and does not conduct a thorough analysis of how varying image resolutions affect VLM performance on privacy-infringing inferences.
- What evidence would resolve it: Conducting experiments with images of varying resolutions and comparing VLM accuracy across these resolutions would provide concrete evidence on the impact of resolution on private attribute inference.

### Open Question 2
- Question: Can privacy-preserving techniques be effectively integrated into VLMs without significantly compromising their general capabilities?
- Basis in paper: [inferred] The paper highlights the need for developing defenses against inference-based privacy attacks but does not explore potential mitigation strategies or their impact on model performance.
- Why unresolved: While the paper advocates for further research into privacy-preserving techniques, it does not provide any concrete analysis of how such techniques might affect VLM capabilities or the trade-offs involved in implementing them.
- What evidence would resolve it: Implementing and evaluating various privacy-preserving techniques (e.g., adversarial anonymization, differential privacy) on VLMs and measuring their impact on both privacy protection and general model performance would provide insights into the feasibility of such approaches.

### Open Question 3
- Question: How do VLMs perform on privacy-infringing inferences in real-world, uncontrolled environments compared to the curated VIP dataset?
- Basis in paper: [explicit] The paper acknowledges that the VIP dataset is limited in size and may not fully capture the variability of real-world scenarios, suggesting a need for larger-scale public benchmarks.
- Why unresolved: The evaluation is conducted on a manually collected and annotated dataset, which may not accurately represent the diversity and complexity of images encountered in real-world applications of VLMs.
- What evidence would resolve it: Conducting privacy-infringing inference experiments on a larger, more diverse dataset of real-world images (e.g., from social media platforms) would provide insights into how VLMs perform outside of controlled laboratory conditions.

## Limitations

- The evaluation relies on Reddit images which may not represent the broader diversity of online content, potentially introducing sampling bias
- The manual annotation process for personal attributes limits dataset scale and may introduce human rater subjectivity
- The analysis focuses on a specific set of 7 VLMs, excluding other prominent models that may have different vulnerability profiles

## Confidence

**High Confidence**: The core finding that VLMs can infer personal attributes from non-human images (77.6% accuracy) is well-supported by direct experimental evidence across multiple models. The observation that inference accuracy scales with general model capabilities is also strongly evidenced through correlation analysis.

**Medium Confidence**: The claim that safety-aligned VLMs can be easily circumvented through prompt engineering is supported by experimental results, but the brittleness of safeguards may vary across different alignment approaches not tested in this study. The cost and speed advantages over human annotation are based on reasonable comparisons but depend on specific implementation details.

**Low Confidence**: Predictions about future privacy risks from upcoming VLM generations, while logically following from current scaling trends, remain speculative without empirical validation on next-generation models.

## Next Checks

1. **Dataset Diversity Validation**: Test the same VLMs on a more diverse corpus of images from multiple social platforms and cultural contexts to assess whether the 77.6% accuracy generalizes beyond Reddit.

2. **Alignment Robustness Testing**: Systematically test a wider range of safety alignment mechanisms (including constitutional AI approaches) against the same prompt engineering techniques to map the landscape of model vulnerability.

3. **Adversarial Attack Comparison**: Compare the effectiveness of simple prompt engineering against more sophisticated adversarial attack methods (like those used in computer vision) to establish whether current bypass techniques represent best-case or worst-case scenarios for privacy protection.