---
ver: rpa2
title: Benchmarking LLMs on the Semantic Overlap Summarization Task
arxiv_id: '2402.17008'
source_url: https://arxiv.org/abs/2402.17008
tags:
- document
- documents
- information
- teler
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks popular Large Language Models on the Semantic
  Overlap Summarization (SOS) task, which requires extracting and condensing shared
  information between two alternative narratives. To expand the available benchmarks,
  the authors introduce the PrivacyPolicyPairs (3P) dataset containing 135 high-quality
  SOS samples sourced from privacy policy documents.
---

# Benchmarking LLMs on the Semantic Overlap Summarization Task

## Quick Facts
- arXiv ID: 2402.17008
- Source URL: https://arxiv.org/abs/2402.17008
- Reference count: 40
- This paper benchmarks 16 popular LLMs on the Semantic Overlap Summarization task using 905,216 distinct prompts and human evaluation on 540 samples.

## Executive Summary
This paper systematically benchmarks Large Language Models on the Semantic Overlap Summarization (SOS) task, which requires extracting and condensing shared information between two alternative narratives. The authors introduce the PrivacyPolicyPairs (3P) dataset containing 135 high-quality SOS samples from privacy policy documents to complement existing benchmarks. Using the TELeR prompting taxonomy, they evaluate 16 models across 905,216 distinct prompts and conduct human evaluation on 540 samples. Results show that TELeR L1 prompts generally perform best automatically, though human annotators slightly prefer TELeR L2 prompts. METEOR, SMS, and Sem-F1 metrics show the strongest correlation with human judgments. gpt-3.5-turbo achieves the highest human preference scores, followed by mpt-30b-chat among open-source models.

## Method Summary
The study benchmarks 16 LLMs across 7 families on the SOS task using zero-shot prompting with the TELeR taxonomy (levels 0-4 plus In-Context Learning). The evaluation covers 905,216 distinct samples across two datasets: AllSides (news articles) and the newly introduced PrivacyPolicyPairs (privacy policies). Automatic evaluation uses 11 metrics including ROUGE, BERTscore, SEM-F1, BLEU, METEOR, chrF, TER, CIDEr, BLEURT, MoverScore, and SMS. Human evaluation involves 3 annotators scoring 540 samples on a 0-5 scale for overlapping information capture. The study systematically tests prompt variations to identify optimal configurations for SOS task performance.

## Key Results
- TELeR L1 prompts generally achieve the best automatic evaluation performance, while human annotators slightly prefer TELeR L2 prompts
- METEOR, SMS, and Sem-F1 metrics show the strongest correlation with human judgments (ρ = 0.46-0.47)
- gpt-3.5-turbo achieves the highest human preference scores, followed by mpt-30b-chat among open-source models
- The 3P dataset provides valuable complementary data but shows different performance patterns than AllSides
- Different TELeR levels impact each model and dataset differently, suggesting domain-specific prompt optimization is necessary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic Overlap Summarization (SOS) is well-suited for LLMs because overlapping content across sources can be used to strengthen citation quality and reduce hallucination in generation.
- Mechanism: By focusing on the common information between two alternative narratives, LLMs can ground their outputs in verifiable facts present in both sources, reducing the likelihood of generating unsupported content.
- Core assumption: The overlapping content between two independent narratives reliably represents factual information that can be used to corroborate claims.
- Evidence anchors:
  - [abstract] "SOS thus enables LLMs to produce outputs that are not only concise but also grounded in their multiple inputs, enhancing transparency and trustworthiness in generation."
  - [section] "Such capabilities are especially well suited for LLMs, which can process long-context inputs and generate fact-based responses grounded in multiple sources."
- Break condition: If the two narratives contain contradictory information rather than complementary overlapping facts, or if the overlap itself is misleading or false.

### Mechanism 2
- Claim: TELeR prompting taxonomy allows systematic exploration of prompt variations to find optimal configurations for SOS task performance.
- Mechanism: TELeR provides a structured framework with 7 levels of prompt complexity, enabling researchers to test how different degrees of detail and instruction affect LLM performance on SOS.
- Core assumption: The performance of LLMs on SOS varies significantly with prompt structure, and a systematic taxonomy can reveal these variations.
- Evidence anchors:
  - [abstract] "we use a standard prompting taxonomy called TELeR to create and evaluate 905, 216 distinct LLM-generated summaries"
  - [section] "we use a standard prompting taxonomy, TELeR (Santu and Feng, 2023), to devise a comprehensive set of prompts with different degrees of detail"
- Break condition: If LLM performance is relatively stable across different prompt styles, making the systematic exploration unnecessary, or if the taxonomy levels don't capture meaningful differences in prompt complexity.

### Mechanism 3
- Claim: Automatic evaluation metrics show varying degrees of correlation with human judgments for SOS task, with METEOR, SMS, and Sem-F1 showing the strongest correlation.
- Mechanism: Different automatic metrics capture different aspects of text quality, and some are better aligned with human perception of semantic overlap quality than others.
- Core assumption: Human evaluation of SOS quality can be approximated by automatic metrics, and some metrics are better approximations than others.
- Evidence anchors:
  - [abstract] "METEOR, SMS, and Sem-F1 metrics show the strongest correlation with human judgments"
  - [section] "we report the System-level Kendall's τ and Pearson's ρ correlation coefficients between all our metrics and our human annotations"
- Break condition: If automatic metrics fail to capture the nuanced aspects of semantic overlap that humans consider important, or if human evaluation itself is inconsistent or unreliable.

## Foundational Learning

- Concept: Semantic Overlap Summarization (SOS)
  - Why needed here: Understanding the specific constraints and goals of SOS is essential for interpreting the benchmarking results and the design of the study.
  - Quick check question: What distinguishes SOS from general multi-document summarization?

- Concept: TELeR Prompting Taxonomy
  - Why needed here: The study uses TELeR to systematically explore prompt variations, so understanding its levels and structure is crucial for interpreting the experimental design.
  - Quick check question: How do TELeR levels 0-4 differ in terms of the instructions and context they provide?

- Concept: Automatic Evaluation Metrics for Text Generation
  - Why needed here: The study uses multiple automatic metrics to evaluate LLM performance, so understanding what each metric measures and its strengths/weaknesses is important.
  - Quick check question: What is the key difference between lexical overlap metrics like ROUGE and embedding-based metrics like BERTscore?

## Architecture Onboarding

- Component map:
  - Datasets: AllSides (2,788 training, 137 test samples) and 3P (135 samples)
  - Models: 16 models across 7 families (gpt-3.5-turbo, gemini-pro, various open-source models)
  - Prompts: TELeR taxonomy levels 0-4 plus In-Context Learning
  - Evaluation: Automatic metrics (11 types) + Human evaluation (3 annotators)
  - Infrastructure: VLLM library for open-source models, GPU acceleration

- Critical path:
  1. Load datasets and prepare document pairs
  2. Generate prompts using TELeR templates for each model
  3. Run inference with each model using generated prompts
  4. Collect and evaluate outputs using automatic metrics
  5. Select samples for human evaluation
  6. Human annotators score selected samples
  7. Analyze correlation between automatic and human scores

- Design tradeoffs:
  - Zero-shot prompting vs. fine-tuning: The study chose zero-shot to establish baselines, but fine-tuning could potentially improve performance.
  - Prompt variety vs. computational cost: Testing 905,216 distinct samples is comprehensive but computationally expensive.
  - Dataset size vs. quality: The 3P dataset is smaller but carefully curated, while AllSides is larger but from a different domain.

- Failure signatures:
  - Low correlation between automatic and human scores indicates automatic metrics may not capture SOS-specific quality
  - Inconsistent human annotations suggest ambiguity in the task definition
  - Poor performance across all models and prompts may indicate fundamental mismatch between task requirements and LLM capabilities

- First 3 experiments:
  1. Test a single model (e.g., gpt-3.5-turbo) with TELeR L1 prompts on a small subset of AllSides data to verify basic functionality
  2. Run all models with TELeR L0 prompts on both datasets to establish baseline performance
  3. Test human evaluation process on 5 samples to calibrate scoring consistency before full deployment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning or LoRA adaptation affect LLMs' performance on the SOS task compared to zero-shot prompting?
- Basis in paper: [inferred] The paper explicitly states that no fine-tuning was performed on evaluated models and mentions that additional performance could be gained using methods like LoRA (Hu et al., 2021).
- Why unresolved: The study intentionally used pre-trained weights to establish baselines, leaving the potential performance gains from fine-tuning unexplored.
- What evidence would resolve it: Comparative experiments showing performance differences between fine-tuned/LoRA-adapted models and zero-shot baselines on the same SOS benchmarks.

### Open Question 2
- Question: What is the relationship between prompt complexity (TELeR level) and performance across different document domains beyond news articles and privacy policies?
- Basis in paper: [explicit] The paper notes that different TELeR levels impact each model and dataset differently, suggesting domain-specific prompt optimization is necessary.
- Why unresolved: The study only evaluated two domains (news articles from AllSides and privacy policies), limiting generalizability of TELeR-level effectiveness.
- What evidence would resolve it: Systematic evaluation of TELeR prompts across diverse domains (legal documents, medical literature, social media, etc.) to identify domain-specific prompt preferences.

### Open Question 3
- Question: Can automatic evaluation metrics for SOS be improved to better align with human judgments, and what would constitute an optimal metric?
- Basis in paper: [explicit] The study found that while SEM-F1 was proposed as a specialized metric for SOS, it was matched by SMS and outperformed by METEOR in Pearson correlation with human judgments.
- Why unresolved: The paper demonstrates current automatic metrics are unreliable for SOS, but doesn't propose solutions for developing more accurate evaluation methods.
- What evidence would resolve it: Development and validation of new metrics specifically designed for SOS that achieve higher correlation with human judgments than existing metrics.

## Limitations

- The 3P dataset contains only 135 samples, which may not provide sufficient coverage to draw definitive conclusions about model performance across the broader space of privacy policy documents
- The zero-shot evaluation approach may underestimate model capabilities compared to fine-tuned alternatives
- Human evaluation, though performed by three annotators, could be influenced by subjective interpretations of what constitutes effective semantic overlap extraction

## Confidence

**High Confidence Claims:**
- The TELeR taxonomy effectively captures meaningful variations in prompt structure that impact SOS performance
- Automatic metrics show varying correlation strengths with human judgments, with METEOR, SMS, and Sem-F1 demonstrating the strongest alignment
- gpt-3.5-turbo consistently achieves top performance across evaluation metrics

**Medium Confidence Claims:**
- TELeR L1 prompts generally perform best in automatic evaluation while L2 prompts are slightly preferred by human annotators
- The PrivacyPolicyPairs dataset provides valuable complementary data to existing benchmarks for SOS task evaluation
- Performance differences between model families follow expected patterns based on model size and architecture

**Low Confidence Claims:**
- The specific numerical performance gaps between models on the 3P dataset are robust findings
- The relative importance of different automatic metrics generalizes beyond the specific datasets used

## Next Checks

1. **Dataset Expansion Validation**: Expand the 3P dataset by 2-3x through additional privacy policy pairs and rerun the full evaluation pipeline to verify whether current performance rankings hold with larger sample sizes.

2. **Fine-tuning Comparison**: Fine-tune a representative open-source model (e.g., mpt-30b-chat) on the AllSides training data and evaluate on both datasets to quantify the performance gap between zero-shot and fine-tuned approaches.

3. **Cross-Domain Transfer**: Test the best-performing models from the AllSides and 3P datasets on an unrelated domain (e.g., news articles or scientific papers) to assess the generalization capabilities of SOS models across different content types.