---
ver: rpa2
title: Scaling Training Data with Lossy Image Compression
arxiv_id: '2407.17954'
source_url: https://arxiv.org/abs/2407.17954
tags:
- scaling
- compression
- image
- test
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how lossy image compression affects the scaling
  of machine learning models trained on large datasets, where storage space is a key
  constraint. The authors propose a 'storage scaling law' that describes the joint
  evolution of test error with the number of training samples and the number of bits
  per image.
---

# Scaling Training Data with Lossy Image Compression

## Quick Facts
- arXiv ID: 2407.17954
- Source URL: https://arxiv.org/abs/2407.17954
- Reference count: 20
- When storage is limited, it's better to have more images of lower quality than fewer images of higher quality

## Executive Summary
This paper studies how lossy image compression affects the scaling of machine learning models trained on large datasets, where storage space is a key constraint. The authors propose a 'storage scaling law' that describes the joint evolution of test error with the number of training samples and the number of bits per image. They prove that this law holds within a stylized model for image compression and verify it empirically on three computer vision tasks: image classification, semantic segmentation, and object detection. By optimizing the lossy compression level, they show that models trained on optimally compressed images present significantly smaller test error compared to models trained on the original data at a given storage budget.

## Method Summary
The authors develop a theoretical framework called the "storage scaling law" that describes how test error scales with both the number of training samples and the bits per image when using lossy compression. They prove this law within a stylized model for image compression, where the storage budget S is the product of the number of samples n and bits per image b (S = n × b). The storage scaling exponent is given by the harmonic mean of the sample size exponent and the bits exponent. They empirically validate this framework across three computer vision tasks (image classification, semantic segmentation, and object detection) by training models on datasets compressed at various bitrates and measuring performance relative to storage budget.

## Key Results
- The storage scaling law holds empirically across image classification, semantic segmentation, and object detection tasks
- Optimizing lossy compression levels leads to significantly smaller test errors compared to using uncompressed images at a given storage budget
- The optimal strategy when storage is limited is to use more compressed images rather than fewer high-quality images
- The storage scaling exponent equals the harmonic mean of the sample size exponent and the bits exponent

## Why This Works (Mechanism)
When storage is constrained, the total information content available for training is fixed as the product of sample count and bits per sample. The storage scaling law captures how the test error evolves with this joint constraint. By compressing images more aggressively, you can fit more samples within the same storage budget, potentially capturing more diverse patterns from the data distribution. The harmonic mean relationship between sample size and bits exponents reflects the trade-off between having more examples versus higher quality examples, where the optimal balance depends on the relative rates at which error decreases with each dimension.

## Foundational Learning
- **Storage scaling law**: A theoretical framework describing how test error scales with both training sample count and bits per image. Needed to understand the joint constraint of limited storage. Quick check: Verify that S = n × b defines the storage budget correctly.
- **Harmonic mean**: The mathematical operation that combines the sample size exponent and bits exponent to yield the storage scaling exponent. Needed to understand the trade-off between quantity and quality. Quick check: Confirm that the harmonic mean appropriately weights the two exponents.
- **Lossy image compression**: Techniques that reduce file size by removing information, typically measured in bits per pixel. Needed to understand how image quality affects model performance. Quick check: Verify that compression levels are properly controlled and measured.

## Architecture Onboarding
- **Component map**: Storage budget (S) -> (Number of samples n, Bits per image b) -> Test error
- **Critical path**: The storage scaling law provides the theoretical foundation, empirical validation across tasks confirms the theory, and optimization of compression levels demonstrates practical utility
- **Design tradeoffs**: The key tradeoff is between having more training samples at lower quality versus fewer samples at higher quality, with the optimal point depending on the relative scaling exponents
- **Failure signatures**: If the storage scaling law doesn't hold empirically, or if optimizing compression doesn't improve performance at fixed storage budgets, the framework would be invalidated
- **First experiments**: 1) Verify the storage scaling law holds on a simple dataset with controlled compression, 2) Test different compression algorithms to see if results generalize, 3) Explore whether the harmonic mean relationship holds across different model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework is derived within a stylized model for image compression that may not capture all real-world complexities
- Empirical validation is limited to three computer vision tasks and may not generalize to all ML problems
- The paper doesn't extensively discuss potential biases introduced by lossy compression for tasks sensitive to specific image features

## Confidence
- **High Confidence**: The theoretical derivation of the storage scaling law and its consistency with observed scaling behavior in ML models
- **Medium Confidence**: The empirical validation across different computer vision tasks given the limited scope of tasks and datasets tested
- **Medium Confidence**: The claim that optimizing lossy compression levels leads to significantly smaller test errors at a given storage budget, as this may depend on the specific compression algorithm and task requirements

## Next Checks
1. **Broader Task and Dataset Validation**: Extend the empirical validation of the storage scaling law to a wider range of machine learning tasks and datasets, including those outside of computer vision, to assess the universality of the findings.

2. **Compression Algorithm Sensitivity**: Investigate how the proposed scaling law and optimization strategy are affected by different lossy compression algorithms, including more advanced or task-specific methods.

3. **Long-term Model Performance**: Conduct studies on the long-term performance of models trained on compressed data, including their robustness, ability to generalize to uncompressed data, and performance on downstream tasks that may require high-fidelity input.