---
ver: rpa2
title: Integrating Supertag Features into Neural Discontinuous Constituent Parsing
arxiv_id: '2410.08766'
source_url: https://arxiv.org/abs/2410.08766
tags:
- parsing
- discontinuous
- figure
- constituent
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis investigates the integration of CCG supertag information
  into neural discontinuous constituent parsing. A manual analysis reveals that CCG
  supertags provide useful information for resolving some discontinuous phenomena
  like wh-extraction, fronted quotations, and it-extrapositions, while being less
  helpful for others such as circumpositioned quotations and extraposed dependents.
---

# Integrating Supertag Features into Neural Discontinuous Constituent Parsing

## Quick Facts
- arXiv ID: 2410.08766
- Source URL: https://arxiv.org/abs/2410.08766
- Authors: Lukas Mielczarek
- Reference count: 40
- Primary result: Auxiliary approach with CCG supertagging improves F-score by 0.47 and discontinuous F-score by 3.06 over baseline

## Executive Summary
This thesis explores integrating Combinatory Categorial Grammar (CCG) supertag information into neural discontinuous constituent parsing. The work demonstrates that CCG supertags, which encode lexical syntactic categories and argument structures, can enhance parsing performance for discontinuous phenomena like wh-extraction, fronted quotations, and it-extrapositions. Two approaches are investigated: a pipeline method using pre-trained supertagger output as features, and an auxiliary multi-task learning approach that jointly trains parsing and supertagging. Experiments on the discontinuous Penn Treebank show the auxiliary approach outperforms both the pipeline model and baseline parser, validating the effectiveness of CCG supertag integration for handling discontinuities.

## Method Summary
The work employs a stack-free transition-based parser with bi-directional LSTM layers, enhanced with either pipeline or auxiliary supertag integration. The pipeline approach concatenates pre-trained supertag probability distributions with token representations as input features. The auxiliary approach jointly trains parsing and supertagging tasks using shared LSTM layers with residual connections (gated or additive). Multiple auxiliary tasks are explored including chunking, dependency parsing, and LCFRS supertagging. The model uses a dynamic oracle for training and static oracle for evaluation, with performance measured via F-score and discontinuous F-score on the DPTB corpus.

## Key Results
- Auxiliary CCG supertag model improves F-score by 0.47 and discontinuous F-score by 3.06 over baseline
- Pipeline approach with pre-trained supertags shows moderate improvements but underperforms auxiliary method
- Manual analysis reveals supertags are particularly effective for wh-extraction, fronted quotations, and it-extrapositions
- Gated residual connections in multi-task models provide additional improvements over standard additive residuals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supertag features provide direct syntactic role information that helps resolve discontinuous constituent parsing ambiguities
- Mechanism: CCG supertags encode both argument structure and syntactic roles at the lexical level, allowing the parser to infer discontinuous relationships without needing to reconstruct them from surface word order
- Core assumption: The statistical correlation between CCG supertag assignments and discontinuous constituent structures is strong enough to be learned by neural models
- Evidence anchors:
  - [abstract]: "CCG supertags indicate a word's structural role and syntactic relationship with surrounding items" and "CCG supertags can effectively enhance discontinuous constituent parsing by leveraging statistical correlations"
  - [section 5.3]: Manual analysis shows specific supertag assignments correspond to different discontinuity types (wh-extraction, fronted quotations, it-extrapositions)
  - [corpus]: Weak - the corpus neighbors show no citations yet, suggesting this is novel territory
- Break condition: If the correlation between supertag assignments and discontinuous structures is too weak or inconsistent across phenomena, the neural model cannot learn useful patterns

### Mechanism 2
- Claim: Multi-task learning with CCG supertagging creates shared representations that capture syntactic dependencies beneficial for parsing
- Mechanism: Joint training on parsing and supertagging forces the model to build intermediate representations that encode both tasks' requirements, leading to better contextual embeddings for discontinuous structures
- Core assumption: CCG supertagging and discontinuous constituent parsing share sufficient underlying syntactic knowledge to benefit from shared representations
- Evidence anchors:
  - [abstract]: "auxiliary approach jointly training parsing and supertagging" shows this was implemented and tested
  - [section 6.2.1]: "exploiting statistical correlations between the two tasks" and hierarchical arrangement of easier supertagging task below parsing
  - [section 6.3.2]: Results show CCGgate model improves F-score by 0.47 and discontinuous F-score by 3.06 compared to baseline
- Break condition: If the tasks are too dissimilar or the shared representation becomes dominated by one task's requirements, the auxiliary benefit disappears

### Mechanism 3
- Claim: Gated residual connections in multi-layer LSTMs enable better propagation of syntactic information across layers for discontinuous parsing
- Mechanism: Residual gates allow selective flow of information from lower layers to higher layers, preventing vanishing gradients and enabling the model to dynamically route useful syntactic features for discontinuity resolution
- Core assumption: The additional computational capacity and dynamic routing provided by gated residuals improves the model's ability to capture long-range dependencies
- Evidence anchors:
  - [section 6.2.2]: Introduces gated residual connections as improvement over standard additive residuals
  - [section 6.3.2]: Ctrgate3 shows improvements over baseline (+0.39 F, +2.0 DF) and CCGgate further improves on this (+0.08 F, +1.06 DF)
  - [section 6.2.1]: Hierarchical arrangement assumes supertagging benefits from lower-level shared representations
- Break condition: If the gating mechanism doesn't learn useful routing patterns or if the additional complexity causes overfitting, the benefits disappear

## Foundational Learning

- Concept: Linear Context-Free Rewriting Systems (LCFRS)
  - Why needed here: LCFRS is the formal framework that naturally captures discontinuous constituents used in treebanks like NeGra, TIGER, and DPTB
  - Quick check question: What is the key difference between CFG and LCFRS that allows LCFRS to handle discontinuities?

- Concept: Transition-based parsing
  - Why needed here: The stack-free transition system used in this work processes sentences incrementally using local actions, making it suitable for neural implementation
  - Quick check question: How does the stack-free configuration differ from traditional shift-reduce parsing?

- Concept: Lexicalized grammar formalisms (CCG, LTAG)
  - Why needed here: These formalisms associate rich syntactic information with individual words (supertags), which is the basis for the supertag integration approach
  - Quick check question: What is the main advantage of lexicalized grammars over traditional CFGs for handling discontinuities?

## Architecture Onboarding

- Component map: Token representation layer -> BiLSTM stack -> Task-specific heads -> Transition system -> Oracle
- Critical path: Token representation → BiLSTM layers → Parsing action prediction → Transition execution → Tree construction
- Design tradeoffs:
  - Pipeline vs. auxiliary approach: Pipeline is simpler but loses gradient flow; auxiliary enables shared learning but adds complexity
  - Number of BiLSTM layers: More layers increase capacity but risk overfitting and training difficulty
  - Residual connection type: Gated allows dynamic routing but is more complex than additive
  - Supertag source: Pre-trained vs. learned during training affects domain coverage and training time
- Failure signatures:
  - Training instability: Likely from poor residual connection design or task imbalance
  - Degraded discontinuous parsing: May indicate supertags are introducing noise or the model can't learn the correlation
  - Overfitting on development set: Suggests model capacity is too high relative to data size
- First 3 experiments:
  1. Implement baseline stack-free parser without supertags to establish performance floor
  2. Add CCG supertags as pipeline feature using pre-trained depccg output to test direct feature integration
  3. Implement auxiliary multi-task model with CCG supertagging at intermediate layer to test shared representation benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are supertags for specific types of discontinuous phenomena (e.g. wh-extraction vs. circumpositioned quotations)?
- Basis in paper: [explicit] The paper performs a detailed per-phenomenon analysis comparing the pipeline and auxiliary models' performance on various types of discontinuities.
- Why unresolved: While the paper provides results for specific phenomena, the underlying reasons for the varying effectiveness of supertags across different types are not fully explored.
- What evidence would resolve it: A deeper analysis of the CCG category assignments for each phenomenon type and their correlation with the DPTB annotations would help explain the varying effectiveness.

### Open Question 2
- Question: What is the impact of using gated residual connections in multi-task models for discontinuous constituent parsing?
- Basis in paper: [explicit] The paper explores using gated residual connections in the auxiliary CCG model and shows improved performance compared to the baseline.
- Why unresolved: The paper does not fully investigate the mechanisms by which gated residual connections improve performance or compare them to other types of residual connections.
- What evidence would resolve it: Experiments comparing different types of residual connections (e.g. additive, multiplicative) and analyzing their impact on the model's ability to learn shared representations would be beneficial.

### Open Question 3
- Question: How useful are other syntactic sequence labeling tasks (e.g. chunking, dependency parsing) as auxiliary objectives for discontinuous constituent parsing?
- Basis in paper: [explicit] The paper performs experiments with various auxiliary tasks including chunking, dependency parsing, and LCFRS supertagging.
- Why unresolved: The paper provides initial results but does not fully explore the potential synergies between these tasks and discontinuous constituent parsing or investigate the impact of task imbalance.
- What evidence would resolve it: Further experiments with different task combinations, architectures, and regularization techniques would help determine the optimal auxiliary tasks and training strategies.

## Limitations

- Small training corpus (20K sentences) may limit generalizability of results
- Manual analysis of supertag effectiveness is subjective and may not capture all relevant factors
- Specific architectural choices (4-layer BiLSTM, gated residuals) may be overfitted to this particular corpus
- Limited evaluation on other discontinuous treebanks prevents assessment of broader applicability

## Confidence

- **High Confidence**: The basic experimental setup (pipeline vs auxiliary approach) is well-specified and reproducible. The performance improvements over baseline are measurable and statistically significant within the reported experimental framework.
- **Medium Confidence**: The manual analysis connecting specific supertag assignments to discontinuity resolution is reasonable but subjective. The claim that CCG supertags provide useful information for some phenomena while being less helpful for others is supported but needs broader validation.
- **Low Confidence**: The generalizability of the improvements to other discontinuous treebanks or languages remains untested. The specific architectural choices (4-layer BiLSTM, gated residuals) may be overfitted to this particular corpus and task.

## Next Checks

1. **Cross-corpus validation**: Test the auxiliary CCG supertag model on other discontinuous treebanks (NeGra, TIGER) to verify the improvements are not corpus-specific artifacts.

2. **Ablation study**: Systematically remove individual components (gated residuals, specific supertag features) to isolate which aspects of the approach drive the performance gains versus baseline.

3. **Phenomenon-specific analysis**: Expand the per-phenomenon evaluation beyond the 8 tested categories to include all discontinuity types in the corpus, with quantitative measures of when supertags help versus hurt.