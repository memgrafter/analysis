---
ver: rpa2
title: 'LookALike: Human Mimicry based collaborative decision making'
arxiv_id: '2403.10824'
source_url: https://arxiv.org/abs/2403.10824
tags:
- learning
- llms
- knowledge
- tasks
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method called LookALike to enhance
  LLM agents' problem-solving skills through human mimicry and collaborative decision-making.
  The core idea is to use a vision LLM (vLLM) as a domain expert to analyze human
  gameplay videos and generate reward models for a player LLM (pLLM).
---

# LookALike: Human Mimicry based collaborative decision making

## Quick Facts
- arXiv ID: 2403.10824
- Source URL: https://arxiv.org/abs/2403.10824
- Reference count: 8
- Key outcome: 100% success on Zork, 56% on procgen tasks vs standalone pLLM failure

## Executive Summary
This paper introduces LookALike, a novel method that enhances LLM agents' problem-solving skills through human mimicry and collaborative decision-making. The approach uses a vision LLM (vLLM) as a domain expert to analyze human gameplay videos and generate reward models for a player LLM (pLLM). Through swarm collaboration and vLLM feedback, pLLM agents improve their performance on tasks like text-based games and visual scenarios without relying on stored data or pretraining. Experiments demonstrate significant improvements in plan success rates compared to standalone pLLM approaches.

## Method Summary
LookALike employs a vision LLM (vLLM) to analyze human gameplay videos and generate reward models that guide player LLM (pLLM) decision-making. Multiple pLLM instances with different temperature settings collaborate in a swarm, sharing successful strategies while exploring diverse approaches. The vLLM provides real-time reward signals based on human gameplay patterns, enabling pLLM agents to improve their performance through collaborative decision-making without requiring pretraining or stored data.

## Key Results
- 100% plan success rate on Zork text-based game
- 56% success rate on procgen visual tasks
- Standalone pLLM failed on procgen tasks, while LookALike achieved 56% success

## Why This Works (Mechanism)

### Mechanism 1
- Claim: vLLM extracts temporal and contextual patterns from human gameplay videos to generate reward models for pLLM
- Mechanism: vLLM uses its multimodal understanding to analyze human gameplay sequences, identifying optimal action patterns and contextual cues that inform the reward structure given to pLLM
- Core assumption: Gemini 1.5 Pro's large context window and multimodal capabilities enable effective extraction of actionable patterns from human gameplay videos
- Evidence anchors: [abstract] "use a vision LLM (vLLM) as a domain expert to analyze human gameplay videos and generate reward models"; [section 4.2] "Gemini 1.5 is capable of handling a very large context and also multimodal"
- Break condition: If vLLM cannot effectively translate visual gameplay patterns into meaningful reward signals for text-based decision making

### Mechanism 2
- Claim: pLLM swarm collaboration with different temperature settings enables exploration of diverse strategies while maintaining shared learning
- Mechanism: Multiple pLLM instances with varying temperature parameters explore different action paths, their outcomes are evaluated by vLLM rewards, and successful patterns are shared across the swarm
- Core assumption: Different temperature settings in pLLM create sufficiently diverse exploration paths while remaining within the bounds of effective gameplay
- Evidence anchors: [section 5.1] "for pLLM swarms, for each step, after each reward/punishment for each of the pLLMs for each step, the results are stored and passed back as 'memory' to the pLLMs"
- Break condition: If swarm diversity leads to conflicting reward signals that prevent convergence on effective strategies

### Mechanism 3
- Claim: Real-time human mimicry enables knowledge distillation without pretraining or stored data, preserving contextual nuances
- Mechanism: vLLM learns from human gameplay patterns in real-time and generates reward signals that encode domain-specific knowledge, which pLLM agents internalize through collaborative decision-making
- Core assumption: Human gameplay videos contain sufficient information to reconstruct domain-specific reasoning patterns without explicit rule encoding
- Evidence anchors: [abstract] "without relying on any stored data or pretraining"; [section 4.1] "We believe the ability to faithfully learn from others is a powerful tool for LLMs"
- Break condition: If human gameplay patterns are too complex or domain-specific to be effectively translated into reward signals

## Foundational Learning

- Concept: Reinforcement Learning with Human Feedback (RLHF)
  - Why needed here: The system uses reward models derived from human gameplay to guide pLLM decision-making, similar to how RLHF aligns language models with human preferences
  - Quick check question: How does the reward signal flow from vLLM to pLLM in this architecture?

- Concept: Multimodal Learning
  - Why needed here: vLLM must process visual gameplay data and translate it into textual reward signals that pLLM can use for text-based game decisions
  - Quick check question: What challenges arise when translating visual patterns into text-based reward structures?

- Concept: Multi-Agent Systems and Swarm Intelligence
  - Why needed here: The pLLM swarm with different temperature settings enables exploration of diverse strategies while maintaining collaborative learning
  - Quick check question: How does the swarm mechanism prevent individual pLLM agents from getting stuck in suboptimal strategies?

## Architecture Onboarding

- Component map: Human gameplay videos -> vLLM (Gemini 1.5 Pro) -> Reward model -> pLLM swarm (gemma-7b) -> Game environment -> Feedback loop
- Critical path: 1. Human gameplay videos fed to vLLM; 2. vLLM generates reward model and optimal pathways; 3. pLLM agents receive environment state and reward guidance; 4. pLLM makes decisions and receives outcome feedback; 5. Swarm shares successful strategies across temperature variants; 6. Process repeats until goal achievement
- Design tradeoffs: Using gemma-7b instead of larger models for pLLM trades raw capability for faster iteration and clearer demonstration of framework effectiveness; Real-time video analysis by vLLM enables dynamic adaptation but may limit scalability compared to pre-trained reward models; Swarm approach increases exploration but adds complexity in reward signal aggregation
- Failure signatures: pLLM consistently ignores vLLM reward signals; Swarm agents converge to identical suboptimal strategies; vLLM reward model becomes too complex for pLLM to effectively utilize; Temperature diversity fails to produce meaningful strategy variation
- First 3 experiments: 1. Test standalone pLLM performance on Zork without any vLLM reward guidance to establish baseline; 2. Test single pLLM with vLLM reward guidance on Zork to validate basic reward signal effectiveness; 3. Test pLLM swarm with vLLM rewards on Zork to verify collaborative improvement over individual performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LookALike scale with increasing complexity of the game environment?
- Basis in paper: [inferred] The paper mentions that current test environments were relatively simple compared to some planning challenges, and suggests exploring if LLMs can scale to more complex logic.
- Why unresolved: The paper does not provide experimental results on more complex game environments or planning scenarios.
- What evidence would resolve it: Experiments testing LookALike on increasingly complex game environments or planning scenarios, measuring performance and comparing it to baseline methods.

### Open Question 2
- Question: What is the impact of using different types of vision models (e.g., different architectures or training datasets) as the domain expert (vLLM) on the performance of LookALike?
- Basis in paper: [explicit] The paper uses Google Gemini 1.5 Pro as the vLLM but does not explore the impact of using other vision models or variations in its architecture or training data.
- Why unresolved: The paper only uses one specific vision model and does not investigate the sensitivity of LookALike to different choices of vision models.
- What evidence would resolve it: Experiments using different vision models as the vLLM, comparing their performance in generating reward models and their impact on the overall performance of LookALike.

### Open Question 3
- Question: How does the collaborative decision-making process in LookALike handle conflicting or contradictory feedback from different pLLM agents?
- Basis in paper: [inferred] The paper mentions that pLLMs with different temperature assume different roles and achieve efficiency by collaborating and passing contextual information, but does not explicitly address how conflicting feedback is resolved.
- Why unresolved: The paper does not provide details on the mechanism for resolving conflicts or contradictions in the feedback from different pLLM agents.
- What evidence would resolve it: Experiments analyzing the decision-making process in LookALike, particularly focusing on how it handles conflicting feedback from pLLM agents, and evaluating the impact of different conflict resolution strategies on performance.

## Limitations

- Limited generalization to broader problem domains beyond text-based games and visual scenarios
- Lack of detail about human gameplay videos used for vLLM training makes scalability assessment difficult
- No analysis of computational costs or real-time performance constraints of the multi-stage architecture

## Confidence

- High confidence: Basic architectural claims about vLLM generating reward models from human gameplay and pLLM using these rewards for decision-making
- Medium confidence: Claims about swarm collaboration with different temperature settings producing superior results compared to individual pLLM agents
- Medium confidence: Assertion that knowledge transfer occurs without pretraining or stored data

## Next Checks

1. Ablation study on swarm configuration: Run experiments comparing pLLM performance with and without swarm collaboration (single pLLM instance) while keeping vLLM rewards constant to isolate the contribution of temperature diversity to performance improvements.

2. Cross-domain generalization test: Apply the LookALike framework to a different type of game or decision-making task (e.g., strategy games or real-world planning scenarios) to verify whether the human mimicry approach generalizes beyond the specific text-based and visual environments tested.

3. Reward signal interpretability analysis: Conduct qualitative analysis of vLLM-generated reward signals to verify that they meaningfully capture human decision patterns rather than arbitrary correlations, including testing whether removing or modifying specific reward components degrades pLLM performance in predictable ways.