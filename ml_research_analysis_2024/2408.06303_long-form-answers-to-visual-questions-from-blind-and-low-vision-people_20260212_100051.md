---
ver: rpa2
title: Long-Form Answers to Visual Questions from Blind and Low Vision People
arxiv_id: '2408.06303'
source_url: https://arxiv.org/abs/2408.06303
tags:
- gid00001
- answer
- answers
- image
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces VizWiz-LF, a dataset of long-form visual question
  answers (LFVQA) for questions from blind and low-vision (BLV) people. The dataset
  contains 4.2k long-form answers collected from human experts and six vision-language
  models (VLMs) for 600 visual questions.
---

# Long-Form Answers to Visual Questions from Blind and Low Vision People

## Quick Facts
- arXiv ID: 2408.06303
- Source URL: https://arxiv.org/abs/2408.06303
- Reference count: 40
- Key outcome: This work introduces VizWiz-LF, a dataset of long-form visual question answers (LFVQA) for questions from blind and low-vision (BLV) people, revealing that while BLV people prefer long-form answers to short ones, generated answers often hallucinate incorrect visual details, especially for unanswerable questions.

## Executive Summary
This paper introduces VizWiz-LF, a dataset of long-form visual question answers for questions from blind and low-vision (BLV) people, containing 4.2k long-form answers collected from human experts and six vision-language models for 600 visual questions. The study develops a taxonomy of functional roles and information sources for LFVQA sentences and evaluates six VLMs' ability to generate helpful long-form answers. Human evaluation with BLV and sighted participants shows that while BLV people prefer long-form answers to short ones, generated answers often hallucinate incorrect visual details, especially for unanswerable questions. The work highlights the importance of evaluating LFVQA beyond factual accuracy to consider relevance and plausibility for end users.

## Method Summary
The study collected 4.2k long-form answers for 600 visual questions from the VizWiz dataset, with expert-generated descriptions serving as references. Six vision-language models (GPT-4V, Gemini, LLaVA, InstructBLIP, QWEN, BLIP-2) were prompted to generate long-form answers using zero-shot instruction prompting. Functional roles and information sources were annotated for all answer sentences, with a GPT-4 classifier trained to predict these labels. The research employed both automatic evaluation metrics (ROUGE, METEOR, BERTScore, LAVE) and human evaluation with BLV and sighted participants to assess answer quality. Abstemion strategies were tested by prompting VLMs with instructions to evaluate image quality and object visibility before answering.

## Key Results
- BLV participants preferred long-form answers over short-form answers for visual questions
- Generated answers from VLMs often hallucinate incorrect visual details, especially for unanswerable questions
- GPT-4 achieved the highest abstention recall (0.82) for identifying unanswerable questions when prompted with abstention instructions
- Sighted people's evaluation without an image is not a strong proxy for BLV preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstention instruction prompting increases VLMs' ability to correctly identify and skip unanswerable questions
- Mechanism: By explicitly instructing the model to evaluate image quality and object visibility before answering, the model learns to recognize when its confidence is too low to provide a reliable answer
- Core assumption: VLMs can effectively parse quality indicators in images when explicitly prompted to do so
- Evidence anchors:
  - [section] "Only GPT-4 achieved high recall for abstention (0.82) followed by QWEN (0.42) with default settings. We further experiment with multiple abstention approaches and reveal that prompting VLMs with abstain instructions can reduce hallucinations."
  - [abstract] "The study evaluates VLM abstention strategies, finding that GPT-4 achieves the highest recall (0.82) for abstaining from answering unanswerable questions."
  - [corpus] Weak - no corpus evidence available for this specific abstention mechanism
- Break condition: If the instruction prompt fails to activate the model's quality assessment capabilities, or if the model consistently misclassifies image quality despite prompting

### Mechanism 2
- Claim: Long-form answers provide richer contextual information that better serves BLV users' needs
- Mechanism: Extended responses allow for explanations, suggestions, and auxiliary information beyond simple factual answers, addressing the complexity of real-world visual accessibility challenges
- Core assumption: BLV users benefit more from comprehensive descriptions that include context and reasoning rather than brief factual responses
- Evidence anchors:
  - [abstract] "BLV people perceive both human-written and generated long-form answers to be plausible, but generated answers often hallucinate incorrect visual details"
  - [section] "Our results also reveal that sighted people's evaluation without an image is not a strong proxy for BLV preferences"
  - [corpus] Moderate - related work shows BLV users prefer detailed descriptions (MacLeod et al., 2017; Salisbury et al., 2017)
- Break condition: If the additional information in long-form answers consistently proves irrelevant or distracting to users, reducing overall utility

### Mechanism 3
- Claim: Functional role classification enables more nuanced evaluation of LFVQA beyond factual accuracy
- Mechanism: By categorizing sentences into roles like confirmation, explanation, and suggestion, evaluators can assess different aspects of answer quality appropriate to user needs
- Core assumption: Different sentence types serve different communicative purposes and should be evaluated according to their specific role rather than uniform factual accuracy
- Evidence anchors:
  - [section] "To understand the content of LFVQA, we design and annotate functional roles of sentences in LFVQA"
  - [abstract] "We develop a taxonomy of functional roles (e.g., answer, explanation, suggestion) and information sources"
  - [corpus] Weak - no corpus evidence available for this specific functional role evaluation mechanism
- Break condition: If functional role classification proves too subjective or inconsistent to provide reliable evaluation guidance

## Foundational Learning

- Concept: Visual Question Answering (VQA) fundamentals
  - Why needed here: Understanding how VQA systems process images and generate answers is essential for grasping the limitations of short-form vs long-form responses
  - Quick check question: What are the key differences between short-form and long-form VQA in terms of output structure and user utility?

- Concept: Multimodal model architecture
  - Why needed here: Understanding how vision-language models integrate visual and textual information explains their capability to generate detailed responses
  - Quick check question: How do modern VLMs like GPT-4V and Gemini differ in their approach to combining vision and language processing?

- Concept: Hallucination detection in AI systems
  - Why needed here: The work heavily focuses on hallucination issues in VLMs, making it crucial to understand how models generate incorrect information
  - Quick check question: What are the primary causes of hallucination in vision-language models when processing low-quality images?

## Architecture Onboarding

- Component map: Visual Question → VQA Model Generation → Functional Role Classification → Evaluation → User Feedback Loop
- Critical path: Question → VQA model generation → Functional role classification → Evaluation → User feedback loop
- Design tradeoffs: Longer answers provide more context but increase hallucination risk; comprehensive evaluation requires more resources but provides better insights into model limitations
- Failure signatures: Low abstention recall indicates models struggle with identifying unanswerable questions; high hallucination rates suggest quality assessment mechanisms need improvement
- First 3 experiments:
  1. Test abstention instruction effectiveness across all six VLMs using the same prompt format
  2. Compare functional role distributions between human expert and model-generated answers
  3. Evaluate the correlation between automatic metrics and human judgments for different answer types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the functional roles of long-form visual question answers (LFVQA) differ across domains beyond visual questions asked by blind and low-vision people?
- Basis in paper: Explicit - The authors state "While we analyzed long-form answers to visual questions asked by BLV people, LFVQA also occurs in other use cases... Future work will explore extending our functional roles and information types to other domains."
- Why unresolved: The current study focuses specifically on visual questions from BLV individuals. The paper acknowledges that other domains may have different proportions of functional roles and potentially additional roles not yet observed.
- What evidence would resolve it: Conducting a similar functional role analysis on LFVQA datasets from other domains (e.g., math problem screenshots, product identification, medical imaging) and comparing the distribution of functional roles and information types to the current dataset.

### Open Question 2
- Question: What is the optimal balance between providing detailed long-form answers and avoiding hallucinations when VQA models cannot confidently answer a question?
- Basis in paper: Inferred - The authors evaluate VQA models' ability to abstain from answering unanswerable questions and find that prompting VLMs with abstain instructions can reduce hallucinations. However, they don't explore the optimal balance between detail and accuracy.
- Why unresolved: The study shows that abstention instructions help reduce hallucinations but doesn't determine the ideal level of detail for long-form answers when models are uncertain.
- What evidence would resolve it: Human evaluation studies comparing user preferences for long-form answers with varying levels of detail and uncertainty expression, measuring both user satisfaction and error rates.

### Open Question 3
- Question: How does visual priming bias affect the evaluation of long-form visual question answers by sighted versus blind/low-vision individuals?
- Basis in paper: Explicit - The authors conduct evaluations under conditions of {BLV, sighted}×{with image, without image} and find that "sighted people's evaluation without an image is not a strong proxy for BLV preferences."
- Why unresolved: While the study reveals differences in evaluation between BLV and sighted participants, it doesn't fully explore the mechanisms of visual priming bias or its impact on different aspects of answer evaluation (e.g., relevance, helpfulness, plausibility).
- What evidence would resolve it: Controlled experiments manipulating visual priming conditions and measuring their effects on evaluation metrics across BLV and sighted groups, potentially using eye-tracking or reaction time measures to understand cognitive processes.

## Limitations

- The VizWiz-LF dataset, while valuable, is relatively small with only 600 visual questions and 4.2k long-form answers, potentially limiting generalizability to all BLV accessibility challenges
- Reliance on expert-generated answers as reference may not reflect the full range of response styles that could be helpful to BLV users in practice
- The study focuses specifically on visual questions from BLV individuals, leaving open questions about how functional roles and evaluation criteria might differ in other domains

## Confidence

- Abstention mechanism effectiveness: Medium - Only GPT-4 showed strong performance, and results may vary with different prompting strategies
- BLV preference for long-form answers: High - Directly supported by human evaluation with BLV participants
- Functional role taxonomy generalizability: Low - Developed specifically for this dataset without extensive validation across different domains

## Next Checks

- Test the functional role classifier on a separate dataset to assess generalizability across different domains
- Evaluate abstention performance with additional VLMs and prompt variations to determine optimal prompting strategies
- Conduct longitudinal user studies to determine whether long-form answer preferences persist over time and across different use contexts