---
ver: rpa2
title: 'When Parts Are Greater Than Sums: Individual LLM Components Can Outperform
  Full Models'
arxiv_id: '2406.13131'
source_url: https://arxiv.org/abs/2406.13131
tags:
- components
- accuracy
- component
- language
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models perform in-context
  learning by analyzing the contributions of individual model components (attention
  heads and MLPs). It identifies three types of components: good-performing ones that
  can outperform the full model, bad-performing ones that do worse than chance, and
  label-biased ones that always predict the same label.'
---

# When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models

## Quick Facts
- arXiv ID: 2406.13131
- Source URL: https://arxiv.org/abs/2406.13131
- Authors: Ting-Yun Chang; Jesse Thomason; Robin Jia
- Reference count: 40
- This paper investigates how large language models perform in-context learning by analyzing the contributions of individual model components (attention heads and MLPs).

## Executive Summary
This paper investigates how large language models perform in-context learning by analyzing the contributions of individual model components (attention heads and MLPs). It identifies three types of components: good-performing ones that can outperform the full model, bad-performing ones that do worse than chance, and label-biased ones that always predict the same label. The study finds that component behaviors are consistent across different demonstrations and similar prompt templates, even when full model performance varies greatly. Based on these insights, the authors propose component reweighting, which learns to scale component activations from a few labeled examples. This method improves 24-shot in-context learning accuracy by an average of 6.0% across 8 tasks on various models including Llama-2-7B and Mistral-Instruct-7B, while maintaining fast inference speed.

## Method Summary
The method decomposes a Transformer into attention heads and MLPs, treating each as a separate "component" whose direct contribution to output logits can be calculated through early decoding using the output embedding matrix. Component reweighting caches activations from a few labeled examples, then learns a weight for each component to scale its contribution to the output logits. This soft selection of components outperforms both equal weighting and prompt selection. The approach operates within the in-context learning framework, where models adapt to tasks using few labeled examples without parameter updates.

## Key Results
- Component reweighting improves 24-shot in-context learning accuracy by an average of 6.0% across 8 tasks on various models including Llama-2-7B and Mistral-Instruct-7B
- Individual components can exhibit task-specific behaviors that persist across different prompt variants, enabling transfer learning within the same model
- Good-performing components emerge early in pretraining before the full model acquires the ability to perform the task well

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Component reweighting improves ICL accuracy by learning to scale the direct contributions of individual model components based on few labeled examples.
- Mechanism: The method caches component activations from a few labeled examples, then learns a weight for each component to scale its contribution to the output logits. This soft selection of components outperforms both equal weighting and prompt selection.
- Core assumption: The best-performing components identified on a small training set will generalize to improve predictions on unseen test data.
- Evidence anchors:
  - [abstract] "Given 24 labeled examples, our method improves by an average of 6.0% accuracy points over 24-shot ICL across 8 tasks"
  - [section 5.3] "Overall, COMP RW achieves the best average accuracy in all setups, outperforming STANDARD 12 by 6.0%, 1.8%, 2.6%, 1.4% on Llama-2-7B, Llama-2-13B, Mistral-Instruct-7B, and Llama-3-8B, respectively"
  - [corpus] Weak - the corpus neighbors don't directly address this specific mechanism, though "Component Based Quantum Machine Learning Explainability" touches on component importance in ML models
- Break condition: If component behaviors are highly inconsistent across different demonstrations or templates, the learned weights would fail to generalize.

### Mechanism 2
- Claim: Individual components can exhibit task-specific behaviors that persist across different prompt variants, enabling transfer learning within the same model.
- Mechanism: The decomposition reveals that certain components consistently perform well (or poorly) across different demonstrations and minimally contrastive templates, suggesting these components encode task-relevant information independent of specific prompt formatting.
- Core assumption: Component behaviors are relatively stable across different demonstrations and similar prompt templates, even when full model accuracy varies.
- Evidence anchors:
  - [section 4.1] "we find that component accuracies correlate well across different demonstrations (r = 0.80 on average) and contrast set templates (r = 0.57)"
  - [section 4.2] "TRANSFER-1 closely matches ORACLE-1 overall, suggesting that the top-performing components are transferable across data distribution"
  - [corpus] Weak - corpus doesn't provide direct evidence for this specific claim about component consistency
- Break condition: If component behaviors are highly sensitive to prompt formatting or demonstration selection, transfer would fail.

### Mechanism 3
- Claim: Good-performing components emerge early in pretraining before the full model acquires the ability to perform the task well.
- Mechanism: Monitoring component accuracies during pretraining shows that top-performing components achieve good accuracy plateaus early, while the full model's accuracy fluctuates and improves later.
- Core assumption: The ability to perform a task is distributed across components, with some components acquiring task-relevant capabilities before others.
- Evidence anchors:
  - [section 6] "While the full model (green) fluctuates and has a large variance across prompts, the top-1 components (solid blue) achieve good accuracy at an early step and plateau quickly"
  - [section 6] "Our findings also hold on SST2 and Pythia-1.4B (see Figure 6 in the appendix), suggesting that the model's ability to do a task emerges before it is apparent from the full model on these tasks"
  - [corpus] Weak - corpus doesn't directly address pretraining dynamics of components
- Break condition: If task-relevant abilities emerge simultaneously across all components or if pretraining dynamics vary significantly across tasks.

## Foundational Learning

- Concept: Residual stream computation in Transformers
  - Why needed here: Understanding how component contributions flow through the residual stream is essential for decomposing the model output into individual component contributions
  - Quick check question: In the residual stream formulation, what mathematical operation combines the contributions of attention heads, MLPs, and the initial hidden state?

- Concept: Early decoding and component activation interpretation
  - Why needed here: The method relies on interpreting component activations by projecting them through the output embedding matrix to obtain their direct contributions to the logits
  - Quick check question: How does early decoding differ from standard decoding in terms of when the output embedding matrix is applied?

- Concept: Few-shot learning and in-context learning paradigms
  - Why needed here: The work operates within the ICL framework, where models adapt to tasks using few labeled examples without parameter updates
  - Quick check question: What distinguishes in-context learning from traditional few-shot learning approaches that involve parameter updates?

## Architecture Onboarding

- Component map: Attention heads and MLPs are treated as separate components; their contributions flow through residual connections and are combined in the residual stream
- Critical path: Forward pass with demonstrations → Cache component activations → Train component weights on labeled examples → Inference using weighted component contributions
- Design tradeoffs: Soft component selection (reweighting) vs hard selection (pruning), few-shot adaptation vs full fine-tuning, cache-based training vs real-time computation
- Failure signatures: Large variance in component accuracies across demonstrations indicates instability; poor performance of reweighted model suggests bad component weight learning; training divergence indicates hyperparameter issues
- First 3 experiments:
  1. Verify component decomposition by comparing full model output with sum of individual component contributions
  2. Test component consistency by measuring correlation of component accuracies across different demonstration sets
  3. Evaluate reweighting effectiveness by comparing against baseline ICL and prompt selection methods on a simple binary classification task

## Open Questions the Paper Calls Out

# Open Question 1
- Question: How do bad-performing components function across different tasks beyond the SST2 case study?
- Basis in paper: [inferred] The paper observes bad-performing components that do much worse than chance, but only provides a detailed mechanistic understanding for SST2 task.
- Why unresolved: The paper only investigates the mechanism behind bad-performing heads on SST2 using Llama-2-7B, but doesn't explore other tasks or models.
- What evidence would resolve it: A comprehensive analysis of bad-performing components across all 8 tasks and 4 models used in the paper, identifying common patterns or task-specific behaviors.

# Open Question 2
- Question: Can the component reweighting method be generalized to non-classification tasks like generation?
- Basis in paper: [explicit] The paper mentions this as a limitation, stating they only experiment with classification tasks for ease of evaluation and leave generalization to generation tasks for future work.
- Why unresolved: The paper focuses on classification tasks and doesn't explore how their decomposition and reweighting approach would work for generation tasks.
- What evidence would resolve it: Experiments applying the component reweighting method to generation tasks, showing improvements in metrics like perplexity or quality of generated text.

# Open Question 3
- Question: What is the relationship between component behavior during pretraining and their effectiveness in ICL?
- Basis in paper: [explicit] The paper studies training dynamics using Pythia checkpoints and finds that good-performing components emerge early in pretraining, but doesn't fully explain the connection between pretraining behavior and ICL performance.
- Why unresolved: While the paper shows that good components emerge early in pretraining, it doesn't explain why these components are effective for ICL or how their pretraining dynamics relate to their ICL performance.
- What evidence would resolve it: A detailed analysis of component behavior during pretraining, correlating their development with their effectiveness in ICL tasks, potentially revealing key insights about how LLMs acquire ICL capabilities.

## Limitations
- The component decomposition method requires caching activations for all components during the forward pass with demonstrations, which increases memory usage linearly with the number of components
- The method assumes that component behaviors are relatively stable across demonstrations and similar prompt templates, but this may not hold for tasks requiring complex reasoning
- The reweighting approach requires a few labeled examples for training, which may not be available in true few-shot settings

## Confidence
- **High Confidence:** The existence of good-performing, bad-performing, and label-biased components is well-supported by the correlation analyses across demonstrations and templates. The observation that top components emerge early in pretraining is consistently observed across multiple tasks.
- **Medium Confidence:** The effectiveness of component reweighting for improving ICL accuracy is demonstrated, but the improvement varies across models and tasks (6.0% average improvement reported, but with variation). The transferability of top-performing components across different data distributions shows promise but is not universal.
- **Low Confidence:** The claim that component reweighting maintains "fast inference speed" needs more rigorous benchmarking against baselines. The assertion that this approach provides "interpretable insights into ICL" is somewhat subjective and could benefit from more concrete interpretability analyses.

## Next Checks
1. **Memory Overhead Validation:** Measure the actual memory usage increase when caching component activations for different model sizes (7B, 13B, 70B parameters) and determine the practical scaling limits of the approach.

2. **Cross-Task Transferability:** Test whether components identified as good-performing on one task (e.g., sentiment analysis) remain effective when transferred to structurally different tasks (e.g., natural language inference or question answering).

3. **Prompt Sensitivity Analysis:** Systematically vary prompt templates, demonstration selection, and formatting to quantify how sensitive component behaviors are to these factors, and determine if there are specific prompt characteristics that cause component instability.