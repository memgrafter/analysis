---
ver: rpa2
title: Attention with Dependency Parsing Augmentation for Fine-Grained Attribution
arxiv_id: '2412.11404'
source_url: https://arxiv.org/abs/2412.11404
tags:
- attribution
- attention
- target
- union
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses fine-grained attribution in retrieval-augmented
  generation, aiming to provide supporting evidence for every span in generated answers.
  The core method involves two techniques: aggregating token-wise evidence through
  set union operations to preserve granularity, and integrating dependency parsing
  to enrich semantic completeness of target spans.'
---

# Attention with Dependency Parsing Augmentation for Fine-Grained Attribution

## Quick Facts
- arXiv ID: 2412.11404
- Source URL: https://arxiv.org/abs/2412.11404
- Authors: Qiang Ding; Lvzhou Luo; Yixuan Cao; Ping Luo
- Reference count: 40
- One-line primary result: State-of-the-art fine-grained attribution in RAG, achieving 93.3% accuracy on QuoteSum and 84.6% on VERI-GRAN.

## Executive Summary
This paper addresses fine-grained attribution in retrieval-augmented generation, aiming to provide supporting evidence for every span in generated answers. The core method involves two techniques: aggregating token-wise evidence through set union operations to preserve granularity, and integrating dependency parsing to enrich semantic completeness of target spans. Using attention weights as similarity metrics, the approach consistently outperforms prior works, achieving state-of-the-art results on benchmarks like QuoteSum and VERI-GRAN, with accuracy improvements of up to 93.3% and 84.6% respectively. The method also demonstrates strong generalization to sentence-level attribution and practical efficiency.

## Method Summary
The method consists of two main components: attention-based similarity metric and dependency parsing augmentation. First, it extracts attention weights from a specific decoder layer, averages them across all heads, and uses these as similarity scores between response and prompt tokens. For each target token, it identifies the top-k most attended prompt tokens and aggregates these scores via set union operations. The dependency parsing augmentation enriches target spans by incorporating related tokens from atomic facts - for each target token, it finds its closest verb ancestor, collects that verb's successors (excluding irrelevant coordinating constituents), and adds these tokens to the attribution pool with updated evidence scores.

## Key Results
- Achieved 93.3% accuracy on QuoteSum benchmark for span-level attribution
- Achieved 84.6% accuracy on VERI-GRAN benchmark for span-level attribution
- Demonstrated strong generalization to sentence-level attribution and practical efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Token-wise attention aggregation via set union preserves fine-grained attribution signals better than averaging hidden states.
- **Mechanism**: For each target token, the method identifies the top-k most attended prompt tokens, assigns them evidence scores, and aggregates these scores via set union across all tokens in the target span. This avoids averaging that would dilute token-level distinctions.
- **Core assumption**: Attention weights from a single layer (L* = ⌊L/2⌋ + 1) provide sufficiently discriminative evidence for each token without being too early or too late in the model's processing.
- **Evidence anchors**:
  - [abstract]: "We aggregate token-wise evidence through set union operations, preserving the granularity of representations."
  - [section 3.2]: Formal definition of token-wise attribution with union aggregation.
  - [corpus]: No direct corpus support for this specific claim; based on internal experiments.
- **Break condition**: If attention weights are poorly discriminative at the chosen layer, or if top-k selection fails to capture relevant tokens, the union aggregation will produce noisy or incomplete evidence.

### Mechanism 2
- **Claim**: Dependency parsing augmentation enriches semantic completeness of target spans by incorporating related tokens from atomic facts.
- **Mechanism**: For each target token, the method finds its closest verb ancestor, collects that verb's successors (excluding irrelevant coordinating constituents), and adds these tokens to the target span's attribution pool, updating evidence scores accordingly.
- **Core assumption**: The dependency parse tree of the local sentence accurately reflects the semantic structure of atomic facts, so adding verb ancestors and their relevant successors will provide missing context.
- **Evidence anchors**:
  - [abstract]: "integrating dependency parsing to enrich the semantic completeness of target spans."
  - [section 3.3]: Detailed algorithm for atomic fact recognition and attribution augmentation.
  - [corpus]: Weak; only synthetic Chinese dataset experiments (Table 8) show improvement over ATTN UNION alone.
- **Break condition**: If the dependency parser misidentifies verb ancestors or incorrectly includes irrelevant coordinating constituents, the augmentation will add noise rather than useful context.

### Mechanism 3
- **Claim**: Using attention weights as similarity metric outperforms hidden state similarity for attribution accuracy.
- **Mechanism**: The method extracts attention weights from a specific decoder layer, averages them across all heads, and uses the result as similarity scores between response and prompt tokens.
- **Core assumption**: Attention weights capture the same or better alignment information than hidden state cosine similarity, and are computationally cheaper than gradient-based saliency scores.
- **Evidence anchors**:
  - [abstract]: "For practical implementation, our approach employs attention weights as the similarity metric."
  - [section 3.4]: Comparison of attention weights vs hidden state similarity and saliency scores.
  - [corpus]: Table 1 and Figure 3 show ATTN UNION (attention-based) outperforming HSSUNION (hidden state-based) on multiple benchmarks.
- **Break condition**: If the attention weights at the chosen layer do not align well with the semantic content needed for attribution, or if the averaging across heads obscures important distinctions, the method's accuracy will degrade.

## Foundational Learning

- **Concept**: Attention mechanisms in Transformers and how they compute token-token similarity
  - **Why needed here**: The entire attribution method relies on interpreting attention weights as similarity scores between response and prompt tokens.
  - **Quick check question**: How does self-attention compute the weight from token i to token j in a Transformer layer?

- **Concept**: Dependency parsing and dependency tree structures
  - **Why needed here**: The DEP augmentation requires understanding how to extract verb ancestors and their relevant successors from a dependency parse tree.
  - **Quick check question**: What is the difference between a dependency label like "nsubj" and "dobj" in terms of grammatical function?

- **Concept**: Set operations (union, intersection) and their computational complexity
  - **Why needed here**: The core aggregation mechanism uses set union to combine token-wise evidence without losing granularity.
  - **Quick check question**: What is the time complexity of computing the union of k sets each of size m?

## Architecture Onboarding

- **Component map**: Documents, question, and generated response tokens → Attention extraction → Token-wise attribution → (Optional DEP augmentation) → Union aggregation → Evidence output
- **Critical path**: Response → Attention extraction → Token-wise attribution → (Optional DEP augmentation) → Union aggregation → Evidence output
- **Design tradeoffs**: Using attention weights trades off some precision for computational efficiency compared to gradient-based methods; dependency parsing adds complexity but improves semantic coverage; set union preserves granularity but may include more noise than averaging.
- **Failure signatures**: 
  - Low attribution accuracy: attention weights at chosen layer are uninformative
  - Noisy evidence: DEP incorrectly includes irrelevant coordinating constituents
  - High memory usage: large context windows or inefficient attention extraction
- **First 3 experiments**:
  1. Run ATTN UNION on QuoteSum with default hyperparameters (k=2, τ=2) and verify accuracy matches reported ~79.4%
  2. Add DEP augmentation and measure improvement on VERI-GRAN validation set
  3. Compare attention weights vs hidden state similarity on a small subset to confirm attention superiority

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed method perform on abstractive answers that do not contain verbatim spans from the documents?
- Basis in paper: [explicit] The paper notes that the current evaluation focuses on verbatim spans from documents and suggests evaluating with more abstractive answers as future work.
- Why unresolved: Current benchmarks (QuoteSum, VERI-GRAN) are limited to verbatim span attribution, and the paper acknowledges this limitation.
- What evidence would resolve it: Empirical results on benchmarks containing abstractive answers, such as those requiring paraphrasing or summarization of document content.

### Open Question 2
- Question: How sensitive is the method to the choice of dependency parsing parser across different languages?
- Basis in paper: [explicit] The paper states that dependency parsing augmentation may require effort to adapt to other languages since dependency parsing is language-specific.
- Why unresolved: The experiments only demonstrate effectiveness with LAL-Parser for English, and the adaptation process for other languages is not explored.
- What evidence would resolve it: Comparative results using different dependency parsers (e.g., UDPipe, spaCy) across multiple languages, along with qualitative analysis of parsing accuracy.

### Open Question 3
- Question: Can the attention-based similarity metric be effectively replaced by other similarity metrics without degrading performance?
- Basis in paper: [explicit] The paper compares attention weights to hidden state cosine similarity and shows attention weights perform better, but suggests exploring other metrics.
- Why unresolved: Only two similarity metrics were empirically compared, and the choice of attention weights was justified by empirical performance rather than theoretical analysis.
- What evidence would resolve it: Systematic comparison of additional similarity metrics (e.g., MLP-based, contrastive learning-based) across multiple datasets and models, with ablation studies on each component.

### Open Question 4
- Question: What is the impact of different dependency parsing augmentation rules on the final attribution quality?
- Basis in paper: [explicit] The paper describes rule-based dependency parsing augmentation and notes that it could be improved by machine learning in the future.
- Why unresolved: The current method uses fixed rules for recognizing atomic fact elements and excluding irrelevant coordinating constituents, without exploring alternative approaches.
- What evidence would resolve it: Comparative results using different augmentation strategies (e.g., learned rules vs. fixed rules, different coordination handling approaches) on the same datasets, with error analysis of attribution mistakes.

### Open Question 5
- Question: How does the method scale with longer documents and larger context windows?
- Basis in paper: [inferred] The paper mentions memory efficiency improvements but does not evaluate performance on datasets with significantly longer documents or larger context windows.
- Why unresolved: The current evaluation focuses on datasets with moderate document lengths, and the memory optimizations are not tested on extreme cases.
- What evidence would resolve it: Performance and latency measurements on datasets with varying document lengths (e.g., 10x longer documents), along with analysis of how the attention weight extraction scales with context size.

## Limitations

- The dependency parsing augmentation rules for handling coordinating structures may not generalize well across languages and domains, with effectiveness primarily validated on synthetic Chinese data.
- Computational overhead of extracting attention weights from large language models can be substantial, with memory requirements not fully analyzed or compared to gradient-based alternatives.
- The method's reliance on a single attention layer assumes optimal semantic alignment, but different layers may be more appropriate for different question types without exploration of layer sensitivity.

## Confidence

- **High Confidence**: The core mechanism of using attention weights as similarity metrics and aggregating token-wise evidence through set unions is well-supported by experimental results across multiple benchmarks.
- **Medium Confidence**: The dependency parsing augmentation's effectiveness is supported primarily by experiments on synthetic Chinese data, with limited validation on real-world datasets and potential language/domain generalization issues.
- **Low Confidence**: The computational efficiency claims relative to gradient-based methods are not fully substantiated with concrete comparisons of memory usage or inference time.

## Next Checks

1. **Layer Sensitivity Analysis**: Conduct systematic experiments varying the attention layer index (⌊L/2⌋+1) across different model sizes (7B, 13B, 70B parameters) and question types to determine optimal layer selection and assess the robustness of the chosen default layer.

2. **Cross-Lingual and Cross-Domain Generalization**: Evaluate the dependency parsing augmentation on English and multilingual datasets beyond the synthetic Chinese data, including real-world RAG scenarios with diverse document types (legal, medical, technical) to test the method's robustness to syntactic and semantic variations.

3. **Computational Overhead Measurement**: Implement detailed profiling of memory usage and inference time for the full attribution pipeline, comparing attention-based similarity extraction against gradient-based methods on identical hardware, and analyze the trade-offs between accuracy gains and computational costs across different context window sizes.