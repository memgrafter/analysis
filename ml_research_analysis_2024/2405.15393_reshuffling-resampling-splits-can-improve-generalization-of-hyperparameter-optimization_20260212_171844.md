---
ver: rpa2
title: Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter
  Optimization
arxiv_id: '2405.15393'
source_url: https://arxiv.org/abs/2405.15393
tags:
- performance
- reshuffling
- fold
- test
- holdout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the common practice of using fixed train-validation
  splits during hyperparameter optimization (HPO). The authors show both theoretically
  and empirically that reshuffling these splits for each hyperparameter configuration
  can lead to improved generalization performance.
---

# Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization

## Quick Facts
- arXiv ID: 2405.15393
- Source URL: https://arxiv.org/abs/2405.15393
- Authors: Thomas Nagler; Lennart Schneider; Bernd Bischl; Matthias Feurer
- Reference count: 40
- Primary result: Reshuffling train-validation splits for each hyperparameter configuration during HPO can improve generalization performance

## Executive Summary
This paper challenges the standard practice of using fixed train-validation splits during hyperparameter optimization. The authors demonstrate both theoretically and empirically that reshuffling these splits for each hyperparameter configuration can lead to improved generalization performance. Their theoretical analysis reveals that reshuffling reduces correlation between validation losses across different configurations, which helps avoid overfitting to specific data splits. Empirical results on various datasets and learning algorithms show that reshuffled holdout validation can achieve comparable or better generalization performance than standard 5-fold cross-validation while being computationally cheaper.

## Method Summary
The authors implement random search with 500 evaluations and Bayesian optimization (HEBO, SMAC3) with 250 evaluations across multiple resampling strategies including holdout, 5-fold CV, 5-fold holdout, and 5x 5-fold CV, with and without reshuffling. They use tabular datasets from OpenML for binary classification with varying sizes (500-5000 samples for training/validation, 5000 for testing). Models tested include XGBoost, CatBoost, Elastic Net, and Funnel MLP. The reshuffling mechanism involves independently drawing train-validation splits for each hyperparameter configuration rather than using a single fixed split.

## Key Results
- Reshuffling reduces correlation between validation losses across different configurations, leading to improved generalization
- Reshuffled holdout validation can achieve comparable or better generalization performance than standard 5-fold cross-validation
- Reshuffling provides computational efficiency by reducing the number of model fits needed compared to cross-validation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reshuffling reduces correlation between validation losses across different configurations
- Mechanism: By independently drawing train-validation splits for each hyperparameter configuration, reshuffling decorrelates the validation loss surface across configurations, making it less likely to over-optimize for a specific data split
- Core assumption: The validation loss estimates for different configurations are sufficiently independent when reshuffling is applied
- Evidence anchors: [abstract], [section 2.2], theoretical analysis showing reshuffling affects asymptotic behavior of validation loss surface

### Mechanism 2
- Claim: Reshuffling can help avoid overfitting to specific data splits during HPO
- Mechanism: When using fixed splits, HPO can find configurations that are specifically tailored to those splits rather than configurations that generalize well. Reshuffling mitigates this by evaluating each configuration on different splits
- Core assumption: Fixed train-validation splits can lead to overfitting during HPO
- Evidence anchors: [abstract], [section 1], discussion of "overtuning" or "oversearching" problem

### Mechanism 3
- Claim: Reshuffling can make holdout validation competitive with cross-validation while being computationally cheaper
- Mechanism: By reducing the correlation between validation losses while maintaining the computational efficiency of holdout (single split), reshuffling creates a favorable trade-off between variance and correlation in the validation estimate
- Core assumption: Holdout validation is computationally cheaper than cross-validation but has higher variance
- Evidence anchors: [abstract], [section 4.2], experimental results showing reshuffled holdout performs similarly to 5-fold CV with fewer model fits

## Foundational Learning

- Concept: Resampling methods (holdout, cross-validation)
  - Why needed here: Understanding how different resampling strategies estimate generalization error is fundamental to grasping why reshuffling affects HPO outcomes
  - Quick check question: What's the key difference between holdout and k-fold cross-validation in terms of data usage and variance?

- Concept: Hyperparameter optimization as black-box optimization
  - Why needed here: Reshuffling is analyzed in the context of noisy, black-box optimization where the objective function is estimated through resampling
  - Quick check question: Why is HPO typically considered a black-box optimization problem rather than a gradient-based one?

- Concept: Signal-to-noise ratio in optimization
  - Why needed here: The theoretical analysis connects reshuffling benefits to the ratio between the curvature of the loss surface and the noise level
  - Quick check question: How does the signal-to-noise ratio affect the difficulty of finding optimal hyperparameters?

## Architecture Onboarding

- Component map: Data splitting module -> Configuration evaluation engine -> Optimization algorithm -> Performance tracking system
- Critical path: Split generation → Model training → Validation → Performance recording → Configuration selection
- Design tradeoffs:
  - Fixed vs. reshuffled splits: computational efficiency vs. decorrelation benefits
  - Number of folds: variance reduction vs. computational cost
  - Optimization algorithm choice: convergence speed vs. generalization performance
- Failure signatures:
  - High variance in validation performance across configurations
  - Large gap between validation and test performance
  - Unstable selection of optimal configurations across different runs
- First 3 experiments:
  1. Compare fixed vs. reshuffled holdout on a small tabular dataset with random search
  2. Test reshuffling effect on 5-fold cross-validation to verify theoretical prediction of minimal impact
  3. Evaluate reshuffled holdout vs. standard 5-fold CV on a larger dataset with different learning algorithms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reshuffling effect scale with dataset size? Specifically, is there a threshold dataset size below which reshuffling consistently improves generalization performance, and above which it becomes neutral or harmful?
- Basis in paper: [explicit] The authors mention in the discussion section that "from a practical perspective, this also ensures computational feasibility given the large number of HPO runs in our experiments" and suspect "the effect of the resampling variant used and whether the resampling is reshuffled to be larger for smaller datasets, where the variance of the validation loss estimator is naturally higher."
- Why unresolved: The experimental results only show data for n ∈ {500, 1000, 5000}, leaving a gap in understanding how reshuffling performs on very small or very large datasets.
- What evidence would resolve it: A systematic study varying dataset sizes from very small (e.g., n < 100) to very large (e.g., n > 100,000) while keeping other factors constant.

### Open Question 2
- Question: Does the benefit of reshuffling depend on the specific HPO algorithm used, or is it a general property of the validation process that would benefit any optimization algorithm?
- Basis in paper: [explicit] The authors note that "the choice of optimizer might have less impact on final generalization performance compared to other choices such as whether the resampling is reshuffled during HPO or not" based on their BO experiments.
- Why unresolved: While the authors show that both random search and BO benefit from reshuffling, they don't test a comprehensive range of optimization algorithms or investigate whether the magnitude of benefit varies by algorithm type.
- What evidence would resolve it: Experiments comparing reshuffling effects across multiple optimization algorithms (e.g., evolutionary strategies, gradient-based methods adapted for HPO, etc.) on identical problems.

### Open Question 3
- Question: How does the reshuffling effect interact with different types of hyperparameter spaces and model architectures?
- Basis in paper: [inferred] The experiments focus on tree-based models (XGBoost, CatBoost) and a linear model (Elastic Net), but the theoretical analysis applies to general loss surfaces. The authors mention "tabular data and binary classification" as limitations.
- Why unresolved: The current experiments don't explore how reshuffling affects hyperparameter optimization for deep neural networks, models with conditional hyperparameters, or different types of hyperparameter spaces (continuous vs discrete, low vs high dimensional).
- What evidence would resolve it: Systematic experiments varying model architectures (CNNs, RNNs, transformers) and hyperparameter space characteristics across multiple problem domains.

## Limitations
- The theoretical analysis assumes i.i.d. noise in the validation loss, which may not hold in practice
- Empirical results show reshuffling benefits primarily with random search, with less consistent effects for Bayesian optimization methods
- The study focuses on tabular datasets and may not generalize to high-dimensional or image data
- Reshuffling may hurt performance when sample sizes are very small

## Confidence
- Theoretical mechanism (correlation reduction): High confidence
- Empirical performance gains: Medium confidence (depends on dataset and optimization method)
- Computational efficiency claims: High confidence
- Generalization to non-tabular data: Low confidence

## Next Checks
1. Test reshuffling on non-tabular datasets (images, text) to assess generalizability beyond the current experimental scope
2. Evaluate reshuffling with different optimization budgets (fewer/more than 500 configurations) to understand scaling effects
3. Implement reshuffling with adaptive resampling methods to dynamically adjust the number of splits based on configuration uncertainty