---
ver: rpa2
title: Local transfer learning Gaussian process modeling, with applications to surrogate
  modeling of expensive computer simulators
arxiv_id: '2410.12690'
source_url: https://arxiv.org/abs/2410.12690
tags:
- transfer
- data
- xnew
- target
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a local transfer learning Gaussian process
  (LOL-GP) model to improve surrogate modeling of expensive computer simulators by
  leveraging data from related systems. The key innovation is a latent regularization
  model that identifies regions where transfer is beneficial and regions where it
  should be avoided, addressing the risk of negative transfer.
---

# Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators

## Quick Facts
- arXiv ID: 2410.12690
- Source URL: https://arxiv.org/abs/2410.12690
- Authors: Xinming Wang; Simon Mak; John Miller; Jianguo Wu
- Reference count: 40
- Key outcome: LOL-GP achieves RMSE of 0.100 and CRPS of 0.053 for multi-source transfer, and RMSE of 0.078 and CRPS of 0.049 for multi-fidelity transfer in jet turbine application

## Executive Summary
This paper introduces a Local transfer learning Gaussian process (LOL-GP) model to improve surrogate modeling of expensive computer simulators by leveraging data from related systems. The key innovation is a latent regularization model that identifies regions where transfer is beneficial and regions where it should be avoided, addressing the risk of negative transfer. The model uses a ReLU activation function on latent transfer functions to enforce this local transfer behavior. For the multi-source setting, the LOL-GP models the target as a weighted sum of source simulators with input-dependent transfer weights. For multi-fidelity learning, it extends the Bayesian Kennedy-O'Hagan model to account for local transfer between fidelity levels.

## Method Summary
The LOL-GP employs a latent regularization model with ReLU activation to enable local transfer learning. For multi-source transfer, it models the target system as a weighted sum of source simulators, where weights are determined by latent transfer functions. These latent functions are modeled as GPs and passed through ReLU activation to produce transfer weights. For multi-fidelity learning, the model extends the Kennedy-O'Hagan framework by incorporating local transfer between fidelity levels. The model uses Gibbs sampling for posterior inference and approximate MAP optimization for kernel hyperparameters. The key novelty is the ability to identify and restrict transfer in regions where source and target systems behave differently.

## Key Results
- LOL-GP outperforms existing methods in both multi-source and multi-fidelity settings, particularly in tempering negative transfer risks
- In 1D Forrester function experiments, LOL-GP achieves RMSE of 0.123 and CRPS of 0.139 for multi-source transfer, outperforming KO and BKO models
- In jet turbine application, LOL-GP achieves RMSE of 0.100 and CRPS of 0.053 for multi-source transfer, and RMSE of 0.078 and CRPS of 0.049 for multi-fidelity transfer

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The LOL-GP mitigates negative transfer by using a ReLU activation on latent transfer functions to zero out transfer in regions where source and target systems behave differently.
- **Mechanism:** The latent function ω_l(x) is modeled as a GP, and transfer is permitted only when ω_l(x) > 0. In regions where ω_l(x) ≤ 0, the transfer weight ρ_l(x) = ReLU{ω_l(x)} = 0, effectively disabling transfer and avoiding the propagation of misleading information.
- **Core assumption:** The true transfer relationship between source and target systems can be well-approximated by a multiplicative form ReLU{ω_l(x)}·f_l(x), and the regions of beneficial transfer are identifiable from data via the posterior of ω_l(x).
- **Evidence anchors:**
  - [abstract] "The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided."
  - [section] "When the latent function ω_l(x) exceeds zero, transfer is permitted by setting the transfer weight as ρ_l(x) = ω_l(x) > 0; when ω_l(x) falls below zero, transfer is restricted by zeroing out its transfer weight."
  - [corpus] Weak: No direct neighbor corpus evidence of ReLU-based regularization for transfer learning in GP surrogate models.
- **Break condition:** If the underlying transfer mechanism cannot be captured by the multiplicative form, or if the local transfer behavior is not smooth enough for the GP prior on ω_l(x) to identify beneficial regions, the regularization may fail.

### Mechanism 2
- **Claim:** The Gibbs sampling algorithm efficiently samples from the posterior predictive distribution by leveraging closed-form full conditional distributions.
- **Mechanism:** The model is structured so that full conditionals for both latent weights ω_l(xi) (via two-mixture truncated normals) and latent function values f_l(xi) (via standard normals) are available in closed form. This allows efficient cyclic updates in the MCMC chain without requiring expensive numerical integration.
- **Core assumption:** The latent parameters Θ = {ω_l(XT), f_l(XT)} capture all necessary uncertainty for prediction, and the conditional independence structure of the GP allows factorization into tractable full conditionals.
- **Evidence anchors:**
  - [section] "We show next that, with a careful selection of latent parameters Θ, such full conditional distributions can be derived in closed form, enabling efficient posterior sampling of f_T(x_new)."
  - [section] "Such a sampler proceeds as follows. First, the latent parameters in Θ are initialized... Next, one sequentially samples (and subsequently updates) each parameter θ ∈ Θ from the full conditional distributions (3.2) and (3.5)."
  - [corpus] Weak: No direct neighbor corpus evidence of Gibbs sampling for local transfer GP models.
- **Break condition:** If the latent parameter set is misspecified or if the conditional independence structure breaks down (e.g., due to strong coupling between sources), the closed-form conditionals may no longer be accurate.

### Mechanism 3
- **Claim:** The approximate MAP optimization of kernel hyperparameters using plug-in estimates of latent parameters enables efficient model training without sacrificing predictive accuracy.
- **Mechanism:** Instead of full Bayesian treatment of hyperparameters (which would require long MCMC chains), the method uses the posterior mean of latent parameters ˆΘ from Gibbs sampling as plug-in estimates. The likelihood and marginal likelihood then become analytically tractable Gaussian forms, enabling fast gradient-based optimization (e.g., L-BFGS with automatic differentiation).
- **Core assumption:** The plug-in estimator ˆΘ is sufficiently close to the true latent parameter values that the resulting hyperparameter estimates are nearly optimal for prediction, and the Gaussian likelihood forms are valid given ˆΘ.
- **Evidence anchors:**
  - [section] "Let Ξ = {Ξ_δ, Ξ_f, Ξ_ω} denote the set of kernel hyperparameters to optimize... With this, we employ the optimization formulation: ˆΞ := arg max Ξ log[Ξ|data, ˆΘ] = arg max Ξ log { [data|ˆΘ, Ξ] [ˆΘ|Ξ] [Ξ] }."
  - [section] "The key benefit of using this estimator is that, under the LOL-GP, the terms [data |Ξ, ˆΘ] and [ˆΘ|Ξ] in (4.1) both admit analytic closed-form expressions."
  - [corpus] Weak: No direct neighbor corpus evidence of approximate MAP with plug-in latent estimates for transfer GP models.
- **Break condition:** If the plug-in estimator ˆΘ is far from the true latent values (e.g., due to poor MCMC mixing or identifiability issues), the resulting hyperparameters may lead to suboptimal predictions.

## Foundational Learning

- **Concept:** Gaussian Process (GP) regression and kernel hyperparameters
  - **Why needed here:** The LOL-GP is built on GP priors for source functions, discrepancy terms, and latent transfer functions. Understanding how kernel hyperparameters control smoothness and length-scales is essential for interpreting model behavior and for implementing the MAP optimization.
  - **Quick check question:** In a GP with squared-exponential kernel, what effect does increasing the length-scale hyperparameter have on the smoothness of sample paths?

- **Concept:** Transfer learning and negative transfer
  - **Why needed here:** The motivation for LOL-GP is to address negative transfer, where transferring information from source to target systems can worsen performance. Understanding the conditions under which transfer is beneficial versus detrimental is crucial for interpreting the local transfer mechanism.
  - **Quick check question:** In the motivating example, why does the Kennedy-O'Hayan (KO) model with constant transfer worsen predictions in the region x < 0.5?

- **Concept:** Gibbs sampling and full conditional distributions
  - **Why needed here:** The posterior inference in LOL-GP relies on Gibbs sampling, which requires deriving and sampling from full conditional distributions. Understanding the mechanics of Gibbs sampling and how closed-form conditionals enable efficient inference is essential for implementation.
  - **Quick check question:** In the LOL-GP Gibbs sampler, what is the form of the full conditional distribution for the latent weights ω_l(xi)?

## Architecture Onboarding

- **Component map:** Data layer -> Model layer -> Inference layer -> Prediction layer
- **Critical path:**
  1. **Preprocessing:** Normalize inputs to [0,1]^d; ensure design points are not shared between source and target (unless using extended algorithm)
  2. **Initialization:** Initialize latent parameters Θ[0] (e.g., zeros for ω_l, source function values at design points)
  3. **Gibbs sampling:** Run B iterations, updating each parameter in Θ via full conditionals (3.2) and (3.5)
  4. **Hyperparameter optimization:** Compute ˆΘ as posterior mean of MCMC samples; optimize Ξ via (4.1) using L-BFGS with automatic differentiation
  5. **Prediction:** For each MCMC sample Θ[b], sample f_T(x_new) via (3.7)-(3.8); aggregate samples for posterior predictive distribution

- **Design tradeoffs:**
  - **Transfer model flexibility vs. risk of overfitting:** The ReLU activation regularizes transfer to avoid overly flexible models that may overfit with limited target data (vs. BKO model without ReLU)
  - **Computational cost vs. accuracy:** Gibbs sampling and MAP optimization are more expensive than MLE for standard GP, but necessary for modeling local transfer; nested designs can reduce cost in multi-fidelity setting
  - **Identifiability vs. prediction:** Latent functions ω_l and transfer functions ρ_l may be non-identifiable, but this does not affect prediction; care needed if interpreting these functions

- **Failure signatures:**
  - **Poor MCMC mixing:** Effective sample size (ESS) for latent parameters is low; posterior predictive samples are highly correlated
  - **Negative transfer persists:** LOL-GP RMSE/CRPS worse than standard GP without transfer; indicates local transfer not well-captured or hyperparameters poorly optimized
  - **Identifiability issues:** Posterior distributions of ω_l and ρ_l are highly uncertain or multimodal; interpretation of transfer behavior unreliable

- **First 3 experiments:**
  1. **1-d Forrester multi-source experiment (Section 5.1):** Replicate the motivating example with n1=n2=32, nT=7 to verify LOL-GP outperforms KO and BKO in presence of local transfer
  2. **1-d Forrester multi-fidelity experiment (Section 5.2):** Test LOL-GP on low-to-high fidelity transfer with nested design (n1=21, n2=7) to verify local transfer regularization works in fidelity setting
  3. **Jet turbine multi-source experiment (Section 6):** Apply LOL-GP to real-world multi-source transfer with ceramic vs. nickel-based blade materials (n1=32, nT=8) to verify practical effectiveness

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The ReLU-based regularization mechanism relies on the assumption that beneficial transfer regions can be identified via the posterior of ω_l(x); if this latent structure is misspecified, the model may still exhibit negative transfer
- The closed-form Gibbs sampling approach assumes conditional independence that may not hold with strongly coupled sources
- The approximate MAP optimization trades full Bayesian treatment of hyperparameters for speed, potentially compromising predictive accuracy if the plug-in estimator ˆΘ is far from true values

## Confidence
- **High**: The model architecture and mathematical derivations are sound; the Gibbs sampling algorithm correctly implements the intended inference procedure
- **Medium**: The empirical performance gains are well-demonstrated, but the extent of improvement depends on problem structure and may be less pronounced in high-dimensional settings
- **Low**: The interpretability of latent transfer functions ω_l and ρ_l is limited by potential non-identifiability; care is needed when using these to explain transfer behavior

## Next Checks
1. Test LOL-GP on higher-dimensional problems (d > 10) to assess scalability and whether local transfer regularization remains effective
2. Compare MCMC mixing diagnostics (ESS, autocorrelation) for latent parameters across different problem sizes and transfer difficulty levels
3. Conduct ablation studies removing the ReLU regularization to quantify its contribution to mitigating negative transfer