---
ver: rpa2
title: A new Input Convex Neural Network with application to options pricing
arxiv_id: '2411.12854'
source_url: https://arxiv.org/abs/2411.12854
tags:
- convex
- network
- function
- where
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new type of neural network that is inherently
  convex with respect to its inputs, making it well-suited for pricing options with
  convex payoffs. The key idea is to build on the fact that any convex function can
  be represented as the supremum of affine functions it dominates.
---

# A new Input Convex Neural Network with application to options pricing

## Quick Facts
- arXiv ID: 2411.12854
- Source URL: https://arxiv.org/abs/2411.12854
- Reference count: 12
- Introduces a Convex Network (CN) architecture that is inherently convex with respect to inputs, suitable for pricing options with convex payoffs

## Executive Summary
This paper presents a new neural network architecture called the Convex Network (CN) that is inherently convex with respect to its inputs, making it particularly well-suited for pricing options with convex payoffs. The key innovation builds on the representation of convex functions as the supremum of affine functions they dominate, using multiple hidden layers without activation functions followed by a maximum function output. The authors introduce a "scrambling" phase to improve training by allowing multiple hyperplanes to be activated simultaneously. Theoretical convergence bounds are established based on optimal quantization results, and numerical experiments demonstrate superior accuracy compared to state-of-the-art methods, particularly in high dimensions.

## Method Summary
The Convex Network (CN) architecture consists of multiple hidden layers without activation functions, followed by a maximum function as the output activation. The network exploits the fact that any convex function can be represented as the supremum of affine functions it dominates. A key innovation is the "scrambling" phase during training, which allows multiple hyperplanes to be activated simultaneously, improving the network's ability to approximate complex convex functions. The authors establish theoretical convergence bounds for the approximation of convex functions using the CN, both on compact sets and in L^p spaces, based on optimal quantization results from probability theory. The method is applied to price three types of options with convex payoffs: Basket options, Bermudan options, and Swing options.

## Key Results
- The CN achieves relative errors of less than 1% compared to benchmark Monte Carlo methods for Basket options, even in 20-dimensional problems
- The network outperforms other state-of-the-art methods in terms of accuracy, particularly in high dimensions
- Greeks of the options can be computed "for free" by examining the weights of the activated hyperplanes

## Why This Works (Mechanism)
The paper's approach works by leveraging the fundamental property that any convex function can be represented as the supremum of affine functions it dominates. The CN architecture builds on this by using multiple hidden layers without activation functions, which allows the network to learn multiple affine transformations, and then applying a maximum function as the output activation to take the supremum. This design ensures that the network's output is inherently convex with respect to its inputs, which is crucial for pricing options with convex payoffs. The "scrambling" phase during training allows the network to activate multiple hyperplanes simultaneously, improving its ability to approximate complex convex functions and leading to better convergence properties.

## Foundational Learning
- **Convex function representation**: Understanding that convex functions can be represented as the supremum of affine functions is crucial because it provides the theoretical foundation for the CN architecture. Quick check: Verify that the network's output is indeed the supremum of the affine transformations learned by the hidden layers.
- **Optimal quantization**: Knowledge of optimal quantization results is needed to establish the theoretical convergence bounds for the CN. Quick check: Ensure that the quantization error bounds used in the proofs are applicable to the specific structure of the CN.
- **Option pricing with convex payoffs**: Familiarity with the pricing of options with convex payoffs (such as Basket, Bermudan, and Swing options) is important for understanding the practical applications of the CN. Quick check: Confirm that the option payoffs being priced are indeed convex functions of the underlying asset prices.

## Architecture Onboarding

**Component Map**: Input -> Hidden Layers (no activation) -> Scrambling Phase -> Maximum Function Output

**Critical Path**: The critical path in the CN architecture is from the input through the hidden layers (where affine transformations are learned) to the maximum function output (which takes the supremum). The scrambling phase is an additional step during training that allows multiple hyperplanes to be activated simultaneously, improving the network's ability to approximate complex convex functions.

**Design Tradeoffs**: The main tradeoff in the CN design is between the depth of the network (number of hidden layers) and the ability to accurately approximate complex convex functions. Deeper networks can learn more complex affine transformations but may require more training data and computational resources. The scrambling phase introduces additional complexity but improves the network's convergence properties and ability to activate multiple relevant hyperplanes simultaneously.

**Failure Signatures**: Potential failure modes include:
- Underfitting: If the network is not deep enough or the scrambling phase is not effective, the CN may fail to accurately approximate complex convex functions, leading to high pricing errors.
- Overfitting: If the network is too deep or the training data is limited, the CN may overfit to the training data, resulting in poor generalization to unseen option prices.
- Numerical instability: The scrambling phase, while improving convergence, may introduce numerical instability in some cases, particularly if the learning rate is not properly tuned.

**3 First Experiments**:
1. Train the CN on a simple convex function (e.g., a quadratic function) and verify that the output is indeed convex with respect to the inputs.
2. Apply the CN to price a Basket option with a small number of underlying assets (e.g., 2-3) and compare the results to a benchmark method (e.g., Monte Carlo simulation).
3. Test the effect of the scrambling phase by training two versions of the CN (with and without scrambling) on the same option pricing problem and compare their convergence rates and final pricing errors.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical convergence bounds rely on optimal quantization results that may not fully capture the behavior of neural networks in practice, particularly for high-dimensional problems where the curse of dimensionality could still impact performance.
- The scrambling phase, while shown to improve training, lacks rigorous analysis of its impact on convergence rates and may introduce instability in some cases.
- The empirical results are limited to three specific option types and may not generalize to all convex payoff structures.

## Confidence
High for the theoretical framework and convergence proofs; Medium for empirical results and computational efficiency claims; Low for generalization to all convex payoff structures and very high-dimensional problems.

## Next Checks
1. Test the model on additional convex payoff structures beyond basket, Bermudan, and swing options to verify generalization capabilities.
2. Conduct systematic scalability tests with dimensions exceeding 20 to evaluate computational efficiency and accuracy degradation.
3. Implement uncertainty quantification methods to assess prediction reliability and identify potential overfitting issues.