---
ver: rpa2
title: 'HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants'
arxiv_id: '2405.09186'
source_url: https://arxiv.org/abs/2405.09186
tags:
- arxiv
- evaluation
- human
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HumanRankEval (HRE), an automatic evaluation
  task for assessing large language models (LMs) as conversational assistants. HRE
  leverages human-authored questions and ranked answers from StackExchange and StackOverflow
  to approximate human preferences.
---

# HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants

## Quick Facts
- arXiv ID: 2405.09186
- Source URL: https://arxiv.org/abs/2405.09186
- Authors: Milan Gritta; Gerasimos Lampouras; Ignacio Iacobacci
- Reference count: 14
- Primary result: HRE scores correlate with human judgements (0.96 correlation with Chatbot Arena ratings)

## Executive Summary
HumanRankEval (HRE) introduces an automatic evaluation task for assessing large language models as conversational assistants by leveraging human-authored questions and ranked answers from StackExchange and StackOverflow. The method computes log-likelihoods of human-authored answers under the LM's distribution and correlates them with human rankings using Pearson correlation. HRE demonstrates strong correlation with human judgements while providing unique insights into model differences, particularly responsiveness to instruction-tuning compared to knowledge-based automatic evaluation tasks.

## Method Summary
HRE is an automatic evaluation task that assesses large language models as conversational assistants by comparing their answer preferences with human preferences. The method uses human-authored questions and ranked answers from StackExchange and StackOverflow, computing log-likelihoods of each answer under the LM's distribution. These log-likelihoods are then ranked and correlated with corresponding human rankings using Pearson correlation. The evaluation dataset consists of 7K questions across 14 topics with answers rated by 100+ domain experts, providing a robust measure of how well LMs align with human preferences in conversational settings.

## Key Results
- HRE scores correlate strongly with human judgements (0.96 correlation with Chatbot Arena ratings)
- Effectively differentiates pretrained and instruction-tuned LMs
- Demonstrates unique responsiveness to instruction-tuning compared to knowledge-based evaluation tasks
- Dataset includes 7K questions across 14 topics with 28K answers rated by 100+ domain experts

## Why This Works (Mechanism)
HRE correlates human preference rankings with LM log-likelihood rankings by computing the Pearson correlation between the log-likelihood scores that an LM assigns to answers and the human-ranked preferences for those answers. Higher correlation indicates better alignment between the LM's "preference" and human preferences. The core assumption is that the log-likelihood of an answer under an LM's distribution meaningfully proxies for how much the LM "prefers" that answer, which correlates with human preferences.

## Foundational Learning
- Log-likelihood computation: Why needed - Measures how probable an answer is under the LM's probability distribution. Quick check - Verify log-likelihood calculations match expected values on known test cases.
- Pearson correlation: Why needed - Quantifies the linear relationship between LM rankings and human rankings. Quick check - Confirm correlation values are within expected range (-1 to 1).
- Answer ranking methodology: Why needed - Establishes a consistent way to compare LM preferences with human preferences. Quick check - Ensure ranking algorithms handle ties and duplicates appropriately.

## Architecture Onboarding

Component map:
Dataset (StackExchange/StackOverflow) -> Preprocessing pipeline -> Log-likelihood computation -> Answer ranking -> Pearson correlation -> HRE score

Critical path: The critical path involves processing the dataset through preprocessing, computing log-likelihoods for all answers, ranking these scores, and calculating the Pearson correlation with human rankings. Any bottleneck in log-likelihood computation directly impacts evaluation time.

Design tradeoffs: The choice between Pearson and Spearman correlation represents a key tradeoff between assuming linear relationships (Pearson) versus monotonic relationships (Spearman). HRE opts for Pearson correlation based on empirical evidence showing clearer model separation.

Failure signatures: Low correlation scores may indicate data contamination from training sources, improper preprocessing, or incorrect log-likelihood implementation. High variance across runs suggests instability in the ranking or correlation calculation.

First experiments:
1. Compute HRE scores on a small subset of questions to validate the complete pipeline
2. Compare HRE scores between a baseline LM and an instruction-tuned variant
3. Analyze correlation between HRE scores and human ratings on held-out test questions

## Open Questions the Paper Calls Out
1. How does data contamination from StackExchange/StackOverflow affect HRE scores, and what methods could mitigate this issue? The paper notes that training LMs on content from these sources is common and could inflate HRE scores, but doesn't provide a definitive method to quantify or eliminate this risk.

2. Would using Spearman rank correlation instead of Pearson correlation improve the accuracy of HRE in aligning with human preferences? While the paper provides empirical evidence favoring Pearson, it doesn't explore the theoretical underpinnings or conduct a comprehensive comparison across different datasets and model types.

3. How can HRE be extended to evaluate LMs in specialized domains or non-English languages? The paper acknowledges coverage limitations but doesn't provide detailed methodology or guidelines for extending HRE to new domains or languages.

## Limitations
- Reliance on StackExchange/StackOverflow data introduces domain-specific biases toward technical Q&A
- Assumption that log-likelihood rankings directly correlate with human preferences may not hold across all query types
- Dataset construction methodology may introduce selection bias limiting generalizability

## Confidence
High Confidence: The correlation results between HRE scores and human judgements (0.96 with Chatbot Arena ratings) are well-supported by empirical evidence and robust statistical analysis.

Medium Confidence: The claim that HRE effectively differentiates pretrained and instruction-tuned models is supported but may be influenced by specific dataset characteristics and evaluation protocol.

Low Confidence: The assertion that HRE is uniquely responsive to instruction-tuning compared to other evaluation methods requires further validation across different model families and instruction-tuning approaches.

## Next Checks
1. Validate HRE's generalizability by testing on conversational assistant benchmarks outside the StackExchange/StackOverflow domain, such as general knowledge Q&A or customer service dialogues.

2. Conduct ablation studies to quantify the impact of different preprocessing steps on dataset quality and HRE score reliability.

3. Perform cross-lingual evaluation to assess HRE's effectiveness for non-English language models and culturally diverse question-answering scenarios.