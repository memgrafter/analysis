---
ver: rpa2
title: 'MemoryFormer: Minimize Transformer Computation by Removing Fully-Connected
  Layers'
arxiv_id: '2411.12992'
source_url: https://arxiv.org/abs/2411.12992
tags:
- memory
- layer
- hash
- transformer
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MemoryFormer, a novel transformer architecture
  that significantly reduces computational complexity by replacing fully-connected
  layers with Memory Layers. The key innovation is using locality-sensitive hashing
  to retrieve discrete vectors from in-memory lookup tables, which approximates the
  matrix multiplication of fully-connected layers at a fraction of the computational
  cost.
---

# MemoryFormer: Minimize Transformer Computation by Removing Fully-Connected Layers

## Quick Facts
- arXiv ID: 2411.12992
- Source URL: https://arxiv.org/abs/2411.12992
- Authors: Ning Ding; Yehui Tang; Haochen Qin; Zhenli Zhou; Chao Xu; Lin Li; Kai Han; Heng Liao; Yunhe Wang
- Reference count: 31
- Key outcome: Achieves 81% FLOP reduction by replacing fully-connected layers with Memory Layers using locality-sensitive hashing

## Executive Summary
MemoryFormer introduces a novel transformer architecture that significantly reduces computational complexity by replacing fully-connected layers with Memory Layers based on locality-sensitive hashing. The key innovation is using in-memory lookup tables to approximate matrix multiplication at a fraction of the computational cost. The model demonstrates effectiveness across different scales (tiny, small, base) and outperforms existing efficient transformer methods in both speed and accuracy while maintaining competitive performance on multiple NLP benchmarks.

## Method Summary
MemoryFormer replaces all fully-connected layers in transformer blocks with Memory Layers that use locality-sensitive hashing to retrieve discrete vectors from in-memory lookup tables. The sign of input embeddings is used to compute hash indices, which retrieve correlated vectors from the hash table. These retrieved vectors are aggregated to approximate the result of matrix multiplication. The hash table vectors are made learnable through backpropagation using a probability weighting function, allowing the model to adapt the hash table contents during training. This approach trades memory resources for computational efficiency, achieving O(s(τ+h)K) complexity instead of O(sdh) for fully-connected layers.

## Key Results
- Achieves approximately 81% reduction in FLOPs compared to standard transformers
- Maintains competitive performance across multiple NLP benchmarks (PIQA, WinoGrande, WSC, ARC-E, ARC-C, LogiQA)
- Outperforms existing efficient transformer methods in both speed and accuracy
- Demonstrates effectiveness across different scales from tiny to base models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MemoryFormer uses locality-sensitive hashing to map similar input embeddings to similar hash buckets, retrieving correlated vectors that approximate linear transformation.
- Core assumption: The sign-based hash function preserves locality such that similar inputs map to similar buckets with correlated output vectors.
- Evidence: [abstract] "we use a hash algorithm to retrieve a correlated subset of vectors dynamically based on the input embedding"
- Break condition: Approximation fails when embeddings have similar signs but very different values.

### Mechanism 2
- Claim: MemoryFormer achieves 81% FLOP reduction by eliminating matrix multiplication in favor of hash table lookups.
- Core assumption: Hash table operations are computationally negligible compared to matrix multiplication.
- Evidence: [abstract] "retrieving data blocks from memory is a much cheaper operation which requires little computations"
- Break condition: FLOP reduction breaks when τ becomes large enough that hash table operations approach matrix multiplication complexity.

### Mechanism 3
- Claim: Learnable hash table vectors maintain competitive performance through backpropagation.
- Core assumption: Learnable vectors can adapt to approximate linear transformation effectively during training.
- Evidence: [abstract] "We also design a method to make all the vectors stored in hash tables learnable via the back-propagation of gradients"
- Break condition: Performance degrades when hash table capacity is insufficient or learning rate too low.

## Foundational Learning

- Concept: Matrix multiplication and its computational complexity
  - Why needed here: Understanding why FC layers are computationally expensive (O(sdh)) and how MemoryFormer reduces this to O(s(τ+h)K)
  - Quick check question: If a transformer has hidden size d=1024, sequence length s=2048, and MemoryFormer uses τ=8, K=128, what is the approximate reduction factor in FLOPs for the FC layers?

- Concept: Locality-sensitive hashing (LSH) and its properties
  - Why needed here: The core mechanism relies on LSH to map similar inputs to similar hash buckets
  - Quick check question: How does the sign-based hash function in MemoryFormer differ from traditional LSH methods like Hyperplane-LSH?

- Concept: Backpropagation through non-differentiable operations
  - Why needed here: Understanding how the probability weighting function p(zk) makes the hashing operation differentiable
  - Quick check question: Why can't gradients flow directly through the integer hash indices, and how does p(zk) solve this problem?

## Architecture Onboarding

- Component map: Input embedding → Normalization → Memory Layer Q/K/V projection → MHA computation → Add & Norm → Memory Block (two Memory Layers with normalization) → Output
- Critical path: The critical computational path remains the MHA operation, with Memory Layers replacing FFN components.
- Design tradeoffs: MemoryFormer trades memory resources for computational efficiency - storing large hash tables (O(2^τ K h) space) to reduce computation (O(sdh) → O(s(τ+h)K)).
- Failure signatures: Performance degradation when hash table capacity is insufficient, when sequence length becomes extremely large making MHA dominant, or when memory bandwidth becomes the bottleneck.
- First 3 experiments:
  1. Measure FLOPs reduction on a small MemoryFormer model with varying τ values (4, 8, 16) to verify theoretical O(sdh) → O(s(τ+h)K) reduction.
  2. Compare validation perplexity with and without probability weighting function p(zk) to confirm its necessity for differentiability.
  3. Test model performance with different hash table sizes (varying K) on a benchmark task to find optimal memory-computation tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MemoryFormer performance scale with larger model sizes (e.g., 10B+ parameters) compared to standard transformers?
- Basis: Paper demonstrates effectiveness up to base models (1.3B parameters) but doesn't explore truly large models
- Why unresolved: Only tests up to base model size, leaving uncertainty about scalability to frontier model sizes
- Resolution: Experimental results comparing MemoryFormer to standard transformers on models of 10B+ parameters

### Open Question 2
- Question: What is the optimal temperature hyperparameter t for different task domains and model scales?
- Basis: Paper mentions temperature as hyperparameter but only provides ablation studies for single model scale
- Why unresolved: Only uses fixed temperature value across all experiments without exploring domain-specific tuning
- Resolution: Systematic experiments varying temperature across different tasks and model scales

### Open Question 3
- Question: How does MemoryFormer perform with extremely long sequences (100K+ tokens) where attention computation becomes dominant?
- Basis: Paper shows advantages for moderate sequence lengths but doesn't explore extreme lengths
- Why unresolved: Only tests up to sequence length 2048, leaving uncertainty about performance when attention becomes primary bottleneck
- Resolution: Experiments comparing MemoryFormer with attention-efficient variants on sequences of 100K+ tokens

## Limitations

- Performance fundamentally depends on quality of locality-sensitive hashing approximation, which may break down for complex linear transformations
- Effectiveness relies heavily on memory bandwidth for hash table lookups, potentially creating bottlenecks that offset computational gains
- Sign-based hashing mechanism may struggle with embeddings having similar signs but very different magnitudes

## Confidence

**High confidence**: The 81% FLOP reduction claim is well-supported by theoretical analysis comparing O(sdh) matrix multiplication complexity to O(s(τ+h)K) hash table operations.

**Medium confidence**: The claim of maintaining competitive performance while reducing computation depends on effectiveness of learnable hash tables and probability weighting function, which were not fully validated across all scenarios.

**Low confidence**: Scalability analysis to extremely large models or very long sequence lengths was not thoroughly examined, and memory bandwidth requirements were not fully characterized.

## Next Checks

1. **Scalability stress test**: Evaluate MemoryFormer performance on sequence lengths beyond 2048 tokens (e.g., 4096, 8192) to verify computational benefits persist when MHA operations become more dominant and measure memory bandwidth impact.

2. **Hash table capacity sensitivity**: Systematically vary the number of hash buckets and vectors per bucket (K values beyond 64-128) to identify point at which performance degradation begins and determine optimal memory-computation tradeoffs.

3. **Cross-task generalization**: Test MemoryFormer on a broader range of NLP tasks including fine-tuning benchmarks, not just zero-shot evaluation, to assess whether hashing approximation maintains effectiveness across different learning paradigms and task complexities.