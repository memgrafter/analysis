---
ver: rpa2
title: Are Expressive Models Truly Necessary for Offline RL?
arxiv_id: '2412.11253'
source_url: https://arxiv.org/abs/2412.11253
tags:
- learning
- offline
- tasks
- planning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the need for highly expressive models in
  offline reinforcement learning by proposing Recursive Skip-Step Planning (RSP),
  a lightweight method using shallow MLPs. RSP employs a recursive coarse-grained
  planning scheme to predict long-horizon sub-goals, reducing sequential modeling
  errors without complex architectures.
---

# Are Expressive Models Truly Necessary for Offline RL?

## Quick Facts
- **arXiv ID**: 2412.11253
- **Source URL**: https://arxiv.org/abs/2412.11253
- **Reference count**: 13
- **Primary result**: Lightweight Recursive Skip-Step Planning (RSP) achieves state-of-the-art performance on D4RL tasks while requiring significantly less training time and inference latency than transformer/diffusion-based methods.

## Executive Summary
This paper challenges the prevailing notion that highly expressive models are necessary for effective offline reinforcement learning. The authors propose Recursive Skip-Step Planning (RSP), a lightweight method that uses shallow MLPs with recursive coarse-grained planning to predict long-horizon sub-goals. By replacing fine-grained sequential modeling with this hierarchical approach, RSP reduces sequential modeling errors without requiring complex architectures like transformers or diffusion models. Experiments on D4RL benchmark tasks demonstrate that RSP achieves comparable or superior performance to expressive model methods while requiring only 180 seconds of training time and under 1ms inference latency, making it particularly effective for long-horizon and multi-stage tasks.

## Method Summary
RSP reformulates offline RL as a sequential modeling task by learning to recursively predict coarse-grained future sub-goals rather than fine-grained step-by-step transitions. The method uses simple two-layer MLPs (1024 units each) for both dynamics models and a goal-conditioned policy. Training employs hindsight relabeling where trajectories are augmented with future sub-goals as targets. During planning, RSP recursively predicts sub-goals at increasing horizons (typically k=32 steps) using multiple levels of dynamics models, then executes actions using the goal-conditioned policy. This hierarchical approach stabilizes long-horizon predictions by bypassing error accumulation that occurs in sequential fine-grained modeling.

## Key Results
- RSP achieves state-of-the-art or comparable performance to transformer/diffusion-based methods on D4RL tasks (AntMaze, Kitchen, Adroit, MuJoCo)
- Training time reduced to 180 seconds versus significantly longer times for expressive model baselines
- Inference latency under 1ms, substantially faster than transformer/diffusion alternatives
- Excels particularly on long-horizon and multi-stage tasks where sequential modeling errors typically compound

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive Skip-Step Planning reduces long-horizon compounding errors by replacing fine-grained sequential modeling with coarse-grained recursive sub-goal planning
- Mechanism: Instead of predicting every step, RSP predicts distant sub-goals (e.g., 32 steps ahead) recursively. Each level of recursion uses the previous sub-goal prediction to stabilize the next, effectively bypassing the accumulation of small errors over many steps
- Core assumption: Distant sub-goals contain enough information to guide current actions while being less sensitive to modeling noise than immediate next-step predictions
- Evidence anchors:
  - [abstract] "recursively planning coarse-grained future sub-goals based on current and target information, and then executes the action with a goal-conditioned policy"
  - [section] "Coarse-grained planning uses skip-step dynamics, predicting intermediate sub-goals via f(st+k|st, g) with an extended planning horizon k"
- Break condition: If sub-goal predictions drift significantly from the true trajectory, the policy may receive poor guidance, leading to compounding errors despite the recursive structure

### Mechanism 2
- Claim: Recursive planning with lightweight MLPs achieves performance on par with or better than expressive models by leveraging hierarchical stability rather than raw model capacity
- Mechanism: By using a hierarchy of simpler models, each focusing on a coarser temporal resolution, RSP achieves the same long-horizon modeling capability as transformers or diffusion models but with far fewer parameters and faster inference
- Core assumption: The recursive structure can capture long-term dependencies without needing the full representational power of large expressive models
- Evidence anchors:
  - [abstract] "RSP achieves state-of-the-art or comparable performance to transformer/diffusion-based methods while requiring significantly less training time (180s) and inference latency (under 1ms)"
  - [section] "adoption of expressive models imposes a large amount of computational load and complexity"
- Break condition: If the hierarchy depth is insufficient or the skip-step horizon is poorly chosen, the model may fail to capture necessary long-term dependencies, leading to degraded performance

### Mechanism 3
- Claim: Goal-conditioned supervised learning (GCSL) reformulation bypasses the credit assignment problem, enabling stable training of RSP without complex value function updates
- Mechanism: By relabeling trajectories with future goals and learning a policy via supervised maximum likelihood, RSP avoids the distributional shift and value overestimation issues common in value-based offline RL
- Core assumption: The relabeled dataset contains sufficient coverage of relevant state-goal pairs to learn a robust policy
- Evidence anchors:
  - [abstract] "reformulates the offline RL problem as a sequential modeling task, therefore bypassing the notoriously difficult credit assignment challenge of value learning"
  - [section] "RSP can be easily implemented using simple two-layer shallow MLPs for recursive skip-step dynamics models and the goal-conditioned policy"
- Break condition: If the dataset lacks diversity in goal reachability or contains significant out-of-distribution states, the supervised learning objective may fail to generalize

## Foundational Learning

- **Concept**: Markov Decision Process (MDP)
  - Why needed here: RSP operates within the standard RL framework, requiring understanding of states, actions, transitions, and rewards
  - Quick check question: What is the difference between the transition function T(st+1|at, st) and the policy π(at|st)?

- **Concept**: Goal-conditioned reinforcement learning
  - Why needed here: RSP is a goal-conditioned method that learns to reach arbitrary goals by conditioning on future states or returns
  - Quick check question: How does hindsight relabeling enable training a goal-conditioned policy from a fixed dataset?

- **Concept**: Sequential modeling and compounding errors
  - Why needed here: The paper's core innovation addresses the accumulation of errors in long-horizon sequential predictions
  - Quick check question: Why do fine-grained step-by-step predictions tend to accumulate more error than coarse-grained skip-step predictions?

## Architecture Onboarding

- **Component map**: Current state → Recursive sub-goal prediction (N levels) → Goal-conditioned policy → Action execution
- **Critical path**: State → Recursive sub-goal prediction (N levels) → Policy extraction → Action execution
- **Design tradeoffs**:
  - Recursion depth N vs. computational cost: Deeper recursion improves stability but increases inference latency
  - Skip-step horizon k vs. policy complexity: Larger k reduces error accumulation but makes policy learning harder
  - Model capacity vs. efficiency: Lightweight MLPs suffice due to recursive structure, avoiding need for transformers/diffusion
- **Failure signatures**:
  - High RMSE between predicted and actual sub-goals over long horizons
  - Policy performance degrades on tasks requiring precise long-horizon planning
  - Training instability when dataset lacks sufficient goal coverage
- **First 3 experiments**:
  1. Validate sub-goal prediction accuracy across different horizons and recursion depths on a simple navigation task
  2. Compare policy performance with varying recursion depth N while keeping horizon fixed
  3. Benchmark training time and inference latency against a transformer-based baseline on AntMaze tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal recursion depth N for different task complexities?
- **Basis in paper**: [explicit] The paper states "we observe that the RMSE of model rollout predictions achieves a lower asymptotic value and remains closer to the ground truth over longer-horizon rollouts, demonstrating the effectiveness of recursive planning" and "choose minimal recursion depth that achieves saturated performance: N = 2 is sufficient for most benchmark tasks or those with similar configurations"
- **Why unresolved**: While the paper provides some guidance on recursion depth selection, it doesn't comprehensively explore the relationship between recursion depth and task complexity across different environments. The paper only mentions N = 2 as sufficient for most benchmark tasks but doesn't provide a systematic analysis of when deeper recursion might be beneficial.
- **What evidence would resolve it**: Systematic ablation studies varying recursion depth across tasks of different complexities, with quantitative metrics showing the trade-off between performance gains and computational costs at different recursion depths.

### Open Question 2
- **Question**: How does the choice of sub-goal horizon k affect performance in tasks with varying action space dimensions?
- **Basis in paper**: [explicit] The paper states "selecting recursion depth and horizon requires to strike the delicate balance between the expressiveness of policy models and the long-horizon capability of dynamics models" and shows ablation studies on different horizon values for specific tasks.
- **Why unresolved**: The paper focuses on tasks with similar action space dimensions (24-DoF for Adroit, standard continuous actions for others) but doesn't explore how the optimal horizon k might change for tasks with significantly different action space characteristics.
- **What evidence would resolve it**: Experiments testing RSP on tasks with varying action space dimensions (e.g., discrete actions, high-dimensional continuous actions) while systematically varying the sub-goal horizon k.

### Open Question 3
- **Question**: Can RSP be effectively combined with more advanced policy learning methods beyond 2-layer MLPs?
- **Basis in paper**: [explicit] The paper states "Strong policy models may exhibit robustness to slight inaccuracies that accumulate in sub-goal predictions, enabling safe extension of recursion depth and fully harnessing the potential of RSP" and mentions that practitioners "using more advanced policy modeling and learning methods may consider enlarging the horizon and deepening the recursion process accordingly."
- **Why unresolved**: The paper deliberately uses simple 2-layer MLPs for policy learning to demonstrate RSP's effectiveness, but doesn't explore how more sophisticated policy architectures might enhance RSP's performance or allow for deeper recursion.
- **What evidence would resolve it**: Comparative experiments using RSP with various policy architectures (e.g., transformers, diffusion models, hierarchical policies) to quantify performance improvements and determine the optimal policy architecture for different task types.

## Limitations
- The recursive skip-step planning mechanism may not generalize equally well to domains with different state spaces, reward structures, or task complexities beyond the D4RL benchmark suite
- The paper's reliance on hindsight relabeling assumes the dataset contains sufficient coverage of reachable state-goal pairs, which may not hold for all offline RL scenarios
- The choice of recursion depth N and skip-step horizon k requires careful tuning and may not transfer directly across task types

## Confidence

- **High confidence**: RSP's performance advantages in training efficiency and inference latency are well-supported by the experimental results showing 180s training time versus transformer baselines and sub-1ms inference latency
- **Medium confidence**: The claim that lightweight MLPs with recursive planning can match expressive models on long-horizon tasks is supported by D4RL results but requires validation on more diverse environments
- **Medium confidence**: The mechanism by which recursive skip-step planning reduces compounding errors is theoretically sound but lacks ablation studies isolating the contribution of recursion depth versus skip-step horizon

## Next Checks
1. **Ablation study on recursion depth**: Systematically vary N (recursion depth) and k (skip-step horizon) to quantify their individual contributions to performance and error accumulation on a controlled navigation task
2. **Cross-domain generalization**: Test RSP on non-D4RL benchmarks (e.g., MetaWorld, RoboNet) with different state dimensions, action spaces, and task structures to assess robustness
3. **Dataset coverage analysis**: Quantify the proportion of state-goal pairs in the training data that are reachable versus unreachable to validate the assumption underlying the GCSL reformulation