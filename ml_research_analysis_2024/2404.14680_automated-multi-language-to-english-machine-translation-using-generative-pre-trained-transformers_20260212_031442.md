---
ver: rpa2
title: Automated Multi-Language to English Machine Translation Using Generative Pre-Trained
  Transformers
arxiv_id: '2404.14680'
source_url: https://arxiv.org/abs/2404.14680
tags:
- phi-1
- english
- score
- remm-v2-l2-13b
- phi-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated 16 open-source GPT models for multi-language
  translation into English using TED Talk transcripts. The best models for different
  quality metrics were ReMM-v2-L2-13B (BLEU, GLEU, METEOR) and Llama2-chat-AYT-13B
  (chrF), with mean scores ranging from 0.152 to 0.448 across languages.
---

# Automated Multi-Language to English Machine Translation Using Generative Pre-Trained Transformers

## Quick Facts
- arXiv ID: 2404.14680
- Source URL: https://arxiv.org/abs/2404.14680
- Reference count: 40
- 16 open-source GPT models evaluated for multi-language translation into English using TED Talk transcripts

## Executive Summary
This study evaluates 16 open-source GPT models for zero-shot, sentence-wise translation from 50 non-English languages into English using TED Talk transcripts. The evaluation uses four quality metrics (BLEU, GLEU, METEOR, chrF) and wall-clock time measurements, comparing model performance against Google Translate API. Models are tested without fine-tuning or language-specific prompts, relying solely on their pretraining knowledge. The study identifies ReMM-v2-L2-13B as the top performer for most metrics and Llama2-chat-AYT-13B for chrF scoring, while revealing significant challenges with low-resource languages.

## Method Summary
The study evaluates 16 open-source GPT models for zero-shot translation using TED Talk transcripts as reference data. Models are tested without fine-tuning or language-specific prompts, using Huggingface's transformers library for local inference on A100 GPUs. Translation is performed sentence-wise to accommodate context window limitations, with a fixed prompt ("Translate the following sentence into clearly written English text") applied across all languages. Quality is measured using BLEU, GLEU, METEOR, and chrF metrics, with results compared against Google Translate API as a baseline. The evaluation covers 50 non-English languages with 1,000 sentences per language.

## Key Results
- ReMM-v2-L2-13B achieved highest average scores across BLEU (0.152), GLEU (0.201), and METEOR (0.291) metrics
- Llama2-chat-AYT-13B performed best on chrF scoring (0.448) while maintaining competitive performance on other metrics
- Low-resource languages (Mongolian, Burmese, Kazakh, Kurdish, Armenian, Georgian) consistently scored below 0.05 across all metrics
- Translation quality was generally comparable to Google Translate API for high-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot translation works because GPT models have learned multilingual representations during pretraining that generalize to unseen language pairs.
- Mechanism: The models map source language tokens into a shared semantic space where English equivalents exist, enabling translation without explicit language-pair training.
- Core assumption: Pretraining corpora contained sufficient multilingual diversity for the model to learn cross-lingual mappings.
- Evidence anchors:
  - [abstract] states the study evaluates "black-box translation using current local GPT models" without fine-tuning, implying reliance on pretraining.
  - [section] mentions the task is "language agnostic" with no prompt tuning for each language, suggesting the model can handle unknown languages.
- Break condition: If source languages are truly low-resource and absent from pretraining data, translation quality degrades significantly (observed for Mongolian, Burmese, Kazakh).

### Mechanism 2
- Claim: Sentence-wise translation with small context windows works because translation quality metrics are computed per sentence, making context length less critical.
- Mechanism: By processing individual sentences, the model avoids context overflow issues while metrics (BLEU, GLEU, METEOR, chrF) can still evaluate translation quality on isolated units.
- Core assumption: Sentence-level metrics are sufficient proxies for overall translation quality.
- Evidence anchors:
  - [section] explains "we apply the translations for each individual sentence primarily to mitigate the problems that arise if we attempt to generate text that has a longer token length than what the GPT model was designed to process."
  - [abstract] notes "these GPT model inference calls are performed strictly locally, on single A100 Nvidia GPUs" with models having small context windows.
- Break condition: If translation quality depends heavily on broader context (e.g., pronouns, idiomatic expressions spanning sentences), sentence-wise processing may fail.

### Mechanism 3
- Claim: Fixed prompts work because GPT models have been fine-tuned on instruction datasets that generalize across diverse translation tasks.
- Mechanism: The prompt "Translate the following sentence into clearly written English text" leverages the model's instruction-following capability to produce reasonable translations without language-specific prompts.
- Core assumption: Instruction fine-tuning captured sufficient translation patterns to handle novel language pairs.
- Evidence anchors:
  - [section] describes using "the following text prompt" for all languages without modification, testing language-agnostic capability.
  - [abstract] emphasizes "no custom fine-tuning" and evaluates "black-box translation using current local GPT models."
- Break condition: If instruction fine-tuning was too narrow or focused on specific languages, the model may struggle with truly unseen languages.

## Foundational Learning

- Concept: BLEU, GLEU, METEOR, and chrF metrics
  - Why needed here: These metrics quantify translation quality by comparing generated text to reference translations, enabling objective model comparison.
  - Quick check question: If a translation perfectly matches reference tokens but uses different word order, which metric would still give high score?
- Concept: Context window limitations
  - Why needed here: GPT models have maximum token limits, necessitating sentence-wise processing for translation tasks.
  - Quick check question: If a sentence exceeds the context window, what processing strategy would you use?
- Concept: Zero-shot learning
  - Why needed here: The study evaluates translation without task-specific fine-tuning, relying on pretraining knowledge.
  - Quick check question: What distinguishes zero-shot from few-shot learning in this context?

## Architecture Onboarding

- Component map: Local GPT inference pipeline → Sentence tokenizer → Prompt formatter → GPU inference → Postprocessing → Metric computation
- Critical path: Tokenization → Prompt generation → GPU inference → Output cleaning → Metric calculation
- Design tradeoffs: Sentence-wise processing avoids context overflow but may lose coherence; fixed prompts are simple but suboptimal; local inference ensures privacy but may be slower than cloud APIs.
- Failure signatures: Consistently low scores for specific languages suggest data scarcity; high variance in scores suggests prompt instability; slow inference suggests model size or GPU limitations.
- First 3 experiments:
  1. Test translation quality on a single high-resource language (e.g., Spanish) to verify basic functionality
  2. Compare fixed prompt vs. language-specific prompts on the same language to assess prompt optimization potential
  3. Measure inference time scaling with model size to identify performance bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompts affect translation quality across low-resource languages?
- Basis in paper: [inferred] The study used a fixed, language-agnostic prompt and noted that better prompts could likely be found, but did not explore prompt variations.
- Why unresolved: The paper only tested one prompt format and explicitly stated that prompt optimization was not performed.
- What evidence would resolve it: Systematic testing of different prompt formats and styles for each low-resource language to identify optimal prompting strategies.

### Open Question 2
- Question: What is the minimum training data required for GPT models to achieve acceptable translation quality for a new language?
- Basis in paper: [inferred] The study found that low-resource languages (Mongolian, Burmese, Kazakh, Kurdish, Armenian, Georgian) performed poorly, suggesting data limitations.
- Why unresolved: The paper did not test models trained on different amounts of data for each language or explore the relationship between training data quantity and translation quality.
- What evidence would resolve it: Training experiments with varying amounts of data for different languages to establish minimum data requirements for acceptable performance.

### Open Question 3
- Question: How do GPT models perform on document-level translation compared to sentence-level translation?
- Basis in paper: [explicit] The study performed sentence-level translation due to context window limitations and noted this approach was taken to mitigate problems with longer text.
- Why unresolved: The paper only evaluated sentence-level translation and acknowledged this limitation but did not explore document-level performance.
- What evidence would resolve it: Testing GPT models on complete documents with techniques to handle context windows, then comparing results to sentence-level translations.

### Open Question 4
- Question: What architectural modifications could improve GPT model performance on low-resource languages?
- Basis in paper: [inferred] The consistent poor performance across all models for certain languages suggests fundamental architectural limitations rather than model-specific issues.
- Why unresolved: The study used existing models without modifications and did not explore architectural changes that might improve low-resource language performance.
- What evidence would resolve it: Testing modified architectures (e.g., with additional language-specific components or different tokenization schemes) and comparing their performance to standard GPT models.

## Limitations
- Zero-shot translation without fine-tuning or language-specific prompts may not be optimal for low-resource languages
- Sentence-wise processing may underestimate translation quality for languages requiring broader discourse context
- Performance degradation for low-resource languages suggests fundamental limitations in pretraining data coverage

## Confidence
**High Confidence**: The finding that zero-shot translation works reasonably well for high-resource languages is well-supported by the data. The consistent performance across multiple quality metrics (BLEU, GLEU, METEOR, chrF) for languages like Spanish, French, and German provides robust evidence for this claim.

**Medium Confidence**: The observation that low-resource languages perform poorly has strong empirical support but may be confounded by the specific dataset characteristics or the choice of evaluation metrics. The performance gap could potentially be reduced through targeted prompt engineering or limited fine-tuning.

**Low Confidence**: The claim that fixed prompts are sufficient for all languages lacks strong support. The study doesn't test language-specific prompts or explore prompt optimization strategies, making it unclear whether the observed performance represents the true ceiling for these models.

## Next Checks
1. **Prompt Optimization Study**: Test whether language-specific prompts or few-shot demonstrations improve translation quality for low-resource languages. This would determine if the zero-shot approach is truly optimal or if modest intervention could significantly improve results.

2. **Context Window Impact Analysis**: Evaluate whether processing multiple sentences together (when they fit within context windows) improves translation quality, particularly for languages showing poor performance in sentence-wise processing. This would validate whether the sentence-level approach is masking context-dependent translation issues.

3. **Pretraining Data Analysis**: Investigate the actual pretraining data coverage for poorly performing languages to quantify the relationship between training data availability and translation quality. This would help determine whether performance limitations are fundamental (no data) or architectural (data insufficient for zero-shot learning).