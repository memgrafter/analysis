---
ver: rpa2
title: 'PECAN: LLM-Guided Dynamic Progress Control with Attention-Guided Hierarchical
  Weighted Graph for Long-Document QA'
arxiv_id: '2410.04790'
source_url: https://arxiv.org/abs/2410.04790
tags:
- nodes
- graph
- attention
- spongebob
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PECAN, a method that combines the accuracy
  of LLMs with the efficiency of RAG for long-document QA. It constructs a hierarchical
  graph using LLM attention weights to represent events, then dynamically retrieves
  relevant information guided by LLM feedback and attention.
---

# PECAN: LLM-Guided Dynamic Progress Control with Attention-Guided Hierarchical Weighted Graph for Long-Document QA

## Quick Facts
- **arXiv ID:** 2410.04790
- **Source URL:** https://arxiv.org/abs/2410.04790
- **Authors:** Xinyu Wang; Yanzheng Xiang; Lin Gui; Yulan He
- **Reference count:** 40
- **Primary result:** Achieves LLM-level performance with computational costs comparable to RAG methods, outperforming baselines like MeMWalker, RAPTOR, and Llama-3.1 on four datasets.

## Executive Summary
This paper introduces PECAN, a method that combines the accuracy of LLMs with the efficiency of RAG for long-document QA. It constructs a hierarchical graph using LLM attention weights to represent events, then dynamically retrieves relevant information guided by LLM feedback and attention. Experiments show PECAN achieves LLM-level performance with computational costs comparable to RAG methods, outperforming baselines like MeMWalker, RAPTOR, and Llama-3.1 on four datasets. Key results include F1 scores up to 61.1% on NarrativeQA and 43.5% on HotpotQA, with significantly lower TFLOPs than full LLM inference.

## Method Summary
PECAN constructs a Hierarchical Weighted Directed Acyclic Graph (HWDAG) using LLM attention weights between document chunks and LLM-generated Information Points (IPs). The method employs LLM-Guided Dynamic Progress Control, where the LLM determines if retrieved nodes are sufficient for answering a query, and Attention-Guided Retrieval that uses normalized attention weights as edge weights in the graph. The approach dynamically adjusts retrieval depth based on query complexity, combining attention weights with embedding similarity for node selection. The method processes queries iteratively, using KV caching to avoid additional computation while maintaining LLM-level performance with RAG-level efficiency.

## Key Results
- Achieves F1 scores up to 61.1% on NarrativeQA and 43.5% on HotpotQA
- Outperforms baselines including MeMWalker, RAPTOR, and Llama-3.1 across four datasets
- Demonstrates TFLOPs efficiency comparable to RAG methods while maintaining LLM-level accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Progress Control dynamically adjusts the amount of retrieved information per query based on LLM feedback, avoiding fixed retrieval sizes.
- Mechanism: At each retrieval step, the LLM is prompted to judge whether current visited nodes are sufficient to answer the query. If "No", retrieval continues; if "Yes", it stops. This is done without additional computation via KV caching.
- Core assumption: LLM can accurately judge sufficiency of retrieved nodes for a given query.
- Evidence anchors:
  - [abstract] "Our method introduces two key improvements: (1) LLM-Guided Dynamic Progress Control: We leverage LLMs to dynamically control the retrieval process, adjusting the amount of retrieved information based on different queries to achieve a better balance of effectiveness and efficiency."
  - [section 3.2.1] "The LLM is prompted to determine whether the current set of visited nodes S is sufficient to answer a given query. The prompt asks the LLM, 'Can this question be answered by the following information?', followed by the query and visited nodes S."
- Break condition: LLM fails to accurately judge sufficiency, leading to under-retrieval (insufficient info) or over-retrieval (computational waste).

### Mechanism 2
- Claim: Attention-Guided Retrieval constructs a hierarchical graph where edges are derived from LLM attention weights, enabling efficient event tracking rather than simple text chunk retrieval.
- Mechanism: LLM attention weights between high-level and low-level nodes are averaged, normalized, and used as edge weights. Retrieval uses these weights plus query-node attention to identify relevant nodes across multiple paths.
- Core assumption: Attention weights between nodes meaningfully capture semantic relationships between events.
- Evidence anchors:
  - [abstract] "Attention-Guided Retrieval: We propose a novel retrieval method that constructs a hierarchical graph where edges are derived by LLM attention weights."
  - [section 3.1] "The attention from a higher-level node vl+1 i to a lower-level node vl j is averaged and then normalized across attention weights between nodes, yielding the edge weight ei,j from node vi to node vj."
- Break condition: Attention weights fail to capture true semantic relationships, leading to incorrect graph structure and poor retrieval performance.

### Mechanism 3
- Claim: Combining attention weights with embedding similarity provides complementary retrieval signals, with attention playing a more critical role.
- Mechanism: Final retrieval score combines attention-based scores (z) with embedding similarity scores. Ablation shows removing attention hurts performance more than removing embeddings.
- Core assumption: Attention weights and embedding similarity capture different aspects of relevance, with attention being more important for hierarchical relationships.
- Evidence anchors:
  - [section 4.2] "w/o Embedding Similarity We remove embedding similarity from the final score z, relying entirely on attention weights to compute retrieval scores. Across all four datasets, performance shows a slight decrease, though this drop is much smaller than that observed when the attention-guided retrieval is removed."
  - [section 4.2] "w/o Attention-Guided Retrieval We remove the use of attention and rely solely on embedding similarity for node search... Performance dropped across all datasets, with the most significant drop occurring in NarrativeQA."
- Break condition: If attention weights become unreliable or embeddings become the dominant signal, the retrieval performance may degrade.

## Foundational Learning

- Concept: Hierarchical information organization and graph construction
  - Why needed here: The method relies on structuring information into hierarchical levels where each node represents an event or information point, connected by attention-derived relationships
  - Quick check question: How does the method ensure that higher-level nodes capture the essence of multiple lower-level nodes?

- Concept: Dynamic resource allocation based on query complexity
  - Why needed here: Different queries require different amounts of information; fixed retrieval sizes lead to either waste or insufficient retrieval
  - Quick check question: What mechanism allows the system to determine when enough information has been retrieved for a specific query?

- Concept: Attention weight interpretation and normalization
  - Why needed here: The method uses attention weights as edge weights in the graph, requiring understanding of how to extract, aggregate, and normalize these weights
  - Quick check question: How are token-level attention weights converted to node-level edge weights in the hierarchical graph?

## Architecture Onboarding

- Component map: Document → Attention Graph Construction → Graph Storage → Query → Dynamic Graph Search → Answer
- Critical path: Document → Attention Graph Construction → Graph Storage → Query → Dynamic Graph Search → Answer
- Design tradeoffs:
  - Fixed vs. dynamic retrieval: Dynamic offers better balance but requires LLM sufficiency judgment
  - Attention vs. embedding: Attention captures hierarchical relationships better but may be noisier
  - Graph complexity vs. efficiency: More levels provide better organization but increase construction cost
- Failure signatures:
  - Under-retrieval: LLM prematurely judges sufficiency, missing critical information
  - Over-retrieval: LLM fails to recognize when enough information is available, wasting computation
  - Graph errors: Attention weights don't capture true relationships, leading to poor retrieval paths
  - Caching issues: KV cache not properly maintained across retrieval steps
- First 3 experiments:
  1. Compare fixed retrieval size (e.g., top-5) vs. dynamic retrieval on a small dataset to measure F1 improvement
  2. Test attention-guided retrieval vs. embedding-only retrieval on the same dataset to quantify attention's contribution
  3. Vary stop patience parameter (tn) on a validation set to find optimal effectiveness-efficiency tradeoff

## Open Questions the Paper Calls Out
- **Open Question 1**: How does the choice of stop patience (tn) and confidence threshold (tp) hyperparameters affect the effectiveness-efficiency trade-off across different query types and document lengths?
- **Open Question 2**: What is the impact of token-level attention averaging on retrieval quality, particularly for semantically irrelevant tokens like function words?
- **Open Question 3**: How does PECAN's performance scale with extremely long documents (e.g., 500K+ tokens) compared to other long-context approaches?
- **Open Question 4**: How does the quality of the generated Information Points (IPs) vary with different LLM models and prompt formulations?

## Limitations
- The effectiveness of LLM-guided sufficiency judgments depends on the LLM's ability to accurately assess when enough information has been retrieved.
- The hierarchical graph construction relies on LLM attention weights, but there's limited discussion of how sensitive the method is to different LLM architectures or attention patterns.
- The claim that PECAN achieves "LLM-level performance with RAG-level efficiency" is based on TFLOPs comparison, but actual wall-clock time and real-world deployment costs are not reported.

## Confidence
- **High Confidence**: The core architectural components (hierarchical graph construction, dynamic retrieval control, attention-guided scoring) are well-defined and the experimental methodology is sound.
- **Medium Confidence**: The claim that attention weights are more critical than embedding similarity for retrieval performance is supported by ablation studies.
- **Low Confidence**: The generalizability of the approach to different document domains beyond the tested datasets, and the scalability to significantly longer documents than tested, remains uncertain.

## Next Checks
1. **Sufficiency Judgment Analysis**: Conduct a detailed analysis of LLM sufficiency judgments by logging when the LLM terminates retrieval across different query types. Calculate precision/recall of sufficiency judgments compared to oracle decisions to quantify the accuracy of the dynamic control mechanism.

2. **Attention Weight Sensitivity Test**: Re-run the experiments using attention weights from different LLM architectures to assess how sensitive the hierarchical graph construction and subsequent retrieval performance are to the choice of attention source.

3. **Scalability Validation**: Test PECAN on documents significantly longer than the current datasets to evaluate whether the attention-guided hierarchical approach maintains its efficiency advantages as document length increases.