---
ver: rpa2
title: 'Diffusion Models in Low-Level Vision: A Survey'
arxiv_id: '2406.11138'
source_url: https://arxiv.org/abs/2406.11138
tags:
- image
- diffusion
- arxiv
- data
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey comprehensively reviews the use of diffusion models\
  \ in low-level vision tasks, covering natural images, medical imaging, remote sensing,\
  \ and video processing. The paper identifies three generic diffusion modeling frameworks\u2014\
  DDPM, NCSN, and SDE\u2014and explores their connections to other deep generative\
  \ models."
---

# Diffusion Models in Low-Level Vision: A Survey

## Quick Facts
- arXiv ID: 2406.11138
- Source URL: https://arxiv.org/abs/2406.11138
- Reference count: 40
- One-line primary result: Comprehensive survey of diffusion models in low-level vision, covering six representative tasks with evaluations of over 300 methods.

## Executive Summary
This survey provides a systematic review of diffusion models applied to low-level vision tasks, including image restoration, enhancement, and reconstruction across natural images, medical imaging, remote sensing, and video processing. The paper identifies three generic diffusion modeling frameworks—DDPM, NCSN, and SDE—and explores their connections to other deep generative models. It categorizes diffusion models based on training manners (supervised vs. zero-shot) and application goals, highlighting their advantages in producing high-quality, diverse samples while addressing traditional limitations like over-smoothing and artifacts.

## Method Summary
The survey systematically reviews diffusion model-based approaches in low-level vision by analyzing over 300 works across six representative tasks. It organizes methods by three generic frameworks (DDPM, NCSN, SDE) and categorizes them by training strategy and application domain. The evaluation framework compares performance using standard metrics (PSNR, SSIM, LPIPS) across benchmark datasets for each task. The paper identifies key challenges including computational efficiency, perceptual-distortion trade-offs, and content consistency, while proposing future directions for improving sampling efficiency and leveraging multi-modal advances.

## Key Results
- Diffusion models excel in super-resolution (IDM, DiffIR), deblurring (MSGD), and low-light enhancement (GSAD, Reti-Diff) with superior perceptual quality
- Three generic frameworks identified: DDPM (discrete reverse process), NCSN (continuous score matching), and SDE (stochastic differential equations)
- Major limitations include computational inefficiency and challenges maintaining content consistency during sampling
- Future directions focus on improving sampling efficiency, enhancing perceptual-distortion trade-offs, and multi-modal integration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Diffusion models leverage a denoising iterative process that progressively refines degraded images by reversing a noise injection forward process.
- **Mechanism**: The forward diffusion adds Gaussian noise over many timesteps, destroying the original image; the reverse denoising process learns to undo this noise, conditioned on the degraded input, enabling restoration across diverse degradation types.
- **Core assumption**: The data distribution is sufficiently continuous and the degradation process is reversible in the learned latent space.
- **Evidence anchors**:
  - [abstract] "diffusion model-based approaches, which employ a forward diffusion process to degrade an image and a reverse denoising process for image generation"
  - [section] "Diffusion models operate through a forward diffusion process, which introduces noise to the data, and a reverse diffusion process that learns to remove the noise"
- **Break condition**: If the degradation is too severe or non-invertible, the reverse process may fail to reconstruct meaningful content.

### Mechanism 2
- **Claim**: Diffusion models provide superior perceptual quality over traditional pixel-wise distortion minimization by modeling the image distribution rather than just matching pixel values.
- **Mechanism**: Instead of minimizing mean squared error, diffusion models sample from a learned distribution that captures natural image statistics, allowing reconstruction of textures and fine details even when exact pixel matches are impossible.
- **Core assumption**: The learned distribution over images includes realistic high-frequency details that are perceptually important.
- **Evidence anchors**:
  - [abstract] "diffusion models have become particularly prominent for producing high-quality, diverse samples with intricate texture details"
  - [section] "diffusion models have shown prowess in generating high-quality outputs with intricate details, addressing over-smoothing and artifacts"
- **Break condition**: If the distribution learned is too broad or biased, reconstructions may hallucinate unrealistic content.

### Mechanism 3
- **Claim**: Conditioning diffusion models on degraded inputs allows flexible adaptation to different inverse problems without retraining from scratch.
- **Mechanism**: By incorporating the degraded image as an additional input during the denoising steps (conditional diffusion), the model can focus the reverse process on restoring missing or corrupted content while preserving the available structure.
- **Core assumption**: The conditional information is sufficient to guide the reverse process toward the correct solution manifold.
- **Evidence anchors**:
  - [section] "Here the degraded LQ images are used as conditional inputs to guide the latent variables during inference"
  - [section] "Conditional denoising estimator sθ (x, y, t ) to learn an approximation to the posterior score function ∇xtlog p (xt|y)"
- **Break condition**: If the conditioning signal is too weak or noisy, the model may ignore it and revert to unconditional generation.

## Foundational Learning

- **Concept**: Markov Chain Monte Carlo (MCMC) sampling
  - **Why needed here**: Diffusion models rely on iterative sampling from a learned distribution; understanding MCMC helps grasp convergence and sampling efficiency.
  - **Quick check question**: What role does the step size play in MCMC convergence, and how does that relate to the noise schedule in diffusion models?

- **Concept**: Score matching and score functions
  - **Why needed here**: The core training objective of diffusion models is to estimate the gradient of the log-density (the score); familiarity with score matching clarifies why the loss function takes its form.
  - **Quick check question**: How does denoising score matching differ from vanilla score matching, and why is it more practical for image data?

- **Concept**: Variational inference and evidence lower bound (ELBO)
  - **Why needed here**: Diffusion models can be interpreted as variational inference methods with a specific parameterization; understanding ELBO helps relate them to other generative models.
  - **Quick check question**: In what way does the diffusion model objective correspond to a reweighted ELBO, and what are the implications for training stability?

## Architecture Onboarding

- **Component map**: Degraded input → Conditioning embedding → U-Net backbone → Time step embedding → Iterative denoising steps → Output restoration
- **Critical path**: Input degradation → conditioning embedding → iterative denoising steps → output restoration
- **Design tradeoffs**:
  - Sampling steps vs. quality: More steps improve fidelity but increase runtime
  - Conditioning strength: Strong conditioning improves accuracy but may reduce diversity
  - Latent space choice: Direct image space is simpler but more expensive than latent space diffusion
- **Failure signatures**:
  - Over-smoothing: Too few steps or overly aggressive noise schedule
  - Hallucinations: Weak conditioning or insufficient training data
  - Slow convergence: Poorly tuned scheduler or suboptimal network architecture
- **First 3 experiments**:
  1. Train a conditional diffusion model on a small super-resolution dataset and measure PSNR vs. number of sampling steps.
  2. Compare direct image-space vs. latent-space diffusion for inpainting in terms of runtime and visual quality.
  3. Evaluate the effect of different conditioning mechanisms (concatenation vs. cross-attention) on restoration accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can diffusion models be effectively adapted for real-world image restoration tasks involving multiple and unknown degradations?
- **Basis in paper**: [explicit] The paper discusses the challenges of real-world image restoration and mentions two potential approaches: distortion invariant learning (DIL) and distortion estimation (DE).
- **Why unresolved**: The paper highlights the need for methods that can generalize to diverse and unknown degradations, but does not provide concrete solutions or evaluations of these approaches in real-world scenarios.
- **What evidence would resolve it**: Experimental results demonstrating the effectiveness of DIL or DE techniques in handling real-world, multi-degradation scenarios, with comparisons to existing methods.

### Open Question 2
- **Question**: What are the most effective strategies for reducing the computational cost of diffusion models without significantly compromising their performance?
- **Basis in paper**: [explicit] The paper identifies high computational overhead as a major limitation of diffusion models and discusses several potential strategies, including reducing sample steps, compressing model consumption, and architectural optimizations.
- **Why unresolved**: While various strategies are mentioned, the paper does not provide a comprehensive comparison or analysis of their effectiveness in reducing computational cost while maintaining performance.
- **What evidence would resolve it**: Comparative studies evaluating the trade-offs between computational cost and performance for different strategies, such as model quantization, pruning, knowledge distillation, and efficient sampling techniques.

### Open Question 3
- **Question**: How can diffusion models be effectively integrated with other deep generative models to leverage their respective strengths for low-level vision tasks?
- **Basis in paper**: [explicit] The paper discusses the connections between diffusion models and other generative models like VAEs, normalizing flows, and GANs, and mentions hybrid models as a potential approach.
- **Why unresolved**: The paper highlights the potential benefits of hybrid models but does not provide concrete examples or evaluations of successful integrations for low-level vision tasks.
- **What evidence would resolve it**: Case studies or experimental results demonstrating the effectiveness of hybrid models that combine diffusion models with other generative models for specific low-level vision tasks, such as super-resolution or inpainting.

## Limitations
- Performance generalization across diverse real-world degradation scenarios remains unclear
- Computational efficiency trade-offs lack detailed comparative analysis
- Evaluation metric consistency varies across different papers and implementations

## Confidence
- **High confidence**: Identification of three generic diffusion modeling frameworks and their connections to other generative models
- **Medium confidence**: Categorization of diffusion models by training manners and application goals
- **Medium confidence**: Advantages of diffusion models in perceptual quality are supported but not fully quantified across all tasks

## Next Checks
1. Reproduce benchmark experiments: Implement and evaluate a subset of diffusion models (e.g., IDM, DiffIR for super-resolution) on standardized datasets (DIV2K, Urban100) using consistent evaluation metrics and compare results with traditional methods.

2. Analyze computational efficiency: Measure and compare the runtime and resource requirements (GPU memory, sampling steps) of different diffusion model implementations across multiple tasks to quantify practical deployment constraints.

3. Assess generalization across degradations: Test the performance of representative diffusion models on a diverse set of degradation types (e.g., blur kernels, noise levels, compression artifacts) to evaluate robustness and identify failure modes.