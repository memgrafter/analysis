---
ver: rpa2
title: Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient
  Descent using Constant and Decaying Learning Rates
arxiv_id: '2402.15344'
source_url: https://arxiv.org/abs/2402.15344
tags:
- decay
- batch
- size
- constant
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the iteration and stochastic first-order oracle
  (SFO) complexities of stochastic gradient descent (SGD) for nonconvex optimization
  in deep learning. The study examines SGD with constant and decaying learning rates
  under varying batch sizes.
---

# Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates

## Quick Facts
- arXiv ID: 2402.15344
- Source URL: https://arxiv.org/abs/2402.15344
- Authors: Kento Imaizumi; Hideaki Iiduka
- Reference count: 40
- Key outcome: Identifies critical batch sizes that minimize stochastic first-order oracle (SFO) complexity for SGD with constant and decaying learning rates in deep learning

## Executive Summary
This paper analyzes the iteration and stochastic first-order oracle (SFO) complexities of stochastic gradient descent (SGD) for nonconvex optimization in deep learning. The study examines SGD with constant and decaying learning rates under varying batch sizes. Key findings include the identification of critical batch sizes that minimize SFO complexity, with numerical experiments showing SGD using these critical batch sizes outperforms existing optimizers like momentum, Adam, AdamW, and RMSProp on CIFAR datasets.

## Method Summary
The paper theoretically analyzes SGD with constant and decaying learning rates, deriving convergence bounds and identifying critical batch sizes that minimize SFO complexity. The method involves implementing SGD with different learning rate schedules (constant, decaying with exponents a=1/4, 1/2, 3/4) and training ResNet and Wide-ResNet architectures on CIFAR-10/100 datasets. Batch sizes are varied systematically (21-212) and SFO complexity is measured at specific test accuracy thresholds. The theoretical predictions are validated through numerical experiments comparing SGD's performance against other optimizers.

## Key Results
- For SGD with constant learning rate, SFO complexity exhibits convexity with respect to batch size, with a critical batch size b* = 2C₂/ϵ² minimizing complexity
- Numerical experiments show SGD using critical batch sizes outperforms momentum, Adam, AdamW, and RMSProp in minimizing SFO complexity
- Measured critical batch sizes closely match theoretical predictions from convergence bounds
- For decaying learning rate SGD with a = 1/2, SFO complexity increases monotonically with batch size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For constant learning rate SGD, increasing batch size decreases iteration count but eventually increases SFO complexity due to diminishing returns.
- Mechanism: As batch size b increases, the variance term σ²/b in the convergence bound decreases, allowing fewer iterations K to reach target accuracy. However, SFO complexity N = Kb initially decreases then increases because K(b) ∝ b/(ϵ²b - C₂) decreases slower than b increases beyond critical batch size b* = 2C₂/ϵ².
- Core assumption: The variance term σ²/b dominates the convergence behavior and the critical batch size formula holds.
- Evidence anchors:
  - [abstract]: "For SGD with a constant learning rate, the number of iterations needed to achieve a target accuracy decreases monotonically and convexly with batch size" and "The SFO complexity exhibits convexity with respect to batch size" and "A critical batch size exists that minimizes the SFO complexity"
  - [section]: Theorem 3.2(ii) shows K(b) is monotone decreasing and convex, and Theorem 3.3(ii) shows N(b) is convex with critical batch size b* = 2C₂/ϵ²
  - [corpus]: Paper 48542 also studies minimizing SFO complexity through adaptive batch size, supporting this mechanism
- Break condition: If the variance term doesn't dominate (e.g., if σ² is very small), the critical batch size effect may disappear.

### Mechanism 2
- Claim: For decaying learning rate SGD with a = 1/2, SFO complexity increases monotonically with batch size, so critical batch size is approximately D₂/ϵ².
- Mechanism: With decaying rate αₖ = 1/√(⌊k/T⌋+1), the convergence bound has K ∝ 1/√b and N = Kb ∝ √b, which increases with b. Thus no interior minimum exists, and the best SFO complexity is achieved at smallest feasible batch size b ≈ D₂/ϵ².
- Core assumption: The 1/√b scaling of K holds and b > D₂/ϵ² is required.
- Evidence anchors:
  - [abstract]: "The SFO complexity increases once the batch size exceeds that size" and "The critical batch size for constant learning rates is b* = 2C₂/ϵ²"
  - [section]: Theorem 3.3(iii) states N'(b) > 0 for all b > D₂/ϵ² for a = 1/2 case
  - [corpus]: Paper 168537 studies increasing batch size with decaying learning rates, supporting this mechanism
- Break condition: If the decay schedule is different or if other terms dominate the convergence bound.

### Mechanism 3
- Claim: For decaying learning rate SGD with a ∈ (0,1/2) ∪ (1/2,1), critical batch size exists and minimizes SFO complexity.
- Mechanism: The convergence bound gives K ∝ b^(-a) or K ∝ b^(a-1) depending on a, and N = Kb has interior minimum at b* = (1-a)D₂/(a(1-2a)D₁) for a ∈ (0,1/2) or b* = 2a²D₂/((1-a)(2a-1)D₁) for a ∈ (1/2,1).
- Core assumption: The decay schedule αₖ = α/(⌊k/T⌋+1)^a holds and the critical batch size formula is correct.
- Evidence anchors:
  - [abstract]: "SGD using the critical batch size minimizes the SFO complexity" and "A critical batch size exists that minimizes the SFO complexity"
  - [section]: Theorem 3.3(ii) gives critical batch sizes for a ∈ (0,1/2) and a ∈ (1/2,1)
  - [corpus]: Paper 91114 studies increasing batch size with decaying learning rates, supporting this mechanism
- Break condition: If the decay schedule is different or if the theoretical bounds don't match practice.

## Foundational Learning

- Concept: Stochastic first-order oracle (SFO) complexity
  - Why needed here: SFO complexity N = Kb measures total gradient computations, which is the key metric being optimized in this paper.
  - Quick check question: What is the formula for SFO complexity and what does it represent in terms of computational cost?

- Concept: Critical batch size
  - Why needed here: The paper's main theoretical contribution is identifying critical batch sizes that minimize SFO complexity for different learning rate schedules.
  - Quick check question: How is the critical batch size defined mathematically and what is its significance for SGD performance?

- Concept: Nonconvex optimization convergence rates
  - Why needed here: Understanding the convergence rates (O(1/K) for constant LR vs O(1/√K) for decaying LR) is crucial for interpreting the theoretical results.
  - Quick check question: What are the typical convergence rates for SGD in nonconvex optimization and how do they depend on the learning rate schedule?

## Architecture Onboarding

- Component map: SGD optimizer with constant/decaying learning rates -> ResNet/Wide-ResNet training on CIFAR datasets -> SFO complexity measurement and critical batch size identification
- Critical path: Theoretical analysis → Critical batch size identification → Numerical validation on CIFAR datasets → Performance comparison
- Design tradeoffs: Larger batch sizes reduce iterations but increase per-iteration cost; decaying learning rates may have better final convergence but worse iteration complexity
- Failure signatures: If measured critical batch sizes don't match theoretical predictions, or if other optimizers outperform SGD at their critical batch sizes
- First 3 experiments:
  1. Verify K(b) is monotone decreasing and convex for constant learning rate SGD on a simple convex problem.
  2. Plot N(b) = K(b)b for decaying learning rate SGD with a = 1/2 to confirm it's increasing.
  3. Measure SFO complexity for SGD vs momentum at their respective critical batch sizes on CIFAR-10 with ResNet-18.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the existence of a critical batch size extend to variants of SGD beyond momentum and Adam, such as Nesterov accelerated gradient or AdaGrad?
- Basis in paper: The paper explicitly states that the results can only be applied to SGD and suggests investigating whether the results extend to variants of SGD.
- Why unresolved: The paper's theoretical framework and proofs are specifically constructed for SGD and do not account for the adaptive learning rate mechanisms or momentum terms in other optimizers.
- What evidence would resolve it: Empirical studies comparing the iteration and SFO complexities of SGD variants with different batch sizes, identifying critical batch sizes for each variant.

### Open Question 2
- Question: How does the choice of batch size affect the generalization performance of deep neural networks trained with SGD, beyond just the training loss?
- Basis in paper: The paper focuses on minimizing the training loss (expected risk) and does not explicitly discuss the impact of batch size on the generalization ability of the trained models.
- Why unresolved: The paper's analysis is limited to the convergence of the training process and does not consider the trade-off between training loss minimization and generalization performance.
- What evidence would resolve it: Empirical studies evaluating the test accuracy and generalization gap of deep neural networks trained with different batch sizes, analyzing the relationship between batch size and generalization performance.

### Open Question 3
- Question: Can the theoretical analysis of critical batch sizes be extended to stochastic optimization problems with non-convex loss functions that have multiple local minima?
- Basis in paper: The paper's analysis assumes that the loss function is L-smooth and has a unique global minimum, which may not hold for all non-convex optimization problems.
- Why unresolved: The paper's theoretical framework relies on the properties of L-smooth functions and does not account for the potential presence of multiple local minima or non-smooth loss functions.
- What evidence would resolve it: Theoretical extensions of the analysis to handle non-convex loss functions with multiple local minima, and empirical studies evaluating the impact of batch size on the convergence to different local minima.

## Limitations

- Theoretical analysis relies on idealized assumptions about gradient noise and smoothness constants that may not precisely hold in deep learning settings
- Critical batch size formulas depend on problem-specific constants C2 and D2 that are estimated rather than computed exactly
- Study focuses on specific architectures (ResNet, Wide-ResNet) and datasets (CIFAR), limiting generalizability to other domains or model types

## Confidence

- High confidence: The monotonic decrease of iterations with batch size for constant learning rates, supported by both theory (Theorem 3.2) and experimental validation
- Medium confidence: The existence and precise values of critical batch sizes, as these depend on estimated problem constants and may vary with implementation details
- Medium confidence: The SFO complexity comparisons with other optimizers, as this depends on proper hyperparameter tuning for each method

## Next Checks

1. Conduct ablation studies varying the noise level σ² to test the sensitivity of critical batch sizes to gradient variance assumptions
2. Extend experiments to additional architectures (DenseNet, EfficientNet) and datasets (ImageNet, COCO) to test generalizability
3. Perform robustness analysis by measuring how critical batch sizes change when using different initial learning rates or training durations