---
ver: rpa2
title: 'Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies'
arxiv_id: '2401.06760'
source_url: https://arxiv.org/abs/2401.06760
tags:
- accuracy
- metrics
- metric
- bleu
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of interpreting score differences
  between machine translation systems when multiple metrics with varying dynamic ranges
  are used. It proposes a method to map metric deltas to estimated pairwise system-level
  accuracy by analyzing a large human evaluation dataset, ToShip23.
---

# Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies

## Quick Facts
- arXiv ID: 2401.06760
- Source URL: https://arxiv.org/abs/2401.06760
- Reference count: 40
- One-line primary result: Different MT evaluation metrics require different score deltas to achieve the same human-perceived accuracy

## Executive Summary
This paper addresses the challenge of interpreting score differences between machine translation systems when using multiple evaluation metrics with varying dynamic ranges. The authors propose a method to map metric deltas to estimated pairwise system-level accuracy by analyzing a large human evaluation dataset. They demonstrate that different metrics require different score deltas to achieve the same level of human-perceived accuracy, and provide a tool to compare accuracy thresholds across metrics.

## Method Summary
The authors use the ToShip23 dataset to analyze how metric score differences correlate with human judgments of system quality. They bin system pairs by metric delta, calculate accuracy relative to human judgments, and fit sigmoid curves to estimate accuracy for any given delta. This approach transforms raw score differences into a unified accuracy scale, enabling meaningful comparison across metrics with different dynamic ranges.

## Key Results
- Different metrics require different deltas to achieve the same human-perceived accuracy (e.g., 1.06 BLEU ≈ 0.24 CometKiwiQE)
- Metric delta is stable across testset sizes, unlike statistical significance
- String-based metrics like BLEU perform poorly for unrelated systems
- The paper provides a tool to compare accuracy thresholds across different metrics

## Why This Works (Mechanism)

### Mechanism 1
Mapping metric deltas to estimated pairwise system-level accuracy enables meaningful comparison across metrics with different dynamic ranges. The method bins system pairs by metric delta, calculates accuracy relative to human judgments, and fits sigmoid curves to estimate accuracy for any given delta.

### Mechanism 2
String-based metrics like BLEU should not be used for unrelated systems due to poor performance in distinguishing such systems. The analysis shows BLEU has low accuracy when comparing unrelated systems, while metrics like CometKiwiQE maintain higher accuracy across both iterated and unrelated systems.

### Mechanism 3
Metric delta is stable across testset sizes, unlike statistical significance which becomes more stable as testset size grows. By sampling subsets of varying sizes, the study finds that metric delta fluctuates but remains mostly constant, while p-values decrease with larger testsets.

## Foundational Learning

- **Concept**: Pairwise system-level accuracy
  - **Why needed here**: It serves as the metric to evaluate how well automatic metrics align with human judgment
  - **Quick check question**: How is pairwise accuracy calculated from system-level scores and human judgments?

- **Concept**: Sigmoid fitting for accuracy estimation
  - **Why needed here**: It provides a smooth, bounded function to model the relationship between metric delta and accuracy
  - **Quick check question**: Why is a sigmoid function chosen over a linear model for fitting accuracy?

- **Concept**: Statistical significance vs. metric delta
  - **Why needed here**: Understanding the difference helps in interpreting results and choosing appropriate evaluation methods
  - **Quick check question**: How does increasing testset size affect p-values and metric delta differently?

## Architecture Onboarding

- **Component map**: ToShip23 dataset -> Automatic metrics (BLEU, ChrF, Comet variants) -> Human evaluation data -> Sigmoid fitting algorithm -> Accuracy thresholds
- **Critical path**: Data preparation → Metric scoring → Binning by delta → Accuracy calculation → Sigmoid fitting → Threshold determination
- **Design tradeoffs**: Using a large proprietary dataset ensures robustness but limits reproducibility. The choice of sigmoid fitting is arbitrary but effective; other models might capture nuances differently.
- **Failure signatures**: Inconsistent human judgments, poor metric performance on certain language pairs, or instability of metric delta with testset size.
- **First 3 experiments**:
  1. Verify that binning system pairs by metric delta produces expected accuracy trends
  2. Test the stability of metric delta across different testset sizes
  3. Compare accuracy thresholds for a subset of metrics to validate the sigmoid fitting approach

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do metric deltas and accuracy vary when evaluating LLM-based machine translation systems compared to traditional neural MT models?
- **Basis in paper**: The paper acknowledges that almost all MT systems used in the evaluation are not based on LLMs, and explicitly states that different behavior might be observed when evaluating LLM-based models.
- **Why unresolved**: The evaluation dataset (ToShip23) primarily consists of traditional neural MT systems, with only a limited number of LLM-based translations included.
- **What evidence would resolve it**: Conducting a large-scale evaluation of LLM-based MT systems using the same methodology as ToShip23, comparing metric deltas and accuracy across various LLM models and traditional MT systems.

### Open Question 2
- **Question**: How do metric deltas and accuracy differ across various language families or typological features beyond the broad categories of into-English, out-of-English, and CJK languages?
- **Basis in paper**: The paper briefly investigates language pair effects by binning languages into into-English, out-of-English, and CJK groups, but acknowledges that there isn't enough data to examine each language pair individually.
- **Why unresolved**: The ToShip23 dataset, while large, doesn't provide sufficient data to analyze individual language pairs or more specific language family groupings.
- **What evidence would resolve it**: Analyzing metric deltas and accuracy across more specific language family groupings (e.g., Romance languages, Germanic languages, Slavic languages) or typological features with a dataset large enough to provide statistical significance.

### Open Question 3
- **Question**: How does the inclusion of different types of translation errors or quality levels in the test sets affect the relationship between metric deltas and accuracy?
- **Basis in paper**: The paper mentions that the metric delta can be manipulated by adding segments that are more difficult than the rest, but doesn't investigate how different types or distributions of errors affect metric performance.
- **Why unresolved**: The paper focuses on overall pairwise accuracy without delving into how different types of translation errors or varying quality levels within test sets impact the relationship between metric deltas and accuracy.
- **What evidence would resolve it**: Conducting experiments where test sets are constructed with controlled distributions of different error types or quality levels, then measuring how metric deltas and accuracy change across these controlled variations.

## Limitations

- The proprietary nature of the ToShip23 dataset limits reproducibility and independent validation of results
- The sigmoid fitting approach is somewhat arbitrary, and alternative models might capture the relationship between metric deltas and accuracy differently
- The study focuses primarily on pairwise system comparisons, which may not fully represent the needs of ranking multiple systems simultaneously

## Confidence

- **High confidence**: The observation that different metrics require different deltas to achieve the same human-perceived accuracy is well-supported by the data
- **Medium confidence**: The stability of metric delta across testset sizes is demonstrated but could benefit from additional validation with more diverse datasets
- **Medium confidence**: The recommendation against using string-based metrics for unrelated systems is supported but may not generalize to all language pairs or domains

## Next Checks

1. Validate the sigmoid fitting approach by testing alternative curve-fitting methods (e.g., polynomial regression) to assess robustness of accuracy estimates
2. Replicate the metric delta stability analysis using publicly available WMT datasets with varying testset sizes to confirm generalizability
3. Conduct cross-lingual validation by comparing metric performance across different language pairs to test the universality of the proposed thresholds