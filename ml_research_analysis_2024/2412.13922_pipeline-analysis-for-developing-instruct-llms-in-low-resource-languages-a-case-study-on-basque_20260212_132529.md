---
ver: rpa2
title: 'Pipeline Analysis for Developing Instruct LLMs in Low-Resource Languages:
  A Case Study on Basque'
arxiv_id: '2412.13922'
source_url: https://arxiv.org/abs/2412.13922
tags:
- basque
- language
- english
- dataset
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a detailed analysis of strategies for developing
  a model capable of following instructions in a low-resource language, specifically
  Basque. The study focuses on three key stages: pre-training, instruction tuning,
  and alignment with human preferences.'
---

# Pipeline Analysis for Developing Instruct LLMs in Low-Resource Languages: A Case Study on Basque

## Quick Facts
- arXiv ID: 2412.13922
- Source URL: https://arxiv.org/abs/2412.13922
- Authors: Ander Corral; Ixak Sarasua; Xabier Saralegi
- Reference count: 40
- Key outcome: Pre-training, instruction tuning, and alignment pipeline achieves 24-point improvement in instruction-following for Basque

## Executive Summary
This work presents a comprehensive analysis of strategies for developing instruction-following language models for low-resource languages, using Basque as a case study. The authors systematically evaluate three key stages: continual pre-training with a high-quality Basque corpus, instruction tuning using automatically translated datasets, and human preference alignment. The study demonstrates that adapting existing foundational models through this pipeline significantly improves performance on both natural language understanding and instruction-following tasks in the target language. The resulting Llama-eus models establish new state-of-the-art performance for Basque in the sub-10B parameter category.

## Method Summary
The methodology follows a three-stage pipeline: (1) continual pre-training of Llama-3.1-8B on a high-quality Basque corpus (ZelaiHandi) mixed with English data in an 80:20 ratio for 4 epochs; (2) instruction tuning using automatically translated versions of No_Robots and SlimOrca datasets with LoRA fine-tuning; (3) human preference alignment using automatically translated UltraFeedback dataset with DPO. The training infrastructure utilized NVIDIA A100 GPUs with DeepSpeed ZeRO optimization. Manual evaluation assessed instruction-following performance on 100 translated Basque instructions, categorizing responses as correct, partially correct, or wrong.

## Key Results
- Continual pre-training with 600M-word Basque corpus improved NLU performance by over 12 points
- Instruction tuning and alignment using translated datasets achieved 24-point improvement in instruction-following
- Llama-eus-8B and Llama-eus-8B-instruct models establish new state-of-the-art for Basque in sub-10B parameter category
- 80:20 English-to-Basque mixing ratio proved effective for continual pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pre-training with high-quality Basque corpus improves NLU over the foundational model.
- Mechanism: Fine-tuning the foundational model on domain-specific language data reinforces language-specific grammar, syntax, and vocabulary, while preserving functional competencies (reasoning, world knowledge) learned from the original English training.
- Core assumption: The foundational model's learned functional competencies transfer across languages and remain intact during fine-tuning.
- Evidence anchors:
  - [abstract] "Our findings demonstrate that continual pre-training with a high-quality Basque corpus of around 600 million words improves natural language understanding (NLU) of the foundational model by over 12 points."
  - [section 3.2] Description of the high-quality Basque corpus (ZelaiHandi) used for pre-training.
  - [corpus] Corpus signals show related work on low-resource language adaptation and instruction tuning, supporting the general approach.
- Break Condition: Catastrophic forgetting occurs, causing the model to lose its functional competencies in English or if the pre-training corpus is too small or of poor quality.

### Mechanism 2
- Claim: Automatic translation of English instruction datasets is effective for instruction tuning in low-resource languages.
- Mechanism: Translated instruction datasets provide sufficient linguistic diversity and task variety to teach the model to follow instructions in the target language, even without manually curated datasets.
- Core assumption: Machine translation preserves the semantic intent and task structure of the original instructions, allowing effective instruction tuning.
- Evidence anchors:
  - [abstract] "instruction tuning and human preference alignment using automatically translated datasets proved highly effective, resulting in a 24-point improvement in instruction-following performance."
  - [section 4.1] Description of the use of No_Robots and SlimOrca datasets translated to Basque for instruction tuning.
  - [corpus] Corpus signals show related work on low-resource language adaptation and instruction tuning, supporting the general approach.
- Break Condition: Machine translation introduces significant semantic errors or the translated instructions lack the linguistic diversity needed for effective instruction tuning.

### Mechanism 3
- Claim: Using a language-adapted foundational model as the base for instruction tuning and alignment improves performance compared to using a non-adapted model.
- Mechanism: Pre-training the model on the target language data provides a better foundation for learning task-specific instructions and aligning with human preferences in that language.
- Core assumption: The language-specific knowledge gained during pre-training facilitates the learning of instruction-following and preference alignment in the same language.
- Evidence anchors:
  - [abstract] "The resulting models, Llama-eus-8B and Llama-eus-8B-instruct, establish a new state-of-the-art for Basque in the sub-10B parameter category."
  - [section 4.4] Comparison of Llama-eus-8B + SlimOrca_eu with Llama-3.1-8B + SlimOrca_eu, showing superior performance of the language-adapted model.
  - [corpus] Corpus signals show related work on low-resource language adaptation and instruction tuning, supporting the general approach.
- Break Condition: The pre-training process does not effectively transfer language-specific knowledge, or the instruction tuning and alignment datasets are not sufficiently diverse or high-quality.

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding catastrophic forgetting is crucial for designing effective pre-training strategies that improve language-specific performance without degrading the model's general capabilities.
  - Quick check question: What strategies can be employed to mitigate catastrophic forgetting during pre-training on a new language?

- Concept: Transfer Learning
  - Why needed here: Transfer learning principles are essential for understanding how knowledge gained during pre-training on one language can be leveraged to improve performance on another language.
  - Quick check question: How does the foundational model's knowledge of grammar and syntax in English contribute to its ability to learn Basque during pre-training?

- Concept: Instruction Tuning
  - Why needed here: Instruction tuning is a key technique for adapting LLMs to follow instructions in a specific language, and understanding its principles is crucial for effective implementation.
  - Quick check question: What are the key differences between instruction tuning and standard fine-tuning, and how do these differences impact the adaptation process?

## Architecture Onboarding

- Component map:
  Foundational Model (Llama-3.1-8B) -> Pre-training Data (ZelaiHandi dataset) -> Instruction Tuning Data (No_Robots_eu and SlimOrca_eu) -> Alignment Data (UltraFeedback_eu) -> Training Infrastructure (NVIDIA A100 GPUs, DeepSpeed, Transformers library)

- Critical path:
  1. Pre-train the foundational model on the Basque corpus
  2. Instruction tune the pre-trained model using translated instruction datasets
  3. Align the instruction-tuned model with human preferences using translated preference data

- Design tradeoffs:
  - Model size vs. computational efficiency: Smaller models are more efficient but may have lower performance
  - Data quality vs. data quantity: High-quality data is more effective but may be harder to obtain in low-resource languages
  - Automatic translation vs. manual translation: Automatic translation is faster and cheaper but may introduce errors

- Failure signatures:
  - Catastrophic forgetting: The model loses its general capabilities while adapting to the new language
  - Poor translation quality: The translated instruction and alignment datasets introduce significant errors, hindering the adaptation process
  - Insufficient data: The available data is not diverse or comprehensive enough to effectively train the model

- First 3 experiments:
  1. Evaluate the foundational model's performance on Basque NLU tasks to establish a baseline
  2. Pre-train the foundational model on the Basque corpus and evaluate its performance on Basque NLU tasks
  3. Instruction tune the pre-trained model using translated instruction datasets and evaluate its ability to follow instructions in Basque

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are other automatic translation methods (e.g., GPT-4, other neural machine translation models) compared to the Elia platform for generating Basque instruction datasets?
- Basis in paper: The paper mentions using the Elia machine translation platform for generating Basque instruction datasets, but does not compare its performance to other translation methods.
- Why unresolved: The paper only evaluates the effectiveness of the translated datasets themselves, not the translation method used to generate them.
- What evidence would resolve it: A comparative study evaluating the quality of Basque instruction datasets generated using different automatic translation methods (e.g., GPT-4, other neural machine translation models) on the same English source datasets.

### Open Question 2
- Question: What is the impact of using different pre-training datasets (e.g., Latxa, ZelaiItxi) on the final performance of the Llama-eus models?
- Basis in paper: The paper mentions evaluating different pre-training datasets (ZelaiHandi, ZelaiItxi, Latxa) but does not provide a detailed analysis of their impact on the final model performance.
- Why unresolved: The paper only reports the average performance results of continual pre-training using different datasets, but does not delve into the specific effects of each dataset on different tasks or model capabilities.
- What evidence would resolve it: A comprehensive analysis of the impact of different pre-training datasets on the performance of Llama-eus models across various tasks and benchmarks, including detailed comparisons of their strengths and weaknesses.

### Open Question 3
- Question: How does the performance of Llama-eus models compare to other multilingual models (e.g., GPT-4, Mixtral) on Basque language tasks?
- Basis in paper: The paper focuses on evaluating Llama-eus models against other Llama variants and Latxa models, but does not compare their performance to other multilingual models.
- Why unresolved: The paper does not provide a benchmark against state-of-the-art multilingual models, which could offer insights into the relative strengths and weaknesses of Llama-eus models in the context of the broader landscape of language models.
- What evidence would resolve it: A comparative study evaluating the performance of Llama-eus models against other multilingual models (e.g., GPT-4, Mixtral) on a set of standardized Basque language tasks and benchmarks.

## Limitations

- The study relies heavily on automatic translation for instruction tuning and preference alignment, which introduces potential semantic errors that could affect the quality of the adapted models
- The evaluation methodology combines manual assessment with a single automated metric (MT-Bench), creating potential gaps in objective performance measurement across different task types
- The authors acknowledge that the results may not generalize beyond similar low-resource languages, and the study focuses specifically on Basque without exploring whether the same pipeline would work for other low-resource language contexts

## Confidence

**High Confidence** (4 claims):
- The pre-training corpus contains approximately 600 million words of high-quality Basque text
- Instruction tuning using automatically translated datasets improved instruction-following performance by 24 points
- The resulting models achieve state-of-the-art performance for Basque in the sub-10B parameter category
- The 80:20 English-to-Basque mixing ratio was effective for continual pre-training

**Medium Confidence** (2 claims):
- The catastrophic forgetting was successfully avoided during pre-training
- The machine translation quality was sufficient for effective instruction tuning

**Low Confidence** (1 claim):
- The models will generalize effectively to other low-resource languages beyond Basque

## Next Checks

1. **Translation Quality Assessment**: Conduct a detailed analysis comparing machine-translated instruction datasets against professionally translated versions to quantify semantic drift and identify specific error patterns that could affect model performance.

2. **Cross-Lingual Transfer Evaluation**: Test the pre-trained Basque model on English benchmarks to empirically verify that functional competencies were preserved during pre-training and quantify any performance degradation.

3. **Generalization Study**: Apply the same pipeline (pre-training + instruction tuning + alignment) to another low-resource language with different linguistic features (e.g., a non-Indo-European language) to validate the approach's broader applicability.