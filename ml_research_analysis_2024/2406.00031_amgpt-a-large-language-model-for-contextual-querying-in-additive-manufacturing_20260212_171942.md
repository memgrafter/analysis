---
ver: rpa2
title: 'AMGPT: a Large Language Model for Contextual Querying in Additive Manufacturing'
arxiv_id: '2406.00031'
source_url: https://arxiv.org/abs/2406.00031
tags:
- used
- laser
- high
- alloys
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AMGPT, a specialized LLM for metal additive
  manufacturing queries, using Llama2-7B with Retrieval-Augmented Generation (RAG)
  and 50 domain-specific papers. By integrating RAG, AMGPT delivers precise, contextually
  accurate responses to technical queries, outperforming general LLMs in domain-specific
  knowledge.
---

# AMGPT: a Large Language Model for Contextual Querying in Additive Manufacturing

## Quick Facts
- arXiv ID: 2406.00031
- Source URL: https://arxiv.org/abs/2406.00031
- Authors: Achuth Chandrasekhar; Jonathan Chan; Francis Ogoke; Olabode Ajenifujah; Amir Barati Farimani
- Reference count: 40
- Key outcome: AMGPT achieves 80% response fidelity without hallucinations for metal additive manufacturing queries using RAG with 50 domain papers

## Executive Summary
This work introduces AMGPT, a specialized LLM for metal additive manufacturing queries, using Llama2-7B with Retrieval-Augmented Generation (RAG) and 50 domain-specific papers. By integrating RAG, AMGPT delivers precise, contextually accurate responses to technical queries, outperforming general LLMs in domain-specific knowledge. Key results include high response fidelity without hallucinations for 80% of prompts, and optimal performance with controlled sampling temperature, similarity top_k, and system prompt. This open-source approach enables real-time retrieval of up-to-date AM literature, supporting researchers with reliable, evidence-based insights for decision-making in additive manufacturing.

## Method Summary
The method employs Llama2-7B as the base LLM in a RAG setup, using Mathpix to convert PDF documents to TeX format for processing. A vector database is created using sentence-transformers embeddings stored in LlamaIndex. The system retrieves relevant documents through semantic search using cosine similarity between query and document embeddings, then generates responses through the LLM. A Streamlit interface provides user interaction, with inference parameters like temperature and top_k values controlled for optimal performance.

## Key Results
- High response fidelity without hallucinations for 80% of prompts
- Optimal performance achieved through controlled sampling temperature, similarity top_k, and system prompt
- Outperforms general LLMs in domain-specific knowledge for metal additive manufacturing queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation (RAG) enables AMGPT to provide precise, contextually accurate responses to technical queries by dynamically incorporating domain-specific information from a corpus of 50 AM papers.
- Mechanism: RAG integrates a retriever component that uses semantic embeddings to identify the most relevant documents from a vector database. These documents are then fed into the LLM alongside the query, allowing the model to generate responses grounded in actual literature rather than relying solely on its pre-training data.
- Core assumption: The retriever can accurately identify relevant information from the domain corpus using semantic similarity, and the LLM can effectively incorporate this retrieved context into its responses.
- Evidence anchors:
  - [abstract] "Instead of training from scratch, we employ a pre-trained Llama2-7B model from Hugging Face in a Retrieval-Augmented Generation (RAG) setup, utilizing it to dynamically incorporate information from âˆ¼50 AM papers and textbooks in PDF format."
  - [section] "Our RAG system implements a dual-encoder retrieval mechanism, comprising a query encoder and a document encoder. Both encoders are fine-tuned versions of transformer-based models, optimized to encode text inputs into high-dimensional vectors."
- Break condition: If the semantic similarity search fails to retrieve relevant documents, or if the LLM cannot effectively incorporate the retrieved context into its responses, the RAG system will not provide accurate or contextually relevant answers.

### Mechanism 2
- Claim: Fine-tuning the embedding model for semantic search improves the quality of retrieved documents, leading to more accurate and relevant responses from the LLM.
- Mechanism: The embedding model, "sentence-transformers/all-mpnet-base-v2", is used to convert both the query and the documents into high-dimensional vectors. These vectors are then compared using cosine similarity to identify the most relevant documents. By using a pre-trained embedding model fine-tuned for semantic search, the system can capture the semantic meaning of the query and documents, rather than just keyword matching.
- Core assumption: The embedding model can accurately capture the semantic meaning of the query and documents, and the cosine similarity measure is an effective way to compare these embeddings and identify relevant documents.
- Evidence anchors:
  - [abstract] "Mathpix is used to convert these PDF documents into TeX format, facilitating their integration into the RAG pipeline managed by LlamaIndex."
  - [section] "Semantic search is different from standard keyword search by focusing on the capture of the intention of the user's query [51]."
- Break condition: If the embedding model does not accurately capture the semantic meaning of the query and documents, or if cosine similarity is not an effective measure for comparing these embeddings, the retrieved documents will not be relevant, and the LLM will not be able to provide accurate responses.

### Mechanism 3
- Claim: Controlling inference parameters such as sampling temperature, system prompt, max token length, and similarity top_k allows for fine-tuning the behavior and quality of the LLM's responses.
- Mechanism: By adjusting these parameters, the system can control the randomness, creativity, specificity, and detail of the LLM's responses. For example, a lower sampling temperature leads to more deterministic and factual responses, while a higher temperature allows for more creative exploration. Similarly, the system prompt can be used to control the tone and style of the response, while the max token length limits the response length.
- Core assumption: The LLM's behavior and response quality are significantly influenced by these inference parameters, and adjusting them allows for fine-tuning the system to meet specific requirements.
- Evidence anchors:
  - [section] "Sampling temperature [55] is a parameter that determines the randomness and hence, the creativity of the output of the LLM during the RAG execution."
  - [section] "The system prompt is a set of instructions provided to the large language model (LLM) before the knowledge retrieval step."
- Break condition: If the LLM's behavior and response quality are not significantly influenced by these inference parameters, or if adjusting them does not lead to the desired changes in behavior, the system will not be able to fine-tune its responses to meet specific requirements.

## Foundational Learning

- Concept: Vector embeddings and semantic search
  - Why needed here: Vector embeddings allow for the representation of text as numerical vectors, enabling the comparison of semantic similarity between queries and documents. This is crucial for the retriever component of the RAG system to identify relevant documents.
  - Quick check question: What is the difference between semantic search and keyword search, and why is semantic search more effective for retrieving relevant documents in a domain-specific corpus?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG combines the strengths of retrieval-based and generation-based approaches, allowing the LLM to access and incorporate external knowledge from a domain-specific corpus. This is essential for providing accurate and contextually relevant responses to technical queries in metal additive manufacturing.
  - Quick check question: How does RAG differ from fine-tuning the LLM on a domain-specific corpus, and what are the advantages and disadvantages of each approach?

- Concept: Inference parameters and their impact on LLM behavior
  - Why needed here: Understanding how inference parameters such as sampling temperature, system prompt, max token length, and similarity top_k influence the LLM's behavior is crucial for fine-tuning the system to meet specific requirements. This knowledge allows for the optimization of the system's performance and response quality.
  - Quick check question: How does the sampling temperature parameter influence the randomness and creativity of the LLM's responses, and what are the implications of using different temperature values in a domain-specific context?

## Architecture Onboarding

- Component map: User Interface (Streamlit) -> Query Processor -> Embedding Model (sentence-transformers/all-mpnet-base-v2) -> Retriever (dual-encoder with cosine similarity) -> Vector Database (LlamaIndex) -> LLM (Llama2-7B) -> Response Generator

- Critical path:
  1. User submits query through the Streamlit interface.
  2. Query is embedded using the embedding model.
  3. Retriever identifies the most relevant documents from the vector database using cosine similarity.
  4. Retrieved documents and query are fed into the LLM.
  5. LLM generates response based on the provided context.
  6. Response is returned to the user through the Streamlit interface.

- Design tradeoffs:
  - Using a pre-trained LLM vs. fine-tuning on a domain-specific corpus: Pre-trained LLMs are more general and require less data, but may not capture domain-specific knowledge as effectively as a fine-tuned model.
  - Size of the domain corpus: A larger corpus may provide more comprehensive coverage of the domain, but also increases the computational cost of retrieval and may introduce more noise.
  - Choice of embedding model: Different embedding models may capture semantic meaning differently, and the choice of model can impact the quality of retrieved documents.
  - Inference parameter values: The values of inference parameters such as sampling temperature and max token length can significantly influence the behavior and quality of the LLM's responses.

- Failure signatures:
  - Irrelevant or inaccurate responses: This may indicate issues with the retriever's ability to identify relevant documents or the LLM's ability to incorporate the retrieved context.
  - Slow response times: This may indicate computational bottlenecks in the retrieval or generation process.
  - Inconsistent responses to the same query: This may indicate issues with the stability of the LLM or the retrieval process.

- First 3 experiments:
  1. Vary the sampling temperature parameter and observe its impact on the randomness and creativity of the LLM's responses.
  2. Test the system with queries that require information from specific documents in the corpus and evaluate the retriever's ability to identify the relevant documents.
  3. Experiment with different values of the similarity top_k parameter and observe its impact on the comprehensiveness and focus of the LLM's responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AMGPT compare to other retrieval-augmented language models specifically trained on additive manufacturing literature?
- Basis in paper: [inferred] The paper evaluates AMGPT against GPT-4 but does not compare it to other domain-specific LLMs or RAG-based systems trained on AM literature.
- Why unresolved: The authors only provide a comparison with GPT-4, which is a general-purpose LLM, and do not explore how AMGPT performs relative to other specialized models in the same domain.
- What evidence would resolve it: Benchmarking AMGPT against other AM-specific LLMs or RAG systems using standardized datasets and evaluation metrics would clarify its relative performance.

### Open Question 2
- Question: What is the optimal corpus size and composition for training AMGPT to achieve the best balance between response accuracy and computational efficiency?
- Basis in paper: [explicit] The paper uses approximately 50 AM papers and textbooks, but does not explore how varying the corpus size or composition affects performance.
- Why unresolved: The authors do not experiment with different corpus sizes or compositions, leaving uncertainty about whether the current dataset is optimal.
- What evidence would resolve it: Systematic experiments varying the number and types of documents (e.g., papers, textbooks, patents) in the corpus, while measuring response accuracy and computational requirements, would identify the optimal configuration.

### Open Question 3
- Question: How does the inclusion of non-textual data (e.g., images, tables, equations) from AM literature impact the performance of AMGPT?
- Basis in paper: [explicit] The paper mentions using Mathpix to convert PDF documents into TeX format for mathematical expressions, but does not explore the integration of other non-textual data.
- Why unresolved: The authors focus on text-based retrieval and generation, without investigating how incorporating images, tables, or other structured data might enhance the model's ability to answer queries.
- What evidence would resolve it: Evaluating AMGPT's performance with and without the inclusion of non-textual data from AM literature, and measuring the impact on response quality and comprehensiveness, would determine the value of such integration.

## Limitations
- Data scope limitation: The system was trained and evaluated on only ~50 papers, representing a small fraction of the additive manufacturing literature.
- Evaluation methodology gaps: The claim of "80% response fidelity without hallucinations" lacks detail about evaluation protocols and criteria.
- Generalizability concerns: Performance is reported specifically for metal additive manufacturing queries without addressing other AM processes.

## Confidence
- High confidence: Technical implementation details of the RAG architecture including Llama2-7B, sentence-transformers embeddings, and dual-encoder retrieval mechanism
- Medium confidence: Claim that RAG enables precise, contextually accurate responses (theoretically sound but limited evaluation data)
- Low confidence: Specific performance claims regarding 80% fidelity and optimal parameter settings (without transparent evaluation protocols)

## Next Checks
1. Conduct a blind evaluation where domain experts assess responses to a standardized set of technical AM queries, comparing AMGPT outputs against both general LLMs and human expert responses.
2. Test the system's performance as the corpus size increases from 50 to 200+ papers to understand how retrieval accuracy and response quality change with larger document collections.
3. Apply the same RAG architecture and parameter settings to a different technical domain using domain-specific literature to validate whether optimal parameter values are truly domain-agnostic.