---
ver: rpa2
title: 'MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large
  Language Models'
arxiv_id: '2401.07598'
source_url: https://arxiv.org/abs/2401.07598
tags:
- performance
- dataset
- quantisation
- finetuned
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates parameter-efficient fine-tuning (PEFT) of multilingual
  large language models (LLMs) using LoRA across six downstream tasks in 40 languages.
  We finetuned Llama-2-7B and Mistral-7B on two multilingual instruction datasets
  and varied LoRA rank and quantization.
---

# MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models

## Quick Facts
- arXiv ID: 2401.07598
- Source URL: https://arxiv.org/abs/2401.07598
- Reference count: 40
- This work evaluates parameter-efficient fine-tuning (PEFT) of multilingual large language models (LLMs) using LoRA across six downstream tasks in 40 languages.

## Executive Summary
This paper presents MAPLE, a comprehensive evaluation of parameter-efficient fine-tuning techniques for multilingual large language models. The authors investigate how LoRA-based PEFT affects model performance across 40 languages on six downstream tasks using Llama-2-7B and Mistral-7B models. Through systematic experimentation with different LoRA ranks and quantization levels, they demonstrate that PEFT can significantly improve performance on low-resource languages while sometimes degrading performance on high-resource languages like English. The study reveals important tradeoffs between model capacity, computational efficiency, and cross-lingual transfer capabilities.

## Method Summary
The authors fine-tuned Llama-2-7B and Mistral-7B models using LoRA with varying ranks (8, 16, 32, 64, 128) and quantization levels (4-bit, 8-bit, 16-bit) on two multilingual instruction datasets covering 40 languages. They evaluated the resulting models on six downstream tasks including classification (XNLI, XCOPA, Belebele), question-answering (MLQA, XQUAD), and summarization (XLSUM), along with English instruction following capability using AlpacaEval. The experiments systematically varied LoRA parameters to understand their effects on multilingual performance and cross-lingual transfer.

## Key Results
- PEFT improves low-resource language performance while sometimes degrading high-resource language performance
- Cross-lingual transfer occurs even with PEFT, and English performance decreases after multilingual finetuning
- Higher LoRA ranks and better quantization preserve English ability while maintaining multilingual downstream task performance
- PEFT on Mistral-7B surpasses GPT-4 on MLQA and XQUAD question-answering tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher LoRA ranks improve performance for low-resource languages by increasing model capacity for adaptation
- Mechanism: Larger rank values in LoRA allow more degrees of freedom for adaptation, helping the model adjust to language-specific patterns without overfitting on small datasets
- Core assumption: Low-resource languages have less data, so more parameters are needed to capture variability
- Evidence anchors:
  - [abstract] "we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and higher quantisation values benefit low-resource languages."
  - [section] "We find that PEFT of smaller open-source models sometimes bridges the gap between the performance of these models and the larger ones, however, English performance can take a hit."
- Break Condition: Rank becomes too high, causing overfitting on small datasets or computational inefficiency

### Mechanism 2
- Claim: Cross-lingual transfer occurs even with PEFT, allowing improvements in low-resource languages
- Mechanism: Even when finetuning on a subset of languages, the shared multilingual representations enable the model to generalize knowledge to unseen languages
- Core assumption: Multilingual LLMs learn shared representations that transfer across languages
- Evidence anchors:
  - [abstract] "Cross-lingual transfer occurs even with PEFT, and English performance decreases after multilingual finetuning."
  - [section] "We observe that language-wise performance does not get affected by the choice of the training dataset, i.e. MULTI ALPACA BACTRIAN -X-11 and BACTRIAN -X-22 have similar performances across languages and tasks."
- Break Condition: Cross-lingual transfer fails when languages are too dissimilar or lack shared linguistic features

### Mechanism 3
- Claim: Higher quantization values help preserve English performance while still benefiting low-resource languages
- Mechanism: Better quantization (e.g., 16-bit vs. 4-bit) retains more information from the base model, which helps maintain English capabilities while adapting to other languages
- Core assumption: Lower quantization loses important model information, hurting performance on the source language
- Evidence anchors:
  - [abstract] "Higher LoRA ranks and better quantization preserve English ability."
  - [section] "Higher capacity adapters (higher rank, better quantisation) are better at maintaining English performance along with multilingual downstream task performance."
- Break Condition: Quantization is too low, causing loss of critical information for both English and low-resource languages

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: LoRA is the PEFT technique used in this work; understanding how it modifies model weights is essential to grasp the results
  - Quick check question: How does LoRA differ from full fine-tuning in terms of trainable parameters and memory usage?

- Concept: Cross-lingual transfer
  - Why needed here: The paper shows that even PEFT can enable transfer learning across languages; understanding this helps interpret the performance gains
  - Quick check question: Why might a model trained on 11 languages perform well on a 12th language it hasn't seen?

- Concept: Quantization (4-bit, 8-bit, 16-bit)
  - Why needed here: The study tests different quantization levels; knowing their impact on model quality is key to understanding the tradeoffs
  - Quick check question: What is the main tradeoff between using 4-bit vs. 16-bit quantization in PEFT?

## Architecture Onboarding

- Component map: Llama-2-7B/Mistral-7B -> LoRA adapters (rank/quantization) -> multilingual instruction datasets -> downstream evaluation tasks

- Critical path:
  1. Load base model (Llama-2-7B or Mistral-7B)
  2. Apply LoRA adapters with specified rank and quantization
  3. Finetune on instruction dataset
  4. Evaluate on multilingual downstream tasks
  5. Compare results across configurations

- Design tradeoffs:
  - Higher rank vs. memory/compute cost
  - Better quantization vs. model capacity
  - More languages in training vs. performance on each

- Failure signatures:
  - English performance drops significantly
  - Low-resource languages don't improve
  - Model overfits on small datasets

- First 3 experiments:
  1. Compare LoRA rank 8 vs. rank 64 on a low-resource language task
  2. Test 4-bit vs. 16-bit quantization impact on English AlpacaEval
  3. Evaluate cross-lingual transfer by testing on a language not in the finetuning set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of parameter-efficient fine-tuning (PEFT) techniques like LoRA compare to full fine-tuning across different language families and linguistic typologies?
- Basis in paper: [explicit] The paper compares PEFT with full fine-tuning but focuses primarily on language resource levels (high vs low) rather than linguistic properties. It notes that PEFT can sometimes outperform full fine-tuning on specific tasks like MLQA and XQUAD.
- Why unresolved: The study evaluates languages but doesn't systematically analyze performance across linguistic families or structural similarities. The authors mention that the quality and abilities of the base model outweigh dataset choices, suggesting potential language-specific factors not explored.
- What evidence would resolve it: A controlled study comparing PEFT performance across languages grouped by linguistic family (e.g., Indo-European, Sino-Tibetan, Afro-Asiatic) while controlling for resource levels, or analyzing correlation between linguistic similarity and cross-lingual transfer efficiency.

### Open Question 2
- Question: What is the optimal strategy for balancing multilingual instruction data sampling to maximize cross-lingual transfer while minimizing catastrophic forgetting of English capabilities?
- Basis in paper: [inferred] The paper finds that multilingual finetuning can degrade English performance and that higher ranks/quantization help preserve English ability. It also shows that having more languages doesn't necessarily improve performance if dataset sizes are comparable.
- Why unresolved: The authors observe trade-offs between multilingual and English performance but don't provide a systematic framework for optimal sampling strategies. They note that BACTRIAN-X-11 (11 languages) performs similarly to MULTI-ALPACA while BACTRIAN-X-22 (22 languages) doesn't show significant improvement.
- What evidence would resolve it: Experiments varying the proportion of English vs multilingual data in instruction datasets, or analyzing performance as a function of the ratio of high-resource to low-resource language instructions in the finetuning data.

### Open Question 3
- Question: How do different tokenization strategies (vocabulary expansion vs shared vocabulary) affect the efficacy of parameter-efficient multilingual instruction tuning?
- Basis in paper: [explicit] The authors mention that "most multilingual LLAMA resort to vocabulary expansion during pre-finetuning phase" and hypothesize that Mistral's better tokenizer contributes to its superior multilingual performance compared to Llama.
- Why unresolved: While the paper compares models with different tokenization approaches, it doesn't isolate the effect of tokenization strategy on PEFT performance. The discussion of "curse of multilinguality" suggests tokenization could be a key factor not fully explored.
- What evidence would resolve it: Comparative studies of PEFT performance on models using shared vocabularies vs vocabulary expansion, or analysis of how tokenization choices interact with rank and quantization parameters in multilingual settings.

## Limitations
- The study evaluates only two base models (Llama-2-7B and Mistral-7B) and one PEFT method (LoRA), limiting generalizability to other architectures and adaptation techniques
- The experiments focus on 40 languages but provide limited analysis of which specific language families benefit most from the observed effects
- The paper does not explore the computational tradeoffs in detail or provide runtime/memory comparisons across different rank and quantization configurations

## Confidence

**Major Claim Clusters Confidence:**

- **High Confidence**: The observation that PEFT improves low-resource language performance while sometimes degrading high-resource language performance is well-supported by the experimental results across multiple tasks and languages. The evidence from comparing different rank and quantization values is robust.

- **Medium Confidence**: The claim that cross-lingual transfer occurs even with PEFT is supported by the results, but the mechanism is not thoroughly investigated. The paper shows that performance is similar across different training datasets, but doesn't deeply analyze which linguistic features enable this transfer.

- **Medium Confidence**: The finding that higher LoRA ranks and better quantization preserve English ability is supported by the data, but the exact thresholds where this effect breaks down are not clearly established. The relationship between rank/quantization and English performance degradation needs more systematic exploration.

## Next Checks

1. **Language Family Analysis**: Conduct a detailed analysis of which language families (e.g., Romance, Germanic, Turkic) benefit most from higher LoRA ranks and better quantization to understand the linguistic factors driving the observed effects.

2. **Overfitting Threshold Testing**: Systematically test higher LoRA ranks (beyond rank 128) on low-resource languages to identify the point where performance gains plateau or begin to degrade due to overfitting.

3. **Cross-Lingual Transfer Mechanism**: Design experiments that isolate specific linguistic features (e.g., word order, morphology) to determine which aspects of language enable cross-lingual transfer with PEFT, potentially using synthetic languages or controlled perturbations.