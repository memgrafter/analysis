---
ver: rpa2
title: Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning
  Shortcuts
arxiv_id: '2403.11314'
source_url: https://arxiv.org/abs/2403.11314
tags:
- reasoning
- spurious
- sip-bart
- rule
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how transformer models can better approximate
  logical reasoning while avoiding spurious correlations in the data. The authors
  augment a propositional logic dataset with proofs and train two models: WP-BART,
  which generates entire proofs, and SIP-BART, a neuro-symbolic model that generates
  individual proof steps combined with a symbolic checker.'
---

# Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts
## Quick Facts
- arXiv ID: 2403.11314
- Source URL: https://arxiv.org/abs/2403.11314
- Authors: Daniel EnstrÃ¶m; Viktor Kjellberg; Moa Johansson
- Reference count: 40
- Key outcome: SIP-BART achieves nearly perfect accuracy on logical reasoning tasks by generating proof steps with symbolic verification, outperforming both baseline BERT classifier and full-proof generation approaches

## Executive Summary
This paper addresses the challenge of spurious correlations in transformer-based reasoning models by proposing a neuro-symbolic approach that generates reasoning steps incrementally and verifies them symbolically. The authors create an augmented propositional logic dataset with proofs and train two models: WP-BART, which generates entire proofs, and SIP-BART, which generates individual proof steps that are checked by a symbolic verifier. The results show that SIP-BART significantly outperforms both the baseline BERT classifier and WP-BART, achieving nearly perfect accuracy across all test sets while avoiding the statistical shortcuts that typically plague transformer reasoning models.

## Method Summary
The authors augment a propositional logic dataset with proofs and train two transformer models: WP-BART generates entire proofs in one pass, while SIP-BART generates individual proof steps that are verified by a symbolic checker. The evaluation compares these approaches against a BERT classifier baseline across various test sets designed to measure reliance on spurious correlations versus genuine logical reasoning. The key innovation is the combination of incremental step generation with symbolic verification, which forces the model to follow valid reasoning paths rather than exploiting statistical patterns in the training data.

## Key Results
- SIP-BART achieves nearly perfect accuracy across all test sets, significantly outperforming both BERT classifier and WP-BART
- The neuro-symbolic approach successfully mitigates reliance on spurious correlations and reasoning shortcuts
- Four types of consistency errors in SIP-BART are identified and proposed mitigation strategies are suggested
- The incremental step generation approach, combined with symbolic verification, substantially improves logical reasoning capabilities

## Why This Works (Mechanism)
The mechanism works by constraining the transformer to follow valid logical inference rules through symbolic verification at each step, rather than allowing it to generate proofs based on statistical patterns learned from the training data. By generating proof steps incrementally and verifying each one, the model cannot rely on superficial correlations between premises and conclusions that would lead to incorrect reasoning.

## Foundational Learning
- Propositional logic and inference rules: Essential for understanding the reasoning tasks and proof structures
- Transformer architectures and attention mechanisms: Needed to comprehend how BART processes logical expressions
- Neuro-symbolic integration: Understanding how neural generation combines with symbolic verification
- Spurious correlations in machine learning: Key concept explaining why models learn shortcuts instead of genuine reasoning
- Proof generation vs. proof step generation: Critical distinction between the two training approaches

## Architecture Onboarding
- Component map: Input -> BART Encoder -> BART Decoder -> Proof Steps -> Symbolic Checker -> Final Proof
- Critical path: The sequence from generating individual proof steps to their verification determines overall system performance
- Design tradeoffs: Full proof generation (faster but prone to shortcuts) vs. step-by-step generation with verification (slower but more accurate)
- Failure signatures: Step generation errors, verification failures, inconsistency in proof construction
- First experiments: 1) Test BERT baseline on simple entailment tasks, 2) Evaluate WP-BART on proof generation without verification, 3) Measure SIP-BART step-by-step generation accuracy

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding scalability to more complex reasoning domains, computational efficiency of the step-by-step approach, and potential applications to natural language inference and commonsense reasoning tasks.

## Limitations
- Evaluation limited to propositional logic domain, may not generalize to more complex reasoning
- Handcrafted proof structures may not reflect naturally occurring logical reasoning patterns
- Focus on relatively small BART-base model leaves questions about scalability to larger architectures
- Symbolic proof checker assumed to be perfect, which may not hold in more complex domains

## Confidence
- High confidence: SIP-BART outperforms WP-BART and BERT on tested logic reasoning tasks
- Medium confidence: SIP-BART avoids spurious patterns in the specific logic domain tested
- Medium confidence: Effectiveness of consistency error mitigation strategies demonstrated qualitatively

## Next Checks
1. Test SIP-BART on natural language inference datasets and commonsense reasoning benchmarks to evaluate generalization beyond propositional logic
2. Conduct ablation studies removing the symbolic proof checker to measure its contribution to performance
3. Scale the experimental setup to larger transformer architectures and measure whether the SIP approach maintains its advantage