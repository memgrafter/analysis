---
ver: rpa2
title: Generative Reward Models
arxiv_id: '2410.12832'
source_url: https://arxiv.org/abs/2410.12832
tags:
- reward
- reasoning
- arxiv
- preference
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GenRM, a unified approach combining RLHF and
  RLAIF for improving reward models. The key idea is to use an LLM as a generative
  reward model, trained via self-supervised fine-tuning on preference data.
---

# Generative Reward Models

## Quick Facts
- arXiv ID: 2410.12832
- Source URL: https://arxiv.org/abs/2410.12832
- Reference count: 20
- One-line primary result: GenRM achieves in-distribution accuracy comparable to Bradley-Terry models while significantly outperforming them on out-of-distribution tasks (10-45% improvement).

## Executive Summary
This paper proposes GenRM, a unified approach combining RLHF and RLAIF for improving reward models. The key innovation is using an LLM as a generative reward model, trained via self-supervised fine-tuning on preference data. Empirically, GenRM matches Bradley-Terry models on in-distribution tasks while significantly outperforming them on out-of-distribution tasks (10-45% improvement). Additionally, GenRM surpasses zero-shot LLM-as-a-judge performance on both in-distribution (9-31%) and out-of-distribution tasks (2-6%).

## Method Summary
GenRM uses an LLM as a generative reward model trained via self-supervised fine-tuning on preference data. The approach employs STaR (Self-Taught Reasoner) for iterative bootstrapping, generating reasoning traces for preference judgments and retaining only those leading to correct judgments. The method includes variants like STaR-SFT (using standard fine-tuning) and STaR-DPO (using Direct Preference Optimization). Majority voting over multiple samples improves accuracy, and CoT-GenRM forces explicit intermediate reasoning before preference judgments. The approach aims to align synthetic preference labels with human judgments while improving generalization to out-of-distribution tasks.

## Key Results
- GenRM matches Bradley-Terry models on in-distribution tasks while outperforming them on out-of-distribution tasks by 10-45%
- GenRM surpasses zero-shot LLM-as-a-judge on both in-distribution (9-31%) and out-of-distribution tasks (2-6%)
- Majority voting at 32 samples consistently improves performance by 1.6-4.9% across datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STaR-DPO achieves better out-of-distribution generalization by self-generating reasoning traces aligned with the model's own capabilities, avoiding distribution mismatch between training and generation.
- Mechanism: During each STaR iteration, the model samples rationales for preference judgments. Only rationales leading to correct judgments are retained for training (bootstrapping). This creates a curriculum where the model learns to produce reasoning traces it can reliably generate and evaluate, improving alignment with its own prediction strengths.
- Core assumption: The model's ability to generate correct rationales correlates with its ability to make correct preference judgments; thus, training on self-generated rationales improves reward modeling accuracy.
- Evidence anchors: [abstract] "GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%)." [section 5.1] "On the other hand, the STaR-DPO model outperforms or matches baselines across all RewardBench categories."
- Break condition: If the model's self-generated rationales are systematically flawed or hallucinatory, the bootstrapping loop could reinforce incorrect reasoning patterns.

### Mechanism 2
- Claim: Majority voting over multiple samples from the generative reward model improves accuracy by smoothing stochastic variability in preference judgments.
- Mechanism: For each pairwise comparison, the model samples the same prompt multiple times (e.g., 32 samples). The final judgment is determined by majority vote across these samples, reducing the impact of sampling noise and improving reliability.
- Core assumption: Preference judgments from the same model on the same input exhibit stochastic variability; aggregating over samples reduces this noise.
- Evidence anchors: [section 5.4] "Our results are shown in Fig. 5. We see that majority voting at 32 improves performance consistently and adds 1.6% accuracy on the UltraFeedback Dataset and 3.8% on RewardBench in that case." [section 5.4] "On UltraInteract majority voting improves performance by 4.6% and 4.9% on RewardBench."
- Break condition: If the model's sampling process is deterministic or the variance in judgments is negligible, majority voting provides no benefit.

### Mechanism 3
- Claim: CoT-GenRM improves performance over GenRM by forcing the model to produce explicit intermediate reasoning, leading to more accurate and defensible preference judgments.
- Mechanism: Instead of directly outputting a preference token, the model first generates a reasoning chain (CoT) before producing the final preference judgment. This intermediate step encourages more structured, step-by-step evaluation rather than relying on a single implicit judgment.
- Core assumption: Explicit reasoning traces lead to more accurate preference judgments than implicit, direct judgments.
- Evidence anchors: [abstract] "The CoT-GenRM approach additionally prompts the model to provide intermediate Chain-of-Thought reasoning ˆI, ˆr ∼ πϕ(I, r|x, y1, y2) before providing the final answer indicator token." [section 5.1] "We see that the zero-shot methods with trained models, we find that both approaches substantially under-perform the Bradley-Terry RM, PairRM and trained GenRM models which all have comparable accuracies, around 73-74%."
- Break condition: If the model's reasoning chains are superficial or do not meaningfully contribute to the final judgment, CoT may add computational overhead without accuracy gains.

## Foundational Learning

- Concept: Bradley-Terry preference modeling
  - Why needed here: The baseline comparison method; understanding its assumptions (point-wise reward estimates, logistic preference distribution) is crucial for appreciating GenRM's generalization advantage.
  - Quick check question: What is the key limitation of Bradley-Terry models when applied to out-of-distribution data?

- Concept: Reinforcement Learning from Human Feedback (RLHF) pipeline
  - Why needed here: GenRM is positioned as a hybrid approach combining RLHF and RLAIF; understanding the standard RLHF stages (SFT, reward modeling, RL) clarifies where GenRM intervenes.
  - Quick check question: Which stage of the RLHF pipeline does GenRM replace or augment?

- Concept: Self-Taught Reasoner (STaR) bootstrapping
  - Why needed here: STaR is the core iterative method used to generate and refine reasoning traces for CoT-GenRM; understanding its rejection sampling and retraining loop is essential.
  - Quick check question: How does STaR decide which generated rationales to keep for training?

## Architecture Onboarding

- Component map: prompt template (user question + two assistant responses) -> LLM inference (with or without CoT) -> preference judgment (A/B) or reasoning trace + judgment
- Critical path: 1. Generate preference judgments (with or without CoT) 2. Filter correct judgments (STaR-SFT) or apply DPO loss (STaR-DPO) 3. Retrain model on filtered/self-generated data 4. Repeat for multiple iterations
- Design tradeoffs:
  - CoT vs. no CoT: CoT improves accuracy but increases inference cost; no CoT is faster but less accurate.
  - STaR-SFT vs. STaR-DPO: SFT is simpler but may underfit; DPO directly optimizes preference alignment but is more complex.
  - Majority voting: Improves accuracy but linearly increases inference cost.
- Failure signatures:
  - Accuracy plateaus or degrades over STaR iterations: indicates bootstrapping loop is reinforcing incorrect patterns.
  - Majority voting provides no improvement: suggests low stochasticity in model outputs.
  - CoT models perform worse than GenRM: suggests reasoning traces are misleading or superficial.
- First 3 experiments:
  1. Train GenRM (no CoT) on UltraFeedback and evaluate on held-out in-distribution data to establish baseline.
  2. Train STaR-DPO with 3 iterations and evaluate on RewardBench to measure out-of-distribution gains.
  3. Compare majority voting at 32 samples vs. single sample inference on UltraInteract to quantify inference-time compute benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GenRM compare to traditional RLHF methods like PPO on complex, multi-turn reasoning tasks?
- Basis in paper: [inferred] The paper mentions that GenRM matches Bradley-Terry models on in-distribution tasks and outperforms them on out-of-distribution tasks, but does not directly compare to PPO on complex reasoning tasks.
- Why unresolved: The paper focuses on comparing GenRM to Bradley-Terry and PairRM models, not directly to PPO.
- What evidence would resolve it: A head-to-head comparison of GenRM and PPO on a benchmark of multi-turn reasoning tasks, measuring both accuracy and computational efficiency.

### Open Question 2
- Question: What is the impact of using different sampling strategies (e.g., temperature, top-p) on the quality of synthetic preference labels generated by GenRM?
- Basis in paper: [explicit] The paper mentions using temperature 1.0 and top-p 0.95 for generation, but does not explore the impact of varying these parameters.
- Why unresolved: The paper does not investigate the sensitivity of GenRM to sampling hyperparameters.
- What evidence would resolve it: An ablation study varying temperature and top-p values, measuring the resulting performance on a held-out dataset.

### Open Question 3
- Question: How does the quality of synthetic preference labels generated by GenRM compare to human-annotated labels on tasks requiring nuanced judgment (e.g., safety-related prompts)?
- Basis in paper: [inferred] The paper shows that GenRM outperforms zero-shot LLM-as-a-judge on safety tasks, suggesting that the synthetic labels are of reasonable quality, but does not directly compare to human labels.
- Why unresolved: The paper does not provide a direct comparison between GenRM-generated and human-annotated labels.
- What evidence would resolve it: A study where both GenRM-generated and human-annotated labels are used to train reward models, and their performance is compared on a held-out dataset.

## Limitations

- Lack of direct ablation studies on relative contributions of STaR bootstrapping, majority voting, and CoT reasoning to performance gains
- Claims about matching Bradley-Terry in-distribution performance need careful interpretation given 9-45% accuracy differences
- Majority voting improvement percentages may be sensitive to exact sampling procedure and prompt engineering details

## Confidence

- **High confidence**: The empirical results showing GenRM's superior out-of-distribution performance are well-supported by reported accuracy numbers across multiple datasets and evaluation benchmarks.
- **Medium confidence**: The mechanism explanation for why STaR bootstrapping improves generalization is plausible but not definitively proven - the paper shows correlation but doesn't establish causation through controlled experiments.
- **Low confidence**: The claim that majority voting consistently improves performance by specific percentages (1.6-4.9%) may be sensitive to the exact sampling procedure and prompt engineering details that aren't fully specified.

## Next Checks

1. Conduct an ablation study isolating the effects of STaR iterations vs. majority voting vs. CoT reasoning on out-of-distribution performance to quantify each mechanism's contribution.
2. Test GenRM on additional out-of-distribution datasets beyond RewardBench to verify the generalization claims are robust across different task domains.
3. Compare the computational cost (inference time, training iterations) against the accuracy improvements to assess the practical trade-offs of the GenRM approach versus simpler alternatives.