---
ver: rpa2
title: '3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less
  Hallucination'
arxiv_id: '2406.05132'
source_url: https://arxiv.org/abs/2406.05132
tags:
- grounding
- object
- language
- dataset
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 3D-GRAND, a large-scale, densely-grounded
  3D-text dataset comprising 40,087 household scenes paired with 6.2 million instructions.
  The dataset supports diverse 3D-text tasks including object reference, scene description,
  and question answering with dense grounding between text phrases and 3D objects.
---

# 3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination

## Quick Facts
- arXiv ID: 2406.05132
- Source URL: https://arxiv.org/abs/2406.05132
- Reference count: 40
- Models trained on 3D-GRAND show 93.34% precision on 3D-POPE hallucination benchmark

## Executive Summary
This paper introduces 3D-GRAND, a large-scale, densely-grounded 3D-text dataset comprising 40,087 household scenes paired with 6.2 million instructions. The dataset supports diverse 3D-text tasks including object reference, scene description, and question answering with dense grounding between text phrases and 3D objects. The authors also propose 3D-POPE, a benchmark for evaluating hallucination in 3D-LLMs through object presence/absence questions. Experiments show that training 3D-LLMs with 3D-GRAND significantly improves grounding accuracy (e.g., 93.34% precision on 3D-POPE) and reduces hallucinations compared to previous methods. Notably, the model trained on synthetic data demonstrates strong zero-shot performance on real-world ScanNet scenes, indicating effective sim-to-real transfer.

## Method Summary
The authors create 3D-GRAND by generating synthetic household scenes using 3D-Front and Structured3D datasets, then applying automated hallucination filters and template augmentation to produce 6.2 million densely-grounded scene-language instructions. They use GPT-4 to generate detailed scene graphs with object categories, centroids, and extents, then create instructions with dense grounding annotations. The model is fine-tuned using LoRA on Llama-2 with DeepSpeed ZeRO-2 and FlashAttention. For evaluation, they introduce 3D-POPE as a hallucination benchmark and test on ScanRefer for grounding accuracy.

## Key Results
- Models trained on 3D-GRAND achieve 93.34% precision on 3D-POPE hallucination benchmark
- Grounding accuracy reaches 47.82% accuracy@0.25IoU and 26.51% accuracy@0.5IoU on ScanRefer
- Zero-shot performance on ScanNet demonstrates effective sim-to-real transfer from synthetic data
- Scaling effect observed: larger datasets lead to better grounding and reduced hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense grounding reduces hallucinations by enforcing object-text consistency during training.
- Mechanism: By associating every noun phrase with a specific 3D object, the model learns to verify language against spatial context, limiting unsupported claims.
- Core assumption: Hallucinations occur when the model generates text without grounding in the scene; enforcing grounding during training reduces this tendency.
- Evidence anchors:
  - [abstract] "instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs"
  - [section] "Our results trained with 3D-GRAND highlight the dataset's effectiveness in enhancing grounding and reducing hallucination for 3D-LLMs"
  - [corpus] Weak: No direct corpus evidence linking dense grounding to hallucination reduction.

### Mechanism 2
- Claim: Large-scale synthetic 3D-text data enables effective sim-to-real transfer for grounding tasks.
- Mechanism: Synthetic scenes provide abundant, diverse training examples that capture generalizable spatial and linguistic patterns, allowing the model to perform well on real-world ScanNet scenes without direct exposure.
- Core assumption: Patterns learned in synthetic environments generalize to real-world 3D scenes.
- Evidence anchors:
  - [abstract] "models trained on large synthetic data can perform well on real-world 3D scans"
  - [section] "models can successfully transfer from sim-to-real, providing an early signal for a low-cost and sustainable future of scaling synthetic 3D data"
  - [corpus] Weak: No corpus evidence confirming generalization to real-world scenes.

### Mechanism 3
- Claim: Scaling densely-grounded data improves both grounding accuracy and hallucination reduction.
- Mechanism: Increasing dataset size provides more examples of correct object-text associations, strengthening the model's ability to ground language and reducing reliance on hallucination.
- Core assumption: More training data leads to better learning of object-text associations and fewer hallucinations.
- Evidence anchors:
  - [abstract] "scaling effect between dataset size and 3D-LLM performance"
  - [section] "Our model hallucinates less when exposed to more data from 3D-GRAND"
  - [corpus] Weak: No corpus evidence linking data scale to hallucination reduction.

## Foundational Learning

- Concept: 3D scene understanding and object detection
  - Why needed here: The model must identify and locate objects in 3D scenes to ground language descriptions accurately.
  - Quick check question: Can you explain how 3D object detection differs from 2D object detection in terms of input and output?

- Concept: Language grounding and spatial reasoning
  - Why needed here: The model needs to connect linguistic descriptions to specific objects and their spatial relationships in the 3D environment.
  - Quick check question: How would you describe the process of grounding a phrase like "the lamp on the table" in a 3D scene?

- Concept: Hallucination in vision-language models
  - Why needed here: Understanding the causes and manifestations of hallucinations is crucial for evaluating and improving model performance.
  - Quick check question: What are some common causes of hallucinations in vision-language models, and how can they be mitigated?

## Architecture Onboarding

- Component map: Pre-trained Llama-2 -> LoRA fine-tuning -> Object-centric context (scene graph) + Text instruction -> Grounded response with object references
- Critical path: Input scene graph → text instruction → model processing → grounded response with object references
- Design tradeoffs: Using synthetic data for scalability vs. potential domain shift; dense grounding for accuracy vs. annotation complexity; LoRA fine-tuning for efficiency vs. potential performance limitations.
- Failure signatures: Poor grounding accuracy on ScanRefer; high hallucination rates on 3D-POPE; failure to generalize from synthetic to real scenes.
- First 3 experiments:
  1. Evaluate grounding accuracy on ScanRefer with different amounts of 3D-GRAND training data.
  2. Measure hallucination rates on 3D-POPE for models trained with and without dense grounding.
  3. Test sim-to-real transfer by evaluating models trained on synthetic data on real ScanNet scenes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of 3D-GRAND models vary across different types of synthetic 3D environments (e.g., structured vs. unstructured layouts)?
- Basis in paper: [inferred] The paper uses both 3D-Front and Structured3D datasets but does not compare model performance across different synthetic environment types.
- Why unresolved: The paper does not provide performance comparisons across different synthetic dataset types or analyze how environmental structure affects model generalization.
- What evidence would resolve it: Systematic evaluation of 3D-GRAND models trained on different synthetic environment types, comparing their performance on both synthetic and real-world test sets.

### Open Question 2
- Question: What is the relationship between the level of detail in synthetic scene graphs and the resulting model's grounding accuracy?
- Basis in paper: [explicit] The paper mentions using scene graphs for annotation but does not explore how varying levels of detail in these graphs affect model performance.
- Why unresolved: The paper does not investigate the impact of scene graph complexity on model performance or explore what level of detail is optimal for training.
- What evidence would resolve it: Controlled experiments varying the richness of scene graph annotations and measuring corresponding changes in model grounding accuracy.

### Open Question 3
- Question: How does the size and diversity of the synthetic training data affect the model's ability to handle rare or unseen objects in real-world scenarios?
- Basis in paper: [inferred] The paper discusses scaling effects but does not specifically analyze how data diversity impacts handling of rare objects.
- Why unresolved: The paper does not provide detailed analysis of how model performance changes with different object distributions in training data or how well models generalize to rare objects.
- What evidence would resolve it: Systematic evaluation of model performance on rare objects as a function of training data size and diversity, including analysis of failure cases.

## Limitations
- Hallucination evaluation relies on binary presence/absence questions, which may not capture more subtle forms of hallucination
- The sim-to-real transfer claim is based on limited real-world scene diversity (primarily ScanNet)
- Annotation quality depends heavily on GPT-4 outputs with only 10% human verification coverage

## Confidence

**High Confidence**: The effectiveness of dense grounding for improving object reference accuracy is well-supported by quantitative results on 3D-POPE and ScanRefer benchmarks.

**Medium Confidence**: The claim that larger datasets reduce hallucinations is supported by trend observations but lacks rigorous ablation studies across multiple dataset sizes.

**Medium Confidence**: The sim-to-real transfer capability is demonstrated through zero-shot ScanNet performance, but the limited diversity of tested real-world scenes leaves generalization uncertainty.

**Low Confidence**: The assertion that this approach provides a "low-cost and sustainable" path to scaling 3D data is not empirically validated against alternative data collection methods.

## Next Checks

1. **Dataset Size Scaling Study**: Systematically train models on 25%, 50%, 75%, and 100% of 3D-GRAND to quantify the relationship between dataset size and hallucination reduction, measuring both grounding accuracy and hallucination rates.

2. **Cross-Dataset Generalization**: Evaluate models trained on 3D-GRAND on multiple real-world 3D scene datasets (beyond ScanNet) including Matterport3D and Gibson to assess robustness of sim-to-real transfer.

3. **Hallucination Granularity Analysis**: Extend 3D-POPE to include graded hallucination detection (partial vs. complete hallucinations) and analyze which types of hallucinations are most effectively reduced by dense grounding.