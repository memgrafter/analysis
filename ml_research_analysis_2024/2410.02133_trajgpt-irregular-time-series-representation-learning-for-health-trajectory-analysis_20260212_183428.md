---
ver: rpa2
title: 'TrajGPT: Irregular Time-Series Representation Learning for Health Trajectory
  Analysis'
arxiv_id: '2410.02133'
source_url: https://arxiv.org/abs/2410.02133
tags:
- trajgpt
- time
- series
- inference
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of modeling irregularly-sampled
  time series in healthcare settings, where observations occur at uneven intervals.
  To tackle this, the authors propose Trajectory Generative Pre-trained Transformer
  (TrajGPT), a novel time-series Transformer that employs a Selective Recurrent Attention
  (SRA) mechanism with data-dependent decay to adaptively filter past information.
---

# TrajGPT: Irregular Time-Series Representation Learning for Health Trajectory Analysis

## Quick Facts
- arXiv ID: 2410.02133
- Source URL: https://arxiv.org/abs/2410.02133
- Authors: Ziyang Song; Qingcheng Lu; He Zhu; David Buckeridge; Yue Li
- Reference count: 35
- Top-10 recall scores of 71.7% for forecasting and 67.2% for insulin usage prediction

## Executive Summary
This paper addresses the challenge of modeling irregularly-sampled time series in healthcare settings, where observations occur at uneven intervals. To tackle this, the authors propose Trajectory Generative Pre-trained Transformer (TrajGPT), a novel time-series Transformer that employs a Selective Recurrent Attention (SRA) mechanism with data-dependent decay to adaptively filter past information. By interpreting TrajGPT as discretized ordinary differential equations, it captures underlying continuous dynamics and enables time-specific inference for forecasting arbitrary timesteps. Experimental results demonstrate that TrajGPT achieves superior performance in trajectory forecasting, drug usage prediction, and phenotype classification, outperforming baselines with top-10 recall scores of 71.7% for forecasting and 67.2% for insulin usage prediction.

## Method Summary
TrajGPT uses a Selective Recurrent Attention (SRA) mechanism that incorporates data-dependent decay to adaptively filter past information based on context. The model interprets SRA updates as discretized ordinary differential equations, enabling it to capture continuous dynamics in irregularly-sampled data. TrajGPT employs time-specific inference for forecasting arbitrary timesteps with constant complexity, avoiding sequential autoregressive generation. The model is pre-trained on EHR data using next-token prediction and evaluated on forecasting, drug usage prediction, and phenotype classification tasks.

## Key Results
- Achieved top-10 recall scores of 71.7% for trajectory forecasting, outperforming baselines by significant margins
- Obtained 67.2% top-10 recall for insulin usage prediction, demonstrating effectiveness in clinical applications
- Successfully predicted unseen diseases based on clinically relevant phenotypes, enabling interpretable health trajectory analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: TrajGPT's Selective Recurrent Attention (SRA) with data-dependent decay can adaptively filter irrelevant past information based on contexts.
- **Mechanism**: The decay term γn is computed as Sigmoid(Xn wγ ⊤) / τ, where wγ encodes task-specific importance. This allows the model to dynamically control how much past information is retained, with higher γn slowing decay for long-term dependencies (e.g., chronic diseases) and lower γn accelerating decay for recent events (e.g., acute conditions).
- **Core assumption**: The context representation in Xn can capture meaningful differences between relevant and irrelevant information.
- **Evidence anchors**:
  - [abstract]: "utilizes a data-dependent decay to adaptively filter out irrelevant past information based on contexts"
  - [section 3.1]: "The data-dependent decay γn ∈ (0, 1] and learnable decay vector wγ ∈ R1×d enable SRA to selectively forget irrelevant past information based on contexts."
- **Break condition**: If the context representation in Xn cannot capture meaningful differences between relevant and irrelevant information, the decay gating will fail to selectively filter.

### Mechanism 2
- **Claim**: Interpreting TrajGPT as discretized ordinary differential equations enables it to capture continuous dynamics in irregularly-sampled data.
- **Mechanism**: The recurrent SRA updates Sn = γn Sn-1 + K⊤n Vn correspond to a discretized ODE with zero-order hold (ZOH) rule. This discretization models continuous temporal evolution even with varying time intervals between observations, allowing both interpolation and extrapolation.
- **Core assumption**: The underlying medical time series can be meaningfully modeled as continuous dynamics that can be discretized with varying step sizes.
- **Evidence anchors**:
  - [abstract]: "By interpreting TrajGPT as discretized ordinary differential equations (ODEs), it effectively captures the underlying continuous dynamics"
  - [section 3.2]: "In this section, we establish theoretical connection between our proposed SRA module and ODE."
- **Break condition**: If the medical phenomena do not follow continuous dynamics or if the discretization introduces unacceptable approximation errors for the time scales involved.

### Mechanism 3
- **Claim**: TrajGPT's time-specific inference can directly predict observations at arbitrary target timesteps with constant complexity O(1).
- **Mechanism**: Instead of autoregressive generation, time-specific inference computes On' = Qn' Sn' where Sn' is updated using the target timestep tn' and previous state Sn with discrete step size ∆tn',n = tn' - tn. This allows direct forecasting without sequential generation.
- **Core assumption**: The learned continuous dynamics from the SRA-ODE interpretation can be accurately evaluated at arbitrary time points without degradation.
- **Evidence anchors**:
  - [abstract]: "enables time-specific inference for forecasting arbitrary target timesteps"
  - [section 3.2]: "At inference time, we explore two strategies for forecasting irregularly-sampled time series: auto-regressive and time-specific inference"
- **Break condition**: If the continuous dynamics learned during training do not generalize well to time points far from training observations.

## Foundational Learning

- **Concept**: Rotary Position Embedding (RoPE) for encoding relative positional information
  - Why needed here: RoPE handles varying time intervals between observations by encoding relative distance tn - tm, which is crucial for irregularly-sampled medical data
  - Quick check question: How does RoPE encode the relative positional information between tokens n and m?

- **Concept**: Linear attention mechanism for computational efficiency
  - Why needed here: Standard transformers have quadratic complexity O(N²) which is prohibitive for long medical sequences; linear attention reduces this to O(N)
  - Quick check question: What is the computational complexity of TrajGPT's SRA compared to standard self-attention?

- **Concept**: Ordinary differential equations and discretization
  - Why needed here: The theoretical connection between SRA and discretized ODEs enables modeling continuous dynamics in irregular time series
  - Quick check question: How does the zero-order hold discretization rule connect the recurrent SRA updates to continuous ODEs?

## Architecture Onboarding

- **Component map**: Token embedding + RoPE -> SRA Layer(s) -> Output projection
- **Critical path**: Irregular time series → Token embedding → RoPE → SRA layers (recurrent/parallel) → Output projection → Next-token prediction
- **Design tradeoffs**:
  - SRA vs standard self-attention: Linear vs quadratic complexity, but potentially less expressive
  - Time-specific vs autoregressive inference: Constant vs linear complexity, but may have accuracy trade-offs
  - Fixed vs data-dependent decay: Simplicity vs adaptability to context
- **Failure signatures**:
  - Poor forecasting performance: Check if decay gating is learning meaningful γn values or if ODE discretization is introducing errors
  - Slow training: Verify linear attention implementation is correct (should be O(N), not O(N²))
  - Overfitting on small datasets: Monitor decay parameters and consider regularization
- **First 3 experiments**:
  1. Verify SRA computes correct cumulative decay matrix D by comparing recurrent and parallel implementations on a simple sequence
  2. Test time-specific inference on synthetic continuous data with known dynamics to validate ODE interpretation
  3. Ablation study: Remove decay gating (fixed γ) and RoPE to measure their individual contributions to forecasting performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SRA mechanism's data-dependent decay compare to attention mechanisms with fixed decay rates in terms of computational efficiency and model interpretability?
- Basis in paper: [explicit] The paper introduces a data-dependent decay in the SRA mechanism to adaptively filter past information based on contexts, contrasting it with data-independent decay used in other models.
- Why unresolved: The paper does not provide a direct comparison of computational efficiency or interpretability between SRA and fixed decay attention mechanisms.
- What evidence would resolve it: A detailed computational analysis comparing the SRA mechanism with fixed decay attention, including training time, inference speed, and interpretability metrics.

### Open Question 2
- Question: Can TrajGPT's trajectory analysis be extended to predict rare diseases or conditions not represented in the training data?
- Basis in paper: [inferred] The paper demonstrates TrajGPT's ability to predict unseen diseases based on clinically relevant phenotypes, but does not explore its effectiveness with rare diseases.
- Why unresolved: The paper does not provide experiments or analysis on rare disease prediction, which could highlight limitations or extensions of the model.
- What evidence would resolve it: Experimental results showing TrajGPT's performance on rare disease prediction tasks, possibly using a dataset with rare disease labels.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the number of heads or the temperature parameter, affect the model's performance in different healthcare applications?
- Basis in paper: [explicit] The paper mentions the use of a temperature parameter and head-specific decay vectors but does not explore the impact of varying these hyperparameters.
- Why unresolved: The paper does not conduct an ablation study on the impact of hyperparameters on model performance across different tasks.
- What evidence would resolve it: A comprehensive ablation study varying hyperparameters like the number of heads and temperature parameter, and analyzing their impact on tasks like trajectory forecasting and drug usage prediction.

## Limitations
- Evaluation relies on a single healthcare dataset (PopHR), limiting generalizability
- Theoretical connection to ODEs is presented but not extensively validated with sensitivity analysis
- Comparison against baselines doesn't include recent continuous-time transformer approaches

## Confidence
- Forecasting Performance Claims: High confidence - Multiple baselines tested with consistent improvements across K values
- SRA Mechanism Effectiveness: Medium confidence - Strong theoretical motivation but limited ablation analysis
- ODE Interpretation Validity: Medium confidence - Mathematical derivation provided but practical implications not thoroughly explored
- Interpretability Claims: Low confidence - Phenotype prediction demonstrated but clinical validation is absent

## Next Checks
1. **Ablation Study**: Implement and compare three variants - (a) TrajGPT with fixed decay (γn = constant), (b) TrajGPT without RoPE encoding, and (c) standard transformer with autoregressive inference on the same dataset to quantify individual component contributions.

2. **Temporal Robustness Test**: Evaluate TrajGPT's forecasting accuracy on sequences with artificially increased time gaps between observations (2x, 5x, 10x original intervals) to validate the ODE interpretation claims about continuous dynamics modeling.

3. **Cross-Dataset Generalization**: Test TrajGPT on at least one additional irregularly-sampled healthcare dataset (e.g., MIMIC-III) to assess whether the performance gains transfer beyond the PopHR dataset, particularly focusing on zero-shot transfer capability.