---
ver: rpa2
title: 'Human Variability vs. Machine Consistency: A Linguistic Analysis of Texts
  Generated by Humans and Large Language Models'
arxiv_id: '2412.03025'
source_url: https://arxiv.org/abs/2412.03025
tags:
- features
- llms
- texts
- humans
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares human-written and machine-generated texts across
  four domains using 250 linguistic features. Results show that human texts are longer,
  richer in vocabulary, and exhibit higher variability in linguistic features compared
  to texts generated by LLMs.
---

# Human Variability vs. Machine Consistency: A Linguistic Analysis of Texts Generated by Humans and Large Language Models

## Quick Facts
- arXiv ID: 2412.03025
- Source URL: https://arxiv.org/abs/2412.03025
- Reference count: 21
- 87% accuracy achieved in distinguishing human vs machine-generated texts using linguistic features

## Executive Summary
This study compares human-written and machine-generated texts across four domains using 250 linguistic features. The analysis reveals that human texts are longer, richer in vocabulary, and exhibit higher variability in linguistic features compared to texts generated by large language models (LLMs). Humans produce less syntactically complex sentences, employ more diverse semantic content, and express higher emotional intensity, particularly negative emotions. A logistic classifier trained on these linguistic features achieves 87% accuracy in distinguishing between human and machine-generated texts.

## Method Summary
The study analyzes texts from the SemEval 2024 Task 8 M4 dataset, which contains aligned human-written and machine-generated texts from five LLMs (ChatGPT, Cohere, GPT-3.5, BLOOMz-176B, Dolly-v2) across four domains (Wikipedia, Wikihow, Arxiv, Reddit). Using the LFTK toolkit, 250 linguistic features were extracted, including syntactic depth, semantic similarity, and emotional content. Statistical tests (Kruskal-Wallis, Dunn's) assessed differences, PCA reduced features for visualization, and logistic regression classified texts. The evaluation protocol followed a same-generator, same-domain approach.

## Key Results
- Human texts are longer, richer in vocabulary, and show higher variability in linguistic features than LLM-generated texts
- Humans produce less syntactically complex sentences and express higher emotional intensity, particularly negative emotions
- A two-dimensional PCA reduction reveals significant variability in human texts across domains, while LLM-generated texts show more consistency
- Logistic classifier achieves 87% accuracy in distinguishing human from machine-generated texts

## Why This Works (Mechanism)

### Mechanism 1
Human-written texts show higher variability in linguistic features than LLM-generated texts, especially in domains with less rigid style constraints. Humans exhibit individual writing styles and adapt language use to context, creating more diverse feature distributions. LLMs, trained on large corpora, generate more consistent outputs.

### Mechanism 2
Humans use less syntactically complex sentences than LLMs, reflecting cognitive load management. Humans prefer simpler syntactic structures to manage cognitive processing demands during writing. LLMs, unconstrained by cognitive limits, generate more complex sentences.

### Mechanism 3
Human texts contain richer emotional content, particularly negative emotions, compared to LLM-generated texts. LLMs may be aligned to reduce negative emotional expressions to appear more helpful and less harmful. Humans naturally express a broader emotional spectrum.

## Foundational Learning

- **Concept: Principal Component Analysis (PCA)**
  - Why needed here: Used to reduce 250 linguistic features to 2 dimensions for visualization and variability analysis
  - Quick check question: What does the first principal component typically capture in PCA?

- **Concept: Logistic Regression Classification**
  - Why needed here: Employed to classify texts as human or machine-generated using interpretable linguistic features
  - Quick check question: What is the difference between logistic regression and other classifiers like random forests?

- **Concept: Kruskal-Wallis Test and Dunn's Test**
  - Why needed here: Used to assess statistical significance of differences between LLMs and humans, and for pairwise comparisons
  - Quick check question: When would you use Kruskal-Wallis instead of ANOVA?

## Architecture Onboarding

- **Component map:**
  Data Ingestion -> Feature Extraction (LFTK + custom features) -> Analysis (statistical tests, PCA) -> Classification (Logistic Regression) -> Evaluation

- **Critical path:**
  1. Load and preprocess dataset
  2. Extract linguistic features using LFTK
  3. Calculate additional features (syntactic depth, semantic distance, emotionality)
  4. Perform statistical analysis
  5. Apply PCA for visualization
  6. Train and evaluate logistic classifier
  7. Analyze feature importance

- **Design tradeoffs:**
  - Using interpretable features vs. black-box models for classification
  - Comprehensive feature set (250 features) vs. computational efficiency
  - Statistical tests for significance vs. potential false positives in multiple comparisons

- **Failure signatures:**
  - High variance in feature extraction could indicate data quality issues
  - Low classification accuracy might suggest feature insufficiency or model limitations
  - Non-significant statistical tests could indicate lack of meaningful differences

- **First 3 experiments:**
  1. Run feature extraction pipeline on a small subset of data to verify LFTK and custom feature calculations
  2. Perform PCA on extracted features to check for data structure and dimensionality reduction effectiveness
  3. Train logistic regression on a small balanced dataset to establish baseline classification performance

## Open Questions the Paper Calls Out

### Open Question 1
How do linguistic features vary across different languages when comparing human-written and machine-generated texts? The paper acknowledges that the study is limited to English language texts and that linguistic features may vary significantly across different languages.

### Open Question 2
How do linguistic patterns in machine-generated texts evolve as LLM technology advances over time? The paper notes that the analysis focuses on a set of LLMs that may already be superseded by more advanced versions, highlighting the rapid pace of advancement in the field.

### Open Question 3
How do individual writing styles and cognitive processes influence the variability of linguistic features in human-written texts across different domains? The paper suggests that human-written texts exhibit greater variability depending on the context and domain.

## Limitations
- Relies on a specific dataset (SemEval 2024 Task 8 M4) that may not generalize to other domains or writing styles
- LFTK toolkit may not capture all relevant linguistic features
- Assumes alignment procedures uniformly reduce negative emotional content across LLMs, which may vary by model and training approach

## Confidence

**High Confidence:** The observation that human texts are longer and richer in vocabulary is well-supported by the statistical analysis and aligns with established linguistic research on human writing patterns.

**Medium Confidence:** The claim about humans producing less syntactically complex sentences is supported but could be domain-dependent. The mechanism assumes cognitive constraints universally influence writing, which may not hold for all contexts.

**Medium Confidence:** The finding that human texts express higher emotional intensity, particularly negative emotions, is plausible but depends on the specific alignment procedures of the LLMs studied. This may vary across different models and training approaches.

## Next Checks
1. Test the findings on additional datasets from different domains to assess generalizability beyond the SemEval 2024 Task 8 M4 dataset
2. Analyze individual LLM outputs separately to determine if the observed patterns hold consistently across all five models or if they vary by model architecture and training approach
3. Examine whether the linguistic differences between human and machine-generated texts change over time as LLM capabilities evolve, particularly in response to alignment updates