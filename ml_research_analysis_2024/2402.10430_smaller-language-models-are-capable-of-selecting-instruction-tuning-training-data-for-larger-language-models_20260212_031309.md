---
ver: rpa2
title: Smaller Language Models are capable of selecting Instruction-Tuning Training
  Data for Larger Language Models
arxiv_id: '2402.10430'
source_url: https://arxiv.org/abs/2402.10430
tags:
- data
- training
- trained
- dataset
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for selecting high-quality
  training data for instruction-tuned language models using smaller models. The core
  idea is to measure sample difficulty from the model's perspective using a "learning
  percentage" metric, which indicates how much learning occurs in earlier epochs.
---

# Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models

## Quick Facts
- arXiv ID: 2402.10430
- Source URL: https://arxiv.org/abs/2402.10430
- Authors: Dheeraj Mekala; Alex Nguyen; Jingbo Shang
- Reference count: 20
- Smaller models can curate high-quality training data for larger models using learning percentage metric

## Executive Summary
This paper presents a novel approach for selecting high-quality training data for instruction-tuned language models using smaller models. The core idea is to measure sample difficulty from the model's perspective using a "learning percentage" metric, which indicates how much learning occurs in earlier epochs. The authors show that even a small 350M model can curate high-quality training data for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset.

## Method Summary
The method uses a learning percentage (LP) metric to measure sample difficulty based on perplexity changes during training. A smaller model computes LP scores across training epochs, then the data is clustered and top-k% most difficult samples are selected from each cluster. The larger model is then trained on this curated subset. The approach includes an approximation LP app that requires only one epoch of training while maintaining effectiveness.

## Key Results
- A 350M model can effectively select challenging samples that improve a 13B model's performance
- Models trained on just 3% of the original data can outperform those trained on the full dataset
- LP app(1) metric provides a computationally efficient approximation of LP(1) while maintaining selection quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models can identify harder samples that improve the performance of larger models.
- Mechanism: The learning percentage (LP) metric measures how much learning occurs in earlier epochs. Higher LP in early epochs indicates easier samples. By selecting samples with low LP(1), we target difficult samples that the model learns less from initially, which are more informative for generalization.
- Core assumption: Harder samples require more epochs to learn and thus have lower LP values in early epochs.
- Evidence anchors:
  - [abstract] "the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model"
  - [section 3] "a higher learning percentage in the early epochs implies easy-to-learn samples"
  - [corpus] Weak - only 5 neighbor papers, none directly address cross-model data hardness transfer
- Break condition: If the learning dynamics differ significantly between model sizes, the hardness ranking would not transfer.

### Mechanism 2
- Claim: Larger models need fewer hard samples to achieve good performance.
- Mechanism: As model capacity increases, the same hard samples provide more learning signal per sample, reducing the required dataset size.
- Core assumption: Model capacity correlates with learning efficiency from difficult samples.
- Evidence anchors:
  - [section 3.1] "as the model's size increases, the amount of challenging data required decreases"
  - [section 3.1] "the OPT-13B model outperforms its full dataset counterpart with only 3% of the training data"
  - [corpus] Weak - no neighbor papers discuss scaling relationships between model size and data requirements
- Break condition: If model scaling does not improve sample efficiency, this mechanism would fail.

### Mechanism 3
- Claim: LP app(1) approximates LP(1) well enough for practical data selection.
- Mechanism: Since language models typically learn most information in the first epoch, LP app(1) computed after one epoch provides a reasonable approximation of LP(1) while requiring only one training run.
- Core assumption: Most learning occurs in the first epoch, making later epoch perplexities relatively constant across samples.
- Evidence anchors:
  - [section 4] "LP app(i) = (Pi-1 - Pi)/P0" vs "LP(i) = (Pi-1 - Pi)/(P0 - Pn)"
  - [section 4.1] "we observe that the model trained on data selected via LP app outperforms its counterpart trained on data selected via LP"
  - [corpus] Weak - no neighbor papers discuss LP approximations or epoch-wise learning dynamics
- Break condition: If learning is not concentrated in the first epoch, LP app(1) would be a poor approximation.

## Foundational Learning

- Concept: Perplexity as a measure of model uncertainty
  - Why needed here: LP metrics use perplexity to measure how much a model learns from each sample
  - Quick check question: If a sample's perplexity drops significantly in epoch 1, is it considered easy or hard according to LP(1)?

- Concept: Transfer learning and model scaling
  - Why needed here: The paper relies on the observation that smaller models can select data that benefits larger models
  - Quick check question: What property of neural networks allows a 350M model to identify samples that help a 13B model?

- Concept: Clustering for diversity preservation
  - Why needed here: The selection method uses k-means clustering to ensure diverse sample selection within each cluster
  - Quick check question: Why might selecting only the hardest samples globally lead to poor performance compared to per-cluster selection?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline (tokenization, clustering)
  - Model training module (for LP computation)
  - LP computation module (perplexity tracking across epochs)
  - Data selection module (ranking and subset creation)
  - Evaluation module (win rate computation)

- Critical path:
  1. Compute LP scores on full dataset
  2. Cluster and select top-k% from each cluster
  3. Train target model on selected subset
  4. Evaluate against baseline

- Design tradeoffs:
  - LP vs LP app: Accuracy vs computational efficiency
  - Cluster size: Balance between diversity and computational cost
  - Selection percentage: Trade-off between data efficiency and performance

- Failure signatures:
  - Win rates consistently below 50%: Data selection not effective
  - Win rates improve with easier samples: LP metric inverted or computed incorrectly
  - No improvement with larger selection percentage: Clustering not preserving diversity

- First 3 experiments:
  1. Compute LP(1) scores on a small dataset and manually verify that high LP corresponds to easier samples
  2. Test LP app(1) vs LP(1) on a small dataset to confirm they produce similar rankings
  3. Run selection with k=10% and k=33% to verify that performance improves with more data but not linearly

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The theoretical foundation for cross-model data hardness transfer is weak, with no direct neighbor papers addressing this phenomenon
- The approximation LP app(1) â‰ˆ LP(1) is asserted without strong theoretical justification or citation support
- Claims about generalizability to much larger models (beyond 13B) are not tested and rely on untested scaling assumptions

## Confidence
- High confidence: Experimental results showing models trained on smaller subsets can match or exceed full-dataset performance are well-documented and reproducible
- Medium confidence: LP-based difficulty ranking effectiveness within tested model sizes (up to 13B parameters) is supported by results but lacks strong theoretical grounding
- Low confidence: Claims that this approach generalizes to much larger models or different instruction-tuning paradigms are not tested

## Next Checks
1. Test whether the LP-based selection method transfers across different instruction-tuning datasets to verify robustness of the data hardness transfer mechanism
2. Evaluate whether the approach works for different model architectures to test generality of the perplexity-based difficulty metric
3. Measure whether LP rankings remain stable when computed with different random seeds and training schedules to assess reliability across training conditions