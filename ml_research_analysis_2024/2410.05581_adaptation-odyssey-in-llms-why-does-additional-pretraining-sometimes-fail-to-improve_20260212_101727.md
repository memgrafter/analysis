---
ver: rpa2
title: 'Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail
  to Improve?'
arxiv_id: '2410.05581'
source_url: https://arxiv.org/abs/2410.05581
tags:
- domain
- domains
- adaptation
- perplexity
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effects of additional pretraining (adaptation)
  on large language models (LLMs) using the M2D2 dataset, which contains over 200
  text domains from Wikipedia and Semantic Scholar. The research focuses on decoder-only
  GPT2 models of various sizes, OLMo-1B, and LLaMA-7B, examining how adapting these
  models to specific domains impacts their perplexity on test data from the same domains.
---

# Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?

## Quick Facts
- arXiv ID: 2410.05581
- Source URL: https://arxiv.org/abs/2410.05581
- Reference count: 40
- Additional pretraining can degrade perplexity, particularly when the additional training domain is similar to the original pretraining corpus

## Executive Summary
This study investigates why additional pretraining (adaptation) sometimes fails to improve large language models. Using the M2D2 dataset containing over 200 text domains, the researchers examined decoder-only GPT2 models, OLMo-1B, and LLaMA-7B. They found that adaptation to Wikipedia domains often increased perplexity while adaptation to Semantic Scholar domains consistently improved it. The degradation was positively correlated with similarity between the additional training domain and original pretraining corpus, and was primarily caused by a small number of tokens unrelated to the domain, such as special characters like "\n".

## Method Summary
The researchers used the M2D2 dataset to study the effects of additional pretraining on LLMs across different domain combinations. They tested decoder-only GPT2 models of various sizes, OLMo-1B, and LLaMA-7B, measuring perplexity on test data from adapted domains. The study systematically examined how adapting models to specific domains impacts performance, comparing results across Wikipedia and Semantic Scholar domains as pretraining sources. They analyzed domain similarity effects and token-level contributions to perplexity changes.

## Key Results
- Adaptation to Wikipedia domains often increased perplexity, while adaptation to Semantic Scholar domains consistently improved it
- Perplexity degradation is positively correlated with similarity between the additional training domain and original pretraining corpus
- Degradation is primarily caused by a small number of tokens unrelated to the domain, such as special characters like "\n"
- Larger models appear to be less susceptible to perplexity degradation through adaptation

## Why This Works (Mechanism)
The mechanism behind perplexity degradation in adaptation appears to be related to domain similarity and token-level effects. When additional training data is similar to the original pretraining corpus, the model may overfit to domain-specific patterns that don't generalize well to the test set. The analysis revealed that degradation is primarily caused by a small number of tokens unrelated to the domain, suggesting that the model's attention mechanisms and token representations become biased toward these specific tokens during adaptation. This token-level interference disrupts the model's ability to handle the broader distribution of test data effectively.

## Foundational Learning
- **Perplexity**: A measurement of how well a probability model predicts a sample. Lower perplexity indicates better predictive performance. Why needed: Core metric for evaluating language model quality after adaptation.
- **Domain similarity**: Quantitative measure of how similar two text domains are in terms of vocabulary, style, and content. Why needed: Key factor determining whether adaptation will help or hurt performance.
- **Token-level analysis**: Examination of how individual tokens contribute to overall model performance. Why needed: Reveals that degradation is caused by specific tokens rather than wholesale model failure.
- **Adaptation vs. pretraining**: Distinction between initial model training and subsequent domain-specific training. Why needed: Framework for understanding when additional training helps versus harms performance.

## Architecture Onboarding
- **Component map**: Tokenizer -> Embedding layer -> Transformer blocks (self-attention + feed-forward) -> Output layer
- **Critical path**: Input tokens → Token embeddings → Multi-head attention → Feed-forward network → Output logits → Probability distribution
- **Design tradeoffs**: Model size vs. adaptation susceptibility, domain similarity vs. performance gain, token-level vs. global optimization
- **Failure signatures**: Increased perplexity on test data, degradation correlated with domain similarity, token-specific performance drops
- **3 first experiments**:
  1. Test adaptation on highly dissimilar domains to verify improvement trend
  2. Remove or mask specific problematic tokens to measure impact on perplexity
  3. Compare adaptation effects across different model sizes to validate scaling trends

## Open Questions the Paper Calls Out
None

## Limitations
- Limited domain diversity - only examines Wikipedia and Semantic Scholar as pretraining sources
- Token-specific analysis may be tokenizer-dependent and needs verification across different preprocessing methods
- Model size constraints - findings may not scale to larger models beyond 7B parameters

## Confidence
- High Confidence: Adaptation to Wikipedia domains can degrade perplexity while adaptation to Semantic Scholar domains improves it
- Medium Confidence: Domain similarity is positively correlated with perplexity degradation
- Low Confidence: Larger models are less susceptible to perplexity degradation

## Next Checks
1. Test the domain similarity hypothesis with additional domain pairs beyond Wikipedia and Semantic Scholar
2. Verify token-specific degradation findings using different tokenizers and preprocessing methods
3. Conduct experiments with larger models (13B-70B parameters) to validate scaling trends