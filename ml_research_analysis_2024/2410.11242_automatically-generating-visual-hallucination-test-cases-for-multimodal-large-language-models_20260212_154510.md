---
ver: rpa2
title: Automatically Generating Visual Hallucination Test Cases for Multimodal Large
  Language Models
arxiv_id: '2410.11242'
source_url: https://arxiv.org/abs/2410.11242
tags:
- test
- accuracy
- cases
- symmetric
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of visual hallucination (VH) in multimodal
  large language models (MLLMs), where models generate incorrect visual details in
  their responses. The authors propose VHExpansion, an automated framework that expands
  VH test cases by negating questions and answers, and applying common and adversarial
  image perturbations.
---

# Automatically Generating Visual Hallucination Test Cases for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2410.11242
- Source URL: https://arxiv.org/abs/2410.11242
- Reference count: 40
- Key outcome: VHExpansion framework expands VH test cases through negation and perturbations, achieving improved VH mitigation through fine-tuning while maintaining general VQA performance

## Executive Summary
This paper addresses visual hallucination (VH) in multimodal large language models (MLLMs), where models generate incorrect visual details in their responses. The authors propose VHExpansion, an automated framework that expands VH test cases by negating questions and answers, and applying common and adversarial image perturbations. They introduce symmetric accuracy as a new unbiased evaluation metric for VH that remains unaffected by class imbalance. Experiments on three VH datasets show that VHExpansion effectively identifies more VH test cases, and fine-tuning MLLMs on expanded VH datasets generated by VHExpansion mitigates VH more effectively than fine-tuning on manually annotated datasets while maintaining general VQA performance.

## Method Summary
The paper introduces VHExpansion, a framework that automatically generates VH test cases by applying three types of transformations to existing test cases: negation of questions/answers using an LLM, common image perturbations (color, scale, rotation, translation, blur, noise), and adversarial perturbations optimized to maximize the difference in visual embeddings. The framework uses symmetric accuracy as an evaluation metric, which measures the proportion of correctly answered VH test-case pairs (original and negated) and is theoretically proven to be unbiased against class imbalance. The method also includes fine-tuning MLLMs on expanded VH datasets to mitigate VH while maintaining general VQA performance.

## Key Results
- VHExpansion successfully expands VH test cases, with adversarial perturbations generating more effective test cases than common perturbations
- Symmetric accuracy is an unbiased evaluation metric that remains unaffected by class imbalance when measuring MLLM vulnerability to VH
- Fine-tuning LLaVA-1.5 on expanded VH test cases achieved symmetric accuracy of 0.711 on POPE dataset compared to 0.180 before fine-tuning
- VHExpansion maintains general VQA performance while significantly improving VH mitigation compared to fine-tuning on manually annotated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetric accuracy is an unbiased evaluation metric for measuring MLLM vulnerability to VH
- Mechanism: It compares performance on original and negated VH test cases, canceling out class imbalance bias
- Core assumption: Random guessing should yield the same performance regardless of class imbalance
- Evidence anchors: Theoretical analysis showing symmetric accuracy remains unaffected by class imbalance when p ≠ 1/2
- Break condition: If negation is incorrectly generated, or if the MLLM's behavior is not random guessing

### Mechanism 2
- Claim: Adversarial perturbations generate more effective VH test cases than common perturbations
- Mechanism: Adversarial perturbations are optimized to maximize the difference in visual embeddings, causing MLLMs to hallucinate
- Core assumption: Small, targeted perturbations can significantly alter model behavior while remaining imperceptible
- Evidence anchors: VHExpansion uses both common and adversarial perturbations, with adversarial being more effective at triggering VH
- Break condition: If the model becomes robust to the specific perturbation method, or if perturbations become too large to remain imperceptible

### Mechanism 3
- Claim: Fine-tuning on expanded VH test cases mitigates VH while maintaining general VQA performance
- Mechanism: Expanded test cases expose the model to more diverse hallucinatory scenarios, improving its ability to distinguish real from hallucinated content
- Core assumption: Additional training data representing VH scenarios will improve model robustness
- Evidence anchors: Fine-tuning on expanded VH test cases significantly improves symmetric accuracy across three VH datasets
- Break condition: If fine-tuning causes catastrophic forgetting of general VQA capabilities, or if expanded test cases don't represent real-world VH scenarios

## Foundational Learning

- Concept: Visual hallucination in multimodal models
  - Why needed here: Understanding what VH is and why it matters is fundamental to grasping the paper's contribution
  - Quick check question: What distinguishes visual hallucination from other types of model errors?

- Concept: Evaluation metrics and their biases
  - Why needed here: The paper's core contribution is a new unbiased metric, so understanding evaluation metric limitations is crucial
  - Quick check question: Why might standard accuracy be misleading when evaluating MLLMs for VH?

- Concept: Adversarial example generation
  - Why needed here: The paper uses adversarial perturbations to generate test cases, so understanding this concept is essential
  - Quick check question: What makes an adversarial example effective at triggering model failures?

## Architecture Onboarding

- Component map: VHExpansion → negation module + common perturbation module + adversarial perturbation module → expanded test cases → evaluation with symmetric accuracy
- Critical path: Initial VH test case → negation → common perturbations → adversarial perturbations → expanded test set → fine-tuning → improved MLLM
- Design tradeoffs: Automated vs manual test case generation (automation trades human expertise for scalability), common vs adversarial perturbations (common is more realistic but less effective, adversarial is more effective but less realistic)
- Failure signatures: High symmetric accuracy but low standard accuracy (suggests class imbalance), poor performance on adversarial perturbations (suggests model vulnerability), fine-tuning degradation on general VQA (suggests overfitting to VH)
- First 3 experiments:
  1. Implement negation module and verify it correctly negates sample questions
  2. Apply common perturbations and measure their effect on symmetric accuracy
  3. Implement adversarial perturbation generation and compare effectiveness against common perturbations

## Open Questions the Paper Calls Out

- Question: How does VHExpansion's performance scale with larger datasets and more complex visual scenes?
  - Basis in paper: The paper demonstrates VHExpansion's effectiveness on three existing VH datasets but does not explore its scalability to larger or more complex datasets
  - Why unresolved: The paper's experiments focus on existing datasets, which may not fully represent the diversity and complexity of real-world visual data
  - What evidence would resolve it: Experiments applying VHExpansion to significantly larger and more complex datasets, such as COCO or Visual Genome, and evaluating the resulting VH test cases

- Question: How does the performance of VHExpansion compare to other automated methods for generating VH test cases, such as those based on reinforcement learning or generative adversarial networks (GANs)?
  - Basis in paper: The paper introduces VHExpansion as the first automated method for generating VH test cases but does not compare it to other automated approaches
  - Why unresolved: The paper's focus is on introducing VHExpansion and its evaluation metric, leaving a comparison with other methods for future work
  - What evidence would resolve it: Experiments comparing VHExpansion's performance to other automated methods for generating VH test cases, using the same datasets and evaluation metrics

- Question: How does the choice of LLM for negation impact the quality and effectiveness of VHExpansion's generated test cases?
  - Basis in paper: The paper uses GPT-4o for negation but does not explore the impact of different LLMs on the quality of generated test cases
  - Why unresolved: The paper assumes GPT-4o's state-of-the-art performance but does not investigate whether other LLMs might perform better or worse in this specific task
  - What evidence would resolve it: Experiments using different LLMs for negation and evaluating the resulting VH test cases' quality and effectiveness in triggering VH in MLLMs

## Limitations
- The effectiveness of VHExpansion depends heavily on the quality of negation generation, which uses LLM-based methods with unspecified prompt details
- While adversarial perturbations are shown to be more effective than common perturbations, they may be less representative of real-world VH triggers
- Empirical validation is limited to three datasets and seven MLLMs, which may not generalize to all VH scenarios or model architectures

## Confidence
- **High**: VHExpansion successfully expands test cases and improves symmetric accuracy through fine-tuning on expanded datasets
- **Medium**: Symmetric accuracy is an unbiased metric for VH evaluation across all conditions
- **Medium**: Adversarial perturbations consistently outperform common perturbations in generating effective VH test cases

## Next Checks
1. **Validate negation quality**: Manually evaluate a sample of negated questions to verify semantic preservation and correct negation of VH content
2. **Test generalization**: Apply VHExpansion to additional VH datasets and MLLM architectures not included in the original evaluation to assess robustness
3. **Analyze perturbation realism**: Compare the visual similarity between original and adversarially perturbed images using human evaluation to quantify perceptual impact