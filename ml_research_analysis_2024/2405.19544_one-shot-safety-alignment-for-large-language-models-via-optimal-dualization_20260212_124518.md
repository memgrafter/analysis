---
ver: rpa2
title: One-Shot Safety Alignment for Large Language Models via Optimal Dualization
arxiv_id: '2405.19544'
source_url: https://arxiv.org/abs/2405.19544
tags:
- safety
- dual
- reward
- alignment
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  (LLMs) with safety constraints while maintaining their helpfulness. The authors
  propose a novel approach called Constrained Alignment via dualizatioN (CAN) that
  reduces constrained alignment to an equivalent unconstrained problem by pre-optimizing
  a smooth and convex dual function with a closed-form expression.
---

# One-Shot Safety Alignment for Large Language Models via Optimal Dualization

## Quick Facts
- **arXiv ID:** 2405.19544
- **Source URL:** https://arxiv.org/abs/2405.19544
- **Reference count:** 40
- **Key outcome:** Novel CAN framework achieves better Pareto trade-offs between safety and helpfulness than existing methods while requiring only moderate amounts of offline data

## Executive Summary
This paper introduces CAN (Constrained Alignment via dualizatioN), a one-shot approach for aligning large language models with safety constraints. Unlike existing methods that require iterative primal-dual optimization, CAN pre-optimizes a smooth and convex dual function with a closed-form expression, eliminating the need for iterative updates. The framework is instantiated in two practical algorithms: MOCAN (model-based) for scenarios with pre-trained reward and safety models, and PECAN (preference-based) for scenarios relying on human-annotated preference data.

## Method Summary
CAN addresses constrained alignment by reducing it to an equivalent unconstrained problem through pre-optimizing a dual function with closed-form expression. The framework leverages Donsker-Varadhan variational formula to obtain this expression, enabling one-shot optimization of dual variables. MOCAN uses pre-trained reward and safety models to generate pseudo-preferences for supervised learning, while PECAN uses human-annotated preference data with a two-stage pre-alignment process. Both methods achieve stable LM updates without reinforcement learning, significantly reducing computational cost while maintaining or improving safety-helpfulness trade-offs.

## Key Results
- MOCAN and PECAN effectively improve both safety and helpfulness of LLMs, achieving better Pareto trade-offs than existing methods
- The dual optimization in MOCAN accurately predicts safety improvements of practically aligned models
- The method requires only moderate amounts of offline data for effective alignment, avoiding the need for large-scale preference annotations
- CAN demonstrates superior stability compared to iterative primal-dual optimization approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual function has a closed-form expression that enables one-shot optimization
- Mechanism: By applying Donsker-Varadhan variational formula, the dual function D(λ) can be expressed as β times the log expectation of an exponential term over the reference policy distribution
- Core assumption: The reward and safety models are bounded and the reference policy can generate responses for prompts
- Evidence anchors:
  - [abstract] "by pre-optimizing a smooth and convex dual function that has a closed form"
  - [section 3.1] "an application of Donsker and Varadhan's variational formula yields a closed-form expression for the dual function"
  - [corpus] Weak - corpus neighbors focus on constrained alignment but don't explicitly discuss closed-form dual functions
- Break condition: If reward/safety models are unbounded or reference policy cannot generate responses

### Mechanism 2
- Claim: Optimal dual variables enable reduction of constrained alignment to unconstrained alignment
- Mechanism: Once optimal dual variables λ⋆ are found by minimizing D(λ), the constrained problem becomes equivalent to maximizing L(π, λ⋆), which is unconstrained
- Core assumption: Strong duality holds and the optimal policy π⋆ maximizes the Lagrangian at λ⋆
- Evidence anchors:
  - [abstract] "reduces constrained alignment to an equivalent unconstrained alignment problem"
  - [section 3.2] "once λ⋆ is well approximated, the second stage is an unconstrained alignment task"
  - [corpus] Moderate - several papers discuss Lagrangian-based methods but don't explicitly show this reduction
- Break condition: If strong duality doesn't hold or the optimal policy doesn't maximize the Lagrangian at λ⋆

### Mechanism 3
- Claim: Pseudo-preference optimization enables stable LM updates without RL
- Mechanism: Using the optimal reward rλ⋆ = r + ⟨λ⋆, g⟩, construct synthetic preferences via Bradley-Terry model and train via supervised learning (DPO-style) instead of RL
- Core assumption: The Bradley-Terry model approximates human preferences and large enough synthetic dataset exists
- Evidence anchors:
  - [section 4.1] "train the LM supervised with pseudo-preferences, constructed with the modified reward rλ⋆"
  - [section 4.1] "leverage the approximate equivalence between RL and supervised training with carefully defined loss functions"
  - [corpus] Strong - several papers discuss preference-based optimization and Bradley-Terry models
- Break condition: If synthetic preferences poorly approximate human preferences or dataset is too small

## Foundational Learning

- Concept: Lagrangian duality and saddle point theory
  - Why needed here: Understanding why optimal dual variables enable reduction of constrained to unconstrained problems
  - Quick check question: Why does finding optimal dual variables λ⋆ allow us to convert a constrained optimization problem into an unconstrained one?

- Concept: Donsker-Varadhan variational formula
  - Why needed here: This formula provides the closed-form expression for the dual function that makes one-shot optimization possible
  - Quick check question: What does the Donsker-Varadhan formula tell us about the relationship between log-expectation and KL divergence?

- Concept: Bradley-Terry preference model
  - Why needed here: This model is used both for reward modeling and constructing synthetic preferences for LM training
  - Quick check question: How does the Bradley-Terry model connect reward differences to preference probabilities?

## Architecture Onboarding

- Component map: Dual optimization module -> Pseudo-preference generator -> Supervised training pipeline -> Evaluation module
- Critical path: 1. Collect offline data with reference policy responses and reward/safety scores; 2. Optimize dual function to find λ⋆; 3. Generate pseudo-preferences using rλ⋆; 4. Train LM using supervised learning with pseudo-preferences; 5. Evaluate aligned LM performance
- Design tradeoffs:
  - One-shot vs iterative: One-shot is faster but requires accurate dual function optimization
  - Model-based vs preference-based: Model-based uses existing reward/safety models, preference-based relies on human annotations
  - Synthetic vs real preferences: Synthetic is scalable but may not perfectly capture human preferences
- Failure signatures:
  - Dual optimization diverges: Check boundedness of reward/safety models
  - Aligned LM doesn't satisfy constraints: Verify dual optimization converged to true optimum
  - Safety degrades after alignment: Check if pseudo-preferences poorly represent human preferences
- First 3 experiments:
  1. Verify dual function properties (convexity, smoothness) on small synthetic dataset
  2. Test MOCAN on single constraint with known optimal solution
  3. Compare MOCAN vs PECAN on small dataset with both methods available

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CAN framework perform when extended to multiple safety constraints simultaneously?
- Basis in paper: [inferred] The authors mention that their experiments are limited to a single safety constraint due to lack of suitable datasets, and suggest this as future work.
- Why unresolved: The paper only tests CAN with one safety constraint. Multi-constraint scenarios may introduce new optimization challenges or trade-off dynamics not captured in single-constraint settings.
- What evidence would resolve it: Experiments applying CAN to datasets with multiple safety annotations, measuring performance across all constraints simultaneously.

### Open Question 2
- Question: Can the CAN framework be adapted to work with more general preference models beyond Bradley-Terry?
- Basis in paper: [explicit] The authors state "Given the use of the Bradley-Terry preference setup, it is important to extend our two-stage strategy to accommodate more general preference setups."
- Why unresolved: The theoretical development and practical implementations are all built on Bradley-Terry assumptions. Generalizing to other preference models would require different optimization formulations.
- What evidence would resolve it: Mathematical extensions of CAN to other preference frameworks, followed by empirical validation showing comparable performance.

### Open Question 3
- Question: How does CAN's performance scale with model size compared to existing constrained RLHF methods?
- Basis in paper: [inferred] The authors mention computational constraints prevented testing on larger models, but believe CAN "should scale seamlessly to larger models."
- Why unresolved: All experiments were conducted on a 7B model. The computational advantages claimed for CAN may diminish or change characteristics at larger scales.
- What evidence would resolve it: Head-to-head scaling experiments comparing CAN against existing methods (Safe-RLHF, Constrained DPO) on models ranging from 7B to 70B+ parameters, measuring both performance and computational efficiency.

## Limitations

- Scalability to extremely large models remains untested, though the authors claim CAN should scale seamlessly
- The closed-form dual expression assumes bounded reward/safety models, which may not hold in practice
- Performance depends on quality of offline data collection and the ability of reference policy to generate diverse responses

## Confidence

**High**: The fundamental reduction of constrained to unconstrained alignment via optimal dual variables (Mechanism 2)

**Medium**: The practical effectiveness of MOCAN and PECAN in improving safety-helpfulness trade-offs (empirical results depend on dataset quality)

**Medium**: The stability advantage over iterative primal-dual methods (limited comparison to alternative approaches)

## Next Checks

1. Test CAN on larger model scales (e.g., 30B+ parameters) to verify computational advantages persist
2. Conduct ablation studies removing the boundedness assumption to measure sensitivity to reward/safety model characteristics
3. Compare against a broader range of constrained alignment methods including iterative primal-dual approaches on identical datasets