---
ver: rpa2
title: 'From Multiple-Choice to Extractive QA: A Case Study for English and Arabic'
arxiv_id: '2404.17342'
source_url: https://arxiv.org/abs/2404.17342
tags:
- answer
- span
- question
- dataset
- belebele
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We explored the feasibility of repurposing a multiple-choice question
  answering dataset for extractive question answering. We developed annotation guidelines
  and manually annotated answer spans in the English and Modern Standard Arabic portions
  of the Belebele dataset, excluding questions unsuitable for the new task.
---

# From Multiple-Choice to Extractive QA: A Case Study for English and Arabic
## Quick Facts
- arXiv ID: 2404.17342
- Source URL: https://arxiv.org/abs/2404.17342
- Reference count: 28
- Primary result: Manual annotation converted MCQA to EQA format with IAA γ=0.84; performance below SQuAD, especially for dialectal Arabic

## Executive Summary
This paper explores converting a multiple-choice question answering dataset (BELEBELE) to extractive QA format for English and Modern Standard Arabic. Through manual annotation guidelines and semi-automatic projection methods, the authors created a parallel EQA dataset while identifying challenges in MCQA-to-EQA conversion. Evaluation using a multilingual QA model showed performance below SQuAD benchmarks, with notable difficulties for dialectal Arabic questions, suggesting the dataset poses realistic cross-lingual QA challenges.

## Method Summary
The authors filtered BELEBELE to remove unsuitable MCQA pairs (e.g., "which of the following" questions), leaving ~415 QA pairs. They manually annotated English answer spans using detailed guidelines, marking unanswerable questions as "X" and complex reasoning questions as "BB". For MSA, they projected EN spans using translation and sliding-window n-gram matching, then reviewed by native speakers. Evaluation used a pretrained XLM-R model (NQ+TyDi+SQuAD) on F1/EM metrics, with normalization variants for stopwords.

## Key Results
- Manual annotation achieved IAA γ=0.84 for span alignment
- Belebele-EQA performance: 71.1 F1 (vs SQuAD's ~90), 50.0 EM (vs ~80)
- Dialectal Arabic questions showed 18% improvement when translated to passage language
- Normalization reduced EN references by 30% but only MSA by 10%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manual annotation is more reliable than automated extraction for MCQs with paraphrases, coreference, and world knowledge.
- Mechanism: Annotators skip MCQs requiring reasoning beyond exact text spans and label them "BB" or "X", ensuring dataset consistency.
- Core assumption: Not all MCQs are suitable for extractive QA; human judgment improves span selection.
- Evidence anchors:
  - [section] "The simplest MCQ types are perhaps Exact Match, Partial Overlap Match and Morphological Match… Relatively more complex are cases that may be resolvable through Paraphrasing, Entailment, or Semantic Similarity… Next comes the cases that require higher levels of awareness of context… Finally we present examples of complex kinds of MCQ… These challenges made it apparent that (a) not all MCQAs are usable for the EQA task; and (b) that human annotation was a more realistic approach…"
  - [abstract] "Evaluation using a multilingual QA model showed performance below that on SQuAD, with lower results for dialectal Arabic questions compared to MSA and English, suggesting the dataset poses realistic challenges for cross-lingual QA."
- Break condition: If span quality degrades across annotators, IAA score falls below 0.7.

### Mechanism 2
- Claim: Semi-automatic span projection from English to MSA preserves alignment while reducing manual effort.
- Mechanism: Translate EN span → find MSA span with same length and highest n-gram overlap; review by native MSA speakers.
- Core assumption: Parallel passages contain equivalent content; n-gram overlap indicates semantic alignment.
- Evidence anchors:
  - [section] "For the MSA data, we exploited the parallel nature of the dataset to project pre-annotations from the EN passages to the MSA passages as part of a bootstrapping approach… Using a sliding-window approach to identify the MSA span with the same length and highest n-gram overlap with the EN span."
  - [corpus] No direct citation but follows common MT evaluation practice; assumes availability of reliable MT for EN→MSA.
- Break condition: If MSA translation fails to preserve core answer content, manual correction cost rises.

### Mechanism 3
- Claim: Cross-lingual QA models benefit from stopword normalization and dialect-to-target translation.
- Mechanism: Normalize by removing stopwords; translate dialect questions into passage language before inference.
- Core assumption: Word matching is sensitive to stopwords; dialectal vocabulary differs significantly from MSA/EN.
- Evidence anchors:
  - [section] "Normalized F1 and EM The normalization results are generally consistent in rank order; but the effect of normalization in EN is much higher than MSA… We note that the normalization process reduces the length of references and predictions by 30% for EN, but only 10% for MSA."
  - [section] "Automatically translating dialectal questions into the target passage language using Google Translate results in notable improvements: an average increase of 18% for English passages and 4.4% for MSA passages…"
- Break condition: If translation introduces noise, performance drops more than baseline.

## Foundational Learning

- Concept: Span annotation boundaries in morphologically rich vs. whitespace-delimited languages.
  - Why needed here: Arabic spans may include clitics; EN spans follow word boundaries.
  - Quick check question: Does including prepositions in MSA spans risk mismatched alignment with EN spans?

- Concept: IAA measures for span overlap.
  - Why needed here: Gamma (γ) aligns multi-annotator spans; standard F1 misreports partial overlaps.
  - Quick check question: What is the minimum γ score indicating reliable guidelines?

- Concept: Parallel corpus bootstrapping.
  - Why needed here: Enables scaling annotation from one language to 120+ Belebele variants.
  - Quick check question: What MT quality threshold is needed before projecting EN spans to another language?

## Architecture Onboarding

- Component map: Belebele-EQA dataset → PrimeQA toolkit → XLM-R based model (NQ+TyDi+SQuAD) → evaluation (F1/EM/F1n/EMn)
- Critical path: Annotate EN spans → project to MSA → run inference → compute metrics → analyze dialect performance
- Design tradeoffs: Manual vs. automatic span selection (accuracy vs. scalability); stopword removal vs. context loss; dialect translation vs. model robustness
- Failure signatures: Low γ (IAA), large F1 gap vs. SQuAD, dialect-specific performance drop, inconsistent BB/X labels
- First 3 experiments:
  1. Run PrimeQA on SQuAD validation set to confirm baseline F1/EM (~90/80)
  2. Run PrimeQA on Belebele-EQA All subset to establish initial performance (~71/50)
  3. Translate DA questions to EN and rerun to measure gain (~+18% F1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific characteristics of MCQs that make them unsuitable for extractive QA conversion, and how can these be systematically identified and filtered?
- Basis in paper: [explicit] Section 3.2 discusses the various MCQ types and their challenges for EQA conversion, including Exact Match, Paraphrasing, Entailment, Coreference, and Pragmatic Knowledge requirements.
- Why unresolved: While the paper identifies some unsuitable MCQ types, it doesn't provide a comprehensive framework for automatically detecting and filtering all problematic MCQs.
- What evidence would resolve it: A detailed analysis of MCQA-EQA conversion failures with automated classification rules for unsuitable questions.

### Open Question 2
- Question: Can the manual annotation process for converting MCQA to EQA be fully automated, and if so, what are the key technical challenges and potential solutions?
- Basis in paper: [inferred] The paper acknowledges the manual effort required and mentions "new avenues for examining how those 'unsuitable' QA pairs could be reformulated to be useful in EQA" and "potentially automating the process."
- Why unresolved: The paper focuses on manual annotation and provides insights for future automation but doesn't explore or implement automated solutions.
- What evidence would resolve it: A successful automated system for MCQA to EQA conversion with performance metrics and error analysis.

### Open Question 3
- Question: How does the performance of multilingual QA models vary across different language pairs and question types when applied to the converted EQA dataset?
- Basis in paper: [explicit] Section 5.2 presents experimental results comparing performance across language pairs (EN-EN, MSA-MSA, cross-lingual, and dialectal experiments) and mentions the subset excluding BB questions.
- Why unresolved: While initial results are provided, the paper doesn't conduct an in-depth analysis of performance variations across different question types or explore optimization strategies for specific language pairs.
- What evidence would resolve it: A comprehensive analysis of model performance across all language pairs and question types, including optimization strategies and error analysis.

## Limitations
- Manual annotation required for ~45% of questions due to fundamental MCQ-EQA incompatibilities
- Semi-automatic projection relies on n-gram overlap that may not preserve semantic equivalence in morphologically rich Arabic
- Dataset size (415 QA pairs) limits generalizability across diverse question types
- Claims about scalability to 120+ languages lack empirical validation beyond English and MSA

## Confidence
- High: Annotation methodology and IAA results are well-documented and reproducible
- Medium: Semi-automatic projection method is plausible but not independently validated
- Low: Scalability claim for 120+ languages lacks empirical validation

## Next Checks
1. Replicate the inter-annotator agreement study with at least three independent annotators using the published guidelines to verify the γ=0.84 score
2. Manually verify 50 randomly selected MSA spans projected from English to assess semantic alignment quality
3. Evaluate the translated dialectal questions across multiple Arabic dialects (Egyptian, Levantine, Gulf) to determine if the 18% improvement generalizes