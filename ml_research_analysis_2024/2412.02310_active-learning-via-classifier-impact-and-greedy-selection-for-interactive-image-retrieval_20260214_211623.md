---
ver: rpa2
title: Active Learning via Classifier Impact and Greedy Selection for Interactive
  Image Retrieval
arxiv_id: '2412.02310'
source_url: https://arxiv.org/abs/2412.02310
tags:
- learning
- samples
- classifier
- active
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the cold-start, open-set, and class-imbalanced
  challenge in interactive content-based image retrieval (IIR) using active learning.
  The authors introduce GAL (Greedy Active Learning), a batch-mode AL framework that
  leverages a novel acquisition function to measure the impact of unlabeled samples
  on the classifier, and embeds this in a greedy selection approach to better exploit
  samples within each batch.
---

# Active Learning via Classifier Impact and Greedy Selection for Interactive Image Retrieval

## Quick Facts
- arXiv ID: 2412.02310
- Source URL: https://arxiv.org/abs/2412.02310
- Reference count: 0
- Primary result: Novel batch-mode active learning framework (GAL) that outperforms existing methods by 5-10% in mAP for interactive image retrieval

## Executive Summary
This paper introduces GAL (Greedy Active Learning), a batch-mode active learning framework for interactive content-based image retrieval that addresses cold-start, open-set, and class-imbalanced challenges. GAL uses a novel acquisition function to measure the impact of unlabeled samples on the classifier, combined with a greedy selection approach to maximize diversity within each batch. The method is evaluated with linear (SVM) and non-linear classifiers (MLP, Gaussian Process) and demonstrates up to 5-10% improvement in mAP over existing approaches on four diverse datasets.

## Method Summary
GAL is a batch-mode active learning framework that addresses the challenges of interactive image retrieval through a novel acquisition function measuring classifier impact and greedy selection. The method works by first selecting top-K candidate samples based on relevance probabilities, then using an impact-based acquisition function to greedily select a batch of samples that maximizes diversity and uncertainty. For Gaussian Process classifiers, GAL provides a theoretical guarantee through the (1-1/e)-Approximation Theorem, while for SVM and MLP classifiers, it uses impact-based acquisition functions with pseudo-labeling. The framework is designed to combine both uncertainty and diversity in sample selection, improving retrieval performance while maintaining computational efficiency.

## Key Results
- GAL outperforms existing active learning methods by 5-10% in mAP on four diverse datasets (Paris-6K, Places365, FSOD-IR, MIRFLICKR-25K)
- Theoretical guarantee for Gaussian Process case through (1-1/e)-Approximation Theorem
- Computational efficiency comparable to state-of-the-art methods
- Demonstrated superiority over baselines including Random, COD, MaxiMin, RBMAL, Kmeans++, Coreset, and ITAL

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Greedy selection in GAL avoids redundant samples by discouraging selection of nearby points in the embedding space once one sample is chosen.
- **Mechanism:** When a sample is added to the training set, the impact value for nearby samples decreases because selecting a similar point would not maximize the impact score. The Max operation in the acquisition function naturally promotes selection of distant points.
- **Core assumption:** The impact value calculation is sensitive to the diversity of the training set, and adding a sample makes nearby samples less informative.
- **Evidence anchors:**
  - [abstract]: "We further embed this strategy in a greedy selection approach, better exploiting the samples within each batch."
  - [section]: "Whenever a sample point is added to the labeled set, selection of a similar point will result in a low impact value and will be discouraged due to the Max operation, promoting selection of distant points."
  - [corpus]: Weak. Corpus lacks direct evidence for this specific greedy diversity mechanism.
- **Break condition:** If the impact value calculation does not adequately penalize nearby samples, or if the embedding space has poor discriminative properties, the greedy approach may fail to enforce diversity.

### Mechanism 2
- **Claim:** GAL combines uncertainty and diversity inherently by selecting samples that maximize impact on the decision boundary.
- **Mechanism:** The MaxMin paradigm ensures that samples near the decision boundary (high uncertainty) are selected when they provide the largest change to the classifier, while distant samples are selected for diversity when they provide the largest change.
- **Core assumption:** The acquisition function can effectively measure the impact of samples on the decision boundary, and the greedy selection can balance uncertainty and diversity.
- **Evidence anchors:**
  - [abstract]: "This method effectively combines both uncertainty and diversity, as demonstrated in Section 4, providing a robust solution to challenges in IIR."
  - [section]: "GAL, on the other hand, showcases intermediate values and the lowest accuracy error...These metrics confirm that GAL suggests an adaptive strategy that integrates both uncertainty and diversity."
  - [corpus]: Weak. Corpus lacks direct evidence for this specific hybrid mechanism.
- **Break condition:** If the acquisition function cannot accurately measure impact, or if the greedy selection does not balance uncertainty and diversity effectively.

### Mechanism 3
- **Claim:** For Gaussian Process classifiers, GAL provides a theoretical guarantee of performance through the (1-1/e)-Approximation Theorem.
- **Mechanism:** The acquisition function for GP minimizes overall uncertainty, which is submodular and monotone, satisfying the conditions of the theorem.
- **Core assumption:** The variance reduction in GP is strictly positive and submodular, and the acquisition function is a non-negative linear combination of submodular functions.
- **Evidence anchors:**
  - [abstract]: "For the Gaussian Process case, we show a theoretical guarantee on the greedy approximation."
  - [section]: "Consequently, our acquisition function (15) satisfies the conditions of the (1-1/e)-Approximation theorem."
  - [corpus]: Weak. Corpus lacks direct evidence for this specific theoretical guarantee.
- **Break condition:** If the variance reduction is not strictly positive or submodular, or if the acquisition function does not satisfy the conditions of the theorem.

## Foundational Learning

- **Concept:** Gaussian Process (GP) classification
  - **Why needed here:** GP is used as one of the classifiers in GAL, and its properties (uncertainty estimation, submodularity) are crucial for the theoretical guarantee and the acquisition function.
  - **Quick check question:** What is the difference between a Gaussian Process regressor and a Gaussian Process classifier?

- **Concept:** Active Learning (AL) and pool-based AL
  - **Why needed here:** GAL is an active learning framework for interactive image retrieval, and understanding the principles of AL and pool-based AL is essential for grasping the problem setting and the proposed solution.
  - **Quick check question:** What are the main differences between pool-based AL and stream-based AL?

- **Concept:** Submodularity and the (1-1/e)-Approximation Theorem
  - **Why needed here:** The theoretical guarantee for GAL with GP relies on the submodularity of the acquisition function and the (1-1/e)-Approximation Theorem.
  - **Quick check question:** What is the definition of a submodular function, and what are the conditions for the (1-1/e)-Approximation Theorem to hold?

## Architecture Onboarding

- **Component map:** Image features -> Classifier (SVM/MLP/GP) -> AL module (candidate selection) -> AL module (greedy batch selection) -> User feedback -> Updated training set -> Retrain classifier

- **Critical path:** Image features → Classifier → AL module (candidate selection) → AL module (greedy batch selection) → User feedback → Updated training set → Retrain classifier

- **Design tradeoffs:**
  - Linear vs. non-linear classifiers: Linear classifiers (SVM) are faster and less prone to overfitting but may have lower accuracy. Non-linear classifiers (MLP, GP) can achieve higher accuracy but are slower and more prone to overfitting.
  - Candidate pool size: A smaller candidate pool (top-K) reduces computation but may miss some informative samples. A larger candidate pool increases computation but may improve performance.
  - Batch size: A smaller batch size (B=1) is more efficient but may require more user interactions. A larger batch size (B>1) reduces user interactions but may accumulate errors in pseudo-labels.

- **Failure signatures:**
  - Low pseudo-label accuracy: Indicates that the classifier is not confident enough to provide reliable pseudo-labels, leading to poor batch selection.
  - High redundancy in selected samples: Indicates that the greedy approach is not effectively enforcing diversity.
  - Poor performance on open-set or imbalanced datasets: Indicates that the acquisition function is not adequately handling these challenges.

- **First 3 experiments:**
  1. **Toy example:** Implement the 2D toy example (Fig. 3) to verify that GAL combines uncertainty and diversity and outperforms random selection.
  2. **SVM with small candidate pool:** Implement GAL with SVM classifier and a small candidate pool (K=100) on a simple dataset (e.g., MNIST) to verify that it outperforms random selection and other baselines.
  3. **GP with theoretical guarantee:** Implement GAL with Gaussian Process classifier and verify that the (1-1/e)-Approximation Theorem holds by comparing the performance of the greedy algorithm to the optimal batch selection.

## Open Questions the Paper Calls Out

- **Question:** How does the performance of GAL compare to state-of-the-art methods on other datasets, such as ImageNet or COCO?
  - **Basis in paper:** [inferred] The paper evaluates GAL on several diverse datasets, but does not include large-scale datasets like ImageNet or COCO.
  - **Why unresolved:** The paper focuses on interactive image retrieval tasks and uses datasets that are specifically designed for this purpose. Including larger datasets would require additional experiments and resources.
  - **What evidence would resolve it:** Conducting experiments on ImageNet or COCO and comparing the performance of GAL to state-of-the-art methods on these datasets.

- **Question:** Can GAL be extended to handle multi-class classification tasks, beyond the binary classification setting used in the paper?
  - **Basis in paper:** [explicit] The paper focuses on binary classification for interactive image retrieval, but does not explore the extension to multi-class scenarios.
  - **Why unresolved:** Extending GAL to multi-class classification would require modifications to the acquisition functions and the greedy selection approach, which may impact the performance and computational efficiency.
  - **What evidence would resolve it:** Implementing and evaluating GAL on multi-class classification tasks, comparing its performance to existing methods for active learning in multi-class settings.

- **Question:** How does the performance of GAL scale with the size of the unlabeled dataset? Is there a point where the computational cost becomes prohibitive?
  - **Basis in paper:** [explicit] The paper discusses the computational complexity of GAL and suggests that it remains reasonable for practical use cases. However, it does not provide a detailed analysis of how the performance scales with dataset size.
  - **Why unresolved:** The computational complexity of GAL depends on several factors, including the size of the unlabeled dataset, the feature dimension, and the number of labeled samples. Understanding the scaling behavior would require extensive experiments on datasets of varying sizes.
  - **What evidence would resolve it:** Conducting experiments on datasets of increasing size and measuring the performance and computational cost of GAL at each scale. Analyzing the results to identify any trends or limitations in terms of scalability.

## Limitations

- The effectiveness of GAL relies heavily on the quality of pseudo-labels generated by the classifier, which may propagate errors in early AL cycles.
- The framework's robustness in highly noisy or open-set scenarios is not fully explored in the paper.
- Computational efficiency claims depend on the candidate pool size K, which is not thoroughly analyzed for scalability.

## Confidence

- **High Confidence:** The theoretical guarantee for the Gaussian Process case and the overall experimental methodology are well-founded.
- **Medium Confidence:** The mechanism combining uncertainty and diversity through impact values is plausible but lacks direct empirical validation in the paper.
- **Low Confidence:** The specific implementation details of the acquisition functions and the exact impact value calculations are not fully specified, which could affect reproducibility.

## Next Checks

1. **Implementation Verification:** Implement the 2D toy example (Fig. 3) to verify that GAL combines uncertainty and diversity and outperforms random selection.
2. **Scalability Analysis:** Test GAL with varying candidate pool sizes K to assess its computational efficiency and performance trade-offs.
3. **Robustness Testing:** Evaluate GAL's performance in noisy or open-set scenarios to assess its robustness beyond the presented datasets.