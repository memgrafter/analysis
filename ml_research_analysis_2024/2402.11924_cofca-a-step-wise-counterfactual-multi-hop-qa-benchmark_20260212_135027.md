---
ver: rpa2
title: 'Cofca: A Step-Wise Counterfactual Multi-hop QA benchmark'
arxiv_id: '2402.11924'
source_url: https://arxiv.org/abs/2402.11924
tags:
- llms
- reasoning
- data
- performance
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CofCA, a novel benchmark designed to evaluate
  the real multi-step reasoning abilities of large language models (LLMs) on multi-hop
  question answering tasks. The benchmark addresses the limitations of existing factual
  MHQA datasets, which may suffer from data contamination in LLMs' pre-training stage
  and fail to assess the reasoning process itself.
---

# Cofca: A Step-Wise Counterfactual Multi-hop QA benchmark

## Quick Facts
- arXiv ID: 2402.11924
- Source URL: https://arxiv.org/abs/2402.11924
- Authors: Jian Wu; Linyi Yang; Zhen Wang; Manabu Okumura; Yue Zhang
- Reference count: 33
- Primary result: LLMs show significantly lower performance on counterfactual vs. factual multi-hop QA data, revealing data contamination issues in existing benchmarks

## Executive Summary
This paper introduces CofCA, a novel benchmark designed to evaluate the real multi-step reasoning abilities of large language models (LLMs) on multi-hop question answering tasks. The benchmark addresses the limitations of existing factual MHQA datasets, which may suffer from data contamination in LLMs' pre-training stage and fail to assess the reasoning process itself. CofCA consists of both factual data from existing benchmarks (HotpotQA, 2WikiMultihopQA, and MuSiQue) and counterfactual data generated by rewriting Wikipedia passages and annotating corresponding QA pairs. The counterfactual data ensures that knowledge is non-existent in the real world, forcing LLMs to reason solely on the given context.

## Method Summary
The authors create counterfactual data by rewriting Wikipedia passages using named entity, noun phrase, and synonym replacement followed by back-translation. They annotate QA pairs and decompose multi-hop questions into sub-questions with intermediate answers. LLMs are evaluated on both factual and counterfactual data using zero-shot prompting, with metrics including sub-question answering evaluation (F1 and EM scores), reasoning chain evaluation, and joint performance that combines sub-QA and final answer scores.

## Key Results
- LLMs show significantly lower performance on counterfactual vs. factual data, indicating data contamination in existing benchmarks
- Performance degrades as reasoning chain length increases from 2-hop to 4-hop in counterfactual data
- Joint performance scores reveal that LLMs often bypass correct reasoning chains despite achieving correct final answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rewriting Wikipedia passages with named entity, noun phrase, and synonym replacement followed by back-translation creates counterfactual data that cannot be answered from LLMs' internal memory.
- Mechanism: The key information (entities, facts) in the passage is replaced with semantically similar but factually incorrect information. Back-translation further alters the surface form. This ensures the knowledge required to answer questions about the passage does not exist in the pre-training corpus.
- Core assumption: The replaced entities and facts do not exist in the pre-training data of the LLMs being evaluated.
- Evidence anchors:
  - [abstract] "counterfactual data generated by rewriting Wikipedia passages and annotating corresponding QA pairs"
  - [section] "we consider the counterfactual evaluation method... make modifications to existing knowledge in Wikipedia, deriving new passages and ensuring that knowledge in these passages is non-existent in the real world"
  - [corpus] Weak - no direct corpus evidence, but supported by the paper's methodology description
- Break condition: If the replaced entities or facts accidentally match real-world knowledge present in the pre-training data, the counterfactual nature is compromised.

### Mechanism 2
- Claim: Evaluating LLMs on sub-questions in sequence reveals the reasoning chain quality, exposing cases where models bypass the correct reasoning path.
- Mechanism: Multi-hop questions are decomposed into sub-questions with intermediate answers. By checking the correctness of each sub-answer in sequence, we can identify whether the model followed the correct reasoning chain or took shortcuts.
- Core assumption: The correctness of each sub-answer depends on the correctness of the previous sub-answer in the reasoning chain.
- Evidence anchors:
  - [abstract] "LLMs are expected to engage in intricate reasoning processes that involve evidence retrieval and answering a series of sub-questions"
  - [section] "For example, in the 2-hop dataset, each data contains a 2-hop question, 2 sub-questions, 2 intermediate answers, and a final answer"
  - [corpus] Supported - the paper describes the reasoning chain evaluation methodology
- Break condition: If sub-questions are independent and don't form a coherent reasoning chain, this evaluation method loses its effectiveness.

### Mechanism 3
- Claim: The joint performance metric (combining sub-QA and MHQA performance) penalizes models that bypass incorrect reasoning chains to arrive at correct final answers.
- Mechanism: The joint score is calculated as a negative logarithm of the product of individual performance scores. This means that even if the final answer is correct, a poor reasoning chain results in a very low joint score.
- Core assumption: The product of individual performance scores accurately represents the quality of the entire reasoning chain.
- Evidence anchors:
  - [section] "Joint Performance... we introduce a joint performance that combines the evaluation of Sub-QA performance and MHQA performance"
  - [section] "The larger score means the worse performance on the reasoning chain"
  - [corpus] Supported - the paper describes the joint performance calculation
- Break condition: If the relationship between individual performance scores and overall reasoning quality is not multiplicative, the joint score may not accurately reflect reasoning chain quality.

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: The benchmark relies on counterfactual data to evaluate reasoning abilities independent of memorized knowledge.
  - Quick check question: What distinguishes counterfactual data from factual data in the context of LLM evaluation?

- Concept: Multi-hop reasoning
  - Why needed here: The benchmark evaluates the ability to integrate information from multiple pieces of evidence to answer complex questions.
  - Quick check question: How does multi-hop reasoning differ from single-hop reasoning in question answering?

- Concept: Reasoning chain evaluation
  - Why needed here: The benchmark decomposes multi-hop questions into sub-questions to evaluate the quality of the reasoning process.
  - Quick check question: Why is evaluating the reasoning chain important in addition to evaluating the final answer?

## Architecture Onboarding

- Component map: Wikipedia passage rewriting -> QA pair generation -> human review -> LLM evaluation -> Sub-question answering -> Reasoning chain evaluation -> Joint performance calculation
- Critical path: 1. Annotate counterfactual passages and QA pairs 2. Evaluate LLMs on sub-questions in sequence 3. Calculate joint performance scores 4. Compare performance on factual vs. counterfactual data
- Design tradeoffs: Manual vs. automatic annotation: Manual ensures quality but is slower; automatic is faster but may have quality issues; Exact match vs. partial match evaluation: Exact match is stricter but may miss semantically correct answers; partial match is more lenient but may be less precise
- Failure signatures: High performance on factual data but low performance on counterfactual data indicates data contamination; High performance on final answers but low reasoning chain scores indicates shortcut reasoning
- First 3 experiments: 1. Run LLMs on factual data from HotpotQA, 2WikiMultihopQA, and MuSiQue 2. Run LLMs on counterfactual data from CofCA (2-hop, 3-hop, 4-hop) 3. Evaluate reasoning chain quality by checking sub-question correctness in sequence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a more universal evaluation metric for multi-hop QA that addresses the limitations of exact match (EM) scoring, particularly in cases where answers have abbreviations, aliases, or alternative expressions?
- Basis in paper: [explicit] The paper discusses the limitations of EM scoring, citing an example where "Lionel Messi" and "Messi" should be treated as equivalent but are not by EM. It also mentions that all LLM-generated answers in their dataset face this issue.
- Why unresolved: The paper identifies the problem but does not propose a solution. Developing a more robust evaluation metric is crucial for accurately assessing LLM performance on multi-hop reasoning tasks.
- What evidence would resolve it: A new evaluation metric that can accurately compare answers with abbreviations, aliases, or alternative expressions, and a comparison showing improved performance assessment using this metric versus EM.

### Open Question 2
- Question: How does the performance of LLMs on counterfactual multi-hop QA tasks change when the reasoning chain length increases beyond 4 hops?
- Basis in paper: [explicit] The paper presents results for 2-hop, 3-hop, and 4-hop counterfactual tasks, showing a significant performance drop as the reasoning chain length increases. It mentions that "LLMs' reasoning ability decreases with the increases in reasoning chain length."
- Why unresolved: The paper only tests up to 4-hop questions. It's unclear how LLMs would perform on even more complex reasoning tasks with longer chains.
- What evidence would resolve it: Experimental results showing LLM performance on counterfactual multi-hop QA tasks with reasoning chains longer than 4 hops, ideally demonstrating a clear trend in performance degradation.

### Open Question 3
- Question: What specific techniques or modifications to LLMs could improve their performance on counterfactual multi-hop reasoning tasks, particularly in terms of following the correct reasoning chain?
- Basis in paper: [explicit] The paper finds that LLMs often bypass the correct reasoning chain, leading to inflated performance. It also notes that providing sub-questions as additional information can help guide LLMs towards the correct reasoning path.
- Why unresolved: While the paper identifies the problem of incorrect reasoning chains, it doesn't explore potential solutions or modifications to LLMs that could address this issue.
- What evidence would resolve it: Experiments comparing the performance of different LLM architectures, fine-tuning strategies, or prompting techniques on counterfactual multi-hop reasoning tasks, with a focus on improving the accuracy of reasoning chains.

## Limitations
- The counterfactual data generation process relies on automatic rewriting with limited human validation, which may introduce subtle factual errors that could affect evaluation validity
- The study does not address potential contamination from LLM training data beyond Wikipedia, such as other web sources that might contain similar counterfactual scenarios
- The human-in-the-loop annotation process lacks detailed procedural documentation, making full reproducibility challenging

## Confidence
- High confidence: The core finding that LLMs perform significantly worse on counterfactual vs. factual data (indicating data contamination issues)
- Medium confidence: The claim that current evaluation metrics overestimate multi-hop reasoning capabilities due to shortcut reasoning
- Medium confidence: The assertion that CofCA provides a more reliable evaluation framework for LLMs' reasoning abilities

## Next Checks
1. Cross-dataset validation: Test whether performance degradation on counterfactual data persists across different types of knowledge (e.g., historical vs. scientific facts) to confirm contamination is not domain-specific
2. Human baseline comparison: Establish human performance on both factual and counterfactual data to determine if the difficulty gap is appropriate and not artificially inflated
3. Alternative counterfactual generation: Implement a second counterfactual generation method (e.g., using different entity replacement strategies) to verify that performance differences are not artifacts of the specific rewriting approach used