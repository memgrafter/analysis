---
ver: rpa2
title: Improving Socratic Question Generation using Data Augmentation and Preference
  Optimization
arxiv_id: '2403.00199'
source_url: https://arxiv.org/abs/2403.00199
tags:
- questions
- socratic
- question
- student
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to improve Socratic question generation
  for code debugging using data augmentation and preference optimization. The authors
  first propose a method to synthetically generate invalid Socratic questions of four
  different types, using GPT-4 and then performing consistency checking to filter
  out inconsistent questions.
---

# Improving Socratic Question Generation using Data Augmentation and Preference Optimization

## Quick Facts
- arXiv ID: 2403.00199
- Source URL: https://arxiv.org/abs/2403.00199
- Authors: Nischal Ashok Kumar; Andrew Lan
- Reference count: 40
- Key outcome: Preference-optimized 7B Llama 2 outperforms GPT models by 7.89 and 3.65 points on Rouge-L and BERTScore recall for Socratic question generation in code debugging

## Executive Summary
This paper presents a novel approach to improve Socratic question generation for code debugging by combining synthetic data augmentation with preference optimization. The authors develop a method to generate invalid Socratic questions using GPT-4, filter them through consistency checking, and then use this augmented dataset to perform Direct Preference Optimization (DPO) on Llama 2. Their approach demonstrates significant improvements over existing state-of-the-art prompting methods, achieving better performance with a much smaller model. The study also includes case studies showing that DPO produces more diverse and valid questions compared to Supervised Fine-Tuning (SFT).

## Method Summary
The authors propose a two-stage approach: first, they synthetically generate invalid Socratic questions of four different types using GPT-4, followed by consistency checking to filter out inconsistent questions. Second, they use this augmented dataset containing both valid and invalid questions to perform preference optimization on Llama 2 using Direct Preference Optimization (DPO). The DPO process trains the model to distinguish between valid and invalid questions based on the augmented data. The resulting model is then evaluated on its ability to generate high-quality Socratic questions for code debugging tasks, with performance measured using ROUGE-L and BERTScore recall metrics.

## Key Results
- Preference-optimized 7B Llama 2 outperforms existing prompting methods using larger GPT models by 7.89 points on ROUGE-L and 3.65 points on BERTScore recall
- DPO generates significantly more diverse questions compared to Supervised Fine-Tuning (SFT)
- Case studies demonstrate that DPO produces valid Socratic questions while maintaining diversity in question types

## Why This Works (Mechanism)
The approach works by exposing the model to a broader distribution of both valid and invalid Socratic questions during training. By generating synthetic invalid questions and using them in a preference optimization framework, the model learns to distinguish between high-quality and low-quality questions. This contrastive learning approach helps the model internalize the characteristics of good Socratic questions more effectively than traditional supervised learning alone. The preference optimization objective directly aligns the model's output with human preferences for question quality, rather than just matching reference outputs.

## Foundational Learning
- **Socratic Questioning**: A pedagogical technique using targeted questions to guide learners toward discovering answers themselves. Needed to understand the specific requirements for question generation in educational contexts. Quick check: Can the model generate questions that promote critical thinking rather than providing direct answers?
- **Preference Optimization**: A training paradigm that uses pairwise comparisons to align model outputs with human preferences. Needed to move beyond traditional supervised learning and incorporate quality judgments. Quick check: Does the model consistently prefer valid questions over invalid ones in pairwise comparisons?
- **Data Augmentation**: Techniques for expanding training datasets through synthetic generation. Needed to create sufficient training examples of both valid and invalid questions. Quick check: Are the synthetic invalid questions diverse and representative of real-world errors?
- **ROUGE-L and BERTScore**: Evaluation metrics for text generation quality. Needed to quantitatively assess question generation performance. Quick check: Do these metrics correlate with human judgments of question quality?
- **Direct Preference Optimization (DPO)**: A specific preference optimization algorithm that avoids the need for explicit reward modeling. Needed for efficient training on preference data. Quick check: Does DPO converge faster than alternative preference learning methods?
- **Code Debugging Context**: The specific domain where questions guide learners through debugging processes. Needed to define the scope and requirements of the generated questions. Quick check: Are the generated questions appropriate for the specific debugging scenarios tested?

## Architecture Onboarding

Component Map: GPT-4 (synthetic data generation) -> Consistency Checker -> Augmented Dataset -> DPO Trainer -> Preference-Optimized Llama 2

Critical Path: The core workflow involves generating synthetic invalid questions, filtering them through consistency checking, combining with valid questions to create an augmented dataset, and then using this dataset to perform preference optimization on Llama 2 through the DPO algorithm.

Design Tradeoffs: The approach trades computational cost during training (DPO is more expensive than standard fine-tuning) for improved performance and generalization. Using synthetic data generation via GPT-4 reduces the need for expensive human annotation but introduces potential bias from the generative model. The choice of DPO over other preference learning methods balances simplicity with effectiveness.

Failure Signatures: Poor performance may manifest as: (1) Generated questions that are too similar to each other, indicating insufficient diversity in the augmented dataset; (2) Questions that don't effectively guide debugging, suggesting the synthetic data generation doesn't capture the nuances of good Socratic questions; (3) Overfitting to the synthetic invalid questions, resulting in overly conservative question generation.

First Experiments:
1. Generate a small batch of synthetic invalid questions and manually evaluate their quality and diversity
2. Run a pilot DPO training with a subset of the augmented data to check convergence behavior
3. Compare the preference-optimized model against baseline models on a held-out validation set using both automated metrics and human evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on GPT-4 for both generating invalid questions and consistency checking introduces dependencies on proprietary models
- Synthetic data generation approach lacks validation against human-annotated invalid questions
- Evaluation metrics may not fully capture pedagogical effectiveness or quality from student perspective
- Study focuses narrowly on code debugging domain, limiting generalizability to other educational contexts

## Confidence
High: Major claim that preference-optimized Llama 2 outperforms larger GPT models
Medium: Effectiveness of data augmentation approach
Low: Generalization to other domains or question types

## Next Checks
1. Validate synthetic invalid questions against human-annotated dataset to assess quality and diversity
2. Conduct user study with students/educators to evaluate pedagogical effectiveness beyond automated metrics
3. Test method on broader range of domains (math, science, humanities) to assess generalizability and robustness