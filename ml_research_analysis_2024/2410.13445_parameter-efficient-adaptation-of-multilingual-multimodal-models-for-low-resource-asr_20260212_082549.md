---
ver: rpa2
title: Parameter-efficient Adaptation of Multilingual Multimodal Models for Low-resource
  ASR
arxiv_id: '2410.13445'
source_url: https://arxiv.org/abs/2410.13445
tags:
- speech
- language
- adaptation
- text
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of low-resource automatic speech
  recognition (ASR) by combining parameter-efficient fine-tuning and text-only adaptation
  techniques within a multilingual multimodal model (SeamlessM4T). The proposed approach
  uses adapter modules to adapt the model with limited speech and text data.
---

# Parameter-efficient Adaptation of Multilingual Multimodal Models for Low-resource ASR

## Quick Facts
- arXiv ID: 2410.13445
- Source URL: https://arxiv.org/abs/2410.13445
- Reference count: 11
- Combines parameter-efficient fine-tuning and text-only adaptation within SeamlessM4T for low-resource ASR

## Executive Summary
This work addresses low-resource automatic speech recognition (ASR) by combining parameter-efficient fine-tuning and text-only adaptation techniques within the multilingual multimodal model SeamlessM4T. The approach uses adapter modules to adapt the model with limited speech and text data, leveraging the shared decoder component to learn language-specific syntax from parallel English-X text data. Results show that encoder adapters provide the most effective parameter-efficient adaptation for speech data, while text-only adaptation of decoder adapters is superior to ASR fine-tuning. The method achieves up to 17% relative WER reduction for low-resource languages like Odia, with cross-lingual transfer using related high-resource language data.

## Method Summary
The method combines parameter-efficient fine-tuning using adapter modules with text-only adaptation within the SeamlessM4T architecture. Encoder adapters are fine-tuned on transcribed speech data for acoustic-phonetic adaptation, while decoder adapters are fine-tuned on English-to-target language translation data to learn language-specific syntax. For zero-shot cross-lingual settings, the length adapter is fine-tuned using data from a related high-resource language (Bengali) and evaluated on low-resource languages (Maithili, Odia) without their speech data. The approach leverages the shared decoder component between ASR and translation pipelines to enable cross-task knowledge transfer.

## Key Results
- Encoder adapters with 50M parameters provide the most effective speech-based adaptation, significantly outperforming decoder adapter fine-tuning
- Text-only adaptation of decoder adapters outperforms ASR fine-tuning, with length adapter fine-tuning requiring fewer parameters while providing similar benefits
- Cross-lingual transfer via length adapter fine-tuning using Bengali data achieves up to 17% relative WER reduction for Maithili and Odia in zero-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-only adaptation of the decoder improves ASR by leveraging cross-task knowledge transfer from the shared decoder
- Mechanism: The text decoder in SeamlessM4T is shared between ASR and translation pipelines. Fine-tuning this shared decoder on English-to-target language translation teaches it the target language's syntax, which it can then apply to ASR decoding
- Core assumption: The shared decoder learns language-specific syntax from translation data that is useful for ASR output
- Evidence anchors:
  - [abstract]: "Multimodal models are able to leverage unlabeled text via text-only adaptation with further parameter-efficient ASR fine-tuning, thus boosting ASR performance"
  - [section 3.3]: "We hypothesize that we can improve the ASR performance for a target language by fine-tuning the text decoder adapters via text-to-text translation into that language"
- Break condition: If the target language syntax is very different from English, or if the translation data quality is poor, the transfer may fail

### Mechanism 2
- Claim: Encoder adapters provide more effective parameter-efficient speech adaptation than decoder adapters because they handle the acoustic-phonetic mapping
- Mechanism: Speech encoder adapters are fine-tuned on transcribed speech data, allowing them to learn language-specific acoustic features and phoneme representations. Since the decoder is shared and already has language syntax knowledge from text-only adaptation, encoder adaptation is more impactful
- Core assumption: The speech encoder learns acoustic-phonetic representations that are crucial for ASR, and these are more language-specific than decoder syntax
- Evidence anchors:
  - [section 5.1]: "From the results, it is evident that fine-tuning the length adapter requires fewer parameters while providing similar benefits to text decoder fine-tuning... Additionally, the ASR fine-tuning of the speech encoder proves to be significantly beneficial, although it involves training a substantially larger number of parameters"
  - [section 5.1]: "The results demonstrate that larger encoder adapters with 50M parameters are the most beneficial in enhancing the ASR performance... for the same number of trainable parameters, speech-based training of encoder adapters performs much better than that of decoder adapters"
- Break condition: If speech data is extremely limited, encoder adaptation may overfit, or if the base encoder already generalizes well, further adaptation may have diminishing returns

### Mechanism 3
- Claim: Length adapter fine-tuning using data from a related high-resource language enables cross-lingual transfer for zero-shot ASR adaptation
- Mechanism: The length adapter maps speech embeddings to the multimodal space and may learn prosodic characteristics like phoneme durations. Fine-tuning it with Bengali speech data (a related language) teaches it duration patterns that are useful for Maithili and Odia ASR, even without those languages' speech data
- Core assumption: Prosodic characteristics like phoneme durations are shared across related languages and can be transferred via the length adapter
- Evidence anchors:
  - [section 3.2.1]: "We hypothesize that the length adapter module could potentially learn prosodic characteristics of languages, such as phoneme durations, by mapping speech embeddings — which include both segmental and suprasegmental information — to text embeddings that contain only content information"
  - [section 5.3]: "We hypothesize that the length adapter could capture content-agnostic prosodic characteristics of a language without overfitting on its syntax. Consequently, fine-tuning this adapter using data from a closely related high-resource language might enhance the model's predictions for a low-resource target language"
- Break condition: If the pivot and target languages are not closely related, or if duration patterns differ significantly, the transfer will fail

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT) using adapters
  - Why needed here: Full fine-tuning of a 1.2B parameter model on limited low-resource data would overfit and be computationally expensive. Adapters allow targeted adaptation with fewer parameters
  - Quick check question: How many parameters are introduced by encoder and decoder adapters in the described experiments?

- Concept: Cross-modal representation learning in multimodal models
  - Why needed here: SeamlessM4T learns a joint speech-text representation space. Understanding this is crucial for why text-only adaptation can help ASR
  - Quick check question: What is the role of the length adapter in bridging the speech and text modalities?

- Concept: Cross-lingual transfer learning
  - Why needed here: The work uses a related high-resource language (Bengali) to improve ASR for zero-shot low-resource languages (Maithili, Odia). Understanding transfer mechanisms is key
  - Quick check question: What linguistic property is hypothesized to be transferable via the length adapter between related languages?

## Architecture Onboarding

- Component map:
  Speech Encoder (Conformer blocks) → Length Adapter → Text Decoder (Transformer blocks)
  Adapters inserted: after each Conformer layer in encoder, after each Transformer layer in decoder, in length adapter
  Training pathways: speech-to-text (ASR) and text-to-text (translation)

- Critical path: Speech → Encoder adapters → Length adapter → Decoder adapters → Text output
  - For text-only adaptation: Text → Decoder adapters → Text output
  - For cross-lingual transfer: Speech (Bengali) → Length adapter adapters → Length adapter (fine-tuned) → Text (Maithili/Odia)

- Design tradeoffs:
  - Adapter dimension D2: 256 (6M params) vs 2048 (50M params) - smaller is more parameter-efficient but may limit adaptation capacity
  - ASR fine-tuning vs text-only adaptation: Encoder adapters better for speech, decoder better for text
  - Single language vs cross-lingual: Length adapter better for cross-lingual, encoder better for in-language

- Failure signatures:
  - Overfitting: WER improvement on training data but degradation on test data
  - Catastrophic forgetting: Performance drop on high-resource languages after low-resource adaptation
  - Transfer failure: No WER improvement from cross-lingual pivot, or worse than baseline

- First 3 experiments:
  1. Fine-tune only encoder adapters with 5 hours of target language speech - measure WER improvement vs baseline
  2. Fine-tune only decoder adapters with English-target language translation data - measure WER improvement vs baseline
  3. Fine-tune length adapter with Bengali speech, then evaluate on Maithili/Odia without their speech data - measure zero-shot WER reduction

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of text-only adaptation depends heavily on the quality of machine-translated parallel text, which may introduce noise and limit syntax transfer benefits
- Cross-lingual transfer via the length adapter assumes prosodic similarity between Bengali and target languages, but this relationship is not quantitatively validated
- The work does not address potential catastrophic forgetting of high-resource language performance after low-resource adaptation

## Confidence

- High confidence: Encoder adapters are more effective than decoder adapters for speech-based adaptation with the same number of trainable parameters
- Medium confidence: Text-only adaptation improves ASR through cross-task knowledge transfer from the shared decoder, though this mechanism needs more empirical validation
- Medium confidence: Cross-lingual transfer via length adapter is effective for related languages, but the extent of relatedness required is not quantified

## Next Checks

1. **Syntax transfer validation**: Design an experiment that isolates the contribution of decoder syntax transfer by comparing text-only adaptation with and without parallel text quality controls (e.g., human-translated vs machine-translated)

2. **Prosodic similarity quantification**: Measure actual prosodic feature similarity (duration, intonation patterns) between Bengali and each target language using forced alignment and compare with transfer effectiveness

3. **Catastrophic forgetting assessment**: After low-resource adaptation, evaluate WER on high-resource languages to quantify performance degradation and test mitigation strategies like elastic weight consolidation