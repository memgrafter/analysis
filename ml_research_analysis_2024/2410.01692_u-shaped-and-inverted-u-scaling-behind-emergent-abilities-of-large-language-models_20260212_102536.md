---
ver: rpa2
title: U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language
  Models
arxiv_id: '2410.01692'
source_url: https://arxiv.org/abs/2410.01692
tags:
- scaling
- compute
- brier
- score
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates emergent abilities in large language models
  by grouping questions by difficulty and analyzing scaling trends. It finds that
  hard questions exhibit U-shaped scaling (performance initially worsens then improves
  with scale), while easy questions show inverted-U scaling followed by steady improvement.
---

# U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models

## Quick Facts
- arXiv ID: 2410.01692
- Source URL: https://arxiv.org/abs/2410.01692
- Authors: Tung-Yu Wu; Pei-Yu Lo
- Reference count: 40
- Primary result: Slice-and-Sandwich pipeline better predicts emergent ability thresholds and performance beyond them compared to Sigmoid-based regression

## Executive Summary
This paper investigates emergent abilities in large language models by analyzing scaling trends of performance on questions grouped by difficulty. The authors discover that hard questions exhibit U-shaped scaling (initially worsening then improving with scale), while easy questions show inverted-U scaling followed by steady improvement. These opposing trends initially offset each other, causing apparent stagnation until the scaling trend on easy questions reverts from inverse to standard scaling, leading to emergent abilities. Based on this observation, the authors propose a simple yet effective pipeline called Slice-and-Sandwich to predict emergence thresholds and model performance beyond them.

## Method Summary
The Slice-and-Sandwich pipeline groups questions into difficulty levels, fits scaling trends separately for easy and hard questions using models before the emergence threshold, and forecasts performance beyond the threshold. The method uses polynomial regression (degree 5 for easy questions, degree 2 for hard questions) on a continuous metric called TC Brier Score, then projects back to accuracy using OLS regression. The approach is validated on three datasets (MMLU, arithmetic, Persian-QA) and compared against Sigmoid-based regression baselines, showing superior prediction of performance improvements at emergence thresholds.

## Key Results
- Hard questions exhibit U-shaped scaling while easy questions show inverted-U scaling, offsetting each other until easy questions transition from inverse to standard scaling
- The Slice-and-Sandwich pipeline better predicts emergence thresholds and performance beyond them compared to Sigmoid-based regression methods
- The point where easy questions revert from inverse to standard scaling roughly coincides with the emergence threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping questions by difficulty creates opposing scaling trends that mask performance gains until a critical threshold.
- Mechanism: U-shaped scaling on hard questions and inverted-U scaling on easy questions offset each other in aggregate, creating apparent stagnation.
- Core assumption: The model's learning dynamics differ systematically by question difficulty, with harder questions initially showing inverse scaling due to distractor effects.
- Evidence anchors: [abstract] "performance on hard questions exhibits U-shaped scaling... In contrast, performance on easy questions exhibits an inverted U-shape followed by steady improvement"; [section] "The two scaling patterns initially offset each other, causing stagnant overall performance"
- Break condition: If question difficulty distribution changes dramatically or if a metric fails to capture the nuanced confidence shifts that distinguish easy from hard questions.

### Mechanism 2
- Claim: The point where easy question performance reverts from inverse to standard scaling coincides with emergence thresholds.
- Mechanism: As model capacity increases, easy questions transition from the classical regime (overfitting, inverse scaling) to the modern regime (improved generalization), while hard questions continue their U-shaped trajectory, leading to sharp overall performance gains.
- Core assumption: The classical-modern regime transition in deep double descent theory applies to question difficulty stratification.
- Evidence anchors: [section] "the point at which performance reverts from inverse to standard scaling roughly coincides with the emergence threshold"; [section] "initially, the bias-variance trade-off in classical statistical learning theory applies... once the model is large enough (the 'modern regime')... improving performance"
- Break condition: If the emergence threshold is driven by factors unrelated to the easy question group's scaling transition, or if the metric used doesn't align with the classical-modern regime distinction.

### Mechanism 3
- Claim: Continuous metrics like TC Brier Score capture granular performance changes that traditional metrics miss, enabling earlier detection of scaling transitions.
- Mechanism: By conditioning on available classes and focusing on target class probability, TC Brier Score differentiates between random guessing and structured prediction failures, revealing the true scaling dynamics.
- Core assumption: The correlation between TC Brier Score and accuracy is stable enough across model scales to allow reliable conversion.
- Evidence anchors: [section] "we propose the Target-Conditioned (TC) Brier Score that lumps all non-target available classes into one class"; [section] "Fig. 9 shows the close and almost linear relation between accuracy and TC Brier Score"
- Break condition: If the relationship between TC Brier Score and accuracy becomes non-linear or unstable beyond certain model scales, making conversion unreliable.

## Foundational Learning

- Concept: Bias-variance tradeoff in classical statistical learning theory
  - Why needed here: Explains why easy questions initially show inverse scaling (overfitting in classical regime) before transitioning to standard scaling
  - Quick check question: What distinguishes the classical regime from the modern regime in deep double descent?

- Concept: Deep double descent phenomenon
  - Why needed here: Provides theoretical framework for understanding why performance on easy questions initially improves then worsens before improving again
  - Quick check question: How does the deep double descent curve differ from traditional U-shaped learning curves?

- Concept: Continuous performance metrics vs. discrete metrics
  - Why needed here: TC Brier Score captures confidence-level changes that accuracy misses, revealing true scaling dynamics
  - Quick check question: Why might a continuous metric be more informative than accuracy for detecting early signs of emergent abilities?

## Architecture Onboarding

- Component map: Question difficulty stratification → Continuous metric calculation → Polynomial regression fitting → Scaling law projection → Emergence threshold prediction
- Critical path: Difficulty-based grouping → TC Brier Score calculation → Polynomial fit (degree 5 for easy, degree 2 for hard) → Averaging → OLS regression to accuracy
- Design tradeoffs: Higher polynomial degrees capture more complex patterns but risk overfitting; simpler metrics are more interpretable but may miss subtle transitions
- Failure signatures: Flat or noisy scaling trends across difficulty groups; poor correlation between continuous and traditional metrics; polynomial fits that don't capture observed U/inverted-U shapes
- First 3 experiments:
  1. Validate U-shaped vs. inverted-U scaling on a new emergent task using TC Brier Score
  2. Test sensitivity of emergence prediction to polynomial degree choice (vary degrees systematically)
  3. Compare TC Brier Score correlation with accuracy across different model scales and tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do U-shaped and inverted-U scaling patterns change when using different continuous performance metrics beyond TC Brier Score?
- Basis in paper: The paper acknowledges that different continuous metrics could be used in their pipeline and mentions preliminary analysis with token edit distance and modified cosine similarity for string-matching tasks.
- Why unresolved: The paper primarily uses TC Brier Score and only briefly explores alternatives. Different metrics might reveal different scaling patterns or better capture the underlying phenomena.
- What evidence would resolve it: Comprehensive experiments comparing scaling patterns across multiple continuous metrics (like continuous accuracy, probability calibration measures, or other probabilistic scoring rules) on the same datasets.

### Open Question 2
- Question: What are the fundamental mechanisms that cause easy questions to show inverted-U scaling while hard questions show U-shaped scaling?
- Basis in paper: The paper provides possible explanations based on deep double descent theory and distraction tasks, but these remain speculative and don't fully explain the systematic difference between easy and hard questions.
- Why unresolved: The explanations offered are observational and don't provide a mechanistic understanding of why question difficulty correlates with these specific scaling patterns.
- What evidence would resolve it: Detailed analysis of model internal representations (like attention patterns or feature evolution) that reveals why easy questions follow one pattern and hard questions follow another as scale increases.

### Open Question 3
- Question: Can the Slice-and-Sandwich pipeline be generalized to predict emergent abilities in non-multiple-choice tasks?
- Basis in paper: The paper acknowledges limitations in applying their method to string-matching tasks and discusses this in Appendix H, noting the challenge of finding appropriate difficulty grouping and continuous metrics.
- Why unresolved: The paper only demonstrates success on multiple-choice tasks and shows preliminary difficulties with string-matching tasks, leaving the general applicability unknown.
- What evidence would resolve it: Successful application of the pipeline to diverse task types (regression, generation, reasoning) with appropriate difficulty stratification and continuous metrics that correlate with target performance measures.

### Open Question 4
- Question: What is the relationship between the emergence threshold identified through accuracy jumps and the point where easy questions revert from inverse to standard scaling?
- Basis in paper: The paper notes that "the point at which performance reverts from inverse to standard scaling roughly coincides with the emergence threshold" but doesn't establish a precise relationship.
- Why unresolved: The paper only observes this coincidence without investigating whether it's consistent across tasks, whether one causes the other, or if they're both symptoms of a deeper phenomenon.
- What evidence would resolve it: Systematic analysis across many tasks showing the exact relationship between these two transition points, including whether the scaling reversion always precedes, coincides with, or follows the accuracy jump.

## Limitations
- The method relies on accurate question difficulty stratification, which may be challenging for tasks without clear difficulty gradients
- The TC Brier Score correlation with accuracy may not hold uniformly across all tasks or model scales
- The approach has only been validated on multiple-choice tasks, with preliminary difficulties applying to string-matching tasks

## Confidence

**High**: Slice-and-Sandwich pipeline's effectiveness as a predictive tool
- The method is well-specified and the improvement over Sigmoid-based baselines is quantitatively demonstrated

**Medium**: U-shaped and inverted-U scaling mechanisms
- While empirical observations are compelling, the exact reasons why question difficulty creates distinct scaling patterns remain partially speculative

**Critical Unknown**: TC Brier Score correlation stability
- The relationship between TC Brier Score and accuracy may not be stable across all tasks or model scales

## Next Checks

1. **Cross-task generalization test**: Apply Slice-and-Sandwich to a new emergent ability task (e.g., mathematical reasoning or code generation) with independently measured difficulty stratification to verify the U-shaped and inverted-U patterns persist across domains.

2. **Robustness to difficulty measurement**: Systematically vary the number of difficulty levels (e.g., 5 vs. 10 levels) and the method of difficulty calculation to assess whether the observed scaling patterns are robust to different question stratification approaches.

3. **Temporal validation**: Use Slice-and-Sandwich to predict emergence thresholds for models that were not in the original training set, then evaluate whether the actual performance of these held-out models matches the predictions at and beyond the predicted thresholds.