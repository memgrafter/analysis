---
ver: rpa2
title: 'How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models'
arxiv_id: '2407.00369'
source_url: https://arxiv.org/abs/2407.00369
tags:
- explanations
- detection
- dataset
- mocheg
- fact-checking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates knowledge transfer strategies for improving
  multimodal fact verification models, focusing on intra-domain mixtures of fact-checking
  datasets, inter-domain transfer from related tasks like toxicity detection, and
  knowledge distillation from LLM-generated explanations. Experiments on two multimodal
  benchmarks (Mocheg and Fakeddit) using CLIP-based and LLaVA-based verifiers show
  that dataset diversity matters more than scale alone, with certain mixtures improving
  Fakeddit F1 by 1.7% and Mocheg F1 by 2.9% over the state-of-the-art.
---

# How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models

## Quick Facts
- arXiv ID: 2407.00369
- Source URL: https://arxiv.org/abs/2407.00369
- Reference count: 5
- Primary result: Dataset diversity improves multimodal fact verification more than scale alone

## Executive Summary
This paper evaluates knowledge transfer strategies for multimodal fact verification, demonstrating that dataset diversity matters more than model scale alone. Through experiments on Mocheg and Fakeddit benchmarks using CLIP-based and LLaVA-based verifiers, the authors show that carefully constructed dataset mixtures improve performance by up to 2.9% F1 on Mocheg and 1.7% on Fakeddit over state-of-the-art. Additionally, GPT-4o-generated explanations further boost performance by up to 1.8 F1, achieving a new best of 65.07 F1 on Mocheg. The study also reveals that knowledge transfer from fact-checking datasets can enhance hate speech detection by up to 13.65% F1.

## Method Summary
The study fine-tunes CLIP-large-336 and LLaVA models on 12 public multimodal fact verification benchmarks, evaluating various knowledge transfer strategies including dataset mixtures and LLM-generated explanations. Models are trained using Adam optimizer with learning rate 1e-3, batch size 2048 (with gradient accumulation), and LoRA fine-tuning for LLaVA models. The experiments systematically test different combinations of training datasets, including fact-checking benchmarks and toxicity detection datasets, while also evaluating the impact of incorporating GPT-4o-generated explanations as additional training data.

## Key Results
- Dataset mixtures improve Fakeddit F1 by 1.7% and Mocheg F1 by 2.9% over state-of-the-art
- GPT-4o-generated explanations boost Mocheg performance by up to 1.8 F1, achieving 65.07 F1
- Knowledge transfer from fact-checking data enhances hate speech detection by up to 13.65% F1
- Models remain brittle to temporal distribution shifts, showing poor generalization to time-shifted data
- Fakeddit's poor generalization despite large scale (1,063,106 samples) indicates diversity matters more than scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset diversity improves multimodal fact verification more than model scale alone.
- Mechanism: Mixing datasets with different domain structures and label spaces exposes the model to varied reasoning patterns, reducing overfitting to a single dataset's spurious correlations.
- Core assumption: Different datasets contain complementary information that, when combined, enhance generalization.
- Evidence anchors:
  - [abstract] "knowledge transfer strategies can improve Fakeddit performance over the state-of-the-art by up to 1.7% and Mocheg performance by up to 2.9%"
  - [section 6.1] "Fakeddit does not generalize well on its own despite being significantly larger than other training datasets (1,063,106 samples), indicating there is still benefit from data diversity introduced by mixtures over a single large-scale dataset."
  - [corpus] Found 25 related papers with average FMR 0.447, indicating moderate relevance but no direct experimental validation of diversity vs scale tradeoff.
- Break condition: If datasets are too heterogeneous in structure, the mixture may degrade performance due to incompatible label spaces or evidence formats.

### Mechanism 2
- Claim: Knowledge transfer from fact-checking to toxicity detection improves hate speech detection performance.
- Mechanism: Shared reasoning skills between tasks (e.g., evidence evaluation, claim verification) allow cross-task knowledge transfer.
- Core assumption: Fact-checking datasets contain reasoning patterns useful for toxicity detection.
- Evidence anchors:
  - [abstract] "knowledge transfer from fact-checking data can also enhance hate speech detection by up to 13.65% F1"
  - [section 6.2] "the Mocheg + HateXplain mixture improves HateXplain performance by 13.65% and also performs better than HateXplain alone (by 9.97%) on Toxigen"
  - [corpus] Moderate neighbor relevance (FMR 0.447) suggests related work but no direct experimental validation of this specific transfer direction.
- Break condition: If the reasoning patterns are task-specific and not transferable, performance may degrade or show no improvement.

### Mechanism 3
- Claim: GPT-4o-generated explanations improve multimodal fact verification performance over baselines.
- Mechanism: Explanations provide additional reasoning context that guides the model to learn better feature representations for veracity prediction.
- Core assumption: High-quality explanations contain reasoning patterns that can be learned by smaller models.
- Evidence anchors:
  - [abstract] "GPT-4o-generated explanations further boost Mocheg performance by up to 1.8 F1, achieving a new best of 65.07 F1"
  - [section 7.2] "GPT-4o explanation augmented models consistently outperform the LLaVA baseline, leading to state-of-the-art open model results (65.07 F1) on Mocheg"
  - [corpus] Low citation count (0) in neighbor papers suggests limited validation of this mechanism in related work.
- Break condition: If explanations are noisy or contain incorrect reasoning, they may confuse the model and degrade performance.

## Foundational Learning

- Concept: Multimodal embedding and classification
  - Why needed here: The core task requires jointly processing text and image evidence for fact verification
  - Quick check question: Can you explain how CLIP and LLaVA models differ in their approach to multimodal representation?

- Concept: Knowledge distillation through explanations
  - Why needed here: LLM-generated explanations are used to transfer reasoning capabilities to smaller models
  - Quick check question: What are the potential risks of using LLM explanations as training data for fact verification models?

- Concept: Temporal distribution shift
  - Why needed here: Fact verification models must handle evolving news topics and claims over time
  - Quick check question: How would you design an evaluation to test a model's robustness to temporal distribution shift?

## Architecture Onboarding

- Component map: Evidence encoder (CLIP/LLaVA) → Stance predictor → Veracity classifier → (Optional) Explanation generator
- Training pipeline: Dataset → Encoder fine-tuning → Classification head training → Evaluation
- Explanation pipeline: Claim + Evidence → LLM explanation generation → Training data augmentation

- Critical path:
  1. Encode multimodal evidence using pre-trained CLIP or LLaVA model
  2. Predict claim-evidence stance relationship
  3. Aggregate stance predictions to determine claim veracity
  4. (Optional) Generate and incorporate LLM explanations

- Design tradeoffs:
  - Larger encoders provide better representations but require more computational resources
  - Dataset mixtures improve diversity but may introduce label space conflicts
  - LLM explanations can boost performance but may introduce noise or bias

- Failure signatures:
  - High in-distribution performance but poor generalization to new datasets
  - Degradation when mixing datasets with incompatible label spaces
  - Overfitting to specific evidence patterns in training data

- First 3 experiments:
  1. Train baseline model on single dataset (Mocheg or Fakeddit) to establish performance floor
  2. Train model on dataset mixture (e.g., Mocheg + PubHealth) to test diversity benefits
  3. Train model with GPT-4o explanations to evaluate knowledge distillation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of dataset mixtures depend on the relative size of the datasets being combined, or is diversity of data more important than scale?
- Basis in paper: [explicit] The paper states "Data Diversity Over Scale" and notes that "Fakeddit does not generalize well on its own despite being significantly larger than other training datasets (1,063,106 samples), indicating there is still benefit from data diversity introduced by mixtures over a single large-scale dataset."
- Why unresolved: The experiments show that scale alone is insufficient but do not systematically vary dataset sizes to determine the relative importance of scale versus diversity.
- What evidence would resolve it: Controlled experiments mixing datasets of varying sizes while keeping diversity constant, and vice versa, to isolate the effects of scale versus diversity.

### Open Question 2
- Question: Can knowledge transfer from fact-checking datasets to toxicity detection tasks generalize to other content moderation domains beyond hate speech detection?
- Basis in paper: [explicit] The paper finds "knowledge transfer from fact-checking data can boost performance by 13.65% F1" for hate speech detection and states this has "implications for other important content moderation domains like hate speech detection."
- Why unresolved: The study only tests transfer to one specific toxicity/hate speech detection task (HateXplain) and one other hate speech dataset (Toxigen), leaving open whether this transfer generalizes to other moderation tasks.
- What evidence would resolve it: Testing knowledge transfer from fact-checking datasets to a broader range of content moderation tasks such as cyberbullying detection, violent content detection, or sexual content moderation.

### Open Question 3
- Question: How do different types of explanations (commonsense reasoning, domain knowledge, event knowledge) contribute differently to improving model performance in multimodal fact verification?
- Basis in paper: [explicit] The human evaluation shows "explanations use a combination of commonsense reasoning and domain-specific knowledge, as well as evidence retrieval from known events" but the study does not isolate which type is most effective.
- Why unresolved: The study aggregates all explanation types together without analyzing their individual contributions to model performance.
- What evidence would resolve it: Experiments that systematically vary the proportion of different explanation types (only commonsense, only domain knowledge, only event knowledge) and measure their individual impact on model performance.

## Limitations

- Dataset mixtures improve performance but optimal mixture ratios and compatibility criteria are not systematically explored
- Knowledge transfer benefits from fact-checking to toxicity detection may be task-specific and not generalizable to other domains
- Effectiveness of GPT-4o explanations relies on the quality and consistency of LLM-generated explanations, introducing potential noise

## Confidence

- High Confidence: Experimental results on Mocheg and Fakeddit benchmarks, CLIP vs LLaVA comparison, temporal distribution shift findings
- Medium Confidence: Dataset diversity benefits, cross-task knowledge transfer mechanisms, explanation-based knowledge distillation
- Low Confidence: Generalizability to other multimodal tasks, long-term effectiveness of LLM explanations, optimal dataset mixture strategies

## Next Checks

1. **Ablation study on dataset mixture composition**: Systematically vary the ratios and combinations of datasets in mixtures to identify optimal diversity patterns and quantify the marginal benefit of each additional dataset.

2. **Temporal robustness evaluation**: Design a controlled experiment testing model performance across multiple time periods with known distribution shifts, measuring degradation rates and identifying specific temporal vulnerabilities.

3. **Explanation quality assessment**: Evaluate the consistency and correctness of GPT-4o-generated explanations through human annotation, measuring their correlation with actual reasoning patterns and their impact on model calibration.