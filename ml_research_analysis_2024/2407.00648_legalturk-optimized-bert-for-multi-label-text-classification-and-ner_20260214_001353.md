---
ver: rpa2
title: LegalTurk Optimized BERT for Multi-Label Text Classification and NER
arxiv_id: '2407.00648'
source_url: https://arxiv.org/abs/2407.00648
tags:
- bert
- text
- tokens
- pre-training
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study aims to enhance the BERT model's performance in the legal
  Turkish domain by modifying its pre-training tasks. The authors propose replacing
  the Next Sentence Prediction (NSP) task with Sentence Order Prediction (SOP) and
  combining Masked Language Model (MLM) with Term Frequency-Inverse Document Frequency
  (TF-IDF).
---

# LegalTurk Optimized BERT for Multi-Label Text Classification and NER

## Quick Facts
- arXiv ID: 2407.00648
- Source URL: https://arxiv.org/abs/2407.00648
- Authors: Farnaz Zeidi; Mehmet Fatih Amasyali; Çiğdem Erol
- Reference count: 35
- Primary result: F-measure of 72.15% in multi-label text classification and 77.23% in NER on legal Turkish text

## Executive Summary
This study proposes modifications to the BERT model's pre-training tasks to enhance performance in the legal Turkish domain. The authors replace Next Sentence Prediction (NSP) with Sentence Order Prediction (SOP) and combine Masked Language Model (MLM) with Term Frequency-Inverse Document Frequency (TF-IDF) token replacement. Their models, pre-trained from scratch on legal Turkish corpora, outperform the original BERT and BERTurk on Named Entity Recognition (NER) and multi-label text classification tasks. The results demonstrate the effectiveness of domain-specific pre-training and task modifications for improving BERT's performance in specialized domains.

## Method Summary
The researchers pre-trained five modified BERT models from scratch using legal Turkish corpora of varying sizes (50 MB to 2 GB), comparing them against the original BERT and BERTurk models. They implemented three key modifications: replacing NSP with SOP, removing NSP entirely (MLM-only), and combining MLM with TF-IDF by replacing 10% of masked tokens with high TF-IDF valued tokens. All models used BERT base structure with 12 hidden layers and were fine-tuned for a single epoch using AdamW optimizer with a learning rate of 4e-5 and batch size of 128. Performance was evaluated on human-annotated NER and multi-label text classification datasets using Precision, Recall, and F-measure metrics.

## Key Results
- LegalTurk models pre-trained on 50 MB legal corpus achieved F-measure of 72.15% in multi-label classification and 77.23% in NER
- Models pre-trained on 2 GB legal corpus outperformed BERTurk (trained on 35 GB non-specific corpus) in both tasks
- SOP modification showed positive influence on both NER and multi-label text classification compared to original BERT
- MLM-only approach with TF-IDF token replacement yielded significant improvements in multi-label classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing Next Sentence Prediction (NSP) with Sentence Order Prediction (SOP) improves model performance on legal Turkish text.
- Mechanism: SOP requires the model to determine the correct order of two sentences, which is more semantically demanding than NSP's binary next-sentence classification. This encourages the model to learn more nuanced sentence-level coherence, beneficial for legal text's complex structure.
- Core assumption: Legal texts contain long, potentially noisy sentences where sentence-level coherence is more critical than simple adjacency.
- Evidence anchors:
  - [abstract]: "Replacing Next Sentence Prediction (NSP) with Sentence Order Prediction (SOP) and combining Masked Language Model (MLM) with Term Frequency-Inverse Document Frequency (TF-IDF)."
  - [section 6.1]: "When comparing the SOP-based model (SOP_MLM_80_10_10) with the original BERT model (NSP_MLM_80_10_10), a positive influence of this substitution on both NER and multi-label text classification tasks is evident."
- Break condition: If the legal corpus contains mostly short, simple sentences where adjacency is sufficient, SOP may not provide additional benefit over NSP.

### Mechanism 2
- Claim: Removing NSP entirely and focusing on MLM-only training improves performance for legal domain tasks.
- Mechanism: NSP can be noisy for long legal documents, and its removal allows the model to focus more on token-level understanding through MLM. This is particularly beneficial when dealing with domain-specific terminology and complex sentence structures.
- Core assumption: Legal texts benefit more from token-level understanding than from sentence-pair coherence tasks.
- Evidence anchors:
  - [abstract]: "replacing Next Sentence Prediction (NSP) task with Sentence Order Prediction (SOP) and combining Masked Language Model (MLM) with Term Frequency-Inverse Document Frequency (TF-IDF)."
  - [section 6.2]: "significant improvements are seen in both multi-label text classification and NER tasks...retaining MLM and removing NSP has shown better performance compared to the alternative of replacing NSP with SOP."
- Break condition: If the corpus contains many short, simple legal documents where sentence-pair relationships are informative, removing NSP entirely may reduce performance.

### Mechanism 3
- Claim: Replacing 10% of MLM-selected tokens with high TF-IDF valued tokens improves multi-label classification performance.
- Mechanism: TF-IDF tokens represent domain-specific keywords that are more informative for the legal domain. By replacing random tokens with these high-value tokens during MLM, the model learns to better recognize and utilize legal terminology.
- Core assumption: Legal domain tasks benefit from explicit exposure to domain-specific terminology during pre-training.
- Evidence anchors:
  - [abstract]: "combining Masked Language Model (MLM) with Term Frequency-Inverse Document Frequency (TF-IDF). In our innovative approach, we propose replacing 10% of the tokens selected for MLM with those having high TF-IDF values."
  - [section 6.3]: "utilizing tokens from a token list with high TF-IDF scores, instead of selecting tokens from the tokenizer, also yields positive results...in multi-label text classification, introducing more random tokens generally enhances performance."
- Break condition: If the corpus contains too few documents or if TF-IDF scores don't capture meaningful domain-specific terms, this approach may not provide benefit over random token replacement.

## Foundational Learning

- Concept: Bidirectional Encoder Representations from Transformers (BERT)
  - Why needed here: The entire study builds on BERT's architecture and pre-training methodology. Understanding how BERT works is fundamental to understanding the proposed modifications.
  - Quick check question: What are the two main pre-training tasks in BERT, and how do they differ from traditional language model pre-training?

- Concept: Self-supervised learning and transfer learning
  - Why needed here: The study leverages self-supervised pre-training on unlabeled legal text followed by fine-tuning on labeled downstream tasks. Understanding this paradigm is crucial for grasping the approach.
  - Quick check question: How does pre-training on a large corpus benefit downstream task performance compared to training only on task-specific data?

- Concept: Named Entity Recognition (NER) and multi-label text classification
  - Why needed here: These are the two downstream tasks used to evaluate the modified BERT models. Understanding their differences and evaluation metrics is essential for interpreting results.
  - Quick check question: What is the difference between multi-label classification and multi-class classification, and why is F-measure used as an evaluation metric?

## Architecture Onboarding

- Component map: PDF extraction -> sentence segmentation -> tokenization (BERTurk tokenizer) -> pre-training (5 modified BERT architectures + original BERT) -> fine-tuning (80/10/10 split) -> evaluation (Precision, Recall, F-measure comparison)
- Critical path: Corpus collection and preprocessing -> Model pre-training (from scratch) -> Fine-tuning on downstream tasks -> Performance evaluation and comparison
- Design tradeoffs: Single epoch training vs. multi-epoch training (resource constraints) | Smaller legal corpus vs. larger general corpus (domain specificity vs. data volume) | Modified pre-training tasks vs. further pre-training existing models (innovation vs. proven approach)
- Failure signatures: Low F-measure across all models (corpus quality issues or insufficient domain relevance) | High variance between runs (need for better hyperparameter tuning or more training data) | Similar performance across all modifications (modifications may not be impactful for this domain)
- First 3 experiments: 1) Run baseline: Pre-train original BERT on 50MB legal corpus, fine-tune on both tasks, record baseline F-measures | 2) Test SOP modification: Pre-train SOP_MLM_80_10_10 on 50MB corpus, compare F-measures to baseline | 3) Test MLM-only modification: Pre-train MLM_80_10_10 on 50MB corpus, compare F-measures to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LegalTurk Optimized BERT scale with corpus size beyond 2 GB, and is there an optimal corpus size for legal Turkish text?
- Basis in paper: [explicit] The authors note that increasing corpus size improves performance and hypothesize that even larger legal corpora could yield better results than BERTurk.
- Why unresolved: The study only tested up to 2 GB, leaving uncertainty about performance at larger scales.
- What evidence would resolve it: Pre-training LegalTurk models on progressively larger legal corpora (e.g., 5 GB, 10 GB) and comparing their performance against BERTurk and each other.

### Open Question 2
- Question: Does the LegalTurk Optimized BERT model maintain its performance advantage when applied to other Turkish legal NLP tasks such as question answering or binary text classification?
- Basis in paper: [explicit] The authors mention extending analysis to other downstream tasks but only tested NER and multi-label classification.
- Why unresolved: The study's scope was limited to two tasks, so generalizability to other legal NLP applications is unknown.
- What evidence would resolve it: Fine-tuning LegalTurk models on diverse legal NLP tasks and benchmarking against BERTurk and other baselines.

### Open Question 3
- Question: Would training LegalTurk Optimized BERT for multiple epochs instead of a single epoch lead to significant performance improvements?
- Basis in paper: [explicit] The authors acknowledge that all models were trained for only one epoch and suggest longer training as future work.
- Why unresolved: Single-epoch training may limit convergence, especially with smaller datasets.
- What evidence would resolve it: Conducting multi-epoch training for both pre-training and fine-tuning phases and measuring performance changes.

## Limitations

- Small corpus size (50 MB) may limit generalizability of results and doesn't provide definitive conclusions about proposed modifications
- Lack of ablation studies prevents isolation of individual contributions from each modification (SOP, MLM-only, TF-IDF replacement)
- Single epoch fine-tuning may not allow models to fully converge, potentially limiting observed performance gains
- TF-IDF approach may be specific to legal Turkish corpus and may not generalize to other domains or languages

## Confidence

**High Confidence**: Domain-specific pre-training improves performance on legal Turkish tasks (supported by comparison with BERTurk)
**Medium Confidence**: Specific modifications (SOP, MLM-only, TF-IDF token replacement) show improvements, but individual contributions remain uncertain
**Low Confidence**: Claims about generalization to other languages or domains are not supported by evidence

## Next Checks

1. **Ablation Study**: Replicate the experiments with isolated modifications (only SOP, only MLM-only, only TF-IDF replacement) on the same 50 MB corpus to quantify individual contributions and identify which modifications drive the most improvement.

2. **Corpus Size Scaling**: Test the models on progressively larger legal corpora (100 MB, 500 MB, 1 GB, 5 GB) to determine if the observed improvements scale with data size and identify potential saturation points for each modification.

3. **Cross-Domain Transfer**: Evaluate the best-performing model on non-legal Turkish text and legal text from other languages to assess the domain specificity of the improvements and determine whether the benefits come from legal domain knowledge or Turkish language modeling capabilities.