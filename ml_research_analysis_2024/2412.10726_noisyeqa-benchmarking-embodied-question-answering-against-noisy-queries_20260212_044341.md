---
ver: rpa2
title: 'NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries'
arxiv_id: '2412.10726'
source_url: https://arxiv.org/abs/2412.10726
tags:
- noise
- uni00000048
- questions
- noisy
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces NoisyEQA, a benchmark designed to evaluate\
  \ embodied question answering agents' ability to handle noisy human queries in real-world\
  \ scenarios. The authors define four types of noise\u2014latent hallucination, memory,\
  \ perception, and semantic\u2014and develop an automated framework to generate 500\
  \ noisy questions."
---

# NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries

## Quick Facts
- arXiv ID: 2412.10726
- Source URL: https://arxiv.org/abs/2412.10726
- Reference count: 40
- Key outcome: Introduces NoisyEQA benchmark and Self-Correction mechanism that significantly improves EQA agents' ability to handle noisy human queries

## Executive Summary
This paper addresses a critical gap in Embodied Question Answering (EQA) research by introducing NoisyEQA, a benchmark designed to evaluate agents' ability to handle noisy human queries in real-world scenarios. The authors define four types of noise - Latent Hallucination, Memory, Perception, and Semantic - and develop an automated framework to generate 500 noisy questions. They propose a Self-Correction mechanism with two variants (NAP and NACoT) that helps agents detect and correct noise before answering. Experiments show that standard EQA agents struggle significantly with noisy questions, but the Self-Correction mechanism substantially improves performance, with GPT-4o+NACoT achieving 77.6% DR and 35.0% CR compared to 46.2% DR and 17.4% CR for the baseline.

## Method Summary
The paper introduces a novel NoisyEQA benchmark that evaluates EQA agents' robustness to noisy human queries through a Self-Correction mechanism. The method involves generating 500 noisy questions using an LLM-powered framework based on OpenEQA and ExploreEQA datasets, with four defined noise types. The Self-Correction mechanism works by first selecting the most reliable view through confidence scoring, then applying either NAP (Noise Aware Prompt) or NACoT (Noise Aware Chain of Thought) to inspect questions for noise by comparing visual observations with the query. The approach is evaluated using an LLM-based scoring framework with custom metrics including Detection Rate (DR) and Correction Rate (CR), showing substantial improvements over baseline EQA agents.

## Key Results
- Standard EQA agents show poor performance on noisy questions, with baseline detection rates around 46% and correction rates around 17%
- Self-Correction mechanism with GPT-4o+NACoT achieves 77.6% Detection Rate and 35.0% Correction Rate
- Performance varies significantly by noise type, with Semantic and Perception noise being most challenging to detect and correct
- LLM-based evaluation framework shows strong correlation (0.909 Spearman) with human judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Self-Correction mechanism improves EQA agent robustness by prompting agents to detect noise in human queries before generating answers.
- Mechanism: The mechanism works by first asking the agent a "confidence question" to identify the most reliable view from the scene, then applying either NAP (Noise Aware Prompt) or NACoT (Noise Aware Chain of Thought) to inspect the question for noise by comparing visual observations with the query.
- Core assumption: Agents can effectively detect noise by comparing their visual observations with human questions when explicitly prompted to do so.
- Evidence anchors:
  - [abstract]: "we introduce a NoisyEQA benchmark designed to evaluate an agent's ability to recognize and correct noisy questions" and "we also propose a 'Self-Correction' prompting mechanism"
  - [section 4.1]: "a simple yet effective prompt is appended to the input of the agent, which reminds the agent to carefully review the question-related information before generating a response"
  - [corpus]: Weak - The corpus neighbors focus on EQA improvements but don't directly address noise detection mechanisms.
- Break condition: The mechanism breaks when noise involves subtle visual or semantic changes that require more complex reasoning than simple comparison, as evidenced by the low correction rates for Perception Noise and Semantic Noise even when detected.

### Mechanism 2
- Claim: NACoT provides more granular noise detection by systematically verifying each component of the question against visual content.
- Mechanism: NACoT deconstructs questions into key components (object identification, attribute verification, function/semantic verification) and prompts the agent to check each component's consistency with visual observations.
- Core assumption: Breaking down questions into systematic verification steps enables more thorough noise detection than simple prompting.
- Evidence anchors:
  - [section 4.2]: "NACoT deconstructs a specific question into key components, and systematically prompts the agent to verify whether each component is consistent with the visual content"
  - [abstract]: "we introduce a Self-Correction mechanism with two variants (NAP and NACoT)"
  - [corpus]: Weak - The corpus doesn't provide evidence about systematic verification approaches for noise detection.
- Break condition: The mechanism breaks when the verification process becomes too complex for the agent's reasoning capabilities, particularly for nuanced semantic noise.

### Mechanism 3
- Claim: The LLM-based evaluation framework provides reliable assessment of noise detection and correction capabilities.
- Mechanism: The framework uses an evaluation scale (1-5) assessing noise detection, noise correction, and accurate generation, with LLM-Match accuracy measuring overall response quality and Detection Rate/Correction Rate specifically measuring noise handling.
- Core assumption: LLMs can reliably score agent responses on noise detection and correction when provided with ground truth comparisons.
- Evidence anchors:
  - [section 5]: "we elaborately design an evaluation scale based on all three aspects and employ a Large Language Model (LLM) for automatic assessment"
  - [section 6.1]: "the LLM-Match accuracy is adopted to evaluate the overall quality of all responses"
  - [section 3.2]: "LLM-Match achieves an impressive Spearman correlation coefficient of 0.909 to human evaluations"
- Break condition: The mechanism breaks when LLM scoring diverges significantly from human judgment, though the evidence suggests strong correlation.

## Foundational Learning

- Concept: Vision-Language Models (VLMs)
  - Why needed here: VLMs form the backbone of EQA agents, enabling them to process both visual inputs and natural language queries simultaneously.
  - Quick check question: Can you explain how VLMs differ from traditional computer vision models in processing multimodal information?

- Concept: Embodied Question Answering (EQA)
  - Why needed here: Understanding EQA is crucial as it involves agents actively exploring environments to answer questions, unlike static image-based question answering.
  - Quick check question: What are the key differences between EQA and traditional Visual Question Answering (VQA)?

- Concept: Noise types in human queries
  - Why needed here: The paper defines four specific noise types (Latent Hallucination, Memory, Perception, Semantic) that are critical for understanding the benchmark's design and the self-correction mechanism's targets.
  - Quick check question: Can you differentiate between Perception Noise and Semantic Noise in the context of human queries?

## Architecture Onboarding

- Component map: EQA agent (Llama2-EQA or GPT-4o-EQA) -> View selection (confidence scoring) -> Self-Correction (NAP/NACoT) -> Response generation -> LLM evaluation
- Critical path: Noisy question → View selection (confidence scoring) → Self-Correction (NAP/NACoT) → Response generation → LLM evaluation
- Design tradeoffs: The system trades computational overhead (additional confidence checking and systematic verification) for improved noise robustness. The choice between NAP and NACoT involves a tradeoff between simplicity and thoroughness.
- Failure signatures: Poor performance on Perception and Semantic Noise despite high detection rates indicates the system struggles with nuanced visual/semantic distinctions. Low correction rates even when noise is detected suggest reasoning limitations.
- First 3 experiments:
  1. Run baseline agent on clean questions to establish performance baseline
  2. Run baseline agent on each noise type separately to identify specific weaknesses
  3. Apply NAP to each noise type to measure detection rate improvements before testing NACoT

## Open Questions the Paper Calls Out
No specific open questions are called out in the paper.

## Limitations
- Reliance on simulated environments and controlled noise injection may not capture real-world query complexity
- Limited testing conditions restrict generalizability to truly uncontrolled real-world settings
- Potential systematic biases in LLM-based automated scoring not fully addressed

## Confidence
- Benchmark design and evaluation methodology: High
- Effectiveness of Self-Correction mechanism: Medium
- Generalizability to real-world scenarios: Low

## Next Checks
1. **Cross-domain validation**: Test the Self-Correction mechanism on EQA agents operating in significantly different environments (e.g., outdoor settings, office spaces) to assess generalizability beyond the house exploration domain.

2. **Human evaluation expansion**: Conduct a larger-scale human evaluation study with diverse annotators to validate the LLM-based scoring system's reliability across different cultural contexts and language backgrounds.

3. **Real-world deployment pilot**: Implement a small-scale pilot study where the EQA agents with Self-Correction are deployed in a real household for a week, collecting and analyzing actual human queries to assess performance in authentic noisy query scenarios.