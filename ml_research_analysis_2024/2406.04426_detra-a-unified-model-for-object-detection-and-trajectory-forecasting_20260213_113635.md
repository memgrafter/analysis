---
ver: rpa2
title: 'DeTra: A Unified Model for Object Detection and Trajectory Forecasting'
arxiv_id: '2406.04426'
source_url: https://arxiv.org/abs/2406.04426
tags:
- object
- detection
- forecasting
- attention
- lidar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DETra unifies object detection and trajectory forecasting as a
  trajectory refinement task, representing object poses and multi-modal future trajectories
  with learnable queries that are iteratively refined through attention to LiDAR point
  clouds and HD maps. The model outperforms state-of-the-art baselines on Argoverse
  2 Sensor and Waymo Open Dataset, achieving 73.0 AP on Argoverse 2 Sensor and 70.4
  AP on Waymo Open Dataset.
---

# DeTra: A Unified Model for Object Detection and Trajectory Forecasting

## Quick Facts
- **arXiv ID**: 2406.04426
- **Source URL**: https://arxiv.org/abs/2406.04426
- **Reference count**: 40
- **Key outcome**: DETRA unifies object detection and trajectory forecasting as a trajectory refinement task, achieving 73.0 AP on Argoverse 2 Sensor and 70.4 AP on Waymo Open Dataset.

## Executive Summary
DETra proposes a unified model for object detection and trajectory forecasting in autonomous driving. It formulates these tasks as a trajectory refinement problem, using learnable queries to represent object poses and future trajectories. The model iteratively refines these queries through attention to LiDAR point clouds and HD maps, outperforming state-of-the-art baselines on standard benchmarks.

## Method Summary
DETra represents object poses and multi-modal future trajectories with learnable queries that are iteratively refined through attention to LiDAR point clouds and HD maps. The model uses deformable LiDAR attention and k-NN map attention for efficient cross-attention, along with factorized self-attention across object, time, and mode dimensions. It is trained end-to-end with a combination of focal loss, IoU loss, Laplacian loss, and cross-entropy.

## Key Results
- Achieves 73.0 AP on Argoverse 2 Sensor dataset
- Achieves 70.4 AP on Waymo Open Dataset
- Outperforms state-of-the-art baselines on joint detection and forecasting metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DETRA avoids compounding errors by unifying detection and forecasting into a single trajectory refinement task rather than cascading them.
- **Mechanism**: By predicting a trajectory for each object that includes both the current pose (detection) and future waypoints (forecasting), errors in one step do not propagate to the next. Refinement transformer blocks iteratively update both poses and object queries, allowing joint optimization.
- **Core assumption**: A trajectory-based formulation is more robust than a cascaded detection-then-forecasting approach for self-driving perception and planning.
- **Evidence anchors**:
  - [abstract]: "our approach formulates the union of the two tasks as a trajectory refinement problem"
  - [section]: "Traditional autonomous systems tackle object detection and trajectory forecasting as separate tasks, connecting them in a cascaded fashion through tracking... this cascading decomposition suffers from compounding errors."
- **Break condition**: If the trajectory refinement objective is not properly balanced between detection and forecasting, or if refinement blocks fail to propagate useful information, performance degrades.

### Mechanism 2
- **Claim**: Factorized self-attention across object, time, and mode dimensions enables efficient and expressive modeling of multi-object, multi-modal, and multi-time interactions.
- **Mechanism**: Instead of attending over all queries at once, DETRA restricts attention to specific axes: time self-attention only attends to the same object and mode across time steps; mode self-attention attends to the same object and time across modes; object self-attention attends to the same time and mode across objects.
- **Core assumption**: Factorizing attention is a good inductive bias that improves learning efficiency and prevents interference between unrelated query dimensions.
- **Evidence anchors**:
  - [section]: "For efficiency purposes, we factorize self-attention into time self-attention, mode self-attention and object self-attention."
- **Break condition**: If the factorization prevents necessary interactions (e.g., if temporal dependencies require cross-mode information), performance may suffer.

### Mechanism 3
- **Claim**: Local attention mechanisms (deformable LiDAR attention and k-NN map attention) make cross-attention computationally feasible while focusing on relevant spatial regions.
- **Mechanism**: For LiDAR, DETRA predicts offsets from object poses and attends only to a small local neighborhood around each reference point; for HD maps, it attends only to the k nearest map tokens per object.
- **Core assumption**: The most relevant features for an object are spatially close to its pose in LiDAR point clouds and nearby in the HD map graph.
- **Evidence anchors**:
  - [section]: "Since the number of LiDAR tokens Nlidar is considerable and fully connected attention brings unaffordable memory consumption... we follow [71] and utilize deformable attention" and "we limit the cross-attention to the k nearest map tokens from the object pose, reducing the computation from O(N Nmap) to O(N k)".
- **Break condition**: If relevant features lie outside the local neighborhood, the model will miss them, harming performance.

## Foundational Learning

- **Concept**: Transformer attention mechanisms
  - Why needed here: DETRA relies on cross-attention to fuse LiDAR and map features with object queries and self-attention to model interactions between queries.
  - Quick check question: What is the difference between cross-attention and self-attention in a transformer block?

- **Concept**: Multi-object tracking and tracking metrics
  - Why needed here: DETRA is evaluated on joint detection and forecasting metrics that require understanding object tracking concepts like IOU, recall, and false positives/negatives.
  - Quick check question: How does Average Precision (AP) differ from Average Recall (AR) in object detection?

- **Concept**: Spatio-temporal reasoning and trajectory forecasting
  - Why needed here: The core task is to predict future object trajectories from sensor data, requiring modeling of motion patterns and interactions over time.
  - Quick check question: What is the difference between a deterministic and a probabilistic trajectory forecast?

## Architecture Onboarding

- **Component map**: LiDAR encoder → Map encoder → Query initialization → Refinement transformer (B blocks, each with deformable LiDAR attention, k-NN map attention, time self-attention, mode self-attention, object self-attention, and pose update) → Detection and forecasting outputs
- **Critical path**: LiDAR and map features → cross-attention → self-attention → pose update → repeat for B blocks → final outputs
- **Design tradeoffs**: Global attention is computationally prohibitive but local attention may miss relevant features; factorized self-attention is efficient but may limit some interactions; multiple refinement blocks improve accuracy but increase latency.
- **Failure signatures**: Poor detection localization (low AP@IoU0.7), off-map trajectory forecasts, high miss rate, or failure to converge during training.
- **First 3 experiments**:
  1. Train with B=1 refinement block to verify basic functionality before scaling up.
  2. Compare global vs. deformable LiDAR attention to confirm the local attention design choice.
  3. Evaluate with T=1 and F=1 to isolate the effect of temporal and multi-modal modeling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal number of refinement blocks for DETRA to balance performance and computational efficiency?
- **Basis in paper**: [explicit] The paper mentions that DETRA's performance improves from 1 to 3 refinement blocks but then plateaus, and suggests that different training recipes, hyperparameters, and amounts of data would be necessary to scale DETRA.
- **Why unresolved**: The paper only provides results for up to 5 refinement blocks and does not explore the computational efficiency of using more blocks.
- **What evidence would resolve it**: Experiments testing DETRA with a wider range of refinement blocks, including very high numbers, while measuring both performance and computational efficiency.

### Open Question 2
- **Question**: How does the ordering of attention layers within cross-attention (LiDAR and map) and self-attention (time, mode, object) impact DETRA's performance?
- **Basis in paper**: [explicit] The paper shows that the order within cross-attention and self-attention matters little, but the selected order of lidar, map, time, future, and object is the best in most metrics.
- **Why unresolved**: The paper only provides results for a few specific orderings and does not explore the full space of possible orderings.
- **What evidence would resolve it**: Experiments testing DETRA with all possible orderings of the attention layers, or a more comprehensive search of the ordering space.

### Open Question 3
- **Question**: How does the performance of DETRA change with different query volume dimensions, such as varying the number of time steps or modes?
- **Basis in paper**: [explicit] The paper shows that the time dimension T = 10 is crucial for good trajectory forecasting performance, and the mode dimension F = 6 is also important for trajectory forecasting performance, particularly at K = 6.
- **Why unresolved**: The paper only provides results for a few specific query volume dimensions and does not explore the full space of possible dimensions.
- **What evidence would resolve it**: Experiments testing DETRA with a wider range of query volume dimensions, including very high numbers of time steps and modes, while measuring performance.

## Limitations
- Limited evaluation to Argoverse 2 Sensor and Waymo Open Dataset, without testing on other datasets or real-world conditions.
- No evaluation of robustness to sensor failures or degraded HD map quality.
- The exact contribution of each component to overall performance is not fully isolated.

## Confidence
- **High Confidence**: The mechanism of factorized self-attention and local attention mechanisms (deformable LiDAR attention and k-NN map attention) is well-supported by the ablation studies and computational analysis.
- **Medium Confidence**: The claim that DETRA significantly outperforms cascaded approaches is supported by benchmark results, but the exact contribution of each component to the overall performance gain is not fully isolated.
- **Low Confidence**: The paper's assertion that DETRA generalizes well to unseen scenarios is not empirically tested beyond the two primary datasets.

## Next Checks
1. Evaluate DETRA on additional datasets such as nuScenes or KITTI to assess its performance across different sensor configurations and driving environments.
2. Test DETRA's performance under simulated sensor failures (e.g., missing LiDAR sweeps) and degraded HD map quality to validate its robustness in real-world conditions.
3. Conduct a detailed ablation study isolating the contribution of each attention mechanism (deformable LiDAR, k-NN map, and factorized self-attention) to the overall performance.