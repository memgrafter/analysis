---
ver: rpa2
title: Retrofitting Temporal Graph Neural Networks with Transformer
arxiv_id: '2409.05477'
source_url: https://arxiv.org/abs/2409.05477
tags:
- temporal
- training
- graph
- sampling
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TF-TGN adapts temporal graph neural networks (TGNNs) to Transformer
  decoder architecture by formulating temporal message aggregation as sequence modeling
  using suffix infilling and temporal graph attention with self-loop operations. The
  approach unifies TGNN training paradigms and leverages Transformer's efficient codebase
  including flash-attention, memory-efficient attention, and distributed training
  schemes.
---

# Retrofitting Temporal Graph Neural Networks with Transformer

## Quick Facts
- arXiv ID: 2409.05477
- Source URL: https://arxiv.org/abs/2409.05477
- Authors: Qiang Huang, Xiao Yan, Xin Wang, Susie Xi Rao, Zhichao Han, Fangcheng Fu, Wentao Zhang, Jiawei Jiang
- Reference count: 40
- Primary result: Achieves SOTA performance on dynamic link prediction with 2.20× average training speedup and up to 10.31× acceleration

## Executive Summary
This paper presents TF-TGN, a novel framework that adapts temporal graph neural networks (TGNNs) to Transformer decoder architecture by reformulating temporal message aggregation as sequence modeling using suffix infilling. The approach leverages causal masking self-attention to preserve temporal causality while benefiting from Transformer's efficient codebase including flash-attention, memory-efficient attention, and distributed training schemes. The framework demonstrates significant performance improvements on dynamic link prediction tasks across 9 real-world temporal graphs with billions of edges.

## Method Summary
TF-TGN converts temporal message aggregation into sequence modeling by arranging chronological temporal neighbors in suffix infilling format, enabling the use of Transformer decoder's causal masking self-attention. The framework employs parallelized CSR format conversion and graph sampling strategies to accelerate data preprocessing, while directly leveraging optimized Transformer attention kernels and distributed training capabilities. The temporal graph attention with self-loop operations preserves node temporal features while enabling efficient computation through existing Transformer optimization techniques.

## Key Results
- Achieves state-of-the-art performance on dynamic link prediction across 9 real-world temporal graphs
- Accelerates training by 2.20× on average and up to 10.31× compared to existing TGNN frameworks
- Speeds up CSR format conversion by 1466.45× and sampling by 16.9× using parallel strategies
- Handles graphs with up to billions of edges while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
The temporal message aggregation in TGNNs can be structured as sequence modeling using suffix infilling. Nodes in temporal graphs occur chronologically, allowing temporal neighbors to be arranged in sequence. By suffix infilling the current node at the end of its temporal neighbors, the message aggregation operation becomes equivalent to predicting the next element in a sequence. This works under the assumption that chronological ordering is sufficient for modeling temporal dependencies.

### Mechanism 2
The Transformer decoder's causal masking self-attention mechanism aligns with temporal message aggregation requirements. Causal masking allows each element to attend only to itself and preceding elements, matching the forward-time nature of temporal graphs. This enables the Transformer decoder to aggregate temporal neighbor features while preserving causality. The mechanism assumes temporal direction in graphs corresponds to sequence order in Transformer attention.

### Mechanism 3
Leveraging Transformer's optimized codebase significantly accelerates TGNN training. By adapting TGNNs to Transformer architecture, TF-TGN can directly use highly optimized attention kernels and distributed training schemes developed for language modeling, reducing computational overhead. This assumes the computational patterns of temporal message aggregation are sufficiently similar to sequence modeling to benefit from Transformer optimizations.

## Foundational Learning

- **Temporal graph representation and temporal neighbor sampling**: Needed to understand how temporal graphs are structured and how to efficiently sample temporal neighbors for sequence modeling. Quick check: What is the difference between static and temporal neighbor sampling, and why does temporal sampling require T-CSR format conversion?

- **Transformer decoder architecture and attention mechanisms**: Required to understand how TF-TGN adapts temporal message aggregation to Transformer decoder's causal masking self-attention. Quick check: How does causal masking in Transformer decoder differ from standard self-attention, and why is this important for temporal graph modeling?

- **Sequence modeling and autoregressive prediction**: Essential for understanding the core insight that temporal message aggregation resembles sequence modeling. Quick check: What is the relationship between sequence modeling in language tasks and temporal message aggregation in TGNNs?

## Architecture Onboarding

- **Component map**: CSR format conversion and temporal neighbor sampling (parallelized) → Sequence formatting (suffix infilling) → Transformer decoder with temporal graph attention and self-loop → Prediction → Loss computation
- **Critical path**: Temporal interactions → CSR conversion → Neighbor sampling → Sequence formatting (suffix infilling) → Transformer attention → Prediction → Loss computation
- **Design tradeoffs**: Sequence length vs. computational efficiency; sampling number vs. performance; batch size vs. GPU utilization
- **Failure signatures**: Slow CSR conversion or sampling indicates inefficient parallelization; degraded accuracy suggests issues with temporal neighbor sampling or sequence formatting; memory errors indicate sequence length or batch size settings are too large
- **First 3 experiments**: 
  1. Verify CSR conversion speedup: Compare TGL's CSR conversion time vs. Algorithm 1 on a medium-sized dataset
  2. Validate sequence modeling equivalence: Compare TF-TGN's attention outputs with traditional TGAT on a small dataset
  3. Benchmark training speedup: Measure end-to-end training time per epoch on a large dataset with varying GPU counts

## Open Questions the Paper Calls Out

### Open Question 1
How does the suffix infilling operation in TF-TGN compare to other infilling methods (like prefix-suffix-middle) in terms of model performance and efficiency for temporal graph tasks? The paper mentions suffix infilling is used but doesn't conduct ablation studies comparing different infilling strategies or evaluate whether other infilling methods could improve performance on temporal graph tasks.

### Open Question 2
What is the optimal sampling number for different types of temporal graphs, and how does it vary with graph characteristics like density, temporal dynamics, and task requirements? While the paper observes variation in optimal sampling numbers across datasets, it doesn't provide systematic analysis of what graph characteristics drive these differences or develop predictive models for choosing sampling numbers.

### Open Question 3
How does TF-TGN's performance scale with extremely large temporal graphs containing billions or trillions of edges, and what are the practical limits of the parallel sampling strategy? The paper demonstrates effectiveness on graphs with up to billions of edges but doesn't test scalability beyond this point or analyze theoretical scaling limits of the parallel sampling strategy.

## Limitations

- The suffix infilling approach assumes temporal causality is strictly forward-directed, which may not capture all temporal dependencies in graphs where future events critically inform past predictions
- The computational characteristics of temporal graph attention may differ sufficiently from language modeling to prevent full optimization benefits from Transformer's codebase
- The parallel sampling strategy may introduce implementation complexity that limits practical deployment, despite exceptional speedup claims

## Confidence

- **High Confidence**: Training speedup results (2.20× average, up to 10.31×) are well-supported by experimental evidence across 9 datasets
- **Medium Confidence**: Sequence modeling equivalence between temporal message aggregation and suffix infilling is conceptually sound but may not generalize to all temporal graph patterns
- **Low Confidence**: Claim that all TGNN training paradigms can be unified under TF-TGN framework requires more evidence and limited exploration of different TGNN variants

## Next Checks

1. **Bidirectional Temporal Dependency Test**: Evaluate TF-TGN on a temporal graph task where future events influence past predictions (such as traffic flow forecasting with loop closures) to quantify accuracy loss from unidirectional sequence modeling approach.

2. **Optimization Component Ablation**: Systematically disable flash-attention, memory-efficient attention, and distributed training components individually while measuring training speed and accuracy to isolate which optimizations contribute most to reported speedups.

3. **Cross-Domain Generalization Test**: Apply TF-TGN to temporal graph datasets from domains outside the paper's evaluation (such as biological interaction networks or financial transaction graphs) to assess the framework's generalizability claims.