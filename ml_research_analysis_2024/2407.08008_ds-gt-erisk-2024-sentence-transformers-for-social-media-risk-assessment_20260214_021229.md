---
ver: rpa2
title: 'DS@GT eRisk 2024: Sentence Transformers for Social Media Risk Assessment'
arxiv_id: '2407.08008'
source_url: https://arxiv.org/abs/2407.08008
tags:
- task
- data
- text
- sentence
- eating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The team developed NLP systems for mental health risk assessment
  from social media data, focusing on depression symptom detection (Task 1) and eating
  disorder severity prediction (Task 3). For Task 1, they attempted to rank depression-relevant
  documents using binary classifiers trained on BDI-II questionnaire relevance, but
  found classifiers poorly calibrated for ranking tasks, resulting in poor leaderboard
  performance.
---

# DS@GT eRisk 2024: Sentence Transformers for Social Media Risk Assessment

## Quick Facts
- arXiv ID: 2407.08008
- Source URL: https://arxiv.org/abs/2407.08008
- Reference count: 20
- Primary result: BERT embeddings with classical ML models achieved MAE of 1.965 and top 5 ranking among 5 teams for eating disorder severity prediction

## Executive Summary
The DS@GT team developed NLP systems for mental health risk assessment from social media data, focusing on depression symptom detection (Task 1) and eating disorder severity prediction (Task 3). They found that while sentence transformers are effective for text representation in mental health applications, binary classifiers trained for classification tasks are poorly calibrated for ranking tasks. For eating disorder prediction, BERT embeddings combined with classical ML models achieved competitive results, while their depression symptom detection approach using repurposed classifiers showed a significant gap between internal validation and leaderboard performance.

## Method Summary
The team employed two distinct approaches for the eRisk 2024 challenge. For Task 1 (depression symptom detection), they trained binary classifiers using logistic regression for each BDI-II question, optimizing for F1 scores and accuracy using relevance labels, then repurposed the predicted probabilities for document ranking. For Task 3 (eating disorder severity prediction), they used BERT-base-uncased to generate 768-dimensional embeddings for user posts, reduced these to 50 dimensions via PCA, and fed them into classical ML models including Random Forest, Extra Trees, and XGBoost to predict EDE-Q scores. They found that sentence transformers (all-MiniLM-L6-v2) outperformed traditional methods like count vectorizers and Word2Vec for depression symptom detection.

## Key Results
- Task 1 ranking system using binary classifiers showed poor leaderboard performance despite high internal validation (89% mean F1, 90% mean accuracy)
- Task 3 eating disorder severity prediction achieved MAE of 1.965, ranking in top 5 among 5 teams
- BERT embeddings with classical ML models performed competitively for eating disorder prediction
- Sentence transformers were identified as a powerful tool for downstream modeling in mental health applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binary classifiers trained on BDI-II question relevance can be repurposed for document ranking in depression symptom detection
- Mechanism: The team trained logistic regression classifiers for each BDI-II question using binary relevance labels, then used the predicted probabilities to rank documents by their relevance to depression symptoms
- Core assumption: Classifier probability outputs are well-calibrated and monotonic with document relevance
- Evidence anchors:
  - [abstract] "We propose a ranking system for Task 1 that predicts symptoms of depression based on the Beck Depression Inventory (BDI-II) questionnaire using binary classifiers trained on question relevancy as a proxy for ranking"
  - [section] "We utilized logistic regression to make binary relevance predictions for each symptom in the questionnaire, optimizing for F1 scores and accuracy using the relevance labels from the QRELS"
  - [corpus] Weak - only 2 related papers mention ranking/classification approaches but no direct evidence of classifier-to-ranker conversion success
- Break condition: Classifier probability outputs are not well-calibrated for ranking (as evidenced by the team's poor leaderboard performance despite high internal validation scores)

### Mechanism 2
- Claim: BERT embeddings capture meaningful semantic patterns for eating disorder severity prediction
- Mechanism: The team used BERT-base-uncased to generate 768-dimensional embeddings for user posts, which were then reduced to 50 dimensions via PCA and fed into classical ML models (Random Forest, Extra Trees, XGBoost) to predict EDE-Q scores
- Core assumption: The semantic information captured in BERT embeddings is sufficient and relevant for predicting eating disorder severity
- Evidence anchors:
  - [abstract] "For Task 3, we use embeddings from BERT to predict the severity of eating disorder symptoms based on user post history"
  - [section] "We leveraged BERT to generate text embeddings... The goal of task 3 was to predict the responses of the 2024 users to the EDE-Q questionnaire"
  - [corpus] Weak - corpus neighbors mention transformers and sentence embeddings but no direct evidence of BERT-based severity prediction for eating disorders
- Break condition: BERT embeddings fail to capture disorder-specific semantic patterns, or the reduced dimensions lose critical information

### Mechanism 3
- Claim: Sentence transformers provide superior text representation for mental health applications compared to traditional NLP methods
- Mechanism: The team used all-MiniLM-L6-v2 sentence transformer model to generate embeddings, which they found achieved 89% mean F1 and 90% mean accuracy on internal validation for Task 1, outperforming count vectorizers and Word2Vec
- Core assumption: Sentence transformer embeddings capture richer semantic relationships than traditional methods
- Evidence anchors:
  - [abstract] "Representation of text data is crucial in both tasks, and we find that sentence transformers are a powerful tool for downstream modeling"
  - [section] "We find that binary classifiers are not well calibrated for ranking, and perform poorly during evaluation. For Task 3, we use embeddings from BERT to predict the severity of eating disorder symptoms based on user post history"
  - [corpus] Weak - corpus neighbors mention sentence transformers but no direct comparison evidence between sentence transformers and traditional methods for mental health applications
- Break condition: Sentence transformers do not capture task-relevant semantic information, or traditional methods perform equally well with less computational cost

## Foundational Learning

- Concept: Difference between classification and ranking tasks
  - Why needed here: The team initially treated ranking as classification, leading to poor performance. Understanding the distinction is crucial for proper task formulation
  - Quick check question: If a classifier is optimized for F1 score, why might its probability outputs be poorly calibrated for ranking documents by relevance?

- Concept: Text embedding dimensionality and PCA
  - Why needed here: The team reduced BERT's 768-dimensional embeddings to 50 dimensions for computational efficiency and potential performance gains
  - Quick check question: What information might be lost when reducing 768-dimensional BERT embeddings to 50 dimensions, and how could this affect model performance?

- Concept: Multi-label classification for questionnaire prediction
  - Why needed here: Task 3 required predicting 22 different EDE-Q question responses simultaneously, requiring understanding of multi-output classification approaches
  - Quick check question: How does multi-label classification differ from multi-class classification, and what are the implications for evaluation metrics?

## Architecture Onboarding

- Component map: Data preprocessing -> Text embedding generation (sentence transformers/BERT) -> Dimensionality reduction (PCA) -> Classical ML model training (Random Forest, Extra Trees, XGBoost, etc.) -> Prediction and evaluation
- Critical path: The embedding generation and dimensionality reduction steps are critical, as poor embeddings or inappropriate dimensionality reduction will cascade through to poor final predictions
- Design tradeoffs: High-dimensional embeddings capture more information but are computationally expensive and may overfit; low-dimensional embeddings are efficient but may lose important information
- Failure signatures: Poor leaderboard performance despite good internal validation suggests calibration issues or overfitting; consistently low scores across all metrics suggests fundamental representation problems
- First 3 experiments:
  1. Compare sentence transformer embeddings vs. traditional methods (count vectorizer, TF-IDF, Word2Vec) on a small validation set to verify the claimed superiority
  2. Test different dimensionality reduction techniques (PCA vs. t-SNE vs. UMAP) on the BERT embeddings to find the optimal balance between information retention and computational efficiency
  3. Implement a proper learning-to-rank approach for Task 1 instead of repurposed classifiers to verify if ordinal regression methods perform better than binary classification for ranking tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do binary classifiers perform poorly when repurposed for ranking tasks in depression symptom detection?
- Basis in paper: [explicit] The paper states "We find that binary classifiers are not well calibrated for ranking, and perform poorly during evaluation" and discusses this finding in the Task 1 results section.
- Why unresolved: The paper only observes this phenomenon but does not investigate the underlying reasons for the poor calibration or provide detailed analysis of why classification-based ranking fails.
- What evidence would resolve it: Detailed analysis of classifier calibration curves, examination of the relationship between classification confidence scores and ranking quality, and comparison with proper ordinal regression or learning-to-rank approaches would help explain this performance gap.

### Open Question 2
- Question: What is the optimal dimensionality for BERT embeddings when predicting eating disorder severity scores?
- Basis in paper: [inferred] The paper compares high-dimensional (768) versus reduced (50) dimensional embeddings using PCA and finds different models perform better at different dimensions, suggesting dimensionality is a key factor.
- Why unresolved: The paper only tests one reduction to 50 dimensions and does not explore the full range of possible dimensions or determine if there's an optimal trade-off between information retention and model performance.
- What evidence would resolve it: Systematic evaluation of models across multiple dimensionalities (e.g., 50, 100, 200, 300, 400, 768) with consistent hyperparameter tuning would identify optimal dimensionality for this task.

### Open Question 3
- Question: How can the pre-filtering process be improved to better identify relevant documents for depression symptom detection?
- Basis in paper: [explicit] The paper mentions they "had to aggressively filter out low-quality sentences" and discusses using compression ratios and simpler model predictions for filtering, but notes this was insufficient.
- Why unresolved: The paper identifies the need for better filtering but does not propose or test specific alternative filtering strategies beyond what was implemented.
- What evidence would resolve it: Evaluation of different filtering approaches such as keyword-based information retrieval, semantic similarity thresholds, or hybrid filtering methods would demonstrate which strategies most effectively improve ranking performance.

## Limitations
- Binary classifiers are poorly calibrated for ranking tasks, leading to significant performance gaps between internal validation and leaderboard results
- Weak corpus evidence for claimed superiority of sentence transformers over traditional methods
- Limited exploration of dimensionality reduction techniques and their impact on model performance

## Confidence

- **High Confidence**: The use of BERT embeddings for text representation and classical ML models (Random Forest, XGBoost) for prediction is well-established and empirically validated in the results
- **Medium Confidence**: The claim that sentence transformers outperform traditional methods for mental health applications, based on internal validation but lacking strong external validation
- **Low Confidence**: The repurposing of binary classifiers for ranking tasks, as evidenced by the significant performance gap between internal validation and leaderboard results

## Next Checks

1. **Calibration Analysis**: Perform detailed analysis of classifier probability calibration for Task 1 using metrics like Brier score, reliability diagrams, and Expected Calibration Error to understand why internal validation scores differ from leaderboard performance

2. **Ablation Study**: Conduct systematic ablation studies comparing sentence transformers against traditional methods (TF-IDF, Word2Vec) and different embedding dimensions on a held-out validation set to verify the claimed superiority

3. **Ranking Method Comparison**: Implement and compare proper learning-to-rank methods (pointwise, pairwise, listwise approaches) against the repurposed classifier approach for Task 1 to determine if ordinal regression methods would have performed better