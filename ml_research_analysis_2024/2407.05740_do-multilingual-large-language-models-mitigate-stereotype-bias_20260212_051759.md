---
ver: rpa2
title: Do Multilingual Large Language Models Mitigate Stereotype Bias?
arxiv_id: '2407.05740'
source_url: https://arxiv.org/abs/2407.05740
tags:
- bias
- language
- multilingual
- linguistics
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether multilingual training reduces\
  \ stereotype bias in large language models. Six 2.6B parameter models are trained\u2014\
  five monolingual (English, German, French, Italian, Spanish) and one multilingual\
  \ model trained on equal token distribution across these languages\u2014all from\
  \ scratch on web data."
---

# Do Multilingual Large Language Models Mitigate Stereotype Bias?

## Quick Facts
- **arXiv ID**: 2407.05740
- **Source URL**: https://arxiv.org/abs/2407.05740
- **Reference count**: 40
- **Key outcome**: Multilingual models consistently exhibit less bias than monolingual models across all tested languages and benchmarks

## Executive Summary
This paper investigates whether multilingual training reduces stereotype bias in large language models by training six 2.6B parameter models from scratch - five monolingual (English, German, French, Italian, Spanish) and one multilingual model with equal token distribution across these languages. The models are evaluated on translated bias benchmarks (CrowS-Pairs and BBQ) using human-verified translation quality. Results demonstrate that multilingual models exhibit significantly less bias than their monolingual counterparts and outperform open-source 7B models across all languages, while also achieving higher prediction accuracy. The study provides evidence that multilingual training can effectively mitigate stereotype bias through exposure to diverse cultural perspectives.

## Method Summary
The study trains six 2.6B parameter decoder-only transformer models from scratch on web data, with five monolingual models trained on individual languages and one multilingual model trained on 20% token distribution from each language. Bias benchmarks CrowS-Pairs and BBQ are translated into all five target languages and validated by human annotators for translation quality and bias preservation. Models are evaluated using zero-shot inference on the translated benchmarks, with bias scores calculated and compared across monolingual and multilingual variants. The methodology controls for model size while varying language exposure, allowing isolation of the multilingual training effect on bias reduction.

## Key Results
- Multilingual models show consistently lower bias scores than monolingual models across all five languages on both CrowS-Pairs and BBQ benchmarks
- Multilingual models achieve higher prediction accuracy than monolingual models, particularly on ambiguous contexts
- Multilingual 2.6B models outperform open-source 7B models (Falcon-7B, Llama2-7B, Mistral-7B) on bias metrics while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual training reduces stereotype bias by exposing the model to diverse cultural perspectives that dilute dominant-group stereotypes.
- Mechanism: When a model is trained on equal token distribution across five languages, it encounters a broader range of social norms and representations, preventing over-representation of any single cultural stereotype.
- Core assumption: Each language's training data contains unique, culturally-specific representations of social groups that differ from the others.
- Evidence anchors:
  - [abstract] "multilingual model trained on an equal distribution of data across these languages"
  - [section] "multilingual model does much better than the monolingual models" in ambiguous contexts
  - [corpus] corpus contains 52 billion tokens total, with 20% from each language for multilingual model
- Break condition: If training data from different languages contains the same dominant stereotypes, the dilution effect disappears.

### Mechanism 2
- Claim: Equal token distribution across languages ensures balanced exposure to different social group representations.
- Mechanism: By sampling 20% from each monolingual corpus for multilingual training, the model receives comparable representation of each language's social groups, preventing dominance of any single group's stereotypes.
- Core assumption: The monolingual corpora have similar total token counts and represent comparable social diversity.
- Evidence anchors:
  - [section] "20% of each monolingual training corpus" used for multilingual model
  - [section] "same number of tokens" across all models
  - [corpus] Table 5 shows similar total token counts across languages
- Break condition: If one language's corpus is much larger or contains significantly more stereotype examples, balance is lost.

### Mechanism 3
- Claim: Translation quality and bias preservation ensure fair comparison across languages.
- Mechanism: Human-validated automatic translation with high quality ratings (DeepL average score ~50.3/60) ensures that bias benchmarks accurately represent stereotypes in each language.
- Core assumption: Translation preserves the intended bias while maintaining natural language expression.
- Evidence anchors:
  - [section] "translation quality of DeepL was rated better, with a higher margin for French and German"
  - [section] "bias was rated by the annotators in a translation sample as equal to the English original in most cases"
  - [corpus] "inter-annotator agreement by Cohen's Kappa" shows moderate to fair agreement
- Break condition: If translation introduces new biases or removes existing ones, the comparison becomes invalid.

## Foundational Learning

- Concept: Token distribution and its effect on model training
  - Why needed here: Understanding how equal token distribution across languages affects bias requires grasping how training data quantity and diversity impact learned representations
  - Quick check question: If a multilingual model uses 20% from each of 5 languages for 100B total tokens, how many tokens does each language contribute?

- Concept: Pseudo-log-likelihood and its role in bias measurement
  - Why needed here: The CrowS-Pairs evaluation uses pseudo-log-likelihood of unmodified tokens conditioned on modified tokens, which is key to understanding how bias is quantified
  - Quick check question: Why does CrowS-Pairs calculate p(U|M,θ) rather than p(M|U,θ)?

- Concept: Bias score calculation and interpretation
  - Why needed here: Understanding the BBQ bias score formula and its range (-100% to 100%) is essential for interpreting results across languages
  - Quick check question: What does a BBQ bias score of 0% indicate about a model's behavior?

## Architecture Onboarding

- Component map:
  - Web data preprocessing with Ungoliant pipeline
  - 2.6B parameter decoder-only transformer with causal language modeling objective
  - SentencePiece tokenizer (32,768 vocab for monolingual, 100,352 for multilingual)
  - Translated bias benchmarks with human validation
  - Zero-shot evaluation framework

- Critical path:
  - Data preparation and tokenization
  - Model training with causal LM objective
  - Benchmark translation and quality validation
  - Zero-shot evaluation across all languages
  - Comparison with open-source 7B models

- Design tradeoffs:
  - Smaller models (2.7B) vs larger (7B+ open-source) - better bias but lower accuracy
  - Equal token distribution vs proportional sampling - ensures balance but may underrepresent some languages
  - Zero-shot evaluation vs fine-tuning - fairer comparison but potentially lower performance

- Failure signatures:
  - High bias scores across all languages indicate fundamental training issues
  - Language-specific bias patterns suggest corpus contamination or translation problems
  - Inconsistent results between benchmarks indicate evaluation methodology issues

- First 3 experiments:
  - Train a 2.6B monolingual model on English only and evaluate on English CrowS-Pairs
  - Train a multilingual model using 20% from each language and evaluate on all five languages
  - Compare bias scores between monolingual and multilingual models to verify the mitigation effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bias reduction achieved through multilingual training scale with model size? Would the bias mitigation effect observed in 2.6B parameter models be even more pronounced in larger models (e.g., 7B or 70B parameters)?
- Basis in paper: [explicit] The paper notes that while GPT-3 (150B parameters) achieves lower accuracy than their models, their 2.7B parameter models still surpass 7B open-source LLMs in bias reduction, but they cannot perform the comparison at 150B parameters due to computational constraints.
- Why unresolved: The authors explicitly state they cannot perform this comparison at 150B parameters due to limited computational resources, leaving the relationship between model size and multilingual bias mitigation unexplored.
- What evidence would resolve it: Training and evaluating comparable multilingual and monolingual models at multiple larger scales (7B, 30B, 70B parameters) using the same controlled methodology would reveal how the bias mitigation effect scales with model size.

### Open Question 2
- Question: To what extent do the observed bias reductions in multilingual models reflect genuine cultural understanding versus surface-level linguistic differences? Are multilingual models actually learning to navigate different cultural contexts, or are they simply becoming more conservative in their responses?
- Basis in paper: [inferred] The paper notes that multilingual models achieve higher accuracy on ambiguous contexts but slightly lower accuracy on disambiguated contexts, suggesting they become more "conservative" and likely to respond with "I don't know" when information is insufficient.
- Why unresolved: The authors observe a behavioral shift in multilingual models but don't investigate whether this reflects deeper cultural understanding or merely statistical patterns across languages. The study focuses on measuring bias reduction but not on understanding the underlying mechanism.
- What evidence would resolve it: Comparative analysis of model behavior on culturally-specific versus culture-neutral prompts across languages, combined with probing tasks that specifically test cultural knowledge and reasoning, would help distinguish between genuine cultural understanding and conservative generalization.

### Open Question 3
- Question: How would the bias mitigation effect of multilingual training differ when training on truly balanced multilingual data versus the current approach of sampling 20% from each language corpus? Would equal representation of all languages throughout training yield different results?
- Basis in paper: [explicit] The authors created the multilingual training corpus by sampling 20% from each monolingual training corpus, resulting in the same total number of tokens but potentially uneven exposure to different language contexts during training.
- Why unresolved: The paper doesn't explore alternative data sampling strategies or training schedules for the multilingual model, leaving open whether the observed bias reduction is optimal or could be enhanced through different approaches to multilingual data integration.
- What evidence would resolve it: Training multilingual models with different data sampling strategies (e.g., strict round-robin token allocation, language-balanced curriculum learning, or dynamic sampling based on language representation) and comparing their bias reduction effects would reveal whether the current approach is optimal.

## Limitations

- Training data composition uses web-scraped data without detailed demographic representation statistics, limiting verification of assumed cultural diversity
- Focus on five Western European languages restricts generalizability to truly multilingual contexts including non-Latin scripts or different language families
- Comparison with open-source models at 7B parameters doesn't account for potential architectural differences beyond parameter count that could influence bias outcomes

## Confidence

**High Confidence**: The claim that multilingual models trained on equal token distribution across languages exhibit lower stereotype bias than monolingual counterparts is well-supported by experimental results showing consistent bias reduction across both benchmarks and all five languages.

**Medium Confidence**: The assertion that multilingual training inherently reduces bias due to exposure to diverse cultural perspectives is plausible but requires additional validation, as the paper doesn't provide direct evidence that cultural diversity is the causal factor.

**Low Confidence**: The comparison showing multilingual 2.6B models outperforming larger 7B open-source models on bias metrics should be interpreted cautiously due to differences in training conditions and potential architectural influences beyond parameter count.

## Next Checks

1. **Cross-linguistic bias consistency test**: Evaluate the multilingual model on translated versions of the same bias benchmarks in multiple languages to verify that bias reduction isn't an artifact of translation quality or language-specific effects.

2. **Controlled ablation study**: Train additional models with varying token distributions (e.g., 50% English/50% other languages, or proportional to corpus size) to isolate whether equal distribution specifically drives bias reduction.

3. **Cultural diversity audit**: Analyze the training corpora for cultural representation diversity by examining topic distributions, named entities, and social group references across languages to validate assumed cultural diversity.