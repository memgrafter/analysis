---
ver: rpa2
title: 'Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual
  Enhancement'
arxiv_id: '2412.04003'
source_url: https://arxiv.org/abs/2412.04003
tags:
- multilingual
- languages
- data
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Marco-LLM bridges the multilingual capability gap in LLMs by conducting
  massive multilingual continual pretraining and post-training, including supervised
  fine-tuning and preference alignment, based on the Qwen2 model. By collecting a
  substantial amount of multilingual data for several low-resource languages and employing
  a two-stage continual pretraining strategy with optimized data mixture and learning
  rates, Marco-LLM significantly improves performance on multilingual benchmarks such
  as MMMLU, AGIEval, Belebele, Flores-200, and XCOPA.
---

# Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement

## Quick Facts
- **arXiv ID**: 2412.04003
- **Source URL**: https://arxiv.org/abs/2412.04003
- **Reference count**: 32
- **Primary result**: Achieved 89.6 accuracy on Belebele, 94.5 on CEVAL, and 72.7 on AGIEval through massive multilingual continual pretraining

## Executive Summary
Marco-LLM addresses the multilingual capability gap in large language models by conducting massive multilingual continual pretraining and post-training based on the Qwen2 model. The approach involves collecting substantial multilingual data for low-resource languages and employing a two-stage continual pretraining strategy with optimized data mixture and learning rates. Through supervised fine-tuning and preference alignment, Marco-LLM significantly improves performance on multilingual benchmarks including MMMLU, AGIEval, Belebele, Flores-200, and XCOPA, outperforming state-of-the-art LLMs in cross-lingual understanding and machine translation tasks.

## Method Summary
Marco-LLM builds upon the Qwen2 model through a comprehensive training pipeline that includes massive multilingual data collection, two-stage continual pretraining, and multilingual post-training. The method collects web-scale multilingual data from sources like Common Crawl, parallel data from OPUS and CCAligned, and generates synthetic data for low-resource languages. The two-stage continual pretraining balances adaptation to multilingual capabilities while preventing catastrophic forgetting of high-resource language capabilities. Post-training includes supervised fine-tuning and preference alignment to enhance model performance across diverse tasks and languages.

## Key Results
- Achieved 89.6 accuracy on Belebele benchmark, demonstrating strong cross-lingual understanding
- Reached 94.5 accuracy on CEVAL, showing effective multilingual capability enhancement
- Scored 72.7 on AGIEval, indicating substantial improvements in general multilingual evaluation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual continual pretraining improves low-resource language performance while preserving high-resource capabilities
- Mechanism: Two-stage pretraining with balanced data mixture and optimized learning rates allows knowledge transfer from high-resource languages to low-resource languages without catastrophic forgetting
- Core assumption: The base Qwen2 model has sufficient multilingual capacity to be extended through targeted training
- Evidence anchors:
  - [abstract]: "conducting massive multilingual continual pre-training and post-training, including supervised fine-tuning and preference alignment, based on the Qwen2 model"
  - [section]: "We propose an advanced two-stage continual pretraining learning approach designed to facilitate the transfer of commonsense knowledge, primarily acquired in English and Chinese, to a variety of low-resource languages"
  - [corpus]: "The corpus of Malay is only 2.9B, resulting in its utilization rate is up to 64.4%"
- Break condition: If learning rates are too high, catastrophic forgetting of high-resource language capabilities occurs; if too low, multilingual adaptation is insufficient

### Mechanism 2
- Claim: High-quality parallel data enhances cross-lingual semantic alignment and improves machine translation tasks
- Mechanism: Parallel corpora provide explicit mappings between languages, enabling the model to learn translation patterns and improve multilingual understanding
- Core assumption: Parallel data quality directly impacts the effectiveness of cross-lingual transfer
- Evidence anchors:
  - [section]: "Empirically, introducing parallel corpora can enhance semantic alignment between cross-languages"
  - [section]: "Compared to base model Qwen2, our Marco-LLM that has been continuously pre-trained with the processed parallel data, shows significant improvements"
  - [corpus]: "There are many bad cases, such as translation errors, in these open-source data"
- Break condition: If parallel data contains too many errors or low-quality pairs, the negative impact outweighs the benefits, especially in larger models

### Mechanism 3
- Claim: Synthetic multilingual data scales training resources and improves low-resource language coverage
- Mechanism: Generated data using LLMs like GPT-4 creates diverse, topic-rich examples in target languages, supplementing scarce real-world data
- Core assumption: Synthetic data quality is sufficient to effectively train multilingual models
- Evidence anchors:
  - [section]: "Our synthetic data mainly consists of two parts: keywords-based explanation and story data, and ability-oriented SFT data"
  - [section]: "The quality and diversity of synthetic data enhances Marco-LLM performance, thus its utilization rate is up to 73.3%"
  - [corpus]: "To enhance the diversity of synthetic data, we employ several superb models (like GPT-4, Deepseek-v2, DBRX, Command-R Plus, etc.) to act as the generator"
- Break condition: If synthetic data lacks linguistic diversity or contains systematic biases, it may reinforce incorrect patterns rather than improve performance

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The model must retain English and Chinese capabilities while learning new languages
  - Quick check question: What happens if you train a multilingual model on low-resource languages without maintaining high-resource language data?

- Concept: Parallel corpus alignment
  - Why needed here: Cross-lingual semantic alignment is critical for translation and multilingual understanding
  - Quick check question: Why is sentence-level alignment important in parallel corpora for machine translation?

- Concept: Data quality filtering in web-scale corpora
  - Why needed here: Raw web data contains noise, duplicates, and low-quality content that can harm model performance
  - Quick check question: What are the main categories of filters applied to multilingual web data?

## Architecture Onboarding

- Component map: Qwen2 base model → Massive multilingual data collection → Two-stage continual pretraining → Supervised fine-tuning → Preference alignment → Evaluation
- Critical path: Data collection → Model training (pretraining + post-training) → Evaluation
- Design tradeoffs: Larger models show better multilingual performance but are more sensitive to low-quality parallel data
- Failure signatures: Performance degradation in high-resource languages indicates catastrophic forgetting; poor low-resource performance suggests insufficient training data or quality issues
- First 3 experiments:
  1. Train Marco-1.5B with different data mixtures to find optimal balance between high-resource and low-resource language tokens
  2. Test different learning rates in the two-stage pretraining to optimize the trade-off between adaptation and forgetting
  3. Evaluate the impact of parallel data filtering quality on machine translation performance across different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of data quality on the performance of larger models (e.g., 72B) compared to smaller models (e.g., 7B) during continual pretraining?
- Basis in paper: [explicit] The paper discusses that the 72B model shows performance degradation when low-quality parallel data is used, while smaller models (1.5B and 7B) show improvements
- Why unresolved: The paper does not fully explore the underlying reasons for this discrepancy in performance between model sizes, particularly the role of parameter redundancy and monosemantic neurons in larger models
- What evidence would resolve it: Further experiments isolating the effects of data quality on models of varying sizes, along with an analysis of neuron redundancy and monosemanticity, would help clarify this issue

### Open Question 2
- Question: How does the inclusion of synthetic data affect the model's ability to handle low-resource languages compared to high-resource languages?
- Basis in paper: [explicit] The paper mentions that synthetic data is used to enhance the model's capacity in math and coding, and to improve cross-lingual and multilingual abilities, but does not specifically address its impact on low-resource languages
- Why unresolved: The paper does not provide a detailed analysis of how synthetic data contributes to the performance gap between high-resource and low-resource languages
- What evidence would resolve it: Experiments comparing the performance of models trained with and without synthetic data, specifically focusing on low-resource languages, would provide insights into the effectiveness of synthetic data in this context

### Open Question 3
- Question: What are the long-term effects of continual pretraining on the model's performance across different languages, and how can these effects be mitigated?
- Basis in paper: [inferred] The paper discusses the challenges of balancing adaptation and catastrophic forgetting during continual pretraining, but does not explore the long-term effects on model performance
- Why unresolved: The paper does not provide a longitudinal study or analysis of how continual pretraining impacts the model's performance over time, particularly in terms of maintaining capabilities across languages
- What evidence would resolve it: A long-term study tracking the model's performance across various languages and tasks over multiple training iterations would help understand the sustainability of continual pretraining benefits

## Limitations

- The paper lacks ablation studies isolating the contribution of each component (parallel data, synthetic data, two-stage pretraining) to overall performance gains
- Evaluation focuses primarily on benchmark performance without extensive qualitative analysis of actual cross-lingual task capabilities in real-world scenarios
- Limited discussion of potential biases introduced through synthetic data generation and their effects on downstream applications in different cultural contexts

## Confidence

- **High Confidence**: The mechanism of two-stage continual pretraining with optimized learning rates to balance multilingual adaptation and catastrophic forgetting is well-supported by the empirical results and aligns with established continual learning principles
- **Medium Confidence**: The claim that synthetic multilingual data significantly enhances low-resource language performance is supported by utilization rates but lacks detailed analysis of synthetic data quality control and potential biases
- **Medium Confidence**: The effectiveness of parallel data for cross-lingual semantic alignment is demonstrated, though the paper acknowledges that low-quality parallel data can be harmful, suggesting sensitivity to data quality that isn't fully quantified

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of parallel data, synthetic data, and the two-stage pretraining strategy to overall performance improvements
2. Perform qualitative analysis of cross-lingual task performance on real-world applications beyond benchmark datasets to validate practical utility
3. Evaluate the sensitivity of different model sizes to parallel data quality by systematically varying the filtering thresholds and measuring performance impacts across language pairs