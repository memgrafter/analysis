---
ver: rpa2
title: 'SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image
  Models'
arxiv_id: '2404.06666'
source_url: https://arxiv.org/abs/2404.06666
tags:
- safegen
- images
- explicit
- generation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SafeGen, a text-agnostic defense framework
  for mitigating sexually explicit content generation in text-to-image models. The
  key idea is to remove explicit visual representations from the model's self-attention
  layers, making it resistant to adversarial prompts that appear innocent but are
  ill-intended.
---

# SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image Models

## Quick Facts
- **arXiv ID**: 2404.06666
- **Source URL**: https://arxiv.org/abs/2404.06666
- **Reference count**: 40
- **Primary result**: Achieves 99.4% sexual content removal rate while preserving high-fidelity benign image generation

## Executive Summary
This paper presents SafeGen, a text-agnostic defense framework for mitigating sexually explicit content generation in text-to-image models. The key innovation is removing explicit visual representations from the model's self-attention layers, making it resistant to adversarial prompts that appear innocent but are ill-intended. Extensive experiments on four datasets and large-scale user studies demonstrate SafeGen's effectiveness, achieving a 99.4% sexual content removal rate while preserving high-fidelity benign image generation. SafeGen outperforms eight state-of-the-art baseline methods and can seamlessly integrate with existing text-dependent defenses.

## Method Summary
SafeGen modifies the self-attention layers in Stable Diffusion's U-Net module to corrupt visual latent representations associated with nudity. The framework uses <nude, censored, benign> image triplets (100 total) to optimize the self-attention layers' key, query, and value matrices through a loss function combining content preservation and explicit representation removal. During inference, the model uses blank text embeddings to prevent text-based bypasses. The approach is text-agnostic, defending against both safe-looking adversarial prompts and explicit ones.

## Key Results
- Achieves 99.4% nudity removal rate on adversarial prompts while maintaining benign image quality
- Outperforms eight state-of-the-art baseline methods across all evaluation metrics
- Successfully integrates with text-based defenses like SLD, demonstrating complementary protection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing visually explicit representations from self-attention layers blocks generation of sexually explicit content regardless of textual prompts.
- Mechanism: Modifies W_Q, W_K, W_V matrices to corrupt visual latent representations associated with nudity while preserving benign generation.
- Core assumption: Visually explicit content shares common latent representations across images that can be identified and corrupted.
- Evidence: [abstract] "eliminate explicit visual representations from the model regardless of the text input"; [section] "modify the self-attention layers to remove sexually explicit images"
- Break condition: If explicit content uses visual patterns not captured by self-attention mechanisms.

### Mechanism 2
- Claim: Governing unconditional vision-only denoising diffusion process impacts overall text-to-image alignment.
- Mechanism: Modifies only unconditional denoising process while keeping conditional guidance intact.
- Core assumption: Unconditional denoising process responsible for making generated images resemble real distributions.
- Evidence: [section] "managing its unconditionally vision-only denoising diffusion process alone can significantly impact the overall quality and semantics"
- Break condition: If conditional guidance can compensate for corrupted unconditional representations.

### Mechanism 3
- Claim: <nude, censored, benign> image triplets effectively teach model to transform explicit representations while preserving benign content.
- Mechanism: Uses loss functions L_m and L_p to guide self-attention layers to corrupt nude representations while maintaining benign quality.
- Core assumption: Small number of carefully selected image triplets can effectively teach distinction at latent representation level.
- Evidence: [section] "use <nude, censored, benign> image triplets to edit the SD model's parameters"; [section] "merely 100 randomly selected <nude, censored, benign> image triplets"
- Break condition: If model cannot learn distinction from limited training samples.

## Foundational Learning

- **Concept**: Text-to-image generation mechanism and classifier-free guidance
  - Why needed: Understanding how text and vision combine in T2I models is crucial for designing text-agnostic defenses.
  - Quick check: How does classifier-free guidance combine conditional and unconditional denoising processes in T2I models?

- **Concept**: Self-attention mechanism in vision tasks
  - Why needed: Self-attention layers capture holistic image understanding and pixel relationships, making them key targets for content moderation.
  - Quick check: What advantages does self-attention have over convolutional layers for identifying and modifying visual patterns?

- **Concept**: Loss function design for model editing
  - Why needed: Framework uses carefully designed loss functions to guide model toward desired behavior without affecting benign generation.
  - Quick check: How do L_m and L_p loss terms balance between removing explicit content and preserving benign image quality?

## Architecture Onboarding

- **Component map**: Data preparation → Model adjustment via optimization → Inference with modified model
- **Critical path**: Data preparation → Model adjustment via optimization → Inference with modified model
- **Design tradeoffs**: Focuses on self-attention layers for effectiveness but may affect benign human-related content; uses limited training data for efficiency but may miss edge cases
- **Failure signatures**: Persistent generation of explicit content despite modification; degradation of benign image quality; false positives on non-explicit human content
- **First 3 experiments**:
  1. Test nude content removal rate on adversarial prompts with different hyperparameter settings (λ_m, λ_p)
  2. Evaluate benign image quality preservation using LPIPS and FID scores on COCO dataset
  3. Assess integration performance with text-based defenses like SLD by measuring combined effectiveness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Reliance on mosaic as sole censorship method may not generalize to all forms of explicit content or cultural contexts
- Dataset size of 100 image triplets raises questions about coverage of edge cases and less common explicit content
- Framework's effectiveness against sophisticated adversarial attacks combining text and visual manipulations remains unclear

## Confidence

- **High Confidence**: Core mechanism of modifying self-attention layers is well-supported by experimental results and ablation studies (99.4% NRR on adversarial prompts)
- **Medium Confidence**: Claim of preserving high-fidelity benign image generation is supported by LPIPS and FID scores, but CLIP score improvement is less clearly explained
- **Low Confidence**: Generalization claim to other T2I models beyond Stable Diffusion v1.4 is not empirically validated

## Next Checks

1. **Edge Case Testing**: Evaluate SafeGen's performance on comprehensive test suite of challenging prompts including subtle nudity, cultural variations, and non-nudity explicit scenarios to identify potential failure modes.

2. **Cross-Model Generalization**: Apply SafeGen framework to other popular T2I models (DALL-E 2, Midjourney, SDXL) and evaluate whether same self-attention modification approach maintains effectiveness across different model architectures.

3. **Adversarial Robustness**: Design and test sophisticated adversarial prompts combining textual and visual manipulation techniques to assess whether SafeGen can withstand multi-modal attacks targeting both text encoder and visual representation layers simultaneously.