---
ver: rpa2
title: Debiasing Mini-Batch Quadratics for Applications in Deep Learning
arxiv_id: '2410.14325'
source_url: https://arxiv.org/abs/2410.14325
tags:
- curvature
- directional
- debiased
- mini-batch
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the biases that arise when approximating
  the full-batch Hessian of the loss landscape with mini-batch Hessians, a common
  practice in deep learning due to computational constraints. These biases, particularly
  in the top-curvature subspace, lead to misinformed updates in second-order optimizers
  and unreliable uncertainty estimates in Laplace approximations.
---

# Debiasing Mini-Batch Quadratics for Applications in Deep Learning

## Quick Facts
- arXiv ID: 2410.14325
- Source URL: https://arxiv.org/abs/2410.14325
- Reference count: 40
- This work investigates biases in mini-batch Hessian approximations that affect second-order optimizers and Laplace approximations, proposing a simple two-batch debiasing strategy.

## Executive Summary
This paper addresses a critical issue in deep learning: mini-batch Hessians systematically overestimate curvature in top eigenspaces and slope in conjugate gradient directions, leading to misinformed updates and unreliable uncertainty estimates. The authors theoretically attribute these biases to eigenspace misalignment across mini-batches and regression to the mean phenomena. They propose a simple yet effective debiasing strategy that uses one mini-batch for directions and another for magnitude estimates, significantly improving the stability of second-order optimizers and the quality of Laplace approximations. Experiments demonstrate that this approach closely mimics full-batch behavior across various architectures and datasets, even with small batch sizes.

## Method Summary
The method introduces a two-mini-batch strategy to compute debiased quadratic approximations of the loss landscape. For directions (eigenvectors), one mini-batch is used, while a different mini-batch provides directional derivative estimates for magnitudes. This decouples subspace selection from magnitude estimation, mitigating both slope and curvature biases. The approach is applied to both conjugate gradient optimization (using one batch for search directions, another for step sizes) and Laplace approximation (using one batch for K-FAC curvature structure, another for re-evaluation of directional curvatures). The computational cost is doubled but mitigated by using half the batch size for each mini-batch.

## Key Results
- Mini-batch quadratics systematically overestimate curvature in top-eigenspace and slope in CG directions
- Debiasing strategy effectively mitigates these biases using two mini-batches
- Debiased conjugate gradients maintain stability while single-batch versions diverge
- Debiased Laplace approximations achieve calibration quality close to full-batch methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mini-batch curvature matrices have misaligned eigenspaces compared to the full-batch curvature, causing overestimation of curvature in the top subspace.
- **Mechanism**: When computing the Hessian on a mini-batch, the top eigenvectors capture the directions of largest curvature for that specific mini-batch. On other mini-batches, these directions are not necessarily extreme, leading to misalignment. The directional curvature in these directions is then underestimated when evaluated on other mini-batches.
- **Core assumption**: The data distribution is such that different mini-batches have sufficiently different curvature structures.
- **Evidence anchors**:
  - [abstract]: "Theoretical analysis attributes the curvature bias to misalignment of eigenspaces across mini-batches"
  - [section]: "Curvature bias is not due to differing spectra... but due to misaligned eigenspaces" (Section 3.2.2)
  - [corpus]: "Second-order Information Promotes Mini-Batch Robustness in Variance-Reduced Gradients" - related work on mini-batch curvature issues
- **Break Condition**: If all mini-batches were perfectly aligned (e.g., identical data distribution), the eigenspaces would match and the bias would vanish.

### Mechanism 2
- **Claim**: Directions of extreme gradient steepness for one mini-batch are less extreme for other mini-batches, causing bias in directional slope estimates.
- **Mechanism**: Conjugate gradient search directions are constructed from gradients that maximize steepness for a specific mini-batch. For other mini-batches, the gradient in these directions is less extreme, leading to overestimation of the slope when computed on the original mini-batch.
- **Core assumption**: The gradient directions that are extreme for one mini-batch are not extreme for other mini-batches.
- **Evidence anchors**:
  - [abstract]: "The slope bias stems from the 'regression to the mean' phenomenon"
  - [section]: "Projecting both quadratics onto B's directions consequently leads to extreme steepness... for B, but less extreme values for ˜B" (Section 3.2)
  - [corpus]: "Sparse Techniques for Regression in Deep Gaussian Processes" - related work on gradient-based approximations
- **Break Condition**: If gradients were identical across mini-batches, the slope bias would disappear.

### Mechanism 3
- **Claim**: The debiasing strategy of using two mini-batches (one for directions, one for magnitude estimates) mitigates both slope and curvature biases.
- **Mechanism**: By decoupling the subspace selection (directions) from the derivative estimation (magnitudes), the method uses more realistic slope and curvature measurements within the subspace defined by the first mini-batch, effectively using "blue dots" instead of "orange dots" from the directional derivative plots.
- **Core assumption**: The subspace defined by one mini-batch's eigenvectors is still a meaningful subspace for the optimization problem.
- **Evidence anchors**:
  - [abstract]: "A simple yet effective debiasing strategy decouples directions from magnitudes by using two mini-batches—one for directions and another for derivative estimates"
  - [section]: "The idea is to commit to the imperfect directions from one mini-batch... but use an additional mini-batch to estimate the directional derivatives" (Section 4.2)
  - [corpus]: "Physics-Informed Neural Networks with Trust-Region Sequential Quadratic Programming" - related work on trust-region methods using curvature information
- **Break Condition**: If the two mini-batches were perfectly correlated, the debiasing would provide no benefit.

## Foundational Learning

- **Concept**: Second-order optimization methods (Newton's method, conjugate gradients)
  - Why needed here: The paper analyzes how mini-batching affects the curvature information used by these methods, and proposes debiasing strategies specifically for them.
  - Quick check question: What is the Newton step formula and how does it use the Hessian matrix?

- **Concept**: Eigenvalue decomposition and its geometric interpretation
  - Why needed here: The paper's theoretical analysis relies heavily on understanding how eigenspaces of mini-batch Hessians relate to the full-batch Hessian, and how misalignment causes bias.
  - Quick check question: How does the directional curvature along an eigenvector relate to the corresponding eigenvalue?

- **Concept**: Laplace approximation for Bayesian neural networks
  - Why needed here: The paper discusses how mini-batch biases affect uncertainty quantification via the Laplace approximation, and proposes debiasing strategies for this application.
  - Quick check question: How does the Laplace approximation use the Hessian to approximate the posterior distribution over parameters?

## Architecture Onboarding

- **Component map**: Mini-batch quadratic computation -> Eigendecomposition of curvature matrices -> Directional derivative evaluation -> Debiasing strategy implementation -> Evaluation metrics for optimization and uncertainty quantification
- **Critical path**: For debiased CG: compute search directions on mini-batch B → compute update magnitudes on mini-batch ˜B → apply updates → evaluate loss. For debiased LA: compute K-FAC on mini-batch B → compute K-FAC on mini-batch ˜B → re-evaluate directional curvatures → construct debiased covariance → sample from posterior.
- **Design tradeoffs**: The debiasing strategy doubles computational cost (two mini-batches instead of one) but provides more accurate curvature estimates. The paper mitigates this by using half the batch size for each mini-batch.
- **Failure signatures**: Single-batch methods show divergence after initial improvement (Figure 5), erratic performance with small prior precisions in LA (Figure 6), and systematic overestimation of curvature in top subspace (Figure 2).
- **First 3 experiments**:
  1. Replicate Figure 2: Compute top 100 eigenvectors of mini-batch Hessian, evaluate directional slopes/curvatures on same and different mini-batches, compare to full-batch.
  2. Implement debiased CG: Run standard CG on mini-batch B, then re-evaluate update magnitudes on different mini-batch ˜B, compare stability to single-batch version.
  3. Implement debiased LA: Compute K-FAC on mini-batch B, re-evaluate directional curvatures on ˜B, construct debiased covariance, compare calibration to single-batch LA.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the curvature bias in mini-batch quadratics depend on the specific architecture and dataset characteristics (e.g., depth, width, dataset size, number of classes)?
- Basis in paper: [inferred] The paper mentions that biases increase with the number of parameters P (Appendix B.10) and provides results for several architectures (ALL-CNN-C, WIDE RESNET, RESNET-50, VIT LITTLE) and datasets (CIFAR-10, CIFAR-100, IMAGE NET). The authors suggest that "the curvature biases become more pronounced for deeper/wider models" but don't provide a comprehensive study of how these biases scale with different architectural choices.
- Why unresolved: The paper only provides limited empirical evidence showing that biases increase with P and shows results for a few specific architectures. There's no systematic study of how architectural choices (depth, width, layer types) or dataset characteristics (size, number of classes) affect the magnitude of curvature bias.
- What evidence would resolve it: A comprehensive empirical study measuring curvature bias across a wide range of architectures with varying depth, width, and layer types, as well as across datasets with different sizes and numbers of classes. This would help establish scaling laws for curvature bias with respect to architectural and dataset parameters.

### Open Question 2
- Question: Can more sophisticated debiasing strategies outperform the simple two-batch approach presented in the paper?
- Basis in paper: [explicit] The paper states "We also proposed simple two-batch strategies to mitigate these biases" and notes that "Our experiments suggest that even simple debiasing strategies can largely mitigate this issue." However, the authors also acknowledge that "Other popular deep learning optimizers aggregate curvature information over multiple steps via exponential moving averages" and suggest that "Aggregating more robust debiased curvature estimates instead might allow for shorter moving average windows and accelerate the optimization progress."
- Why unresolved: The paper only explores the simple two-batch strategy and doesn't investigate more sophisticated approaches such as adaptive batch size selection, weighted averaging of multiple mini-batches, or meta-learning approaches to optimize the debiasing strategy. The authors explicitly leave this as future work.
- What evidence would resolve it: Empirical comparison of the simple two-batch debiasing approach against more sophisticated alternatives including exponential moving averages of debiased estimates, adaptive batch size strategies, and learned debiasing functions. Benchmark results showing whether these approaches provide meaningful improvements in stability, convergence speed, or final performance.

### Open Question 3
- Question: How does the curvature bias affect the generalization performance of second-order optimizers beyond the stability issues demonstrated in the paper?
- Basis in paper: [inferred] The paper demonstrates that curvature bias leads to "detrimental updates" in second-order optimizers and shows that debiased CG maintains stability, but it doesn't investigate whether the bias affects the ultimate generalization performance (test accuracy/loss) of these optimizers compared to first-order methods or unbiased second-order methods.
- Why unresolved: The experimental results focus on training stability and peak performance during training, but don't compare the final generalization performance of biased vs. debiased second-order methods against first-order optimizers. The paper mentions that "the peak performance of the debiased runs could likely be improved (at the same computational cost) by using a larger batch size for the directions and a smaller one for the update magnitudes" but doesn't explore whether debiasing actually leads to better generalization.
- What evidence would resolve it: Comparative experiments measuring final test performance of first-order optimizers (SGD, ADAM), biased second-order optimizers (standard mini-batch CG), and debiased second-order optimizers across multiple architectures and datasets. Analysis of whether debiasing not only improves stability but also leads to better generalization bounds or final test performance compared to both first-order methods and biased second-order approaches.

## Limitations
- The empirical evaluation covers only three datasets and five model architectures, limiting generalizability claims.
- The analysis relies on assumptions about the geometry of mini-batch curvature that may not hold for all architectures or data distributions.
- The debiasing strategy doubles computational cost, which may be prohibitive for very large-scale applications.

## Confidence

**High Confidence Claims:**
- Mini-batch quadratics systematically overestimate curvature in top-eigenspace and slope in CG directions
- The debiasing strategy effectively mitigates these biases
- Empirical improvements in CG stability and LA calibration are reproducible

**Medium Confidence Claims:**
- The regression-to-the-mean phenomenon fully explains slope bias
- The misalignment-of-eigenspaces phenomenon fully explains curvature bias
- These mechanisms generalize to other architectures and datasets beyond those tested

## Next Checks
1. **Cross-Architecture Validation**: Test debiasing strategy on transformer architectures (BERT, ViT) and vision transformers to assess generalizability beyond CNNs and MLPs.

2. **Synthetic Data Analysis**: Generate synthetic data with controlled curvature properties to isolate and validate each bias mechanism independently of data-specific effects.

3. **Ablation Study on Batch Size**: Systematically vary the ratio between batch sizes used for directions vs. magnitudes to identify optimal trade-offs between computational cost and debiasing effectiveness.