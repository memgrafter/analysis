---
ver: rpa2
title: 'DOCE: Finding the Sweet Spot for Execution-Based Code Generation'
arxiv_id: '2408.13745'
source_url: https://arxiv.org/abs/2408.13745
tags:
- reranking
- candidates
- code
- unit
- mbr-exec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOCE (Decoding Objectives for Code Execution),
  a comprehensive framework for improving LLM-based code generation by systematically
  studying candidate generation, reranking, and self-debugging. The authors propose
  sampling multiple candidates with high temperature, filtering based on trial unit
  tests, and applying self-debugging to multiple candidates before reranking.
---

# DOCE: Finding the Sweet Spot for Execution-Based Code Generation

## Quick Facts
- arXiv ID: 2408.13745
- Source URL: https://arxiv.org/abs/2408.13745
- Reference count: 40
- Primary result: Sampling with high temperatures (1.6-1.8) combined with trial unit test filtering and multi-candidate self-debugging achieves state-of-the-art reranking performance for code generation

## Executive Summary
DOCE (Decoding Objectives for Code Execution) introduces a comprehensive framework for improving LLM-based code generation by systematically studying candidate generation, reranking, and self-debugging. The authors find that sampling with previously unseen high temperatures (1.6-1.8) leads to better oracle and reranking performance compared to traditional low-temperature approaches. They demonstrate that execution-based filtering using trial unit tests dramatically improves reranking performance, and that applying self-debugging to multiple candidates (SD-Multi) consistently outperforms single-candidate approaches. Their approach achieves state-of-the-art reranking performance across multiple benchmarks including HumanEval, MBPP, and LiveCodeBench using various model sizes from 7B to 16B parameters.

## Method Summary
DOCE systematically studies three key components of execution-based code generation: candidate generation with temperature sampling, execution-based filtering using trial unit tests, and self-debugging applied to multiple candidates. The method generates 50 candidates using CodeLlama or DeepSeekCoder models with high temperatures (1.6-1.8 for HumanEval/MBPP, 1.2-1.4 for LiveCodeBench), filters candidates based on trial unit test execution, applies Minimum Bayes Risk decoding with execution-based reranking, and uses self-debugging on multiple candidates to improve performance. The framework is evaluated on HumanEval, MBPP, and LiveCodeBench using execution accuracy as the primary metric.

## Key Results
- Sampling with temperatures 1.6-1.8 produces better oracle and reranking performance than previously suggested low temperatures
- Execution-based filtering using trial unit tests provides the largest gain from random baseline compared to other reranking methods
- Self-debugging applied to multiple candidates (SD-Multi) consistently outperforms single-candidate approaches (SD-1)
- MBR-Exec with trial unit test filtering approaches oracle performance across all tested benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher temperature sampling (1.6-1.8) produces better oracle and reranking performance by increasing token diversity
- Core assumption: Model probability distributions at higher temperatures maintain reasonable quality while increasing diversity
- Evidence anchors: Abstract shows high temperatures lead to better performance; section shows peak performance between 1.6-1.8 for HumanEval/MBPP
- Break condition: If high temperatures produce too many nonsensical candidates, degrading candidate pool quality

### Mechanism 2
- Claim: Execution-based filtering using trial unit tests dramatically improves reranking by eliminating incorrect candidates
- Core assumption: Trial unit tests are representative enough to filter out incorrect solutions while preserving correct ones
- Evidence anchors: Abstract emphasizes trial unit test filtering as effective; section shows filtering gives largest gain from baseline
- Break condition: If trial tests are too weak or strict, filtering may miss incorrect candidates or eliminate correct ones

### Mechanism 3
- Claim: SD-Multi outperforms SD-1 because it generates more candidates that pass trial unit tests
- Core assumption: Self-debugging can successfully modify multiple candidates to pass trial unit tests
- Evidence anchors: Abstract states SD-Multi achieves state-of-the-art performance; section shows consistent SD-Multi superiority
- Break condition: If self-debugging fails to modify candidates sufficiently, SD-Multi may not provide advantage

## Foundational Learning

- Concept: Execution-based evaluation metrics
  - Why needed here: Core innovation relies on using actual program execution to evaluate code correctness
  - Quick check question: How would you modify the approach if you only had access to static analysis instead of execution?

- Concept: Temperature sampling in language models
  - Why needed here: Temperature findings (1.6-1.8) are central to methodology
  - Quick check question: What happens to token probability distribution when temperature approaches 0 or infinity?

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR decoding is a core reranking method studied in the paper
  - Quick check question: How does MBR decoding differ from simple likelihood-based reranking?

## Architecture Onboarding

- Component map: Candidate Generation (LLM sampling) -> Filtering (trial unit test execution) -> Reranking (MBR-Exec) -> Self-Debugging (multi-candidate) -> Final Selection

- Critical path: Candidate Generation → Filtering → Reranking → Self-Debugging → Final Selection

- Design tradeoffs:
  - Higher temperature vs. candidate quality: More diversity vs. more noise
  - Number of candidates vs. computational cost: More candidates improve oracle but increase expense
  - Number of unit tests vs. accuracy: More tests improve filtering but increase execution time

- Failure signatures:
  - Poor performance despite high temperature: Model may not benefit from diversity at given scale
  - Filtering removes all candidates: Trial unit tests may be too strict or poorly designed
  - Self-debugging fails to improve candidates: Model may lack debugging capability or candidates too broken

- First 3 experiments:
  1. Test temperature sensitivity: Run sampling at 0.2, 0.8, 1.2, 1.6, 1.8 and measure oracle performance
  2. Validate filtering effectiveness: Compare reranking performance with and without trial unit test filtering
  3. Test SD-Multi vs SD-1: Apply both methods to same candidate set and measure improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does filtering quality impact the effectiveness of execution-based reranking methods across different code complexity levels?
- Basis: Authors emphasize filtering helps but don't analyze how filtering quality affects performance across difficulty levels
- Why unresolved: Shows filtering helps but doesn't explore sensitivity to filtering accuracy or problem complexity variation
- Evidence needed: Systematic experiments varying filtering thresholds and measuring performance degradation, stratified by problem difficulty

### Open Question 2
- Question: What is the optimal sampling temperature range for different code generation model architectures and sizes?
- Basis: Authors find optimal temperatures for specific models but note DeepSeek models require lower temperatures and don't explore full parameter space
- Why unresolved: Study focuses on limited model sizes and doesn't systematically explore how optimal temperature varies with architecture
- Evidence needed: Comprehensive ablation study across model families, training objectives, and parameter counts

### Open Question 3
- Question: How does the quality and diversity of trial unit tests impact the effectiveness of both filtering and MBR-Exec reranking?
- Basis: Authors use extended unit tests and show MBR-Exec performs well with 5-20 tests but don't analyze test quality impact
- Why unresolved: Assumes quality unit tests but doesn't investigate whether these are necessary or how characteristics influence reranking
- Evidence needed: Experiments varying test quality metrics and measuring correlation with reranking performance

## Limitations

- Temperature sensitivity appears dataset-specific rather than model-specific, limiting generalizability across domains
- Filtering effectiveness heavily depends on quality and comprehensiveness of unit tests, which may not hold for all problem types
- Self-debugging computational overhead scales linearly with candidate count, with no cost-benefit analysis provided

## Confidence

- High Confidence: Execution-based filtering mechanism significantly improves reranking performance (well-supported by experimental evidence)
- Medium Confidence: Higher temperature sampling produces better performance than low temperatures (supported but dataset-specific)
- Medium Confidence: SD-Multi outperforms SD-1 in most experiments (strong evidence but model-dependent effects noted)

## Next Checks

1. Cross-Dataset Temperature Validation: Test optimal temperature ranges on at least two additional code generation datasets to verify generalizability across domains

2. Filtering Robustness Test: Systematically vary trial unit test quality and comprehensiveness to determine minimum viable test coverage required for effective filtering

3. SD-Multi Scaling Analysis: Measure marginal benefit as candidate count increases from 10 to 100 to 500 and calculate computational cost per percentage point improvement