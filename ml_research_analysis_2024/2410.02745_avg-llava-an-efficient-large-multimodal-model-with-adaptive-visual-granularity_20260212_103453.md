---
ver: rpa2
title: 'AVG-LLaVA: An Efficient Large Multimodal Model with Adaptive Visual Granularity'
arxiv_id: '2410.02745'
source_url: https://arxiv.org/abs/2410.02745
tags:
- visual
- tokens
- granularity
- router
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AVG-LLaVA, a large multimodal model that
  adaptively selects the appropriate visual granularity based on the input image and
  instruction. The model applies multiple pooling layers to obtain visual tokens at
  different granularities and uses a visual granularity router to select the most
  suitable granularity.
---

# AVG-LLaVA: An Efficient Large Multimodal Model with Adaptive Visual Granularity

## Quick Facts
- arXiv ID: 2410.02745
- Source URL: https://arxiv.org/abs/2410.02745
- Reference count: 25
- Key outcome: Achieves superior performance across 11 benchmarks while reducing visual tokens by 85.3% and increasing inference speed by 2.53× on AI2D

## Executive Summary
AVG-LLaVA introduces an adaptive visual granularity approach for large multimodal models, enabling efficient computation by dynamically selecting appropriate levels of visual detail based on input images and instructions. The model employs multiple pooling layers to generate visual features at different granularities and uses a visual granularity router to select the most suitable level. A novel training paradigm called Ranking Granularity based on LMM Feedback (RGLF) aligns the router's predictions with the language model's preferences using a ranking loss, eliminating the need for manual annotations. This approach achieves state-of-the-art performance across diverse benchmarks while significantly reducing computational overhead.

## Method Summary
AVG-LLaVA implements a two-stage training process. First, it performs multi-granularity visual instruction tuning using cross-entropy loss on the ALLaVA dataset. The visual granularity scaler generates features at five levels (24×24, 24×12, 12×12, 12×6, 6×6) through sequential 1×2 and 2×1 average pooling layers. The visual granularity router, consisting of Transformer, MLP, and voter layers, selects the most appropriate granularity. In the second stage, RGLF training applies both ranking loss and cross-entropy loss to align router predictions with LLM preferences, with α=0.1 and k=32. The model demonstrates robust performance improvements while maintaining efficiency through reduced visual token processing.

## Key Results
- 85.3% reduction in visual tokens and 2.53× faster inference on AI2D benchmark
- Superior performance across 11 benchmarks including GQA, ScienceQA, VizWiz, TextVQA, ChartQA, DocVQA, MME, MMB, POPE, and MMMU
- Ablation studies show ranking loss is more crucial than cross-entropy loss for router training
- Visual token levels of 72 and 288 are rarely selected, validating the granularity selection strategy

## Why This Works (Mechanism)

### Mechanism 1
Adaptive visual granularity selection improves performance by reducing redundant information and focusing computation on task-relevant visual detail. The router dynamically selects appropriate detail levels (fine to coarse) based on image and instruction, reducing processed visual tokens while maintaining or improving accuracy. The core assumption is that not all visual tasks require the same level of detail—some can be solved with coarse information while others need fine-grained features.

### Mechanism 2
The ranking loss (RGLF) effectively trains the router to align with the language model's preferences without manual annotations. The router is trained using a ranking loss that compares log probabilities of LLM predictions at different granularities, with higher-ranked granularities receiving stronger gradients. The core assumption is that the LLM's preference ranking of visual granularities correlates with actual task performance.

### Mechanism 3
Spatial pyramid pooling enables multi-granularity feature extraction without additional training overhead. Sequential 1×2 and 2×1 average pooling layers progressively halve visual tokens, creating a hierarchy of features from fine to coarse. The core assumption is that average pooling preserves sufficient spatial information for coarser granularity levels while reducing computational cost.

## Foundational Learning

- **Mixture of Experts (MoE) routing**: The visual granularity router uses MoE principles to select among different visual feature "experts" (granularities). Quick check: How does the router determine which granularity expert to activate for a given input?

- **Ranking loss functions in model training**: RGLF uses ranking loss to align router predictions with LLM preferences without manual supervision. Quick check: What is the difference between ranking loss and cross-entropy loss in this context?

- **Spatial pyramid pooling**: Enables creation of multi-resolution features through sequential pooling operations. Quick check: How does average pooling preserve spatial information while reducing resolution?

## Architecture Onboarding

- **Component map**: Base LLaVA-NeXT (visual encoder + connector + LLM) → Visual granularity scaler (spatial pyramid pooling layers) → Visual granularity router (Transformer + MLP + voter) → Selected granularity → Connector → LLM → Output

- **Critical path**: Image → visual encoder → granularity scaler → router → selected granularity → connector → LLM → output

- **Design tradeoffs**: Multiple granularities increase model complexity but enable adaptive computation; ranking loss requires additional computation during training but eliminates need for manual annotations; spatial pyramid pooling is efficient but may lose fine spatial details at coarse levels

- **Failure signatures**: Router consistently selects inappropriate granularity (performance degradation); training instability with ranking loss (learning rate tuning needed); memory overflow with high-resolution images (batch size reduction required)

- **First 3 experiments**:
  1. Test router performance with fixed vs. adaptive granularity on a single benchmark
  2. Evaluate impact of ranking loss vs. cross-entropy loss on router training
  3. Measure inference speed improvement with different granularity selection strategies

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the visual granularity router's performance vary across different types of images (e.g., natural scenes vs. synthetic diagrams vs. text-heavy documents)? The paper mentions that the router adapts to different tasks but does not provide systematic analysis across image types.

- **Open Question 2**: What is the optimal balance between the ranking loss and cross-entropy loss (α hyperparameter) for different task domains? The paper notes that setting a smaller α is beneficial but doesn't explore domain-specific optimization.

- **Open Question 3**: Can the visual granularity scaling approach be extended to include more than five granularity levels without degrading performance? The paper uses five granularity levels and notes that 72 and 288-token levels are rarely selected, suggesting potential for expansion.

- **Open Question 4**: How does the RGLF training paradigm compare to alternative self-supervised approaches for training the router? The paper contrasts RGLF with visual instruction fine-tuning but doesn't explore other self-supervised methods.

## Limitations

- Router generalization across diverse domains remains uncertain, with limited evidence of performance on novel visual domains not represented in training data
- Trade-off transparency is lacking, with insufficient analysis of scenarios where fixed granularity might outperform adaptive approaches
- Training stability evidence is minimal, with no thorough addressing of hyperparameter sensitivity or potential for mode collapse in router learning

## Confidence

- **High Confidence**: The core mechanisms of adaptive granularity selection and ranking loss training are well-supported by experimental results, particularly the 85.3% reduction in visual tokens and 2.53× inference speedup on AI2D
- **Medium Confidence**: The spatial pyramid pooling approach is straightforward and well-established, but the paper doesn't thoroughly validate whether average pooling optimally preserves spatial information across all granularity levels
- **Low Confidence**: Claims about effectiveness across diverse benchmarks are supported by aggregate results, but individual benchmark performance and failure cases are not analyzed in depth, particularly regarding robustness to domain shifts

## Next Checks

1. **Router Robustness Testing**: Evaluate the visual granularity router on a held-out domain distinct from the training data (e.g., medical imaging or satellite imagery). Measure whether the router's granularity selection remains appropriate and whether performance degrades compared to fixed granularity baselines.

2. **Ablation of Pooling Strategies**: Replace the average pooling layers in the visual granularity scaler with alternative methods (max pooling, strided convolution, or learned pooling). Compare the resulting model's performance and inference speed to quantify the impact of pooling choice on spatial information preservation.

3. **Ranking Loss Sensitivity Analysis**: Systematically vary the ranking loss hyperparameters (α and k) and the relative weighting between ranking loss and cross-entropy loss. Analyze how these changes affect router training stability, convergence speed, and final performance to identify optimal configurations and potential failure modes.