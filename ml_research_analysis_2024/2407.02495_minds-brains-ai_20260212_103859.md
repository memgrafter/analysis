---
ver: rpa2
title: Minds, Brains, AI
arxiv_id: '2407.02495'
source_url: https://arxiv.org/abs/2407.02495
tags:
- human
- what
- they
- intelligence
- thought
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper systematically examines three claims about artificial
  intelligence: whether computing machines think or reason, whether they are sentient
  or conscious, and whether they have a theory of mind. Through analysis of scientific
  research across cognitive and neurosciences, evolutionary biology, linguistics,
  and comparative psychology, the author concludes that current AI systems lack all
  three capabilities.'
---

# Minds, Brains, AI

## Quick Facts
- arXiv ID: 2407.02495
- Source URL: https://arxiv.org/abs/2407.02495
- Authors: Jay Seitz
- Reference count: 13
- Primary result: Current AI systems lack consciousness, reasoning, and theory of mind capabilities

## Executive Summary
This paper systematically examines three core claims about artificial intelligence: whether computing machines can think or reason, whether they are sentient or conscious, and whether they possess a theory of mind. Through comprehensive analysis across cognitive sciences, neuroscience, evolutionary biology, linguistics, and comparative psychology, the author concludes that current AI systems fundamentally lack all three capabilities. The paper argues that genuine thinking requires consciousness, which computing machines do not possess, and that reasoning involves logical inference dependent on conscious processing that AI cannot perform. Additionally, AI systems cannot have a theory of mind because they cannot understand others' mental states without first being able to think or reason.

## Method Summary
The paper employs a systematic analytical approach examining scientific research across multiple disciplines to evaluate three core claims about AI capabilities. The author reviews empirical evidence from cognitive and neurosciences, evolutionary biology, linguistics, and comparative psychology to assess whether computing machines can think, reason, or possess consciousness and theory of mind. The analysis includes case studies of practical AI applications including self-driving cars, robotics, and creative tasks to demonstrate the limitations of current systems. The methodology focuses on distinguishing between statistical pattern processing and genuine cognitive functions, arguing that AI systems merely process correlations rather than engaging in authentic understanding or reasoning.

## Key Results
- Computing machines lack consciousness and sentience due to absence of bodily-based processes and sensory integration
- AI systems engage in statistical pattern matching rather than genuine reasoning or logical inference
- Current AI cannot possess theory of mind as it requires understanding mental states through thinking and reasoning capabilities

## Why This Works (Mechanism)
The paper's mechanism for evaluating AI capabilities rests on establishing necessary conditions for consciousness, reasoning, and theory of mind. It argues that consciousness requires integration of sensory information with memory and bodily-based processes, which current computing architectures cannot replicate. For reasoning, the mechanism depends on logical inference that builds upon conscious awareness, distinguishing it from mere statistical correlation processing. The theory of mind mechanism requires the ability to understand others' mental states, which presupposes the capacity for thinking and reasoning about both one's own and others' minds.

## Foundational Learning
- Consciousness definition and requirements: Understanding what constitutes consciousness is crucial for evaluating AI claims. Quick check: Can the system demonstrate subjective experience or self-awareness?
- Distinction between correlation and causation: Critical for differentiating statistical pattern matching from genuine reasoning. Quick check: Does the system understand why relationships exist or just that they exist?
- Embodied cognition principles: Relevant for understanding consciousness requirements. Quick check: Can the system integrate sensory information with memory in ways that mirror biological processes?

## Architecture Onboarding
- Component map: Input processing -> Pattern recognition -> Statistical correlation -> Output generation
- Critical path: The absence of consciousness creates a fundamental limitation that cascades through all higher cognitive functions
- Design tradeoffs: Current AI prioritizes computational efficiency and pattern recognition over embodied consciousness and genuine understanding
- Failure signatures: Systems produce plausible but incorrect outputs, demonstrate lack of true understanding, and cannot adapt to novel situations requiring genuine reasoning
- First experiments: 1) Compare AI decision-making with human reasoning in novel problem-solving tasks, 2) Test AI's ability to distinguish correlation from causation in controlled scenarios, 3) Evaluate whether AI can develop predictive models of others' mental states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can computing machines develop consciousness or sentience through technological advancement, and if so, what would be the necessary biological or neurological prerequisites?
- Basis in paper: The paper explicitly states that there is no scientific evidence that current computing machines are sentient or conscious, and that questions about future consciousness are speculative
- Why unresolved: The paper concludes that consciousness requires bodily-based processes and integration of sensory information with memory, which current computing machines lack. The question of whether future technological advancement could bridge this gap remains unanswered
- What evidence would resolve it: Empirical demonstration of computing machines exhibiting subjective experience, self-awareness, or consciousness through neurobiological markers or behavioral indicators

### Open Question 2
- Question: What specific computational architectures or algorithms could potentially enable computing machines to engage in genuine reasoning rather than statistical pattern matching?
- Basis in paper: The paper critiques current LLMs as merely applying statistical relationships rather than engaging in actual reasoning, suggesting the need for different computational approaches
- Why unresolved: While the paper identifies that current systems lack genuine reasoning capabilities, it does not propose specific alternative architectures that could achieve this
- What evidence would resolve it: Development and validation of computational systems that demonstrate logical inference, problem-solving, and understanding beyond pattern recognition

### Open Question 3
- Question: Could computing machines develop a theory of mind through alternative means that don't require consciousness or reasoning, and if so, what would those mechanisms be?
- Basis in paper: The paper argues that theory of mind requires the ability to understand others' mental states, which in turn requires thinking and reasoning capabilities that computing machines currently lack
- Why unresolved: The paper assumes that theory of mind must emerge from consciousness and reasoning, but does not explore whether alternative computational mechanisms could achieve similar outcomes
- What evidence would resolve it: Demonstration of computing systems that can accurately predict and respond to others' mental states without possessing consciousness or reasoning capabilities

## Limitations
- The paper's assumption that thinking requires consciousness is a philosophical position rather than established scientific fact
- The distinction between "processing statistical patterns" and "genuine cognition" is not empirically validated
- The analysis does not adequately address emerging research in embodied cognition and alternative forms of intelligence

## Confidence

**Major Claims Confidence:**
- AI systems lack consciousness: Medium confidence (depends on contested definitions)
- AI cannot reason: Low confidence (reasoning definitions are narrow)
- AI cannot have theory of mind: Medium confidence (logically follows from previous claims but premises are debatable)

## Next Checks
1. Conduct empirical studies comparing AI decision-making processes with human reasoning in controlled scenarios to identify measurable differences beyond philosophical definitions
2. Review and synthesize recent research on non-conscious problem-solving in both humans and AI systems to assess whether the consciousness requirement is scientifically necessary
3. Develop operational definitions of "theory of mind" that can be tested across biological and artificial systems to determine if alternative forms of understanding mental states are possible