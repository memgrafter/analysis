---
ver: rpa2
title: 'Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading
  Scenarios'
arxiv_id: '2411.02708'
source_url: https://arxiv.org/abs/2411.02708
tags:
- misleading
- rate
- answer
- implicit
- explicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a systematic evaluation of multimodal large\
  \ language models' (MLLMs) vulnerability to misleading instructions, revealing that\
  \ 65% of previously correct answers are overturned after a single deceptive cue.\
  \ A two-stage pipeline is proposed: first recording unperturbed responses, then\
  \ injecting explicit (false-answer hints) and implicit (contextual contradictions)\
  \ misleading instructions, and measuring the misleading rate\u2014the fraction of\
  \ correct-to-incorrect flips."
---

# Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios

## Quick Facts
- arXiv ID: 2411.02708
- Source URL: https://arxiv.org/abs/2411.02708
- Reference count: 40
- Primary result: 65% of correct MLLM answers are overturned by single deceptive cues; fine-tuning reduces misleading rates to 6.97% (explicit) and 32.77% (implicit)

## Executive Summary
This paper systematically evaluates how multimodal large language models (MLLMs) respond to misleading instructions, revealing significant vulnerability: 65% of previously correct answers are overturned by a single deceptive cue. The authors propose a two-stage evaluation pipeline to measure misleading rates—the fraction of correct-to-incorrect flips—under explicit (false-answer hints) and implicit (contextual contradictions) instructions. A Multimodal Uncertainty Benchmark (MUB) is constructed from the most susceptible examples, stratified by difficulty. Extensive evaluation across twelve open-source and five closed-source models shows average misleading rates exceeding 86%. Fine-tuning all open-source models on a compact 2000-sample mixed-instruction dataset dramatically reduces misleading rates while boosting consistency by nearly 29.37% on highly deceptive inputs.

## Method Summary
The authors introduce a two-stage evaluation pipeline: first recording unperturbed model responses on multimodal inputs, then injecting explicit or implicit misleading instructions and measuring the proportion of correct-to-incorrect flips (misleading rate). They construct a Multimodal Uncertainty Benchmark (MUB) from examples with the highest misleading rates, stratified into low, medium, and high difficulty levels. To improve robustness, they fine-tune all open-source MLLMs on a 2000-sample mixed-instruction dataset using LoRA, combining explicit and implicit misleading examples. The evaluation spans twelve open-source and five closed-source models across nine standard multimodal datasets, measuring misleading rates, consistency, and accuracy.

## Key Results
- Across nine standard datasets, twelve state-of-the-art open-source MLLMs overturn a previously correct answer in 65% of cases after receiving a single deceptive cue
- Average misleading rate exceeds 86% across all evaluated models, with explicit cues at 67.19% and implicit cues at 80.67%
- Fine-tuning on a 2000-sample mixed-instruction dataset reduces misleading rates to 6.97% (explicit) and 32.77% (implicit), boosting consistency by nearly 29.37% on highly deceptive inputs
- Fine-tuned models achieve a 2.61% improvement in accuracy on MMStar and 2.32% on AI2D compared to their pre-trained counterparts

## Why This Works (Mechanism)

### Mechanism 1
Misleading instructions can systematically flip correct answers in MLLMs by exploiting the models' sensitivity to explicit or implicit cues. The two-stage evaluation pipeline first captures the model's unperturbed response, then introduces either explicit cues (e.g., "The true answer is No") or implicit contextual contradictions. By measuring the proportion of correct-to-incorrect flips, the misleading rate quantifies how often the model abandons its original correct response.

### Mechanism 2
Fine-tuning on a compact, carefully curated dataset of high-misleading-rate examples substantially improves model robustness to both explicit and implicit misleading instructions. By mixing explicit and implicit misleading instructions in a balanced 2000-sample dataset, the model learns to recognize and resist deceptive cues.

### Mechanism 3
The misleading rate metric provides a more direct and efficient measure of model uncertainty under deceptive inputs than traditional consistency rates. Instead of requiring multiple repeated runs to compute consistency, the misleading rate compares the correctness of responses before and after introducing misleading cues in a single pass.

## Foundational Learning

- **Multimodal Large Language Models (MLLMs)**: Understanding how MLLMs process both visual and textual inputs is essential to grasp why they are susceptible to misleading multimodal cues.
  - Quick check: How does an MLLM typically integrate visual features and text embeddings to produce a final answer?

- **Explicit vs. Implicit Misleading Instructions**: Differentiating between overt false-answer hints and subtle contextual contradictions is key to understanding the two-stage pipeline and the fine-tuning strategy.
  - Quick check: Give an example of an explicit misleading instruction and an implicit one for the same image-question pair.

- **Misleading Rate (MR) Metric**: The misleading rate is the core evaluation metric; understanding its calculation and interpretation is necessary to follow the results.
  - Quick check: If a model answers 10 questions correctly originally, and 7 of those become incorrect after misleading instructions, what is the MR(T→F)?

## Architecture Onboarding

- **Component map**: Image + Question (unperturbed) -> Stage 1: MLLM inference -> Original response -> Stage 2: Inject misleading instruction -> MLLM inference -> Misleading response -> Evaluation: Compare correctness before/after -> Compute misleading rate -> Optional: Fine-tuning stage
- **Critical path**: Data preparation (image, question, correct answer) -> Unperturbed inference -> Misleading instruction generation -> Misleading inference -> Correctness evaluation and misleading rate calculation
- **Design tradeoffs**: Data volume vs. robustness (2000 samples achieves significant robustness, but more data might improve further); Explicit vs. implicit mix (combining explicit instructions per sample reduces data but may oversimplify; implicit instructions kept separate preserve diversity); Evaluation speed vs. accuracy (misleading rate requires only one comparison vs. multiple runs for consistency)
- **Failure signatures**: High misleading rate indicates vulnerability; low misleading rate after fine-tuning indicates success; If misleading rate remains high after fine-tuning, the fine-tuning data may be insufficient or not representative
- **First 3 experiments**: 1) Run the two-stage pipeline on a small held-out dataset to verify the MR calculation and correlation with consistency; 2) Fine-tune a single open-source MLLM on the mixed dataset and evaluate MR before/after to confirm robustness gain; 3) Vary the proportion of explicit vs. implicit data in fine-tuning to find the optimal mix for robustness

## Open Questions the Paper Calls Out

### Open Question 1
How do different fine-tuning strategies impact the robustness of MLLMs against implicit misleading instructions? The paper compares three fine-tuning strategies (S5, C5, C10) and finds S5 performs best, but further investigation into implicit instruction robustness is needed.

### Open Question 2
What is the long-term generalization capability of fine-tuned MLLMs when exposed to novel misleading scenarios not seen during training? The study focuses on reducing misleading rates within the scope of the benchmark, but does not investigate how well the fine-tuned models adapt to completely new misleading strategies or contexts.

### Open Question 3
How does the inclusion of multimodal misleading information (e.g., misleading visual cues combined with textual instructions) affect the misleading rates of MLLMs compared to unimodal misleading information? The paper tests image-based misleading information by adding watermarks and finds higher misleading rates, but does not systematically explore combinations of visual and textual misleading cues.

## Limitations
- The fine-tuning approach's scalability and generalization to out-of-distribution misleading scenarios remains uncertain
- The evaluation focuses exclusively on visual-language models, leaving applicability to other modalities unexplored
- The study does not provide statistical significance testing or confidence intervals for the reported metrics

## Confidence

- **High Confidence**: The core empirical finding that 65% of correct answers are overturned by single deceptive cues is well-supported by extensive evaluation across twelve open-source and five closed-source models
- **Medium Confidence**: The effectiveness of fine-tuning on the compact 2000-sample dataset is demonstrated, but lack of ablation studies on dataset size or composition introduces uncertainty
- **Medium Confidence**: The correlation between misleading rate and consistency is established, but the paper does not provide statistical significance testing or confidence intervals

## Next Checks
1. **Out-of-distribution robustness test**: Evaluate the fine-tuned models on a held-out set of misleading instructions that differ structurally from the training data to assess generalization
2. **Statistical validation**: Compute confidence intervals and p-values for the misleading rate differences before and after fine-tuning across multiple runs to establish statistical significance
3. **Cross-modal generalization**: Test whether the same fine-tuning strategy reduces misleading rates in pure language models or other multimodal combinations (e.g., text-audio) to determine if the approach is modality-specific