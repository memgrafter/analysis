---
ver: rpa2
title: 'Time Will Tell: Timing Side Channels via Output Token Count in Large Language
  Models'
arxiv_id: '2412.15431'
source_url: https://arxiv.org/abs/2412.15431
tags:
- output
- token
- language
- attack
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper demonstrates a new side-channel that enables an adversary
  to extract sensitive information about inference inputs in large language models
  (LLMs) based on the number of output tokens in the LLM response. We construct attacks
  using this side-channel in two common LLM tasks: recovering the target language
  in machine translation tasks and recovering the output class in classification tasks.'
---

# Time Will Tell: Timing Side Channels via Output Token Count in Large Language Models

## Quick Facts
- **arXiv ID**: 2412.15431
- **Source URL**: https://arxiv.org/abs/2412.15431
- **Reference count**: 40
- **Primary result**: Attackers can infer target language in translation tasks with >75% precision and classification output with >70% precision by exploiting output token count variations via timing side-channels

## Executive Summary
This paper demonstrates a novel timing side-channel attack that enables adversaries to extract sensitive information about LLM inference inputs by analyzing output token counts. The attack exploits language-specific tokenization biases in translation tasks and explanation length biases in classification tasks. Due to the auto-regressive generation mechanism in LLMs, an adversary can reliably estimate output token counts using timing measurements, even over the network against commercial models like GPT-4o. The authors propose multiple mitigations including tokenizer adjustments, system-level modifications, and prompt engineering to defend against these attacks.

## Method Summary
The attack methodology involves two phases: profiling and exploitation. For translation attacks, the authors profile token densities and byte ratios across target languages using 1000 samples per language, then apply 2D Gaussian Mixture Models with Bhattacharyya Distance to classify user requests. For classification attacks, they profile explanation length biases using 200 samples per class and apply threshold-based classification. The timing side-channel is implemented by measuring total generation time and adjusting for network/system delays through concurrent profiling. The attack works in both streaming (direct token count observation) and non-streaming (timing-based estimation) modes.

## Key Results
- Achieved >75% precision in recovering target language across three different translation models (Tower, M2M100, MBart50)
- Successfully extracted output class in text classification tasks with >70% precision from open-source models (Llama-3.1, Llama-3.2, Gemma2) and production models (GPT-4o)
- Timing-based estimation reliably predicts output token count with strong linear correlation between generation time and token count
- Attack effectiveness persists over network against closed-source commercial LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attackers can infer target language by analyzing output token count variations due to language-specific tokenization efficiency
- Mechanism: Subword tokenizers exhibit biases from unbalanced training data and morphological differences. Resource-poor languages receive more tokens for similar content than high-resource languages, creating predictable token density profiles
- Core assumption: Output token count is observable through streaming or timing measurements
- Evidence anchors: Abstract mentions timing channel for output token count recovery; section 4.2 details language morphology affecting vocabulary construction; corpus shows weak evidence for tokenizer bias mechanisms
- Break condition: Uniform token density across languages or elimination of autoregressive timing

### Mechanism 2
- Claim: Attackers can infer output class by analyzing explanation length variations in classification tasks
- Mechanism: LLMs generate explanations of varying lengths based on classification output class. This bias can be amplified by few-shot examples, creating a correlation between output token count and classification result
- Core assumption: LLM generates explanations alongside classification outputs
- Evidence anchors: Abstract states inherent bias in explanation lengths; section 5.2 explains bias from few-shot examples; corpus shows weak evidence for explanation length bias mechanisms
- Break condition: Fixed-length explanations or classification format changes

### Mechanism 3
- Claim: Attackers can estimate output token count using execution time measurements
- Mechanism: Autoregressive decoding generates tokens sequentially with constant time per token. Total generation time scales linearly with output token count, enabling timing-based estimation
- Core assumption: LLM uses autoregressive generation and attacker can measure execution time accurately
- Evidence anchors: Section 2.1 describes linear relationship between tokens and time; section 6.3 confirms correlation; corpus shows weak evidence for timing-based estimation
- Break condition: Speculative decoding or other optimizations disrupting linear relationship

## Foundational Learning

- **Subword tokenization and Byte Pair Encoding**: Why needed - Understanding tokenizer bias mechanisms is fundamental to grasping language leakage; Quick check - Why do resource-poor languages typically require more tokens than English for similar content?

- **Autoregressive generation in transformers**: Why needed - Sequential token generation enables timing side-channel; Quick check - What makes decode stage time predictable and linear?

- **Statistical classification with Gaussian Mixture Models**: Why needed - Attack methodology uses GMMs for profiling and classification; Quick check - How does 2D GMM help distinguish between different languages based on token characteristics?

## Architecture Onboarding

- **Component map**: User request -> LLM processing -> Autoregressive token generation -> Output streaming/timing -> Attacker measurement and classification

- **Critical path**: 1) User sends input to LLM, 2) LLM processes input and generates tokens autoregressively, 3) Output tokens counted (streaming) or generation time measured (non-streaming), 4) Attacker correlates timing/length with target language or classification output

- **Design tradeoffs**: Streaming mode provides exact token counts but may be less efficient; non-streaming mode is more efficient but requires timing-based estimation; fixed-length explanations eliminate classification bias but reduce explainability

- **Failure signatures**: Attack success drops with speculative decoding; timing measurements become unreliable with high server load or network congestion; token density differences diminish with language-optimized tokenizers

- **First 3 experiments**: 1) Profile token densities for different languages on local LLM to verify patterns, 2) Measure generation time vs token count correlation to validate timing estimation, 3) Test classification task with biased and unbiased few-shot examples to confirm explanation length bias

## Open Questions the Paper Calls Out

- **Impact of speculative decoding**: How optimization techniques affect the timing side-channel attack remains unexplored experimentally, though the paper mentions potential disruption to linear timing relationships

- **High-throughput, high-latency scenarios**: The paper discusses continuous batching but doesn't evaluate attack performance under realistic high-throughput conditions with batching

- **Dynamic few-shot examples from RAG**: The potential for RAG-derived examples to introduce additional side-channels is mentioned but not empirically tested

## Limitations

- Attack success rates show significant variance across tasks and models, with some classification tasks achieving less than 70% precision
- Methodology relies on substantial profiling data collection (1000 samples for translation, 200 for classification), limiting practicality
- Network-based timing measurements are subject to significant noise and variability, requiring concurrent profiling techniques

## Confidence

**High Confidence** (★★★☆☆):
- Fundamental mechanism of using output token count variations to leak information is theoretically sound
- Linear relationship between token count and generation time in autoregressive LLMs is empirically validated
- Language-specific tokenization biases are documented in prior research

**Medium Confidence** (★★☆☆☆):
- Attack success rates highly dependent on specific model architectures and may not generalize across all LLM variants
- Effectiveness of timing-based estimation over network conditions not thoroughly validated across diverse network environments
- Proposed mitigations may introduce significant utility costs that aren't fully quantified

**Low Confidence** (★☆☆☆☆):
- Generalizability of classification task bias across different explanation generation methods
- Robustness of attack against active defenses like dynamic token length adjustments
- Long-term viability as LLM architectures evolve toward non-autoregressive generation

## Next Checks

1. **Timing Robustness Test**: Implement the timing attack across multiple network conditions (varying latency, packet loss, server load) to validate whether concurrent profiling maintains effectiveness under realistic network variability

2. **Mitigation Impact Assessment**: Quantitatively measure utility degradation when implementing proposed mitigations (fixed-length explanations, token count adjustments) across multiple downstream tasks to determine if security benefits justify performance costs

3. **Speculative Decoding Resistance**: Test the attack against LLMs using speculative decoding or other optimization techniques to verify whether linear timing relationship assumption holds, determining attack's longevity as optimization techniques become more prevalent