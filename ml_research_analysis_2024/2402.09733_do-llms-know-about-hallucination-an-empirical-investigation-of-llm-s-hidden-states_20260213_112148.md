---
ver: rpa2
title: Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden
  States
arxiv_id: '2402.09733'
source_url: https://arxiv.org/abs/2402.09733
tags:
- awareness
- hidden
- arxiv
- question
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) possess\
  \ awareness of hallucination by analyzing their hidden states. An experimental framework\
  \ is introduced where two inputs\u2014one with a correct answer and one with a hallucinated\
  \ answer\u2014are processed by the LLM, and their hidden states are compared."
---

# Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States

## Quick Facts
- arXiv ID: 2402.09733
- Source URL: https://arxiv.org/abs/2402.09733
- Authors: Hanyu Duan; Yi Yang; Kar Yan Tam
- Reference count: 11
- Primary result: LLMs show different hidden state responses to correct vs hallucinated answers, with final states more influenced by correct responses.

## Executive Summary
This paper investigates whether large language models (LLMs) possess awareness of hallucination by analyzing their hidden states. The authors introduce an experimental framework where two inputs—one with a correct answer and one with a hallucinated answer—are processed by the LLM, and their hidden states are compared. Experiments on LLaMA-2 models using TruthfulQA and HaluEval datasets reveal that LLMs react differently to correct versus hallucinated responses, with their final hidden states being more susceptible to correct answers. Awareness of hallucination varies across models and can be influenced by prompting strategies and the inclusion of reference knowledge. The transition between hidden states encodes truthfulness information, which can be leveraged to mitigate hallucination.

## Method Summary
The study processes pairs of correct and hallucinated inputs through LLaMA-2 models, extracting three critical hidden states: s1 (question), s2 (hallucinated input), and s3 (correct input) from the final transformer layer. The framework calculates awareness scores using cosine similarity differences between hidden states and performs statistical tests for significance. Interpretation techniques including PCA are applied to derive truthfulness directions, and attention manipulation experiments block input components to understand information flow. The approach analyzes whether LLMs can differentiate between correct and hallucinated responses at the hidden state level.

## Key Results
- LLMs show statistically significant different hidden state responses when processing correct versus hallucinated answers
- Final hidden states are more susceptible to influence from correct responses compared to hallucinated ones
- Awareness of hallucination varies across model sizes (7B vs 13B) and can be enhanced by including reference knowledge
- PCA analysis reveals that transition vectors between hidden states encode truthfulness information in principal components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit different hidden state responses when processing correct vs hallucinated answers.
- Mechanism: Comparing hidden states of correct vs hallucinated answers reveals discriminative power in capturing truthfulness.
- Core assumption: Hidden states capture sufficient semantic information to differentiate correct vs incorrect responses.
- Evidence anchors: Empirical findings show LLMs react differently to genuine vs fabricated responses; final hidden state is more influenced by correct responses.
- Break condition: If hidden states become too abstract or compressed, they may lose discriminative power.

### Mechanism 2
- Claim: The direction of hidden state transition encodes truthfulness information.
- Mechanism: PCA on transition vectors (s1-s2 for hallucinated, s1-s3 for correct) reveals principal components aligned with correctness.
- Core assumption: Principal components of transition vectors capture semantic directions related to truthfulness.
- Evidence anchors: PCA identifies principal components for correct and hallucinated transition vectors; tokens relate to information truthfulness.
- Break condition: If transition vectors become too similar or PCA fails to find meaningful components, truthfulness encoding breaks down.

### Mechanism 3
- Claim: Including reference knowledge improves LLM awareness of hallucinations.
- Mechanism: External knowledge allows cross-referencing and validation of responses, increasing hallucination awareness.
- Core assumption: LLMs can effectively use external knowledge to validate responses.
- Evidence anchors: Including reference knowledge significantly improves awareness, particularly for LLaMA-2-Chat 7B; awareness varies with prompting strategies.
- Break condition: If external knowledge is irrelevant or contradictory, it may confuse rather than improve awareness.

## Foundational Learning

- Concept: Cosine similarity for hidden state comparison
  - Why needed here: Used to quantify the influence of correct vs hallucinated answers on the final hidden state
  - Quick check question: How would you compute the similarity between two hidden state vectors?

- Concept: Principal Component Analysis (PCA)
  - Why needed here: Used to identify principal components of transition vectors that encode truthfulness information
  - Quick check question: What does PCA do to a set of vectors?

- Concept: Attention weight manipulation
  - Why needed here: Used to examine which input components (question, answer) are critical for hallucination detection
  - Quick check question: How would you block attention from one token to another?

## Architecture Onboarding

- Component map: Question + Correct/Hallucinated answer → Tokenization → Transformer layers → Hidden states (s1, s2, s3) → Analysis
- Critical path: Input → Tokenization → Transformer layers → Hidden states → Analysis
- Design tradeoffs:
  - Model size: Larger models may be more confident but less aware of hallucinations
  - Reference knowledge: Including knowledge improves awareness but adds complexity
  - Layer selection: Middle layers are better for hallucination detection than early or late layers
- Failure signatures:
  - Awareness score close to zero: LLM cannot differentiate correct vs hallucinated answers
  - PCA components not aligned with truthfulness: Transition vectors don't encode meaningful information
  - Attention blocking has no effect: Input components are not critical for hallucination detection
- First 3 experiments:
  1. Compare awareness scores across model sizes (7B vs 13B)
  2. Test effect of including/excluding reference knowledge on awareness
  3. Analyze attention patterns when blocking question tokens

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the hidden states of LLMs react to different categories of hallucination, such as factual fabrication, instruction inconsistency, and logical inconsistency?
- Basis in paper: The paper mentions it does not differentiate between fine-grained categories of hallucination and suggests future work could explore how LLMs react to different types of hallucination.
- Why unresolved: The current experimental framework does not account for nuances between different hallucination types.
- What evidence would resolve it: Experiments categorizing and separately analyzing LLM's hidden states for each type of hallucination.

### Open Question 2
- Question: How do intermediate transformer layers contribute to the formation and detection of hallucination in LLMs?
- Basis in paper: The paper notes it focuses only on the final hidden state and suggests exploring intermediate layers could be interesting.
- Why unresolved: The study does not examine hidden states in intermediate transformer layers.
- What evidence would resolve it: Analyzing hidden states across all transformer layers to track changes and identify patterns associated with hallucination formation and detection.

### Open Question 3
- Question: Can the experimental framework be adapted to assess hallucination in more complex or domain-specific tasks, such as numerical reasoning or financial text comprehension?
- Basis in paper: The paper states its experiments are performed on conventional QA tasks and suggests exploring more complicated or domain-specific tasks as future work.
- Why unresolved: The current framework is tailored to simple QA tasks and may not capture intricacies of hallucination in specialized domains.
- What evidence would resolve it: Adapting the framework to domain-specific tasks and evaluating its effectiveness in detecting and understanding hallucination in those contexts.

## Limitations

- Analysis is limited to LLaMA-2 models, restricting generalizability across different architectures
- Focus on final-layer hidden states may miss awareness present in intermediate layers
- Experimental framework assumes availability of correct answers, which is impractical in real-world hallucination detection

## Confidence

- **High Confidence**: LLMs show measurable differences in hidden states when processing correct versus hallucinated answers
- **Medium Confidence**: Middle layers are better for hallucination detection than early/late layers
- **Medium Confidence**: Including reference knowledge improves hallucination awareness
- **Low Confidence**: Hidden state analysis can be directly used for hallucination mitigation in deployed systems

## Next Checks

1. **Architecture Generalization Test**: Replicate hidden state analysis across multiple LLM architectures (GPT, Claude, Mistral) to determine if observed awareness patterns are architecture-specific or universal.

2. **Temporal Stability Analysis**: Evaluate whether hallucination awareness in hidden states degrades over model fine-tuning or with continued pretraining, and whether this correlates with performance on hallucination benchmarks.

3. **Real-world Deployment Validation**: Test whether PCA-derived truthfulness directions can actually improve hallucination detection in practical scenarios where ground truth answers are unavailable, using a blind evaluation with human annotators.