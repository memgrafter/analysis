---
ver: rpa2
title: Is Function Similarity Over-Engineered? Building a Benchmark
arxiv_id: '2410.22677'
source_url: https://arxiv.org/abs/2410.22677
tags:
- function
- binary
- functions
- https
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REFuSe-BENCH, a new benchmark for binary
  function similarity detection (BFSD) that addresses limitations in existing datasets
  and evaluation methods. The benchmark includes high-quality datasets reflecting
  real-world use cases, with proper labeling and diverse data including Windows binaries
  and malware.
---

# Is Function Similarity Over-Engineered? Building a Benchmark

## Quick Facts
- arXiv ID: 2410.22677
- Source URL: https://arxiv.org/abs/2410.22677
- Authors: Rebecca Saul; Chang Liu; Noah Fleischmann; Richard Zak; Kristopher Micinski; Edward Raff; James Holt
- Reference count: 40
- Primary result: REFuSe, a simple CNN-based model using raw byte features, achieves state-of-the-art performance on binary function similarity detection across multiple datasets.

## Executive Summary
This paper challenges the assumption that complex models with engineered features are necessary for binary function similarity detection (BFSD). The authors introduce REFuSe-BENCH, a comprehensive benchmark with high-quality datasets reflecting real-world use cases, including Windows binaries and malware. The key contribution is REFuSe, a simple baseline model that operates directly on raw function bytes using a basic CNN architecture, achieving state-of-the-art performance without requiring disassembly or other preprocessing. The results demonstrate that sophisticated feature engineering may be over-engineering for BFSD tasks, with simpler approaches providing significant practical value.

## Method Summary
REFuSe-BENCH addresses limitations in existing BFSD datasets by providing properly labeled data with diverse real-world binaries including Windows and malware samples. The REFuSe model modifies the MalConv architecture to extract function embeddings rather than binary classifications, operating directly on raw function bytes. It uses triplet learning with cosine distance to train embeddings where similar functions are close together and dissimilar functions are far apart. The model is trained on the Assemblage dataset and evaluated using Mean Reciprocal Rank (MRR) across multiple datasets. A key innovation is the exploration of different label normalization schemes, finding that simplified labeling can significantly improve performance by reducing false negatives.

## Key Results
- REFuSe achieves highest MRR scores across multiple datasets including Assemblage, MOTIF, and Marcelli Dataset-1
- Performance varies significantly (0.088 to 0.731 MRR) based on labeling scheme choices
- Raw byte features without disassembly are sufficient for competitive BFSD performance
- Simple CNN architecture outperforms more complex graph neural network approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Raw byte features without disassembly are sufficient for binary function similarity detection.
- Mechanism: The convolutional neural network directly learns byte-level patterns that correlate with function semantics, bypassing the need for expensive feature engineering.
- Core assumption: Function semantics can be inferred from raw byte sequences without requiring structural information like control flow graphs or assembly instructions.
- Evidence anchors:
  - [abstract] "Our benchmark reveals that a new, simple baseline — one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing — is able to achieve state-of-the-art performance in multiple settings."
  - [section 4] "REFuSe modifies MalConv to extract embeddings rather than binary classifications" and "The model takes in a function's raw bytes as input"
  - [corpus] Weak - only general mention of related papers without specific byte-based approach validation
- Break condition: When function semantics depend critically on structural relationships that cannot be captured through byte-level patterns alone.

### Mechanism 2
- Claim: Triplet learning with cosine distance is effective for binary function similarity tasks.
- Mechanism: The push-pull mechanism in triplet loss trains the model to embed similar functions close together and dissimilar functions far apart in the embedding space, measured by cosine distance.
- Core assumption: Cosine distance is a suitable metric for measuring similarity between binary function embeddings, and triplet mining can effectively sample informative training pairs.
- Evidence anchors:
  - [section 4.1] "We used the version of triplet loss popularized in Schroff et. al. [51], with the loss function L = ... for an embedding function f, distance function d, and margin α" and "we used the cosine distance, which is agnostic to vector magnitudes and has greater theoretical support for out-of-distribution detection"
  - [section 5] REFuSe achieves highest MRR scores across multiple datasets using this approach
  - [corpus] No direct corpus evidence for this specific triplet + cosine distance combination in BFSD
- Break condition: When the embedding space cannot be meaningfully partitioned by cosine distance, such as when function similarity requires more nuanced multi-dimensional relationships.

### Mechanism 3
- Claim: Simplified labeling schemes can improve model performance by reducing false negatives.
- Mechanism: By relaxing strict label matching rules (e.g., allowing functions with same name but different source code to share labels), the model receives more positive training signal and achieves better generalization.
- Core assumption: Functions with the same name across different binaries are likely semantically similar enough to benefit from shared labels, even if they originate from different source code.
- Evidence anchors:
  - [section 5] "In light of these observations, we explored the impact of various label normalization schemes on REFuSe's performance" and "we chose this labeling scheme because it aligns with the labeling schemes of the other models we evaluated against"
  - [section 5] Table 3 shows MRR improvement from 0.088 to 0.731 with relaxed labeling
  - [corpus] No corpus evidence for this specific labeling relaxation approach
- Break condition: When function name reuse is highly inconsistent across projects, causing semantically different functions to be incorrectly grouped.

## Foundational Learning

- Concept: Binary Function Similarity Detection (BFSD)
  - Why needed here: This is the core problem being addressed, where functions are compared without source code
  - Quick check question: What makes BFSD challenging compared to source code similarity detection?

- Concept: Triplet Learning
  - Why needed here: The training method used for REFuSe that learns embeddings through anchor-positive-negative relationships
  - Quick check question: How does triplet loss differ from standard classification loss functions?

- Concept: Mean Reciprocal Rank (MRR)
  - Why needed here: The primary evaluation metric that measures how quickly the correct function appears in ranked search results
  - Quick check question: Why is MRR more appropriate than accuracy for information retrieval tasks?

## Architecture Onboarding

- Component map: Input layer (byte embedding) → 1D convolutional layers → temporal max pooling → fully connected layer → embedding output
- Critical path: Raw bytes → byte embedding → convolutional feature extraction → pooling → embedding generation → similarity computation via cosine distance
- Design tradeoffs: Simple CNN vs. complex graph neural networks - trade computational efficiency for potential loss in structural understanding
- Failure signatures: Poor performance on functions with similar byte patterns but different semantics; sensitivity to label normalization choices
- First 3 experiments:
  1. Train REFuSe on a small subset of Assemblage data and evaluate MRR to verify basic functionality
  2. Compare REFuSe performance with and without byte embedding layer to assess its contribution
  3. Test different triplet margin values (α) to find optimal separation in embedding space

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper limit of REFuSe's performance when trained on larger datasets with more diverse function representations?
- Basis in paper: [explicit] The paper demonstrates REFuSe achieves state-of-the-art performance on multiple datasets, but its architecture is relatively simple compared to more complex models.
- Why unresolved: The current benchmark uses a specific dataset size and diversity level. Scaling to larger, more diverse datasets with potentially more complex function relationships could reveal limitations of the simple CNN architecture.
- What evidence would resolve it: Training REFuSe on datasets an order of magnitude larger with more diverse function types and architectures, then comparing its performance against state-of-the-art complex models.

### Open Question 2
- Question: How does REFuSe's performance degrade when dealing with obfuscated or heavily optimized binaries compared to models using disassembly-based features?
- Basis in paper: [inferred] The paper notes that complex models often rely on disassembly and control-flow graphs, which are vulnerable to obfuscation, but REFuSe operates directly on raw bytes.
- Why unresolved: While raw bytes are theoretically more robust to obfuscation, the actual impact on similarity detection accuracy hasn't been empirically measured against obfuscated samples.
- What evidence would resolve it: Creating obfuscated versions of benchmark datasets and measuring performance degradation of REFuSe versus disassembly-based models.

### Open Question 3
- Question: What is the optimal labeling scheme for function similarity that balances false positives and false negatives across different application domains?
- Basis in paper: [explicit] The paper demonstrates significant performance variation (0.63 MRR difference) based on different labeling schemes and acknowledges this is an under-explored area.
- Why unresolved: The paper uses heuristic-based labeling but doesn't provide a comprehensive framework for determining optimal labeling criteria across different use cases (malware analysis vs vulnerability detection vs reverse engineering).
- What evidence would resolve it: Developing a systematic method for evaluating labeling schemes based on downstream task performance across multiple domains.

### Open Question 4
- Question: How does the computational efficiency of REFuSe scale when applied to real-world deployment scenarios involving millions of functions?
- Basis in paper: [inferred] The paper notes that complex models face scalability challenges due to expensive feature extraction, but doesn't provide detailed performance metrics for REFuSe at scale.
- Why unresolved: While REFuSe avoids expensive preprocessing, its CNN-based architecture's computational requirements at the scale of real-world deployments (millions of binaries) haven't been characterized.
- What evidence would resolve it: Benchmarking REFuSe's inference time and memory usage on progressively larger datasets approaching real-world deployment scales.

## Limitations

- Labeling Scheme Dependency: REFuSe's performance varies significantly (0.088 to 0.731 MRR) based on label normalization choices, raising concerns about robustness and generalizability.
- Out-of-Distribution Generalization: Limited evaluation of cross-project and zero-shot scenarios where functions from completely different codebases need to be compared.
- Corpus Validation Gap: Lack of direct validation from related work for the specific mechanisms proposed (raw byte features, triplet learning with cosine distance, simplified labeling).

## Confidence

- **High**: The core claim that raw byte features can achieve competitive performance in BFSD tasks is well-supported by the experimental results.
- **Medium**: The assertion that triplet learning with cosine distance is effective for BFSD is supported by REFuSe's performance, but lacks direct validation from related work.
- **Low**: The claim that simplified labeling schemes can improve model performance is primarily based on empirical observations within the paper, with limited external validation.

## Next Checks

1. **Cross-Project Performance Evaluation**: Conduct experiments to assess REFuSe's performance on cross-project and zero-shot scenarios, where functions from completely different codebases need to be compared. This will provide a more comprehensive understanding of the model's generalizability.

2. **Ablation Study on Mechanisms**: Perform an ablation study to isolate the contribution of each mechanism (raw byte features, triplet learning, simplified labeling) to REFuSe's performance. This will help identify which components are truly essential and which might be replaceable with simpler alternatives.

3. **Validation Against Related Work**: Compare REFuSe's mechanisms (raw byte features, triplet learning, simplified labeling) with specific approaches mentioned in related work to assess their novelty and effectiveness. This could involve reimplementing key components from related papers and directly comparing performance.