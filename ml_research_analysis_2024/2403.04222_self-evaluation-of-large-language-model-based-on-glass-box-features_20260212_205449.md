---
ver: rpa2
title: Self-Evaluation of Large Language Model based on Glass-box Features
arxiv_id: '2403.04222'
source_url: https://arxiv.org/abs/2403.04222
tags:
- evaluation
- self-evaluation
- prompt
- glass-box
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores using model internal information\u2014specifically\
  \ softmax distribution entropy and variance\u2014to enable Large Language Models\
  \ to evaluate their own outputs. It compares glass-box features with traditional\
  \ external evaluation methods and finds that softmax-based metrics strongly correlate\
  \ with human annotations, outperforming both GPT-3.5 and Auto-J baselines on MT-Bench\
  \ and Vicuna-Bench."
---

# Self-Evaluation of Large Language Model based on Glass-box Features

## Quick Facts
- arXiv ID: 2403.04222
- Source URL: https://arxiv.org/abs/2403.04222
- Authors: Hui Huang; Yingqi Qu; Jing Liu; Muyun Yang; Bing Xu; Tiejun Zhao; Wenpeng Lu
- Reference count: 4
- One-line primary result: Softmax-based glass-box features enable LLMs to self-evaluate outputs without external APIs, outperforming traditional evaluation baselines.

## Executive Summary
This paper introduces a novel approach for Large Language Models to evaluate their own outputs using internal model information, specifically softmax distribution entropy and variance. The method avoids privacy concerns and reproducibility issues associated with external APIs while achieving strong correlation with human annotations on established benchmarks. The approach demonstrates that glass-box features can serve as reliable quality indicators for self-evaluation, offering a promising path for applications like self-reflection and reward modeling.

## Method Summary
The method extracts softmax distribution entropy and variance at each decoding step during response generation, using these glass-box features to evaluate output quality. The approach optionally incorporates reference augmentation through in-context illustration (prefixing the evaluation prompt with the reference) and probability calibration (quantifying bias by calculating the log-probability of reference answers). The evaluation process combines these features to produce a self-evaluation score that correlates strongly with human annotations, eliminating the need for external evaluators.

## Key Results
- Glass-box features (softmax entropy/variance) achieve 86.7% correlation with human annotations on MT-Bench
- The approach outperforms both GPT-3.5 and Auto-J baselines on MT-Bench and Vicuna-Bench
- Adding reference-based calibration further improves accuracy by mitigating evaluation bias
- The method avoids privacy concerns and reproducibility issues associated with external APIs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Softmax distribution entropy and variance serve as reliable quality indicators for self-evaluation.
- Mechanism: The entropy of the softmax distribution at each decoding step captures the model's confidence in its predictions. High entropy indicates uncertainty and potentially lower quality responses, while low entropy suggests high confidence and likely better quality. Variance of word-level log-probabilities provides additional information about the dispersion of probabilities.
- Core assumption: The confidence of the model, as measured by the softmax distribution, is strongly correlated with the quality of the generated response.
- Evidence anchors:
  - [abstract] "Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features."
  - [section] "Our findings reveal that manipulating the softmax distribution by calculating its entropy and variance exhibits a strong correlation with annotated evaluation results."

### Mechanism 2
- Claim: In-context learning with reference examples improves self-evaluation accuracy.
- Mechanism: By prefixing the evaluation prompt with the instruction and its reference as in-context demonstration, the model tends to generate a similar softmax distribution as it would for the reference, allowing the evaluation to represent the difference between the current answer and the golden answer.
- Core assumption: The model can leverage in-context learning to align its evaluation process with the reference, making the softmax distribution of the evaluated response more comparable to the reference.
- Evidence anchors:
  - [section] "By prefixing the reference, the model tends to generate a similar softmax distribution... the resulting softmax distribution could represent the difference between the current answer and the golden answer, effectively indicating the quality."

### Mechanism 3
- Claim: Probability calibration using reference answers mitigates evaluation bias.
- Mechanism: By calculating the log-probability of the reference answer, the model quantifies its own bias towards superficial quality aspects. Subtracting this bias from the self-evaluation results improves accuracy by aligning the evaluation more closely with human annotations.
- Core assumption: The model's self-evaluation is biased, and this bias can be quantified and mitigated using the reference answer's log-probability.
- Evidence anchors:
  - [section] "As the reference should always be assigned with a maximum score, we can quantify the bias of the model by calculate the log-probability of reference answer... the bias of the self-evaluation can be mitigated by subtracting the result with SentProb-Ref, thereby improving the evaluation accuracy."

## Foundational Learning

- Concept: Softmax function and entropy
  - Why needed here: Understanding the softmax function is crucial as it forms the basis of the confidence metrics (entropy and variance) used for self-evaluation.
  - Quick check question: What does a high entropy value of the softmax distribution indicate about the model's confidence in its predictions?

- Concept: In-context learning
  - Why needed here: In-context learning is the mechanism by which the model leverages reference examples to improve its evaluation process.
  - Quick check question: How does in-context learning with reference examples improve the model's ability to evaluate its own responses?

- Concept: Bias mitigation in machine learning
  - Why needed here: Bias mitigation is a common challenge in machine learning, and understanding how to quantify and mitigate bias is crucial for improving the accuracy of self-evaluation.
  - Quick check question: Why is it important to mitigate bias in self-evaluation, and how does the proposed method attempt to do this?

## Architecture Onboarding

- Component map: Transformer-based LLM -> Glass-box feature extraction module -> Self-evaluation module -> Reference augmentation strategies

- Critical path:
  1. Input instruction and generate response using the LLM.
  2. Extract glass-box features from the generation process (softmax distribution, uncertainty estimation, attention distribution).
  3. Apply reference augmentation strategies if reference is available (in-context illustration and probability calibration).
  4. Combine glass-box features to compute self-evaluation score.
  5. Compare self-evaluation score with human annotations or use it for downstream tasks.

- Design tradeoffs:
  - Using glass-box features avoids privacy concerns and reproducibility issues associated with external APIs but may not capture all aspects of response quality that external evaluators might consider.
  - In-context learning with references improves accuracy but requires references to be available, which may not always be the case.
  - Probability calibration mitigates bias but relies on the assumption that the reference should always be assigned a maximum score, which may not hold in all evaluation scenarios.

- Failure signatures:
  - Poor correlation between glass-box features and human annotations.
  - In-context learning fails to align the model's evaluation process with the reference.
  - Probability calibration introduces more bias or fails to mitigate existing bias.
  - Self-evaluation scores are consistently lower or higher than human annotations, indicating systematic bias.

- First 3 experiments:
  1. Evaluate the correlation between softmax distribution entropy/variance and human annotations on a small dataset.
  2. Test the effectiveness of in-context learning with references on a subset of the evaluation benchmarks.
  3. Assess the impact of probability calibration on mitigating evaluation bias using a set of known biased responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the softmax distribution entropy and variance metrics be further improved to better correlate with human annotations?
- Basis in paper: The paper shows that softmax-based metrics strongly correlate with human annotations but suggests that combining both entropy and variance yields further improvement.
- Why unresolved: The paper does not explore other potential ways to combine or enhance these metrics.
- What evidence would resolve it: Experiments comparing different combinations of softmax-based metrics and their correlation with human annotations.

### Open Question 2
- Question: Can the proposed glass-box features be used for self-evaluation of LLMs on tasks other than response generation?
- Basis in paper: The paper focuses on response generation tasks but does not explore other potential applications.
- Why unresolved: The paper does not investigate the generalizability of the proposed method to other LLM tasks.
- What evidence would resolve it: Experiments applying the glass-box features to self-evaluation of LLMs on tasks like question answering, summarization, or code generation.

### Open Question 3
- Question: How does the proposed self-evaluation method perform with larger LLMs (e.g., >7B parameters)?
- Basis in paper: The paper mentions that experiments are conducted on 7B-sized models and suggests that incorporating larger models would provide a more thorough evaluation.
- Why unresolved: The paper does not evaluate the proposed method on larger models.
- What evidence would resolve it: Experiments comparing the performance of the proposed method on LLMs of different sizes.

## Limitations

- The method relies on the assumption that softmax entropy/variance directly reflects response quality, which may not hold for all types of questions or model architectures.
- The effectiveness of reference augmentation strategies depends heavily on the quality and similarity of reference answers, which may not be available for all evaluation scenarios.
- The paper does not address potential adversarial cases where high-quality responses might have high entropy or where low-entropy responses might be factually incorrect.

## Confidence

- High confidence: The correlation results between glass-box features and human annotations are well-supported by experimental data from two established benchmarks (MT-Bench and Vicuna-Bench).
- Medium confidence: The effectiveness of reference augmentation strategies (in-context illustration and probability calibration) is demonstrated but may not generalize to all evaluation contexts or model types.
- Medium confidence: The claim that this approach avoids privacy concerns and reproducibility issues compared to external APIs is valid but not extensively validated across different deployment scenarios.

## Next Checks

1. **Adversarial Case Testing**: Evaluate the method on intentionally constructed adversarial cases where high-quality responses have high entropy (creative writing, diverse perspectives) to identify potential failure modes and limitations of the softmax-based metrics.

2. **Cross-Architecture Generalization**: Test the glass-box self-evaluation approach on different model architectures (e.g., GPT-style vs. encoder-decoder models) and sizes (1B vs. 70B parameters) to validate whether the entropy/variance correlation holds across the broader LLM landscape.

3. **Real-World Deployment Simulation**: Implement a simulation where the self-evaluation system must operate without access to reference answers, using only the glass-box features, and compare performance against the reference-augmented approach to quantify the practical value of the calibration methods in deployment scenarios.