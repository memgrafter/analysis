---
ver: rpa2
title: Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model
  through Reinforcement Learning from AI Feedback
arxiv_id: '2411.00897'
source_url: https://arxiv.org/abs/2411.00897
tags:
- data
- language
- medical
- large
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for enhancing large language models
  for Traditional Chinese Medicine (TCM) tasks using limited data. The method combines
  supervised fine-tuning (SFT) with reinforcement learning from AI feedback (RLAIF)
  to improve model performance on TCM diagnosis and prescription tasks.
---

# Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback

## Quick Facts
- arXiv ID: 2411.00897
- Source URL: https://arxiv.org/abs/2411.00897
- Reference count: 32
- Key outcome: Presents a framework combining supervised fine-tuning with reinforcement learning from AI feedback to enhance large language models for Traditional Chinese Medicine tasks using limited data

## Executive Summary
This paper introduces a novel framework that enhances large language models' capabilities in Traditional Chinese Medicine by combining supervised fine-tuning with reinforcement learning from AI feedback. The approach addresses the challenge of limited TCM data by leveraging automated AI-based labeling and ranking to create preference data for direct preference optimization training. The framework demonstrates significant improvements over traditional methods, achieving superior performance on TCM diagnosis and prescription tasks while maintaining alignment with expert preferences, even with small amounts of high-quality TCM data.

## Method Summary
The method combines supervised fine-tuning (SFT) with reinforcement learning from AI feedback (RLAIF) to improve model performance on TCM tasks. The process involves collecting real medical case data, performing SFT on open-source LLMs, generating diverse outputs, and using automated AI-based labeling and ranking to create preference data for DPO training. This approach effectively addresses the data scarcity challenge in specialized domains like TCM while maintaining alignment with expert preferences.

## Key Results
- Models trained with the framework significantly outperform zero-shot, few-shot, and SFT-only baselines on TCM tasks
- Achieved superior performance on ROUGE, BLEU, and BERT-Score metrics
- Framework proves effective even with small amounts of high-quality TCM data

## Why This Works (Mechanism)
The framework's effectiveness stems from its hybrid approach that leverages both supervised fine-tuning and reinforcement learning from AI feedback. SFT establishes the foundational TCM knowledge from domain-specific data, while RLAIF provides continuous refinement through preference learning. The automated AI-based labeling and ranking system creates scalable preference data that would be prohibitively expensive to collect from human experts. This combination allows the model to learn not just the factual knowledge of TCM but also the implicit preferences and decision-making patterns of experienced practitioners, resulting in outputs that better align with expert judgment while maintaining clinical relevance.

## Foundational Learning
- Reinforcement Learning from AI Feedback (RLAIF): Why needed - provides scalable feedback mechanism without human experts; Quick check - verify automated feedback quality matches expert judgment
- Direct Preference Optimization (DPO): Why needed - efficient fine-tuning method for preference learning; Quick check - confirm preference ranking consistency across different runs
- Supervised Fine-Tuning (SFT): Why needed - establishes baseline knowledge from domain-specific data; Quick check - measure task-specific performance improvements

## Architecture Onboarding

Component Map:
TCM Case Data -> SFT Training -> RLAIF Pipeline -> Preference Data Generation -> DPO Training -> Enhanced LLM

Critical Path:
TCM Case Data -> SFT Training -> Diverse Output Generation -> AI-based Labeling -> Preference Data -> DPO Training

Design Tradeoffs:
- Synthetic vs. human preference data (scalability vs. quality)
- Automated labeling accuracy vs. computational cost
- Model size and capability vs. training efficiency

Failure Signatures:
- Inconsistent preference rankings
- Degraded performance on baseline metrics
- Loss of general knowledge during fine-tuning

First Experiments:
1. Baseline SFT performance comparison on TCM tasks
2. RLAIF pipeline output quality validation
3. Preference data generation consistency check

## Open Questions the Paper Calls Out
The paper identifies several open questions that warrant further investigation. How does the framework perform when scaled to larger, more diverse datasets? What is the long-term stability of the RLAIF-enhanced models, and do they maintain performance over time without additional fine-tuning? Can the approach be effectively extended to other specialized medical domains beyond Traditional Chinese Medicine? Additionally, the paper questions whether alternative preference learning methods might yield even better results than the current DPO approach, and how the balance between synthetic and human-generated preference data affects final model quality.

## Limitations
- Use of synthetic data for preference learning rather than expert human feedback
- Focus on specific TCM tasks without exploring broader medical knowledge domains
- Evaluation metrics primarily designed for text generation rather than medical accuracy
- Assumption that AI-generated rankings correlate with actual clinical efficacy
- Limited exploration of potential biases introduced by automated preference labeling

## Confidence
High confidence in technical methodology combining SFT with RLAIF
Medium confidence in reported performance improvements given synthetic preference data
Low confidence in clinical relevance without human expert validation

## Next Checks
1. Conduct blinded study comparing model outputs with expert TCM practitioner assessments
2. Test framework effectiveness across multiple specialized domains beyond TCM
3. Implement longitudinal study tracking model performance over time with additional data points
4. Validate whether AI-generated preference rankings actually improve clinical outcomes
5. Explore hybrid approaches combining synthetic and human preference data