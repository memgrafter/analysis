---
ver: rpa2
title: Variational Entropy Search for Adjusting Expected Improvement
arxiv_id: '2402.11345'
source_url: https://arxiv.org/abs/2402.11345
tags:
- function
- entropy
- variational
- acquisition
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that Expected Improvement (EI) is a special
  case of Max-value Entropy Search (MES) under variational inference, and introduces
  the Variational Entropy Search (VES) framework. By approximating the unknown density
  in MES with a parameterized family of distributions, the authors derive a lower
  bound on the MES acquisition function, called ESLB.
---

# Variational Entropy Search for Adjusting Expected Improvement

## Quick Facts
- arXiv ID: 2402.11345
- Source URL: https://arxiv.org/abs/2402.11345
- Reference count: 22
- Variational Entropy Search (VES) framework derives Expected Improvement (EI) as a special case of Max-value Entropy Search (MES) under variational inference

## Executive Summary
This paper introduces Variational Entropy Search (VES), a framework that unifies Expected Improvement (EI) and Max-value Entropy Search (MES) through variational inference. By approximating the intractable posterior density in MES with a parameterized family of distributions, the authors derive a lower bound called ESLB. When restricted to exponential distributions, ESLB reduces exactly to EI, establishing a theoretical connection between these acquisition functions. The framework is extended to Gamma distributions, resulting in VES-Gamma, which includes an "anti-EI" term to reduce over-exploitation and improve exploration-exploitation balance.

## Method Summary
The VES framework uses variational inference to approximate the unknown posterior density p(y*|y_x,Dt) in MES with a tractable variational distribution q(y*|y_x,Dt). This leads to a closed-form expression for the MES acquisition function called ESLB. When the variational family is restricted to exponential distributions, ESLB reduces to EI. Extending to Gamma distributions introduces an "anti-EI" term that helps balance exploration and exploitation. The method uses Gaussian process regression with a Matern 5/2 kernel and optimizes over a grid to find the next evaluation point. Implementation involves iterating between sampling from the posterior, solving for optimal variational parameters, and maximizing ESLB.

## Key Results
- VES-Gamma outperforms both EI and MES on benchmark test functions (Rosenbrock, Three-Hump Camel, Himmelblau's) in terms of log regret across 100 iterations
- VES-Gamma shows superior performance on real-world datasets (diabetes and iris) with 45 iterations, reporting mean and 0.1 standard deviation
- VES-Gamma demonstrates stronger dependence on initialization points compared to EI and MES

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VES works by replacing the intractable posterior density p(y*|y_x,Dt) in MES with a tractable variational distribution q(y*|y_x,Dt), leading to a lower bound on the acquisition function.
- Mechanism: The VES framework uses variational inference to approximate the unknown posterior density p(y*|y_x,Dt) with a parameterized family of distributions q(y*|y_x,Dt). This approximation enables a closed-form expression for the MES acquisition function, called ESLB (Entropy Search Lower Bound), which serves as a lower bound on the true MES acquisition function.
- Core assumption: The variational distribution q(y*|y_x,Dt) can effectively approximate the true posterior density p(y*|y_x,Dt) within the chosen family of distributions.
- Evidence anchors:
  - [abstract] "By approximating the unknown density in MES with a parameterized family of distributions, the authors derive a lower bound on the MES acquisition function, called ESLB."
  - [section 3] "In this study, we introduce variational distributions as alternatives to the sampling-based strategies that are typically used in other ES methods for approximating the ES acquisition function."
- Break condition: If the chosen family of variational distributions is too restrictive or poorly matches the true posterior, the approximation error becomes large, reducing the effectiveness of VES.

### Mechanism 2
- Claim: When the variational family is restricted to exponential distributions, ESLB reduces exactly to Expected Improvement (EI).
- Mechanism: By restricting the variational distribution q(y*|y_x,Dt) to exponential distributions, the ESLB formula simplifies to the Expected Improvement acquisition function. This establishes a direct link between EI and MES, showing that EI is a special case of MES under variational inference.
- Core assumption: The exponential distribution is a valid member of the variational family and can represent the posterior density p(y*|y_x,Dt) in certain cases.
- Evidence anchors:
  - [abstract] "When restricted to exponential distributions, ESLB reduces exactly to EI."
  - [section 3] "When the family Qexp is selected as in Equation (3.4) and function evaluation is noise-free, the output of Algorithm 1 is equivalent to the output of the EI method."
- Break condition: If the true posterior density p(y*|y_x,Dt) is not well-approximated by an exponential distribution, the connection between EI and MES breaks down, and the benefits of VES may not be realized.

### Mechanism 3
- Claim: Extending the variational family to Gamma distributions introduces an "anti-EI" term that helps balance exploration and exploitation, reducing over-exploitation.
- Mechanism: By expanding the variational family to Gamma distributions, the ESLB formula includes an additional "anti-EI" term. This term encourages the selection of x values that result in lower GP evaluations, acting as a regulatory mechanism to balance the potential over-exploitation tendency of EI.
- Core assumption: The Gamma distribution is a more flexible family than exponential distributions and can better capture the characteristics of the posterior density p(y*|y_x,Dt).
- Evidence anchors:
  - [abstract] "The framework is then extended to Gamma distributions, leading to the VES-Gamma algorithm, which includes an 'anti-EI' term to reduce over-exploitation."
  - [section 3] "The 'anti-EI' term, as implied by its name, acts in contrast to the EI term. Maximizing this term with respect to x encourages the selection of x values that result in lower GP evaluations, serving as a regulatory mechanism to balance the potential over-exploitation tendency of EI."
- Break condition: If the Gamma distribution still fails to capture the true posterior density p(y*|y_x,Dt), the "anti-EI" term may not effectively balance exploration and exploitation, leading to suboptimal performance.

## Foundational Learning

- Concept: Gaussian Processes (GPs)
  - Why needed here: GPs are used as the surrogate model for the black-box function f in Bayesian optimization. They provide a probabilistic framework for modeling the unknown function and quantifying uncertainty.
  - Quick check question: What are the key components of a Gaussian Process, and how are they used to make predictions at new input points?

- Concept: Acquisition Functions
  - Why needed here: Acquisition functions guide the selection of the next point to evaluate in Bayesian optimization. They balance exploration (reducing uncertainty) and exploitation (improving the current best estimate).
  - Quick check question: How do different acquisition functions, such as Expected Improvement (EI) and Max-value Entropy Search (MES), trade off exploration and exploitation?

- Concept: Variational Inference
  - Why needed here: Variational inference is used in the VES framework to approximate the intractable posterior density p(y*|y_x,Dt) with a tractable variational distribution q(y*|y_x,Dt).
  - Quick check question: What is the main idea behind variational inference, and how does it differ from other approximation methods like MCMC?

## Architecture Onboarding

- Component map:
  Gaussian Process -> Variational Family -> ESLB -> VES-Gamma Algorithm

- Critical path:
  1. Initialize the Gaussian Process with observed data
  2. Define the variational family (e.g., Gamma distributions)
  3. For each iteration:
     a. Sample from the posterior p(y*,y_x|Dt) given the current x
     b. Solve for the optimal variational parameters (k,Î²) using Equations (3.9) and (3.10)
     c. Update x using the ESLB formula with the optimized variational parameters
  4. Return the optimized x after N iterations

- Design tradeoffs:
  - Choice of variational family: More flexible families (e.g., Gamma) may better capture the posterior density but increase computational complexity
  - Number of iterations (N): More iterations may lead to better optimization but increase runtime
  - Sampling strategy: The number and quality of samples used to approximate the expectations in Equations (3.9) and (3.10) affect the accuracy of the optimization

- Failure signatures:
  - Poor performance compared to EI or MES: Indicates that the variational approximation is not effective
  - High sensitivity to initial points: Suggests that the optimization may be getting stuck in local optima
  - Slow convergence: May indicate that the chosen variational family is too restrictive or the optimization is not well-tuned

- First 3 experiments:
  1. Compare VES-Gamma with EI and MES on a simple 1D test function (e.g., Rosenbrock) to validate the theoretical connection and observe the effect of the "anti-EI" term.
  2. Evaluate VES-Gamma on a multi-modal 2D test function (e.g., Three-Hump Camel) to assess its ability to handle complex landscapes and balance exploration and exploitation.
  3. Apply VES-Gamma to a real-world hyperparameter tuning problem (e.g., XGBoost on the diabetes dataset) to demonstrate its practical utility and compare its performance with EI and MES.

## Open Questions the Paper Calls Out
- Question: How does the choice of the parameter k in the Gamma distribution affect the performance of VES-Gamma compared to other values of k?
- Question: Can the VES framework be extended to other distributions beyond Gamma, such as chi-squared or generalized Gamma distributions?
- Question: How does the performance of VES-Gamma compare to other acquisition functions, such as Upper Confidence Bound (UCB) and Knowledge Gradient (KG), in terms of exploration-exploitation trade-off?

## Limitations
- Limited benchmark diversity (3 synthetic functions, 2 datasets) may not capture all failure modes
- Theoretical connection between EI and MES lacks extensive empirical validation across diverse function classes
- Strong initialization sensitivity of VES-Gamma is demonstrated but the underlying mechanism remains unexplained

## Confidence
- High confidence in the mathematical derivation linking EI and MES through variational inference
- Medium confidence in the empirical performance claims, given limited benchmark diversity
- Medium confidence in the initialization sensitivity findings, as the underlying mechanism is not fully explored

## Next Checks
1. Test VES-Gamma on additional benchmark functions with varying characteristics (high-dimensional, noisy, multi-modal) to assess generalizability
2. Conduct ablation studies to isolate the contribution of the anti-EI term versus the variational approximation itself
3. Investigate initialization strategies to reduce sensitivity, potentially through ensemble methods or robust initialization schemes