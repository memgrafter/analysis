---
ver: rpa2
title: An Empirical Study of In-context Learning in LLMs for Machine Translation
arxiv_id: '2401.12097'
source_url: https://arxiv.org/abs/2401.12097
tags:
- language
- translation
- in-context
- source
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates in-context learning (ICL)
  for machine translation (MT), revealing that ICL is primarily example-driven rather
  than instruction-driven. Through extensive experimentation with perturbations and
  demonstrations across multiple languages and models, it demonstrates that target
  distribution in demonstrations is more critical than source distribution.
---

# An Empirical Study of In-context Learning in LLMs for Machine Translation

## Quick Facts
- arXiv ID: 2401.12097
- Source URL: https://arxiv.org/abs/2401.12097
- Reference count: 40
- Primary result: In-context learning for MT is primarily example-driven rather than instruction-driven, with target distribution being more critical than source distribution.

## Executive Summary
This study systematically investigates in-context learning (ICL) for machine translation, revealing that ICL is primarily example-driven rather than instruction-driven. Through extensive experimentation with perturbations and demonstrations across multiple languages and models, it demonstrates that target distribution in demonstrations is more critical than source distribution. The study finds that spatial proximity of examples significantly impacts performance, heterogeneous perturbations can act as regularizers in some cases, and examples from related tasks can substitute for direct demonstrations when target languages match. Additionally, it shows that demonstration directionality has minimal effect and that models remain vulnerable to contextual misinformation, underscoring the need for robustness in future ICL research.

## Method Summary
The study investigates in-context learning for machine translation using multiple LLM families (BLOOM, Llama 2, and their variants) across several language pairs. Experiments involve constructing prompts with varying numbers of demonstrations, applying different types of perturbations to examples, and evaluating translation quality using the ChrF++ metric. The research systematically tests instruction variations, homogeneous and heterogeneous perturbations, spatial arrangements of examples, demonstration directionality, and allied task demonstrations. The study uses datasets including FLORES-200 and IN22-Gen, testing across languages like Bengali, Hindi, Tamil, Czech, German, and Russian.

## Key Results
- ICL for MT is primarily example-driven rather than instruction-driven
- Target distribution in demonstrations is more critical than source distribution
- Spatial proximity of examples significantly impacts performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL for MT is primarily example-driven rather than instruction-driven
- Mechanism: The model learns translation capabilities through demonstration examples in the prompt rather than from explicit task instructions
- Core assumption: The pretraining distribution of LLMs contains sufficient translation patterns that can be activated through examples
- Evidence anchors:
  - [abstract]: "we first establish that ICL is primarily example-driven and not instruction-driven"
  - [section 5.1]: "Figure 1 illustrates that base LLMs such as BLOOM-7B and Llama 2 7B exhibit less variability to instruction perturbations"
  - [corpus]: Weak - related papers focus on example selection strategies but don't directly address instruction vs example primacy
- Break condition: When demonstration examples are absent or contain conflicting patterns that overwhelm the instruction signal

### Mechanism 2
- Claim: Target distribution in demonstrations is more critical than source distribution
- Mechanism: The model prioritizes learning the output translation patterns over the input source patterns when provided with demonstrations
- Core assumption: The pretraining distribution contains richer information about target language patterns that can be matched with demonstrations
- Evidence anchors:
  - [abstract]: "While we establish the significance of the quality of the target distribution over the source distribution of demonstrations"
  - [section 5.2.3]: "Figure 4 shows that the source distribution of in-context examples has a marginal effect on downstream MT performance"
  - [corpus]: Moderate - papers like "Effective Self-Mining of In-Context Examples" suggest example quality matters but don't explicitly compare source vs target focus
- Break condition: When source and target languages have dramatically different structures that prevent pattern matching

### Mechanism 3
- Claim: Spatial proximity of examples significantly impacts performance
- Mechanism: Clean examples placed closer to the test example have stronger influence on the model's generation than distant examples
- Core assumption: The model processes demonstration examples in order and gives higher weight to recent examples
- Evidence anchors:
  - [abstract]: "spatial proximity of examples significantly impacts performance"
  - [section 5.2.2]: "placing noisy examples closer to the test example has a more detrimental impact than placing them farther away"
  - [corpus]: Weak - related papers focus on example selection but don't explicitly study proximity effects
- Break condition: When the model uses attention mechanisms that equally weight all examples regardless of position

## Foundational Learning

- Concept: In-context learning mechanism
  - Why needed here: Understanding how LLMs use demonstrations to perform tasks without parameter updates
  - Quick check question: What happens to performance when all demonstrations are removed from the prompt?

- Concept: Translationese effects and directionality
  - Why needed here: Understanding how the direction of translation (source to target vs target to source) affects model performance
  - Quick check question: Does using source-original vs target-original demonstrations change translation quality for a given direction?

- Concept: Robustness to perturbations
  - Why needed here: Understanding how different types of text corruption affect model performance and whether some perturbations can act as regularizers
  - Quick check question: How does performance change when 25% of characters in demonstration examples are randomly replaced?

## Architecture Onboarding

- Component map:
  Prompt construction -> Model generation -> ChrF++ evaluation -> Performance comparison

- Critical path:
  1. Construct prompt with k demonstrations and test example
  2. Apply perturbations if in perturbation experiments
  3. Generate translation using greedy decoding
  4. Evaluate using ChrF++ metric
  5. Compare against baseline (0 noise)

- Design tradeoffs:
  - Instruction specificity vs. example quality: More specific instructions provide less benefit than high-quality examples
  - Demonstration quantity vs. quality: More demonstrations help but diminishing returns after 4-8 shots
  - Perturbation severity vs. regularization effect: Moderate perturbations can improve performance in some models

- Failure signatures:
  - Performance degradation with homogeneous perturbations to target side
  - Sensitivity to instruction changes only in instruction-tuned models
  - Vulnerability to contextual misinformation through misalignment

- First 3 experiments:
  1. Compare performance with standard instruction vs no instruction with clean demonstrations
  2. Test homogeneous perturbations (span noise) on source vs target side with fixed instructions
  3. Test heterogeneous perturbations with clean examples placed near vs far from test example

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the spatial proximity of clean demonstrations to the test example have a non-linear relationship with downstream performance, or is it strictly monotonic?
- Basis in paper: [inferred] The paper observes that placing noisy examples farther from the test example and cleaner examples closer improves performance, suggesting spatial proximity matters.
- Why unresolved: The paper only tests a limited number of spatial arrangements and does not systematically vary the distances between examples and the test example.
- What evidence would resolve it: Experiments testing a wider range of spatial arrangements, including varying the distances between examples and the test example in a more granular way.

### Open Question 2
- Question: Are the findings on the limited impact of source language choice in allied task demonstrations generalizable to non-English-centric language pairs?
- Basis in paper: [explicit] The paper states that the choice of source language has a marginal effect on downstream MT performance, but this is primarily observed in English-centric setups.
- Why unresolved: The experiments are limited to English-centric language pairs, and it's unclear if the findings hold for non-English-centric pairs.
- What evidence would resolve it: Experiments testing the impact of source language choice on non-English-centric language pairs.

### Open Question 3
- Question: How do linguistic perturbations (e.g., causality alternation, entity replacement) compare to non-linguistic perturbations in their impact on ICL performance?
- Basis in paper: [explicit] The paper notes that their perturbation experiments focused on non-linguistic perturbations and suggests exploring linguistically aware perturbations in future work.
- Why unresolved: The paper does not conduct experiments with linguistic perturbations.
- What evidence would resolve it: Experiments comparing the impact of linguistic and non-linguistic perturbations on ICL performance.

## Limitations
- Findings are based on specific model families (BLOOM, Llama 2) and may not generalize to other architectures
- Perturbation experiments use synthetic noise types that may not capture real-world translation challenges
- Focus on greedy decoding rather than sampling-based approaches may miss different sensitivity patterns
- Evaluation relies solely on ChrF++ metric, potentially missing nuanced aspects of translation quality

## Confidence

**High Confidence**:
- ICL is primarily example-driven rather than instruction-driven
- Target distribution in demonstrations is more critical than source distribution
- Spatial proximity of examples significantly impacts performance

**Medium Confidence**:
- Heterogeneous perturbations can act as regularizers in some cases
- Examples from related tasks can substitute for direct demonstrations when target languages match
- Demonstration directionality has minimal effect

**Low Confidence**:
- Models remain vulnerable to contextual misinformation (requires more systematic investigation)
- The exact mechanisms by which perturbations act as regularizers (requires deeper analysis of attention patterns)

## Next Checks
1. **Cross-model validation**: Replicate the core findings (example-driven vs instruction-driven, target vs source importance) across a broader range of LLM architectures including GPT-family models and smaller specialized MT models to assess generalizability beyond BLOOM and Llama 2.

2. **Real-world perturbation testing**: Replace synthetic perturbations with authentic translation challenges from noisy parallel corpora (e.g., social media translations, OCR outputs from real documents) to validate whether the regularization effects observed with synthetic noise transfer to realistic scenarios.

3. **Multi-metric evaluation**: Conduct the spatial proximity and perturbation experiments using multiple evaluation metrics (ChrF++, COMET, human evaluation) to determine whether the observed effects are consistent across different quality assessments and to identify potential metric-specific artifacts.