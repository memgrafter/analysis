---
ver: rpa2
title: Confidence Preservation Property in Knowledge Distillation Abstractions
arxiv_id: '2401.11365'
source_url: https://arxiv.org/abs/2401.11365
tags:
- distillation
- confidence
- property
- preservation
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether distilled TinyBERT models preserve
  the confidence property of the original BERT model during knowledge distillation.
  The authors introduce a pairwise confidence difference metric to measure how well
  distilled models maintain the confidence levels of the original model on the same
  inputs.
---

# Confidence Preservation Property in Knowledge Distillation Abstractions
## Quick Facts
- arXiv ID: 2401.11365
- Source URL: https://arxiv.org/abs/2401.11365
- Authors: Dmitry Vengertsev; Elena Sherman
- Reference count: 39
- Key outcome: TinyBERT models can preserve confidence property for some GLUE tasks with prediction layer hyperparameter tuning

## Executive Summary
This paper investigates whether knowledge distillation preserves the confidence property of BERT models when creating smaller TinyBERT variants. The authors introduce a pairwise confidence difference metric to measure confidence preservation and find that standard TinyBERT models preserve confidence for only three of six GLUE tasks. By fine-tuning prediction layer hyperparameters, they demonstrate that confidence preservation can be achieved across all tasks without significantly degrading accuracy, revealing that architecture and prediction-layer hyperparameters are critical factors in confidence preservation.

## Method Summary
The study uses TinyBERT models (S4L with 4 layers, S6L with 6 layers) distilled from BERT teacher models to evaluate confidence preservation across six GLUE tasks. The method involves calculating pairwise confidence differences on the same inputs, then fine-tuning prediction layer hyperparameters (learning rate, batch size, epochs, weight decay) for tasks where confidence preservation fails. The researchers measure success using σ(X_train) < 0.05 threshold for confidence preservation and accuracy within 1% of original models.

## Key Results
- TinyBERT S6L models preserve confidence for SST-2, QQP, and QNLI tasks but fail for RTE, MRPC, and CoLA
- Prediction layer hyperparameter tuning can restore confidence preservation for all tasks
- Confidence preservation can be achieved without accuracy degradation beyond 1%
- The number of transformer layers and prediction-layer hyperparameters have the greatest impact on confidence preservation

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Pairwise confidence difference captures confidence preservation better than aggregated confidence distributions.
- Mechanism: Measures confidence differences on the same inputs rather than comparing overall distributions, avoiding misleading comparisons where models have same confidence for different inputs.
- Evidence anchors: [section] - "measure the confidence preservation on the same input example for two models... we adapt a similar idea of 'input-output' equivalence"
- Break condition: If confidence values are not comparable across inputs due to domain shift or extreme outliers.

### Mechanism 2
- Claim: Distillation hyperparameters, particularly in prediction layer, can be tuned to preserve confidence without degrading accuracy.
- Mechanism: Adjusting learning rate, batch size, epochs, and weight decay specifically for prediction layer distillation achieves both high accuracy and confidence preservation.
- Evidence anchors: [section] - "The architecture of the distilled model, such as the number of transformer layers as well as learning hyperparameters at the prediction layer greatly impact φcnf"
- Break condition: If tuning parameters excessively sacrifices accuracy or if architecture is fundamentally incompatible with confidence preservation.

### Mechanism 3
- Claim: Distillation acts as implicit abstraction where student model implicitly preserves properties of teacher without explicit architectural mapping.
- Mechanism: The distillation process creates a smaller model that maintains classification accuracy, allowing it to be viewed as an abstraction of original model, preserving confidence as one of its properties.
- Evidence anchors: [section] - "the question is whether this implicit abstraction preserves properties between two models... we investigate whether the knowledge distillation... preserves pairwise confidence property"
- Break condition: If distillation process focuses solely on accuracy metrics and ignores other properties like confidence.

## Foundational Learning
- Concept: Knowledge Distillation
  - Why needed here: The entire study is about whether distilled models preserve confidence properties, requiring understanding of how distillation works
  - Quick check question: What are the key components of the distillation loss function used in TinyBERT?

- Concept: Confidence Calibration
  - Why needed here: The paper needs to distinguish between traditional confidence measures and calibrated confidence, understanding why softmax confidence might be inadequate
  - Quick check question: Why might traditional softmax-based confidence be insufficient for evaluating model confidence?

- Concept: Functional Equivalence
  - Why needed here: The confidence preservation criterion is based on functional equivalence - same input producing similar confidence outputs
  - Quick check question: How does the concept of functional equivalence apply to comparing confidence values between teacher and student models?

## Architecture Onboarding
- Component map: Input → Embedding layer → Transformer layers → Prediction layer → Softmax → Confidence comparison between teacher and student
- Critical path: Input → Embedding layer → Transformer layers → Prediction layer → Softmax → Confidence comparison between teacher and student
- Design tradeoffs: More transformer layers in student (S6L vs S4L) improves confidence preservation but increases model size; aggressive parameter reduction may sacrifice confidence preservation
- Failure signatures: Confidence preservation fails when σ(X_train) > 0.05 threshold; accuracy drops more than 1% after hyperparameter tuning
- First 3 experiments:
  1. Measure pairwise confidence difference σ(X_train) for S6L and S4L models across all GLUE tasks
  2. Tune prediction layer hyperparameters (learning rate, batch size, epochs) for tasks where confidence preservation fails
  3. Validate that tuned models maintain φcnf while keeping accuracy within 1% of original

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the number of transformer layers in the student model affect confidence preservation across different GLUE tasks?
- Basis in paper: [explicit] The paper shows that S6L models preserve confidence for three tasks while S4L models fail for all tasks, suggesting layer count impacts preservation
- Why unresolved: The paper doesn't systematically explore intermediate layer counts (e.g., 3, 5, 7 layers) or provide theoretical justification for why 6 layers works better than 4
- What evidence would resolve it: Systematic experiments testing additional layer configurations and theoretical analysis of how layer depth affects confidence propagation

### Open Question 2
- Question: What is the relationship between intermediate layer distillation and prediction layer distillation for confidence preservation?
- Basis in paper: [explicit] The authors found that tuning intermediate layer hyperparameters had no effect on confidence preservation while prediction layer tuning was crucial
- Why unresolved: The paper doesn't explain why intermediate layers have minimal impact or what architectural properties cause this difference
- What evidence would resolve it: Ablation studies showing the effect of intermediate layer distillation with different architectures or theoretical analysis of gradient flow through intermediate layers

### Open Question 3
- Question: Can the confidence preservation property be maintained when distilling to models with significantly different architectures than the teacher?
- Basis in paper: [inferred] The authors focus on TinyBERT variants with similar attention-based architectures, but don't explore more divergent architectures like LSTMs or convolutional models
- Why unresolved: The paper only tests distillation within the same architectural family (transformer-based), leaving open whether confidence preservation transfers across architectures
- What evidence would resolve it: Experiments distilling BERT to non-transformer architectures while measuring confidence preservation across the same GLUE tasks

## Limitations
- Confidence preservation metric (σ(X_train) < 0.05) lacks theoretical justification for why this specific threshold represents good preservation
- Study focuses exclusively on TinyBERT models and BERT teachers, limiting generalizability to other distillation frameworks
- Hyperparameter tuning approach shows promising results but doesn't explore full space of possible configurations

## Confidence
- High confidence: The existence of a confidence preservation problem in knowledge distillation
- Medium confidence: The effectiveness of prediction layer hyperparameter tuning in preserving confidence
- Low confidence: The theoretical justification for the pairwise confidence difference metric being superior to other approaches

## Next Checks
1. **Validation on alternative confidence metrics**: Compare pairwise confidence difference approach against KL divergence between confidence distributions and expected calibration error (ECE) to determine which metric best captures confidence preservation.

2. **Cross-architecture generalization**: Apply the same methodology to DistilBERT and other knowledge distillation frameworks beyond TinyBERT to verify whether confidence preservation challenges and solutions generalize across different distillation approaches.

3. **Hyperparameter sensitivity analysis**: Systematically vary learning rate, batch size, and epochs across a broader range (e.g., learning rates from 1e-5 to 1e-3) to map the hyperparameter landscape and identify whether reported solutions represent local optima or global solutions for confidence preservation.