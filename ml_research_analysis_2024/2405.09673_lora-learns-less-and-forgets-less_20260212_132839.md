---
ver: rpa2
title: LoRA Learns Less and Forgets Less
arxiv_id: '2405.09673'
source_url: https://arxiv.org/abs/2405.09673
tags:
- lora
- finetuning
- full
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically compares LoRA (Low-Rank Adaptation) to
  full fine-tuning for LLMs in code and math domains. The authors find that in standard
  low-rank settings, LoRA underperforms full fine-tuning on both continued pretraining
  and instruction finetuning tasks.
---

# LoRA Learns Less and Forgets Less

## Quick Facts
- arXiv ID: 2405.09673
- Source URL: https://arxiv.org/abs/2405.09673
- Reference count: 40
- Primary result: LoRA underperforms full fine-tuning but better preserves base model capabilities

## Executive Summary
This paper systematically compares LoRA to full fine-tuning for LLMs on code and math tasks. The authors find that LoRA underperforms full fine-tuning in standard low-rank settings, but significantly mitigates catastrophic forgetting of source domain capabilities. They also show LoRA reduces diversity of generated solutions compared to full fine-tuning while maintaining more diversity than the base model. The study reveals that full fine-tuning learns high-rank perturbations (10-100× greater than typical LoRA ranks), explaining some performance gaps. Based on these findings, the authors recommend using LoRA primarily for instruction fine-tuning with higher ranks (r=256) and targeting all transformer modules when GPU memory allows.

## Method Summary
The authors compare LoRA (with ranks r=16, 64, 256) against full fine-tuning using a Llama-2-7B base model on code and math domains. They use StarCoder-Python (20B tokens) and OpenWebMath (20B tokens) for continued pretraining, and Magicoder-Evol-Instruct-110K (72.97M tokens) and MetaMathQA (103M tokens) for instruction fine-tuning. LoRA adapters are applied to all transformer modules with scaling factor α=2r. They evaluate using HumanEval pass@1 for code, GSM8K for math, and HellaSwag/ARC-Challenge/WinoGrande for measuring catastrophic forgetting. The decoupled LionW optimizer is used with learning rates 1e-5 (full) vs 1e-4 (LoRA) and cosine scheduling with warmup.

## Key Results
- LoRA substantially underperforms full fine-tuning on code and math tasks in standard low-rank settings
- LoRA better maintains base model performance on tasks outside the target domain, mitigating catastrophic forgetting
- Full fine-tuning learns high-rank perturbations (10-100× greater than typical LoRA ranks), which may explain performance gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA underperforms full fine-tuning on code and math tasks in standard low-rank settings.
- Mechanism: LoRA learns only low-rank perturbations (e.g., rank r = 16, 64, 256) to weight matrices, while full fine-tuning learns high-rank perturbations (10-100× greater than LoRA ranks).
- Core assumption: High-rank perturbations capture more complex task-specific adjustments needed for challenging domains.
- Evidence anchors:
  - [abstract] "LoRA substantially underperforms full finetuning. Nevertheless, LoRA better maintains the base model's performance on tasks outside the target domain."
  - [section] "Full finetuning finds high rank weight perturbations (Sec. 4.6)."
  - [corpus] Weak - no direct citations supporting this claim.
- Break condition: If task domain is simple or close to pretraining distribution, LoRA ranks may suffice.

### Mechanism 2
- Claim: LoRA mitigates catastrophic forgetting more than full fine-tuning.
- Mechanism: By constraining updates to low-rank spaces, LoRA limits deviation from the base model, preserving source-domain capabilities.
- Core assumption: Source-domain knowledge is encoded in high-rank components of weight matrices that LoRA cannot easily modify.
- Evidence anchors:
  - [abstract] "LoRA better maintains the base model's performance on tasks outside the target domain."
  - [section] "LoRA mitigates forgetting more than common regularization techniques such as weight decay and dropout."
  - [corpus] Weak - no direct citations supporting this claim.
- Break condition: If source domain is also trained on intensively, forgetting differences may diminish.

### Mechanism 3
- Claim: LoRA maintains more diverse generations compared to full fine-tuning.
- Mechanism: Low-rank constraints prevent distribution collapse during fine-tuning, leading to broader output diversity.
- Core assumption: Full fine-tuning overfits to specific solutions, reducing output variability.
- Evidence anchors:
  - [section] "by analyzing the generated solutions to HumanEval problems, we demonstrate that while full finetuning tends to produce a limited set of solutions, LoRA produces a wider range of solutions."
  - [corpus] Weak - no direct citations supporting this claim.
- Break condition: If LoRA rank is too high, diversity benefits may disappear.

## Foundational Learning

- Concept: Low-rank matrix approximation
  - Why needed here: LoRA relies on approximating weight updates with low-rank matrices (A ∈ Rd×r, B ∈ Rr×k).
  - Quick check question: If d=4096 and r=16, how many parameters does LoRA train per matrix versus full fine-tuning?

- Concept: Catastrophic forgetting
  - Why needed here: Understanding how fine-tuning affects base model capabilities is central to the paper's findings.
  - Quick check question: What is the difference between forgetting and performance degradation in fine-tuning?

- Concept: Rank of a matrix
  - Why needed here: The paper's core finding relates to comparing ranks of learned perturbations between LoRA and full fine-tuning.
  - Quick check question: How is the rank of a perturbation matrix related to the number of singular values needed to explain 90% of variance?

## Architecture Onboarding

- Component map:
  Base model -> Frozen pretrained weights -> LoRA adapters (A,B matrices) -> Optimizer (LionW) -> Training loop

- Critical path:
  1. Initialize A with N(0,1), B=0
  2. Compute forward pass with LoRA-augmented weights
  3. Calculate loss and gradients
  4. Update only A and B matrices
  5. Scale with α/r parameter

- Design tradeoffs:
  - Memory vs. performance: Higher ranks improve accuracy but increase memory usage
  - Source domain preservation vs. target domain learning: Lower ranks better preserve base capabilities
  - Training speed vs. sample efficiency: LoRA slower per token but can use fewer parameters

- Failure signatures:
  - Performance plateaus below full fine-tuning baseline
  - Source domain accuracy drops significantly
  - Training instability with high learning rates
  - No improvement with increased rank

- First 3 experiments:
  1. Compare HumanEval accuracy for LoRA (r=16) vs full fine-tuning on Magicoder dataset
  2. Measure forgetting by comparing base model performance on HellaSwag after fine-tuning
  3. Analyze rank of weight perturbations using SVD on checkpoints from Starcoder pretraining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal LoRA rank for different model sizes and datasets?
- Basis in paper: [explicit] The paper found that LoRA performance varies significantly with rank, with r=256 needed to match full fine-tuning on code tasks, while lower ranks sufficed for math tasks.
- Why unresolved: The paper only tested a limited range of ranks (r=16, 64, 256) on specific model sizes and datasets. The relationship between rank, model size, dataset complexity, and downstream task difficulty is not fully characterized.
- What evidence would resolve it: Systematic experiments varying rank across multiple model sizes (e.g., 7B, 13B, 33B, 70B) and diverse datasets with varying complexity levels, measuring downstream task performance to identify rank scaling laws.

### Open Question 2
- Question: How do different LoRA scaling factors (alpha) interact with rank and learning rate choices?
- Basis in paper: [explicit] The paper found that alpha=2r was crucial for high ranks, but the interaction between alpha, rank, and learning rate remains unexplored.
- Why unresolved: The paper only explored alpha=2r for high ranks but didn't systematically investigate how different alpha values affect performance across different rank and learning rate combinations.
- What evidence would resolve it: Comprehensive hyperparameter sweeps varying alpha values (e.g., 1r, 2r, 4r, 8r) across different ranks and learning rates, measuring both convergence speed and final task performance.

### Open Question 3
- Question: What are the theoretical limits of LoRA's memory efficiency compared to full fine-tuning?
- Basis in paper: [inferred] The paper showed theoretical memory savings with LoRA but didn't fully characterize the practical limits or trade-offs.
- Why unresolved: The paper provided theoretical memory analysis but didn't systematically measure actual memory usage and throughput across different hardware configurations and batch sizes.
- What evidence would resolve it: Detailed empirical measurements of memory usage and training throughput for LoRA vs full fine-tuning across various GPU configurations, batch sizes, and model sizes, identifying practical efficiency limits.

## Limitations
- Findings based on single 7B parameter model and two domains (code and math)
- Only tested standard LoRA ranks (r=16, 64, 256) without exploring full spectrum
- Catastrophic forgetting analysis relies on limited benchmark set

## Confidence
- **High**: LoRA underperforms full fine-tuning on target domain tasks in standard low-rank settings
- **Medium**: LoRA better preserves source domain capabilities compared to full fine-tuning
- **Low**: LoRA maintains more diverse generations compared to full fine-tuning (based on qualitative analysis of solution diversity)

## Next Checks
1. Cross-domain generalization test: Evaluate the forgetting mitigation effect of LoRA across additional domains beyond code and math, including natural language understanding and generation tasks, to assess the universality of the observed forgetting patterns.

2. Rank scaling experiment: Systematically explore the relationship between LoRA rank and performance across a wider range of rank values (e.g., r=1, 4, 8, 32, 128, 512) to identify optimal rank settings for different task complexities and model sizes.

3. Ablation study on adapter placement: Compare the effects of targeting different transformer components (attention layers only vs. all modules) on both task performance and forgetting, to determine if selective placement can optimize the tradeoff between learning and preservation.