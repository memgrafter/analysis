---
ver: rpa2
title: Characterizing Truthfulness in Large Language Model Generations with Local
  Intrinsic Dimension
arxiv_id: '2402.18048'
source_url: https://arxiv.org/abs/2402.18048
tags:
- intrinsic
- dimension
- language
- truthfulness
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces using local intrinsic dimension (LID) to
  detect untruthful or hallucinated outputs from large language models (LLMs). LID
  measures how many dimensions are needed to describe a data point, with higher values
  indicating more complex distributions.
---

# Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension

## Quick Facts
- **arXiv ID**: 2402.18048
- **Source URL**: https://arxiv.org/abs/2402.18048
- **Reference count**: 40
- **Primary result**: LID-based detection significantly outperforms uncertainty-based methods, achieving up to 8% improvement in AUROC for detecting LLM hallucinations.

## Executive Summary
This paper introduces using local intrinsic dimension (LID) to detect untruthful or hallucinated outputs from large language models. LID measures how many dimensions are needed to describe a data point, with higher values indicating more complex distributions. The method estimates LID of model activations using a modified maximum likelihood estimator that accounts for non-uniform density in LLM representations. Experiments on four question-answering datasets show LID-based detection significantly outperforms uncertainty-based methods like entropy estimation, achieving up to 8% improvement in AUROC. The approach is also more stable across different prompt formats and can use out-of-domain data for estimation.

## Method Summary
The paper proposes estimating local intrinsic dimension (LID) of LLM representations to detect untruthful outputs. The method uses a modified maximum likelihood estimator (GeoMLE) that accounts for non-uniform density in LLM representations. LID values are computed for intermediate layer activations, with the last token of the last position selected for analysis. A binary classifier (thresholding) is then applied to LID values to predict truthfulness. The approach is evaluated on four QA datasets using Llama-2-7B and Llama-2-13B models, comparing against entropy-based methods, verbalized uncertainty, and trained classifiers.

## Key Results
- LID-based detection achieves up to 8% improvement in AUROC compared to uncertainty-based methods
- LID values exhibit a "hunchback" shape across model layers, with detection performance "shifting behind" the LID pattern
- LID values correlate with autoregressive generation position, showing sharp decreases when ground-truth answers approach their end
- The method demonstrates stability across different prompt formats and can use out-of-domain data for estimation

## Why This Works (Mechanism)

### Mechanism 1
Truthful outputs, being closer to natural language, are more structured and have smaller LIDs. Untruthful outputs mix human (prompt) and complex model distributions, leading to larger LIDs. This works because LID measures the geometric complexity of model representations, with higher values indicating more complex manifolds that mix human and model distributions.

### Mechanism 2
The paper uses MLE-based LID estimation with geometric correction to account for non-uniform density in LLM representations. Standard MLE assumes constant density around data points, which doesn't hold for causal LLMs. The distance-aware MLE (GeoMLE) corrects for non-uniform density using a polynomial regression approach, improving accuracy by addressing the varying density of LLM representations.

### Mechanism 3
LID values vary systematically across model layers and autoregressive generation positions, providing a rich signal for truthfulness detection. The paper finds that LID values exhibit a "hunchback" shape across layers (increasing then decreasing) and correlate with detection performance. LID values also decrease sharply when ground-truth answers approach their end, unlike incorrect generations, encoding meaningful information about model processing.

## Foundational Learning

- **Concept**: Local Intrinsic Dimension (LID)
  - Why needed here: LID is the core metric used to detect hallucinations. Understanding how it measures the minimal number of dimensions needed to describe data points is crucial for grasping the method's intuition.
  - Quick check question: If a data point lies in a highly curved, complex manifold, would you expect its LID to be high or low? Why?

- **Concept**: Maximum Likelihood Estimation (MLE) for intrinsic dimension
  - Why needed here: The paper builds its LID estimation method on MLE, modifying it to handle non-uniform density in LLM representations. Understanding the basic MLE framework is essential for comprehending the proposed improvements.
  - Quick check question: In the context of LID estimation, what does the Poisson process model in the MLE framework?

- **Concept**: Geometric corrections for non-uniform density
  - Why needed here: The paper introduces a distance-aware MLE that corrects for non-uniform density using polynomial regression. Understanding this correction is key to appreciating why the method outperforms vanilla MLE.
  - Quick check question: Why might the density of LLM representations not be uniform around a given data point? What factors could contribute to this non-uniformity?

## Architecture Onboarding

- **Component map**: Input -> LID Estimation -> Feature Extraction -> Classification -> Output
- **Critical path**:
  1. Extract intermediate representations from LLM
  2. Select optimal layer and token position for LID calculation
  3. Estimate LID using GeoMLE with distance-aware correction
  4. Compare LID value to threshold or feed to classifier
  5. Output truthfulness prediction
- **Design tradeoffs**:
  - Layer selection: Last layer vs. earlier layers (tradeoff between information content and noise)
  - Neighbor count (T): Too few leads to high variance, too many breaks local assumption
  - Dataset size (n): Larger datasets provide more stable estimates but increase computation
  - Distance-aware correction: Improves accuracy but adds complexity
- **Failure signatures**:
  - Poor performance on certain datasets may indicate LID distributions don't separate well for that task
  - Sensitivity to hyperparameters suggests instability in density estimation
  - Cross-task neighbor degradation indicates limited generalizability
- **First 3 experiments**:
  1. Reproduce the main AUROC results on one dataset (e.g., TriviaQA) with both LID-MLE and LID-GeoMLE to verify the 8% improvement claim.
  2. Test the effect of layer selection by computing LID values at each layer and plotting the detection performance curve to observe the "shift behind" phenomenon.
  3. Experiment with different numbers of neighbors (T) and dataset sizes (n) to identify the optimal hyperparameters for your specific use case.

## Open Questions the Paper Calls Out

### Open Question 1
Does the LID-based hallucination detection method generalize to languages other than English? The paper suggests adaptation to other languages is straightforward but doesn't provide empirical validation across multiple languages. Systematic evaluation across language families would demonstrate generalizability.

### Open Question 2
How do LID values correlate with specific types of hallucinations (e.g., confabulation vs. semantic drift vs. factual errors)? The paper establishes LID as a general indicator of truthfulness but doesn't investigate whether different hallucination patterns produce distinct LID signatures. Annotating detected hallucinations with fine-grained categories would reveal patterns.

### Open Question 3
What is the theoretical relationship between LID values and the underlying information geometry of LLM representations? The paper demonstrates empirical correlations between LID values and truthfulness but lacks theoretical justification for why higher LID values indicate untruthfulness. Mathematical analysis connecting information geometry to manifold properties would provide theoretical foundation.

## Limitations
- Limited generalizability to non-QA tasks and different model architectures
- Sample size (2,000 per dataset) and limited model diversity (only Llama-2 variants) constrain robustness claims
- Method's sensitivity to hyperparameters like neighbor count and layer selection introduces uncertainty about optimal configurations

## Confidence

- **High**: LID provides a meaningful signal for truthfulness detection (supported by consistent AUROC improvements)
- **Medium**: LID patterns correlate with model layers and generation position (based on limited analysis across 4 datasets)
- **Medium**: The method outperforms uncertainty-based approaches (AUROC improvements up to 8% but with potential domain-specific effects)

## Next Checks

1. **Cross-domain robustness test**: Evaluate LID-based truthfulness detection on non-QA datasets (e.g., code generation, summarization) to assess generalizability beyond the current scope.

2. **Ablation study on hyperparameters**: Systematically vary neighbor count (T), reference set size (n), and layer selection criteria to identify stable configurations and quantify performance variance.

3. **Real-world deployment simulation**: Test the method's performance on LLM outputs with no ground truth available, using cross-dataset training and evaluating detection accuracy on truly unseen domains.