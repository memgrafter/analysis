---
ver: rpa2
title: 'Creating an Aligned Corpus of Sound and Text: The Multimodal Corpus of Shakespeare
  and Milton'
arxiv_id: '2407.18730'
source_url: https://arxiv.org/abs/2407.18730
tags:
- corpus
- line
- scansion
- have
- poetry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal corpus of poems by Shakespeare
  and Milton, aligning text with audio recordings at line, word, syllable, and phone
  levels. The corpus includes automatic scansion and covers roughly 12.5 hours of
  readings.
---

# Creating an Aligned Corpus of Sound and Text: The Multimodal Corpus of Shakespeare and Milton

## Quick Facts
- **arXiv ID**: 2407.18730
- **Source URL**: https://arxiv.org/abs/2407.18730
- **Reference count**: 5
- **Primary result**: Aligned multimodal corpus of Shakespeare and Milton poems with text, audio, and scansion at line, word, syllable, and phone levels

## Executive Summary
This paper presents a multimodal corpus of Shakespeare and Milton poems, aligning text with audio recordings at multiple granularities (line, word, syllable, phone). The corpus includes automatic scansion and covers approximately 12.5 hours of readings, enabling deeper analysis of rhythm and sound in poetry. Using tools like Aeneas for text-audio alignment and HTK for phoneme alignment, the work bridges linguistics, acoustics, and literature. The resource achieves F1-scores of 0.85–0.90 for syllabification and 0.75–0.80 for scansion, with high correlation (0.72) between word length and duration.

## Method Summary
The corpus creation pipeline involves acquiring public domain texts from Project Gutenberg and Librivox audio recordings, then cleaning and splitting audio into individual poem recordings. Line-level text-audio alignment is performed using Dynamic Time Warping with Aeneas, followed by phoneme extraction via HTK/FAVE and syllabification using Foma. A pretrained BiLSTM+CRF model generates automatic scansion patterns, and the aligned data is encoded in TEI format. The corpus includes interactive web playback capabilities and supports investigations into acoustic realization of poetic stress patterns.

## Key Results
- F1-scores of 0.85–0.90 for syllabification and 0.75–0.80 for scansion
- High correlation (0.72) between word length and duration
- 12.5 hours of aligned audio recordings covering Shakespeare's Sonnets and Milton's Paradise Lost
- Web interface enables interactive poem playback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The corpus enables acoustic realization analysis of poetic stress patterns by providing synchronized text-audio-annotation triples.
- Mechanism: Text-audio alignment tools (Aeneas) and phoneme alignment (HTK) create a shared timeline linking textual elements (lines, words, syllables, phones) with their acoustic counterparts, allowing researchers to query how specific stress markings in scansion correspond to measurable acoustic features.
- Core assumption: The alignment tools accurately synchronize text and audio at the granularity required for syllable and phoneme level analysis.
- Evidence anchors:
  - [abstract]: "aligning text with audio recordings at line, word, syllable, and phone levels"
  - [section]: "For each line we include the following information as XML attributes: beginning (moment in which the utterance starts in the audio file), end (the moment in which the utterance ends)"
- Break condition: If alignment accuracy drops below the threshold needed for reliable syllable-level synchronization, acoustic-to-textural mapping becomes unreliable.

### Mechanism 2
- Claim: The automatic scansion model's output can be empirically validated against acoustic realizations in the corpus.
- Mechanism: The scansion model (BiLSTM+CRF) generates stress patterns for each line, which are stored alongside the aligned audio. Researchers can then measure acoustic correlates (e.g., pitch, duration, amplitude) of predicted stressed vs. unstressed syllables to validate or refine the model.
- Core assumption: The scansion model's errors are systematic enough to allow pattern analysis rather than random noise.
- Evidence anchors:
  - [abstract]: "Using tools like Aeneas for text-audio alignment and HTK for phoneme alignment, the work enables deeper analysis of rhythm and sound in poetry"
  - [section]: "We used a pretrained a BiLSTM+CRF based model... trained on Tucker (2011) available on Github"
- Break condition: If the scansion model's error rate is too high or non-systematic, acoustic validation becomes meaningless.

### Mechanism 3
- Claim: The corpus supports cross-modal analysis by providing rich metadata (text, audio, scansion, alignments) in a standardized TEI format.
- Mechanism: TEI encoding captures all layers of annotation (lines, words, syllables, phones, scansion) with temporal boundaries, enabling queries that bridge literary analysis, linguistics, and acoustics through a unified data structure.
- Core assumption: The TEI format adequately represents the complexity of multimodal poetic data for computational analysis.
- Evidence anchors:
  - [abstract]: "The corpus includes automatic scansion and covers roughly 12.5 hours of readings"
  - [section]: "We decided to encode the poems following the Text Encoding Initiative (TEI Consortium, 2008) in its fifth version (TEI 5.0)"
- Break condition: If the TEI schema proves insufficient for representing certain acoustic or linguistic features, analysis capabilities are limited.

## Foundational Learning

- Concept: Text-audio alignment techniques (Dynamic Time Warping, forced alignment)
  - Why needed here: These methods are the foundation for synchronizing written text with spoken audio at various granularities
  - Quick check question: What is the primary difference between line-level and phoneme-level alignment approaches?

- Concept: Poetic scansion and metrical analysis
  - Why needed here: Understanding how stress patterns are marked and analyzed in poetry is essential for interpreting the corpus annotations
  - Quick check question: How does iambic pentameter differ from trochaic tetrameter in terms of stress pattern?

- Concept: TEI (Text Encoding Initiative) standards for multimodal corpora
  - Why needed here: The corpus uses TEI encoding to represent complex multimodal relationships in a standardized format
  - Quick check question: What TEI elements would you use to represent aligned audio segments with text?

## Architecture Onboarding

- Component map: Text acquisition -> Cleaning -> Line alignment -> G2P conversion -> Syllabification -> Forced alignment -> Scansion -> TEI encoding -> Web interface -> Analysis tools
- Critical path: Text acquisition -> Line-level alignment -> Scansion generation -> TEI encoding (these must succeed for downstream analysis)
- Design tradeoffs: Rule-based vs. neural approaches for syllabification and scansion; British vs. American English models for forced alignment
- Failure signatures: Misaligned audio segments (detected by comparing expected vs. actual durations); OOV words causing scansion errors; TEI encoding validation failures
- First 3 experiments:
  1. Verify alignment accuracy by spot-checking a sample of lines against the audio
  2. Test scansion model accuracy on a held-out validation set of manually scansioned lines
  3. Validate TEI encoding by parsing sample files and checking attribute consistency

## Open Questions the Paper Calls Out

- **Open Question 1**: How does poetic stress acoustically manifest in different types of metrical feet (e.g., iambic vs. anapestic)?
  - Basis in paper: [explicit] The paper discusses analyzing how poetic stress is realized when reading a poem and mentions extending the corpus to include poets using other kinds of feet to check whether the length of each foot is the same regardless of the number of syllables.
  - Why unresolved: The current corpus only contains traditional meter (mostly iambic lines), so there is no empirical data on how different metrical feet acoustically compare.
  - What evidence would resolve it: Recordings and alignments of poems with diverse metrical feet (like anapestic) would allow measuring and comparing the acoustic properties (duration, stress patterns) of different foot types.

- **Open Question 2**: Can changes in amplitude or pitch be detected when reading words with certain sentiment or polarity?
  - Basis in paper: [explicit] The paper suggests exploring the relationship between text-related aspects like sentiment and acoustic information, asking if evidence of higher amplitude can be found when mentioning special characters or if voice changes occur with words of certain sentiment.
  - Why unresolved: While the corpus provides aligned text and audio data, sentiment analysis has not yet been integrated or tested against the acoustic features.
  - What evidence would resolve it: Perform sentiment analysis on the poem texts and correlate the results with acoustic measurements (pitch, amplitude, duration) from the aligned audio to detect systematic patterns.

- **Open Question 3**: How do different readers' idiosyncrasies affect the acoustic realization of poetic stress and rhythm?
  - Basis in paper: [explicit] The paper mentions that including readings from other readers would help reduce possible reader-specific idiosyncrasies in analyses, implying that reader variation is currently a confounding factor.
  - Why unresolved: The corpus currently uses only two readers (one for Shakespeare, one for Milton), so it lacks comparative data across multiple readers.
  - What evidence would resolve it: Collect and align recordings of the same poems read by multiple different readers, then analyze the variance in stress placement, duration, and pitch to quantify individual reading styles.

## Limitations

- Automatic scansion model achieves only 0.75-0.80 F1-score, meaning roughly 20-25% of stress patterns may be incorrect
- Corpus covers only two authors (Shakespeare and Milton) with specific recordings by particular readers, limiting generalizability
- Tool configurations for Aeneas, HTK, and Foma are not fully specified, making exact reproduction difficult

## Confidence

- **High Confidence**: The corpus creation methodology and TEI encoding approach are well-documented and technically sound
- **Medium Confidence**: Reported performance metrics (F1-scores, correlation values) are plausible but haven't been independently verified
- **Low Confidence**: Claims about enabling "deeper analysis of rhythm and sound" are largely aspirational without empirical validation studies shown

## Next Checks

1. **Reproduce Alignment Accuracy**: Manually verify text-audio synchronization for 50 randomly selected lines across both authors, measuring alignment error rates at word and syllable boundaries.

2. **Scansion Model Validation**: Compare the automatic scansion output against a manually scansioned validation set of 100 lines, calculating precision, recall, and identifying systematic error patterns.

3. **Acoustic Feature Analysis**: Using the aligned corpus, measure actual acoustic correlates (duration, pitch, intensity) of syllables marked as stressed vs. unstressed to assess whether the scansion predictions correspond to measurable acoustic differences.