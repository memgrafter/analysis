---
ver: rpa2
title: Random ReLU Neural Networks as Non-Gaussian Processes
arxiv_id: '2405.10229'
source_url: https://arxiv.org/abs/2405.10229
tags:
- relu
- srelu
- random
- processes
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proves that shallow ReLU neural networks with random
  initialization are well-defined non-Gaussian stochastic processes, parameterized
  by the law of weights/biases and density of activation thresholds. By interpreting
  these networks as solutions to stochastic differential equations driven by impulsive
  white noise, the authors derive a remarkably simple closed-form expression for the
  autocovariance function and show that these processes are isotropic and wide-sense
  self-similar with Hurst exponent 3/2.
---

# Random ReLU Neural Networks as Non-Gaussian Processes

## Quick Facts
- arXiv ID: 2405.10229
- Source URL: https://arxiv.org/abs/2405.10229
- Reference count: 9
- Key outcome: Random ReLU neural networks with random initialization are well-defined non-Gaussian processes parameterized by weight/bias distributions and activation threshold density, with simple closed-form autocovariance expressions and isotropic, wide-sense self-similar properties.

## Executive Summary
This paper establishes that shallow ReLU neural networks with randomly initialized parameters are well-defined non-Gaussian stochastic processes. The authors derive a remarkably simple closed-form expression for the autocovariance function and prove that these processes are isotropic and wide-sense self-similar with Hurst exponent 3/2. Crucially, the paper demonstrates that in the infinite-width limit, such networks can converge not only to Gaussian processes but also to non-Gaussian processes depending on the initialization law. Specifically, when weights are drawn from symmetric α-stable distributions with α ∈ (1,2), the limiting process is non-Gaussian, extending the understanding of wide neural networks beyond the Gaussian process paradigm.

## Method Summary
The authors use a framework of generalized stochastic processes, representing random ReLU neural networks as solutions to stochastic differential equations driven by impulsive white noise. They employ Radon transforms and related operators to construct a whitening operator that maps ReLU neurons to even symmetrized Dirac measures. The randomness in activation thresholds is modeled as a Poisson point process, and the complete statistical characterization is achieved through characteristic functionals. The analysis covers both finite-width and infinite-width regimes, with convergence properties depending critically on the initialization law.

## Key Results
- Random ReLU neural networks with random initialization are well-defined non-Gaussian processes parameterized by weight/bias distributions and activation threshold density
- The autocovariance function has a remarkably simple closed-form expression involving cubic terms of Euclidean distance with a 3/2 power
- In the infinite-width limit, networks can converge to non-Gaussian processes when weights follow symmetric α-stable distributions with α ∈ (1,2), while recovering Gaussianity only when α = 2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random ReLU neural networks with random initialization are well-defined non-Gaussian processes parameterized by the law of weights/biases and the density of activation thresholds.
- Mechanism: The paper establishes that shallow ReLU neural networks with randomly initialized parameters can be expressed as solutions to stochastic differential equations driven by impulsive white noise, where the randomness comes from Poisson-type random measures. The density of activation thresholds (controlled by parameter λ) and the law of the weights/biases (PV) fully characterize the statistical distribution of the resulting process.
- Core assumption: The weights/biases are drawn such that the activation thresholds are mutually independent, uniformly distributed in finite volumes, and their expected number in any finite volume is proportional to a constant times a geometric property of the volume.
- Evidence anchors:
  - [abstract]: "We consider a large class of shallow neural networks with randomly initialized parameters and rectified linear unit activation functions. We prove that these random neural networks are well-defined non-Gaussian processes."
  - [section]: "These processes are parameterized by the law of the weights and biases as well as the density of activation thresholds in each bounded region of the input domain."
  - [corpus]: Weak - the corpus papers focus on different aspects like spectral complexity or injectivity capacity, not on the stochastic process characterization itself.
- Break condition: If the initialization law PV does not satisfy the admissibility conditions (Lévy measure and finite absolute moment), the process may not be well-defined.

### Mechanism 2
- Claim: In the infinite-width limit, random ReLU neural networks can converge to non-Gaussian processes depending on the initialization law.
- Mechanism: When weights are drawn from symmetric α-stable distributions with α ∈ (1,2), the limiting process is non-Gaussian. This occurs because the scaling parameter of the α-stable law is proportional to λ^(-1/α), where λ controls the density of activation thresholds. Only when α = 2 (Gaussian case) does the limiting process become Gaussian.
- Core assumption: The expected width of the network (Nλ) grows as λ increases, allowing for an asymptotic regime where convergence can be studied.
- Evidence anchors:
  - [abstract]: "Finally, we show that, under suitable hypotheses, as the expected width tends to infinity, these processes can converge in law not only to Gaussian processes, but also to non-Gaussian processes depending on the law of the weights."
  - [section]: "On the other hand, when PV is a symmetric α-stable (SαS) law with α ∈ (1, 2) and scaling parameter proportional to λ−1/α, sReLU converges in law to a non-Gaussian process."
  - [corpus]: Weak - corpus papers don't directly address convergence to non-Gaussian processes in the infinite-width limit.
- Break condition: If the scaling parameter doesn't scale appropriately with λ, or if α ≤ 1 or α > 2, the convergence properties may change.

### Mechanism 3
- Claim: The autocovariance function of random ReLU neural networks has a remarkably simple closed-form expression.
- Mechanism: By leveraging the framework of generalized stochastic processes and the properties of the whitening operator TReLU, the paper derives that the autocovariance depends on the Euclidean distance between points in a specific cubic form involving the 3/2 power of the distance norm.
- Core assumption: The weights have a finite second moment, which allows for the calculation of second-order statistics.
- Evidence anchors:
  - [abstract]: "We also derive a remarkably simple closed-form expression for their autocovariance function."
  - [section]: "CsReLU(x, y) = λAE[V 2](∥x − y∥3/2 − ∥x∥3/2 − ∥y∥3/2 + 3xTy(∥x∥2 + ∥y∥2)), where A = Γ(−3/2)/(2d+3πd/2Γ((d+3)/2)) and Γ(·) is Euler's gamma function."
  - [corpus]: Weak - corpus papers don't provide closed-form expressions for autocovariance functions of ReLU networks.
- Break condition: If the weights don't have a finite second moment, the autocovariance function may not exist or may have a different form.

## Foundational Learning

- Concept: Generalized stochastic processes and characteristic functionals
  - Why needed here: The paper uses the framework of generalized stochastic processes rather than the traditional "time-series" approach, which allows for elegant arguments about the properties of stochastic processes realized by random neural networks.
  - Quick check question: What is the characteristic functional of a generalized stochastic process, and how does it generalize the characteristic function of a random variable?

- Concept: Radon transform and related operators
  - Why needed here: The whitening operator TReLU = K R ∆, which is central to characterizing random ReLU neural networks as solutions to stochastic differential equations, is based on the Radon transform and related operators (K and ∆).
  - Quick check question: How does the Radon transform relate to the characterization of random ReLU neural networks as solutions to stochastic differential equations?

- Concept: Poisson point processes
  - Why needed here: The randomness in the activation thresholds of the ReLU neurons is modeled as a Poisson point process, which is crucial for establishing the well-definedness of the random neural networks as stochastic processes.
  - Quick check question: What are the key properties of a Poisson point process, and how do they apply to the activation thresholds in random ReLU neural networks?

## Architecture Onboarding

- Component map: Random initialization -> Poisson point process of activation thresholds -> Well-defined stochastic process -> Characteristic functional -> Properties (isotropy, self-similarity) -> Asymptotic behavior

- Critical path: Random initialization -> Poisson point process -> Well-defined stochastic process -> Characteristic functional -> Properties (isotropy, self-similarity) -> Asymptotic behavior

- Design tradeoffs:
  - Using the framework of generalized stochastic processes allows for elegant mathematical treatment but requires familiarity with functional analysis concepts.
  - The paper considers both non-asymptotic (finite-width) and asymptotic (infinite-width) regimes, providing a comprehensive understanding of the behavior of random ReLU neural networks.

- Failure signatures:
  - If the initialization law PV doesn't satisfy the admissibility conditions, the process may not be well-defined.
  - If the scaling parameter of the α-stable distribution doesn't scale appropriately with λ, the convergence properties in the infinite-width limit may change.
  - If the weights don't have a finite second moment, the autocovariance function may not exist or may have a different form.

- First 3 experiments:
  1. Verify that the random neural network is well-defined by checking that the weights/biases satisfy the admissibility conditions and that the activation thresholds follow a Poisson point process.
  2. Compute the characteristic functional of the random neural network for a simple case (e.g., d=1) to verify that it has the expected form.
  3. Numerically simulate random ReLU neural networks with different initialization laws (Gaussian and α-stable) and compare their behavior in the finite-width regime to validate the theoretical predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of the autocovariance function in (40) lead to the observation that wide networks can converge to non-Gaussian processes under certain choices of initialization?
- Basis in paper: Explicit - "Remarkably, the autocovariances of these processes have simple closed-form expressions."
- Why unresolved: The paper derives the closed-form expression but does not explicitly analyze how its structure differs from Gaussian processes or what specific features enable non-Gaussian convergence.
- What evidence would resolve it: Detailed comparison of the autocovariance function's mathematical properties with those of Gaussian processes, identifying key structural differences.

### Open Question 2
- Question: What are the precise conditions under which the characteristic functional of sReLU in (35) exhibits non-Gaussian behavior?
- Basis in paper: Explicit - "With the help of these statistics and the characteristic functional, we show that sReLU is a non-Gaussian process."
- Why unresolved: The paper shows sReLU is non-Gaussian but doesn't provide a rigorous characterization of when and why the characteristic functional deviates from Gaussianity.
- What evidence would resolve it: Mathematical analysis of the characteristic functional's behavior under different initialization laws, identifying thresholds for Gaussian vs. non-Gaussian regimes.

### Open Question 3
- Question: How do the boundary conditions ∂msReLU(0) = 0, |m| ≤ 1, affect the convergence properties of wide neural networks?
- Basis in paper: Explicit - "These boundary conditions are crucial in guaranteeing the existence of solutions to this SDE."
- Why unresolved: The paper mentions these conditions are crucial but doesn't explore their specific impact on asymptotic behavior or convergence rates.
- What evidence would resolve it: Systematic study of how different boundary conditions affect the limiting process's properties and convergence behavior.

## Limitations
- The mathematical framework relies heavily on admissibility conditions for the initialization law that may be difficult to verify in practice
- The analysis assumes shallow networks (single hidden layer), with extension to deep networks requiring additional theoretical work
- The closed-form autocovariance expression assumes finite second moments of weights, potentially excluding heavy-tailed distributions

## Confidence
- High confidence: The well-definedness of random ReLU neural networks as stochastic processes and the characterization via Poisson point processes and characteristic functionals
- Medium confidence: The closed-form autocovariance expression and the isotropy/self-similarity properties, as these depend on specific mathematical derivations that could contain subtle errors
- Medium confidence: The asymptotic convergence results to non-Gaussian processes, particularly for the α-stable case, as these involve delicate limit arguments

## Next Checks
1. Verify the well-definedness condition by implementing numerical checks that the initialization distributions satisfy the admissibility criteria (Lévy measure and finite absolute moment conditions) across different parameter settings
2. Compute empirical autocovariance functions from simulated random ReLU networks and compare them against the theoretical closed-form expression across different input distances and dimensions
3. Conduct numerical experiments on the infinite-width limit by simulating networks with increasing width and measuring the convergence of the empirical characteristic functional to both Gaussian and non-Gaussian limiting distributions under different initialization laws