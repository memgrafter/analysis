---
ver: rpa2
title: 'Don''t Transform the Code, Code the Transforms: Towards Precise Code Rewriting
  using LLMs'
arxiv_id: '2410.08806'
source_url: https://arxiv.org/abs/2410.08806
tags:
- code
- node
- transform
- transformations
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an approach to code transformation that uses
  large language models (LLMs) not to directly rewrite code, but to generate code
  transformations. The method involves synthesizing transformations from input/output
  examples through a chain-of-thought process that includes introspection and feedback.
---

# Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs

## Quick Facts
- arXiv ID: 2410.08806
- Source URL: https://arxiv.org/abs/2410.08806
- Reference count: 11
- Primary result: LLM-generated code transformations achieve 0.95 precision vs 0.60 for direct rewriting

## Executive Summary
This paper introduces a novel approach to code transformation that uses large language models (LLMs) not to directly rewrite code, but to generate explicit code transformations. The method employs a chain-of-thought process that synthesizes transformation code from input/output examples through iterative description, implementation, testing, and refinement. When tested on 16 Python code transformations, the approach achieved perfect precision on 7 transformations and outperformed direct LLM rewriting on the others.

## Method Summary
The approach involves providing LLMs with input/output code examples and asking them to generate explicit transformation code rather than directly rewriting the input. The methodology uses a chain-of-thought process with seven steps: describing the transformation in natural language, implementing it as Python code (typically using AST manipulation), executing the code on test examples, analyzing failures, and refining the implementation through multiple iterations. The transformations are formulated as AST rewrites using Python's ast.NodeTransformer framework, making the transformation logic inspectable and debuggable.

## Key Results
- Achieved 0.95 precision compared to 0.60 for direct LLM rewriting across 16 transformations
- Perfect precision (1.0) on 7 out of 16 transformations
- Superior performance on all transformations except "pure" transformations (which were equally effective with both approaches)
- Demonstrates that explicit transformation code is more reliable than direct code rewriting

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated code transformations achieve higher precision than direct LLM rewriting because the transformation logic becomes inspectable, debuggable, and can be executed repeatedly with minimal compute. This requires the LLM to accurately understand the transformation pattern from examples and translate it into working code.

### Mechanism 2
The chain-of-thought approach with introspection and feedback loops improves transformation synthesis by allowing iterative refinement. The LLM describes the transformation, implements it, tests on examples, analyzes failures, and refines the implementation through multiple loops.

### Mechanism 3
Using Abstract Syntax Tree (AST) representation enables precise code manipulation by allowing the LLM to generate AST manipulation code that preserves code semantics while applying the desired transformation.

## Foundational Learning

- Concept: Abstract Syntax Trees (ASTs)
  - Why needed here: The approach uses ASTs as the representation for code transformations
  - Quick check question: What are the main components of a Python AST node and how would you traverse an AST to find all function definitions?

- Concept: Chain-of-thought reasoning
  - Why needed here: The approach uses multiple reasoning steps including description, implementation, testing, and failure analysis
  - Quick check question: How would you structure a chain-of-thought prompt to first describe a problem, then implement a solution, then test and debug it?

- Concept: Code transformation patterns
  - Why needed here: Understanding common code transformation patterns is essential for recognizing and implementing the transformations
  - Quick check question: What are the key differences between semantics-preserving transformations and semantics-changing optimizations?

## Architecture Onboarding

- Component map:
  - Input/output example parser -> Natural language description generator -> Code synthesis engine -> Test execution environment -> Failure analysis module -> Iteration controller

- Critical path:
  1. Receive input/output examples
  2. Generate natural language description
  3. Generate transformation code
  4. Execute on test examples
  5. Analyze failures (if any)
  6. Refine code based on analysis
  7. Repeat until convergence or max iterations

- Design tradeoffs:
  - Number of examples vs. transformation accuracy: More examples may improve accuracy but increase cost
  - Maximum iterations vs. computation time: More iterations may improve quality but increase latency
  - Model size vs. inference cost: Larger models may generate better code but at higher cost

- Failure signatures:
  - Transformation code that doesn't compile or has syntax errors
  - Generated code that doesn't handle edge cases described in the examples
  - Infinite loops in the iteration process without improvement
  - Generated code that's overly complex or inefficient

- First 3 experiments:
  1. Implement the full pipeline for a simple transformation (like constant folding) and verify it works on all provided examples
  2. Test the failure analysis component by deliberately introducing errors and checking if the system can identify and fix them
  3. Measure the precision/recall tradeoff by testing on transformations that don't apply to some inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the code transformation approach scale with larger and more diverse codebases?
- Basis in paper: The paper tests on 16 transformations with 30 input/output pairs each, but does not explore scaling to larger or more complex codebases.
- Why unresolved: The current experiments are limited in scope and do not address performance on larger or more diverse codebases.
- What evidence would resolve it: Experiments on larger codebases with more complex transformations and varied code patterns.

### Open Question 2
- Question: What is the impact of using different LLM architectures or fine-tuning techniques on the precision and recall of code transformations?
- Basis in paper: The paper mentions that smaller models require more inferences but perform similarly, suggesting potential for improvement with different architectures or fine-tuning.
- Why unresolved: The paper does not explore the impact of different LLM architectures or fine-tuning techniques on performance.
- What evidence would resolve it: Comparative studies using different LLM architectures or fine-tuning techniques to assess their impact on precision and recall.

### Open Question 3
- Question: How can the chain-of-thought approach be optimized to reduce the number of iterations required to synthesize code transformations?
- Basis in paper: The paper discusses the iterative nature of the chain-of-thought approach and its impact on synthesis efficiency.
- Why unresolved: The paper does not explore methods to optimize the chain-of-thought approach to reduce iterations.
- What evidence would resolve it: Studies on optimizing the chain-of-thought approach, such as incorporating more efficient feedback mechanisms or adaptive iteration strategies.

## Limitations
- Limited evaluation scope: Only 16 Python transformations tested, which may not represent real-world diversity
- Resource constraints: Requires Llama 3.1 405B, limiting practical applicability due to computational cost
- Generalizability concerns: Method specialized for Python AST transformations, effectiveness for other languages untested

## Confidence
- High Confidence: The core finding that LLM-generated transformations outperform direct rewriting on tested benchmarks (precision 0.95 vs 0.60)
- Medium Confidence: The claim that inspectable transformation code enables debugging and iterative refinement
- Low Confidence: The assertion that this approach will scale to industrial-strength transformations without significant modifications

## Next Checks
1. **Benchmark Diversity Test**: Apply the methodology to a broader set of transformations (50+ diverse transformations across multiple languages) to validate whether the 0.95 precision generalizes beyond the current 16 Python examples.

2. **Resource Efficiency Analysis**: Measure the actual compute cost (API calls, iteration count, total tokens) for synthesizing each transformation to assess practical viability compared to direct rewriting approaches.

3. **Human-in-the-Loop Validation**: Conduct a user study where developers debug and refine the generated transformation code, measuring whether the inspectable code actually improves the debugging experience compared to opaque direct rewrites.