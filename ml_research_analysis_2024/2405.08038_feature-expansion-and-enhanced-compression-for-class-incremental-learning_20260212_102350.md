---
ver: rpa2
title: Feature Expansion and enhanced Compression for Class Incremental Learning
arxiv_id: '2405.08038'
source_url: https://arxiv.org/abs/2405.08038
tags:
- incremental
- classes
- step
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in class incremental
  learning by introducing a dynamic model expansion/compression approach. The key
  innovation is a Rehearsal-CutMix data augmentation that improves knowledge distillation
  during compression by mixing new class samples with rehearsal memory exemplars.
---

# Feature Expansion and enhanced Compression for Class Incremental Learning

## Quick Facts
- arXiv ID: 2405.08038
- Source URL: https://arxiv.org/abs/2405.08038
- Authors: Quentin Ferdinand; Gilles Le Chenadec; Benoit Clement; Panagiotis Papadakis; Quentin Oliveau
- Reference count: 40
- Key outcome: Achieves up to 78.07% average top-1 accuracy on CIFAR-100 B0 10-step protocol and 94.76% on ImageNet-100 10-step protocol, outperforming state-of-the-art approaches

## Executive Summary
This paper addresses catastrophic forgetting in class incremental learning through a dynamic model expansion and compression approach. The key innovation is a Rehearsal-CutMix data augmentation that improves knowledge distillation during compression by mixing new class samples with rehearsal memory exemplars. By expanding the model with new feature extractors while freezing old ones, then compressing back to original size using this enhanced augmentation, the method achieves superior performance on CIFAR-100 and ImageNet benchmarks compared to existing approaches.

## Method Summary
The method follows a dynamic expansion-compression framework for class incremental learning. During the expansion phase, the previous feature extractor is frozen while a new one is trained alongside an auxiliary classifier to learn new classes from the incremental dataset. In the compression phase, the model is compressed back to its original size using Rehearsal-CutMix data augmentation, which mixes images from the incremental dataset with those from the rehearsal memory to create balanced training samples. Knowledge distillation is then used to transfer knowledge from the expanded model to the compressed one, preserving performance while reducing parameters.

## Key Results
- Achieves 78.07% average top-1 accuracy on CIFAR-100 B0 10-step protocol
- Achieves 94.76% average top-1 accuracy on ImageNet-100 10-step protocol
- Outperforms state-of-the-art methods including DER, FOSTER, and PODNet on both benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Rehearsal-CutMix augmentation improves knowledge distillation by ensuring that training samples contain information about both new and past classes.
- Mechanism: By sampling one image from the incremental dataset (mostly new classes) and one from the rehearsal memory (only previous classes), then mixing them using the CutMix procedure, the method creates training samples that bridge the feature space between old and new classes. This helps the compressed model learn better decision boundaries at the interfaces between old and new class clusters.
- Core assumption: The feature extractors from the expanded model have learned distinct but complementary representations for old and new classes that can be effectively transferred to the compressed model through mixed samples.
- Evidence anchors:
  - [abstract]: "This new data augmentation reduces catastrophic forgetting by specifically targeting past class information and improving its compression."
  - [section 3.3]: "it ensures that most training samples contain information about both new and past classes which allows the knowledge distillation loss to transfer better the overall knowledge of the model and especially at the boundaries between each old and new class."
  - [corpus]: Weak - No direct evidence in corpus neighbors about this specific mixing mechanism, though related papers mention class imbalance and feature expansion.

### Mechanism 2
- Claim: The Rehearsal-CutMix augmentation rebalances the training dataset toward past classes during compression.
- Mechanism: Since the incremental dataset Dt is biased toward new classes, mini-batches sampled from it contain more new class images. By mixing these with images from the rehearsal memory (which contains only previous classes), the effective label distribution in training samples becomes more balanced between old and new classes, reducing the bias in the classification layer.
- Core assumption: The imbalance in the incremental dataset significantly impacts the compression phase, causing the compressed model to perform poorly on previous classes.
- Evidence anchors:
  - [section 2.2]: "Due to the imbalance of Dt, however, such a compression scheme performs better in new than past classes [21], leading to a compressed model with poor performances on previous classes."
  - [section 3.3]: "By taking x j from a second mini-batch sampled solely from Mt, we ensure the generated images are either a new class mixed with a previous class, or two previous classes mixed together which gives three main benefits for our compression step."
  - [corpus]: Weak - Related papers mention class imbalance but not this specific rebalancing mechanism through mixed sampling.

### Mechanism 3
- Claim: The dynamic expansion followed by compression allows the model to achieve a better stability-plasticity tradeoff than conventional incremental learning methods.
- Mechanism: By freezing the previous feature extractor and adding a new one during expansion, the method prevents catastrophic forgetting of previous class features. The compression phase then efficiently consolidates the knowledge from both extractors into a single one, maintaining performance while controlling parameter growth.
- Core assumption: The expansion phase can learn complementary features for new classes without disrupting the frozen previous class features, and these can be effectively merged during compression.
- Evidence anchors:
  - [abstract]: "Recently, dynamic deep learning architectures have been shown to exhibit a better stability-plasticity trade-off by dynamically adding new feature extractors to the model in order to learn new classes followed by a compression step to scale the model back to its original size."
  - [section 3.1]: "By freezing the previous feature extractor and training a new one, it is ensured that no forgetting of previous features happens in this expansion phase."
  - [corpus]: Moderate - Related papers mention dynamic expansion and parameter efficiency, supporting the general approach.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: The compression phase uses knowledge distillation to transfer knowledge from the expanded model (Φt_big) to the compressed model (Φt), preserving performance while reducing parameters.
  - Quick check question: What is the difference between hard targets and soft targets in knowledge distillation, and why are soft targets used here?

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding why models forget previous classes is crucial for grasping why the expansion phase freezes old features and why the compression phase needs special handling for past class knowledge.
  - Quick check question: What are the two main causes of catastrophic forgetting in incremental learning, and how does this method address each?

- Concept: Data Augmentation Techniques (Mixup and CutMix)
  - Why needed here: Rehearsal-CutMix builds on these techniques, so understanding their principles is essential for understanding the novel contribution.
  - Quick check question: How do Mixup and CutMix differ in their approach to mixing samples, and what are the benefits of each?

## Architecture Onboarding

- Component map:
  - Feature extractors: φt-1 (frozen previous), φt_new (adaptable new)
  - Classifiers: Ht_big (expanded, Ct outputs), Ht (compressed, Ct outputs)
  - Rehearsal memory: Mt (stores exemplars from previous classes)
  - Rehearsal-CutMix module: mixes images from Dt and Mt

- Critical path:
  1. Expansion phase: Freeze φt-1, train φt_new and Ht_big on Dt
  2. Compression phase: Initialize Φt from Φt-1, train using Rehearsal-CutMix and knowledge distillation from Φt_big
  3. Update: Discard Φt_big, use Φt for next incremental step

- Design tradeoffs:
  - Freezing φt-1 ensures no forgetting but may limit adaptation to new data distributions
  - Using two feature extractors during expansion increases computation but improves stability
  - Rehearsal-CutMix adds training time overhead but significantly improves compression quality
  - Compression after each step controls parameter growth but adds complexity

- Failure signatures:
  - Performance degrades on previous classes: Indicates compression phase isn't effectively preserving old knowledge
  - Performance degrades on new classes: Suggests expansion phase isn't effectively learning new features
  - Parameter count grows over time: Compression phase isn't working correctly
  - Training becomes unstable: Possible issues with Rehearsal-CutMix implementation or hyperparameter settings

- First 3 experiments:
  1. Run with standard CutMix (not rehearsal-based) during compression to quantify the benefit of the Rehearsal-CutMix modification
  2. Run without any augmentation during compression to establish baseline performance degradation
  3. Run with larger/smaller rehearsal memory to find optimal memory size for the Rehearsal-CutMix procedure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FECIL scale with increasing numbers of incremental steps beyond the tested 5, 10, and 20 steps on CIFAR-100?
- Basis in paper: [inferred] The paper shows that DER without pruning performs better at 20 steps due to parameter growth, suggesting potential limitations of compression-based approaches at higher step counts
- Why unresolved: The paper only evaluates up to 20 steps, and while FOSTER and FECIL show better compression, the long-term scalability with very large numbers of incremental steps remains untested
- What evidence would resolve it: Testing FECIL on datasets with 50+ incremental steps while monitoring accuracy degradation and parameter efficiency would provide clarity

### Open Question 2
- Question: What is the optimal balance between the number of exemplars stored in rehearsal memory and the performance of R-CutMix compression?
- Basis in paper: [explicit] The paper notes that with B50 10-steps protocol (20 exemplars per class), FECIL doesn't reach DER's performance, suggesting memory size affects R-CutMix effectiveness
- Why unresolved: The paper only tests two memory sizes (2000 total and 20 per class) without systematically varying the number of exemplars to find the optimal point
- What evidence would resolve it: Controlled experiments varying memory size while keeping other factors constant would reveal the relationship between memory capacity and R-CutMix performance

### Open Question 3
- Question: How would FECIL perform if the expansion phase used more than two feature extractors before compression?
- Basis in paper: [inferred] FECIL only uses two feature extractors (current and previous), while the paper mentions that earlier approaches used multiple extractors, suggesting potential unexplored benefits of deeper expansion
- Why unresolved: The paper focuses on the compression phase improvements rather than exploring the full potential of multi-extractor expansion before compression
- What evidence would resolve it: Implementing FECIL with 3+ feature extractors during expansion and comparing compression effectiveness would show if more extensive expansion improves final performance

## Limitations

- Limited evaluation to CIFAR-100 and ImageNet-100 datasets with 100 classes each, restricting generalizability to larger-scale problems
- Computational overhead during expansion phase with dual feature extractors not fully characterized in terms of memory and training time requirements
- Lack of ablation studies isolating the contribution of Rehearsal-CutMix from other components of the framework

## Confidence

- **High Confidence**: The general framework of dynamic expansion followed by compression is technically sound and addresses a well-documented problem in incremental learning literature.
- **Medium Confidence**: The Rehearsal-CutMix augmentation likely improves knowledge distillation through mixed sampling, but the exact magnitude of benefit relative to standard CutMix requires further validation.
- **Medium Confidence**: The claim of state-of-the-art performance is supported by CIFAR-100 results but needs additional validation on more diverse datasets and larger class numbers.

## Next Checks

1. **Ablation Study on Augmentation**: Run experiments comparing Rehearsal-CutMix against standard CutMix and no augmentation during the compression phase to quantify the specific contribution of the rehearsal-based mixing mechanism.

2. **Memory Size Sensitivity Analysis**: Systematically vary the rehearsal memory size (M) to determine optimal memory requirements and understand how performance scales with available memory, particularly focusing on the trade-off between memory cost and accuracy preservation.

3. **Cross-Dataset Generalization**: Validate the method on a third dataset with different characteristics (e.g., CIFAR-10 or a domain-specific dataset) to assess robustness and generalizability beyond the two evaluated datasets.