---
ver: rpa2
title: When Context Leads but Parametric Memory Follows in Large Language Models
arxiv_id: '2409.08435'
source_url: https://arxiv.org/abs/2409.08435
tags:
- knowledge
- context
- parametric
- sentences
- contexts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how nine widely used LLMs allocate knowledge
  between local context and global parameters when answering open-ended questions
  in knowledge-consistent scenarios. We introduce a novel dataset, WikiAtomic, and
  systematically vary context sizes to analyze how LLMs prioritize and utilize provided
  information and their parametric knowledge.
---

# When Context Leads but Parametric Memory Follows in Large Language Models

## Quick Facts
- arXiv ID: 2409.08435
- Source URL: https://arxiv.org/abs/2409.08435
- Authors: Yufei Tao; Adam Hiatt; Erik Haake; Antonie J. Jetter; Ameeta Agrawal
- Reference count: 31
- Primary result: LLMs consistently rely on ~70% contextual and ~30% parametric knowledge across varying context sizes

## Executive Summary
This study investigates how nine widely used LLMs allocate knowledge between local context and global parameters when answering open-ended questions. Using a novel WikiAtomic dataset of atomic sentences extracted from Wikipedia, the researchers systematically varied context sizes from 0 to 50 sentences to analyze knowledge utilization patterns. The findings reveal consistent behavior across diverse model architectures, with models maintaining a stable 70/30 ratio of contextual to parametric knowledge, prioritizing earlier context in longer sequences, and reducing hallucinations as context increases.

## Method Summary
The researchers created the WikiAtomic dataset by extracting 10,000 atomic sentences from 200 Wikipedia articles using GPT-4o. They then prompted nine LLMs (GPT-4o, Claude variants, Llama 3, Mixtral, Mistral, and Phi-3) with varying context sizes while keeping the query constant. Model responses were converted to atomic sentences and evaluated using the INFUSE framework to classify sentences as contextual or parametric knowledge, and FActScore to detect hallucinations. The analysis examined knowledge preference patterns, context utilization, and hallucination rates across different context sizes.

## Key Results
- All models consistently rely on approximately 70% contextual knowledge and 30% parametric knowledge
- Hallucinations decrease as context size increases, stabilizing with as little as 10 sentences
- In longer contexts, models disproportionately focus on the first quartile of information
- Knowledge similarity between responses and context increases with context size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs balance contextual and parametric knowledge at a consistent ratio across different model sizes
- Mechanism: Models maintain approximately 70% contextual knowledge and 30% parametric knowledge regardless of context size
- Core assumption: The ratio of contextual to parametric knowledge remains stable even as context varies
- Evidence anchors:
  - [abstract] "Our findings reveal consistent patterns across models, including a consistent reliance on both contextual (around 70%) and parametric (around 30%) knowledge"
  - [section 5.1] "Larger contexts generally increase the model's reliance on contextual knowledge (about 70%) while reducing dependence on parametric knowledge (about 30%)"
- Break condition: If the model's parametric knowledge storage capacity is exceeded or context is too limited to provide sufficient information

### Mechanism 2
- Claim: LLMs prioritize earlier context over later context when processing long inputs
- Mechanism: When context exceeds a certain length, models disproportionately focus on the first quartile of the provided information
- Core assumption: The model's attention mechanism has a bias toward earlier tokens in the input sequence
- Evidence anchors:
  - [section 5.2] "For smaller contexts (k < 10), the model treats all portions of the context equally. As context increases, the model predominantly focuses on the first quartile."
  - [section 5.3] "Earlier information is prioritized in longer contexts, with responses following the order of presented information"
- Break condition: If context is presented in a non-sequential order or if the model uses a different attention mechanism that doesn't prioritize earlier tokens

### Mechanism 3
- Claim: Increasing context reduces hallucinations in LLM responses
- Mechanism: As more factual context is provided, models rely less on parametric knowledge, which may contain inaccuracies or outdated information
- Core assumption: Parametric knowledge can contain errors or become outdated, while context provides current, accurate information
- Evidence anchors:
  - [abstract] "decrease in hallucinations with increasing context"
  - [section 5.4] "For smaller contexts, models have higher hallucination rate, which improves with additional context and converges with as little as 10 sentences in context"
- Break condition: If the context itself contains misinformation or if the model's parametric knowledge is more accurate than the provided context

## Foundational Learning

- Concept: Knowledge grounding
  - Why needed here: Understanding how LLMs combine parametric knowledge with contextual information is crucial for interpreting model behavior
  - Quick check question: What is the difference between parametric knowledge and contextual knowledge in LLMs?

- Concept: Attention mechanisms
  - Why needed here: The model's attention mechanism determines how it processes and prioritizes different parts of the input context
  - Quick check question: How does an LLM's attention mechanism typically handle long sequences of input?

- Concept: Hallucination detection
  - Why needed here: Evaluating the factual accuracy of LLM responses requires understanding how hallucinations are identified and measured
  - Quick check question: What methods are commonly used to detect hallucinations in LLM-generated text?

## Architecture Onboarding

- Component map: WikiAtomic dataset creation -> Model response generation -> Atomic sentence conversion -> INFUSE classification -> FActScore evaluation -> Pattern analysis
- Critical path:
  1. Extract atomic sentences from Wikipedia articles
  2. Generate responses with varying context sizes
  3. Atomize model responses
  4. Classify sentences as contextual or parametric using INFUSE
  5. Evaluate hallucinations using FActScore
  6. Analyze patterns in knowledge utilization
- Design tradeoffs:
  - Atomic sentence extraction vs. natural sentence processing
  - Context size vs. computational efficiency
  - Granularity of hallucination detection vs. speed of evaluation
- Failure signatures:
  - Models showing inconsistent patterns across different context sizes
  - High hallucination rates even with large context
  - Inability to distinguish between contextual and parametric knowledge
- First 3 experiments:
  1. Test different context sizes with a single model to observe knowledge utilization patterns
  2. Compare contextual/parametric knowledge ratios across different model architectures
  3. Evaluate hallucination rates with varying amounts of context to confirm the inverse relationship

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific underlying mechanisms that cause all tested LLMs to exhibit similar patterns in knowledge prioritization between contextual and parametric sources?
- Basis in paper: Explicit - The paper states "Somewhat surprisingly, we found that all models (except Phi-3 in certain cases) show consistently similar patterns" and notes "This uniformity among the models suggests a shared underlying mechanism in processing and responding to contextual information."
- Why unresolved: While the paper identifies the pattern, it doesn't investigate the specific architectural or training-related reasons why different models converge on similar behavior. The uniformity across diverse model architectures (GPT-4o, Claude models, Llama variants, etc.) suggests a deeper underlying principle that requires further investigation.
- What evidence would resolve it: Comparative analysis of model architectures and training data, focusing on attention mechanisms and context window handling across different models, could reveal shared design principles that lead to this convergence.

### Open Question 2
- Question: How would the models' knowledge prioritization and hallucination rates change when processing shuffled or randomized context sequences?
- Basis in paper: Inferred - The paper notes that models "prioritize information sequentially" and that "Earlier information is prioritized in longer contexts, with responses following the order of presented information." This suggests order dependence but doesn't test what happens when context is shuffled.
- Why unresolved: The current experiments maintain the natural sequence of information, but real-world applications might present information in non-sequential or shuffled order. Understanding how models handle disordered context would reveal whether their sequential processing is a fundamental limitation.
- What evidence would resolve it: Controlled experiments comparing model performance with sequential vs. shuffled contexts, measuring changes in contextual vs. parametric knowledge usage and hallucination rates.

### Open Question 3
- Question: What is the minimum threshold of contextual information required for models to significantly reduce parametric knowledge incorporation and hallucinations?
- Basis in paper: Explicit - The paper observes that "Hallucinations decrease as context increases" and notes that for smaller contexts, models show different preferences, while larger contexts generally increase reliance on contextual knowledge (about 70%) while reducing dependence on parametric knowledge (about 30%).
- Why unresolved: While the paper shows a general trend, it doesn't identify a specific tipping point where models transition from predominantly parametric to predominantly contextual knowledge usage. This threshold could be crucial for practical applications.
- What evidence would resolve it: Detailed analysis of model behavior across incremental context sizes, identifying the specific context length where parametric knowledge usage drops below a certain threshold (e.g., 20% or 10%) and hallucination rates stabilize at minimal levels.

## Limitations

- Analysis focused on Wikipedia-based knowledge, which may not generalize to all domains
- INFUSE framework's binary classification threshold (0.5) introduces ambiguity for sentences near the threshold
- Sequential context presentation doesn't test how models handle non-sequential or shuffled information

## Confidence

- **High Confidence**: The observed decrease in hallucinations with increasing context (supported by FActScore metrics across all nine models)
- **Medium Confidence**: The consistent 70/30 contextual/parametric knowledge ratio (robust across models but dependent on INFUSE framework's threshold)
- **Medium Confidence**: The prioritization of earlier context in longer inputs (supported by similarity analyses but requires further validation with different attention mechanisms)

## Next Checks

1. **Cross-Domain Generalization Test**: Apply the same methodology to non-Wikipedia knowledge sources (scientific papers, news articles) to verify whether the 70/30 knowledge ratio and context prioritization patterns persist across different knowledge domains.

2. **Threshold Sensitivity Analysis**: Systematically vary the INFUSE classification threshold from 0.3 to 0.7 to assess how sensitive the contextual/parametric knowledge ratios are to this parameter, and implement the suggested ablation study for ambiguous sentences.

3. **Attention Mechanism Comparison**: Test the same context-response patterns with models using different attention mechanisms (sparse attention, local attention) to determine whether the observed prioritization of earlier context is a fundamental LLM behavior or specific to the tested architectures.