---
ver: rpa2
title: 'WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models'
arxiv_id: '2401.13919'
source_url: https://arxiv.org/abs/2401.13919
tags:
- step
- agent
- task
- click
- webv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WebVoyager introduces a large multimodal model-based web agent
  capable of end-to-end task completion on real-world websites using both visual (screenshots)
  and textual inputs. It employs GPT-4V for element detection and decision-making,
  marking interactive elements on screenshots to guide actions like click, type, and
  scroll.
---

# WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models

## Quick Facts
- arXiv ID: 2401.13919
- Source URL: https://arxiv.org/abs/2401.13919
- Authors: Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu
- Reference count: 23
- Primary result: 59.1% task success rate on 643 tasks across 15 websites

## Executive Summary
WebVoyager introduces a large multimodal model-based web agent capable of end-to-end task completion on real-world websites using both visual (screenshots) and textual inputs. The system employs GPT-4V for element detection and decision-making, marking interactive elements on screenshots to guide actions like click, type, and scroll. A new benchmark of 643 tasks across 15 websites was created, along with an automatic evaluation protocol using GPT-4V that achieves 85.3% agreement with human judgment. WebVoyager achieves a 59.1% task success rate, significantly outperforming both GPT-4 (All Tools) at 30.8% and a text-only baseline at 40.1%.

## Method Summary
WebVoyager is an end-to-end web agent that uses GPT-4V to process both visual (screenshots) and textual inputs for task completion. The system overlays bounding boxes on interactive elements in screenshots, enabling the model to identify and interact with specific web elements without parsing dense HTML accessibility trees. The agent follows a loop of capturing screenshots, generating thoughts and actions through GPT-4V, executing actions via Selenium WebDriver, and evaluating task completion using an automatic evaluation protocol with GPT-4V. The system handles actions including click, type, scroll, wait, back, jump, and answer, with context clipping to manage long trajectories.

## Key Results
- WebVoyager achieves 59.1% task success rate on 643 tasks across 15 websites
- Outperforms GPT-4 (All Tools) at 30.8% and text-only baseline at 40.1%
- Automatic evaluation protocol achieves 85.3% agreement with human judgment
- Navigation stuck accounts for 44.4% of failures in error analysis

## Why This Works (Mechanism)

### Mechanism 1
GPT-4V with visual element marking outperforms text-only baselines on complex visual layouts. By overlaying bounding boxes and numerical labels on screenshots, the agent can identify and interact with specific web elements without parsing dense HTML accessibility trees, reducing decision complexity. Core assumption: The visual cues provided by bounding boxes are sufficient for the model to distinguish and select interactive elements correctly. Evidence: WebVoyager achieves 59.1% task success rate, significantly surpassing text-only performance at 40.1%.

### Mechanism 2
GPT-4V can serve as a reliable automatic evaluator for multimodal web agents. GPT-4V assesses trajectories by comparing task instructions with the agent's actions and final responses, achieving high agreement with human judgments. Core assumption: The multimodal understanding of GPT-4V is sufficient to judge whether a task has been completed successfully based on screenshots and agent outputs. Evidence: The proposed automatic evaluation metric achieves 85.3% agreement with human judgment.

### Mechanism 3
Combining visual and textual inputs yields higher task success rates than either modality alone. Visual inputs simplify navigation of complex GUIs while textual inputs provide precise element identification, with each modality compensating for the other's weaknesses. Core assumption: Some web tasks require visual layout understanding while others require precise text extraction. Evidence: WebVoyager achieves 59.1% success rate while text-only setting reaches only 40.1%.

## Foundational Learning

- **Set-of-Mark Prompting**: Enables the agent to locate interactive elements without requiring a separate object detection model, reducing latency and complexity. Quick check: How does Set-of-Mark Prompting differ from traditional object detection approaches in guiding agent actions?

- **Context Clipping in Long Trajectories**: Prevents the model from being overwhelmed by excessive historical observations by keeping only the last three observations in the prompt. Quick check: What is the trade-off between retaining more historical observations versus keeping the prompt concise?

- **Automatic Evaluation with Multimodal Models**: Addresses the scalability challenge of human evaluation by using GPT-4V to automate evaluation while maintaining high agreement with human judgment. Quick check: Why is agreement with human judgment a critical metric for validating an automatic evaluator?

## Architecture Onboarding

- **Component map**: Selenium WebDriver -> GPT-4V with Set-of-Mark -> Action Executor -> Context Manager -> GPT-4V Evaluator

- **Critical path**: User query → instantiate browser → capture initial screenshot → GPT-4V processes screenshot + auxiliary text + history → generates thought and action → Action Executor performs action → new screenshot captured → loop until ANSWER action or step limit reached → Final screenshot + response → GPT-4V evaluator judges success

- **Design tradeoffs**: Visual vs. HTML inputs (intuitive vs precise for dense text), Fixed window size (1024x768) for consistent resolution but may miss details, Maximum 15 steps limits execution time but may fail complex tasks

- **Failure signatures**: Navigation Stuck (agent repeats scrolling without progress), Visual Grounding Issue (agent misidentifies elements), Hallucination (agent produces incorrect answers), Prompt Misalignment (agent fails to parse generated actions)

- **First 3 experiments**: 1) Ablation study: Compare success rates with and without visual bounding box overlays, 2) Evaluator reliability: Measure GPT-4V agreement with human judgments on varying numbers of screenshots, 3) Step limit impact: Test performance with different maximum step limits (10, 15, 20)

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal combination of visual and textual inputs for web agents to maximize task success rates across different website types? Basis: The paper shows WebVoyager struggles with text-heavy websites while text-only agent performs poorly on visually complex websites, suggesting both modalities are necessary but optimal balance is unclear. Unresolved because the paper doesn't systematically explore different ratios or combinations. Evidence needed: Systematic ablation study varying proportion of visual versus textual inputs across different website categories.

### Open Question 2
How can web agents better handle navigation when encountering repetitive or non-productive actions that lead to getting stuck? Basis: Error analysis shows "Navigation Stuck" accounts for 44.4% of failures. Unresolved because the paper identifies this as major failure mode but doesn't propose concrete solutions. Evidence needed: Development and testing of specific algorithms for detecting non-productive navigation patterns and implementing strategies to break out of loops.

### Open Question 3
What architectural improvements are needed for LMMs to better handle fine-grained visual details required for web navigation tasks? Basis: Existing open-source LMMs have limitations including low image resolution (224x224 or 336x336) and limited context length. Unresolved because the paper acknowledges these limitations but doesn't explore architectural modifications. Evidence needed: Development of LMMs with higher resolution image processing capabilities and larger context windows.

### Open Question 4
How can automatic evaluation protocols be improved to reduce the remaining disagreement between LMM evaluators and human judgments? Basis: GPT-4V evaluator achieves 85.3% agreement with human judgment (kappa = 0.70), indicating room for improvement. Unresolved because while the paper proposes automatic evaluation protocol, it doesn't explore methods to further improve agreement rate. Evidence needed: Development of enhanced evaluation protocols incorporating multiple LMM evaluators, additional context, or hybrid human-AI evaluation approaches.

## Limitations
- **Dataset Bias**: The 643-task benchmark across 15 websites may not represent full diversity of real-world web environments, particularly regarding accessibility features and dynamic content loading patterns.
- **Evaluation Reliability**: While automatic evaluator shows 85.3% agreement with human judgment, 14.7% disagreement rate remains, and conditions under which GPT-4V fails to align with human evaluators are unclear.
- **Generalization Concerns**: System relies on consistent visual layouts and element positioning, making it vulnerable to websites with frequent layout changes, A/B testing variations, or personalized content.

## Confidence

- **High Confidence**: GPT-4V with visual element marking outperforms text-only baselines, supported by direct comparative results (59.1% vs 40.1% success rates) and clear mechanism descriptions.
- **Medium Confidence**: Automatic evaluator's reliability, given 85.3% agreement rate with human judgment, though remaining disagreement rate and specific failure conditions warrant further investigation.
- **Medium Confidence**: Combined visual-textual approach superiority, based on ablation study results, though specific scenarios where each modality excels could be better characterized.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate WebVoyager on a separate, held-out set of websites not included in the original 15-website benchmark to assess performance stability across different web domains and design patterns.

2. **Failure Mode Analysis**: Systematically catalog and analyze the 40.9% of tasks where WebVoyager fails, categorizing failure types (navigation stuck, visual grounding, hallucination) to identify specific improvement opportunities.

3. **Real-World Deployment Stress Test**: Deploy WebVoyager on websites with dynamic content, frequent layout changes, and accessibility features to measure robustness under conditions that differ from the controlled benchmark environment.