---
ver: rpa2
title: Learning Goal-Conditioned Policies from Sub-Optimal Offline Data via Metric
  Learning
arxiv_id: '2402.10820'
source_url: https://arxiv.org/abs/2402.10820
tags:
- learning
- value
- function
- policy
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MetricRL, a method for learning optimal goal-conditioned
  policies from sub-optimal offline datasets in reinforcement learning. The key innovation
  is using metric learning to approximate the optimal value function by learning a
  distance-monotonic representation of the state space, where distances correspond
  to the minimum number of actions needed to reach one state from another.
---

# Learning Goal-Conditioned Policies from Sub-Optimal Offline Data via Metric Learning

## Quick Facts
- arXiv ID: 2402.10820
- Source URL: https://arxiv.org/abs/2402.10820
- Authors: Alfredo Reichlin; Miguel Vasco; Hang Yin; Danica Kragic
- Reference count: 40
- Primary result: Introduces MetricRL, a method for learning optimal goal-conditioned policies from sub-optimal offline datasets using metric learning to approximate optimal value functions

## Executive Summary
This paper introduces MetricRL, a method for learning optimal goal-conditioned policies from sub-optimal offline datasets in reinforcement learning. The key innovation is using metric learning to approximate the optimal value function by learning a distance-monotonic representation of the state space, where distances correspond to the minimum number of actions needed to reach one state from another. The authors prove that this approach enables recovery of optimal policies when actions are invertible. MetricRL avoids the out-of-distribution estimation errors common in offline RL by combining this value function approximation with weighted imitation learning.

## Method Summary
MetricRL learns a distance-monotonic embedding of the state space using contrastive loss, where Euclidean distances in the latent space approximate the minimum number of actions needed to reach one state from another. The value function is defined as the negative of this distance scaled by the discount factor and reward, eliminating the need for Bellman backups. The policy is trained via weighted imitation learning using advantage estimates from the learned value function, which avoids out-of-distribution estimation errors common in offline RL. The method requires deterministic transitions, sparse rewards, and invertible actions to guarantee optimality.

## Key Results
- MetricRL consistently outperforms prior state-of-the-art methods in learning near-optimal behavior from severely sub-optimal datasets, including those collected by random policies
- The method is particularly effective when dataset quality is low, maintaining performance while other methods fail
- Experiments across multiple environments (Maze2D, Reach, Hypermaze, Minigrid) demonstrate the approach's effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distance monotonicity enables learning optimal policies from sub-optimal datasets without conservative constraints
- Mechanism: Metric learning creates a latent space where Euclidean distances reflect minimum action counts to reach states, approximating the optimal value function without requiring Bellman backups or conservative penalties
- Core assumption: MDP has deterministic transitions, sparse rewards, and invertible actions
- Evidence anchors:
  - [abstract] "using metric learning to approximate the optimal value function for goal-conditioned offline RL problems under sparse rewards, invertible actions and deterministic transitions"
  - [section 3.1] "We say ϕ is distance monotonic if for all s1, s2, s3 ∈ S, the following holds: dS(s1, s3) < dS(s2, s3) ⟹ dZ(ϕ(s1), ϕ(s3)) < dZ(ϕ(s2), ϕ(s3))"
- Break condition: If actions are not invertible, the metric space assumption fails and distance monotonicity cannot guarantee optimality

### Mechanism 2
- Claim: Weighted imitation learning avoids out-of-distribution estimation errors common in offline RL
- Mechanism: Policy is trained on in-distribution actions weighted by advantage estimates from the distance-monotonic value function, eliminating the need for max operator in Bellman backups
- Core assumption: Dataset contains all relevant transitions and actions needed for optimal behavior
- Evidence anchors:
  - [abstract] "MetricRL avoids the out-of-distribution estimation errors common in offline RL by combining this value function approximation with weighted imitation learning"
  - [section 3.3] "Using the proposed distance monotonic representation instead of Temporal Difference methods for the critic solves the classic offline RL issue of out-of-distribution transitions"
- Break condition: If dataset is disconnected or missing critical transitions, the learned representation cannot capture true distances

### Mechanism 3
- Claim: The learned representation interpolates missing transitions when inverse actions are present
- Mechanism: Even with incomplete datasets, the distance-monotonic embedding can estimate distances between states that weren't directly observed by leveraging the symmetric structure of invertible actions
- Core assumption: Dataset has sufficient coverage of reversible transitions
- Evidence anchors:
  - [section 3.4] "MetricRL requires significantly fewer data points as it can interpolate missing transitions when the inverse transition is present in the data"
  - [section 4.3] "As stated in Section 3.4, the connectivity assumption can be solved using 'super-states' to join the states into a unique connected component"
- Break condition: If dataset lacks both forward and reverse transitions between critical state pairs, interpolation fails

## Foundational Learning

- Concept: Metric spaces and isometries
  - Why needed here: The method relies on learning a Euclidean metric space where distances correspond to minimum action counts, so understanding metric space properties is fundamental
  - Quick check question: What are the three axioms that define a metric space, and how do they relate to the distance-monotonic property?

- Concept: Goal-conditioned reinforcement learning
  - Why needed here: The paper addresses goal-reaching tasks with sparse rewards, so understanding how value functions and policies operate in this setting is essential
  - Quick check question: How does the value function definition change in goal-conditioned RL compared to standard RL?

- Concept: Offline reinforcement learning challenges
  - Why needed here: The method specifically addresses out-of-distribution estimation errors, so understanding why these occur in offline RL is crucial
  - Quick check question: Why does the max operator in Bellman backups cause out-of-distribution estimation errors in offline RL?

## Architecture Onboarding

- Component map:
  - Representation learner (ϕθ) -> Value function approximator -> Policy learner -> Dataset handler

- Critical path:
  1. Learn distance-monotonic representation (stabilize first, this is the hardest part)
  2. Compute value function from learned representation
  3. Train policy using weighted imitation learning
  4. Evaluate on goal-reaching tasks

- Design tradeoffs:
  - Euclidean vs quasimetric embeddings: Euclidean is easier to optimize but less general; quasimetric can handle non-invertible actions but harder to learn
  - Super-state vs dataset augmentation: Super-states ensure connectivity but change representation structure; augmentation preserves structure but may not work with images
  - Loss stabilization: Logarithmic transformation of contrastive term stabilizes training but weakens negative pull

- Failure signatures:
  - Representation not learning distance monotonicity: Check triplet distances in latent space vs state space
  - Policy not improving: Verify value function estimates are reasonable and advantages are correctly computed
  - Poor performance on images: Ensure super-state is properly implemented and transitions are added

- First 3 experiments:
  1. Verify distance monotonicity: Sample triplets from Maze2D, compute state-space and latent-space distances, check monotonicity ratio increases during training
  2. Test value function quality: Compare learned value function against ground truth in simple mazes across different dataset qualities
  3. Validate policy training: Train policy on synthetic data with known optimal behavior, check if policy converges to optimal actions

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but identifies key limitations in the Limitations section.

## Limitations
- The method requires deterministic transitions and invertible actions, limiting its applicability to stochastic environments
- Performance on complex image-based tasks with long horizons remains untested
- The approach assumes sparse rewards and may not generalize well to dense reward settings

## Confidence
- Core claim about learning optimal policies from sub-optimal data via distance monotonicity: **High**
- Performance on stochastic environments: **Medium**
- Scalability to complex image-based tasks: **Medium**
- Avoiding out-of-distribution estimation errors: **High** (within stated assumptions)

## Next Checks
1. Test MetricRL on stochastic environments where actions have probabilistic outcomes to evaluate robustness beyond deterministic assumptions
2. Implement and evaluate the quasimetric variant to handle non-invertible actions and compare against the Euclidean version
3. Scale experiments to more complex image-based tasks with longer time horizons to assess practical limitations