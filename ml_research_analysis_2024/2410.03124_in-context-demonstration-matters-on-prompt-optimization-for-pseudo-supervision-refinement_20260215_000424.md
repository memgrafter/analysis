---
ver: rpa2
title: 'In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision
  Refinement'
arxiv_id: '2410.03124'
source_url: https://arxiv.org/abs/2410.03124
tags:
- prompt
- data
- learning
- demonstrations
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an unsupervised prompt learning method for
  black-box LLMs that simultaneously learns the prompt and pseudo labels for unlabeled
  data. The key innovation is using in-context demonstrations from reliable pseudo-labeled
  data to align the prompt-learning and prompt-using stages, ensuring consistency.
---

# In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement

## Quick Facts
- arXiv ID: 2410.03124
- Source URL: https://arxiv.org/abs/2410.03124
- Authors: Zhen-Yu Zhang; Jiandong Zhang; Huaxiu Yao; Gang Niu; Masashi Sugiyama
- Reference count: 10
- Primary result: 1.8% accuracy improvement over direct LLM usage on benchmark datasets

## Executive Summary
This paper addresses unsupervised prompt learning for black-box LLMs by jointly optimizing the prompt and pseudo labels for unlabeled data. The key innovation is aligning the prompt-learning and prompt-using stages through in-context demonstrations from reliable pseudo-labeled data, ensuring consistency between training and inference. The method uses policy gradient optimization with entropy minimization for stability, achieving state-of-the-art performance on benchmark datasets while demonstrating general applicability across different black-box LLMs like GPT-4 and GPT-4o-mini.

## Method Summary
The algorithm iteratively optimizes prompts through a loop of pseudo-label generation, reliable data selection, demonstration-based training, and gradient-based prompt refinement. It uses variance-reduced policy gradient to optimize discrete prompt tokens while minimizing entropy for stability. The method selects reliable pseudo-labeled data as in-context demonstrations via K-nearest neighbors, then optimizes the prompt to generate consistent responses when these demonstrations are included, ensuring alignment between training and inference stages.

## Key Results
- Achieves 1.8% accuracy improvement over direct LLM usage on average across benchmark datasets
- Demonstrates general applicability across different black-box LLMs (GPT-4 and GPT-4o-mini)
- Shows reduced variance in results with entropy regularization compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning prompt-learning and prompt-using stages by including in-context demonstrations during training reduces inconsistency.
- Mechanism: The algorithm uses pseudo-labeled data as in-context demonstrations during training, mirroring how the model will be used during inference. This ensures the learned prompt is optimized for the same context it will encounter in practice.
- Core assumption: The in-context learning (ICL) capability of LLMs implicitly constructs a model that performs empirical risk minimization based on demonstrations, making consistency between training and inference crucial for performance.
- Evidence anchors:
  - [abstract] "The proposed learning objective ensures that the optimized prompt guides the LLM to generate consistent responses for a given input when pseudo-supervised data from the downstream task are used as demonstrations, enabling refinement over the entire pseudo-supervision."
  - [section] "These in-context demonstrations are important: they are previously only involved when the prompt is used for prediction, but they are not involved when the prompt is trained; thus, taking them into account during training makes the prompt-learning and prompt-using phases more consistent."
- Break condition: If the ICL mechanism changes fundamentally (e.g., models stop using demonstrations for implicit risk minimization), or if the demonstrations selected are poor quality and harm rather than help the training process.

### Mechanism 2
- Claim: Entropy minimization on the prompt token distribution creates more stable inference outputs.
- Mechanism: Adding an entropy term to the loss function encourages the categorical distributions over vocabulary tokens to be more peaked (less uniform), reducing randomness in prompt generation during inference.
- Core assumption: Reducing the entropy of the categorical distributions will lead to more deterministic token selection, which improves the stability of the generated prompt and consequently the model's predictions.
- Evidence anchors:
  - [abstract] "The prompt is optimized by translating gradient signals into textual critiques, which serve as feedback to iteratively refine the prompt and model responses."
  - [section] "We measure the Shannon entropy (Shannon, 2001) of the categorical distribution for each i-th token... Consequently, we define the loss function for the token probability p as follows: L(p) = Ez∼p [Lmain(z)] +α m∑ i=1 H(pi)"
- Break condition: If the entropy weight α is set too high, it may over-constrain the model and prevent it from finding optimal prompts; if too low, the stability benefit disappears.

### Mechanism 3
- Claim: Using policy gradient with variance reduction enables effective optimization of discrete prompts without access to model gradients.
- Mechanism: The algorithm uses a variance-reduced policy gradient estimator (VR-PGE) to estimate gradients for the categorical distributions over tokens, then updates these distributions using projected stochastic gradient descent.
- Core assumption: The black-box nature of LLMs prevents gradient-based optimization, but policy gradient methods can effectively navigate the discrete search space of prompts by treating token selection as a stochastic process.
- Evidence anchors:
  - [abstract] "The prompt is optimized by translating gradient signals into textual critiques, which serve as feedback to iteratively refine the prompt and model responses."
  - [section] "Since the downstream task is unlabeled, we propose to first identify reliable pseudo-labeled data to learn the prompt... We employ the variance-reduced policy gradient estimator (VR-PGE) (Williams, 1992; Zhou et al., 2021; Diao et al., 2022), a well-developed policy gradient algorithm in discrete optimization."
- Break condition: If the sample size I is too small, gradient estimates become too noisy; if too large, computational cost becomes prohibitive without sufficient benefit.

## Foundational Learning

- Concept: In-context learning (ICL) mechanism in LLMs
  - Why needed here: The entire approach relies on LLMs being able to learn from demonstrations provided in the prompt context. Understanding how ICL works is essential to grasp why including demonstrations during training matters.
  - Quick check question: How does an LLM use in-context demonstrations to make predictions on new queries?

- Concept: Policy gradient methods for discrete optimization
  - Why needed here: Since we can't access model gradients in the black-box setting, we need to understand how policy gradient methods can optimize discrete token sequences through sampling and reinforcement learning techniques.
  - Quick check question: What is the difference between standard gradient descent and policy gradient methods when optimizing discrete parameters?

- Concept: Entropy regularization in optimization
  - Why needed here: The entropy minimization term is crucial for ensuring stable prompt generation during inference, and understanding entropy in probability distributions is key to grasping this mechanism.
  - Quick check question: How does adding an entropy term to a loss function affect the learned probability distributions?

## Architecture Onboarding

- Component map:
  Unlabeled data → Confidence scoring → Reliable pseudo-label selection → Reliable pseudo-labels + prompt → In-context demonstration selection (K-nearest neighbors) → Prompt + demonstrations + unlabeled data → LLM prediction → Loss computation (main loss + entropy term) → VR-PGE gradient estimation → Categorical distribution update → Updated distributions → New prompt sampling for next iteration

- Critical path: The sequence from confidence scoring through demonstration selection to VR-PGE optimization forms the core loop that must function correctly for the algorithm to work.

- Design tradeoffs:
  - Confidence threshold γ: Higher values reduce noise but may limit training data; lower values increase data but risk overfitting
  - Number of demonstrations K: More demonstrations provide better context but increase computational cost and may dilute relevance
  - Entropy weight α: Balances stability vs flexibility in prompt generation
  - Sample size I in VR-PGE: Affects gradient estimate quality vs computational efficiency

- Failure signatures:
  - Poor performance despite high confidence scores → Issue with demonstration selection or confidence estimation
  - High variance in results → Entropy term α may be too low
  - Slow convergence → Sample size I may be too small or learning rate too conservative
  - Overfitting to initial pseudo-labels → Confidence threshold γ may be too high

- First 3 experiments:
  1. Run with only the Direct approach (no optimization) to establish baseline performance
  2. Test with varying confidence thresholds γ to find optimal balance between data quantity and quality
  3. Experiment with different numbers of in-context demonstrations K to determine optimal demonstration set size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the entropy minimization term (α) affect the stability of prompt generation during inference?
- Basis in paper: [explicit] The paper mentions using entropy minimization to generate a more stable prompt during inference.
- Why unresolved: The paper provides experimental results showing reduced variance with increasing α, but does not provide a theoretical analysis of how α affects prompt stability.
- What evidence would resolve it: A theoretical analysis of how α affects the distribution of generated prompts, or additional experiments varying α across different datasets and prompt lengths.

### Open Question 2
- Question: What is the optimal number of in-context demonstrations (K) for different datasets and tasks?
- Basis in paper: [explicit] The paper experiments with different values of K (3 and 5) and finds that K=5 performs best, but does not explore a wider range of values.
- Why unresolved: The paper only tests two values of K, so it is unclear if K=5 is truly optimal or if there are other values that perform better.
- What evidence would resolve it: Experiments testing a wider range of K values (e.g., 1, 3, 5, 7, 10) on multiple datasets to determine the optimal value.

### Open Question 3
- Question: How does the choice of loss function (hinge loss vs. cross-entropy) affect the performance of the algorithm?
- Basis in paper: [explicit] The paper mentions considering both hinge loss and cross-entropy, but only reports results using cross-entropy.
- Why unresolved: The paper does not provide a comparison of the two loss functions, so it is unclear which one is more effective for this task.
- What evidence would resolve it: Experiments comparing the performance of the algorithm using both hinge loss and cross-entropy on multiple datasets.

## Limitations

- Dependency on quality of initial pseudo-labels, as errors cascade through the learning process
- Sensitivity to hyperparameters (confidence threshold γ, number of demonstrations K, entropy weight α) without systematic sensitivity analysis
- Limited testing across diverse black-box LLM families and sizes, primarily focusing on GPT variants

## Confidence

**Claim Cluster 1: The core algorithm improves accuracy over baseline methods** - **High Confidence**
Supported by experimental results showing 1.8% improvement on average across multiple datasets. The methodology is sound and the results are statistically significant.

**Claim Cluster 2: In-context demonstration alignment is the key innovation** - **Medium Confidence**
While the paper provides theoretical justification and ablation studies, the relative contribution of this specific innovation versus other factors (like entropy regularization or the overall framework) could be more precisely quantified.

**Claim Cluster 3: The method generalizes across different black-box LLMs** - **Medium Confidence**
The experiments with GPT-4 and GPT-4o-mini are promising but limited. Testing on a broader range of model families and sizes would strengthen this claim.

## Next Checks

1. **Ablation study on demonstration selection**: Systematically vary the number of demonstrations K (1, 3, 5, 10) and measure the impact on performance to determine optimal demonstration set size and validate the importance of the in-context alignment mechanism.

2. **Sensitivity analysis on confidence threshold γ**: Test the algorithm with confidence thresholds ranging from 0.5 to 0.95 to identify the optimal balance between data quantity and quality, and assess how performance degrades when threshold is set too high or too low.

3. **Cross-model robustness test**: Evaluate the method on a wider variety of black-box LLMs including different model families (Claude, Llama, PaLM) and sizes to verify the claimed general applicability beyond just GPT variants.