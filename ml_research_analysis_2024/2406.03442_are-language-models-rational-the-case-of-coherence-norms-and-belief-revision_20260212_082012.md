---
ver: rpa2
title: Are language models rational? The case of coherence norms and belief revision
arxiv_id: '2406.03442'
source_url: https://arxiv.org/abs/2406.03442
tags:
- language
- belief
- norms
- states
- coherence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether coherence norms of rationality apply
  to language models, focusing on both logical and probabilistic coherence. It introduces
  the Minimal Assent Connection (MAC) to define credence functions for models based
  on next-token probabilities, addressing the challenge of assigning strength of belief
  to model outputs.
---

# Are language models rational? The case of coherence norms and belief revision

## Quick Facts
- arXiv ID: 2406.03442
- Source URL: https://arxiv.org/abs/2406.03442
- Reference count: 5
- Primary result: Language models track truth directionally but often produce incoherent beliefs after fine-tuning, suggesting coherence norms have limited effectiveness

## Executive Summary
This paper investigates whether coherence norms of rationality apply to language models, focusing on both logical and probabilistic coherence. The authors introduce the Minimal Assent Connection (MAC) to define credence functions for models based on next-token probabilities, addressing the challenge of assigning strength of belief to model outputs. Experiments with synthetically constructed pretraining data reveal that while models can track truth directionally, their credences often diverge from ideal Bayesian standards, indicating limited effectiveness of coherence norms. Belief revision through fine-tuning frequently results in incoherent beliefs, suggesting that synchronic and diachronic coherence norms may have minimal impact in practice.

## Method Summary
The study employs synthetic corpus generation to create controlled training environments with known ground truth credences. Language models are pretrained on these corpora, then fine-tuned with new contradictory information. Credences are extracted using the Minimal Assent Connection method, which defines assent/dissent ratios for propositions. The models' beliefs are evaluated against Bayesian gold standards using Brier scores and logical consistency metrics. This controlled approach allows systematic investigation of how coherence norms apply across different training regimes.

## Key Results
- Pretrained models can track truth directionally but their credences diverge from ideal Bayesian standards
- Fine-tuning to adopt new beliefs frequently results in incoherent beliefs, violating coherence norms
- The Minimal Assent Connection method effectively extracts credences from next-token probabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models acquire belief-like states through fine-tuning for truthfulness, not just from pretrained next-token probabilities
- Core assumption: Internal states can acquire new correctness standards via objective shifts in training
- Evidence anchors:
  - Models can track truth directionally but credences diverge from Bayesian ideals; fine-tuning often yields incoherent beliefs
  - Pretrained models lack truth as a standard; fine-tuned models can acquire it, making their states beliefs
  - Related works discuss belief revision and coherence, supporting the relevance of truthfulness in model behavior
- Break condition: If fine-tuning does not reliably shift internal states to truth-seeking, the belief-state claim collapses

### Mechanism 2
- Claim: Credence in language models can be defined via next-token probabilities using the Minimal Assent Connection (MAC)
- Core assumption: Strength of belief correlates with the model's tendency to assent versus dissent when queried
- Evidence anchors:
  - Detailed definition of MAC and credence calculation using assent/dissent ratios
  - Discussion of probabilism and coherence norms applying to models with truth-oriented credences
  - Works on belief revision and coherence in models support the theoretical grounding
- Break condition: If the model's verbal behavior is unreliable (e.g., due to lying or hedging), credence assignment fails

### Mechanism 3
- Claim: Coherence norms (logical and probabilistic) apply to fine-tuned models but not pretrained ones, due to differing correctness standards
- Core assumption: Aiming at truth entails coherence because the world is coherent
- Evidence anchors:
  - Pretrained models lack coherence requirements; fine-tuned models for truthfulness do
  - Links probabilism to truth-aiming models via accuracy arguments (Brier score)
  - Controlled experiments show pretrained models track truth directionally but diverge from Bayesian gold standards; fine-tuning often produces incoherence
- Break condition: If fine-tuned models do not actually aim for truth (e.g., RLHF targets other rewards), coherence norms may not apply

## Foundational Learning

- Concept: Representational states vs. information-carrying states
  - Why needed here: Distinguishes between models merely storing information (like a dictionary) and having beliefs with truth as a standard
  - Quick check question: Can a system have true representations without them being beliefs? Why or why not?

- Concept: Credence and probability assignments
  - Why needed here: Central to understanding how language models can be evaluated for coherence and rational belief revision
  - Quick check question: How does the MAC method convert next-token probabilities into credences for propositions?

- Concept: Coherence norms (logical vs. probabilistic)
  - Why needed here: Core to the paper's argument about which norms apply to language models and under what conditions
  - Quick check question: Why might a model trained on incoherent data still be subject to coherence norms after fine-tuning?

## Architecture Onboarding

- Component map:
  Synthetic corpus generation -> Pretraining -> Fine-tuning -> Evaluation module (credence extraction, coherence testing)

- Critical path:
  1. Generate controlled synthetic corpus with known ground truth credences
  2. Pretrain LM on corpus; extract credences via MAC
  3. Fine-tune LM on new contradictory facts
  4. Evaluate coherence before/after fine-tuning using ground truth

- Design tradeoffs:
  - MAC simplicity vs. potential noise from non-assent/dissent responses
  - Synthetic data control vs. lack of ecological validity
  - Controlled experiments vs. scalability to real-world models

- Failure signatures:
  - Credence extraction yields values near 0.5 for clear truths or falsehoods
  - Fine-tuned model shows logical contradictions without clear data-driven explanation
  - Coherence metrics degrade sharply after belief revision

- First 3 experiments:
  1. Pretrain on fully coherent synthetic corpus; test if credences match Bayesian gold standard
  2. Pretrain on partially incoherent corpus; test if credences track truth directionally but diverge in calibration
  3. Fine-tune on new contradictory facts; test if logical consistency is maintained or breaks down

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively do coherence norms influence belief revision in language models, and under what conditions?
- Basis in paper: The paper highlights that while coherence norms can theoretically apply to fine-tuned language models, empirical studies show that belief revision often results in incoherent beliefs, indicating limited effectiveness
- Why unresolved: The experiments conducted in synthetic settings reveal that models fail to maintain coherence after belief revision, but it's unclear how this scales to more complex, naturalistic scenarios or with different training methodologies
- What evidence would resolve it: Systematic empirical studies comparing belief revision outcomes across models with varying training data coherence and fine-tuning strategies, particularly in naturalistic settings

### Open Question 2
- Question: Can the Minimal Assent Connection (MAC) be universally applied to define credences for all language models, including those with context-sensitive or epistemically marked responses?
- Basis in paper: The paper discusses the potential limitations of MAC, particularly in handling context-sensitive assent/dissent sequences and epistemic markers, which may affect the accuracy of credence assignments
- Why unresolved: The paper suggests that context sensitivity and epistemic markers could complicate the application of MAC, but it does not provide a definitive solution or comprehensive list of assent/dissent sequences
- What evidence would resolve it: Empirical validation of MAC across diverse language models, testing its robustness in handling context-sensitive responses and epistemic markers

### Open Question 3
- Question: To what extent can multimodal models, grounded through perception, achieve coherence norms comparable to human rationality?
- Basis in paper: The paper suggests that multimodal models connected to perception might achieve truthfulness and coherence, but it remains unclear how effectively these models can integrate perceptual input to align with rational norms
- Why unresolved: The paper acknowledges the potential for perceptual grounding to enhance coherence but does not explore the practical implementation or effectiveness of such models in achieving rational coherence
- What evidence would resolve it: Comparative studies of multimodal models with perceptual grounding against traditional language models, focusing on their ability to maintain coherence and truthfulness in complex scenarios

## Limitations

- The synthetic corpus experiments may not generalize to real-world pretraining scenarios
- The mechanism by which fine-tuning causes incoherence is not fully explored
- The MAC method may be sensitive to prompt engineering and could break down for more complex propositions

## Confidence

- Pretrained models track truth directionally but diverge from Bayesian standards: High
- Fine-tuning often produces incoherent beliefs: Medium
- Coherence norms apply only to truth-aiming models: Medium

## Next Checks

1. **Ecological Validity Test**: Apply the MAC credence extraction method to a commercial LLM (e.g., GPT-4) on controlled factual questions and compare against the synthetic corpus results to assess generalizability.

2. **Fine-tuning Robustness**: Conduct systematic ablation studies varying fine-tuning data coherence, learning rates, and regularization to isolate factors causing belief revision incoherence.

3. **Real-World Coherence**: Evaluate a fine-tuned model on a benchmark of logical and probabilistic coherence problems (e.g., Wason selection task variants) to test whether synthetic corpus findings hold for naturalistic reasoning tasks.