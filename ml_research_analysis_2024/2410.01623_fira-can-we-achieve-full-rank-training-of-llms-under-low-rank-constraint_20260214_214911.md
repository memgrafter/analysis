---
ver: rpa2
title: 'Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?'
arxiv_id: '2410.01623'
source_url: https://arxiv.org/abs/2410.01623
tags:
- uni00000014
- uni00000013
- training
- uni00000003
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fira, a novel training framework that achieves
  full-rank training under low-rank constraints, enabling memory-efficient training
  of LLMs. The key insight is that the scaling factor of adaptive optimizers (e.g.,
  Adam) remains similar from low-rank to full-rank training at the matrix level.
---

# Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?

## Quick Facts
- arXiv ID: 2410.01623
- Source URL: https://arxiv.org/abs/2410.01623
- Reference count: 40
- Full-rank training under low-rank constraint achieving 61.1% memory reduction vs full-rank training on LLaMA 1B

## Executive Summary
Fira introduces a novel training framework that enables full-rank training of large language models under low-rank constraints. The key insight leverages the observation that adaptive optimizer scaling factors remain consistent between low-rank and full-rank training at the matrix level. By using scaling factors from low-rank training to correct gradients during full-rank training, Fira achieves memory efficiency comparable to LoRA and GaLore while maintaining or exceeding full-rank performance. The framework includes a norm-growth limiter to prevent training instability and loss spikes.

## Method Summary
Fira operates by first training a model with low-rank updates (similar to LoRA), then switching to full-rank training while using the scaling factors from the low-rank phase to correct gradients. The framework has two main components: norm-based scaling that leverages scaling factors from low-rank training to correct raw gradients in full-rank training, and a norm-growth limiter that smooths gradients by limiting relative increases in gradient norms to prevent loss spikes. The method is designed to be plug-and-play and compatible with existing memory-efficient training techniques like gradient checkpointing and quantization.

## Key Results
- Achieves 61.1% reduction in optimizer states memory compared to full-rank training on LLaMA 1B
- Outperforms both LoRA and GaLore on pre-training and fine-tuning tasks while maintaining full-rank performance levels
- Uses 8x smaller rank than GaLore on LLaMA 7B while achieving superior performance
- Successfully prevents loss spikes through norm-growth limiter, maintaining training stability

## Why This Works (Mechanism)
Fira exploits the mathematical property that adaptive optimizers (like Adam) maintain similar scaling factors when transitioning from low-rank to full-rank training at the matrix level. This allows the framework to use low-rank training as a warm-up phase, extracting scaling information that can correct gradients during full-rank training. The norm-growth limiter addresses the inherent instability that can arise from this correction mechanism by constraining gradient norm increases, preventing the catastrophic loss spikes that would otherwise occur.

## Foundational Learning
- **Low-rank matrix factorization**: Used to approximate weight updates with reduced parameters; needed for memory efficiency in large models; check by verifying rank-r decomposition of weight matrices
- **Adaptive optimization algorithms**: Adam maintains per-parameter statistics that inform scaling; needed for consistent scaling factors across training phases; check by comparing Adam's beta moments between phases
- **Gradient correction mechanisms**: Raw gradients are modified using scaling factors; needed to transition from low-rank to full-rank training; check by comparing corrected vs uncorrected gradients
- **Norm-based regularization**: Gradient norm limiting prevents instability; needed to maintain training stability during full-rank phase; check by monitoring gradient norm growth ratios

## Architecture Onboarding
- **Component map**: Low-rank training -> Scaling factor extraction -> Full-rank training with gradient correction
- **Critical path**: 1) Low-rank warm-up phase 2) Scaling factor calculation 3) Gradient correction during full-rank training 4) Norm-growth limiting
- **Design tradeoffs**: Lower rank reduces memory but may compromise performance; norm-growth limiter prevents instability but may slow convergence; timing of phase transition affects final quality
- **Failure signatures**: Loss spikes indicate incorrect norm-growth limiter implementation; performance degradation suggests poor scaling factor extraction; memory inefficiency indicates incorrect rank selection
- **First experiments**: 1) Verify scaling factor consistency between low-rank and full-rank phases 2) Test norm-growth limiter effectiveness on synthetic gradient data 3) Compare convergence curves of Fira vs pure low-rank training

## Open Questions the Paper Calls Out
**Open Question 1**: How does Fira's performance scale with different learning rates compared to full-rank training, and what is the optimal learning rate range for Fira?
The paper mentions Fira introduces higher randomness that helps escape local optima and that Adam is not sensitive to learning rate magnitude, but doesn't analyze performance across different learning rates or identify an optimal range.

**Open Question 2**: What is the impact of the norm-growth limiter's threshold (γ) on Fira's performance, and is there an optimal value for γ?
The paper uses γ=1.01 across all experiments with satisfactory results but doesn't explore how different values affect performance or determine if an optimal value exists.

**Open Question 3**: How does Fira's memory efficiency compare to other memory-efficient training methods when combined with system-based techniques like gradient checkpointing or quantization?
While the paper states Fira is complementary to system-based techniques, it doesn't provide experimental results on combined memory efficiency with gradient checkpointing or quantization.

## Limitations
- Evaluation limited primarily to LLaMA-family architectures without testing on OPT, GPT-Neo, or other model families
- Performance claims untested on specialized domains like code generation, mathematical reasoning, or multilingual tasks
- Memory efficiency claims based on BF16 precision may not translate directly to FP16 or INT8 formats

## Confidence
- **High**: Core technical contribution about scaling factor consistency is empirically validated
- **Medium**: Comparative performance claims show strong results but are benchmarked primarily against LoRA and GaLore
- **Low**: Generalizability across different model families and tasks beyond LLaMA and commonsense reasoning

## Next Checks
1. Test Fira on diverse model architectures (OPT, GPT-Neo) and specialized tasks (code generation, mathematical reasoning) to assess generalizability
2. Conduct ablation studies to quantify individual contributions of norm-based scaling and norm-growth limiter components
3. Evaluate Fira's memory efficiency and performance under different numerical precisions (FP16, INT8) and compare with other memory-efficient training techniques like ZeRO and DeepSpeed