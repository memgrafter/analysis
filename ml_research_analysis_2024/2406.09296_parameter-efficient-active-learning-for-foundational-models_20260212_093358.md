---
ver: rpa2
title: Parameter-Efficient Active Learning for Foundational models
arxiv_id: '2406.09296'
source_url: https://arxiv.org/abs/2406.09296
tags:
- learning
- active
- foundation
- peal
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of data-efficient transfer learning
  with foundational vision models under budget-constrained active learning (AL) scenarios.
  The core idea is to combine parameter-efficient fine-tuning (via LoRA) with AL sample
  selection strategies, enabling both uncertainty- and diversity-based AL methods
  that were previously restricted when using frozen backbone features.
---

# Parameter-Efficient Active Learning for Foundational models

## Quick Facts
- arXiv ID: 2406.09296
- Source URL: https://arxiv.org/abs/2406.09296
- Reference count: 26
- Primary result: PEAL achieves 90% accuracy on Histology with only 200 labeled samples vs 400 for random sampling

## Executive Summary
This paper addresses the challenge of data-efficient transfer learning with foundational vision models under budget-constrained active learning scenarios. The core contribution is PEAL (Parameter-Efficient Active Learning), which combines parameter-efficient fine-tuning via LoRA with active learning sample selection strategies. By updating feature embeddings through LoRA adapters, PEAL enables both uncertainty- and diversity-based active learning methods that were previously restricted when using frozen backbone features. The framework was evaluated across three challenging image classification datasets using DINOv2 as the foundation model.

## Method Summary
PEAL integrates LoRA parameter-efficient fine-tuning with active learning by injecting low-rank adapters into QKV attention layers of the frozen DINOv2 backbone. The framework supports both entropy-based uncertainty sampling and feature-distance-based diversity sampling. During each active learning cycle, PEAL selects samples based on budget constraints and strategy, annotates them, then updates the model using LoRA while maintaining the frozen backbone. The method uses class-balanced sampling with 50 training epochs, Adam optimizer (lr=1e-3), and Faiss for efficient distance computation.

## Key Results
- Achieved 90% accuracy on Histology dataset with only 200 labeled samples vs 400 for random sampling
- Reached 93% accuracy on EuroSAT with 200 samples
- PEAL (Featdist) and PEAL (Entropy) both achieved 90% accuracy with 250 and 200 samples respectively, compared to 400 for PEAL (Random)
- Improved ratio of unknown to known sample selection in early AL cycles, enhancing annotation efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEAL improves sample selection by updating feature embeddings through low-rank adaptation, enabling diversity-based AL strategies to work effectively.
- Mechanism: Linear probing freezes backbone features, so diversity metrics based on feature distances remain static and ineffective. PEAL injects LoRA adapters into QKV attention layers, updating feature representations as the model learns from newly labeled samples, allowing feature distance calculations to reflect the current model state.
- Core assumption: Updated feature embeddings after each AL cycle are sufficiently different to improve diversity-based sample selection.
- Evidence anchors:
  - [abstract]: "linear probing with frozen features limits the application of feature-embedding and diversity-based sample selection strategies"
  - [section]: "Since finetuning the entire transformer backbone is computationally intensive, we introduce parameter-efficient active learning to enable feature-embedding based AL with foundation models."
  - [corpus]: Weak. The corpus contains general PEFT papers but none directly validate the mechanism of enabling diversity-based AL through feature update.

### Mechanism 2
- Claim: PEAL enables uncertainty-based sampling to select more informative "unknown" samples early in AL cycles.
- Mechanism: Entropy-based uncertainty sampling depends on the model's confidence scores. Frozen features in linear probing lead to poorly calibrated predictions. PEAL allows the model to adjust its decision boundaries via LoRA, producing better-calibrated entropy scores that reflect actual model uncertainty.
- Core assumption: LoRA tuning sufficiently adapts the model to improve entropy score quality for uncertainty sampling.
- Evidence anchors:
  - [abstract]: "We empirically show the effectiveness of PEAL for both uncertainty-based and diversity-based sample selection methods"
  - [section]: "As sampling metrics get refined due to better feature representations and better calibrated entropy"
  - [corpus]: Weak. No direct evidence in corpus that PEFT improves uncertainty calibration in AL.

### Mechanism 3
- Claim: PEAL reduces the number of labeled samples needed to reach target accuracy by enabling better sample selection.
- Mechanism: By improving both uncertainty and diversity sampling through updated feature representations and calibrated predictions, PEAL selects samples that provide more information per annotation, reducing the total number of samples needed to achieve the same or better accuracy.
- Core assumption: Better sample selection translates directly to faster convergence and higher accuracy with fewer samples.
- Evidence anchors:
  - [abstract]: "achieving 90% accuracy on Histology with only 200 labeled samples (vs. 400 for random sampling)"
  - [section]: "PEAL (Featdist) and PEAL (Entropy) achieve 90% accuracy with significantly fewer samplesâ€”250 and 200 respectively, compared to 400 for PEAL (Random)."
  - [corpus]: Weak. The corpus doesn't contain studies comparing sample efficiency between PEFT-enabled and frozen-feature AL.

## Foundational Learning

- Concept: Vision Transformer foundation models and their few-shot transfer learning capabilities
  - Why needed here: The paper relies on DINOv2's ability to generalize from minimal labeled data, which is central to the active learning setup.
  - Quick check question: What architectural feature of ViTs enables few-shot learning compared to CNNs?

- Concept: Parameter-efficient fine-tuning methods (specifically LoRA)
  - Why needed here: PEAL uses LoRA to update feature representations without full fine-tuning, enabling diversity-based AL strategies.
  - Quick check question: How does LoRA decompose weight updates differently from full fine-tuning?

- Concept: Active learning sampling strategies (uncertainty and diversity)
  - Why needed here: The paper evaluates both entropy-based uncertainty sampling and feature-distance-based diversity sampling within the PEAL framework.
  - Quick check question: What's the key difference between uncertainty-based and diversity-based sample selection?

## Architecture Onboarding

- Component map:
  DINOv2 backbone (frozen except LoRA adapters) -> LoRA adapters in QKV attention layers -> Linear classifier head -> Faiss distance computation -> AL cycle manager

- Critical path:
  1. Extract features from unlabeled samples
  2. Compute entropy scores (uncertainty) or distances (diversity)
  3. Select samples based on budget and strategy
  4. Annotate selected samples
  5. Update model with LoRA adapters and classifier
  6. Evaluate on test set
  7. Repeat for next AL cycle

- Design tradeoffs:
  - LoRA rank (16) vs. performance: Higher rank may improve results but increases parameters
  - Class-balanced vs. class-agnostic sampling: Balanced ensures representation but may reduce diversity
  - Faiss approximation vs. exact distance: Faster computation but potential accuracy trade-off

- Failure signatures:
  - No improvement over random sampling: Likely indicates LoRA isn't updating features sufficiently
  - Performance degradation over cycles: Could indicate overfitting to small labeled sets or poor sample selection
  - Extremely long training times: May indicate inefficient LoRA implementation or too many epochs

- First 3 experiments:
  1. Run PEAL with entropy sampling on EuroSAT to verify uncertainty sampling works with updated features
  2. Run PEAL with feature distance sampling on Histology to confirm diversity sampling benefits from feature updates
  3. Compare PEAL vs. linear probing on the same dataset/strategy to measure the impact of feature updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LoRA rank and alpha configurations impact active learning performance across diverse datasets?
- Basis in paper: [explicit] The paper mentions using rank=16 and alpha=16 for LoRA adapters but notes this is a single configuration without exploring alternatives.
- Why unresolved: The paper does not investigate how varying LoRA hyperparameters affects sample selection efficiency or model performance in active learning scenarios.
- What evidence would resolve it: Systematic experiments comparing different LoRA configurations (varying ranks and alphas) across multiple datasets and active learning strategies would clarify optimal hyperparameter choices.

### Open Question 2
- Question: Would combining uncertainty and diversity-based sampling strategies create a more effective hybrid approach than using either method alone?
- Basis in paper: [explicit] The paper notes that "a hybrid strategy could enhance future results" but does not explore this combination experimentally.
- Why unresolved: The paper separately evaluates entropy (uncertainty) and Featdist (diversity) methods but doesn't investigate their potential synergy or whether hybrid approaches could outperform individual strategies.
- What evidence would resolve it: Experiments comparing hybrid sampling strategies (e.g., combining entropy and Featdist scores) against individual methods across multiple datasets would determine if such combinations provide performance benefits.

### Open Question 3
- Question: How would the PEAL framework perform on semantic segmentation and object detection tasks that require extensive annotations?
- Basis in paper: [explicit] The paper concludes by noting that "Further exploration is needed to understand the impacts on tasks like semantic segmentation and object detection, which require extensive annotations."
- Why unresolved: The current evaluation is limited to image classification datasets, leaving the applicability and effectiveness of PEAL for more complex computer vision tasks unexplored.
- What evidence would resolve it: Applying PEAL to semantic segmentation and object detection benchmarks with appropriate adaptation of active learning strategies for these tasks would demonstrate its broader applicability.

## Limitations
- Limited empirical validation to three image classification datasets using a single foundation model (DINOv2)
- Incomplete computational cost analysis - doesn't quantify full training time or memory requirements compared to linear probing
- Doesn't establish whether LoRA updates are sufficient to meaningfully change feature representations across all AL cycles

## Confidence
- High confidence: PEAL improves sample selection efficiency over random sampling, supported by consistent accuracy improvements across all three datasets
- Medium confidence: LoRA updates enable diversity-based AL, as the paper demonstrates improved performance but lacks direct evidence of feature distance changes between cycles
- Low confidence: Generalizability claims, given the narrow scope of evaluation (three datasets, one foundation model, classification only)

## Next Checks
1. Test PEAL with alternative foundation models (e.g., CLIP, MAE) to verify improvements aren't specific to DINOv2's architecture
2. Measure and visualize feature distance distributions before and after LoRA updates across AL cycles to quantify impact on diversity sampling
3. Evaluate PEAL's performance across different annotation budget sizes (smaller than 200 samples) to determine minimum effective sample size