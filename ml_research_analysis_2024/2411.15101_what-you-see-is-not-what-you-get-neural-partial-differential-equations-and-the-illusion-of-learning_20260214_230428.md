---
ver: rpa2
title: 'What You See is Not What You Get: Neural Partial Differential Equations and
  The Illusion of Learning'
arxiv_id: '2411.15101'
source_url: https://arxiv.org/abs/2411.15101
tags:
- expt
- error
- training
- numerical
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper rigorously investigates whether NeuralPDEs trained on
  PDE simulations are as physically interpretable as assumed, highlighting that these
  models learn artifacts from numerical discretization rather than true physics. Using
  (1+1)-dimensional viscous Burgers and geophysical Kortweg-de Vries equations, the
  authors show that NeuralPDEs systematically learn truncation errors from Taylor
  Series expansions of spatial derivatives in the training data.
---

# What You See is Not What You Get: Neural Partial Differential Equations and The Illusion of Learning

## Quick Facts
- **arXiv ID**: 2411.15101
- **Source URL**: https://arxiv.org/abs/2411.15101
- **Reference count**: 40
- **Key outcome**: NeuralPDEs trained on PDE simulations systematically learn truncation errors from numerical discretization rather than true physics, leading to artifacts and limited generalization capability.

## Executive Summary
This paper rigorously investigates whether NeuralPDEs trained on PDE simulations are as physically interpretable as assumed. Through experiments with viscous Burgers and geophysical Kortweg-de Vries equations, the authors demonstrate that NeuralPDEs learn artifacts from numerical discretization errors in training data rather than true physics. The work shows that model accuracy depends critically on the alignment of truncation errors between the training dataset and the NeuralPDE, raising concerns about the integrity of foundation models trained on simulation data from various numerical solvers.

## Method Summary
The authors generate training data using PDE simulations with specific numerical discretization schemes, then train NeuralPDEs to learn the underlying dynamics. They use fully connected feed-forward networks with 3 hidden layers of 20 neurons each and tanh activation functions, training with either autoregressive methods (Burgers equation) or one-shot prediction (gKdV). The trained models are evaluated on unseen initial conditions and different numerical schemes, with eigenvalue analysis of model Jacobians used to predict potential inaccuracies. The study systematically varies numerical schemes and initial conditions to isolate the effects of truncation errors.

## Key Results
- NeuralPDEs systematically learn truncation errors from Taylor Series expansions of spatial derivatives in training data
- Generalization capability depends on fortuitous alignment between truncation errors in training data and NeuralPDE discretization
- Eigenanalysis of model Jacobians can predict a priori whether a NeuralPDE will be inaccurate for out-of-distribution testing
- Model accuracy critically depends on the alignment of truncation errors between training dataset and NeuralPDE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NeuralPDEs learn artifacts from numerical discretization errors in training data rather than true physics.
- Mechanism: Training data from PDE simulations contains truncation errors from Taylor Series expansions of spatial derivatives. NeuralPDEs learn these errors as part of the solution mapping, causing bias in predictions.
- Core assumption: The "ground truth" training data is assumed to be exact physics, but it is actually discrete numerical approximations with inherent truncation errors.
- Evidence anchors: The paper shows that when a PDE and its solution are discretized, the training data carries projection error from truncating the infinite Taylor series.

### Mechanism 2
- Claim: NeuralPDE generalization capability depends on fortuitous alignment between truncation errors in training data and the NeuralPDE discretization.
- Mechanism: When the same numerical scheme is used in both training data generation and NeuralPDE, truncation errors align, creating apparent generalization. When schemes differ, error grows rapidly.
- Core assumption: Generalization appears successful only when truncation error dynamics match between training and inference.
- Evidence anchors: Experiments show that when p = k in the error equation, numerical error is reduced due to matching dynamics of truncated terms.

### Mechanism 3
- Claim: Eigenanalysis of model Jacobians can predict a priori whether a NeuralPDE will be inaccurate for out-of-distribution testing.
- Mechanism: The magnitude of eigenvalues in the Jacobian indicates error growth rate. Large eigenvalues outside the unit circle predict instability and inaccuracy.
- Core assumption: Linear stability analysis of the Jacobian provides reliable indicators of model behavior.
- Evidence anchors: The paper demonstrates that if the eigenvalue is within the unit circle, the dynamical system would yield a stable solution.

## Foundational Learning

- **Concept**: Taylor Series truncation error in numerical differentiation
  - Why needed here: The entire paper hinges on understanding that numerical derivatives in PDE simulations are approximations with truncation errors
  - Quick check question: What is the leading term in the truncation error for a second-order central difference approximation?

- **Concept**: Well-posedness and stability in initial value problems
  - Why needed here: The paper uses energy norms and stability analysis to understand when NeuralPDEs maintain or lose predictive capability
  - Quick check question: What condition must be satisfied for an energy norm to guarantee well-posedness of an initial value problem?

- **Concept**: Automatic differentiation and differentiable programming
  - Why needed here: NeuralPDEs use differentiable programming to embed neural networks within PDEs, making the analysis of learned artifacts possible
  - Quick check question: How does automatic differentiation differ from numerical differentiation in terms of truncation error?

## Architecture Onboarding

- **Component map**: Ground truth PDE solver with numerical discretization -> Neural network parameterized unknown terms -> Differentiable programming framework -> Training pipeline -> Eigenvalue analysis module

- **Critical path**:
  1. Generate training data with specific numerical discretization and initial conditions
  2. Train NeuralPDE to minimize discrepancy between predictions and training data
  3. Analyze model Jacobian eigenvalues to predict stability and generalization capability
  4. Test model on varying initial conditions and numerical schemes

- **Design tradeoffs**:
  - Using same numerical scheme for training and inference reduces truncation error mismatch but may hide fundamental physics learning issues
  - One-shot prediction vs autoregressive training affects stability but both suffer from truncation error artifacts
  - Higher-order numerical schemes reduce truncation error magnitude but don't eliminate the fundamental learning of artifacts

- **Failure signatures**:
  - Rapid increase in prediction error when initial conditions differ from training
  - Eigenvalues of Jacobian moving outside unit circle as rollout time increases
  - Unphysical oscillations or instabilities appearing in predictions
  - Model performs well only when same numerical scheme is used for training and inference

- **First 3 experiments**:
  1. Train Burgers equation NeuralPDE with 6th order central diffusion, test with same and different initial conditions
  2. Train gKdV equation NeuralPDE with 2nd order central dispersive term, compare 2nd vs 6th order dispersive schemes
  3. Perform eigenvalue analysis on trained models for both training and out-of-distribution initial conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do truncation errors from numerical discretization interact with non-uniform grid resolutions in realistic PDE simulations, and can we mathematically quantify their net effect on NeuralPDE accuracy?
- Basis in paper: The paper notes that realistic simulations often use non-uniform grids with varying biases, and that the interplay between numerical diffusion and truncation errors across grid sizes can introduce artifacts.
- Why unresolved: The paper simplifies by assuming constant grid size to isolate truncation errors, but acknowledges that non-uniform grids are the norm in practical applications.
- What evidence would resolve it: A rigorous mathematical framework that quantifies the net effect of truncation errors across varying grid resolutions, validated through experiments on realistic, non-uniform grid simulations.

### Open Question 2
- Question: Can Richardson extrapolation or similar techniques be adapted to estimate the form of the true solution and truncation error in NeuralPDE models trained on numerical simulation data?
- Basis in paper: The paper mentions Richardson extrapolation as an early attempt to address the gap between numerical approximations and true solutions.
- Why unresolved: The paper does not explore how Richardson extrapolation could be practically applied to NeuralPDE training.
- What evidence would resolve it: Empirical validation showing how Richardson extrapolation improves NeuralPDE accuracy by estimating and correcting for truncation errors in the training data.

### Open Question 3
- Question: Do large PDE datasets used for foundation models contain solver-specific truncation error biases that act as unintentional "adversarial attacks," and how can we detect and mitigate these biases?
- Basis in paper: The paper raises concerns that truncation errors unique to each solver generating training data may act as adversarial attacks.
- Why unresolved: The paper highlights the problem but does not provide methods to detect or quantify solver-specific biases in large, heterogeneous datasets.
- What evidence would resolve it: A systematic study identifying solver-specific truncation error patterns in large PDE datasets and demonstrating their impact on foundation model performance.

## Limitations

- The analysis assumes constant grid resolution, but realistic simulations use non-uniform grids with varying biases
- The diagnostic power of Jacobian eigenvalue analysis requires further validation across diverse PDE types and network architectures
- Claims about implications for foundation models are somewhat speculative, as the paper focuses on relatively simple PDE systems

## Confidence

- **High confidence**: The core mechanism that NeuralPDEs learn truncation errors from training data discretization is well-supported by the mathematical analysis and numerical experiments
- **Medium confidence**: The eigenvalue analysis as a diagnostic tool shows promise but requires more extensive validation across different architectures
- **Low confidence**: Claims about the implications for "foundation models" trained on simulation data are somewhat speculative

## Next Checks

1. **Grid refinement study**: Systematically vary the numerical grid resolution in training data generation and observe how this affects the magnitude of learned truncation errors and generalization capability

2. **Higher-dimensional extension**: Apply the same analysis framework to (2+1)-dimensional PDEs, such as the 2D viscous Burgers equation or the 2D Navier-Stokes equations

3. **Alternative architectures**: Repeat the experiments using different neural network architectures (e.g., convolutional networks, physics-informed neural networks) to determine whether the artifact learning phenomenon is architecture-dependent or universal