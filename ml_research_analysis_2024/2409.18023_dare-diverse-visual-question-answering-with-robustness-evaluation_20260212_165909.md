---
ver: rpa2
title: 'DARE: Diverse Visual Question Answering with Robustness Evaluation'
arxiv_id: '2409.18023'
source_url: https://arxiv.org/abs/2409.18023
tags:
- answer
- correct
- question
- questions
- options
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DARE evaluates vision-language models across five challenging reasoning
  categories (conditional counting, ordering, visual commonsense, culture, trick)
  with robustness tests varying prompts, answer options, output formats, and number
  of correct answers. State-of-the-art models struggle especially with conditional
  counting and spatial reasoning, and show up to 34% performance drops under robustness
  variations.
---

# DARE: Diverse Visual Question Answering with Robustness Evaluation

## Quick Facts
- arXiv ID: 2409.18023
- Source URL: https://arxiv.org/abs/2409.18023
- Authors: Hannah Sterz; Jonas Pfeiffer; Ivan Vulić
- Reference count: 37
- Primary result: State-of-the-art VLMs show up to 34% performance drops under robustness variations across five reasoning categories

## Executive Summary
DARE evaluates vision-language models across five challenging reasoning categories (conditional counting, ordering, visual commonsense, culture, trick) with robustness tests varying prompts, answer options, output formats, and number of correct answers. State-of-the-art models struggle especially with conditional counting and spatial reasoning, and show up to 34% performance drops under robustness variations. Open-weight models like LLaVA 1.6 and Idefics2 lag behind closed models (GPT-4, Gemini) in flexibility and robustness, often failing with alternative output formats. Even the newest checkpoints show substantial gains but still fall far short of human performance. DARE reveals critical brittleness in current VLMs, highlighting the need for robustness-aware evaluation and development.

## Method Summary
DARE dataset contains 963 single-correct questions, 710 single-correct plus variations, and 750 multi-correct questions across five reasoning categories. The evaluation uses existing VLMs (GPT-4, Gemini Flash, LLaVA 1.6, Idefics2) with three different prompts and three output formats (GEN, LOG, JSON). The study measures accuracy for single-correct setup, F1 scores for multi-correct setup, and analyzes robustness across prompt variations, output formats, and answer option variations.

## Key Results
- VLMs show up to 34% performance drops when answer options are presented in different syntactic forms
- LLaVA 1.6 and Idefics2 demonstrate strong bias toward single-answer predictions in multi-correct setups
- Performance varies significantly across output formats, with open-weight models failing entirely on JSON format
- Conditional counting and ordering categories show the largest performance gaps under robustness variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Varying the set of answer options exposes brittleness because VLMs rely on answer token frequency patterns learned during pretraining.
- Mechanism: When correct answers are presented in multiple syntactic forms, VLMs fail to generalize across surface variations due to shallow token-level biases.
- Core assumption: The correct answer content is semantically identical across option variations, so failure indicates model brittleness rather than ambiguity.
- Evidence anchors:
  - [abstract] "The worst case performance across the subsets of options is up to 34% below the performance in the standard case."
  - [section] "The gap is very substantial and this holds even for the conditional counting category, where the correct answer is always the same number, which only gets expressed with different templates."
  - [corpus] "Weak" - corpus contains no direct evidence; only mentions related robustness studies.
- Break condition: If the model is retrained with balanced token frequency across all correct answer variants, performance differences would vanish.

### Mechanism 2
- Claim: Multi-correct setups reveal hidden model biases toward single-answer predictions due to pretraining data distribution.
- Mechanism: VLMs trained on single-correct datasets develop strong priors that penalize multiple or zero correct answers, even when the prompt explicitly allows them.
- Core assumption: Pretraining corpora predominantly contain single-correct VQA examples, so the model learns to favor one answer.
- Evidence anchors:
  - [abstract] "Especially LLaV A 1.6 and Idefics2 show a strong bias towards marking exactly one answer as correct."
  - [section] "We hypothesise that this is due to their pretraining bias, where they are skewed towards single-correct setups."
  - [corpus] "Weak" - no corpus evidence; the claim is derived from experimental results.
- Break condition: If the model is fine-tuned on balanced multi-correct datasets, the bias would reduce.

### Mechanism 3
- Claim: Output format sensitivity indicates the model's inability to follow instructions beyond its pretraining distribution.
- Mechanism: VLMs trained to generate answers in a specific format (e.g., single letter) fail when asked to produce structured formats like JSON or CSV because the instruction-following behavior is not robust.
- Core assumption: Instruction tuning for output format was limited to a narrow set of formats during training.
- Evidence anchors:
  - [abstract] "Gemini benefits from predicting the answers in JSON format, while LLaV A and Idefics2 do not perform well when prompted to provide JSON-formatted answers."
  - [section] "LLaV A 1.6 and Idefics2 have been instruction-tuned to provide the letter indicating the correct answer option."
  - [corpus] "Weak" - corpus contains no direct evidence; claim based on experimental observations.
- Break condition: If the model is retrained with diverse output format instruction examples, the format sensitivity would diminish.

## Foundational Learning

- Concept: Token-level bias in VLMs
  - Why needed here: Understanding why VLMs fail on surface variations of the same semantic answer.
  - Quick check question: If the correct answer is "3" and it appears as "three", "3", and "there are 3", should a robust VLM score the same on all?

- Concept: Pretraining distribution effects
  - Why needed here: Explains why models perform poorly on multi-correct setups and rare output formats.
  - Quick check question: If a model sees only single-correct examples during pretraining, what prior does it learn about the number of correct answers?

- Concept: Instruction following in VLMs
  - Why needed here: Critical for understanding why output format variations impact performance.
  - Quick check question: If a model is fine-tuned to output "A" for correct answers, will it automatically know how to output {"answer": "A"}?

## Architecture Onboarding

- Component map:
  Vision encoder (CLIP/ViT) → Projection layer → LLM backbone (Vicuna/LLaVA) → Output decoder

- Critical path:
  Image → Vision encoder → Visual embeddings → LLM input → Text generation → Output parsing
  Performance bottlenecks occur at output parsing and instruction following stages

- Design tradeoffs:
  - Fixed vision encoder vs fine-tuned: trade-off between speed and task adaptation
  - Single-format vs multi-format instruction tuning: trade-off between specialization and robustness
  - Greedy decoding vs sampling: trade-off between determinism and diversity

- Failure signatures:
  - Output format errors → Incorrect answer extraction
  - Single-answer bias → Poor multi-correct performance
  - Surface form sensitivity → High variance across answer option variations

- First 3 experiments:
  1. Test model on same question with answer options in different syntactic forms to measure surface form sensitivity
  2. Evaluate model on multi-correct setup with explicit "All of the above" option to test bias
  3. Prompt model with different output format instructions (JSON, CSV, WORD) to measure instruction-following robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can VLMs be systematically improved to handle robustness variations in output formats without catastrophic performance drops?
- Basis in paper: Explicit - The paper shows that even top models like GPT-4 and Gemini Flash experience significant performance drops (up to 34%) when output formats are varied (e.g., JSON vs. plain text), and open-weight models like LLaVA and Idefics2 fail entirely in some formats.
- Why unresolved: The paper demonstrates the problem but does not explore architectural or training modifications that could make models more format-flexible. It's unclear whether robustness to output variations can be improved through better instruction-tuning or if it requires fundamental changes to model design.
- What evidence would resolve it: Systematic experiments varying instruction-tuning datasets to include diverse output format examples, ablation studies on model architecture components affecting format handling, or transfer learning approaches where models are fine-tuned specifically on format variation tasks.

### Open Question 2
- Question: Is there a fundamental trade-off between achieving high performance on challenging reasoning tasks and maintaining robustness to input variations?
- Basis in paper: Explicit - The paper finds that models achieving peak performance on categories like conditional counting and ordering show the largest drops under robustness variations, suggesting a potential inverse relationship between task complexity and robustness.
- Why unresolved: The paper doesn't explore whether this trade-off is inherent or if it's possible to design models that are both highly capable at reasoning tasks and robust to variations. It's unclear if current model architectures are fundamentally limited in this way.
- What evidence would resolve it: Controlled experiments comparing models trained with and without robustness-focused objectives, analysis of whether certain architectural choices (like different attention mechanisms or reasoning modules) correlate with both high performance and robustness, or studies on whether larger models inherently become more robust.

### Open Question 3
- Question: Can robustness to multi-correct answer scenarios be improved without sacrificing performance on single-correct tasks?
- Basis in paper: Explicit - The paper shows that open-weight models like LLaVA 1.6 and Idefics2 struggle significantly with multi-correct setups (where 0-4 answers can be correct), often defaulting to single-answer predictions, while closed models show better but still imperfect performance.
- Why unresolved: The paper identifies the problem but doesn't investigate whether models can be trained to handle both single-correct and multi-correct scenarios effectively, or whether this requires separate model versions for different task types.
- What evidence would resolve it: Experiments with hybrid training approaches that expose models to both single-correct and multi-correct data during pretraining, analysis of whether prompt engineering can bridge the performance gap, or studies on whether task-specific fine-tuning can improve multi-correct performance without degrading single-correct results.

## Limitations

- The evaluation framework relies on automated post-processing of model outputs, which introduces potential measurement errors through regex-based answer extraction.
- The robustness analysis focuses on surface-level variations rather than deeper semantic variations, limiting understanding of fundamental reasoning capabilities.
- Comparison across different model types may be confounded by differences in evaluation protocols, API access constraints, and version timing with approximate dates rather than exact checkpoints.

## Confidence

**High Confidence**: The finding that VLMs show significant performance drops under robustness variations (up to 34%) is well-supported by systematic experimentation across multiple model types and variations.

**Medium Confidence**: The claim that pretraining bias causes single-answer preference in multi-correct setups is plausible but based on correlational evidence rather than direct causal analysis.

**Low Confidence**: The assertion that surface form sensitivity directly indicates shallow token-level biases rather than other factors (like instruction-following limitations or output parsing issues) lacks direct evidence.

## Next Checks

1. Run a manual verification on a random sample of model outputs to measure the accuracy of the regex-based answer extraction, quantifying potential measurement error.

2. Create a subset of questions where the correct answer is semantically identical but expressed through completely different visual concepts or linguistic descriptions to test deeper semantic variation failures.

3. Fine-tune a VLM on a balanced dataset containing both single-correct and multi-correct examples with explicit instruction about multiple correct answers to test whether pretraining bias is the primary driver of observed failures.