---
ver: rpa2
title: 'Bias Amplification in Language Model Evolution: An Iterated Learning Perspective'
arxiv_id: '2404.04286'
source_url: https://arxiv.org/abs/2404.04286
tags:
- learning
- bias
- iterated
- which
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the evolution of large language models (LLMs)
  through the lens of iterated learning, a framework from cognitive science that studies
  how biases amplify across generations. By showing that LLM sampling and learning
  behaviors can be approximated by Bayesian inference, the authors establish that
  iterated learning naturally amplifies existing biases in model priors.
---

# Bias Amplification in Language Model Evolution: An Iterated Learning Perspective

## Quick Facts
- arXiv ID: 2404.04286
- Source URL: https://arxiv.org/abs/2404.04286
- Reference count: 40
- Key outcome: This paper analyzes the evolution of large language models (LLMs) through the lens of iterated learning, a framework from cognitive science that studies how biases amplify across generations.

## Executive Summary
This paper presents a theoretical framework for understanding bias amplification in large language model (LLM) evolution by drawing parallels with iterated learning from cognitive science. The authors show that LLM sampling and learning behaviors can be approximated by Bayesian inference, leading to natural amplification of existing biases through repeated generations. The framework offers insights into how biases evolve in iterative self-improvement settings and how carefully designed interaction phases can guide evolution in desired directions.

## Method Summary
The authors apply iterated learning theory to LLM evolution, modeling the process as three phases: imitation (updating posterior beliefs), interaction (filtering/re-ranking generated data), and transmission (passing examples to next generation). They approximate LLM sampling behavior as Bayesian inference, where each generation samples from its posterior and learns from that sampled data, effectively multiplying prior bias by likelihood bias. Experiments are conducted across multiple tasks (ACRE task, acronym-brainstorm) and LLM variants (GPT3.5, GPT4, Claude3-haiku, Mixtral-8x7b) to validate the theoretical framework.

## Key Results
- Iterated learning naturally amplifies existing biases in model priors through repeated Bayesian updates
- LLM sampling behavior can be approximated by Bayesian agents through latent hypothesis selection
- Carefully designed interaction phases can steer bias amplification toward desired directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterated learning amplifies existing biases in model priors through repeated Bayesian updates
- Mechanism: Each generation samples from its posterior, then learns from that sampled data, which reinforces the likelihood of hypotheses that generated the data, effectively multiplying prior bias by likelihood bias
- Core assumption: Model sampling behavior approximates Bayesian inference with a fixed prior P0(h)
- Evidence anchors:
  - [abstract] "By showing that LLM sampling and learning behaviors can be approximated by Bayesian inference, the authors establish that iterated learning naturally amplifies existing biases in model priors"
  - [section 3.1] "Learning involves updating the agent's knowledge based on observations, while sampling is a procedure wherein the agent generates data based on its knowledge"
  - [corpus] Weak evidence - corpus mentions bias amplification but lacks specific Bayesian iterated learning connection
- Break condition: If sampling deviates significantly from Bayesian inference or prior changes substantially between generations

### Mechanism 2
- Claim: Carefully designed interaction phases can steer bias amplification toward desired directions
- Mechanism: Interaction phases filter or re-rank generated data, effectively constraining the hypothesis space to Heff, which determines which biases get amplified
- Core assumption: Interaction phase can be modeled as binary filter on h ∈ Heff
- Evidence anchors:
  - [abstract] "experimental results across multiple tasks and LLM variants confirm this bias amplification, and demonstrate that carefully designed interaction phases can guide evolution in desired directions"
  - [section 3.3] "Imposing appropriate Heff (mainly through a carefully designed interaction phase) can control it"
  - [corpus] Weak evidence - corpus discusses bias amplification generally but doesn't detail interaction phase mechanisms
- Break condition: If interaction phase design is ineffective or if Heff constraint is too loose/strict

### Mechanism 3
- Claim: In-context learning behavior of LLMs approximates Bayesian agents through latent hypothesis selection
- Mechanism: LLMs implicitly select most probable hypothesis ht* from prompt examples, then generate new data conditioned on this hypothesis, mimicking Bayesian iterated learning
- Core assumption: LLM behavior in few-shot learning can be decomposed into hypothesis selection followed by conditional generation
- Evidence anchors:
  - [section 4.1] "sampling from the posterior predictive distribution of agent A, i.e., dt ~ Plmw(d | dt-1), can be decomposed into: 1.) ht* → argmaxh Plmw(h | dt-1), and 2.) dt ~ Plmw(d | ht*)"
  - [section 3.1] "sampling y given the prompt, the examples, and the question xtest can be represented as y ~ Plmw(y | xtest, dN)"
  - [corpus] Weak evidence - corpus doesn't specifically address in-context learning as Bayesian inference
- Break condition: If LLM sampling deviates from MAP hypothesis selection or if prompt examples don't effectively constrain hypothesis space

## Foundational Learning

- Concept: Bayesian inference and posterior updating
  - Why needed here: The entire theoretical framework relies on modeling LLM behavior as Bayesian agents that update beliefs based on observed data
  - Quick check question: Can you explain how the posterior P(h|d) ∝ p(d|h)P0(h) combines prior knowledge with new evidence?

- Concept: Iterated learning and cultural evolution
  - Why needed here: The framework draws direct parallels between LLM evolution and human cultural evolution through repeated learning and transmission
  - Quick check question: What are the three phases of iterated learning and how do they correspond to LLM training processes?

- Concept: Hypothesis space and parameter estimation
  - Why needed here: Understanding how hypotheses (h) represent mappings between inputs and outputs is crucial for grasping how biases amplify
  - Quick check question: In the ACRE task, how many possible hypotheses exist and how does this relate to the prior distribution P0(h)?

## Architecture Onboarding

- Component map: Data generation layer -> Learning layer -> Interaction layer -> Transmission layer -> Hypothesis space
- Critical path:
  1. Initialize model with prior P0(h)
  2. Generate examples using current hypothesis distribution
  3. Update beliefs using Bayesian inference
  4. Apply interaction phase constraints (optional)
  5. Transmit filtered examples to next generation
  6. Repeat until convergence
- Design tradeoffs:
  - Temperature vs convergence speed: Higher temperature slows convergence but maintains diversity
  - Interaction phase strength vs bias control: Stronger filtering better controls bias but may reduce useful diversity
  - Number of generations vs stability: More generations amplify bias more but increase risk of collapse
- Failure signatures:
  - Model collapse: Posterior becomes too peaked, losing diversity
  - Inconsistent outputs: Sampling deviates from Bayesian inference
  - Slow convergence: Hypothesis space too large or interaction phase too weak
  - Amplified malicious bias: Interaction phase fails to constrain harmful hypotheses
- First 3 experiments:
  1. Run imitation-only iterated learning on ACRE task with temperature sweep to observe convergence behavior
  2. Add interaction phase with self-refine to see effect on bias amplification and training accuracy
  3. Test hypothesis search with external interpreter to compare effectiveness of different interaction designs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of temperature parameter influence the convergence speed and final bias amplification in iterated learning of LLMs?
- Basis in paper: [explicit] The paper mentions that "Smaller temperature τ makes the convergence faster" and discusses temperature as an important factor for evolution, but leaves exploration of its interaction with different phases in IL for future work.
- Why unresolved: While the paper demonstrates temperature affects convergence, it does not systematically explore how different temperature settings interact with various phases of iterated learning or how to optimize temperature for desired evolutionary outcomes.
- What evidence would resolve it: Experiments varying temperature across different phases (imitation, interaction, transmission) and measuring their effects on convergence speed, final bias amplification, and diversity of hypotheses would clarify optimal temperature strategies.

### Open Question 2
- Question: What mechanisms can effectively identify and constrain hidden biases in LLMs that are implicitly amplified during evolution?
- Basis in paper: [explicit] The paper states "some biases are inevitably hidden and are also amplified during LLM's evolution" and asks "how to pinpoint these biases, or finding a method that can restrain malicious biases even without explicitly knowing them."
- Why unresolved: While the paper demonstrates that explicit interaction phases can constrain known biases, it acknowledges that many biases remain hidden and their identification and mitigation represents an open challenge.
- What evidence would resolve it: Methods that can automatically detect bias amplification without prior knowledge of what biases exist, perhaps through analysis of generated data distributions or novel evaluation metrics, would address this gap.

### Open Question 3
- Question: How does the size of the bottleneck (number of data samples per generation) affect the convergence behavior and bias amplification in iterated learning?
- Basis in paper: [explicit] The paper mentions that "the choice of m is usually considered as the 'bottleneck' parameter" and discusses how too small or too large m affects convergence speed and prior influence, but does not provide systematic experimental validation.
- Why unresolved: While theoretical arguments about bottleneck effects are provided, the paper does not experimentally explore the full range of bottleneck sizes or their interaction with other parameters like prior bias strength.
- What evidence would resolve it: Systematic experiments varying the number of samples per generation across different tasks and measuring effects on convergence speed, final bias strength, and diversity preservation would clarify optimal bottleneck strategies.

## Limitations

- Theoretical framework relies heavily on Bayesian inference approximation for LLM behavior, which may not hold uniformly across different model architectures and tasks
- Static hypothesis space and prior distribution assumptions may break down when models undergo updates or fine-tuning between generations
- Effectiveness of interaction phase designs needs more extensive validation across diverse tasks beyond the specific experiments conducted

## Confidence

**High Confidence**: The core theoretical result that iterated learning amplifies existing biases through repeated Bayesian updates has strong theoretical grounding in the iterated learning literature and is supported by the experimental convergence patterns observed in the ACRE task.

**Medium Confidence**: The approximation of LLM sampling behavior as Bayesian inference with MAP hypothesis selection is plausible given the experimental results, but may not capture all aspects of LLM generation, particularly for models with more complex reasoning capabilities.

**Low Confidence**: The effectiveness of interaction phase designs (self-refine and hypothesis search) in steering bias amplification toward desired directions needs more extensive validation across diverse tasks.

## Next Checks

1. **Cross-task generalization test**: Apply the iterated learning framework to a diverse set of tasks beyond ACRE and acronym-brainstorm, including creative writing and reasoning tasks, to evaluate whether bias amplification patterns hold across different semantic domains and output structures.

2. **Architecture sensitivity analysis**: Test the framework with different LLM architectures (including smaller models and specialized models) to determine if the Bayesian inference approximation holds across the model spectrum.

3. **Interaction phase optimization**: Systematically vary interaction phase strength and design to find optimal configurations for controlling bias amplification while maintaining useful diversity in generated outputs.