---
ver: rpa2
title: 'MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, Asr
  Error Detection, and Asr Error Correction'
arxiv_id: '2401.13260'
source_url: https://arxiv.org/abs/2401.13260
tags:
- speech
- text
- emotion
- module
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes MF-AED-AEC, a multi-task learning approach
  for multimodal speech emotion recognition (SER) that addresses the challenge of
  ASR errors in text modality. The method integrates three key components: an ASR
  error detection (AED) module to identify erroneous words, an ASR error correction
  (AEC) module to fix detected errors, and a multimodal fusion (MF) module to learn
  modality-specific and modality-invariant representations across audio and text.'
---

# MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion, Asr Error Detection, and Asr Error Correction

## Quick Facts
- arXiv ID: 2401.13260
- Source URL: https://arxiv.org/abs/2401.13260
- Reference count: 0
- One-line primary result: MF-AED-AEC achieves 79.3% UAR on IEMOCAP, a 4.1% improvement over baselines

## Executive Summary
This paper addresses the challenge of speech emotion recognition (SER) in the presence of automatic speech recognition (ASR) errors. The proposed MF-AED-AEC framework combines three components: an ASR error detection (AED) module to identify erroneous words, an ASR error correction (AEC) module to fix detected errors, and a multimodal fusion (MF) module to align and integrate audio and text representations. The approach uses a multi-task learning strategy where the model simultaneously learns to detect and correct ASR errors while performing emotion classification. Experimental results on the IEMOCAP dataset demonstrate that this integrated approach significantly improves SER performance, achieving 79.3% unweighted average recall (UAR) compared to 75.2% for baseline models.

## Method Summary
MF-AED-AEC employs a multi-task learning framework that integrates ASR error detection, correction, and multimodal fusion for emotion recognition. The model uses HuBERT for audio embeddings and BERT for text embeddings, with an ASR error detection module that identifies erroneous tokens using longest common subsequence alignment with human transcripts. An ASR error correction module then generates corrected tokens for identified error positions. The multimodal fusion module combines modality-specific and modality-invariant representations through cross-modal encoders and attention mechanisms. The three components are jointly trained with weighted cross-entropy losses for emotion classification, error detection, and error correction tasks.

## Key Results
- MF-AED-AEC achieves 79.3% UAR on IEMOCAP, outperforming baseline HuBERT+BERT (75.2% UAR)
- The full model shows 4.1% absolute improvement in UAR compared to the best baseline
- Ablation studies confirm each component (AED, AEC, MF) contributes to overall performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AED module improves SER by dynamically weighting ASR tokens based on error likelihood.
- Mechanism: AED uses longest common subsequence (LCS) alignment between ASR output and human transcripts to label tokens as KEEP, DELETE, or CHANGE. A prediction layer assigns confidence scores to each token, which the model uses to down-weight likely erroneous tokens during SER.
- Core assumption: Tokens labeled as errors by AED correspond to semantic disruptions that degrade emotion recognition.
- Evidence anchors:
  - [abstract] "Previous studies have proposed using an auxiliary ASR error detection task to adaptively assign weights of each word in ASR hypotheses."
  - [section] "The aligned tokens are labeled KEEP (K), whereas the remaining tokens are labeled DELETE (D) or CHANGE (C)."
- Break condition: If AED consistently mislabels correct tokens as errors, it may discard useful information; if it fails to detect true errors, those errors propagate into the SER task.

### Mechanism 2
- Claim: The AEC module restores semantic coherence by generating corrected token sequences for identified error positions.
- Mechanism: For each CHANGE position, a transformer decoder takes the original token embedding and context from BERT to produce a corrected token sequence, effectively "filling in" missing or incorrect words.
- Core assumption: Replacing erroneous tokens with corrected ones improves the semantic representation used by the SER classifier.
- Evidence anchors:
  - [abstract] "We introduce an AED module to identify the locations of ASR errors. Subsequently, we employ an AEC module to correct these errors, thereby reducing the impact of ASR errors on SER tasks."
  - [section] "Our decoder operates in parallel to the tokens predicted as C. For the kth change position, the decoding sequence can be represented as Zk..."
- Break condition: If the decoder over-corrects (introducing new errors) or under-corrects (leaving noise), the benefit to SER diminishes.

### Mechanism 3
- Claim: The MF module bridges modality distribution gaps by learning shared and modality-specific representations.
- Mechanism: Two cross-modal encoder (CME) blocks process speech and text to produce modality-specific features. A modality-invariant representation (MIR) block extracts shared information across modalities via cross-attention, then concatenates all representations for fusion.
- Core assumption: Learning modality-specific and modality-invariant features jointly enables better alignment between heterogeneous modalities.
- Evidence anchors:
  - [abstract] "the inherent heterogeneity of different modalities leads to distribution gaps between their representations, making their fusion challenging."
  - [section] "Our multimodal fusion (MF) module is composed of two cross-modal encoder (CME) blocks and one modality-invariant representations (MIR) block."
- Break condition: If the modality-specific features become too entangled or the invariant space collapses, the model may lose modality-specific nuance or fail to align properly.

## Foundational Learning

- Concept: ASR error detection and correction
  - Why needed here: ASR errors directly corrupt the text modality, which is a key input to multimodal SER.
  - Quick check question: What labels does the AED module assign to ASR tokens during training?

- Concept: Cross-modal attention and modality alignment
  - Why needed here: Speech and text have different statistical distributions; alignment is required for meaningful fusion.
  - Quick check question: What role do the CME blocks play in the MF module?

- Concept: Multi-task learning with auxiliary objectives
  - Why needed here: Joint training on AED, AEC, and SER forces the model to reason about ASR errors while predicting emotion.
  - Quick check question: How are the three loss functions combined during training?

## Architecture Onboarding

- Component map: HuBERT encoder → BERT encoder → AED → AEC → MF (CME + MIR) → SER classifier
- Critical path: ASR text → BERT → AED (error detection) → AEC (correction) → MF fusion → emotion classifier
- Design tradeoffs: Adding AED/AEC increases model complexity and training time but improves robustness to ASR errors; MF adds fusion overhead but bridges modality gaps.
- Failure signatures: If AED mislabels, AEC may correct correct tokens; if MF fails, modalities may not align and performance degrades.
- First 3 experiments:
  1. Run baseline HuBERT+BERT without AED/AEC/MF; compare to MF-AED-AEC on IEMOCAP.
  2. Remove AED only; measure impact on UAR.
  3. Remove MF only; measure impact on UAR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MF-AED-AEC scale with larger datasets or different emotional categories?
- Basis in paper: [inferred] The paper evaluates MF-AED-AEC on the IEMOCAP dataset with four emotion categories. It would be valuable to assess its performance on larger datasets or with more diverse emotional categories to determine its generalizability and robustness.
- Why unresolved: The current evaluation is limited to a specific dataset and a fixed set of emotions, which may not capture the full potential or limitations of the model.
- What evidence would resolve it: Conducting experiments on multiple datasets with varying sizes and emotional categories, and comparing the results to assess scalability and robustness.

### Open Question 2
- Question: What is the impact of the ASR error detection and correction modules on real-world noisy speech data?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the AED and AEC modules on the IEMOCAP dataset. However, it does not explicitly address their performance on real-world noisy speech data, which is a common challenge in practical applications.
- Why unresolved: The paper focuses on controlled experimental conditions, which may not fully represent the variability and complexity of real-world scenarios.
- What evidence would resolve it: Testing the model on datasets with real-world noisy speech data and comparing its performance to baseline models to determine its robustness and effectiveness in practical applications.

### Open Question 3
- Question: How does the MF-AED-AEC model handle multimodal data with more than two modalities, such as video or facial expressions?
- Basis in paper: [inferred] The paper focuses on integrating audio and text modalities for speech emotion recognition. It would be interesting to explore how the model performs when additional modalities, such as video or facial expressions, are included.
- Why unresolved: The current model is designed for two modalities, and its performance with additional modalities is not addressed.
- What evidence would resolve it: Extending the model to incorporate additional modalities and evaluating its performance on multimodal datasets that include video or facial expressions to assess its adaptability and effectiveness.

## Limitations
- Relies on Whisper-medium for ASR error simulation, which may not capture real-world ASR error distributions
- Requires aligned human transcripts for AED/AEC training, limiting applicability to scenarios without such alignments
- Introduces significant computational overhead that may not scale efficiently to larger datasets or real-time applications

## Confidence

**High Confidence**: The ablation studies demonstrating individual component contributions and the 4.1% UAR improvement over baselines are well-supported by the experimental design.

**Medium Confidence**: The claim that MF-AED-AEC shows "robustness to ASR errors" is supported by controlled experiments on IEMOCAP, but generalizability to other datasets and error distributions remains uncertain.

**Low Confidence**: The paper does not provide extensive analysis of failure cases or scenarios where the model underperforms.

## Next Checks
1. **Dataset Generalization Test**: Evaluate MF-AED-AEC on at least two additional emotion recognition datasets (e.g., MSP-Improv, MELD) with different acoustic conditions and error patterns to assess robustness beyond IEMOCAP.

2. **Error Rate Sensitivity Analysis**: Systematically vary the ASR WER (e.g., 10%, 20%, 30%) in the test data to quantify how performance degrades with increasing error rates and whether the AED/AEC components maintain their effectiveness.

3. **Ablation on Error Detection Precision**: Remove the AEC component while keeping AED active, then measure whether the weighted token approach alone provides comparable benefits to the full correction pipeline, isolating the value of error detection versus correction.