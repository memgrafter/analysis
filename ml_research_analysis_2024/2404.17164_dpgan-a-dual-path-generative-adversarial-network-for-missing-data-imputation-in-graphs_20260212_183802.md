---
ver: rpa2
title: 'DPGAN: A Dual-Path Generative Adversarial Network for Missing Data Imputation
  in Graphs'
arxiv_id: '2404.17164'
source_url: https://arxiv.org/abs/2404.17164
tags:
- graph
- data
- missing
- imputation
- discriminator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses missing data imputation in graph-structured
  data. Existing methods often suffer from over-smoothing issues when using graph
  neural networks (GNNs).
---

# DPGAN: A Dual-Path Generative Adversarial Network for Missing Data Imputation in Graphs

## Quick Facts
- arXiv ID: 2404.17164
- Source URL: https://arxiv.org/abs/2404.17164
- Reference count: 10
- DPGAN achieves up to 27.6% improvement in RMSE over state-of-the-art methods

## Executive Summary
DPGAN addresses missing data imputation in graph-structured data by introducing a dual-path generative adversarial network that combines GraphUNet++ and MLPUnet++ to capture both global and local graph representations. The framework tackles the over-smoothing problem common in graph neural networks by using node-mix MLP layers to handle long-range dependencies while preserving high-frequency characteristics through feature-mix MLPs. Experimental results across five benchmark datasets demonstrate consistent performance improvements, with the model effectively handling various missing data rates including scenarios where all features are missing.

## Method Summary
DPGAN employs a dual-path generator architecture consisting of GraphUNet++ for capturing structural information and long-range dependencies through node-mix MLP layers, and MLPUnet++ for numerical feature fitting using node and feature-mix MLPs. The framework uses a subgraph discriminator instead of full graph evaluation to focus on local subgraph fidelity, with adjustable depth controlled by the number of graph pooling layers. Training combines reconstruction loss (L2) with WGAN-GP adversarial loss, using a two-time-scale update rule for stability. The alpha parameter controls the weighted sum between the two generator paths, adapting to different missing rates.

## Key Results
- Achieves up to 27.6% improvement in RMSE compared to state-of-the-art imputation methods
- Consistently outperforms baseline methods across all five benchmark datasets (ENZYMES, QM9, Synthie, FRANKENSTEIN, FIRSTMM DB, Cora, CiteSeer)
- Effectively handles extreme missing data scenarios, including cases where all features are missing
- Shows robustness across varying missing data rates from 10% to 90%

## Why This Works (Mechanism)

### Mechanism 1
Dual-path architecture prevents over-smoothing by balancing global and local feature learning. GraphUnet++ captures long-range dependencies through node-mix MLP layers following GNNs, while MLPUnet++ restores high-frequency characteristics via feature-mix MLPs. This combination allows the model to handle both structural information and numerical feature fitting simultaneously.

### Mechanism 2
Subgraph discriminator provides targeted adversarial regularization that improves local imputation quality. By focusing on local subgraph fidelity rather than entire graph evaluation, the discriminator can operate at different scales from node-level to full graph-level, allowing adjustable adversarial regularization intensity.

### Mechanism 3
Adjustable alpha weighting between GraphUnet++ and MLPUnet++ enables optimal performance across different missing rates. The alpha parameter controls the weighted sum of outputs, with lower missing rates favoring MLPUnet++ (higher alpha) and higher missing rates benefiting more from GraphUnet++ (lower alpha).

## Foundational Learning

- Graph Neural Networks (GNNs):
  - Why needed here: Forms the foundation for graph representation learning but has limitations with missing data
  - Quick check question: What is the primary limitation of GNNs when dealing with missing data in graph-structured datasets?

- Generative Adversarial Networks (GANs):
  - Why needed here: Provides adversarial training framework that improves imputation quality through generator-discriminator competition
  - Quick check question: How does the adversarial training process in GANs help improve the quality of generated data compared to traditional autoencoders?

- Graph Autoencoders:
  - Why needed here: Forms base architecture for both GraphUnet++ and MLPUnet++, enabling compression and reconstruction of graph features
  - Quick check question: What is the key difference between a standard autoencoder and a graph autoencoder in terms of input and output?

## Architecture Onboarding

- Component map: Input graph (A, X) and mask (R) -> GraphUnet++ and MLPUnet++ -> Weighted sum (alpha * X'' + (1-alpha) * X') -> Subgraph discriminator -> Adversarial and reconstruction losses

- Critical path: 1) Input graph enters both GraphUnet++ and MLPUnet++ 2) Each path produces imputed features 3) Weighted sum creates final output 4) Subgraph discriminator evaluates local fidelity 5) Adversarial and reconstruction losses drive training

- Design tradeoffs: Computational cost vs. imputation quality (dual-path architecture increases complexity but improves performance), local vs. global focus (subgraph discriminator trades comprehensive evaluation for targeted regularization), fixed vs. adaptive weighting (alpha parameter adds flexibility but requires learning)

- Failure signatures: Over-smoothing (generated features become too smooth, losing important details), mode collapse (generator produces limited variety in imputed values), discriminator overpowering (generator cannot keep up, leading to poor imputation), subgraph leakage (training set information leaks through subgraph discriminator)

- First 3 experiments: 1) Single-path baseline (only GraphUnet++ without MLPUnet++), 2) Fixed alpha (set to 0.5 across all missing rates), 3) Subgraph depth ablation (test node-level, 1-hop, and 2-hop subgraph discriminators)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored based on the methodology and results presented.

## Limitations

- Computational overhead may limit scalability to very large graphs with thousands of nodes and edges
- Performance across extremely high missing rates (>90%) remains untested and could reveal model limitations
- The adaptability of alpha values to different domain characteristics needs further investigation

## Confidence

- High Confidence: The core claim that DPGAN outperforms existing methods on benchmark datasets (RMSE improvements up to 27.6%)
- Medium Confidence: The mechanism by which dual-path architecture prevents over-smoothing is theoretically sound but requires empirical validation of each component's contribution
- Low Confidence: The optimal configuration of alpha weighting and subgraph discriminator depth across diverse real-world scenarios

## Next Checks

1. Component Ablation Study: Systematically remove GraphUnet++ or MLPUnet++ to quantify each path's individual contribution to overall performance

2. Cross-Domain Transferability: Evaluate DPGAN on datasets from different domains (e.g., social networks, biological networks) to assess generalizability

3. Real-World Missingness Patterns: Test the framework on datasets with non-random missingness patterns that better reflect real-world scenarios