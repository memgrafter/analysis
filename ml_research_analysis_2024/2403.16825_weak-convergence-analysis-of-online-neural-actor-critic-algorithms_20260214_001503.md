---
ver: rpa2
title: Weak Convergence Analysis of Online Neural Actor-Critic Algorithms
arxiv_id: '2403.16825'
source_url: https://arxiv.org/abs/2403.16825
tags:
- actor
- critic
- convergence
- have
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a rigorous convergence analysis of online neural
  network actor-critic algorithms in reinforcement learning. The authors prove that
  as the number of hidden units and training steps go to infinity, the time-rescaled
  neural network outputs converge weakly to solutions of a system of ordinary differential
  equations (ODEs).
---

# Weak Convergence Analysis of Online Neural Actor-Critic Algorithms

## Quick Facts
- arXiv ID: 2403.16825
- Source URL: https://arxiv.org/abs/2403.16825
- Authors: Samuel Chun-Hei Lam; Justin Sirignano; Ziheng Wang
- Reference count: 40
- Primary result: Proves convergence of online neural network actor-critic algorithms to solutions of ordinary differential equations as hidden units and training steps go to infinity

## Executive Summary
This paper provides a rigorous convergence analysis of online neural network actor-critic algorithms in reinforcement learning. The authors prove that as the number of hidden units and training steps go to infinity, the time-rescaled neural network outputs converge weakly to solutions of a system of ordinary differential equations (ODEs). The analysis addresses the key challenge of non-i.i.d. data samples whose distribution changes dynamically during training. Using a Poisson equation and weak convergence techniques, the authors establish geometric ergodicity of the data samples and prove that fluctuations due to randomly-arriving samples vanish as training progresses. The limit critic network is shown to converge to the true value function, providing the actor with an asymptotically unbiased estimate of the policy gradient. Finally, the limit actor network is proven to converge to a stationary point of the expected reward.

## Method Summary
The authors analyze online actor-critic algorithms using single-layer neural networks with sigmoid activation functions. The algorithm alternates between updating the critic network using temporal difference learning to minimize Bellman error, and updating the actor network using the policy gradient theorem. The key innovation is proving that when both the number of hidden units and training steps go to infinity, the neural network outputs converge in distribution to solutions of a system of ODEs. This convergence is established using the Neural Tangent Kernel (NTK) framework and weak convergence techniques. The analysis carefully handles the non-i.i.d. nature of the data samples by establishing geometric ergodicity and using a Poisson equation to characterize the data distribution.

## Key Results
- Proves weak convergence of time-rescaled neural network outputs to ODEs as hidden units and training steps → ∞
- Shows limit critic network converges to true value function (Bellman equation solution) using two-timescale analysis
- Proves limit actor network converges to stationary point of expected reward
- Establishes geometric ergodicity of data samples using Poisson equation and weak convergence techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm converges to a limit ODE as the number of hidden units and training steps go to infinity.
- Mechanism: The neural network parameters remain within a small neighborhood of their initial values during training, allowing the outputs to be linearized around the initial empirical distribution. This is the Neural Tangent Kernel (NTK) result, which enables the time-rescaled neural network outputs to converge pathwise weakly to an ODE.
- Core assumption: The randomly initialized parameters are i.i.d. mean-zero random variables with distributions that are absolutely continuous with respect to the Lebesgue measure.
- Evidence anchors:
  - [abstract]: "We prove that a single-layer neural network trained with the online actor critic algorithm converges in distribution to a random ordinary differential equation (ODE) as the number of hidden units and the number of training steps → ∞."
  - [section]: "The convergence to the limit ODE is a result of the neural network parameters remaining within a small neighborhood of their initial values as they train. This result is referred to as the Neural Tangent Kernel (NTK) result and was discovered in [20] for feedforward networks in supervised learning."

### Mechanism 2
- Claim: The limit critic network converges to the true value function, providing the actor with an asymptotically unbiased estimate of the policy gradient.
- Mechanism: The limit ODE for the critic is analyzed, showing that it converges to the solution of the Bellman equation. This is achieved by leveraging the two timescales for the actor and critic ODEs (due to their respective learning rates) and using a Poisson equation to address the non-i.i.d. data samples.
- Core assumption: The MDP is ergodic (irreducible and aperiodic) whenever the actor policy selects every action with positive probability.
- Evidence anchors:
  - [abstract]: "Analysis of the limit ODE shows that the limit critic network will converge to the true value function, which will provide the actor an asymptotically unbiased estimate of the policy gradient."
  - [section]: "Leveraging the two timescales for the actor and critic ODEs (due to their respective learning rates), we are able to prove that the critic ODE converges to the true value function (the solution of the Bellman equation) as the training time t → ∞."

### Mechanism 3
- Claim: The limit actor network converges to a stationary point of the objective function as the training time goes to infinity.
- Mechanism: The limit ODE for the actor is analyzed, showing that it converges to a stationary point of the expected reward. This is achieved by using a two timescale analysis to separate the actor and critic updates and by carefully selecting the learning rates.
- Core assumption: The learning rate for the actor is logarithmic, i.e., ζN_k = 1/(1 + k/N), and the learning rate for the critic is constant, i.e., αN = α/N for some α > 0.
- Evidence anchors:
  - [abstract]: "We then prove that the limit actor network will converge to a stationary point of the objective function as t → ∞."
  - [section]: "We prove that the limit actor network will converge to a stationary point of the objective function as t → ∞. Therefore, although in the pre-limit actor-critic algorithm the critic provides a noisy, biased (i.e., there is error) estimate of the value function, we are able to prove that asymptotically the critic will converge sufficiently rapidly such that the actor also converges."

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The paper studies actor-critic algorithms for MDPs, which provide a mathematical framework for modeling sequential decision-making problems.
  - Quick check question: What are the key components of an MDP and how do they relate to the actor-critic algorithm?

- Concept: Value Functions and Policy Gradient Theorem
  - Why needed here: The paper uses value functions (state and action-value functions) to evaluate the performance of policies, and the policy gradient theorem to update the actor parameters.
  - Quick check question: How are value functions defined in the context of MDPs, and how does the policy gradient theorem relate to the actor-critic algorithm?

- Concept: Neural Tangent Kernel (NTK) and Weak Convergence
  - Why needed here: The paper uses the NTK result to show that the neural network outputs converge to a limit ODE, and weak convergence techniques to prove the convergence of the actor and critic networks.
  - Quick check question: What is the NTK result, and how does it relate to the convergence of neural networks in the context of the actor-critic algorithm?

## Architecture Onboarding

- Component map:
  MDP -> Actor Network -> State-Action Pairs -> Critic Network -> Value Function -> Policy Update

- Critical path:
  1. Initialize the actor and critic networks with random parameters.
  2. At each iteration, sample a state-action pair from the MDP using the current actor policy.
  3. Update the critic network using temporal difference learning to minimize the Bellman error.
  4. Update the actor network using the policy gradient theorem to maximize the expected reward.
  5. Repeat steps 2-4 until convergence.

- Design tradeoffs:
  - Exploration vs. Exploitation: The actor policy must balance between exploring new actions and exploiting known good actions.
  - Actor vs. Critic Learning Rates: The learning rates for the actor and critic must be carefully selected to ensure convergence.
  - Network Architecture: The choice of neural network architecture (e.g., number of layers, activation functions) can impact the performance of the algorithm.

- Failure signatures:
  - Non-convergence: If the algorithm does not converge to a limit ODE, it may be due to the parameters escaping the small neighborhood of their initial values or the MDP not being ergodic.
  - Suboptimal Performance: If the algorithm converges but does not achieve good performance, it may be due to poor exploration, suboptimal learning rates, or an inadequate network architecture.

- First 3 experiments:
  1. Verify that the actor and critic networks are properly initialized and can represent the policy and value function, respectively.
  2. Test the algorithm on a simple MDP (e.g., a grid world) to ensure that it can learn a good policy and value function.
  3. Vary the learning rates and network architecture to study their impact on the convergence and performance of the algorithm.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence analysis be extended to multi-layer neural networks in actor-critic algorithms?
- Basis in paper: [explicit] The paper focuses on single-layer neural networks and mentions that multi-layer networks would be an interesting direction for future work.
- Why unresolved: The analysis relies heavily on the NTK result, which is well-established for single-layer networks but becomes significantly more complex for deeper architectures.
- What evidence would resolve it: A rigorous proof showing convergence to a limiting ODE system for actor-critic algorithms with multi-layer neural networks, along with analysis of the properties of the limit ODE.

### Open Question 2
- Question: How does the choice of activation function affect the convergence properties of neural actor-critic algorithms?
- Basis in paper: [explicit] The paper assumes a specific class of activation functions (twice continuously differentiable, bounded, slowly increasing) but does not explore the impact of different activation function choices.
- Why unresolved: The convergence analysis depends on properties of the activation function, but the sensitivity to different choices within the assumed class is not explored.
- What evidence would resolve it: A comprehensive study comparing convergence rates and properties for different activation functions within the assumed class, or a proof showing that convergence holds for a broader class of activation functions.

### Open Question 3
- Question: Can the analysis be extended to continuous state and action spaces?
- Basis in paper: [inferred] The paper assumes a finite state-action space, which is a significant limitation for many real-world applications. The authors mention that extending to continuous spaces is an important direction for future work.
- Why unresolved: The analysis relies on the finiteness of the state-action space for various technical results, including the compactness arguments and the representation of the actor and critic networks.
- What evidence would resolve it: A rigorous convergence proof for actor-critic algorithms with neural networks in continuous state-action spaces, along with an analysis of the properties of the limit ODE system.

## Limitations
- Finite state-action space assumption excludes many practical RL applications
- Bounded reward requirement restricts applicability to problems with known reward bounds
- Strong ergodicity assumptions may not hold in practice
- NTK approximation validity depends on parameters staying close to initialization

## Confidence

**Weak convergence analysis:** High - The mathematical framework is rigorous and well-established

**Convergence to true value function:** Medium - Relies on strong ergodicity assumptions and proper timescale separation

**Convergence to stationary point:** Medium - Depends on critic providing accurate gradient estimates, which may not hold in practice

**Practical applicability:** Low - Assumptions are restrictive and may not hold for many real-world RL problems

## Next Checks

1. **Empirical verification of NTK regime**: Implement the algorithm with varying numbers of hidden units N and empirically verify that parameter updates remain small relative to initialization, validating the NTK approximation assumption.

2. **Timescale separation testing**: Conduct controlled experiments varying the ratio of actor to critic learning rates to determine how sensitive convergence is to the assumed timescale separation, and identify practical bounds on this ratio.

3. **Finite-state approximation study**: Test the algorithm on continuous-state MDPs by discretizing the state space and analyze how the approximation error from discretization affects convergence, comparing with theoretical bounds that assume finite state spaces.