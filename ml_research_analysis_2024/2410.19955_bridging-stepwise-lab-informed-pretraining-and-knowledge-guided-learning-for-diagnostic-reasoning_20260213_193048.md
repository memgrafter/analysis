---
ver: rpa2
title: Bridging Stepwise Lab-Informed Pretraining and Knowledge-Guided Learning for
  Diagnostic Reasoning
arxiv_id: '2410.19955'
source_url: https://arxiv.org/abs/2410.19955
tags:
- prediction
- knowledge
- diagnosis
- dualk
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DuaLK, a dual-expertise framework that bridges
  stepwise lab-informed pretraining and knowledge-guided learning for diagnostic reasoning.
  The framework constructs a diagnosis knowledge graph enriched with hierarchical
  and semantic relations via large language models, and introduces a lab-informed
  pretraining task that models stepwise diagnostic reasoning through lab assignment
  and abnormality detection.
---

# Bridging Stepwise Lab-Informed Pretraining and Knowledge-Guided Learning for Diagnostic Reasoning

## Quick Facts
- arXiv ID: 2410.19955
- Source URL: https://arxiv.org/abs/2410.19955
- Authors: Pengfei Hu; Chang Lu; Fei Wang; Yue Ning
- Reference count: 40
- DuaLK achieves up to 6-11% improvement in weighted F1 score for diagnosis prediction and demonstrates strong generalization with limited training data.

## Executive Summary
This paper proposes DuaLK, a dual-expertise framework that bridges stepwise lab-informed pretraining and knowledge-guided learning for diagnostic reasoning. The framework constructs a diagnosis knowledge graph enriched with hierarchical and semantic relations via large language models, and introduces a lab-informed pretraining task that models stepwise diagnostic reasoning through lab assignment and abnormality detection. Experimental results on MIMIC-III and MIMIC-IV datasets show that DuaLK consistently outperforms existing baselines across diagnosis and heart failure prediction tasks, achieving up to 6-11% improvement in weighted F1 score and demonstrating strong generalization with limited training data. The approach effectively combines structured medical knowledge with individual-level clinical signals to achieve more accurate and interpretable diagnostic predictions.

## Method Summary
DuaLK combines a knowledge-guided learning module with a lab-informed pretraining module within an encoder-decoder architecture. The knowledge-guided module constructs a diagnosis knowledge graph using external medical ontologies and LLM-generated triples, embedding them in polar space to capture both hierarchical and semantic relationships. The lab-informed pretraining module models clinical reasoning as a stepwise process of lab test assignment followed by abnormality detection. These two modules mutually refine patient representations through graph neural networks and attention mechanisms, producing enhanced clinical predictions for diagnosis and heart failure prediction tasks.

## Key Results
- DuaLK achieves 6-11% improvement in weighted F1 score for multi-label diagnosis prediction compared to existing baselines
- The framework demonstrates strong performance in heart failure prediction with significant AUC improvements
- DuaLK shows robust generalization capabilities when training data is limited, maintaining performance with reduced sample sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DuaLK's lab-informed pretraining task mimics clinical reasoning by modeling the stepwise assignment of lab tests followed by abnormality detection.
- Mechanism: The pretraining framework separates the prediction of which lab tests to assign from the prediction of which results are abnormal, creating a conditional probability structure that mirrors physician decision-making.
- Core assumption: Lab test assignment and abnormality detection are sequential, conditionally dependent steps in clinical diagnosis that can be modeled as separate but related tasks.
- Evidence anchors:
  - [abstract] "we further introduce a lab-informed proxy task that guides the model to follow a clinically consistent, stepwise reasoning process based on lab test signals"
  - [section] "To the best of our knowledge, this is the first work to leverage lab test data for pretraining, modeling stepwise diagnostic reasoning through lab assignment and abnormality detection"
  - [corpus] Weak - corpus papers focus on general multimodal reasoning but don't specifically address stepwise lab-informed pretraining

### Mechanism 2
- Claim: The dual-expertise framework combines structured medical knowledge graphs with individual patient EHR data to create more accurate and interpretable predictions.
- Mechanism: DuaLK uses polar-space KG embeddings to capture both hierarchical and semantic relationships in the knowledge graph, then refines these priors through graph neural networks and attention mechanisms using individual patient data.
- Core assumption: Medical knowledge can be effectively represented as a bi-hierarchical knowledge graph that captures both semantic and hierarchical relationships between medical concepts.
- Evidence anchors:
  - [abstract] "we construct a Diagnosis Knowledge Graph (KG) that encodes both hierarchical and semantic relations enriched by large language models (LLM)"
  - [section] "To describe medical concepts from both contextual and hierarchical perspectives, we leverage two properties of the polar coordinate system: (1) Radial coordinate: map entities across different levels of the hierarchy; (2) Angular coordinate: map entities at the same level based on contextual information"
  - [corpus] Moderate - corpus contains papers on knowledge graphs for clinical reasoning but none specifically use polar-space embeddings for hierarchical and semantic knowledge integration

### Mechanism 3
- Claim: The encoder-decoder architecture allows for mutual refinement between knowledge-guided and lab-informed learning modules, creating more robust patient representations.
- Mechanism: The encoder processes graph-enhanced patient embeddings while the decoder refines these through the lab-informed pretraining task, with both modules learning complementary representations that are combined for final predictions.
- Core assumption: Graph-based patient representations and lab-informed pretraining can be effectively combined through an encoder-decoder architecture to create mutually reinforcing representations.
- Evidence anchors:
  - [abstract] "These two expertise are collaborated within an encoder-decoder architecture, enabling mutual refinement and enhancing clinical prediction accuracy"
  - [section] "The proposed encoder-decoder framework not only emphasizes the comprehensive understanding of medical patterns but also seamlessly integrates local knowledge into the learning process"
  - [corpus] Moderate - corpus contains papers on multimodal fusion but none specifically use encoder-decoder architecture for combining KG and lab-informed learning

## Foundational Learning

- Concept: Polar-space KG embeddings
  - Why needed here: Traditional KG embeddings often emphasize either contextual or hierarchical information, but DuaLK requires both simultaneously for clinical knowledge representation
  - Quick check question: How does the polar coordinate system enable simultaneous representation of hierarchical and semantic relationships in medical knowledge graphs?

- Concept: Graph neural networks with attention mechanisms
  - Why needed here: Patient EHR data needs to be processed through both graph-based relationships and attention mechanisms to capture complex medical interactions
  - Quick check question: What advantages does combining GAT convolutions with bi-attention mechanisms provide over using either approach alone for patient representation?

- Concept: Stepwise conditional probability modeling
  - Why needed here: Clinical reasoning involves sequential decision-making that requires modeling conditional probabilities between lab test assignment and abnormality detection
  - Quick check question: How does separating lab test assignment from abnormality detection in the pretraining task better capture the clinical reasoning process?

## Architecture Onboarding

- Component map: Diagnosis KG construction (LLM triples + ontologies) -> Polar-space KG embedding -> GNN processing -> Bi-attention refinement -> Lab-informed pretraining (assignment + abnormality detection) -> Encoder-decoder mutual refinement -> Final prediction

- Critical path: KG construction → KG embedding → GNN processing → Attention refinement → Lab-informed pretraining → Encoder-decoder refinement → Prediction

- Design tradeoffs:
  - Computational cost vs. knowledge richness: Larger KGs provide more comprehensive knowledge but increase memory requirements
  - Pretraining vs. fine-tuning balance: More pretraining epochs improve lab reasoning but may reduce adaptability to specific tasks
  - Model complexity vs. interpretability: More complex architectures capture nuanced relationships but reduce clinical interpretability

- Failure signatures:
  - Memory overflow during KG embedding training (indicates need for dimensionality reduction)
  - Degradation in pretraining task performance (suggests lab assignment/abnormality modeling issues)
  - Inconsistent performance across different clinical tasks (indicates poor generalization)

- First 3 experiments:
  1. Train KG embeddings with link prediction task on synthetic triples to verify polar-space embedding implementation
  2. Run lab-informed pretraining task on small subset of MIMIC data to validate conditional probability modeling
  3. Test encoder-decoder mutual refinement on simple diagnosis prediction task before full model integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dual-expertise framework DuaLK scale to handle larger, more complex knowledge graphs without compromising performance or efficiency?
- Basis in paper: [explicit] The paper notes the efficiency of DuaLK in handling large-scale knowledge graphs and compares it to GraphCare, which struggles with memory usage as the graph size increases.
- Why unresolved: While DuaLK shows better scalability than GraphCare, the paper does not explore the limits of its scalability or the impact of extremely large knowledge graphs on performance.
- What evidence would resolve it: Testing DuaLK on progressively larger and more complex knowledge graphs, measuring performance and resource usage, would clarify its scalability limits.

### Open Question 2
- Question: Can the lab-informed pretraining strategy be effectively applied to other clinical prediction tasks beyond diagnosis and heart failure, such as treatment response or readmission prediction?
- Basis in paper: [inferred] The paper focuses on diagnosis and heart failure prediction tasks but suggests the potential for broader application of the lab-informed pretraining strategy.
- Why unresolved: The paper does not explore the effectiveness of the lab-informed pretraining strategy on other clinical prediction tasks.
- What evidence would resolve it: Applying DuaLK to additional clinical prediction tasks and comparing its performance to existing methods would demonstrate the generalizability of the lab-informed pretraining strategy.

### Open Question 3
- Question: How does the integration of semantic and hierarchical information in the diagnosis knowledge graph impact the model's ability to generalize to unseen or rare diseases?
- Basis in paper: [explicit] The paper highlights the importance of both semantic and hierarchical information in the diagnosis knowledge graph for improving predictive performance.
- Why unresolved: While the paper demonstrates the effectiveness of the diagnosis knowledge graph, it does not specifically investigate its impact on generalization to rare diseases.
- What evidence would resolve it: Evaluating DuaLK's performance on a dataset containing a higher proportion of rare diseases and comparing it to models without the diagnosis knowledge graph would clarify the impact of semantic and hierarchical information on generalization.

## Limitations
- Performance improvements demonstrated primarily on MIMIC datasets which may not represent diverse clinical settings
- LLM-generated knowledge graph triples introduce potential variability depending on prompting strategy
- Limited exploration of framework's scalability to larger, more complex knowledge graphs

## Confidence
- **High confidence**: The dual-expertise architecture concept and its general implementation approach; the use of polar-space KG embeddings for hierarchical and semantic knowledge representation; the stepwise lab-informed pretraining task design
- **Medium confidence**: The specific performance improvements claimed, given the lack of comparison with more recent SOTA models; the generalizability to non-MIMIC clinical datasets; the exact impact of the LLM-generated triples on overall performance
- **Low confidence**: The optimal balance between pretraining and fine-tuning; the scalability of the approach to larger, more diverse knowledge graphs; the reproducibility of the specific hyperparameter settings

## Next Checks
1. Implement ablation studies removing the LLM-generated triples to quantify their specific contribution to performance gains
2. Test the framework on external clinical datasets (e.g., eICU, private hospital data) to assess generalizability beyond MIMIC
3. Conduct sensitivity analysis on key hyperparameters including KG embedding dimensionality, pretraining task weighting, and attention mechanism configurations