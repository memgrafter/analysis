---
ver: rpa2
title: 'MuGSI: Distilling GNNs with Multi-Granularity Structural Information for Graph
  Classification'
arxiv_id: '2406.19832'
source_url: https://arxiv.org/abs/2406.19832
tags:
- graph
- mugsi
- student
- distillation
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MuGSI addresses the challenge of extending knowledge distillation
  from GNNs to MLPs for graph classification, where existing methods designed for
  node classification are ineffective due to sparse learning signals and limited expressiveness
  of student MLPs. It introduces a novel multi-granularity distillation framework
  that combines graph-level, subgraph-level, and node-level distillation losses, along
  with node feature augmentation to enhance student model expressiveness.
---

# MuGSI: Distilling GNNs with Multi-Granularity Structural Information for Graph Classification

## Quick Facts
- arXiv ID: 2406.19832
- Source URL: https://arxiv.org/abs/2406.19832
- Authors: Tianjun Yao; Jiaqi Sun; Defu Cao; Kun Zhang; Guangyi Chen
- Reference count: 40
- Key outcome: Student GA-MLP achieves comparable or superior performance to teacher GNNs in 7/8 datasets

## Executive Summary
MuGSI addresses the challenge of extending knowledge distillation from GNNs to MLPs for graph classification, where existing methods designed for node classification are ineffective due to sparse learning signals and limited expressiveness of student MLPs. It introduces a novel multi-granularity distillation framework that combines graph-level, subgraph-level, and node-level distillation losses, along with node feature augmentation to enhance student model expressiveness. The subgraph-level distillation uses clustering to capture structural information, while the node-level distillation leverages random walks to transfer local substructure knowledge. Experiments show that MuGSI significantly outperforms baseline methods across multiple datasets, with the student GA-MLP achieving comparable or superior performance to teacher GNNs in 7/8 datasets.

## Method Summary
MuGSI implements a multi-granularity knowledge distillation framework that transfers structural knowledge from pre-trained teacher GNNs to student MLPs for graph classification. The method combines three distillation components: graph-level distillation aligns whole-graph representations, subgraph-level distillation uses clustering-based kernel matrices to capture inter-cluster relationships, and node-level distillation employs random walk path consistency to transfer local substructure knowledge. The framework also incorporates node feature augmentation using Laplacian eigenvectors to address the limited expressiveness of MLPs. The student model uses a 1-hop GA-MLP architecture that maintains computational efficiency while providing enhanced expressiveness for graph classification tasks.

## Key Results
- Student GA-MLP achieves comparable or superior performance to teacher GNNs in 7/8 datasets
- MuGSI demonstrates 17.18x speedup in inference compared to teacher models
- The framework shows improved robustness to dynamic graph changes with less accuracy degradation
- Multi-granularity approach significantly outperforms baseline soft-logits distillation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-granularity distillation loss generates dense learning signals for graph classification by capturing structural knowledge at graph, subgraph, and node levels.
- Mechanism: The framework combines three distinct distillation components: graph-level distillation aligns whole-graph representations between teacher and student, subgraph-level distillation uses clustering to capture inter-cluster structural relationships, and node-level distillation leverages random walks to transfer local substructure knowledge. This multi-scale approach compensates for the sparsity of graph-level labels in traditional soft-logits distillation.
- Core assumption: Local substructures and cluster-level relationships contain complementary information to whole-graph representations for effective knowledge transfer.
- Evidence anchors:
  - [abstract]: "This loss function is composed of three distinct components: graph-level distillation, subgraph-level distillation, and node-level distillation. Each component targets a specific granularity of the graph structure, ensuring a comprehensive transfer of structural knowledge"
  - [section 4.1]: "whole-graph distillation loss can be formulated as follows: L_G = E_{G_i ~ D_L} ||h_G_i^T / ||h_G_i^T||_2 - h_G_i^S / ||h_G_i^S||_2||_2^2"
  - [corpus]: Weak - corpus neighbors don't directly address multi-granularity approaches
- Break condition: If clustering fails to capture meaningful substructures for the specific dataset domain, the subgraph-level distillation component loses effectiveness.

### Mechanism 2
- Claim: Node feature augmentation enhances student MLP expressiveness by enlarging the input feature space, addressing the limited expressiveness problem in graph classification.
- Mechanism: MuGSI incorporates Laplacian eigenvectors as node positional encoding to inject structural features into the input space, then uses a 1-hop GA-MLP as the student model. This approach provides the student with richer structural information while maintaining computational efficiency.
- Core assumption: The limited input feature space in graph classification datasets is a primary bottleneck for student MLP performance.
- Evidence anchors:
  - [abstract]: "MuGSI proposes to incorporate a node feature augmentation component, thereby enlarging the input feature space and enhancing the expressiveness of the student MLPs"
  - [section 4.3]: "To further address this issue, we propose using a 1-hop GA-MLP as a more expressive student model"
  - [corpus]: Weak - corpus neighbors focus on MLPs but don't specifically address feature augmentation
- Break condition: If the dataset already provides rich node features, additional Laplacian eigenvector features may provide diminishing returns or even noise.

### Mechanism 3
- Claim: The multi-granularity approach improves robustness to dynamic graph changes by capturing structural information at multiple scales that are less affected by local perturbations.
- Mechanism: By distilling knowledge at graph, cluster, and node levels, the framework creates a more robust representation that can handle node insertions/deletions. The 1-hop GA-MLP student with incremental computation capability is particularly efficient for dynamic updates.
- Core assumption: Structural knowledge distilled at multiple scales provides redundancy that improves robustness to topological changes.
- Evidence anchors:
  - [section 5.5]: "MuGSIGA_MLP* is more robust and less susceptible to topological changes" - shows experimental validation
  - [section 4.3]: "the random-walk path length is another key hyper-parameter in the path consistency loss L_P"
  - [corpus]: Weak - corpus neighbors don't address robustness to dynamic changes
- Break condition: If graph changes are too extensive (majority of nodes/edges modified), even multi-scale knowledge may not maintain accuracy.

## Foundational Learning

- Concept: Knowledge Distillation (KD) principles
  - Why needed here: Understanding how teacher-student knowledge transfer works is fundamental to grasping MuGSI's approach to transferring GNN knowledge to MLPs
  - Quick check question: What is the primary difference between response-based and feature-based knowledge distillation?

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The framework relies on understanding how GNNs aggregate neighborhood information and create graph-level representations
  - Quick check question: How does a GNN create a graph-level representation from node features?

- Concept: Graph clustering and community detection
  - Why needed here: The subgraph-level distillation component uses clustering to identify meaningful substructures for knowledge transfer
  - Quick check question: What is the Louvain method and why is it suitable for clustering in MuGSI?

## Architecture Onboarding

- Component map:
  - Teacher GNN (GIN/GCN/KPGIN) -> Preprocessing (clustering, Laplacian eigenvectors, 1-hop features) -> Student model (MLP/GA-MLP) -> Multi-granularity loss components (L_G, L_C, L_P) -> Optimization (weighted combination)

- Critical path:
  1. Pre-train teacher GNN on dataset
  2. Preprocess graphs: compute clustering, Laplacian eigenvectors, 1-hop features
  3. Initialize student model
  4. For each epoch: compute multi-granularity losses, backpropagate, update student
  5. Evaluate student performance on test set

- Design tradeoffs:
  - Clustering granularity vs. computational cost: Finer clustering captures more detail but increases complexity
  - Random walk path length: Longer paths capture more context but introduce noise
  - Feature augmentation: Laplacian eigenvectors add expressiveness but increase input dimension
  - Student model choice: GA-MLP more expressive than MLP but slightly more complex

- Failure signatures:
  - Training instability: Loss components dominating others (check weight balancing)
  - Poor performance: Insufficient clustering quality or inappropriate random walk length
  - Memory issues: High clustering density or excessive feature augmentation
  - Slow convergence: Inadequate learning rate or poor initialization

- First 3 experiments:
  1. Baseline comparison: Train student MLP with only soft logits distillation (GLNN baseline)
  2. Component ablation: Train with only graph-level distillation to measure individual component impact
  3. Dynamic robustness test: Remove and reinsert nodes sequentially, measure prediction stability and inference time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MuGSI's multi-granularity distillation approach compare to other knowledge distillation methods specifically designed for graph classification tasks?
- Basis in paper: [explicit] The paper mentions that existing GNN-to-MLP KD methods are primarily designed for node classification and do not address the unique challenges of graph classification.
- Why unresolved: The paper only compares MuGSI to baselines that are not specifically designed for graph classification, such as GLNN_MLP and NOSMOG. It does not provide a direct comparison with other methods tailored for graph classification.
- What evidence would resolve it: A comprehensive evaluation of MuGSI against other KD methods specifically designed for graph classification, using the same datasets and metrics.

### Open Question 2
- Question: How does the choice of clustering algorithm affect the performance of MuGSI's subgraph-level distillation?
- Basis in paper: [explicit] The paper uses the Louvain method for clustering but does not explore the impact of different clustering algorithms on performance.
- Why unresolved: The paper does not provide an ablation study or comparison of different clustering algorithms in the context of MuGSI.
- What evidence would resolve it: An experimental comparison of MuGSI's performance using different clustering algorithms (e.g., k-means, spectral clustering) on the same datasets and with the same teacher/student models.

### Open Question 3
- Question: What is the optimal number of random walk paths to sample for MuGSI's node-level distillation, and how does it vary across different graph datasets?
- Basis in paper: [explicit] The paper uses a fixed number of random walk paths (R=8) but does not explore the impact of varying this parameter.
- Why unresolved: The paper does not provide an ablation study or sensitivity analysis for the number of random walk paths sampled.
- What evidence would resolve it: An experimental analysis of MuGSI's performance with different numbers of random walk paths sampled, across various graph datasets with different characteristics (e.g., size, density, average path length).

## Limitations
- Reliance on teacher GNN quality: Performance depends heavily on the quality of pre-trained teacher GNN models
- Clustering dependency: Effectiveness of subgraph-level distillation depends on clustering algorithm's ability to identify meaningful substructures
- Computational overhead: Multi-granularity distillation, particularly random walk sampling, may be prohibitive for very large graphs

## Confidence
- **High**: The core mechanism of multi-granularity distillation combining graph, subgraph, and node-level losses
- **Medium**: The effectiveness of node feature augmentation using Laplacian eigenvectors
- **Medium**: The robustness claims regarding dynamic graph changes

## Next Checks
1. **Component Ablation Study**: Systematically disable each distillation component (graph-level, subgraph-level, node-level) individually and measure performance degradation to quantify the contribution of each component.

2. **Teacher Model Sensitivity**: Test MuGSI with multiple teacher GNN architectures (not just GIN/GCN/KPGIN) and varying teacher model qualities to establish robustness to teacher performance variations.

3. **Dynamic Graph Stress Test**: Design a controlled experiment where nodes/edges are systematically removed and reinserted in varying proportions, measuring both accuracy degradation and inference time to validate robustness claims.