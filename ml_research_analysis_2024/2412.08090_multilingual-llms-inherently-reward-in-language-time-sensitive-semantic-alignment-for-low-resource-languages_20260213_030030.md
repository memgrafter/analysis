---
ver: rpa2
title: Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic Alignment
  for Low-Resource Languages
arxiv_id: '2412.08090'
source_url: https://arxiv.org/abs/2412.08090
tags:
- temporal
- clitssa
- cross-lingual
- across
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the disparity in temporal reasoning capabilities
  between resource-rich and low-resource languages for large language models (LLMs).
  The authors identify that multilingual sentence transformers exhibit superior in-language
  semantic similarity compared to cross-lingual similarity when retrieving time-sensitive
  examples for low-resource languages in cross-lingual in-context learning (X-ICL)
  setups.
---

# Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic Alignment for Low-Resource Languages

## Quick Facts
- arXiv ID: 2412.08090
- Source URL: https://arxiv.org/abs/2412.08090
- Authors: Ashutosh Bajpai; Tanmoy Chakraborty
- Reference count: 17
- Low-resource languages show 11.41-62.92% relative F1 improvements with CLiTSSA

## Executive Summary
This paper addresses a critical limitation in cross-lingual in-context learning (X-ICL) for low-resource languages: multilingual LLMs inherently favor in-language time-sensitive semantic similarity over cross-lingual similarity when retrieving examples. The authors propose CLiTSSA, a method that fine-tunes multilingual retrievers to better align time-sensitive semantic contexts across languages. Through extensive experiments on the mTEMPREASON benchmark covering Romanian, German, and French, CLiTSSA demonstrates significant performance improvements over baselines, with relative mean F1 score enhancements of 11.41%, 30.77%, and 62.92% respectively across three temporal tasks. The method also shows robustness across different LLMs and reduces the performance gap between monolingual and cross-lingual scenarios.

## Method Summary
The authors identify that multilingual sentence-BERT models naturally reward in-language time-sensitive semantic similarity over cross-lingual similarity for low-resource languages in X-ICL setups. To address this, they propose CLiTSSA (Cross-Lingual Time-Sensitive Semantic Alignment), which fine-tunes the retriever using a parallel corpus of cross-lingual query pairs with similarity scores in the low-resource language embedding space. The training data is constructed by selecting top-h similar examples and w random examples for each low-resource query, then replacing translated queries with original English ones. This approach transfers profound semantic space knowledge from monolingual to cross-lingual embedding spheres, enabling better retrieval of semantically aligned examples across languages for temporal reasoning tasks.

## Key Results
- CLiTSSA achieves 11.41%, 30.77%, and 62.92% relative mean F1 score improvements for Romanian, German, and French respectively over X-InSTA baseline
- The method demonstrates consistent improvements across three temporal reasoning tasks (L1: Time-Time, L2: Time-Event, L3: Event-Event)
- CLiTSSA shows robustness across four different LLMs (LLaMA3-8B, Mistral, Vicuna, Bloomz) with minimal performance variance
- Lower-level tasks (L1) provide transferable semantic alignment benefits to more complex tasks (L2, L3), with 13.5% and 14.0% improvements respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual sentence-BERT models inherently reward in-language time-sensitive semantic similarity over cross-lingual similarity when retrieving examples for low-resource languages in X-ICL setups.
- Mechanism: The model computes similarity scores in the low-resource language embedding space, so translated English examples aligned to Romanian queries produce higher similarity scores than direct English-to-Romanian semantic matching.
- Core assumption: The multilingual transformer's embedding space preserves stronger intra-language semantic relationships for time-sensitive queries than cross-language ones.
- Evidence anchors:
  - [abstract] "our investigation reveals that LLMs intrinsically reward in-language semantically aligned cross-lingual instances over direct cross-lingual semantic alignments"
  - [section] "multilingual encoder-only transformers exhibit profound semantic similarity for temporal queries in an in-language similarity framework, outperforming their counterparts in a cross-lingual similarity context"
  - [corpus] Weak - no direct supporting paper found in neighbor corpus.
- Break condition: If the retriever is fine-tuned to bridge the semantic gap (as CLiTSSA does), the advantage of in-language similarity disappears.

### Mechanism 2
- Claim: CLiTSSA improves retrieval by fine-tuning the multilingual sentence-BERT to align time-sensitive semantic contexts across languages.
- Mechanism: Supervised fine-tuning with a parallel corpus of cross-lingual query pairs and similarity scores in the low-resource language embedding space forces the model to produce similar embeddings for semantically aligned queries across languages.
- Core assumption: The monolingual embedding space of the low-resource language captures time-sensitive semantic nuances that can be transferred to the cross-lingual space through fine-tuning.
- Evidence anchors:
  - [section] "we elect to apply the transfer of profound semantic space knowledge within a language to a cross-lingual semantic space for queries influenced by temporality"
  - [section] "the transition of the semantic context from a monolingual to a cross-lingual embedding sphere is attained"
  - [corpus] No direct match in neighbor corpus, but related to cross-lingual alignment work.
- Break condition: If the training corpus doesn't adequately represent the time-sensitive semantic distribution, the fine-tuned retriever may not generalize.

### Mechanism 3
- Claim: CLiTSSA generalizes across temporal tasks, with lower-level tasks (L1) providing transferable semantic alignment benefits to more complex tasks (L2, L3).
- Mechanism: Fine-tuning on L1 task creates a semantic alignment model that captures fundamental temporal reasoning patterns, which partially transfers to L2 and L3 due to shared underlying semantic structures.
- Core assumption: Lower-level temporal tasks share common semantic alignment patterns that are learnable and transferable to higher-level tasks.
- Evidence anchors:
  - [section] "the temporal alignment acquired through the lower-level temporal task (L1) can significantly enhance the relative F1-score of the more complex tasks L2 and L3 by 13.5% and 14.0%, respectively"
  - [corpus] No direct evidence in neighbor corpus.
- Break condition: If higher-level tasks require semantic patterns not present in lower-level tasks, transfer benefits diminish.

## Foundational Learning

- Concept: Cross-lingual in-context learning (X-ICL)
  - Why needed here: Understanding how examples from high-resource languages can be used to support queries in low-resource languages
  - Quick check question: What is the main challenge that X-ICL addresses in multilingual LLM applications?

- Concept: Semantic alignment in embedding spaces
  - Why needed here: The core mechanism relies on how semantically similar queries map to similar positions in embedding space
  - Quick check question: Why does the paper claim in-language similarity outperforms cross-lingual similarity for temporal queries?

- Concept: Supervised fine-tuning of retrievers
  - Why needed here: CLiTSSA's approach depends on fine-tuning multilingual sentence-BERT with parallel corpora and similarity scores
  - Quick check question: What is the purpose of the h and w parameters in constructing the training dataset for CLiTSSA?

## Architecture Onboarding

- Component map: Multilingual Sentence-BERT retriever -> Parallel corpus generator -> CLiTSSA fine-tuning module -> LLM prompt generator -> Evaluation pipeline
- Critical path: Generate parallel corpus → Fine-tune retriever → Retrieve semantically aligned examples → Construct X-ICL prompt → Generate answer → Evaluate
- Design tradeoffs:
  - Using translated examples vs. direct cross-lingual similarity
  - Fine-tuning vs. using off-the-shelf multilingual models
  - Task-specific vs. integrated retrievers across languages/tasks
  - Quadratic complexity vs. top-h sampling in training data construction
- Failure signatures:
  - Low improvement over baselines indicates retriever not learning time-sensitive alignment
  - Degraded performance on higher-level tasks suggests poor transfer learning
  - High variance across runs suggests unstable training or insufficient data
- First 3 experiments:
  1. Compare in-language vs. cross-lingual similarity retrieval on L1 task without fine-tuning
  2. Fine-tune CLiTSSA on L1 task and evaluate on L1 only
  3. Test cross-task generalization by evaluating L1-fine-tuned CLiTSSA on L2 and L3 tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the degree of similarity in embedding space achieved through fine-tuning and the resulting performance gains in temporal reasoning tasks across different languages?
- Basis in paper: [inferred] The paper demonstrates improved performance through CLiTSSA but does not quantify the direct relationship between embedding space similarity metrics and F1 score improvements.
- Why unresolved: The study focuses on end-to-end performance metrics but does not provide a detailed analysis of how changes in embedding space similarity correlate with performance improvements across different tasks and languages.
- What evidence would resolve it: A comprehensive analysis showing correlation coefficients between embedding space similarity metrics (e.g., cosine similarity distributions) and F1 score improvements across different tasks and languages.

### Open Question 2
- Question: How does the performance of CLiTSSA scale with increasing amounts of training data in low-resource languages?
- Basis in paper: [explicit] The paper does not explore the impact of varying training data sizes on CLiTSSA's performance.
- Why unresolved: The study uses a fixed amount of training data but does not investigate how performance scales with more or less training data, which is crucial for understanding the method's effectiveness in truly low-resource settings.
- What evidence would resolve it: Experiments showing performance metrics (e.g., F1 scores) across different sizes of training datasets for each language, demonstrating a learning curve or saturation point.

### Open Question 3
- Question: What is the impact of temporal reasoning performance when using CLiTSSA with languages that have different typological features compared to Romanian, German, and French?
- Basis in paper: [inferred] The study focuses on three specific low-resource languages but does not address how the method performs with languages that have different linguistic structures.
- Why unresolved: The paper's results are limited to languages with relatively similar typological features, leaving open the question of CLiTSSA's effectiveness with languages that have significantly different grammatical or semantic structures.
- What evidence would resolve it: Testing CLiTSSA on a diverse set of low-resource languages with varying typological features (e.g., agglutinative, polysynthetic, or languages with different word orders) and comparing the performance gains to those observed in the original study.

## Limitations

- The fundamental mechanism for why multilingual transformers favor in-language similarity remains unproven and could be model-specific rather than a general property
- The similarity scoring mechanism between translated and original queries isn't fully specified, which is critical for the training data construction
- The robustness claims across different LLMs are based on limited testing (only 4 models), potentially overstating generalizability

## Confidence

- In-language superiority observation: High (strong empirical support)
- Mechanism explanation: Medium (plausible but not rigorously tested)
- Cross-task generalization: Medium (supported by results but mechanism unclear)
- LLM-agnostic robustness: Low-Medium (limited model diversity tested)

## Next Checks

1. **Embedding space analysis**: Compare the angular distance distributions between semantically aligned and unaligned pairs in both monolingual and cross-lingual spaces to quantify the "profound semantic similarity" claim

2. **Model ablation study**: Test CLiTSSA with different multilingual retrievers (e.g., multilingual-BERT, XLM-R) to determine if the improvement is model-specific or general

3. **Cross-lingual transfer direction**: Reverse the training setup - fine-tune on English-to-low-resource pairs instead of low-resource-to-English - to test if the direction of semantic transfer matters for the observed improvements