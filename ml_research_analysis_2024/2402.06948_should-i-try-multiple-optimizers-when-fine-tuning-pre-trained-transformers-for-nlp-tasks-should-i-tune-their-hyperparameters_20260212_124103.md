---
ver: rpa2
title: Should I try multiple optimizers when fine-tuning pre-trained Transformers
  for NLP tasks? Should I tune their hyperparameters?
arxiv_id: '2402.06948'
source_url: https://arxiv.org/abs/2402.06948
tags:
- optimizers
- learning
- tuning
- sgdm
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether it is beneficial to try multiple
  optimizers and/or tune their hyperparameters when fine-tuning pre-trained Transformers
  for NLP tasks. Experiments with five GLUE datasets, two models (DistilBERT and DistilRoBERTa),
  and seven popular optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW,
  and AdaBound) show that when hyperparameters are tuned, there is no substantial
  difference in test performance across the five adaptive optimizers, despite differences
  in training loss.
---

# Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters?

## Quick Facts
- arXiv ID: 2402.06948
- Source URL: https://arxiv.org/abs/2402.06948
- Authors: Nefeli Gkouti; Prodromos Malakasiotis; Stavros Toumpis; Ion Androutsopoulos
- Reference count: 16
- One-line primary result: Tuning just the learning rate is as good as tuning all hyperparameters for adaptive optimizers when fine-tuning Transformers.

## Executive Summary
This paper investigates whether trying multiple optimizers and/or tuning their hyperparameters is beneficial when fine-tuning pre-trained Transformers for NLP tasks. Through extensive experiments with five GLUE datasets, two distilled models (DistilBERT and DistilRoBERTa), and seven popular optimizers, the authors find that when hyperparameters are tuned, adaptive optimizers (Adam, Nadam, AdamW, AdaMax, AdaBound) perform similarly in terms of test accuracy despite differences in training loss. The study recommends picking any well-behaved adaptive optimizer and tuning only its learning rate, as this is as effective as tuning all hyperparameters in most cases. When no hyperparameter tuning is possible, SGD with Momentum is the best choice.

## Method Summary
The authors conducted experiments comparing seven optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, AdaBound) on five GLUE tasks using DistilBERT and DistilRoBERTa models. They evaluated three scenarios: using default hyperparameters, tuning all hyperparameters, and tuning only the learning rate. Hyperparameter tuning was performed using Optuna with 30 trials per optimizer-task combination. The study measured test performance across five random data splits to account for variability.

## Key Results
- When hyperparameters are tuned, adaptive optimizers show similar test performance despite differences in training loss
- Tuning only the learning rate is as effective as tuning all hyperparameters for adaptive optimizers in most cases
- SGD with Momentum is the best choice when no hyperparameters can be tuned
- AdaBound occasionally works well with default hyperparameters due to its gradual transformation from Adam to SGD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive optimizers perform similarly when hyperparameters are tuned
- Mechanism: Adaptive optimizers dynamically adjust learning rates per parameter, allowing them to navigate complex loss landscapes effectively. Tuning hyperparameters levels the playing field among these methods.
- Core assumption: Adaptive nature compensates for differences in update rules when hyperparameters are well-tuned
- Evidence anchors: Abstract states no substantial difference in test performance across adaptive optimizers when hyperparameters are tuned; adaptive optimizers have almost identical development score curves

### Mechanism 2
- Claim: Tuning only the learning rate is sufficient for adaptive optimizers
- Mechanism: The learning rate is the most critical hyperparameter for optimization convergence. Other hyperparameters have secondary effects that can be handled by default values.
- Core assumption: Learning rate impact dominates over other hyperparameters for these tasks and models
- Evidence anchors: Abstract states tuning just the learning rate is as good as tuning all hyperparameters; section confirms tuning only the learning rate is much cheaper and equally effective

### Mechanism 3
- Claim: SGD with Momentum is best when no hyperparameters can be tuned
- Mechanism: SGDM has robust default hyperparameters that perform well across tasks. AdaBound combines benefits of Adam and SGD through gradual transformation.
- Core assumption: Default hyperparameters of SGDM and AdaBound are sufficiently good for general NLP fine-tuning tasks
- Evidence anchors: Abstract identifies SGDM as best choice when no hyperparameters can be tuned; section confirms SGDM is not affected by lack of hyperparameter tuning

## Foundational Learning

- Concept: Stochastic Gradient Descent (SGD) and its variants
  - Why needed here: Understanding baseline optimization methods and how they differ from adaptive optimizers is crucial for interpreting results
  - Quick check question: What is the main difference between SGD and SGD with Momentum (SGDM) in terms of how they update parameters?

- Concept: Adaptive optimization methods (Adam, AdaMax, Nadam, AdamW, AdaBound)
  - Why needed here: These are the primary focus of the study, and understanding their mechanisms explains why their performance converges when hyperparameters are tuned
  - Quick check question: How do adaptive optimizers adjust their learning rates differently from non-adaptive methods like SGD?

- Concept: Hyperparameter tuning and its impact on model performance
  - Why needed here: The study's core finding is about the effectiveness of tuning versus not tuning hyperparameters
  - Quick check question: Why might tuning only the learning rate be sufficient in many cases, rather than tuning all hyperparameters?

## Architecture Onboarding

- Component map: Datasets (GLUE tasks) -> Models (DistilBERT, DistilRoBERTa) -> Optimizers (7 variants) -> Hyperparameter tuning (Optuna) -> Evaluation (test scores)

- Critical path: 1) Load dataset and split into training/development/test sets, 2) Initialize model and optimizer with default hyperparameters, 3) Perform hyperparameter tuning using Optuna, 4) Train model with tuned hyperparameters, 5) Evaluate on test set and record scores, 6) Repeat for all optimizer-model combinations

- Design tradeoffs: Using DistilBERT and DistilRoBERTa instead of full-sized models for computational efficiency; limiting to five GLUE tasks to balance breadth and resource constraints; choosing 30 trials for hyperparameter tuning as compromise between thoroughness and computational cost

- Failure signatures: High variance in test scores across data splits indicating overfitting or instability; optimizer failing to converge (training loss plateaus or increases); significant discrepancies between training loss and evaluation score curves

- First 3 experiments: 1) Compare SGD and SGDM on SST-2 with default hyperparameters to establish baseline non-adaptive performance, 2) Test Adam with tuned hyperparameters on MRPC to demonstrate impact of hyperparameter tuning on adaptive methods, 3) Evaluate AdaBound with default hyperparameters on CoLA to assess its out-of-the-box performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different optimizers perform when fine-tuning larger Transformer models (e.g., BERT, RoBERTa) on diverse NLP tasks, including sequence-to-sequence and generation tasks?
- Basis in paper: Authors acknowledge experiments were limited to two lightweight encoder-only models and classification tasks, suggesting future work with larger models and diverse tasks
- Why unresolved: Authors did not have computational resources to experiment with larger models and tasks
- What evidence would resolve it: Experiments with various Transformer sizes and NLP tasks, comparing optimizer performance and hyperparameter tuning strategies

### Open Question 2
- Question: How does the choice of optimizer and hyperparameter tuning strategy affect training speed and convergence for different Transformer models and tasks?
- Basis in paper: Authors did not measure training speed or convergence, noting it could be inferred from learning curves but not varied explicitly
- Why unresolved: Authors focused on test performance and did not analyze training dynamics
- What evidence would resolve it: Experiments measuring training time and convergence rates for different optimizers and tuning strategies across models and tasks

### Open Question 3
- Question: How do non-constant learning rate schedules (e.g., cosine decay) interact with different optimizers and affect performance when fine-tuning Transformers?
- Basis in paper: Authors only used constant learning rate, mentioning non-constant schedules as potential future work
- Why unresolved: Authors limited experiments to constant learning rates to simplify analysis
- What evidence would resolve it: Experiments comparing constant and non-constant learning rate schedules for different optimizers across models and tasks

## Limitations

- Limited to GLUE classification tasks and distilled Transformer models, which may not generalize to other NLP tasks or larger models
- Only 30 hyperparameter tuning trials per optimizer-task combination may be insufficient to fully explore hyperparameter space
- Does not investigate impact of different learning rate schedules or effect of fine-tuning on downstream tasks with varying dataset sizes

## Confidence

- High confidence in the parity of adaptive optimizers when hyperparameters are tuned
- Medium confidence in the recommendation to tune only the learning rate
- Low confidence in the assertion that SGD with Momentum is best when no hyperparameters can be tuned

## Next Checks

1. Replicate the study with a broader range of NLP tasks, including generative and sequence-to-sequence models, to assess generalizability of findings
2. Investigate impact of learning rate schedules (e.g., cosine decay, warm-up) on performance of adaptive optimizers and effectiveness of tuning only the learning rate
3. Conduct experiments with full-sized pre-trained models (e.g., BERT, RoBERTa) to determine if observed patterns hold for larger models with more parameters