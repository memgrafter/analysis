---
ver: rpa2
title: Are Large Language Models More Honest in Their Probabilistic or Verbalized
  Confidence?
arxiv_id: '2408.09773'
source_url: https://arxiv.org/abs/2408.09773
tags:
- confidence
- llms
- probabilistic
- verbalized
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares how well large language models (LLMs) can
  express their confidence in answers using two methods: probability-based (token
  probability) and verbalized (using words like "certain" or "uncertain"). The study
  found that probabilistic confidence is generally more accurate than verbalized confidence,
  but it requires an in-domain dataset to set the right threshold for interpreting
  the probabilities.'
---

# Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?

## Quick Facts
- arXiv ID: 2408.09773
- Source URL: https://arxiv.org/abs/2408.09773
- Reference count: 24
- Primary result: Probabilistic confidence is more accurate than verbalized confidence but requires in-domain threshold calibration

## Executive Summary
This paper investigates how large language models express confidence in their answers using two methods: probabilistic (token probability) and verbalized (using words like "certain" or "uncertain"). The study finds that probabilistic confidence generally provides more accurate knowledge boundary perception than verbalized confidence, though it requires an in-domain dataset to set appropriate thresholds for binary decision-making. Both confidence methods perform better on less frequent questions, with probabilistic confidence showing larger improvements. However, LLMs struggle to accurately express their internal confidence in natural language, as evidenced by the weak correlation between verbalized and probabilistic confidence measures.

## Method Summary
The study compares probabilistic and verbalized confidence methods in LLMs for open-domain question answering. Using datasets like Natural Questions and Parent-Child datasets, the researchers elicit verbalized confidence through prompts and compute probabilistic confidence from token probabilities. They analyze the correlation between these confidence types using metrics like Alignment, Overconfidence, and Conservativeness, while also examining how question frequency affects confidence expression. The analysis is conducted across multiple LLM architectures including Llama2, Mistral, GPT-Instruct, and ChatGPT.

## Key Results
- Probabilistic confidence outperforms verbalized confidence in accuracy of knowledge boundary perception
- Both confidence methods perform better on less frequent questions
- Weak correlation exists between verbalized and probabilistic confidence at fine granularity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic confidence better reflects internal certainty than verbalized confidence
- Mechanism: Token-level probabilities aggregate internal model state more directly than verbalized expressions, which require additional transformation layers
- Core assumption: Lower perplexity indicates higher internal confidence in the answer
- Evidence anchors:
  - [abstract] "probabilistic confidence is generally more accurate than verbalized confidence"
  - [section] "probabilistic confidence is consistently much lower than the verbalized confidence, and the probabilistic alignment is significantly higher"
  - [corpus] No direct corpus evidence for this specific claim
- Break condition: If token probabilities become noisy due to temperature or decoding strategy changes

### Mechanism 2
- Claim: Question frequency affects model's perception accuracy
- Mechanism: Familiar questions have lower uncertainty in token probabilities, while rare questions create clearer distinctions between known and unknown knowledge
- Core assumption: Model's internal representation is more uncertain on common questions due to ambiguous patterns
- Evidence anchors:
  - [section] "Both perceptions perform better on less frequent questions"
  - [section] "LLMs achieve better QA performance which aligns with the previous findings in [2] and are more confident in the parent dataset compared to the child dataset"
  - [corpus] No corpus evidence directly addressing frequency effects
- Break condition: If question frequency patterns don't match training data distribution

### Mechanism 3
- Claim: Correlation between probabilistic and verbalized confidence is weak at fine granularity
- Mechanism: Verbalized confidence requires additional cognitive steps that introduce noise and inconsistency with underlying probabilistic state
- Core assumption: LLMs cannot perfectly translate internal probabilistic states into natural language expressions
- Evidence anchors:
  - [abstract] "correlation between verbalized and probabilistic confidence is weak"
  - [section] "correlation between the model's probabilistic confidence and verbalized confidence is relatively low although there is an overall trend"
  - [corpus] No corpus evidence directly measuring this correlation
- Break condition: If prompting strategies improve alignment between verbalized and probabilistic expressions

## Foundational Learning

- Concept: Probability calibration in neural networks
  - Why needed here: Understanding why token probabilities need threshold adjustment for reliable binary decisions
  - Quick check question: What does a well-calibrated probability distribution mean in classification tasks?

- Concept: Spearman and Kendall correlation coefficients
  - Why needed here: These metrics measure rank correlation between probabilistic and verbalized confidence
  - Quick check question: How do Spearman and Kendall coefficients differ in handling tied ranks?

- Concept: Question frequency effects on model performance
  - Why needed here: Understanding why common vs rare questions affect knowledge boundary perception
  - Quick check question: What is the relationship between training data frequency and model confidence?

## Architecture Onboarding

- Component map: Question text -> LLM inference with probability tracking -> Answer generation + Token probabilities -> Confidence calculation -> Output

- Critical path: Question → LLM inference → Answer generation + Token probabilities → Confidence calculation → Output

- Design tradeoffs:
  - Probabilistic vs verbalized: Accuracy vs interpretability
  - Threshold selection: Domain specificity vs generalization
  - Temperature setting: Raw probability access vs controlled generation

- Failure signatures:
  - Low alignment between probabilistic and verbalized confidence
  - Overconfidence on common questions
  - Threshold selection causing calibration issues

- First 3 experiments:
  1. Compare probabilistic vs verbalized confidence accuracy on NQ dataset with different thresholds
  2. Test question frequency effects using Parent-Child dataset split
  3. Measure correlation between confidence types using Spearman and Kendall coefficients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be trained or prompted to more accurately verbalize their confidence in answers, bridging the gap between probabilistic and verbalized confidence?
- Basis in paper: [explicit] The paper states that "it is challenging for LLMs to accurately express their internal confidence in natural language" and that verbalized confidence has a weak correlation with probabilistic confidence.
- Why unresolved: The paper demonstrates the problem but does not provide a solution for improving verbalized confidence expression.
- What evidence would resolve it: Research showing improved methods for training or prompting LLMs to verbalize confidence more accurately, with strong correlation between verbalized and probabilistic confidence.

### Open Question 2
- Question: What are the underlying reasons for the performance difference between probabilistic and verbalized confidence in LLMs, and how can these insights inform model design?
- Basis in paper: [explicit] The paper finds that "LLMs' probabilistic perception is generally more accurate than verbalized perception" but requires an in-domain validation set for threshold adjustment.
- Why unresolved: The paper identifies the performance difference but does not explore the root causes or implications for model architecture.
- What evidence would resolve it: Studies analyzing the internal mechanisms of LLMs that lead to different confidence expressions and how these can be leveraged in model design.

### Open Question 3
- Question: How does the frequency of questions affect the reliability of LLMs' knowledge boundaries, and what strategies can be employed to improve perception on both common and uncommon questions?
- Basis in paper: [explicit] The paper shows that "LLMs' perception levels decline on more familiar questions" and that both confidence methods perform better on less frequent questions.
- Why unresolved: The paper observes the phenomenon but does not propose methods to address the reliability issues on different question frequencies.
- What evidence would resolve it: Research demonstrating effective techniques to enhance LLMs' knowledge boundary perception across varying question frequencies.

## Limitations

- Probabilistic confidence requires in-domain threshold calibration, limiting practical deployment
- Weak correlation between verbalized and probabilistic confidence suggests fundamental expression challenges
- Question frequency effects are observed but underlying mechanisms remain partially understood

## Confidence

- **High**: Probabilistic confidence outperforms verbalized confidence in knowledge boundary perception accuracy
- **Medium**: Question frequency significantly affects confidence expression quality
- **Medium**: Weak correlation between verbalized and probabilistic confidence at fine granularity

## Next Checks

1. **Cross-domain threshold calibration**: Test whether the in-domain threshold requirement for probabilistic confidence can be relaxed through domain adaptation techniques or meta-learning approaches.

2. **Multi-token probability aggregation**: Evaluate different methods for aggregating token probabilities (geometric mean, weighted averaging, entropy-based measures) to determine if current aggregation methods are optimal for confidence estimation.

3. **Fine-grained correlation analysis**: Conduct a detailed analysis of when and why verbalized confidence diverges from probabilistic confidence, examining specific question types, answer formats, and model architectures to identify systematic patterns in the misalignment.