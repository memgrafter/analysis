---
ver: rpa2
title: 'EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning'
arxiv_id: '2403.10692'
source_url: https://arxiv.org/abs/2403.10692
tags:
- explorer
- learning
- agent
- rules
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EXPLORER, a neurosymbolic agent for textual
  reinforcement learning in text-based games. The agent combines neural exploration
  with symbolic exploitation, learning interpretable policies in the form of Answer
  Set Programming rules.
---

# EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.10692
- Source URL: https://arxiv.org/abs/2403.10692
- Reference count: 13
- Primary result: Superior performance compared to purely neural and other neurosymbolic baselines on TextWorld games with interpretable policies in Answer Set Programming

## Executive Summary
EXPLORER is a neurosymbolic agent that combines neural exploration with symbolic exploitation for textual reinforcement learning in text-based games. The agent learns interpretable policies in the form of Answer Set Programming (ASP) rules while using a neural module for exploration. EXPLORER employs a novel information-gain based algorithm for rule generalization using WordNet hypernyms, enabling effective generalization over unseen entities. The agent demonstrates superior performance compared to purely neural and other neurosymbolic baselines on TextWorld cooking and TextWorld Commonsense games, while maintaining interpretability through learned ASP rules.

## Method Summary
EXPLORER operates through a two-module architecture: a neural module for exploration that collects state-action-reward tuples using an LSTM-based action selector, and a symbolic module for exploitation that learns interpretable policies using inductive logic programming and ASP. The agent first explores the environment to gather training data, then learns ASP rules from this data using ILP. These rules are generalized using WordNet's hypernym-hyponym relationships to handle unseen entities. When the symbolic module fails to provide a confident action, the neural module serves as a fallback. The entire process emphasizes non-monotonic reasoning to handle partially observable environments where agent beliefs may need to change with new evidence.

## Key Results
- Superior performance compared to purely neural and other neurosymbolic baselines on TextWorld cooking and TextWorld Commonsense games
- Better generalization over unseen entities through WordNet-based rule generalization
- Learning of interpretable policies in the form of Answer Set Programming rules with confidence scores
- Demonstrated importance of non-monotonic reasoning in partially observable environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EXPLORER improves generalization over unseen entities by using WordNet-based hypernym generalization of learned rules.
- Mechanism: The agent learns specific rules (e.g., "insert apple into fridge") during training, then generalizes these rules by replacing specific entities with their hypernyms from WordNet (e.g., "insert fruit into fridge"), allowing the agent to handle unseen but semantically similar entities.
- Core assumption: Unseen entities share meaningful hypernym relationships with training entities, making generalization via WordNet valid and useful.
- Evidence anchors:
  - [abstract] "EXPLORER uses a novel information-gain based algorithm for rule generalization using WordNet"
  - [section 4] "we lift the learned policies using WordNet's (Miller, 1995) hypernym-hyponym relations to get the generalized rules"
- Break condition: If unseen entities don't have meaningful hypernym relationships with training entities, or if hypernym generalization introduces too many false positives.

### Mechanism 2
- Claim: EXPLORER balances exploration and exploitation by combining neural and symbolic modules.
- Mechanism: The neural module explores the environment and collects state-action-reward tuples, while the symbolic module (using ASP) exploits learned rules to make decisions. When the symbolic module fails, the neural module serves as a fallback.
- Core assumption: The neural module can effectively explore to collect useful training data, and the symbolic module can learn meaningful policies from this data.
- Evidence anchors:
  - [abstract] "EXPLORER is neurosymbolic in nature, as it relies on a neural module for exploration and a symbolic module for exploitation"
  - [section 3] "EXPLORER works in a partially observable environment... the reasoning approach of EXPLORER is non-monotonic in nature"
- Break condition: If the neural module fails to explore effectively or if the symbolic module cannot learn useful policies from the collected data.

### Mechanism 3
- Claim: EXPLORER handles partially observable environments through non-monotonic reasoning with exceptions.
- Mechanism: The agent learns default rules with exceptions using negation as failure in ASP, allowing it to update beliefs when new information contradicts existing rules.
- Core assumption: Non-monotonic reasoning is necessary and beneficial in partially observable environments where agent beliefs may need to change with new evidence.
- Evidence anchors:
  - [abstract] "The work highlights the importance of non-monotonic reasoning in partially observable environments"
  - [section 3] "we can model this using a non-monotonic logic programming paradigm that supports default rules and exception to defaults"
- Break condition: If the environment is fully observable or if the complexity of maintaining exceptions outweighs the benefits.

## Foundational Learning

- Concept: Answer Set Programming (ASP)
  - Why needed here: ASP provides a framework for non-monotonic reasoning and default rules with exceptions, which is crucial for handling partially observable environments and learning interpretable policies.
  - Quick check question: What makes ASP particularly suitable for text-based games compared to other logical programming paradigms?

- Concept: Inductive Logic Programming (ILP)
  - Why needed here: ILP allows the agent to learn symbolic rules from examples (state-action-reward tuples), enabling the generation of interpretable policies from experience.
  - Quick check question: How does ILP differ from traditional machine learning approaches in terms of the form of learned models?

- Concept: WordNet hypernym-hyponym relationships
  - Why needed here: WordNet provides the semantic relationships necessary for generalizing learned rules to handle unseen entities, improving the agent's ability to generalize.
  - Quick check question: Why might WordNet be preferred over other knowledge bases for this type of rule generalization?

## Architecture Onboarding

- Component map: Context Encoder (GRU) -> Action Encoder (GRU) -> Neural Action Selector -> (Symbolic Action Selector fallback) -> Action Selection
- Critical path: Observation → Context Encoding → Action Encoding → Neural Action Selector → (Symbolic Action Selector fallback) → Action Selection
- Design tradeoffs:
  - Neural vs. Symbolic balance: Too much reliance on neural module reduces interpretability; too much on symbolic reduces flexibility
  - Generalization depth: More generalization improves OOD performance but increases false positives
  - Rule complexity: More complex rules capture more scenarios but are harder to learn and apply
- Failure signatures:
  - Poor performance on unseen entities: Likely indicates insufficient rule generalization or poor neural exploration
  - Slow convergence: May indicate inefficient exploration or overly complex rule learning
  - Inconsistent behavior: Could suggest conflicts between neural and symbolic modules
- First 3 experiments:
  1. Train EXPLORER on TW-Cooking level 1 without generalization to verify basic functionality
  2. Test EXPLORER on TW-Cooking with increasing levels of rule generalization to find optimal balance
  3. Evaluate EXPLORER on TWC with OOD entities to test generalization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we optimize the balance between neural exploration and symbolic exploitation in the EXPLORER agent to improve performance across different game environments?
- Basis in paper: [explicit] The paper discusses the importance of balancing neural and symbolic modules for effective decision-making in text-based games.
- Why unresolved: The paper highlights the need for an optimal strategy to switch between neural and symbolic modules but does not provide a concrete solution or framework for achieving this balance.
- What evidence would resolve it: Experimental results demonstrating improved performance in text-based games when using an optimized strategy for balancing neural exploration and symbolic exploitation.

### Open Question 2
- Question: What are the limitations of using WordNet for rule generalization in the EXPLORER agent, and how can these limitations be addressed to enhance the agent's performance on unseen entities?
- Basis in paper: [explicit] The paper mentions that excessive generalization can lead to false-positive results and that WordNet's hypernym-hyponym relations are used for rule generalization.
- Why unresolved: The paper identifies the issue of excessive generalization but does not provide a detailed analysis of WordNet's limitations or potential solutions to mitigate these limitations.
- What evidence would resolve it: A comprehensive study comparing the performance of the EXPLORER agent using different knowledge bases or rule generalization techniques, highlighting the impact on handling unseen entities.

### Open Question 3
- Question: How can the EXPLORER agent's policy learning be improved to handle environments with complex, multi-room structures and a high number of objects?
- Basis in paper: [inferred] The paper discusses the EXPLORER agent's performance on TextWorld Commonsense games with varying difficulty levels, including environments with multiple rooms and objects.
- Why unresolved: The paper shows that the EXPLORER agent performs well in simpler environments but does not explore strategies for improving performance in more complex, multi-room settings.
- What evidence would resolve it: Experimental results showing the EXPLORER agent's performance improvements in multi-room environments after implementing strategies to enhance policy learning and generalization.

## Limitations

- Limited evaluation on diverse text-based game environments beyond TextWorld cooking and commonsense games
- No detailed analysis of the precision-recall tradeoff in WordNet-based rule generalization
- Lack of comparison with pure symbolic baselines to isolate the neural module's contribution

## Confidence

- WordNet-based rule generalization: Medium confidence - algorithm described but precision-recall tradeoff not quantified
- Neural-symbolic cooperation: Medium confidence - individual module contributions not isolated through ablation studies
- Non-monotonic reasoning: Low confidence - limited empirical evidence of default rules with exceptions in practice

## Next Checks

1. Conduct ablation studies comparing EXPLORER's performance when using only neural, only symbolic, and the combined approach to quantify each component's contribution
2. Perform precision-recall analysis on the WordNet-based rule generalization to understand the tradeoff between generalization and false positives
3. Test EXPLORER on additional text-based game domains (e.g., TextWorld quests) to evaluate cross-domain generalization and rule transferability