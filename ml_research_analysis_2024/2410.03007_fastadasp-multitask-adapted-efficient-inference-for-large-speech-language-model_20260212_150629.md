---
ver: rpa2
title: 'FastAdaSP: Multitask-Adapted Efficient Inference for Large Speech Language
  Model'
arxiv_id: '2410.03007'
source_url: https://arxiv.org/abs/2410.03007
tags:
- token
- audio
- reduction
- tasks
- merge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FastAdaSP, a token reduction framework for
  efficient inference in multitask Speech Language Models (SpeechLMs). Unlike existing
  methods designed for other modalities, FastAdaSP addresses the unique temporal dependencies
  and sparsity characteristics of speech signals.
---

# FastAdaSP: Multitask-Adapted Efficient Efficient Inference for Large Speech Language Model

## Quick Facts
- arXiv ID: 2410.03007
- Source URL: https://arxiv.org/abs/2410.03007
- Authors: Yichen Lu; Jiaqi Song; Chao-Han Huck Yang; Shinji Watanabe
- Reference count: 40
- Primary result: Achieves 7x memory efficiency and 1.83x decoding throughput without performance degradation on SpeechLM tasks

## Executive Summary
FastAdaSP introduces a novel token reduction framework specifically designed for multitask Speech Language Models (SpeechLMs). Unlike existing methods developed for other modalities, FastAdaSP addresses the unique temporal dependencies and sparsity characteristics of speech signals through task-specific reduction strategies. The framework employs weighted token merging for dense tasks (ASR, ST) and aggressive token reduction with layer selection for sparse tasks (ER, SQA), achieving state-of-the-art efficiency-performance trade-offs without additional training requirements.

## Method Summary
FastAdaSP implements a multitask-adapted token reduction framework that applies different strategies to dense and sparse speech tasks. For dense tasks, it uses weighted token merging based on cosine similarity between adjacent audio tokens, preserving more task-relevant information than naive eviction. For sparse tasks, it employs an aggressive single-layer reduction strategy guided by transfer entropy-based layer selection. The framework operates during pre-filling in decoder blocks, after self-attention and before residual addition, without requiring additional training of the base SpeechLM models.

## Key Results
- Achieves 7x memory efficiency and 1.83x decoding throughput on WavLLM and Qwen-Audio models
- Maintains performance on Emotion Recognition and Spoken Question Answering tasks without degradation
- Outperforms existing token reduction methods (FastV, A-ToMe, H2O) across multiple speech tasks and models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted token merging preserves more task-relevant information than naive token eviction for dense tasks.
- Mechanism: FastAdaSP computes similarity between adjacent audio tokens using their key states and assigns merge weights based on cumulative attention scores across heads and modalities. This prioritizes merging redundant but contextually relevant tokens rather than simply dropping them.
- Core assumption: Adjacent tokens with high cosine similarity in key space are redundant and can be merged without loss of task-critical information.
- Evidence anchors:
  - [abstract]: "merging similar audio tokens can eliminate redundant audio information while preserving essential content"
  - [section 3.2]: Detailed description of similarity computation and weighted merge
  - [corpus]: Weak - no direct comparison to eviction baselines in cited works
- Break condition: If adjacent tokens are semantically distinct but acoustically similar (e.g., rapid speech transitions), merging may lose discriminative cues.

### Mechanism 2
- Claim: Transfer entropy-based layer selection identifies optimal layers for sparse task token reduction.
- Mechanism: FastAdaSP measures information loss at the final layer if weighted merge is applied at layer i, using the difference in entropy between final hidden states with and without reduction. Layers with smaller transfer entropy are selected for merging.
- Core assumption: Layers with lower transfer entropy contribute less to final task performance, so merging tokens at these layers causes minimal degradation.
- Evidence anchors:
  - [section 3.4]: "The smaller the TEi, the less the final information loss caused by the operation on layer i."
  - [section 4.3]: Empirical validation showing TE-based layer selection outperforms random selection
  - [corpus]: Weak - no corpus mention of TE-based selection in audio contexts
- Break condition: If task-relevant information is concentrated in low-entropy layers, selecting them for reduction will degrade performance.

### Mechanism 3
- Claim: Task-specific scheduling (decay for dense, single-layer for sparse) balances efficiency and accuracy.
- Mechanism: Dense tasks use a decay scheduler that linearly reduces merge ratio toward zero at the final layer, preventing aggressive early merging. Sparse tasks apply aggressive single-layer reduction where layer selection is guided by TE.
- Core assumption: Dense tasks require gradual token reduction to preserve sequence-to-sequence dependencies, while sparse tasks benefit from aggressive reduction in less informative layers.
- Evidence anchors:
  - [abstract]: "we designed a novel token reduction method with different strategies for each"
  - [section 3.3]: Describes decay vs constant scheduler for dense tasks and aggressive reduction for sparse tasks
  - [section 4.3]: Ablation shows scheduler choice significantly impacts dense task performance
- Break condition: If task semantics don't align with dense/sparse categorization, the scheduling strategy may be suboptimal.

## Foundational Learning

- Concept: Cosine similarity in key space for token importance
  - Why needed here: FastAdaSP uses cosine similarity between adjacent token keys to identify merge candidates, so understanding this metric is essential for interpreting the similarity score sequence P
  - Quick check question: Why does FastAdaSP use key states rather than value states when computing token similarity?

- Concept: Transfer entropy as information loss metric
  - Why needed here: The TE-based layer selection method relies on measuring entropy differences to estimate information loss, so familiarity with entropy calculations is required
  - Quick check question: How does approximating feature distribution as Gaussian simplify the entropy calculation in FastAdaSP?

- Concept: Attention-based merge weight computation
  - Why needed here: FastAdaSP computes merge weights using cumulative attention scores across heads and modalities, so understanding multi-head attention mechanisms is crucial
  - Quick check question: What role does the text embedding play in determining audio token importance during weighted merge?

## Architecture Onboarding

- Component map: Audio encoder → Multimodal attention blocks → FastAdaSP reduction module (per layer) → Decoder blocks → Output
- Critical path: Token reduction occurs during pre-filling in decoder blocks, specifically after the self-attention operation and before residual addition
- Design tradeoffs: Weighted merge preserves information better than eviction but requires additional computation for similarity and weight calculation; TE-based layer selection adds complexity but improves task-specific performance
- Failure signatures: Performance degradation on dense tasks when merge ratio is too aggressive; memory inefficiency when layer selection is suboptimal for sparse tasks
- First 3 experiments:
  1. Compare weighted merge vs random merge on ASR task at 30% reduction ratio
  2. Test TE-based layer selection vs random layer selection on ER task
  3. Evaluate decay vs constant scheduler impact on ST task at 50% reduction ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Transfer Entropy-based layer selection method perform on SpeechLM tasks beyond those tested (ER, SQA, ASR, ST, AC)?
- Basis in paper: [explicit] The paper introduces a Transfer Entropy-based layer selection method for sparse tasks and tests it on Qwen-Audio ER task
- Why unresolved: The paper only provides a limited analysis of the layer selection method's effectiveness, focusing on a single task and model
- What evidence would resolve it: Testing the Transfer Entropy-based layer selection method on a wider range of SpeechLM tasks and models, comparing its performance to other layer selection strategies

### Open Question 2
- Question: What is the impact of FastAdaSP on SpeechLM performance when dealing with audio containing multiple speakers or background noise?
- Basis in paper: [inferred] The paper mentions that FastAdaSP aims to preserve crucial audio information while reducing tokens, but it doesn't specifically address scenarios with multiple speakers or background noise
- Why unresolved: The paper doesn't provide experiments or analysis on how FastAdaSP handles complex audio scenarios that are common in real-world applications
- What evidence would resolve it: Testing FastAdaSP on datasets with multiple speakers and varying levels of background noise, evaluating its performance on tasks like speaker diarization and speech enhancement

### Open Question 3
- Question: How does FastAdaSP compare to other efficient inference methods that don't rely on token reduction, such as model quantization or knowledge distillation?
- Basis in paper: [explicit] The paper focuses on token reduction techniques and doesn't compare FastAdaSP to other efficient inference methods
- Why unresolved: The paper only benchmarks FastAdaSP against other token reduction methods, leaving a gap in understanding its relative performance compared to alternative approaches
- What evidence would resolve it: Implementing and comparing FastAdaSP to other efficient inference methods like model quantization or knowledge distillation, evaluating their trade-offs in terms of speed, memory usage, and accuracy on various SpeechLM tasks

## Limitations

- Assumes adjacent audio tokens with high cosine similarity are redundant and can be merged without loss of task-critical information, which may not hold for rapid speech transitions
- Performance depends heavily on the quality of similarity computation and accuracy of transfer entropy estimation, both relying on approximations that may introduce errors
- The 7x memory efficiency and 1.83x decoding throughput claims are based on specific model configurations and may not generalize to other SpeechLM architectures

## Confidence

**High Confidence**: The core mechanism of weighted token merging for dense tasks is well-supported by experimental results and convincingly demonstrates improvement over naive token eviction methods.

**Medium Confidence**: The transfer entropy-based layer selection for sparse tasks shows promising results but relies on assumptions about information flow that may not hold universally across all speech tasks.

**Low Confidence**: The generalizability of the dense/sparse task categorization and corresponding scheduling strategies across different SpeechLM architectures and task types remains uncertain.

## Next Checks

1. **Cross-Modal Transfer Validation**: Test FastAdaSP on a multitask SpeechLM that includes both speech and text tasks to evaluate whether the audio-specific token reduction strategies adversely affect multimodal performance.

2. **Extreme Reduction Stress Test**: Apply FastAdaSP at reduction ratios beyond 50% (up to 90%) on ASR tasks to identify the breaking point where performance degradation becomes unacceptable.

3. **Temporal Dependency Preservation Analysis**: Conduct a systematic study measuring the impact of token reduction on temporal dependencies in speech sequences by comparing attention patterns before and after FastAdaSP application.