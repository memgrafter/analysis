---
ver: rpa2
title: Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models
arxiv_id: '2407.10737'
source_url: https://arxiv.org/abs/2407.10737
tags:
- visual
- video
- https
- temporal
- neuronal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of modeling how the retina encodes
  dynamic natural visual scenes by establishing a spatiotemporal mapping between video
  inputs and retinal ganglion cell (RGC) spike responses. The proposed Vi-ST model
  leverages a self-supervised Vision Transformer (ViT) prior for spatial feature extraction
  and a spatiotemporal convolutional neural network (CNN) with causal dilated 3D convolutions
  for temporal modeling.
---

# Aligning Neuronal Coding of Dynamic Visual Scenes with Foundation Vision Models

## Quick Facts
- arXiv ID: 2407.10737
- Source URL: https://arxiv.org/abs/2407.10737
- Reference count: 40
- One-line primary result: Vi-ST achieves superior cross-video generalization with CC=0.334 compared to baseline models (0.108-0.101)

## Executive Summary
This study addresses the challenge of modeling how the retina encodes dynamic natural visual scenes by establishing a spatiotemporal mapping between video inputs and retinal ganglion cell (RGC) spike responses. The proposed Vi-ST model leverages a self-supervised Vision Transformer (ViT) prior for spatial feature extraction and a spatiotemporal convolutional neural network (CNN) with causal dilated 3D convolutions for temporal modeling. The model incorporates RGC receptive field information via a 3D AdaLN Zero module and employs Causal Multiscale Spatiotemporal (CMST) blocks to capture multi-scale temporal dynamics.

## Method Summary
Vi-ST uses frozen DINOv2 ViT features as spatial priors for video frames, which are then processed through Causal 3D TCN with exponentially increasing dilation for spatiotemporal modeling. The receptive field information is incorporated through 3D AdaLN Zero conditioning to align video features with neural response space. CMST blocks with multiple temporal scales capture different temporal dynamics in RGC responses. The model is trained with a Vi-ST loss combining RMSE, ReLU penalty, and SoftDTW components using AdamW optimizer with learning rate warm-up and cosine decay.

## Key Results
- Vi-ST achieves CC=0.334 for cross-video prediction (Mov1→Mov2), significantly outperforming baselines (0.108-0.101)
- Encoding larger populations of RGCs (90 neurons) improves prediction accuracy due to complementary coding effects
- Ablation studies confirm the importance of ViT prior and spatiotemporal modules for accurate predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-supervised ViT prior from DINOv2 provides robust spatial features that improve generalization across different video stimuli.
- Mechanism: DINOv2 is pre-trained on 1.2B images using a discriminative self-supervised method, yielding frozen feature representations with strong generality. These features are used as spatial priors for each video frame, providing a consistent and semantically rich representation that is not dependent on the specific video content.
- Core assumption: Pre-trained self-supervised vision models generalize better than task-specific models when transferring across different visual domains.
- Evidence anchors:
  - [abstract] "The ViT prior is derived from DINOv2, which employs a self-supervised approach for pre-training on a huge dataset containing 1.2B unique images, yielding a set of adversarial feature representations with strong generality and generalization capabilities."
  - [section 2.2] "DINOv2 has been proven to be a powerful universal visual feature extractor in various visual tasks"
- Break condition: If the pre-trained features do not align well with the spatiotemporal dynamics of retinal responses, or if the frozen features are too rigid to adapt to the specific characteristics of the neural data.

### Mechanism 2
- Claim: Causal 3D dilated convolutions effectively capture temporal dynamics while maintaining causality and increasing receptive fields.
- Mechanism: The C3TCN uses causal 3D convolutions with exponentially increasing dilation (2^i) along the time axis. This allows the model to capture long-range temporal dependencies while ensuring predictions at time t only depend on inputs up to time t. The 3D convolutions also maintain spatial relationships across frames.
- Core assumption: Temporal dynamics in retinal responses are best captured through causal, dilated convolutions that can model both local and long-range temporal patterns.
- Evidence anchors:
  - [section 3.1] "C3TCN extends the Residual 1D Dilated Convolution module of MSTCN by directly replacing the 1D CNN with Causal 3D CNN, maintaining MSTCN's approach of increasing receptive fields with layers."
  - [section 3.1] "C3TCN performs dilated convolution calculations only along the time dimension, while still keeping the spatial convolutional kernels size in 3×3"
- Break condition: If the temporal dynamics are not primarily causal, or if the exponential dilation schedule does not match the relevant temporal scales in the data.

### Mechanism 3
- Claim: Incorporating RGC receptive field information through 3D AdaLN Zero conditioning improves alignment between video features and neural response space.
- Mechanism: The receptive field (RF) is downsampled to match the feature map size and fused with video features using the 3D AdaLN Zero module. This conditioning factor ensures that the model accounts for the spatial sensitivity of each RGC, effectively aligning the voxel space of video spatiotemporal information with the neural response space.
- Core assumption: RGC receptive fields contain critical information about how visual features map to neural responses, and this information should be explicitly incorporated into the model architecture.
- Evidence anchors:
  - [section 3.2] "We aimed to incorporate the RGC receptive field information to better align video pixel spaces and RGC spike spaces. Here, the receptive field, RF ∈ R^{W×H}, is downsampled along the spatial axis to match the size of ϵ ∈ R^{T×g×g×C′}, and is used as a conditional factor for fusion with ϵ through the AdaLN Zero module"
  - [section 3.2] "we naturally extended AdaLN Zero to 3D AdaLN Zero"
- Break condition: If the receptive field information is not sufficiently informative about the mapping between pixels and spikes, or if the conditioning approach introduces noise rather than useful constraints.

## Foundational Learning

- Concept: Spatiotemporal modeling with 3D convolutions
  - Why needed here: Retinal responses to natural scenes involve both spatial patterns (receptive fields) and temporal dynamics (spiking patterns). 3D convolutions naturally capture both dimensions simultaneously.
  - Quick check question: What is the difference between causal and non-causal 3D convolutions in the context of neural response prediction?

- Concept: Vision Transformer feature extraction
  - Why needed here: ViTs provide strong spatial representations through self-attention mechanisms that can capture long-range dependencies in images. Using pre-trained ViT features as spatial priors provides a robust starting point for spatiotemporal modeling.
  - Quick check question: How does the hierarchical structure of ViT relate to different semantic levels in visual representations?

- Concept: Multi-scale temporal modeling
  - Why needed here: Retinal responses occur at multiple temporal scales, from rapid transients to sustained responses. The CMST blocks capture these different scales through branches with different temporal kernel sizes.
  - Quick check question: Why is it beneficial to have multiple temporal scales in the same model rather than using a single temporal resolution?

## Architecture Onboarding

- Component map: Video frames → DINOv2 (frozen) → C3TCN (causal 3D dilated) → 3D AdaLN Zero (conditioning with RF) → CMST blocks (multi-scale temporal) → Linear mapping (prediction)
- Critical path: Video frames → DINOv2 (frozen) → C3TCN (causal 3D dilated) → 3D AdaLN Zero (conditioning with RF) → CMST blocks (multi-scale temporal) → Linear mapping (prediction)
- Design tradeoffs: Using frozen ViT features trades adaptability for robustness and speed; causal convolutions ensure biological plausibility but may miss some backward temporal dependencies; multi-scale temporal modeling increases complexity but captures richer dynamics.
- Failure signatures: Poor cross-video generalization indicates inadequate spatial feature extraction; temporal prediction errors suggest insufficient temporal modeling; spatial misalignment between predictions and targets may indicate issues with receptive field conditioning.
- First 3 experiments:
  1. Test C3TCN with different dilation schedules (linear vs exponential) on a single video to identify optimal temporal receptive fields
  2. Compare performance with and without 3D AdaLN Zero conditioning to quantify the benefit of receptive field incorporation
  3. Evaluate different numbers of CMST blocks and temporal scales to find the optimal balance between complexity and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the SD-KL metric be improved to be less sensitive to noise while maintaining its ability to capture temporal dynamics in neural encoding tasks?
- Basis in paper: [explicit] The authors acknowledge that the SD-KL metric exhibits sensitivity to noise and demonstrates limited expressive power, identifying this as a limitation requiring further investigation.
- Why unresolved: The paper introduces SD-KL as a novel metric but does not provide solutions for its sensitivity issues or limitations in expressive power.
- What evidence would resolve it: Experimental results showing SD-KL's performance compared to other metrics under various noise conditions, along with proposed modifications to reduce noise sensitivity while preserving temporal information capture.

### Open Question 2
- Question: What are the key architectural differences between Vi-ST and biological retinal processing that limit its ability to fully capture the complexity of neural encoding?
- Basis in paper: [inferred] The paper compares Vi-ST to biological systems and discusses its capabilities, but does not systematically analyze the architectural gaps between the model and actual retinal processing.
- Why unresolved: While the paper demonstrates Vi-ST's effectiveness, it doesn't provide a detailed comparison of its architecture with biological retinal circuits to identify specific limitations.
- What evidence would resolve it: Comparative studies analyzing specific retinal mechanisms (e.g., lateral inhibition, gap junctions) and how they are or aren't captured in Vi-ST's architecture, along with experimental results showing performance differences.

### Open Question 3
- Question: How would Vi-ST's performance change when applied to more diverse video datasets and larger populations of recorded neurons?
- Basis in paper: [explicit] The authors note that Vi-ST's robustness needs confirmation through acquisition of more extensive datasets, including greater numbers of recorded cells and variety of video types.
- Why unresolved: The current study uses a specific dataset with 90 RGCs and two video clips, limiting generalizability to other conditions.
- What evidence would resolve it: Performance metrics (CC, SD-KL) from experiments using larger neuron populations and diverse video datasets, demonstrating whether Vi-ST maintains or improves its predictive capabilities.

## Limitations

- Cross-video generalization results (CC=0.334) remain relatively modest, suggesting the model may still capture video-specific rather than generalizable spatiotemporal patterns
- Frozen ViT features may limit the model's ability to adapt to specific characteristics of retinal processing and biological vision systems
- Limited dataset (2 videos, 90 cells) constrains the generalizability of findings to other visual conditions and neuron populations

## Confidence

- High confidence in the architectural framework and overall methodology
- Medium confidence in the specific quantitative improvements over baselines
- Low confidence in the biological interpretability of the learned representations

## Next Checks

1. Test model performance on held-out time segments from the same videos to distinguish between true generalization and overfitting to specific spatiotemporal patterns
2. Evaluate feature similarity between ViT representations and known retinal feature selectivity (e.g., center-surround, orientation tuning) using representational similarity analysis
3. Conduct ablation studies systematically removing each major component (ViT prior, receptive field conditioning, multi-scale temporal modeling) to quantify their individual contributions beyond statistical improvements