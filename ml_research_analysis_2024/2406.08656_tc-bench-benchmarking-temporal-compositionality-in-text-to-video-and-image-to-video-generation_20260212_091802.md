---
ver: rpa2
title: 'TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video
  Generation'
arxiv_id: '2406.08656'
source_url: https://arxiv.org/abs/2406.08656
tags:
- video
- frame
- generation
- videos
- transition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TC-Bench, a benchmark for evaluating temporal
  compositionality in video generation models. Unlike existing benchmarks that focus
  on simple actions or static compositions, TC-Bench specifically targets scenarios
  where object attributes, relations, or background scenes change over time.
---

# TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation

## Quick Facts
- arXiv ID: 2406.08656
- Source URL: https://arxiv.org/abs/2406.08656
- Reference count: 40
- Primary result: Current video generation models achieve less than 20% transition completion rates on temporal compositionality tasks

## Executive Summary
This paper introduces TC-Bench, a benchmark for evaluating temporal compositionality in video generation models. Unlike existing benchmarks that focus on simple actions or static compositions, TC-Bench specifically targets scenarios where object attributes, relations, or background scenes change over time. The benchmark includes carefully crafted text prompts, corresponding ground truth videos, and robust evaluation metrics. The authors propose two new metrics, TCR and TC-Score, which use vision language models to verify compositional transitions in generated videos. These metrics show significantly higher correlations with human judgments compared to existing metrics. Comprehensive experiments reveal that current video generation models achieve less than 20% of the compositional changes, highlighting significant room for improvement.

## Method Summary
The authors introduce TC-Bench, a benchmark with 150 text-to-video prompts and 120 image-to-video prompt-video pairs, along with corresponding ground truth videos collected from YouTube. They develop two new metrics (TCR and TC-Score) that use vision language models to generate and verify frame-level assertions about compositional changes. The evaluation framework involves generating videos using 9 different models, then computing TCR (transition completion ratio) and TC-Score (text-video alignment score) metrics. They also introduce a simple baseline (SDXL+SEINE) that generates consistent start and end frames to guide video generation, improving temporal compositionality performance.

## Key Results
- Current video generation models achieve less than 20% transition completion rates on TC-Bench
- TC metrics (TCR and TC-Score) show significantly higher correlation with human judgments than existing metrics
- The SDXL+SEINE baseline improves transition completion rates over direct text-to-video generation
- Models show specific weaknesses in handling different types of temporal compositionality (attribute transitions, object relation changes, background shifts)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The TCR and TC-Score metrics achieve higher correlation with human judgments because they use vision language models to generate and verify frame-level assertions about compositional changes.
- **Mechanism:** The metrics first generate detailed assertions about specific frames (e.g., "Is there a brown chameleon?") and then use VLMs to verify these assertions against generated video frames. This process captures the temporal compositionality that existing metrics miss.
- **Core assumption:** VLMs can reliably verify compositional changes in video frames when provided with clear, targeted assertions.
- **Evidence anchors:**
  - [abstract] "We also develop new metrics to measure the completeness of component transitions in generated videos, which demonstrate significantly higher correlations with human judgments than existing metrics."
  - [section] "We use GPT-4 to generate N index-assertion pairs... To verify each assertion Ai, we input Ai and the corresponding video frames Ii = {Ik|k ∈ K i} to a VLM [1] fVLM."
- **Break condition:** VLMs become unreliable at verifying complex compositional changes, or assertion generation fails to capture the key temporal transitions.

### Mechanism 2
- **Claim:** The SDXL+SEINE baseline improves temporal compositionality by generating consistent start and end frames that guide the video generation process.
- **Mechanism:** The method generates start and end frames using SDXL, then uses these frames as boundary conditions for SEINE to interpolate between them. This constrains the video generation to maintain compositional consistency across time.
- **Core assumption:** Having consistent boundary frames significantly improves the ability of video generation models to maintain temporal compositionality.
- **Evidence anchors:**
  - [abstract] "We introduce a simple and effective baseline to improve the transition completion rate over text-to-video generation models."
  - [section] "We apply the same noise map zT as the initialized noise pattern of both diffusion paths and substitute the self-attention maps of IK with maps from I0's diffusion trajectory for the first half of the timesteps."
- **Break condition:** The boundary frames fail to capture the necessary compositional information, or SEINE cannot interpolate effectively between them.

### Mechanism 3
- **Claim:** The three types of temporal compositionality (attribute transition, object relation change, background shifts) provide a comprehensive evaluation framework that reveals specific weaknesses in current models.
- **Mechanism:** By categorizing prompts into these three types, the benchmark can identify whether models struggle more with attribute changes, spatial relationships, or background dynamics. This targeted approach reveals specific architectural limitations.
- **Core assumption:** Different types of compositional changes require different capabilities from video generation models, and models will show specific weaknesses for each type.
- **Evidence anchors:**
  - [abstract] "TC-Bench, a benchmark of meticulously crafted text prompts, corresponding ground truth videos, and robust evaluation metrics."
  - [section] "We first define three categories of temporal compositionality... Attribute Transition, Object Relation Change, Background Shifts."
- **Break condition:** The three categories don't adequately capture the space of temporal compositionality, or models don't show differentiated performance across categories.

## Foundational Learning

- **Concept: Temporal compositionality in video generation**
  - Why needed here: Understanding how objects, attributes, and relations change over time is fundamental to evaluating video generation models.
  - Quick check question: What distinguishes temporal compositionality from spatial compositionality in video generation?

- **Concept: Vision language models for video understanding**
  - Why needed here: The evaluation metrics rely on VLMs to verify compositional changes in video frames.
  - Quick check question: How do VLMs differ from traditional computer vision models when analyzing video content?

- **Concept: Diffusion models for video generation**
  - Why needed here: Most baseline models use diffusion architectures, and understanding their limitations is crucial for interpreting results.
  - Quick check question: What are the key challenges in extending diffusion models from image to video generation?

## Architecture Onboarding

- **Component map:**
  - TC-Bench: Prompt generation → Ground truth video collection → Metric computation (TCR, TC-Score)
  - Baselines: Direct T2V models → Multi-stage T2V models → I2V models → SDXL+SEINE baseline
  - Evaluation: Human ratings → Automatic metrics comparison → Correlation analysis

- **Critical path:**
  1. Generate or collect temporal compositionality prompts
  2. Create or obtain ground truth videos
  3. Generate videos using baseline models
  4. Compute TCR and TC-Score metrics
  5. Compare with human ratings and existing metrics

- **Design tradeoffs:**
  - Using VLMs for evaluation provides semantic understanding but introduces API dependency and cost
  - Collecting ground truth videos ensures realistic evaluation but limits scalability
  - Focusing on short temporal transitions makes the task more tractable but may miss longer-term compositionality

- **Failure signatures:**
  - Low TCR/TC-Score values indicate models struggle with temporal compositionality
  - High correlation between TC metrics and human ratings validates the evaluation approach
  - Disparate performance across the three compositionality types reveals specific model weaknesses

- **First 3 experiments:**
  1. Run SDXL+SEINE baseline on a small subset of TC-Bench prompts and verify start/end frame consistency
  2. Compare TCR values across all baseline models on attribute transition prompts only
  3. Test VLM assertion verification on ground truth videos to establish baseline accuracy

## Open Questions the Paper Calls Out

1. How can we develop more reliable and robust metrics for evaluating temporal compositionality in video generation that do not rely on image-based assertions and can handle multi-image understanding?
2. What techniques can be employed to improve text-to-video models' ability to interpret descriptions of compositional changes and synthesize various components across different time steps?
3. How can we automatically mine videos with specific temporal compositionality and generate detailed captions to expand benchmarks like TC-Bench?

## Limitations
- Evaluation relies on vision language models which may not reliably verify complex compositional changes
- The relatively small scale of TC-Bench (150 prompts for T2V, 120 for I2V) may not fully represent the space of temporal compositionality
- Ground truth videos sourced from YouTube may contain artifacts that affect evaluation consistency

## Confidence
- **High Confidence:** Existing video generation models struggle with temporal compositionality (well-supported by experimental results)
- **Medium Confidence:** Effectiveness of SDXL+SEINE baseline and three-category framework
- **Low Confidence:** VLM reliability for complex compositional reasoning in video frames

## Next Checks
1. Test VLM assertion verification system on ground truth videos with known compositional changes to establish baseline accuracy rates
2. Conduct detailed correlation analysis between TC metrics and traditional metrics across individual prompts
3. Analyze TCR and TC-Score distribution across the three compositionality types to verify differentiated performance patterns