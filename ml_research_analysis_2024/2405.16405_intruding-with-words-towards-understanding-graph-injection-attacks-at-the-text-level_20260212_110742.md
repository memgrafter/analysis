---
ver: rpa2
title: 'Intruding with Words: Towards Understanding Graph Injection Attacks at the
  Text Level'
arxiv_id: '2405.16405'
source_url: https://arxiv.org/abs/2405.16405
tags:
- performance
- table
- text
- graph
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces text-level graph injection attacks (GIAs)
  on text-attributed graphs, addressing limitations of existing embedding-level approaches.
  The authors propose three novel attack designs: Inversion-based Text-level GIA (ITGIA),
  Vanilla Text-level GIA (VTGIA), and Word-frequency-based Text-level GIA (WTGIA).'
---

# Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level

## Quick Facts
- arXiv ID: 2405.16405
- Source URL: https://arxiv.org/abs/2405.16405
- Authors: Runlin Lei; Yuwei Hu; Yuchen Ren; Zhewei Wei
- Reference count: 40
- Key outcome: This paper introduces text-level graph injection attacks (GIAs) on text-attributed graphs, addressing limitations of existing embedding-level approaches.

## Executive Summary
This paper explores graph injection attacks at the text level, where attackers inject nodes with raw text into text-attributed graphs to manipulate GNN-based classifiers. The authors identify a fundamental gap: attackers typically only have access to raw text while defenders use proprietary embedding methods, creating opportunities for effective attacks. They propose three novel attack designs—ITGIA, VTGIA, and WTGIA—and evaluate their effectiveness against various defenses, finding that interpretability and attack performance present significant trade-offs.

## Method Summary
The paper proposes three text-level GIA methods: ITGIA uses embedding inversion to find interpretable text regions, VTGIA leverages LLMs for direct text generation, and WTGIA combines BoW embeddings with LLM guidance for optimal balance. The methods operate by generating text that satisfies specific word constraints, converting text to embeddings, optimizing graph structure, and injecting nodes to maximize classification error. The attacks are evaluated against standard GNN models and LLM-based predictors as defenses.

## Key Results
- Text-level GIAs create a gap between attacker-accessible raw text and defender-used embeddings, enabling effective attacks
- WTGIA achieves the best balance between attack effectiveness and text interpretability by using word-frequency-based embeddings
- LLM-based predictors provide strong defense against text-level GIAs by bypassing graph structure vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-level GIAs exploit the gap between raw text availability to attackers and processed embeddings used by defenders.
- Mechanism: Attackers inject interpretable raw text into graphs while defenders use proprietary embedding methods that cannot perfectly align with the attacker's text-to-embedding conversion.
- Core assumption: Attackers only have access to raw text, not the defender's embedding pipeline.
- Evidence anchors:
  - [abstract]: "Attackers usually have access only to raw text, not the processed embeddings defenders use."
  - [section 2]: "In real-world scenarios like social or citation networks, it is more practical to inject nodes with raw text rather than node embeddings."
  - [corpus]: Weak - neighbors discuss related attacks but not this specific raw text vs embedding mismatch.
- Break condition: If defenders publish their embedding methods or attackers gain access to embedding pipelines.

### Mechanism 2
- Claim: WTGIA achieves a balance between attack effectiveness and text interpretability by using word-frequency-based embeddings.
- Mechanism: Binary Bag-of-Words (BoW) embeddings provide clear physical meaning (1 = word present, 0 = absent), enabling LLM-guided text generation that satisfies both attack goals and interpretability constraints.
- Core assumption: BoW embeddings create a well-defined mapping between text and embeddings that LLMs can optimize.
- Evidence anchors:
  - [section 4]: "Building on these advantages, we design the Word-frequency-based Text-level GIA (WTGIA), leveraging BoW embeddings and the generative capabilities of LLMs."
  - [section 3.2]: "The task involves generating text containing specified words while excluding prohibited words, which can be effectively done via LLMs."
  - [corpus]: Weak - corpus neighbors discuss text-level attacks but not this specific BoW-LLM combination.
- Break condition: If defenders use embedding methods that don't preserve word presence/absence information.

### Mechanism 3
- Claim: LLMs-as-predictors provide strong defense against text-level GIAs by directly classifying nodes from raw text without GNN vulnerability.
- Mechanism: LLMs can classify nodes using only raw text input, bypassing the graph structure that text-level GIAs manipulate, and can incorporate neighbor information when available.
- Core assumption: LLMs can achieve high accuracy on node classification tasks using only text features.
- Evidence anchors:
  - [section 5.2]: "LLM-based predictors can achieve high accuracy in node classification, meaning defenders can evade the influence of injected nodes."
  - [section 5.2]: "even without utilizing any neighborhood information, LLM-based predictors can achieve high accuracy in node classification"
  - [corpus]: Weak - corpus neighbors discuss prompt injection but not LLM defenses against graph attacks.
- Break condition: If LLMs cannot maintain accuracy on graph classification tasks or if text features become insufficient for classification.

## Foundational Learning

- Concept: Graph Neural Networks and their vulnerability to adversarial attacks
  - Why needed here: The entire paper addresses attacking GNNs through text injection, so understanding GNN architecture and attack vectors is fundamental
  - Quick check question: What makes GNNs vulnerable to graph injection attacks compared to traditional neural networks?

- Concept: Text embedding techniques and their interpretability
  - Why needed here: Different embedding methods (BoW vs PLM) have vastly different properties for attack effectiveness and interpretability
  - Quick check question: How do binary BoW embeddings differ from continuous PLM embeddings in terms of interpretability and attack vulnerability?

- Concept: Large Language Model capabilities for text generation and classification
  - Why needed here: LLMs are central to both the attack methods (text generation) and defense methods (classification)
  - Quick check question: What are the key limitations of using LLMs for generating adversarial text that satisfies specific word constraints?

## Architecture Onboarding

- Component map:
  Attacker components: Text generation (LLMs) -> Embedding conversion (BoW/PLM) -> Graph structure manipulation
  Defender components: GNN models (GCN, EGNNGuard) -> Alternative predictors (LLMs-as-predictors) -> Embedding methods (BoW, GTR, SBERT)
  Shared components: Graph structure, node labels, attack budget constraints

- Critical path:
  1. Attacker generates text using LLM with word constraints
  2. Text converted to embeddings (BoW or PLM)
  3. Injected nodes added to graph with optimized structure
  4. Defender processes graph with their embedding method
  5. Classification results compared to measure attack success

- Design tradeoffs:
  - Text interpretability vs attack effectiveness: More interpretable text often means less effective attacks
  - Embedding method compatibility: Attacks designed for one embedding method may not transfer well to others
  - Defense robustness: LLMs-as-predictors provide strong defense but may not work for all graph tasks

- Failure signatures:
  - High perplexity scores in generated text indicating poor interpretability
  - Low cosine similarity between injected and inverted embeddings
  - Poor transferability when embedding methods change
  - Defense models maintaining high accuracy despite attacks

- First 3 experiments:
  1. Implement WTGIA with GPT-3.5-turbo on Cora dataset using average sparsity budget, compare to embedding-level attacks
  2. Test ITGIA with HAO constraint on CiteSeer dataset, measure cosine similarity and attack effectiveness
  3. Evaluate LLM-as-predictor defense on PubMed dataset, compare performance with and without neighbor information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific embedding-to-text inversion method would enable ITGIA to achieve both interpretability and attack effectiveness?
- Basis in paper: [explicit] The paper states that ITGIA struggles to invert embeddings to interpretable text because the interpretable regions of the injected embeddings are ill-defined, and that even with HAO, the improvement comes at the expense of embedding-level attack performance.
- Why unresolved: The authors demonstrate that current inversion methods like Vec2Text fail to recover coherent text from the embedding regions used in GIAs, and theoretical analysis shows that interpretable regions are much smaller than the feasible embedding space.
- What evidence would resolve it: Empirical demonstration of an inversion method that can recover coherent, semantically meaningful text from embedding regions that maintain strong attack performance against GNNs.

### Open Question 2
- Question: How can text-level GIAs be made transferable across different text embedding methods without significant performance degradation?
- Basis in paper: [explicit] The paper shows that WTGIA's performance drops significantly when transferred from BoW to GTR embeddings, and ITGIA performs even worse when transferred to BoW embeddings.
- Why unresolved: The authors identify this as a major limitation but do not propose solutions for achieving cross-embedding transferability while maintaining attack effectiveness.
- What evidence would resolve it: Successful demonstration of a text-level GIA method that maintains comparable attack performance across multiple embedding methods (BoW, GTR, SBERT, etc.) without requiring knowledge of the defender's specific embedding technique.

### Open Question 3
- Question: What LLM-based defense mechanisms can effectively counter text-level GIAs while maintaining reasonable classification accuracy on clean data?
- Basis in paper: [explicit] The authors show that LLM-based predictors (LLMs-as-Predictor) can achieve high accuracy on clean data and provide a practical upper limit that GIAs struggle to degrade below.
- Why unresolved: While the paper demonstrates the effectiveness of LLM-based defenses, it does not explore optimization strategies for these defenses or investigate their robustness against adaptive attacks.
- What evidence would resolve it: Comparative evaluation of various LLM-based defense architectures showing their effectiveness against adaptive text-level GIAs while maintaining classification performance close to the best LLM-only baseline.

## Limitations

- ITGIA struggles with interpretability due to embedding inversion challenges, limiting its practical applicability
- VTGIA shows that LLM-guided text generation alone may not achieve sufficient attack effectiveness
- Results focus primarily on node classification tasks, leaving open questions about performance on other graph learning tasks

## Confidence

- **High Confidence**: The core observation that text-level attacks create a gap between attacker-accessible raw text and defender-used embeddings is well-supported by empirical results.
- **Medium Confidence**: The effectiveness of WTGIA as a balanced approach is demonstrated, but results may be sensitive to specific dataset characteristics and embedding configurations.
- **Medium Confidence**: The defense mechanisms using LLM predictors show promise, but their effectiveness across diverse real-world scenarios needs further validation.

## Next Checks

1. Test WTGIA performance across a broader range of graph learning tasks (link prediction, graph classification) to assess generalizability beyond node classification.
2. Evaluate attack transferability when defenders use different embedding methods than those used during attack generation to measure robustness.
3. Conduct ablation studies on the LLM predictor defense to identify which components (text features, neighbor information, model architecture) contribute most to its effectiveness.