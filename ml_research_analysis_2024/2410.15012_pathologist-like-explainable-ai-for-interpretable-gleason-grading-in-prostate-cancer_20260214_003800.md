---
ver: rpa2
title: Pathologist-like explainable AI for interpretable Gleason grading in prostate
  cancer
arxiv_id: '2410.15012'
source_url: https://arxiv.org/abs/2410.15012
tags:
- page
- gleason
- patterns
- explanations
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a pathologist-like explainable AI system
  for Gleason grading in prostate cancer, addressing the challenge of transparency
  in AI-assisted pathology. The researchers developed a novel dataset of 1,015 tissue
  microarray images annotated by 54 international pathologists, providing detailed
  localized pattern descriptions aligned with international guidelines.
---

# Pathologist-like explainable AI for interpretable Gleason grading in prostate cancer

## Quick Facts
- arXiv ID: 2410.15012
- Source URL: https://arxiv.org/abs/2410.15012
- Reference count: 0
- Models trained on explanations achieve or exceed performance of methods trained directly for Gleason pattern segmentation (Dice score: 0.713 ± 0.003 vs. 0.691 ± 0.010)

## Executive Summary
This study introduces a pathologist-like explainable AI system for Gleason grading in prostate cancer, addressing the challenge of transparency in AI-assisted pathology. The researchers developed a novel dataset of 1,015 tissue microarray images annotated by 54 international pathologists, providing detailed localized pattern descriptions aligned with international guidelines. Using a U-Net architecture trained on soft labels, the system achieves or exceeds performance of traditional Gleason pattern segmentation methods while maintaining interpretability.

## Method Summary
The method uses a U-Net architecture with EfficientNet-B4 encoder trained on soft labels derived from multiple pathologist annotations. Rather than training directly on Gleason patterns, the model predicts detailed histological explanations that map to Gleason patterns. A custom SoftDiceLoss function emphasizes performance on minority classes. The approach captures interobserver uncertainty and maintains interpretability without sacrificing accuracy, enabling better understanding of pathologists' reasoning processes.

## Key Results
- U-Net trained on soft labels achieves Dice score of 0.713 ± 0.003, exceeding hard label approaches (0.691 ± 0.010)
- Class-averaged loss functions improve minority class performance without sacrificing overall accuracy
- Models trained on explanations achieve equivalent Gleason pattern performance to those trained directly on Gleason patterns
- The dataset and code will be publicly available for further research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft labels improve model performance by preserving interobserver uncertainty
- Mechanism: By treating each pixel's annotations from multiple pathologists as a probability distribution rather than a single majority vote, the model can learn the inherent ambiguity in Gleason grading decisions
- Core assumption: Pathologists' annotations represent a probability distribution over possible labels, not a single ground truth
- Evidence anchors: [abstract] "By employing soft labels during training, we capture the intrinsic uncertainty in the data"; [section] "Instead, we used a soft label approach, obtained by combining the annotations into a per-pixel annotation distribution"
- Break condition: If pathologist annotations are actually consistent (high interobserver agreement), soft labels would provide no benefit over majority voting

### Mechanism 2
- Claim: Training on explanations achieves Gleason pattern performance without direct training on Gleason patterns
- Mechanism: The U-Net architecture learns to predict detailed histological explanations that map directly to Gleason patterns, allowing the model to use pathologists' terminology while maintaining segmentation accuracy
- Core assumption: The explanation ontology is complete and accurately maps to Gleason patterns
- Evidence anchors: [abstract] "This approach circumvents post-hoc explainability methods while maintaining or exceeding the performance of methods trained directly for Gleason pattern segmentation"; [section] "We selected a soft label approach, by treating the different annotations from different annotators over the pixels as probability distributions"
- Break condition: If the explanation ontology is incomplete or doesn't fully capture the features that determine Gleason patterns

### Mechanism 3
- Claim: Class-averaged loss functions improve minority class performance
- Mechanism: The SoftDiceLoss emphasizes performance on minority classes by averaging over all classes, preventing the model from ignoring rare but clinically important patterns
- Core assumption: Rare classes are important for clinical utility and should be predicted even if uncertain
- Evidence anchors: [section] "Models using the SoftDiceLoss, trained on the explanations but evaluated on the Gleason patterns, performed just as well as those trained directly on the Gleason patterns"; [section] "Models trained on majority-voting based explanation labels, as opposed to soft labels, resulted in a slight decrease in segmentation quality"
- Break condition: If rare classes are too ambiguous to be reliably predicted, even with class-balanced losses

## Foundational Learning

- Concept: Interobserver variability in medical image annotation
  - Why needed here: Understanding why soft labels are necessary and how to interpret model uncertainty
  - Quick check question: What percentage of pixels had unanimous majority voting among the 3 pathologists?

- Concept: Soft label training methodology
  - Why needed here: Implementing the soft label approach and understanding its mathematical formulation
  - Quick check question: How are soft labels computed from multiple pathologist annotations per pixel?

- Concept: Gleason grading system and its components
  - Why needed here: Understanding the ontology structure and how explanations map to Gleason patterns
  - Quick check question: What are the three main Gleason patterns and how do they differ histologically?

## Architecture Onboarding

- Component map:
  U-Net with EfficientNet-B4 encoder -> Soft label processing pipeline -> Custom SoftDiceLoss function -> Explanation-to-Gleason pattern remapping logic -> Background masking using Otsu's thresholding

- Critical path:
  1. Load TMA core images and corresponding annotations
  2. Apply foreground/background masking
  3. Generate soft label distributions per pixel
  4. Train U-Net with SoftDiceLoss
  5. Evaluate using Macro SoftDice and L1-norm metrics
  6. Remap predictions to Gleason patterns for final evaluation

- Design tradeoffs:
  - Soft labels vs. majority voting: Better uncertainty capture vs. simpler implementation
  - Class-averaged vs. class-weighted loss: Better minority class performance vs. potential majority class degradation
  - Explanation-level vs. sub-explanation-level training: Better agreement vs. finer detail capture

- Failure signatures:
  - Poor performance on minority classes despite SoftDiceLoss: Annotation uncertainty too high
  - Calibration issues: Loss function not properly matching target distribution
  - Artifacts in segmentation maps: Background masking not working correctly

- First 3 experiments:
  1. Train baseline U-Net with majority voting labels to establish performance floor
  2. Train U-Net with soft labels and compare Macro SoftDice scores
  3. Train U-Net with different loss functions (cross-entropy vs. SoftDiceLoss) to find optimal approach

## Open Questions the Paper Calls Out
- How would GleasonXAI's performance change if trained on a dataset with higher interobserver agreement, particularly for minority classes like comedonecrosis and glomeruloid glands?
- Would integrating conformal prediction techniques into the segmentation task improve the reliability of GleasonXAI's uncertainty estimates?
- How does the performance of GleasonXAI compare to state-of-the-art end-to-end Gleason grading models when evaluated on the same dataset?

## Limitations
- Reliance on soft labels assumes pathologist disagreement represents genuine uncertainty rather than inconsistent annotation practices
- U-Net architecture may not capture long-range spatial dependencies important for Gleason grading decisions
- Clinical significance of moderate interobserver variability (CoH = 0.56) remains unclear

## Confidence
- High: Soft labels improve minority class performance and capture annotation uncertainty (robust experimental results)
- Medium: Explanation-level training achieves Gleason pattern performance without direct training (depends on ontology completeness)
- Low: Generalizability to other medical imaging domains (study focuses specifically on prostate cancer)

## Next Checks
1. Quantify the percentage of pixels with unanimous majority voting among the 3 pathologists to validate the assumption that disagreement represents genuine uncertainty rather than annotation errors
2. Test whether the explanation ontology fully captures all features that determine Gleason patterns by comparing explanation-based predictions with expert Gleason grades on a held-out validation set
3. Apply the soft label approach to a different medical imaging task with known interobserver variability (e.g., breast cancer HER2 scoring) to assess whether the benefits extend beyond prostate cancer