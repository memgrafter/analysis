---
ver: rpa2
title: Transformer Normalisation Layers and the Independence of Semantic Subspaces
arxiv_id: '2406.17837'
source_url: https://arxiv.org/abs/2406.17837
tags:
- layer
- attention
- pre-norm
- when
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the theoretical and empirical consequences
  of using Pre-Norm normalization in transformer architectures, particularly its impact
  on the separation of semantic subspaces. The authors argue that Pre-Norm causes
  interference between independent subspaces by sharing a common normalization factor,
  which can destabilize transformer circuits.
---

# Transformer Normalisation Layers and the Independence of Semantic Subspaces

## Quick Facts
- **arXiv ID**: 2406.17837
- **Source URL**: https://arxiv.org/abs/2406.17837
- **Authors**: Stephen Menary; Samuel Kaski; Andre Freitas
- **Reference count**: 40
- **Primary result**: Pre-Norm normalization causes interference between semantic subspaces, requiring orthogonal spherical representations, while QKV-Norm only needs linear independence

## Executive Summary
This paper analyzes how normalization layer placement in transformers affects the separation of semantic subspaces. The authors demonstrate that Pre-Norm (normalization before attention) interferes with subspace separation by sharing a common normalization factor across independent subspaces, while QKV-Norm (normalization after linear transformations) relaxes these constraints. They introduce the concept of "circuit collapse" where sparse attention distributions become unstable under noise, predicting and empirically observing this phenomenon in a numerical addition task.

## Method Summary
The authors use a decoder transformer architecture with 10 layers, 12 attention heads, and embedding size 512 to perform integer addition on character-level tokenized sequences. They compare two normalization strategies: Pre-Norm (normalization before attention) and QKV-Norm (normalization after linear operators on query, key, and value vectors). The models are trained with AdamW optimizer and custom initialization including checkpoint layers. They evaluate per-token accuracy on in-distribution and out-of-distribution test sets, analyze embedding norm distributions, and simulate interference by adding noise to L2-norms of {q, k, v} vectors.

## Key Results
- Pre-Norm models show narrower embedding L2-norm distributions compared to QKV-Norm
- Circuit collapse occurs at approximately 11% noise with a 1% rate of sparse attention switching tokens
- Pre-Norm achieves 91.4% in-distribution accuracy vs 91.0% for QKV-Norm, but worse out-of-distribution performance (87.5% vs 66.7%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-Norm normalization interferes with semantic subspace separation by sharing a common L2-normalization factor across independent subspaces.
- Mechanism: Pre-Norm normalizes the entire embedding vector before applying attention, so any independent semantic subspaces are mixed through the normalization factor. This prevents linear attention heads from isolating specific subspaces unless those subspaces are represented as orthogonal spheres with constant norms.
- Core assumption: Semantic subspaces exist and are important for circuit formation in transformers.
- Evidence anchors: [abstract] "Pre-Norm causes linear subspaces to interfere through their common normalisation factor" and [section] "We show that Pre-Norm... violates this ability unless the model learns a strict representation structure of orthogonal spheres."

### Mechanism 2
- Claim: QKV-Norm relaxes representational constraints by normalizing queries, keys, and values separately after linear transformation.
- Mechanism: By applying normalization after the linear transformations (WQ, WK, WV), QKV-Norm only requires that semantic subspaces be linearly independent, not orthogonal spheres. This preserves more degrees of freedom in the representation space and reduces interference between subspaces.
- Core assumption: Normalizing after linear transformations rather than before allows for more flexible representation structures.
- Evidence anchors: [abstract] "Theoretically this relaxes the representational constraints" and [section] "By contrast, QKV-Norm requires only that subspaces be linearly independent."

### Mechanism 3
- Claim: Sparse attention distributions are stable to subspace interference until a threshold is reached, causing "circuit collapse."
- Mechanism: Sparse attention is insensitive to small perturbations in queries and keys because it depends only on the order of attention scores. However, when perturbations become large enough to change the order, the attention distribution undergoes a phase transition, causing circuit collapse where sparse attention shifts to a different token.
- Core assumption: Transformers use sparse attention for logical operations like match&pass, and this sparsity makes them vulnerable to phase transitions when interference crosses a critical threshold.
- Evidence anchors: [abstract] "Theoretically, we analyse circuit stability by modelling this interference as random noise on the L2-norms of the query/key/value vectors, predicting a phenomenon of circuit collapse when sparse-attention shifts to a different token."

## Foundational Learning

- Concept: Semantic subspaces in transformer representations
  - Why needed here: The paper's core argument depends on understanding that transformers can decompose their latent space into independent semantic subspaces that can be selectively attended to.
  - Quick check question: Can you explain how the induction circuit uses separate subspaces for position, type, and previous-type information?

- Concept: Normalization layer placement effects
  - Why needed here: Understanding how Pre-Norm vs QKV-Norm affects the geometry of the representation space is crucial for grasping why one causes interference while the other doesn't.
  - Quick check question: What's the geometric difference between normalizing before attention vs normalizing the query/key/value vectors after linear transformation?

- Concept: Attention distribution stability analysis
  - Why needed here: The circuit collapse phenomenon requires understanding how small perturbations affect attention distributions differently in sparse vs isotropic regimes.
  - Quick check question: Why is sparse attention stable to perturbations in queries and keys but vulnerable to perturbations that change the order of attention scores?

## Architecture Onboarding

- Component map:
  Embedding creation (position + type information) -> Normalization layer (Pre-Norm vs QKV-Norm) -> Linear transformations (WQ, WK, WV) -> Attention computation (dot-product, softmax) -> Message passing and circuit formation

- Critical path:
  1. Embedding creation (position + type information)
  2. Normalization layer (Pre-Norm vs QKV-Norm placement)
  3. Linear transformations (WQ, WK, WV)
  4. Attention computation (dot-product, softmax)
  5. Message passing and circuit formation

- Design tradeoffs:
  - Pre-Norm: Better training stability but restrictive representation requirements and potential interference
  - QKV-Norm: More flexible representations but potentially less training stability and worse OOD performance
  - Sparse attention: More efficient and interpretable but vulnerable to circuit collapse
  - Isotropic attention: More stable but less selective and potentially less useful for logical operations

- Failure signatures:
  - Pre-Norm: Narrow norm distributions, potential circuit collapse under noise, possible training instability
  - QKV-Norm: Wider norm distributions, potentially less sparse attention, possible generalization issues
  - Circuit collapse: Sudden performance degradation when attention distributions shift to different tokens

- First 3 experiments:
  1. Measure embedding L2-norm distributions for Pre-Norm vs QKV-Norm models to verify the orthogonal sphere requirement
  2. Add artificial noise to query/key/value norms and measure circuit collapse rates in sparse attention distributions
  3. Compare training stability and generalization performance between Pre-Norm and QKV-Norm across different model sizes and learning rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Pre-Norm and QKV-Norm architectures compare in their ability to learn compositional annotation circuits during training?
- Basis in paper: [inferred] The paper discusses that Pre-Norm may facilitate compositional annotation by inducing a spheroid message structure close to the sphere required for separability in later layers.
- Why unresolved: The paper does not empirically test the ability of Pre-Norm versus QKV-Norm to learn compositional annotation circuits.
- What evidence would resolve it: Empirical studies comparing the learning of compositional annotation circuits in Pre-Norm versus QKV-Norm architectures during training.

### Open Question 2
- Question: What is the impact of Pre-Norm and QKV-Norm on the training stability and convergence speed of transformers in tasks beyond numerical addition?
- Basis in paper: [explicit] The paper mentions that changing the normalisation layer is expected to affect the training rate and stability.
- Why unresolved: The paper only tests this on a numerical addition task.
- What evidence would resolve it: Training and evaluating transformers with Pre-Norm and QKV-Norm on a variety of tasks.

### Open Question 3
- Question: How does the interference between independent subspaces manifest in real-world models, and what are its observable effects on model behavior?
- Basis in paper: [inferred] The paper theorizes that Pre-Norm causes interference between independent subspaces through a common normalization factor.
- Why unresolved: The paper does not directly observe subspace interference in real-world models.
- What evidence would resolve it: Direct observation of subspace interference in trained models through interpretability techniques or controlled experiments.

## Limitations

- The theoretical framework relies on assumptions about semantic subspace existence that lack direct empirical validation
- Circuit collapse phenomenon is observed only under artificial noise injection rather than naturally occurring conditions
- The numerical addition task may not capture the complexity of real-world transformer applications where multiple semantic subspaces interact more dynamically

## Confidence

- **High confidence**: The theoretical analysis of Pre-Norm's interference mechanism through shared normalization factors is mathematically sound
- **Medium confidence**: The empirical observations about norm distributions and circuit collapse rates are reproducible but require additional validation
- **Low confidence**: The generalization performance comparison may be influenced by factors beyond the normalization mechanism

## Next Checks

1. Test circuit collapse predictions on multiple architectures (ViT, MLP-Mixer) and tasks beyond numerical addition to establish whether the phenomenon is architecture-agnostic

2. Implement ablation studies that isolate the effect of initialization standardization and checkpoint layer calibration to determine whether observed differences are primarily due to normalization placement

3. Develop quantitative measures of semantic subspace separation (e.g., subspace orthogonality metrics) to directly test whether Pre-Norm models learn more restrictive representations than QKV-Norm models across diverse tasks