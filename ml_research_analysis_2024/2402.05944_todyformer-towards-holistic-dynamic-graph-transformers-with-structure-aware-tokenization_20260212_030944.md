---
ver: rpa2
title: 'Todyformer: Towards Holistic Dynamic Graph Transformers with Structure-Aware
  Tokenization'
arxiv_id: '2402.05944'
source_url: https://arxiv.org/abs/2402.05944
tags:
- node
- dynamic
- graph
- datasets
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Todyformer addresses over-squashing and over-smoothing in dynamic
  graph representation learning by unifying MPNN-based local encoding with Transformer-based
  global encoding through a novel patchifying and structure-aware tokenization approach.
  The model alternates between local and global message-passing to capture long-range
  dependencies while mitigating computational bottlenecks.
---

# Todyformer: Towards Holistic Dynamic Graph Transformers with Structure-Aware Tokenization

## Quick Facts
- arXiv ID: 2402.05944
- Source URL: https://arxiv.org/abs/2402.05944
- Authors: Mahdi Biparva; Raika Karimi; Faezeh Faez; Yingxue Zhang
- Reference count: 23
- Key outcome: Todyformer achieves state-of-the-art performance on future link prediction and dynamic node classification tasks with significant margins across benchmark datasets.

## Executive Summary
Todyformer addresses critical challenges in dynamic graph representation learning—specifically over-squashing and over-smoothing—through a novel alternating architecture that combines local MPNN encoding with global Transformer processing. The model employs a patchifying approach to partition large dynamic graphs into manageable subgraphs, while using structure-aware tokenization to prepare node embeddings for Transformer processing. By alternating between local and global message-passing, Todyformer effectively captures long-range temporal dependencies while maintaining computational efficiency.

## Method Summary
Todyformer processes Continuous-Time Dynamic Graphs by first partitioning them into non-overlapping patches based on edge timestamps. Each patch undergoes local encoding using a DyG2Vec MPNN to generate semantically meaningful node embeddings, which are then packed into sequential format for Transformer processing. The model alternates between MPNN-based local encoding and Transformer-based global encoding, using positional encoding to maintain temporal ordering. The architecture includes packing and unpacking modules to handle the transition between graph and sequence representations. For downstream tasks, the model employs task-specific decoders with binary cross-entropy loss for link prediction or cross-entropy loss for node classification.

## Key Results
- Achieves state-of-the-art performance on future link prediction tasks with significant margins over existing methods
- Demonstrates superior results on dynamic node classification across multiple benchmark datasets
- Shows effective mitigation of over-squashing and over-smoothing through the alternating architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Patchifying mitigates over-squashing by partitioning the graph into smaller subgraphs for local message passing.
- Mechanism: Large graphs are divided into non-overlapping patches based on edge timestamps, breaking down the context into manageable subgraphs. This allows local message passing to operate independently within each patch rather than aggregating information from the entire large graph at once.
- Core assumption: Partitioning the graph into patches preserves essential temporal dependencies while reducing the computational burden of message passing across a large graph.
- Evidence anchors:
  - [abstract]: "patchifying paradigm for dynamic graphs to improve over-squashing"
  - [section 3.2]: "patchifier even segments the input graph into M non-overlapping subgraphs of equal size, referred to as patches... Partitioning the input graph into M disjoint subgraphs helps message-passing to be completely separated within each patch."

### Mechanism 2
- Claim: Alternating between local MPNN and global Transformer modules mitigates over-smoothing while capturing long-range dependencies.
- Mechanism: MPNNs encode local structural and temporal patterns within patches, while Transformers aggregate global context across nodes. By alternating these modules, the model avoids the deep MPNN layers that typically cause over-smoothing, instead using Transformers to widen the temporal context beyond local neighborhoods.
- Core assumption: Combining local and global encoding in alternating blocks preserves node distinctiveness while still capturing long-range dependencies.
- Evidence anchors:
  - [abstract]: "encoding architecture that alternates between local and global contextualization, mitigating over-smoothing in MPNNs"
  - [section 3.5]: "Over-smoothing is a critical problem in graph representation learning, where MPNNs fall short in encoding long-range dependencies... We propose to address this problem by letting the Transformer widen up the temporal contextual node-wise scope beyond a few hops in an alternating manner."

### Mechanism 3
- Claim: Structure-aware tokenization leverages MPNNs to create meaningful node embeddings for Transformer input.
- Mechanism: DyG2Vec is used as a local encoder to map node features into latent embeddings that capture both structural and temporal patterns within each patch. These embeddings are then packed into a sequential format suitable for Transformer processing.
- Core assumption: MPNNs are effective at encoding local patterns, and these embeddings serve as good inputs for Transformers to capture global context.
- Evidence anchors:
  - [abstract]: "structure-aware parametric tokenization strategy leveraging MPNNs"
  - [section 3.3]: "local encoding in Todyformer utilizes a dynamic GNN to map the input node embeddings to the latent embeddings that a Transformer will process later on... we use DyG2Vec as a powerful attentive message-passing model to locally encode input features into semantically meaningful node tokens."

## Foundational Learning

- Concept: Message Passing Neural Networks (MPNNs)
  - Why needed here: MPNNs form the local encoding component that captures structural and temporal patterns within graph patches.
  - Quick check question: What are the primary limitations of MPNNs in dynamic graph settings that Todyformer aims to address?

- Concept: Transformers
  - Why needed here: Transformers provide global contextualization and long-range dependency modeling beyond what MPNNs can achieve.
  - Quick check question: How does positional encoding help Transformers maintain temporal ordering in dynamic graphs?

- Concept: Graph patchifying
  - Why needed here: Patchifying divides large graphs into smaller subgraphs to reduce computational complexity and mitigate over-squashing.
  - Quick check question: What trade-offs exist between patch size and the preservation of temporal dependencies?

## Architecture Onboarding

- Component map: Input CTDG -> Patchifier -> M patches -> Local Encoder (DyG2Vec) -> Node tokens -> Packing -> Sequential format -> Positional encoding -> Transformer -> Global context -> Unpacking -> Next block or Readout -> Node embeddings -> Decoder -> Predictions

- Critical path:
  1. Input graph → Patchifier → M patches
  2. Each patch → Local Encoder (MPNN) → Node tokens
  3. Node tokens → Packing → Sequential format
  4. Sequential tokens → Positional encoding → Transformer → Global context
  5. Global context → Unpacking (if not final layer) → Next block or Readout
  6. Readout → Node embeddings → Decoder → Predictions

- Design tradeoffs:
  - Number of patches (M): More patches reduce over-squashing but may fragment context
  - MPNN depth: Deeper MPNNs capture more local patterns but risk over-smoothing
  - Transformer layers: More layers capture longer dependencies but increase computation
  - Window size: Larger windows capture more temporal context but increase memory usage

- Failure signatures:
  - Over-smoothing: Node embeddings become indistinguishable across the graph
  - Over-squashing: Performance degrades as message passing depth increases
  - Memory issues: Large window sizes or many patches exceed GPU memory
  - Poor tokenization: Node embeddings fail to capture essential local patterns

- First 3 experiments:
  1. Vary the number of patches (M) on a small dataset to observe effects on over-squashing
  2. Compare performance with and without alternating architecture on a medium dataset
  3. Test different MPNN depths in the local encoder to find optimal tokenization quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Todyformer's alternating architecture affect the model's ability to capture long-range temporal dependencies compared to deeper MPNN-only architectures?
- Basis in paper: [explicit] The paper states that Todyformer's alternating architecture mitigates over-smoothing in MPNNs by letting the Transformer widen up the temporal contextual node-wise scope beyond a few hops.
- Why unresolved: While the paper shows improved performance, it doesn't provide a detailed quantitative analysis of how the alternating architecture specifically impacts the model's ability to capture long-range temporal dependencies.
- What evidence would resolve it: A detailed ablation study comparing the performance of Todyformer with different numbers of alternating blocks on datasets with varying levels of long-range dependencies would provide insights into the effectiveness of the alternating architecture in capturing long-range temporal dependencies.

### Open Question 2
- Question: What is the impact of the number of patches (M) on Todyformer's performance and computational efficiency, and how does this impact vary across different datasets?
- Basis in paper: [explicit] The paper mentions that M manages the trade-off between alleviating over-squashing and maintaining the tokenizer's expressiveness, and presents sensitivity analysis on the number of patches and input window size values on MOOC and LastFM datasets.
- Why unresolved: The paper provides some insights into the impact of M on performance and computational efficiency, but doesn't offer a comprehensive analysis across a wider range of datasets with different characteristics.
- What evidence would resolve it: A thorough sensitivity analysis of Todyformer's performance and computational efficiency across a diverse set of datasets with varying densities, sizes, and long-range dependency patterns would help understand the optimal number of patches for different scenarios.

### Open Question 3
- Question: How does the choice of positional encoding scheme affect Todyformer's performance, and is there a universally optimal scheme for dynamic graphs?
- Basis in paper: [explicit] The paper discusses the use of temporal positional encoding in Todyformer and presents an ablation study comparing different positional encoding schemes, including SineCosine, Time2Vec, Identity, and Linear.
- Why unresolved: While the paper shows that SineCosine positional encoding slightly outperforms the others, it doesn't explore the impact of different positional encoding schemes on the model's ability to capture structural and temporal patterns in dynamic graphs.
- What evidence would resolve it: A comprehensive comparison of Todyformer's performance using various positional encoding schemes, including those specifically designed for dynamic graphs, would help determine the optimal positional encoding strategy for different types of dynamic graph data.

## Limitations

- The exact implementation details of the patchifying module and DyG2Vec tokenizer configuration remain unspecified, requiring empirical validation for faithful reproduction.
- The computational efficiency gains claimed through patchifying may vary significantly depending on graph density and temporal distribution of edges.
- The proposed alternating architecture assumes an optimal balance between local and global encoding that may not generalize across all dynamic graph types and temporal scales.

## Confidence

- **High Confidence**: The fundamental mechanism of alternating between MPNN and Transformer modules to address over-smoothing is well-grounded in existing literature on GNNs and Transformers.
- **Medium Confidence**: The specific implementation of structure-aware tokenization using DyG2Vec and the effectiveness of the alternating architecture in dynamic graph settings requires empirical validation across diverse graph types and temporal patterns.
- **Medium Confidence**: The performance claims on benchmark datasets are credible given the comprehensive evaluation, but the exact magnitude of improvements depends on implementation details and hyperparameter tuning.

## Next Checks

1. **Ablation Study**: Remove the alternating architecture and compare performance with a pure MPNN or pure Transformer approach on the same datasets to quantify the specific contribution of the alternating mechanism.

2. **Parameter Sensitivity Analysis**: Systematically vary the number of patches (M), MPNN depth, and Transformer layers to identify optimal configurations and understand the robustness of performance across different parameter settings.

3. **Cross-Dataset Generalization**: Test Todyformer on datasets with varying temporal patterns (e.g., bursty vs. gradual edge additions) and graph structures (e.g., scale-free vs. random) to assess the generalizability of the approach beyond the benchmark datasets used in the paper.