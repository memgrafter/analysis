---
ver: rpa2
title: 'Evaluating the Evaluator: Measuring LLMs'' Adherence to Task Evaluation Instructions'
arxiv_id: '2408.08781'
source_url: https://arxiv.org/abs/2408.08781
tags:
- criteria
- quality
- human
- evaluation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates how prompting strategies
  influence LLM-based automatic evaluation. The authors introduce a taxonomy grouping
  34 evaluation criteria into 4 categories: Content, Relevance, Integrity, and Engagement.'
---

# Evaluating the Evaluator: Measuring LLMs' Adherence to Task Evaluation Instructions

## Quick Facts
- arXiv ID: 2408.08781
- Source URL: https://arxiv.org/abs/2408.08781
- Authors: Bhuvanashree Murugadoss; Christian Poelitz; Ian Drosos; Vu Le; Nick McKenna; Carina Suzana Negreanu; Chris Parnin; Advait Sarkar
- Reference count: 6
- One-line primary result: Detailed rubric instructions yield minimal improvement in LLM evaluation performance, with perplexity often performing comparably or better, especially for textual quality criteria.

## Executive Summary
This paper systematically investigates how prompting strategies influence LLM-based automatic evaluation performance. The authors introduce a taxonomy grouping 34 evaluation criteria into 4 categories: Content, Relevance, Integrity, and Engagement. They compare different prompting settings (perplexity, generic quality, specific criteria, full rubric) across 8 benchmarks using multiple LLM models. Results show that detailed rubric instructions provide minimal improvement in alignment with human judgments (only ~4% increase in Pearson correlation), with perplexity often performing comparably or better, especially for textual quality criteria. The most powerful models like GPT-4 show high alignment even with minimal prompting, while smaller models benefit more from full rubric information.

## Method Summary
The authors systematically compare four prompting settings across eight benchmark datasets: (1) perplexity calculation, (2) generic quality prompt, (3) specific criteria prompt, and (4) full rubric prompt. They evaluate seven LLM families including GPT-4, Llama3 variants, Mistral, Phi3, and Prometheus-2. The evaluation uses Pearson correlation to measure alignment between LLM-generated scores and human annotations. The study introduces a taxonomy categorizing 34 quality criteria into Content, Relevance, Integrity, and Engagement groups to analyze performance patterns across different types of evaluation criteria.

## Key Results
- Detailed rubric instructions yield only ~4% improvement in Pearson correlation with human judgments
- Perplexity often performs comparably or better than prompting, especially for content-based criteria (0.51 vs 0.44 correlation)
- Large models like GPT-4 show high alignment with human judgments even with minimal prompting, suggesting saturation of prior knowledge
- Engagement-based criteria remain challenging across all approaches with high variance in both human annotations and model predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity-based evaluation aligns with human judgments for content-based criteria because it transparently measures alignment with training data without prompt bias
- Mechanism: When models generate text, the perplexity score reflects how predictable the output is based on the model's training distribution. For content-based criteria like fluency, coherence, and naturalness, the training data likely contains high-quality examples that match human expectations for well-formed text. Thus, lower perplexity (more predictable) correlates with higher human ratings for these criteria.
- Core assumption: The training data contains examples that humans would rate as high quality for content-based criteria
- Evidence anchors:
  - [abstract]: "perplexity can sometimes align better with human judgements than prompting, especially on textual quality"
  - [section]: "Content-based quality criteria correlate the most with perplexity...on average the agreement with human annotations is more then 20% higher when using models' perplexity"
  - [corpus]: Weak evidence - corpus shows related work on automatic evaluation but doesn't directly support this specific mechanism
- Break condition: If the training data contains non-representative examples or if the evaluation criteria diverge significantly from the training distribution

### Mechanism 2
- Claim: LLM-as-a-judge performance saturates with minimal prompting for large models because their prior knowledge already aligns with human preferences
- Mechanism: Large models like GPT-4 have been exposed to vast amounts of diverse training data including human-annotated datasets through RLHF. This prior exposure encodes human preferences about quality, making detailed rubric instructions redundant. The models' internal representations of "good" text already match human judgments for many criteria.
- Core assumption: Large models have sufficient exposure to human-annotated data through RLHF and pre-training to encode human preferences
- Evidence anchors:
  - [abstract]: "GPT4 and Llama3 are expected to have strong alignment with human preferences when prompted for a quality judgement"
  - [section]: "GPT4's judgements do only improve marginally from prompting setting 3 to 4, indicating that GPT4's prior knowledge about evaluating does already agree with the human judgements"
  - [corpus]: Weak evidence - corpus neighbors discuss evaluation but don't directly support this mechanism
- Break condition: If the model hasn't been exposed to sufficient human-annotated data or if the evaluation criteria are highly specialized

### Mechanism 3
- Claim: Engagement-based criteria show high variance because they are subjective and context-dependent
- Mechanism: Engagement criteria like empathy, surprise, and naturalness are inherently subjective and depend on personal taste and context. Unlike objective measures like grammatical correctness, these criteria lack clear boundaries and vary across different audiences and situations. This subjectivity leads to higher variance in both human annotations and model predictions.
- Core assumption: Engagement criteria are inherently more subjective than content-based criteria
- Evidence anchors:
  - [section]: "Engagement-based quality criteria are more challenging to judge since they are often subjective...human annotations for these criteria have on average lower scores with higher variance"
  - [section]: "We observe that there are fewer performance differences between the models...The overall performance of all models on these criteria is lower and the scores generated by the models have higher variance"
  - [corpus]: Weak evidence - corpus neighbors don't specifically address subjectivity in engagement criteria
- Break condition: If engagement criteria can be operationalized with clear, objective guidelines

## Foundational Learning

- Concept: Pearson correlation as evaluation metric
  - Why needed here: The paper uses Pearson correlation to measure alignment between model scores and human judgments, which requires understanding correlation coefficients and their interpretation
  - Quick check question: If a model's scores have a Pearson correlation of 0.8 with human judgments, what percentage of variance in human judgments is explained by the model's scores?

- Concept: Taxonomy creation and categorization
  - Why needed here: The authors created a taxonomy of quality criteria to systematically analyze different types of evaluation criteria, which requires understanding how to group related concepts and create hierarchical structures
  - Quick check question: If you have 34 quality criteria and need to group them into 4 categories, what is the average number of criteria per category?

- Concept: Perplexity calculation and interpretation
  - Why needed here: The paper uses model perplexity as an alternative evaluation metric, which requires understanding how perplexity is calculated and what it represents
  - Quick check question: If a model assigns a perplexity of 10 to a sentence, what does this tell you about the model's prediction confidence for that sentence?

## Architecture Onboarding

- Component map: Task description + AI-generated solution -> Perplexity calculator -> Prompt generator -> LLM-as-a-judge -> Human judgment repository -> Correlation calculator -> Taxonomy classifier -> Results aggregation
- Critical path: Task → Perplexity calculation + Prompt generation → LLM evaluation → Correlation calculation → Taxonomy classification → Results aggregation
- Design tradeoffs:
  - Prompt complexity vs. model performance: More detailed prompts help smaller models but provide minimal benefit for large models
  - Reference-free vs. reference-based evaluation: The paper uses reference-free evaluation, which is more challenging but more general
  - Perplexity vs. prompting: Perplexity is faster and unbiased but may miss task-specific nuances that prompting captures
- Failure signatures:
  - Low correlation across all models suggests dataset issues or poor task formulation
  - Large model performance saturating early suggests redundant prompt information
  - High variance in engagement criteria suggests subjective evaluation challenges
  - Perplexity outperforming prompting for content criteria suggests prompt design issues
- First 3 experiments:
  1. Run perplexity calculation on a small subset of data to establish baseline correlation
  2. Test GPT-4 with generic prompt vs. full rubric on same subset to verify saturation effect
  3. Compare correlation distributions across the four taxonomy categories to identify which benefit most from detailed prompting

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The finding that detailed rubrics provide minimal improvement may be dataset-specific to NLP benchmarks
- The mechanisms explaining perplexity's effectiveness rely on assumptions about training data distribution that weren't empirically validated
- The saturation effect for large models assumes sufficient RLHF exposure, but the extent and quality across different model families remains unclear

## Confidence
- **High confidence**: The empirical finding that perplexity correlates well with human judgments for content-based criteria (0.51 vs. 0.44), as this is directly measured and reproducible
- **Medium confidence**: The taxonomy categorization of evaluation criteria, as it provides useful framework but the boundaries between categories may be somewhat arbitrary
- **Medium confidence**: The conclusion that detailed rubrics minimally improve performance, as this depends heavily on the specific benchmarks and models tested

## Next Checks
1. Test the prompting vs perplexity comparison on non-NLP benchmarks (e.g., code generation, reasoning tasks) to verify the findings generalize beyond the current dataset scope
2. Conduct ablation studies removing RLHF components from large models to quantify how much their performance depends on human preference alignment in training data
3. Perform inter-annotator agreement analysis on the human judgments to measure how subjective the engagement-based criteria actually are, and whether this explains the high variance observed