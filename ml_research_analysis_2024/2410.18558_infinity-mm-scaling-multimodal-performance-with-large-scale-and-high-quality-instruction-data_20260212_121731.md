---
ver: rpa2
title: 'Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality
  Instruction Data'
arxiv_id: '2410.18558'
source_url: https://arxiv.org/abs/2410.18558
tags:
- data
- instruction
- identify
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Infinity-MM, a large-scale multimodal instruction
  dataset with over 40 million samples designed to address the performance gap in
  open-source Vision-Language Models (VLMs) compared to those trained on closed-source
  data. The dataset was constructed by collecting and unifying existing open-source
  multimodal datasets, followed by rigorous quality filtering and deduplication.
---

# Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data

## Quick Facts
- arXiv ID: 2410.18558
- Source URL: https://arxiv.org/abs/2410.18558
- Reference count: 40
- Primary result: Introduced Infinity-MM dataset with 40M+ samples and trained Aquila-VL-2B achieving SOTA performance among models of similar scale

## Executive Summary
Infinity-MM addresses the performance gap in open-source Vision-Language Models (VLMs) by introducing a large-scale multimodal instruction dataset with over 40 million high-quality samples. The dataset was constructed through collecting existing open-source multimodal datasets, rigorous quality filtering, deduplication, and synthetic instruction generation using a novel tagging system. This approach enabled training of Aquila-VL-2B, a 2-billion-parameter VLM that achieves state-of-the-art performance among models of similar scale, outperforming both open-source and some closed-source trained models.

## Method Summary
The method involves constructing Infinity-MM through three main phases: (1) collecting and unifying existing open-source multimodal datasets, (2) applying quality filtering and deduplication to ensure data integrity, and (3) generating synthetic instructions using a tagging system that maps image content to appropriate instruction types via TF-IDF-based frequency analysis. The Aquila-VL-2B model is trained using a four-stage progressive approach with increasing image resolution and data quality, leveraging the Qwen-2.5-Instruct language tower, SigLIP vision tower, and a projector component for feature alignment.

## Key Results
- Trained Aquila-VL-2B achieving state-of-the-art performance among 2-billion-parameter models
- Demonstrated consistent performance improvements with increased data scale across all evaluated benchmarks
- Showed superiority over models trained on other open-source datasets and even some closed-source trained models
- Established a new benchmark for open-source VLM development through the Infinity-MM dataset

## Why This Works (Mechanism)

### Mechanism 1: Tagging System for Contextual Instruction Generation
The tagging system maps image content to instruction types using TF-IDF-based frequency counting in seed data, ensuring generated instructions are contextually relevant and diverse. By establishing correspondences between image tags and instruction tags, this method provides essential guidance during data synthesis, ensuring instructions align with image content while increasing diversity.

### Mechanism 2: Open-Source VLM for Scalable Data Generation
The method leverages open-source VLMs (MiniCPM-V2.6) for large-scale instruction synthesis with quality filtering based on relevance scoring and loss evaluation with Qwen2-VL-2B. This approach provides higher quality data than relying solely on closed-source models while avoiding commercial model costs.

### Mechanism 3: Data Scaling for Performance Gains
Scaling instruction data quantity directly improves model performance across all evaluated tasks through extensive data collection (40M+ samples), quality filtering, and deduplication. Progressive training stages with increasing resolution and data quality progressively enhance model capabilities, following established scaling laws.

## Foundational Learning

- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Used to establish mapping between image tags and instruction types by calculating importance of image tags within instruction type contexts. Quick check: How does TF-IDF help determine which instruction types are most suitable for images with specific tags?

- **Vision-Language Model Architecture**: Understanding the three-component architecture (language tower, vision tower, projector) is crucial for implementing and modifying the training process. Quick check: What is the role of the projector component in aligning visual features with language embeddings?

- **Progressive Training with Increasing Difficulty**: The training strategy involves multiple stages with increasing image resolution and data quality, essential for effective model convergence. Quick check: Why does the training process start with image-caption data before moving to more complex instruction data?

## Architecture Onboarding

- **Component map**: Language Tower (Qwen-2.5-Instruct) -> Vision Tower (SigLIP) -> Projector (Two-layer MLP with GELU) -> Tagging System (Three-level hierarchy) -> Data Pipeline (Collection → Preprocessing → Tagging → Mapping → Synthesis → Filtering → Training)

- **Critical path**: Data collection and preprocessing → Tagging system establishment → Instruction synthesis with filtering → Multi-stage training progression

- **Design tradeoffs**: Open-source vs. closed-source model usage for data synthesis, data quality vs. quantity, computational cost vs. performance gains

- **Failure signatures**: Poor instruction-image alignment (tagging system issues), low data quality despite filtering (synthesis model limitations), training instability (progressive stage design problems)

- **First 3 experiments**: 
  1. Validate the tagging system by testing instruction generation accuracy on a small subset of images with known ground truth
  2. Test the synthesis pipeline end-to-end on a limited dataset to verify filtering effectiveness
  3. Run a single training stage with a small subset of data to verify the progressive training approach works before scaling up

## Open Questions the Paper Calls Out

- **Performance scaling with larger models**: How does Aquila-VL-2B performance scale when trained on larger datasets or with increased model parameters beyond 2 billion? The paper demonstrates performance improvements with increased data but was limited to 2B parameters due to resource constraints.

- **Multilingual and video task effectiveness**: How effective is Infinity-MM for multilingual and video-based multimodal tasks compared to its performance on image-text tasks? The dataset has not yet incorporated multilingual or video data due to time constraints.

- **Synthetic data impact on robustness**: What is the long-term impact of synthetic data on model robustness and generalization, particularly for edge cases or rare instruction types? The evaluation focused on average performance improvements, not on how synthetic data affects model behavior in underrepresented scenarios.

## Limitations
- The tagging system's generalization capability to new images remains untested, creating uncertainty about its effectiveness beyond the training data
- Open-source VLM synthesis quality relative to closed-source alternatives lacks rigorous benchmarking, limiting confidence in the data generation approach
- The paper lacks ablation studies isolating the impact of different data sources and training stages on final performance

## Confidence
- **High Confidence**: The overall data scaling approach and its positive impact on model performance (supported by observed performance improvements with increased data)
- **Medium Confidence**: The effectiveness of the synthetic data generation method using open-source VLMs (reasonable but not directly validated against closed-source alternatives)
- **Medium Confidence**: The tagging system's ability to guide relevant instruction synthesis (plausible methodology but lacks validation on unseen data)

## Next Checks
1. **Tagging System Generalization Test**: Evaluate the instruction-image alignment quality on a held-out test set of images with known ground truth to verify the tagging system's effectiveness beyond the training data.

2. **Open-Source VLM Quality Benchmark**: Compare the quality of synthetic data generated by MiniCPM-V2.6 against data generated by a closed-source VLM (e.g., GPT-4) on the same image set, using human evaluation or established quality metrics.

3. **Data Source Impact Analysis**: Conduct ablation studies to quantify the individual contributions of each data source category (collected vs. synthetic, different quality tiers) to overall model performance, identifying potential over-reliance on any particular source.