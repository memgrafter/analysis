---
ver: rpa2
title: Large Language Models for Psycholinguistic Plausibility Pretesting
arxiv_id: '2402.05455'
source_url: https://arxiv.org/abs/2402.05455
tags:
- plausibility
- human
- sentences
- correlation
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  generate plausibility judgments to replace human pretesting in psycholinguistic
  studies. The authors examine correlation between LLM and human plausibility ratings
  across four datasets with varied syntactic structures, testing multiple models including
  GPT-4, GPT-3.5, and various open-source models.
---

# Large Language Models for Psycholinguistic Plausibility Pretesting

## Quick Facts
- arXiv ID: 2402.05455
- Source URL: https://arxiv.org/abs/2402.05455
- Reference count: 9
- LLMs, especially GPT-4, can generate plausibility judgments highly correlated with human ratings

## Executive Summary
This paper investigates whether large language models can replace human pretesting in psycholinguistic studies by generating plausibility judgments. The authors evaluate multiple LLMs, including GPT-4, GPT-3.5, and various open-source models, across four datasets with different syntactic structures. They find that GPT-4 shows strong correlation with human judgments across all structures, while other models perform better on common syntactic structures than rare ones. The study reveals that LLM judgments are suitable for coarse-grained plausibility filtering but struggle with fine-grained discrimination between similar plausibility levels, even with GPT-4. Additionally, LLM judgments exhibit much lower variance than human judgments, suggesting they may average over multiple human perspectives.

## Method Summary
The authors evaluated multiple LLMs, including GPT-4, GPT-3.5, and open-source models, by comparing their plausibility judgments against human ratings across four existing psycholinguistic datasets. They tested models on various syntactic structures and measured correlation using Pearson correlation coefficients. The study also examined the practical application of LLM judgments for pretesting by filtering implausible materials and assessing the impact on fine-grained discrimination between similar plausibility levels. To understand the variance differences, they compared the spread of LLM judgments against human ratings across all datasets.

## Key Results
- GPT-4 achieved high correlation (up to 0.96 Pearson) with human plausibility judgments across all syntactic structures
- Other models performed well on common syntactic structures but poorly on rare ones
- LLM judgments showed much lower variance than human judgments, suggesting averaging over multiple perspectives

## Why This Works (Mechanism)
The strong correlation between LLM and human plausibility judgments suggests that large language models have learned representations of linguistic acceptability that align with human intuitions about plausibility. This alignment likely stems from LLMs being trained on vast amounts of natural language data, which includes implicit information about what constitutes plausible versus implausible language constructions. The models' ability to capture common syntactic patterns better than rare ones reflects the statistical distribution of language in their training data. The lower variance in LLM judgments compared to human ratings may indicate that the models are providing a consensus view that smooths out individual differences in human judgment.

## Foundational Learning

**Syntactic plausibility judgment**: Understanding how humans assess the naturalness of sentences with varying syntactic structures is crucial for evaluating whether LLMs can replicate this process. Quick check: Review existing psycholinguistic literature on plausibility judgments and their role in experimental design.

**Model evaluation metrics**: Knowledge of appropriate statistical measures for comparing model outputs with human judgments is essential. Quick check: Verify understanding of Pearson correlation and its interpretation in this context.

**Language model capabilities**: Awareness of what different LLMs can and cannot do based on their architecture and training is important for interpreting results. Quick check: Compare the capabilities of GPT-4 versus smaller or open-source models.

## Architecture Onboarding

**Component map**: Human plausibility ratings -> LLM plausibility judgments -> Correlation analysis -> Practical pretesting validation

**Critical path**: The key evaluation pathway is comparing LLM-generated plausibility ratings with human ratings across multiple datasets and syntactic structures, then testing the practical utility of LLM judgments for pretesting.

**Design tradeoffs**: Using LLMs for pretesting trades the nuanced individual differences in human judgment for potentially more consistent but less discriminating evaluations. This tradeoff may be acceptable for coarse-grained filtering but problematic for fine-grained discrimination.

**Failure signatures**: Poor performance on rare syntactic structures, inability to discriminate between similar plausibility levels, and reduced variance compared to human judgments are key indicators of limitations in using LLMs for plausibility pretesting.

**First experiments**:
1. Test LLM judgments on constructed materials designed to probe specific linguistic phenomena not well-represented in existing datasets
2. Conduct controlled experiments with multiple languages to assess cross-linguistic applicability
3. Implement a validation study where psycholinguists use LLM-generated plausibility ratings to design actual experiments

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on existing human plausibility datasets that may not fully capture diverse research needs
- Study focuses primarily on English language materials, limiting generalizability to other languages
- Lower variance in LLM judgments compared to human ratings may indicate oversimplification of nuanced plausibility distinctions

## Confidence

**High**: LLMs can replace human pretesting for coarse-grained plausibility filtering (strong correlations, practical validation)

**Medium**: LLMs perform poorly on fine-grained discrimination between similar plausibility levels (based on limited experimental conditions)

**Low**: LLM judgments represent averaged human perspectives (speculative interpretation without direct evidence)

## Next Checks

1. Test LLM plausibility judgments on constructed materials designed to probe specific linguistic phenomena not well-represented in the existing datasets
2. Conduct controlled experiments with multiple languages to assess cross-linguistic applicability
3. Implement a validation study where psycholinguists use LLM-generated plausibility ratings to design actual experiments and compare outcomes with traditionally pretested materials