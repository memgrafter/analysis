---
ver: rpa2
title: Fine-tuning with Very Large Dropout
arxiv_id: '2403.00946'
source_url: https://arxiv.org/abs/2403.00946
tags:
- dropout
- large
- fine-tuning
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses out-of-distribution (o.o.d.) generalization
  in the fine-tuning setting by proposing very large dropout rates. The core method
  applies extremely high dropout (around 90%) to the penultimate layer during fine-tuning
  of pre-trained networks.
---

# Fine-tuning with Very Large Dropout

## Quick Facts
- arXiv ID: 2403.00946
- Source URL: https://arxiv.org/abs/2403.00946
- Reference count: 27
- Primary result: 90% dropout on penultimate layer during fine-tuning improves OOD generalization by 1.1-2.0% over baselines

## Executive Summary
This paper addresses out-of-distribution (o.o.d.) generalization in the fine-tuning setting by proposing very large dropout rates. The core method applies extremely high dropout (around 90%) to the penultimate layer during fine-tuning of pre-trained networks. This approach outperforms both ensemble methods and weight averaging techniques on four o.o.d. benchmarks (VLCS, OFFICE HOME, PACS, TERRA INCOGNITA), with average improvements of 1.1-2.0% over baselines. The method works because fine-tuning is a near-linear process that can leverage existing features rather than creating new ones.

## Method Summary
The method involves fine-tuning pre-trained RESNET50 or VIT-L-16 models on source domain data while applying 90% dropout to the penultimate layer. The process uses SGD with 0.9 momentum, batch size 32, and learning rates of {10^-3, 5x10^-4} with L2 weight decay {10^-4, 5x10^-5, 10^-5} for 10,000 iterations. The pre-trained models are first trained on ImageNet with data augmentations including TRIVIALAUGMENT, CUTMIX, and RANDOM ERASINGS. Performance is evaluated on held-out target domains to measure o.o.d. generalization.

## Key Results
- 90% dropout during fine-tuning outperforms ERM, ensembles, and weight averaging on all four OOD benchmarks
- Optimal dropout rates range from 90-95% for smaller datasets (VLCS, PACS) and 90% for larger datasets (OFFICE HOME, TERRA INCOGNITA)
- Very large dropout rates only work during fine-tuning, not during training from scratch
- Performance improves with richer pre-trained representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Very large dropout during fine-tuning forces the model to leverage redundant features rather than rely on a single strongly relevant feature
- Mechanism: Dropout masks 90%+ of the penultimate layer units, forcing the model to distribute its reliance across multiple features. Since fine-tuning is a near-linear process (small weight updates), this redistribution leverages existing features rather than creating new ones
- Core assumption: The pre-trained model contains redundant features that were not strongly utilized during original training but can be leveraged during fine-tuning
- Evidence anchors:
  - [abstract]: "very high dropout rates instead of ensembles to obtain such rich representations"
  - [section 3.3]: "randomly masking above 90% of the units in the representation layer"
  - [corpus]: Weak evidence - corpus shows dropout is used in various contexts but not specifically for 90%+ rates during fine-tuning
- Break condition: If the pre-trained model lacks redundant features, dropout would simply degrade performance rather than improve OOD generalization

### Mechanism 2
- Claim: Fine-tuning is a near-linear process that can leverage existing features without creating new ones
- Mechanism: Small weight updates during fine-tuning mean the model cannot create entirely new feature representations. Instead, it redistributes weight across existing features that were previously underutilized
- Core assumption: The fine-tuning process operates within a small attraction basin where weight changes are minimal
- Evidence anchors:
  - [abstract]: "fine-tuning is a near-linear process that can leverage existing features rather than creating new ones"
  - [section 3.2]: "fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances"
  - [section 4.4]: "high dropout rates make it difficult to create new features (a nonlinear operation), but does not prevent leveraging existing features"
- Break condition: If fine-tuning involves large weight changes (nonlinear regime), dropout would prevent learning necessary new features

### Mechanism 3
- Claim: Very large dropout acts as a form of L2 regularization that encourages richer representations
- Mechanism: By forcing the model to use multiple features simultaneously, dropout prevents the model from overfitting to a single strongly relevant feature, creating a more distributed representation
- Core assumption: L2 regularization encourages sparse representations in i.i.d. settings, but very large dropout has the opposite effect in OOD settings
- Evidence anchors:
  - [abstract]: "using very high dropout rates instead of ensembles to obtain such rich representations"
  - [section 3.2]: "L2 regularization on the inner layer parameters plays the different role of encouraging sparse representations"
  - [corpus]: No direct evidence - corpus mentions dropout regularization but not specifically for rich representations in OOD settings
- Break condition: If the model already has optimal sparse representations for the task, adding dropout would only degrade performance

## Foundational Learning

- Concept: Out-of-distribution (OOD) generalization
  - Why needed here: The paper's entire premise is that standard fine-tuning methods fail to generalize when test data distribution differs from training distribution
  - Quick check question: Why does a model that performs well on training data sometimes perform poorly on test data from a different distribution?

- Concept: Feature redundancy and relevance
  - Why needed here: The paper relies on understanding the difference between strongly relevant features (improve training error) and weakly relevant features (may help in OOD settings)
  - Quick check question: What is the difference between a feature that is "strongly relevant" versus "weakly relevant" for a classification task?

- Concept: Linear vs nonlinear feature learning
  - Why needed here: The paper's key insight is that fine-tuning is a linear process that can leverage existing features but cannot create new ones
  - Quick check question: Why can fine-tuning leverage existing features more easily than creating entirely new feature representations?

## Architecture Onboarding

- Component map: Pre-trained RESNET50/VIT-L-16 → Fine-tuning with 90% dropout on penultimate layer → Evaluation on OOD test set
- Critical path: Pre-training (outside scope) → Fine-tuning with dropout → Hyperparameter selection via i.i.d. validation → OOD evaluation
- Design tradeoffs: Higher dropout rates improve OOD performance but may reduce i.i.d. performance; dropout works only with rich pre-trained representations
- Failure signatures: Poor i.i.d. performance suggests dropout rate too high; poor OOD performance suggests insufficient feature redundancy in pre-trained model
- First 3 experiments:
  1. Replicate Table 1 results: Compare ERM vs. very large dropout (90%) on VLCS dataset with RESNET50
  2. Test dropout rate sensitivity: Run fine-tuning with 0%, 50%, 90%, 95% dropout rates on VLCS to verify smooth relationship
  3. Verify pre-training dependency: Compare very large dropout results on RESNET #1 vs RESNET #2 from Table 5 to confirm richer representations yield better results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why very large dropout rates (90%+) are only effective during fine-tuning and not during training from scratch?
- Basis in paper: [explicit] The paper states "Using large dropout rates is akin to a form of L2 regularization, expressing a richer set of features even if redundant" and shows that dropout rates above 50% negatively impact both i.i.d. and o.o.d. performance when training from scratch.
- Why unresolved: The paper demonstrates this empirical phenomenon but does not provide a rigorous theoretical explanation for why this linear approximation works during fine-tuning but fails during initial training.
- What evidence would resolve it: Mathematical analysis showing why the near-linear nature of fine-tuning allows large dropout to work, while the highly non-linear nature of training from scratch makes it ineffective.

### Open Question 2
- Question: How does the optimal dropout rate (90-95%) relate to the size of the fine-tuning dataset?
- Basis in paper: [explicit] The paper notes that optimal dropout rates range from 90-95% for VLCS and PACS (10k examples) and becomes slightly smaller at about 90% for larger datasets OFFICE HOME and TERRA INCOGNITA (15k to 25k examples).
- Why unresolved: The paper observes this relationship but does not explain the underlying mechanism or provide a predictive model for how dropout rate should scale with dataset size.
- What evidence would resolve it: Systematic experiments across a wider range of dataset sizes to establish a quantitative relationship between dataset size and optimal dropout rate.

### Open Question 3
- Question: Can very large dropout be effectively combined with other regularization techniques beyond those tested (larger last-layer learning rates, ensembles, weight averaging)?
- Basis in paper: [explicit] The paper states "very-large dropout approach is compatible to these existing fine-tuning techniques" and shows compatibility with larger last-layer learning rates and ensembles/weight averaging.
- Why unresolved: The paper only tests a limited set of existing fine-tuning techniques and does not explore whether other forms of regularization (L1, L2, mixup, etc.) could provide additional benefits when combined with very large dropout.
- What evidence would resolve it: Comprehensive experiments testing very large dropout with various other regularization methods to identify synergistic combinations.

## Limitations
- The method requires pre-trained models with sufficient feature redundancy to be effective
- Gains are modest (1.1-2.0% average improvement) and may not justify added complexity in all applications
- The theoretical foundation for why 90%+ dropout works specifically during fine-tuning remains incomplete

## Confidence
- Mechanism 1 (feature redundancy): Medium - empirical support is strong but theoretical foundation could be deeper
- Mechanism 2 (near-linear fine-tuning): Medium-High - well-supported by experimental evidence but could use more rigorous analysis
- Overall method effectiveness: High - consistent results across multiple benchmarks and architectures

## Next Checks
1. **Pre-training dependency validation**: Systematically test very large dropout on models pre-trained with varying levels of data augmentation richness to quantify the relationship between pre-training quality and OOD performance gains.

2. **Architecture generalizability**: Evaluate the method on non-residual architectures (e.g., transformers without residual connections) to test whether the approach generalizes beyond residual networks.

3. **Mechanism ablation study**: Compare very large dropout against other regularization techniques (e.g., L2, mixup) during fine-tuning to isolate which aspects of the regularization are most important for OOD generalization.