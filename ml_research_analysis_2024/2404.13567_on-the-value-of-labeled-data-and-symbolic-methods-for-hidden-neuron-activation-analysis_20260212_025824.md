---
ver: rpa2
title: On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation
  Analysis
arxiv_id: '2404.13567'
source_url: https://arxiv.org/abs/2404.13567
tags:
- concept
- images
- activation
- concepts
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel model-agnostic post-hoc explainable
  AI method that interprets hidden neuron activations in deep learning models using
  symbolic reasoning over large-scale background knowledge. The method employs OWL-based
  Concept Induction to automatically generate high-level human-understandable explanations
  from a Wikipedia-derived concept hierarchy of approximately 2 million classes.
---

# On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis

## Quick Facts
- arXiv ID: 2404.13567
- Source URL: https://arxiv.org/abs/2404.13567
- Authors: Abhilekha Dalal; Rushrukh Rayan; Adrita Barua; Eugene Y. Vasserman; Md Kamruzzaman Sarker; Pascal Hitzler
- Reference count: 40
- One-line primary result: Novel model-agnostic post-hoc explainable AI method using symbolic reasoning over large-scale background knowledge achieves statistically significant performance in explaining hidden neuron activations, outperforming both multimodal explainable AI methods and large language models.

## Executive Summary
This paper introduces a novel model-agnostic post-hoc explainable AI method that interprets hidden neuron activations in deep learning models using symbolic reasoning over large-scale background knowledge. The method employs OWL-based Concept Induction to automatically generate high-level human-understandable explanations from a Wikipedia-derived concept hierarchy of approximately 2 million classes. Evaluated on a CNN for image scene classification, the approach achieved statistically significant performance in explaining individual neuron activations, outperforming both a pre-trained multimodal explainable AI method (CLIP-Dissect) and a large language model (GPT-4) in terms of both quantitative metrics and qualitative advantages. The method demonstrates the ability to provide inherently explainable, systematic interpretations of deep learning decisions without requiring manual concept selection or modifications to the underlying model architecture.

## Method Summary
The proposed method extracts dense layer activations from a pre-trained CNN model (ResNet50V2) on the ADE20K dataset, then uses Concept Induction with the ECII system to generate label hypotheses for each neuron based on positive and negative examples derived from activation thresholds. These hypotheses are confirmed by retrieving images from Google Images and testing neuron activation, followed by statistical evaluation using the Mann-Whitney U test to compare activation distributions between target and non-target images. The method also incorporates Concept Activation Analysis using SVM classifiers with CAV/CAR kernels to measure concept relevance in the hidden layer activation space. The approach is compared against CLIP-Dissect and GPT-4 using the same evaluation pipeline, with results showing statistically significant performance advantages.

## Key Results
- Concept Induction with OWL-based reasoning successfully generated human-understandable explanations for hidden neuron activations using a Wikipedia-derived concept hierarchy
- Statistical evaluation using Mann-Whitney U test confirmed label hypotheses with significant p-values (p < 0.05) for target vs. non-target activation distributions
- The proposed method outperformed CLIP-Dissect and GPT-4 in both quantitative metrics (confirmed labels, statistical significance) and qualitative aspects (systematic, explainable interpretations)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept Induction can generate meaningful labels for hidden neurons by using background knowledge.
- Mechanism: Concept Induction uses deductive reasoning over a large-scale concept hierarchy (derived from Wikipedia) to generate class expressions that match positive examples (images activating a neuron) while excluding negative examples (images not activating the neuron).
- Core assumption: The background knowledge contains relevant concepts that align with the features detected by hidden neurons, and the activation patterns of neurons are sufficiently distinct to allow Concept Induction to differentiate them.
- Evidence anchors:
  - [abstract] The method "employs OWL-based Concept Induction to automatically generate high-level human-understandable explanations from a Wikipedia-derived concept hierarchy of approximately 2 million classes."
  - [section 3.1] "For the Concept Induction analysis, positive and negative example sets will contain images from ADE20K... All objects from all images are then mapped to classes in the class hierarchy using the Levenshtein string similarity metric."
  - [corpus] Found 25 related papers with average neighbor FMR=0.471, indicating moderate relevance in the literature space for neuron activation analysis methods.
- Break condition: If the background knowledge lacks relevant concepts or if neuron activation patterns are too noisy or overlapping to be distinguished by Concept Induction, the method will fail to produce meaningful labels.

### Mechanism 2
- Claim: Statistical evaluation using the Mann-Whitney U test can confirm whether generated labels accurately describe neuron activation patterns.
- Mechanism: The method compares activation values for images retrieved using the target label keyword versus images retrieved using other keywords, using a non-parametric test to assess statistical significance.
- Core assumption: There is a measurable difference in neuron activation between target and non-target images that can be detected statistically.
- Evidence anchors:
  - [section 3.4] "We consider each neuron-label pair (rows in Table 5, 6, 7) to be a hypothesis... The corresponding null hypothesis is that activation values are not different."
  - [section 3.4] "We therefore base our statistical assessment on the Mann-Whitney U test which is a non-parametric test that does not require a normal distribution."
  - [corpus] Error-margin Analysis for Hidden Neuron Activation Labels discusses similar statistical approaches for validating activation labels.
- Break condition: If neuron activation values are too similar between target and non-target images, or if the sample sizes are too small, the statistical test may fail to reject the null hypothesis even for valid labels.

### Mechanism 3
- Claim: Concept Activation Analysis can measure the degree of relevance of concepts within the hidden layer activation space, validating the quality of generated explanations.
- Mechanism: The method trains concept classifiers (using SVM with CAV or CAR kernels) to determine whether concepts can be reliably detected in the hidden layer activation space, and uses k-fold cross-validation to ensure robustness.
- Core assumption: The hidden layer activation space contains information that can be mapped to high-level concepts, and concept classifiers can learn this mapping effectively.
- Evidence anchors:
  - [abstract] "Evaluation through statistical analysis and degree of concept activation in the hidden layer show that our method provides a competitive edge."
  - [section 3.4] "For each candidate concept, a set of images are collected using Imageye... The transformed dataset is split into train (80%) and test (20%) datasets. Thereafter, a Support Vector Machine (SVM) is trained using the train split."
  - [section 3.4] "We use Concept Induction, CLIP-Dissect, and GPT-4 as Concept Extraction mechanism. Thereafter we use Concept Activation analysis to measure to what extent such concepts are identifiable in the hidden layer activation space."
- Break condition: If the concept classifiers cannot achieve high test accuracy or if cross-validation p-values are not significant, the concepts may not be reliably identifiable in the activation space.

## Foundational Learning

- Concept: OWL (Web Ontology Language) and Description Logics
  - Why needed here: The method relies on OWL-based reasoning for Concept Induction, so understanding the logical foundations is essential for grasping how explanations are generated.
  - Quick check question: What is the difference between OWL and first-order predicate logic in terms of expressiveness and computational complexity?

- Concept: Statistical hypothesis testing (Mann-Whitney U test)
  - Why needed here: The validation of label hypotheses depends on comparing activation distributions between target and non-target images using non-parametric tests.
  - Quick check question: When would you choose a Mann-Whitney U test over a t-test for comparing two groups?

- Concept: Concept Activation Vectors (CAVs) and Concept Activation Regions (CARs)
  - Why needed here: These are the techniques used to measure concept relevance in the hidden layer activation space through SVM-based classifiers.
  - Quick check question: What is the key difference between CAV and CAR approaches in terms of decision boundaries?

## Architecture Onboarding

- Component map:
  CNN model (ResNet50V2) trained on ADE20K dataset -> Dense layer activations extraction -> Concept Induction engine (ECII) with Wikipedia concept hierarchy -> Image retrieval system (Imageye) for label confirmation -> Statistical evaluation pipeline (Mann-Whitney U test) -> Concept Activation Analysis pipeline (SVM with CAV/CAR)

- Critical path:
  1. Extract dense layer activations from trained CNN
  2. For each neuron, select positive/negative examples based on activation thresholds
  3. Run Concept Induction to generate label hypotheses
  4. Confirm labels by retrieving images and testing neuron activation
  5. Perform statistical evaluation on confirmed labels
  6. Run Concept Activation Analysis on all concepts

- Design tradeoffs:
  - Using a large background knowledge base provides more comprehensive explanations but requires more preprocessing and computational resources
  - The 80% activation threshold for positive examples is somewhat arbitrary and could affect label quality
  - Focusing on individual neurons rather than neuron groups simplifies analysis but may miss distributed representations

- Failure signatures:
  - Low statistical significance in Mann-Whitney U tests (p-values not < 0.05)
  - Concept Activation Analysis showing low test accuracy (< 80%)
  - Concept Induction returning class expressions with very low coverage scores
  - Neuron activation values being too similar between target and non-target images

- First 3 experiments:
  1. Run Concept Induction on a single neuron with known ground truth concept to verify basic functionality
  2. Compare activation distributions for target vs. non-target images for a confirmed label to ensure statistical significance
  3. Test Concept Activation Analysis on a simple concept with high expected relevance to validate the pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Concept Induction compare when using background knowledge bases of different sizes or domains?
- Basis in paper: [inferred] The paper mentions that they used a Wikipedia-derived concept hierarchy with approximately 2 million classes and suggests investigating the impact of different scales of background knowledge.
- Why unresolved: The paper only used one specific background knowledge base (Wikipedia concept hierarchy) and did not compare performance across different knowledge base sizes or domains.
- What evidence would resolve it: Experiments comparing Concept Induction performance using various background knowledge bases (e.g., different sizes, domains, or sources) would provide insights into how the choice of background knowledge affects the quality of explanations generated.

### Open Question 2
- Question: Can the proposed Concept Induction method be effectively applied to other neural network architectures beyond CNNs, such as transformers or recurrent neural networks?
- Basis in paper: [inferred] The paper states that their method is "model-agnostic" and mentions its potential applicability to "any neural network architecture," but only demonstrated it on CNNs.
- Why unresolved: The paper only validated the method on a specific CNN architecture for image scene classification, leaving the generalizability to other architectures unexplored.
- What evidence would resolve it: Applying the Concept Induction method to different neural network architectures (e.g., transformers, RNNs) and evaluating its performance in generating meaningful explanations would demonstrate its generalizability.

### Open Question 3
- Question: How does the proposed method handle cases where the background knowledge base does not contain relevant concepts for certain neuron activations?
- Basis in paper: [inferred] The paper discusses using a large-scale background knowledge base but does not address how the method handles cases where relevant concepts are missing from the knowledge base.
- Why unresolved: The paper does not provide information on how the method handles situations where the background knowledge base lacks concepts that could explain certain neuron activations.
- What evidence would resolve it: Investigating the method's performance when applied to domains or datasets where the background knowledge base may not contain all relevant concepts would reveal its robustness and limitations in such scenarios.

## Limitations
- The 80% activation threshold for selecting positive examples is somewhat arbitrary and may not capture all relevant neuron behaviors, particularly for polysemantic neurons that respond to multiple concepts
- The Levenshtein string similarity metric for mapping ADE20K objects to Wikipedia classes may introduce noise or mismatches, especially for compound concepts or synonyms
- Concept Activation Analysis results are reported for all concepts rather than specifically for confirmed labels, making it difficult to assess whether the most relevant concepts are being detected

## Confidence
- High confidence in the core Concept Induction methodology and its ability to generate symbolic explanations from background knowledge
- Medium confidence in the statistical evaluation approach, as the Mann-Whitney U test assumes independence between samples which may not hold perfectly
- Medium confidence in the comparative results with CLIP-Dissect and GPT-4, given the different nature of these approaches and potential evaluation bias

## Next Checks
1. Test the method on neurons with known ground truth concepts to establish a baseline for expected performance and identify potential failure modes
2. Analyze the distribution of Levenshtein similarity scores used for object-class mapping to quantify potential noise in the background knowledge integration
3. Compare Concept Activation Analysis results specifically for confirmed labels versus all concepts to validate whether the most relevant explanations are being captured