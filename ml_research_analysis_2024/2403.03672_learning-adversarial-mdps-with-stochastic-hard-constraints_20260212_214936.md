---
ver: rpa2
title: Learning Adversarial MDPs with Stochastic Hard Constraints
arxiv_id: '2403.03672'
source_url: https://arxiv.org/abs/2403.03672
tags:
- algorithm
- constraints
- probability
- holds
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses online learning in constrained Markov decision
  processes (CMDPs) with adversarial losses and stochastic hard constraints, under
  bandit feedback. The authors consider three distinct scenarios based on how constraints
  are satisfied: (1) achieving sublinear cumulative positive constraint violation,
  (2) ensuring safety (constraints satisfied at every episode) with high probability,
  and (3) achieving constant cumulative positive constraint violation while maintaining
  sublinear regret.'
---

# Learning Adversarial MDPs with Stochastic Hard Constraints

## Quick Facts
- arXiv ID: 2403.03672
- Source URL: https://arxiv.org/abs/2403.03672
- Reference count: 40
- Primary result: Achieves sublinear regret with either sublinear, constant, or high-probability constraint satisfaction in adversarial CMDPs under bandit feedback

## Executive Summary
This paper addresses online learning in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints under bandit feedback. The authors propose three algorithms that achieve different trade-offs between regret minimization and constraint satisfaction: SV-OPS achieves sublinear regret and sublinear violation, S-OPS ensures safety with high probability while maintaining sublinear regret, and CV-OPS achieves constant violation with sublinear regret by estimating a strictly feasible policy. The work establishes fundamental limits on the achievable trade-off between regret and constraint violation, showing that a dependence on the Slater's parameter is unavoidable for algorithms achieving o(√T) violation.

## Method Summary
The paper proposes three algorithms for online learning in adversarial CMDPs with stochastic hard constraints under bandit feedback. SV-OPS uses optimistic policy selection with online updates of constraint-satisfying policy sets to achieve sublinear regret and sublinear positive constraint violation. S-OPS extends SV-OPS by randomizing between an optimistic policy and a known strictly feasible policy to ensure safety with high probability while maintaining sublinear regret. CV-OPS estimates a strictly feasible policy and its associated constraint violation in a constant number of episodes using two no-regret algorithms, then runs S-OPS with these estimates to achieve constant violation and sublinear regret. All algorithms use online mirror descent with confidence bounds on costs and transitions.

## Key Results
- SV-OPS achieves O(√T) regret and O(√T) positive constraint violation under standard CMDP assumptions
- S-OPS attains O(√T) regret while ensuring safety (constraints satisfied at every episode) with high probability
- CV-OPS achieves constant positive constraint violation and O(√T) regret by estimating a strictly feasible policy
- Lower bound shows any algorithm achieving o(√T) violation must have regret dependent on the Slater's parameter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SV-OPS uses optimistic policy selection to ensure sublinear regret and sublinear positive constraint violation.
- Mechanism: The algorithm maintains an "optimistic" set of occupancy measures that always includes the true feasible set. By projecting onto this optimistic set using confidence bounds, it can select policies that minimize loss while satisfying constraints with high probability. The key is that the optimistic set shrinks toward the true feasible set over time, ensuring convergence.
- Core assumption: There exists a feasible solution to the CMDP constraints (Assumption 1).
- Evidence anchors:
  - [abstract] "SV-OPS, which uses optimistic policy selection and updates the set of constraint-satisfying policies online"
  - [section] "SV-OPS works by selecting policies that optimistically satisfy the constraints"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the optimistic set fails to contain the true feasible set due to overly conservative confidence bounds, the algorithm may violate constraints or incur higher regret.

### Mechanism 2
- Claim: S-OPS achieves safety with high probability by randomizing between an optimistic policy and a known strictly feasible policy.
- Mechanism: At each episode, S-OPS selects a probability λt that determines the randomization between the policy from SV-OPS and a known strictly feasible policy π⋄. This probability is chosen pessimistically using upper confidence bounds on costs and upper occupancy bounds, ensuring that constraints are satisfied with high probability while still allowing exploration.
- Core assumption: A strictly feasible policy π⋄ is known to the learner (Assumption 3).
- Evidence anchors:
  - [abstract] "S-OPS is designed to ensure safety by randomizing between an optimistic policy and a known strictly feasible policy"
  - [section] "S-OPS works by selecting, at each episode, a suitable randomization between the policy that SV-OPS would choose and the (known) policy strictly satisfying the constraints"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the known strictly feasible policy is not truly feasible or if the pessimistic randomization probability becomes too large, the algorithm may fail to achieve sublinear regret.

### Mechanism 3
- Claim: CV-OPS achieves constant violation and sublinear regret by estimating a strictly feasible policy in a constant number of episodes.
- Mechanism: The algorithm runs two no-regret algorithms in parallel - one to minimize violation and another to select the most violated constraint. After a constant number of episodes determined by the Slater's parameter ρ, it estimates a strictly feasible policy and its associated violation, then runs S-OPS with these estimates.
- Core assumption: There exists a strictly feasible policy, but it is not known to the learner (only Assumption 2).
- Evidence anchors:
  - [abstract] "CV-OPS, which estimates a strictly feasible policy and its associated constraint violation in a constant number of episodes, then runs S-OPS with these estimates"
  - [section] "Algorithm 4 needs access to two anytime no-regret algorithms... to learn an estimated strictly feasible policy and one for the full-feedback setting on the simplex, which learns the most violated constraint"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the estimation phase takes too many episodes or the estimated Slater's parameter is too small, the algorithm may fail to achieve constant violation.

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs) - MDPs with additional constraints on expected costs
  - Why needed here: The entire paper addresses learning algorithms for CMDPs with adversarial losses and stochastic hard constraints
  - Quick check question: What distinguishes a CMDP from a standard MDP, and how are constraints typically formulated?

- Concept: Bandit feedback in reinforcement learning - observing only the trajectory taken, not the entire loss function
  - Why needed here: The algorithms are designed to work with bandit feedback, observing only losses and costs for visited state-action pairs
  - Quick check question: How does bandit feedback differ from full feedback in MDPs, and what challenges does it introduce?

- Concept: Online convex optimization and mirror descent - techniques for sequential decision making under uncertainty
  - Why needed here: The algorithms use online mirror descent (OMD) updates for policy selection and constraint satisfaction
  - Quick check question: How does OMD work in the context of MDPs, and what role does the projection step play?

## Architecture Onboarding

- Component map: SV-OPS -> S-OPS -> CV-OPS, where each algorithm builds on the previous one with additional mechanisms for constraint satisfaction
- Critical path: For SV-OPS: update occupancy bounds → compute optimistic loss estimates → perform OMD update → project onto optimistic constraint set → select policy. For S-OPS: same as SV-OPS but with randomization added. For CV-OPS: run primal/dual algorithms → estimate Slater's parameter → run S-OPS with estimates.
- Design tradeoffs: SV-OPS trades conservatism for exploration, S-OPS trades additional regret for safety guarantees, CV-OPS trades constant violation for not requiring known feasible policy. All algorithms balance exploration-exploitation through confidence bounds.
- Failure signatures: Linear violation growth indicates overly conservative bounds, safety violations indicate incorrect feasible policy knowledge, high regret indicates insufficient exploration or poor estimation.
- First 3 experiments:
  1. Implement SV-OPS on a simple CMDP with known feasible solution to verify sublinear violation and regret bounds
  2. Test S-OPS on a CMDP where a feasible policy is known to verify safety guarantees while maintaining sublinear regret
  3. Implement CV-OPS on a CMDP with unknown feasible policy to verify constant violation and sublinear regret after estimation phase

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dependence on the Slater's parameter ρ for achieving sublinear regret with constant cumulative positive constraint violation?
- Basis in paper: [explicit] The paper states "a dependence on the Slater's parameter ρ is unavoidable" and provides a lower bound showing that "any algorithm attaining o(√T) violation cannot avoid a dependence on the Slater's parameter in the regret bound."
- Why unresolved: The paper provides upper bounds showing O(1/ρ^4) dependence but does not establish whether this is tight or if a smaller dependence is achievable.
- What evidence would resolve it: A matching lower bound that shows the regret must be at least Ω(1/ρ^c) for some constant c, proving the optimality of the O(1/ρ^4) upper bound.

### Open Question 2
- Question: Can we achieve sublinear regret and sublinear violation without assuming knowledge of a strictly feasible policy or its costs?
- Basis in paper: [inferred] The paper shows that without the strictly feasible policy (Assumption 3), we can only achieve constant violation, not sublinear violation. This suggests a fundamental limitation.
- Why unresolved: The paper does not provide a definitive answer on whether this limitation is inherent to the problem or if a more sophisticated algorithm could circumvent it.
- What evidence would resolve it: A lower bound showing that any algorithm achieving sublinear regret and sublinear violation must have knowledge of a strictly feasible policy or its costs, or an algorithm that achieves sublinear regret and sublinear violation without such knowledge.

### Open Question 3
- Question: What is the best achievable regret bound when the constraint violation is allowed to be sublinear (o(T)) but not necessarily constant?
- Basis in paper: [explicit] The paper only considers two extremes: sublinear violation (Theorem 1) and constant violation (Theorem 5). It does not explore the intermediate regime of sublinear but non-constant violation.
- Why unresolved: The paper does not provide a comprehensive analysis of the regret-violation trade-off for intermediate regimes of constraint violation.
- What evidence would resolve it: An algorithm that achieves a specific regret bound (e.g., O(T^α) for some α ∈ (0,1)) while maintaining a corresponding violation bound (e.g., O(T^β) for some β ∈ (0,1) with β < α).

## Limitations

- The algorithms require knowing the number of episodes T in advance, limiting their applicability in truly online settings
- The requirement for Slater's condition (strict feasibility) is crucial but may not hold in many practical problems
- The lower bound proof assumes specific problem structures that may not capture all realistic scenarios

## Confidence

- **High Confidence:** Regret bounds for SV-OPS and S-OPS under their respective assumptions
- **Medium Confidence:** The constant violation guarantee of CV-OPS, as it depends on the estimation phase working correctly
- **Low Confidence:** The practical performance of these algorithms compared to heuristic approaches in real-world applications

## Next Checks

1. Implement SV-OPS on a synthetic CMDP with known Slater's parameter to empirically verify the sublinear violation and regret bounds across different problem sizes
2. Test S-OPS in a safety-critical environment where a feasible policy is known but suboptimality needs to be addressed, measuring both safety violations and regret
3. Apply CV-OPS to a problem where no feasible policy is known, validating that the estimation phase correctly identifies a feasible policy within the claimed constant number of episodes