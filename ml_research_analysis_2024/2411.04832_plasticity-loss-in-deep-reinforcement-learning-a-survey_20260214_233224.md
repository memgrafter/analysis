---
ver: rpa2
title: 'Plasticity Loss in Deep Reinforcement Learning: A Survey'
arxiv_id: '2411.04832'
source_url: https://arxiv.org/abs/2411.04832
tags: []
core_contribution: This paper surveys plasticity loss in deep reinforcement learning
  (RL), where neural networks lose the ability to learn and adapt to new data distributions.
  The authors define plasticity loss, review its potential causes (such as rank collapse,
  non-stationarity, and parameter norm growth), and categorize mitigation strategies
  (including weight resets, parameter regularization, and feature rank regularization).
---

# Plasticity Loss in Deep Reinforcement Learning: A Survey

## Quick Facts
- arXiv ID: 2411.04832
- Source URL: https://arxiv.org/abs/2411.04832
- Authors: Timo Klein; Lukas Miklautz; Kevin Sidak; Claudia Plant; Sebastian Tschiatschek
- Reference count: 40
- This paper surveys plasticity loss in deep reinforcement learning (RL), where neural networks lose the ability to learn and adapt to new data distributions.

## Executive Summary
This survey examines plasticity loss in deep reinforcement learning, where neural networks progressively lose their ability to adapt to new data distributions during training. The authors systematically define plasticity loss and review its potential causes, including rank collapse of feature representations, non-stationarity of the learning environment, and growth in parameter norms. They categorize mitigation strategies into weight resets, parameter regularization, and feature rank regularization, finding that well-established techniques like LayerNorm and L2 regularization often outperform domain-specific approaches. The survey also highlights critical gaps in theoretical understanding and calls for broader empirical evaluation across diverse RL domains.

## Method Summary
The authors conducted a comprehensive literature review of plasticity loss in deep RL, synthesizing findings from empirical studies across multiple RL algorithms and environments. They systematically categorized causes of plasticity loss into three main groups: rank collapse of feature representations, non-stationarity effects, and parameter norm growth. Mitigation strategies were organized into three categories based on their mechanisms: weight resets that periodically reinitialize parameters, parameter regularization techniques that constrain weight updates, and feature rank regularization methods that preserve representational capacity. The survey primarily relies on empirical observations from published works rather than introducing new experimental results.

## Key Results
- Plasticity loss manifests as progressive degradation in an agent's ability to learn from new experiences in deep RL
- LayerNorm and L2 regularization are surprisingly effective at mitigating plasticity loss, often outperforming specialized techniques
- Most empirical studies focus on limited domains (Atari, simple control tasks), constraining generalizability
- Theoretical understanding of plasticity loss mechanisms remains limited despite empirical observations

## Why This Works (Mechanism)
Assumption: Plasticity loss occurs when neural networks lose representational capacity during training, preventing adaptation to new data distributions. The effectiveness of LayerNorm and L2 regularization likely stems from their ability to maintain stable gradient flow and prevent parameter explosion, though precise mechanisms remain unclear.

## Foundational Learning
- **Rank collapse**: Why needed - prevents neural networks from representing diverse features; Quick check - measure singular values of feature matrices during training
- **Non-stationarity in RL**: Why needed - changing data distributions affect learning stability; Quick check - track reward distribution shifts over training episodes
- **Parameter norm growth**: Why needed - exploding gradients can saturate activations; Quick check - monitor L2 norm of weight matrices across layers
- **Feature representation analysis**: Why needed - understanding what information is preserved/lost; Quick check - visualize activation patterns using t-SNE
- **Regularization effectiveness**: Why needed - determines which techniques preserve learning capacity; Quick check - compare training curves with/without specific regularizers
- **Spectral properties of networks**: Why needed - relates to information flow and representational capacity; Quick check - compute eigenvalue spectra of weight matrices

## Architecture Onboarding

**Component Map:**
Input -> Feature Extractor -> Value/Policy Head -> Loss Computation -> Parameter Update -> Replay Buffer

**Critical Path:**
Experience collection → Feature extraction → Value/policy estimation → Loss computation → Gradient update → Parameter storage

**Design Tradeoffs:**
- Regularization strength vs. learning speed: Stronger regularization preserves plasticity but may slow initial learning
- Weight reset frequency vs. stability: More frequent resets maintain plasticity but can disrupt learned behaviors
- Feature preservation vs. efficiency: Rank regularization maintains capacity but adds computational overhead

**Failure Signatures:**
- Flattening learning curves despite new experiences
- Increasing variance in performance across training runs
- Degraded performance on previously mastered tasks
- Collapsed singular value spectra in feature representations

**First Experiments:**
1. Measure singular value decay in feature representations across training epochs
2. Compare learning curves with LayerNorm vs. no normalization across multiple RL algorithms
3. Test weight reset intervals on plasticity retention in non-stationary environments

## Open Questions the Paper Calls Out
Unknown: The survey does not explicitly enumerate open questions, but implicit gaps include: How do different RL algorithms exhibit varying degrees of plasticity loss? What are the theoretical bounds on plasticity preservation? How do complex, multi-task environments affect plasticity loss dynamics?

## Limitations
- Empirical studies focus primarily on Atari and simple control tasks, limiting generalizability to complex domains
- Many mitigation strategies lack rigorous theoretical foundations connecting mechanisms to plasticity preservation
- Comparative analyses across different methods are inconsistent due to varying experimental setups
- Interactions between plasticity loss factors (rank collapse, non-stationarity, parameter growth) are not fully characterized

## Confidence
- **High confidence**: The existence and empirical observation of plasticity loss in deep RL agents
- **Medium confidence**: The categorization of plasticity loss causes and mitigation strategies
- **Medium confidence**: The relative effectiveness of LayerNorm and L2 regularization compared to domain-specific methods

## Next Checks
1. Conduct systematic ablation studies isolating the effects of rank collapse, non-stationarity, and parameter norm growth on plasticity loss across multiple RL algorithms
2. Design experiments testing mitigation strategies in more complex, multi-task environments beyond Atari and control benchmarks
3. Develop theoretical frameworks connecting spectral properties of neural network representations to plasticity retention, validated through controlled experiments