---
ver: rpa2
title: Efficient Availability Attacks against Supervised and Contrastive Learning
  Simultaneously
arxiv_id: '2402.04010'
source_url: https://arxiv.org/abs/2402.04010
tags:
- contrastive
- attacks
- learning
- supervised
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AUE and AAP, two efficient availability attacks
  against supervised and contrastive learning. By enhancing data augmentations in
  supervised error minimization/maximization frameworks, these attacks implicitly
  optimize contrastive loss, achieving state-of-the-art worst-case unlearnability
  across SL and CL algorithms with less computation.
---

# Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously

## Quick Facts
- arXiv ID: 2402.04010
- Source URL: https://arxiv.org/abs/2402.04010
- Authors: Yihan Wang; Yifan Zhu; Xiao-Shan Gao
- Reference count: 40
- Key outcome: AUE and AAP achieve state-of-the-art worst-case unlearnability across supervised and contrastive learning algorithms with 3-17x faster computation than CL-based attacks.

## Executive Summary
This paper introduces AUE and AAP, two availability attacks that simultaneously degrade performance of supervised and contrastive learning algorithms. The key insight is that using contrastive-like data augmentations in supervised error minimization/maximization frameworks implicitly optimizes contrastive loss, enabling effective attacks across multiple learning paradigms. The attacks achieve 7.5-38.7% better contrastive unlearnability than existing methods while being significantly more computationally efficient.

## Method Summary
The method involves generating availability attacks through two frameworks: AUE (error minimization) and AAP (error maximization). Both use contrastive-like data augmentations (RandomResizedCrop, ColorJitter, Grayscale) instead of standard supervised augmentations. Reference models are trained with these augmentations, then perturbations are generated via PGD optimization. The attacks target both supervised and contrastive learning algorithms simultaneously, leveraging the implicit relationship between supervised loss minimization with contrastive augmentations and contrastive loss optimization.

## Key Results
- AUE and AAP outperform existing methods by 7.5-38.7% in contrastive unlearnability on CIFAR-10/100 and Tiny/Mini-ImageNet
- The attacks are 3-17x faster than CL-based attacks due to efficient supervised learning frameworks
- Large alignment and uniformity gaps between clean and poisoned features indicate successful contrastive unlearnability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enhancing data augmentations in supervised error minimization frameworks implicitly optimizes contrastive loss
- Mechanism: Strong contrastive augmentations used during supervised training cause the model to learn features that satisfy both supervised and contrastive objectives simultaneously
- Core assumption: Supervised loss with strong augmentations correlates with contrastive loss reduction
- Evidence anchors: [abstract] "we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks to obtain attacks effective for both SL and CL"; [section 4.1] "training a supervised model with contrastive augmentations implicitly optimizes the contrastive loss"
- Break condition: If augmentation strength becomes too high, it may prevent model from learning useful features for either objective

### Mechanism 2
- Claim: Supervised error-maximization with contrastive augmentations creates adversarial examples that deceive contrastive learning
- Mechanism: Adversarial poisoning generates perturbations that target shifted labels while strong augmentations ensure these perturbations generalize to contrastive learning scenarios
- Core assumption: Non-robust features learned through supervised adversarial training transfer to contrastive learning context
- Evidence anchors: [abstract] "we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks"; [section 4.3] "minimizing LSL(π(x + δ(x, y)), y + K; f ∗) with respect to δ updates poisoned to deceive a contrastive-like reference model f ∗"
- Break condition: If label shifting strategy doesn't align with contrastive learning objectives, effectiveness drops

### Mechanism 3
- Claim: Large alignment and uniformity gaps between clean and poisoned features indicate contrastive unlearnability
- Mechanism: Poisoning creates feature distributions that differ significantly from clean data, making classifier trained on poisoned data perform poorly on clean data
- Core assumption: Contrastive learning optimization directly affects feature alignment and uniformity properties
- Evidence anchors: [section 3.2] "contrastive unlearnability is highly related to huge alignment and uniformity gaps"; [section 3.2] "Pearson correlation coefficient (PCC) between the alignment gap and the SimCLR accuracy is −0.82"
- Break condition: If poisoned features accidentally align well with clean features despite gap magnitude, unlearnability may fail

## Foundational Learning

- Concept: Contrastive learning loss functions (InfoNCE)
  - Why needed here: Understanding how contrastive learning optimization relates to feature alignment and uniformity
  - Quick check question: What is the relationship between positive pair similarity and negative pair discrimination in contrastive learning?

- Concept: Data augmentation strategies in supervised vs contrastive learning
  - Why needed here: Key mechanism involves replacing supervised augmentations with contrastive-like ones
  - Quick check question: How do color jitter and grayscale augmentations affect feature learning differently than simple cropping?

- Concept: Adversarial example generation and transferability
  - Why needed here: Attack relies on creating perturbations that generalize across different model architectures
- Quick check question: What properties make adversarial perturbations transfer well across different network architectures?

## Architecture Onboarding

- Component map: Data augmentation module (Kornia library) -> Reference model trainer (supervised learning with enhanced augmentations) -> Perturbation generator (PGD-based optimization) -> Evaluation pipeline (SL and multiple CL algorithms) -> Gap analysis module (alignment and uniformity computation)

- Critical path: 1. Generate strong augmentations based on contrastive settings; 2. Train reference model with these augmentations; 3. Generate perturbations through iterative optimization; 4. Evaluate across all target algorithms; 5. Analyze feature distribution gaps

- Design tradeoffs: Augmentation strength vs. unlearnability effectiveness; Computational cost vs. attack quality; Sample-wise vs. class-wise perturbation strategies; Single algorithm vs. worst-case across multiple algorithms

- Failure signatures: High accuracy on clean data despite poisoning; Low gap values between clean and poisoned features; Strong preference for specific evaluation algorithm; Instability in perturbation generation process

- First 3 experiments: 1. Compare UE vs AUE performance on CIFAR-10 with SimCLR; 2. Measure alignment/uniformity gaps for different augmentation strengths; 3. Test transferability of AUE perturbations across ResNet, VGG, and MobileNet architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of AUE and AAP attacks scale with increasing image resolution beyond ImageNet-100?
- Basis in paper: [inferred] The paper notes that CL-based methods face challenges with high-resolution images, while AUE and AAP are more efficient, but does not provide extensive evaluation on datasets with higher resolution than ImageNet-100.
- Why unresolved: The experiments in the paper focus on datasets up to ImageNet-100 (224x224 images), leaving uncertainty about performance on larger-scale, higher-resolution datasets.
- What evidence would resolve it: Evaluating AUE and AAP attacks on larger datasets like full ImageNet-1k or other high-resolution datasets, measuring both attack effectiveness and computational efficiency.

### Open Question 2
- Question: What are the theoretical limits of the relationship between supervised loss minimization and contrastive loss minimization under data augmentations?
- Basis in paper: [explicit] The paper provides a toy example showing that minimizing supervised loss with contrastive augmentations can implicitly minimize contrastive loss, but this is a simplified case.
- Why unresolved: The theoretical analysis is limited to a specific toy model with idealized assumptions (e.g., balanced dataset, square full-rank classifier), and does not generalize to complex real-world scenarios.
- What evidence would resolve it: Formal proofs or empirical studies on more realistic models and datasets, exploring the conditions under which supervised loss minimization effectively minimizes contrastive loss.

### Open Question 3
- Question: How do AUE and AAP attacks perform under adaptive defenses that specifically target contrastive learning vulnerabilities?
- Basis in paper: [inferred] The paper evaluates defenses like AdvCL and A V ATAR, but these are general defenses not specifically designed to counter contrastive learning attacks.
- Why unresolved: The paper does not explore defenses that are tailored to exploit weaknesses in contrastive learning or specifically mitigate the effects of AUE and AAP.
- What evidence would resolve it: Developing and testing defenses that focus on contrastive learning properties (e.g., feature alignment/uniformity) and evaluating their effectiveness against AUE and AAP attacks.

## Limitations
- The theoretical justification for why contrastive augmentations in supervised frameworks implicitly optimize contrastive loss remains correlational rather than causal
- Computational efficiency claims depend on specific implementation details and hardware configurations that may not generalize
- The gap-based analysis assumes uniform relationship between alignment/uniformity gaps and performance degradation across all contrastive learning algorithms

## Confidence

**High Confidence**: Empirical results showing AUE and AAP outperform existing methods by 7.5-38.7% in contrastive unlearnability, and the computational efficiency gains (3-17x speedup).

**Medium Confidence**: The core mechanism of using contrastive augmentations in supervised frameworks to achieve dual effectiveness, as the theoretical justification is correlational rather than causal.

**Low Confidence**: The universal applicability of the gap-based analysis across all contrastive learning algorithms, given their architectural and objective differences.

## Next Checks

1. **Ablation Study on Augmentation Strength**: Systematically vary the augmentation strength parameter s to determine the optimal balance between supervised and contrastive unlearnability, validating the claim that this is a key design tradeoff.

2. **Cross-Architecture Transferability**: Test AUE/AAP perturbations on additional backbone architectures beyond ResNet-18 (e.g., EfficientNet, ConvNeXt) to verify the claimed transferability properties and identify any architecture-specific failure modes.

3. **Theoretical Analysis of Loss Relationships**: Conduct a formal analysis of the relationship between supervised loss with contrastive augmentations and contrastive loss itself, potentially through linearization or other analytical techniques to validate the implicit optimization claim.