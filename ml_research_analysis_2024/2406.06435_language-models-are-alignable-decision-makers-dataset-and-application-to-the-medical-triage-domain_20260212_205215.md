---
ver: rpa2
title: 'Language Models are Alignable Decision-Makers: Dataset and Application to
  the Medical Triage Domain'
arxiv_id: '2406.06435'
source_url: https://arxiv.org/abs/2406.06435
tags:
- high
- medical
- moral
- fairness
- care
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel medical triage dataset labeled with\
  \ decision-maker attributes (DMAs) to study human-aligned decision-making with large\
  \ language models (LLMs). The dataset contains 62 scenarios across six DMAs\u2014\
  protocol focus, fairness, risk aversion, continuing care, moral desert, and utilitarianism\u2014\
  where multiple answer choices are labeled as exhibiting high or low levels of each\
  \ attribute."
---

# Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain

## Quick Facts
- arXiv ID: 2406.06435
- Source URL: https://arxiv.org/abs/2406.06435
- Reference count: 20
- Models can be aligned to human values using zero-shot prompting with weighted self-consistency

## Executive Summary
This paper introduces a novel dataset of 62 medical triage scenarios labeled with six decision-maker attributes (protocol focus, fairness, risk aversion, continuing care, moral desert, and utilitarianism) to study human-aligned decision-making with large language models. The authors propose a zero-shot prompting approach combined with weighted self-consistency to align LLM decisions to different attributes. Experiments with various open-source models show that this approach significantly improves alignment accuracy compared to unaligned models, with Llama 2 13B achieving the best performance. The study also finds that larger models tend to be more alignable and that including negative samples in self-consistency provides particular benefits.

## Method Summary
The paper introduces a method for aligning large language models to human decision-making attributes (DMAs) through zero-shot prompting and weighted self-consistency. For each scenario, the model samples multiple outputs using both high and low attribute prompts, then weights the selected answers based on the target attribute value. This approach allows models to be conditioned on specific ethical principles without requiring fine-tuning. The method is evaluated on a dataset of 62 medical triage scenarios where each scenario has multiple answer choices labeled as exhibiting high or low levels of each attribute. The approach uses a temperature parameter of 0.7 for sampling and combines positive and negative samples to reinforce the target attribute value.

## Key Results
- Llama 2 13B achieved 86.4% alignment accuracy for high target attributes and 84.3% F1 score overall
- Weighted self-consistency improved alignment accuracy by up to 13.5% compared to unaligned models
- Larger models (Llama 2 13B) showed significantly better alignment performance than smaller models (Llama 2 7B, Falcon 7B)
- Including negative samples in self-consistency was particularly beneficial for alignment accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted self-consistency improves alignment by balancing high and low attribute prompts to reinforce the target value.
- Mechanism: The model samples outputs using both high and low attribute prompts, then weights the selected answers based on the target attribute value. Positive weights reinforce high attribute values, negative weights penalize low attribute values.
- Core assumption: Including both positive and negative samples helps the model distinguish between aligned and misaligned choices.
- Evidence anchors:
  - [abstract] "Finally, we also introduce a new form of weighted self-consistency that improves the overall quantified performance."
  - [section] "For a given question and attribute, we sample multiple outputs for the high and low attribute prompts, which generate both positive and negative samples (relative to the target attribute value)."
- Break condition: If the negative samples are not truly misaligned with the target attribute, weighting could reinforce incorrect patterns.

### Mechanism 2
- Claim: Zero-shot prompting with attribute definitions grounds model decisions in specific ethical principles without requiring fine-tuning.
- Mechanism: The prompt includes a system message defining the attribute and how it manifests at high and low levels, which conditions the model's reasoning before it generates an answer.
- Core assumption: LLMs can interpret and apply abstract attribute definitions to concrete decision scenarios through prompt conditioning.
- Evidence anchors:
  - [abstract] "We present a novel software framework for human-aligned decision-making by utilizing these DMAs, paving the way for trustworthy AI with better guardrails."
  - [section] "For each of the DMAs described in Sec. 3, we created a prompt that defines that particular attribute and describes how that attribute is expressed at either the high or low levels (see Fig. 2, and Appendix C for the detailed prompts)."
- Break condition: If the attribute definitions are too abstract or context-dependent, the model may fail to apply them correctly.

### Mechanism 3
- Claim: Larger models show greater alignability because they have more capacity to encode and apply complex ethical reasoning patterns.
- Mechanism: The study found that Llama2-13B outperformed smaller models like Llama2-7B and Falcon-7B, suggesting size correlates with alignment capability.
- Core assumption: Model size directly relates to the ability to learn and apply nuanced decision-making attributes.
- Evidence anchors:
  - [abstract] "The authors provide an open-source software framework to enable further research on human-aligned decision-making with LLMs."
  - [section] "Comparing Llama2-7B and 13B, alignment accuracy for both the aligned and aligned + self-consistency configurations was higher for the larger 13B model."
- Break condition: If the larger models overfit to the training distribution or the attribute definitions, their apparent alignability may not generalize.

## Foundational Learning

- Concept: Decision-maker attributes (DMAs) as ethical principles that guide choices in moral dilemmas
  - Why needed here: The dataset and alignment method are built around these attributes, so understanding them is essential for implementing and extending the work
  - Quick check question: Can you explain the difference between high fairness and low fairness in terms of how a decision-maker would allocate resources?

- Concept: Zero-shot prompting as a method to align models without fine-tuning
  - Why needed here: The approach relies on prompt engineering rather than parameter updates, which affects how the system is implemented and scaled
  - Quick check question: How does including attribute definitions in the system message differ from few-shot prompting with examples?

- Concept: Self-consistency as a method to improve reliability of LLM outputs
  - Why needed here: The weighted self-consistency module is a key component of the alignment method, and understanding its mechanics is crucial for debugging
  - Quick check question: Why might including negative samples improve alignment accuracy compared to only using positive samples?

## Architecture Onboarding

- Component map: Scenario loader -> DMA prompt generator -> LLM inference (with temperature sampling) -> Weighted voting module -> Explanation generator -> Evaluation metrics
- Critical path: Scenario -> Prompt construction -> Model sampling -> Weighted aggregation -> Final answer selection
- Design tradeoffs: Prompt-based alignment avoids fine-tuning costs but may be less precise than parameter updates; weighted self-consistency improves reliability but increases inference time
- Failure signatures: Poor alignment on specific attributes suggests issues with prompt definitions or attribute understanding; inconsistent outputs suggest sampling or weighting problems
- First 3 experiments:
  1. Test unaligned model performance on a single attribute to establish baseline tendencies
  2. Implement zero-shot prompting for one attribute and measure alignment improvement
  3. Add weighted self-consistency with both positive and negative samples and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model size beyond 13B parameters affect alignment accuracy for different decision-maker attributes?
- Basis in paper: [explicit] The paper states "The initial evidence in our study suggests that larger models are generally more alignable" and mentions planned experiments with larger Falcon and Mistral models.
- Why unresolved: The current study only tested up to 13B parameter models, leaving open questions about scaling effects on alignment performance.
- What evidence would resolve it: Experiments testing models with 30B+ parameters across all six DMAs would reveal whether alignment accuracy continues to improve with scale or plateaus.

### Open Question 2
- Question: How does the proposed alignment approach generalize to multi-attribute alignment scenarios?
- Basis in paper: [explicit] The authors state "Another interesting direction to pursue is to compare the decisions and explanations of LLMs with that of human decision-makers, to better understand potential differences in decision-making and other gaps in the alignment of these systems."
- Why unresolved: The current framework aligns models to one attribute at a time, but real-world decisions involve multiple attributes simultaneously.
- What evidence would resolve it: Testing the approach on scenarios requiring simultaneous alignment to multiple attributes (e.g., high fairness AND high protocol focus) would reveal whether the method generalizes or requires modification.

### Open Question 3
- Question: What is the relationship between the quality of reasoning traces and alignment accuracy?
- Basis in paper: [inferred] The paper mentions that reasoning traces "can serve as a useful form of model explanation" and that conditioning on explanations improved performance, but doesn't analyze trace quality.
- Why unresolved: The paper uses reasoning traces but doesn't evaluate whether better-quality traces correlate with higher alignment accuracy.
- What evidence would resolve it: A systematic analysis comparing trace quality (using human or automated metrics) with alignment accuracy across different models and attributes would reveal this relationship.

## Limitations
- The dataset contains only 62 scenarios, which may limit generalizability across the full space of medical triage decisions.
- The study relies entirely on simulated scenarios rather than real-world medical decisions, raising questions about external validity.
- The weighted self-consistency approach requires multiple model invocations, creating a tradeoff between alignment accuracy and computational cost.

## Confidence
- **High confidence**: The zero-shot prompting approach with attribute definitions works as described and produces measurable alignment improvements across multiple models and attributes.
- **Medium confidence**: The finding that larger models show greater alignability is supported by within-study comparisons but needs external validation across different model families.
- **Low confidence**: The relative importance of different mechanisms (weighted voting vs. negative samples) and the optimal temperature parameter for sampling remain insufficiently explored.

## Next Checks
1. **Dataset expansion validation**: Test whether alignment performance scales predictably as more diverse scenarios are added, particularly for attributes with fewer than 10 scenarios currently.
2. **Real-world pilot study**: Apply the approach to actual medical triage decisions from emergency departments to assess external validity and identify potential deployment challenges.
3. **Computational efficiency analysis**: Characterize the relationship between the number of samples, temperature settings, and alignment accuracy to identify optimal tradeoffs for practical deployment.