---
ver: rpa2
title: 'MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating Multi-Insight
  Multi-Document Extraction Tasks'
arxiv_id: '2411.19689'
source_url: https://arxiv.org/abs/2411.19689
tags:
- insights
- data
- synthetic
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MIMDE, a new task for extracting insights
  from document collections and mapping them back to sources. The authors develop
  an evaluation framework and create two datasets: one with human-generated survey
  responses and another with synthetic data generated by LLMs.'
---

# MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating Multi-Insight Multi-Document Extraction Tasks

## Quick Facts
- arXiv ID: 2411.19689
- Source URL: https://arxiv.org/abs/2411.19689
- Reference count: 8
- Key outcome: Strong correlation (0.71) between synthetic and human data for insight extraction, but synthetic data fails at document-level mapping

## Executive Summary
This paper introduces MIMDE (Multi-Insight Multi-Document Extraction), a new task for extracting insights from document collections and mapping them back to sources. The authors develop an evaluation framework and create two datasets: one with human-generated survey responses and another with synthetic data generated by LLMs. They benchmark 20 state-of-the-art LLMs on both datasets, finding that while synthetic data performs comparably for insight extraction, it significantly underperforms for document-level analysis. The study demonstrates that synthetic data can serve as a cost-effective proxy for human data in insight extraction evaluation but highlights important limitations for complex document analysis tasks.

## Method Summary
The authors created two datasets: human-generated survey responses (5 questions, 1,000 responses each) and synthetic responses generated by LLMs using controlled prompts. They implemented a brute-force LLM approach for insight extraction through iterative prompting, consolidating insights across batches. For evaluation, they compared multiple similarity metrics (distance, keyword, semantic, LLM-based) to map predicted insights to ground truth. The framework measures precision, recall, F1 score, and redundancy at both insight and document levels, using correlation analysis to assess synthetic data's viability as a human data proxy.

## Key Results
- LLM performance on insight extraction from synthetic data strongly correlates with human data (r=0.71, p<0.001)
- State-of-the-art LLMs outperform traditional similarity metrics in matching predicted insights to ground truth
- Synthetic data generation costs ~1/20th of human data collection (£442.87 vs £7,771.83)
- Synthetic data fails at document-level analysis, performing significantly worse at mapping insights back to source documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance on insight extraction from synthetic data strongly correlates with performance on human data (r=0.71, p<0.001)
- Mechanism: Synthetic responses generated with controlled prompts capture similar linguistic patterns and insight distributions as human responses, enabling reliable relative model comparison
- Core assumption: The synthetic data generation process successfully mimics the semantic structure and insight diversity present in human responses
- Evidence anchors:
  - [abstract] "Our analysis reveals a strong correlation (0.71) between the ability of LLMs to extracts insights on our two datasets"
  - [section] "At the insight level, shown on the left side of Figure 2, we observed a strong positive relationship between recall scores across datasets (correlation = 0.71, p-value < 0.001)"
  - [corpus] Weak - only 5 related papers found, none directly addressing MIMDE task correlation

### Mechanism 2
- Claim: State-of-the-art LLMs outperform traditional similarity metrics (distance, keyword, semantic) in matching predicted insights to ground truth
- Mechanism: LLMs can understand contextual meaning and semantic equivalence better than rule-based or embedding-based methods, even without fine-tuning
- Core assumption: Zero-shot LLM reasoning with chain-of-thought can effectively capture human judgment of insight similarity
- Evidence anchors:
  - [abstract] "Overall, LLM and semantic metrics showed superior alignment with human preferences"
  - [section] "LLM and semantic metrics showed superior alignment with human preferences. Distance-based and keyword-based metrics performed poorly"
  - [corpus] Weak - corpus neighbors focus on synthetic data generation, not similarity metric evaluation

### Mechanism 3
- Claim: Synthetic data generation at ~1/20th the cost of human data collection while maintaining sufficient quality for insight extraction evaluation
- Mechanism: LLM-based generation with controlled prompts and stochastic parameters produces diverse, labeled responses that mirror human response structure
- Core assumption: The prompt engineering approach (personality traits, sub-insights, few-shot examples) successfully creates synthetic responses that maintain task-relevant characteristics
- Evidence anchors:
  - [abstract] "The human-generated dataset, including response creation and insight annotation by three annotators for five questions, cost £7,771.83 to produce. In contrast, generating a comparable dataset using LLMs cost only £442.87"
  - [section] "The synthetic data introduced here should well approximate the data collected from humans"
  - [corpus] Weak - no direct cost comparison evidence in related papers

## Foundational Learning

- Concept: Insight extraction and mapping as unsupervised information extraction
  - Why needed here: MIMDE requires identifying and extracting insights without predefined labels, then mapping them back to source documents
  - Quick check question: What distinguishes insight extraction from traditional named entity recognition or keyword extraction?

- Concept: Similarity metric evaluation and thresholding
  - Why needed here: Determining when two insights are semantically equivalent requires comparing various similarity approaches and selecting optimal thresholds
  - Quick check question: Why might BLEU or Levenshtein distance perform poorly for comparing insights compared to LLM-based methods?

- Concept: Correlation analysis for proxy validation
  - Why needed here: Assessing whether synthetic data can substitute for human data requires understanding the relationship between model performance across datasets
  - Quick check question: What does a Pearson correlation of 0.71 between synthetic and human data performance indicate about their relationship?

## Architecture Onboarding

- Component map: Data collection -> Insight extraction (iterative LLM prompting) -> Insight mapping (similarity comparison) -> Performance evaluation

- Critical path:
  1. Generate or collect labeled dataset (human or synthetic)
  2. Extract insights using iterative LLM prompting
  3. Map extracted insights back to source documents
  4. Compare predicted vs. true insights using optimal similarity metric
  5. Calculate precision, recall, F1, and redundancy metrics

- Design tradeoffs:
  - Human data provides ground truth but is expensive and time-consuming
  - Synthetic data is cost-effective but may not capture all human response nuances
  - LLM-based evaluation is more accurate but computationally expensive
  - Simplified brute-force approach is easy to implement but may not capture optimal solutions

- Failure signatures:
  - Low correlation between synthetic and human performance indicates synthetic data inadequacy
  - High redundancy scores suggest poor insight consolidation during extraction
  - Poor document-level performance despite good insight-level scores indicates mapping difficulties
  - Similarity metrics failing to capture semantic equivalence suggests need for better comparison methods

- First 3 experiments:
  1. Test different similarity thresholds on insight-mapping dataset to optimize F1 score
  2. Compare brute-force extraction performance across different batch sizes and context window limits
  3. Evaluate impact of synthetic data generation parameters (temperature, presence penalty) on insight diversity and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can synthetic data generation be improved to better capture document-level complexity in MIMDE tasks?
- Basis in paper: [explicit] The authors note that synthetic data fails to capture document-level complexity, performing significantly worse at mapping insights back to source documents compared to human data
- Why unresolved: The paper identifies this limitation but doesn't provide specific methods to address it, only suggesting the need for verification phases and improved generation techniques
- What evidence would resolve it: Empirical results showing improved synthetic data generation methods that achieve correlation scores comparable to human data at both insight extraction and document mapping levels

### Open Question 2
- Question: Do LLMs exhibit systematic performance differences when analyzing responses from different demographic groups in MIMDE tasks?
- Basis in paper: [inferred] The authors suggest examining potential biases in MIMDE performance, noting that agreement rates among human annotators varied significantly (23% full agreement)
- Why unresolved: The paper acknowledges potential demographic bias but doesn't explore it experimentally, focusing instead on general model performance
- What evidence would resolve it: Comparative performance analysis of LLM models across demographic subgroups, showing whether accuracy varies systematically by age, cultural background, or language proficiency

### Open Question 3
- Question: What is the optimal balance between human and synthetic data for MIMDE task evaluation?
- Basis in paper: [explicit] The authors note synthetic data generation costs only £442.87 compared to £7,771.83 for human data, but also identify limitations in synthetic data's document-level analysis capabilities
- Why unresolved: The paper presents cost-benefit analysis but doesn't explore hybrid approaches or determine optimal data mixing ratios for reliable evaluation
- What evidence would resolve it: Experimental results showing performance trade-offs across different ratios of human to synthetic data in MIMDE evaluation tasks

## Limitations
- Strong correlation (r=0.71) between synthetic and human data performance may not generalize to other document types beyond survey responses
- Synthetic data generation may introduce systematic biases through prompt engineering that could artificially inflate correlation metrics
- Evaluation framework relies heavily on LLM-based similarity metrics, introducing potential circularity when evaluating LLM performance
- Brute-force LLM approach scalability for larger document collections remains untested

## Confidence
- High Confidence: Cost comparison between human and synthetic data collection is well-documented; LLM and semantic metrics outperforming traditional approaches is clearly demonstrated
- Medium Confidence: Correlation between synthetic and human data performance for insight extraction is compelling but requires careful interpretation across domains
- Low Confidence: Scalability of brute-force LLM approach for insight extraction across larger document collections remains untested

## Next Checks
1. **Domain Generalization Test**: Evaluate the synthetic-to-human correlation on a different document type (e.g., news articles or technical documentation) to assess whether the observed relationship generalizes beyond survey responses.

2. **Cost-Benefit Analysis Extension**: Conduct a comprehensive cost analysis that includes the computational expenses of LLM-based evaluation and generation, comparing the total cost of ownership across human, synthetic, and hybrid approaches.

3. **Bias Characterization Study**: Systematically analyze the synthetic data for systematic biases introduced through prompt engineering by comparing the distribution of insight types, sentiment, and complexity between human and synthetic responses.