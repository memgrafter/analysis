---
ver: rpa2
title: Semantic Properties of cosine based bias scores for word embeddings
arxiv_id: '2401.15499'
source_url: https://arxiv.org/abs/2401.15499
tags:
- bias
- scores
- weat
- biases
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes two widely-used cosine-based bias scores\u2014\
  WEAT and the Direct Bias\u2014in terms of their semantic properties for quantifying\
  \ social biases in word embeddings. The authors propose two formal requirements\
  \ for meaningful bias scores: magnitude-comparability (scores should be comparable\
  \ across models) and unbiased-trustworthiness (scores should correctly reflect bias\
  \ as defined by group associations)."
---

# Semantic Properties of cosine based bias scores for word embeddings

## Quick Facts
- arXiv ID: 2401.15499
- Source URL: https://arxiv.org/abs/2401.15499
- Reference count: 25
- Primary result: Analyzes WEAT and Direct Bias scores for bias quantification, showing both have theoretical limitations in magnitude-comparability and unbiased-trustworthiness

## Executive Summary
This paper analyzes two widely-used cosine-based bias scores—WEAT and the Direct Bias—in terms of their semantic properties for quantifying social biases in word embeddings. The authors propose two formal requirements for meaningful bias scores: magnitude-comparability (scores should be comparable across models) and unbiased-trustworthiness (scores should correctly reflect bias as defined by group associations). Through theoretical analysis, they show that WEAT's effect size is magnitude-comparable but not unbiased-trustworthy, while the Direct Bias is magnitude-comparable but not unbiased-trustworthy. Experiments with state-of-the-art language models demonstrate that these theoretical limitations impact real-world bias detection.

## Method Summary
The authors conduct theoretical analysis of WEAT and Direct Bias metrics, defining formal requirements for meaningful bias scores. They examine semantic properties through mathematical proofs showing the magnitude-comparability and unbiased-trustworthiness of each metric. Empirical validation involves testing these properties across different word embedding models and bias scenarios to demonstrate practical implications of the theoretical limitations.

## Key Results
- WEAT's effect size is magnitude-comparable but not unbiased-trustworthy, potentially misleading when comparing bias across models
- Direct Bias is magnitude-comparable but not unbiased-trustworthy, with bias direction misrepresentation due to PCA sensitivity
- Theoretical limitations of these scores impact real-world bias detection in state-of-the-art language models

## Why This Works (Mechanism)
The analysis works by examining how cosine-based similarity measures propagate through bias detection formulas. For WEAT, the mechanism relies on comparing relative distances between target and attribute word sets, while Direct Bias uses PCA to identify principal components of bias. Both methods inherit properties from their underlying cosine similarity calculations, which affects how bias manifests and is measured across different embedding spaces.

## Foundational Learning
- Cosine similarity fundamentals: Understanding vector angle relationships is essential for grasping how bias scores derive from word embeddings
- PCA decomposition: Critical for understanding Direct Bias sensitivity to principal component directions
- Effect size calculation: Necessary for interpreting WEAT's magnitude-comparability properties
- Statistical significance testing: Required for evaluating whether observed bias differences are meaningful
- Embedding space geometry: Important for understanding how semantic relationships manifest in vector representations

## Architecture Onboarding
Component map: Word embeddings -> Cosine similarity calculation -> Bias score computation -> Statistical validation
Critical path: Embedding generation → Similarity matrix construction → Bias metric application → Result interpretation
Design tradeoffs: Computational efficiency vs. bias detection sensitivity; model-agnostic vs. model-specific bias measures
Failure signatures: Inconsistent bias scores across similar models; sensitivity to embedding initialization; spurious correlations in bias detection
First experiments: 1) Compute WEAT scores across multiple GloVe models with identical training data but different random seeds 2) Vary PCA initialization for Direct Bias on the same dataset 3) Compare bias scores across embedding dimensionalities (50, 100, 200, 300)

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical requirements need validation across diverse real-world scenarios
- Empirical experiments focus on specific models and bias types, limiting generalizability
- Limited systematic comparison with other bias detection methods
- Unclear practical impact magnitude of identified limitations

## Confidence
- High confidence: Theoretical analysis of WEAT and Direct Bias properties
- Medium confidence: Empirical demonstration of WEAT's misleading cross-model comparisons
- Medium confidence: Direct Bias PCA sensitivity claims
- Low confidence: Generalizability of findings to all embedding architectures and bias types

## Next Checks
1. Conduct systematic experiments comparing WEAT effect sizes across diverse embedding models (GloVe, Word2Vec, BERT variants) with standardized bias benchmarks to quantify practical impact of misleading comparisons
2. Test Direct Bias sensitivity by systematically varying PCA initialization and computing bias score variance across multiple runs for the same dataset
3. Evaluate whether the proposed semantic requirements hold for other bias metrics (SEAT, Relative Negative Sentiment Bias) and embedding architectures (contextual embeddings, sentence transformers)