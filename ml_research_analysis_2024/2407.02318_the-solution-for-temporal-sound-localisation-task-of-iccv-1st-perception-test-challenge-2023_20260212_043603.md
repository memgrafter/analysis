---
ver: rpa2
title: The Solution for Temporal Sound Localisation Task of ICCV 1st Perception Test
  Challenge 2023
arxiv_id: '2407.02318'
source_url: https://arxiv.org/abs/2407.02318
tags:
- temporal
- action
- audio
- features
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a solution for the Temporal Sound Localization
  task in the ICCV 1st Perception Test Challenge 2023. The proposed approach combines
  visual and audio modalities using a multimodal fusion framework.
---

# The Solution for Temporal Sound Localisation Task of ICCV 1st Perception Test Challenge 2023

## Quick Facts
- arXiv ID: 2407.02318
- Source URL: https://arxiv.org/abs/2407.02318
- Reference count: 35
- Primary result: 2nd place in ICCV 1st Perception Test Challenge 2023 with 0.33 mAP

## Executive Summary
This paper presents a solution for the Temporal Sound Localization task in the ICCV 1st Perception Test Challenge 2023. The proposed approach combines visual and audio modalities using a multimodal fusion framework. High-quality visual features are extracted using VideoMAE V2, a state-of-the-art self-supervised pre-training network, while audio features from MMV are used as complementary information. These features are fused early and processed by a multi-scale Transformer-based ActionFormer model. The method achieves a mean Average Precision (mAP) of 0.33 on the test dataset, securing second place in the challenge. Experimental results show that visual features significantly improve performance compared to audio alone, while multimodal fusion provides additional gains.

## Method Summary
The proposed method employs a multimodal fusion approach for temporal sound localization. It extracts high-quality visual features using VideoMAE V2 and audio features using MMV, then fuses them early through concatenation. The fused features are processed by a multi-scale Transformer-based ActionFormer model with 9 Transformer blocks, local self-attention, and downsampling operations. A lightweight convolutional decoder with classification and regression heads predicts sound event boundaries. The model is trained using focal loss for classification and DIoU loss for regression on a single NVIDIA RTX 3090 GPU with batch size of 2 for 35 epochs.

## Key Results
- Achieved 2nd place in ICCV 1st Perception Test Challenge 2023 with 0.33 mAP
- Video-only performance (30.5 mAP) significantly outperformed audio-only (10.9 mAP)
- Multimodal fusion improved performance to 33.1 mAP, showing audio provides complementary information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early multimodal fusion of visual and audio features provides complementary information for temporal sound localization
- Mechanism: The system extracts high-quality visual features from VideoMAE V2 and audio features from MMV, then concatenates them at the early fusion stage before feeding them into the multi-scale Transformer
- Core assumption: Both visual and audio modalities contain relevant information for sound localization, and their early combination preserves temporal alignment
- Evidence anchors: Visual features significantly improve performance over audio alone; multimodal fusion provides additional gains
- Break condition: If the audio modality provides minimal complementary information, early fusion may not improve performance significantly

### Mechanism 2
- Claim: Multi-scale Transformer architecture with local self-attention effectively captures temporal sound boundaries
- Mechanism: ActionFormer uses downsampling operations with 1D convolution and local self-attention to process features at different temporal granularities while avoiding performance degradation from long attention windows
- Core assumption: Temporal sound events have hierarchical structure that benefits from multi-scale processing
- Evidence anchors: The model uses local self-attention with window size 11 and downsampling operations to capture multi-scale features
- Break condition: If sound events are not hierarchical or if local self-attention misses important long-range dependencies

### Mechanism 3
- Claim: VideoMAE V2's self-supervised pre-training provides superior visual representations for sound localization
- Mechanism: VideoMAE V2 uses a dual-mask strategy for efficient self-supervised pre-training, generating high-dimensional visual features (1408 dimensions) that capture motion and appearance patterns relevant to sound-producing events
- Core assumption: Self-supervised pre-training on large video datasets learns representations that transfer well to temporal sound localization
- Evidence anchors: VideoMAE V2 employs dual-mask strategy for efficient self-supervised pre-training and has achieved state-of-the-art results on various downstream video tasks
- Break condition: If visual patterns learned during pre-training are not relevant to the specific sound events in the dataset

## Foundational Learning

- Concept: Self-supervised pre-training for video representation learning
  - Why needed here: The visual modality needs rich representations to capture complex temporal patterns associated with sound events
  - Quick check question: What is the key difference between masked autoencoders for images versus videos, and why is this important for temporal tasks?

- Concept: Multimodal fusion strategies and their impact on task performance
  - Why needed here: Understanding when and how to combine different modalities is crucial for optimizing the system's performance
  - Quick check question: What are the advantages and disadvantages of early fusion versus late fusion for multimodal tasks?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The multi-scale Transformer is the core model that processes the multimodal features and outputs sound localization predictions
  - Quick check question: How does local self-attention differ from standard self-attention, and what problem does it solve in long sequences?

## Architecture Onboarding

- Component map: Input video → VideoMAE V2 (visual feature extraction, 1408D) → MMV (audio feature extraction, 256D) → Early fusion (concatenation, 1564D) → Multi-scale ActionFormer (9 Transformer blocks with downsampling) → Lightweight decoder (classification + regression heads) → Output (sound class probabilities and boundary distances)

- Critical path: Video and audio feature extraction → Early fusion → Multi-scale Transformer processing → Boundary prediction
  The most time-critical components are the VideoMAE V2 feature extraction and the Transformer forward pass

- Design tradeoffs:
  - Single NVIDIA RTX 3090 limits batch size to 2 and training to 35 epochs
  - Using 17 sound classes instead of the full dataset classes to reduce complexity
  - Local self-attention reduces computational cost but may miss long-range dependencies
  - Early fusion preserves temporal alignment but requires synchronized preprocessing

- Failure signatures:
  - Audio-only performance (10.9 mAP) much lower than video-only (30.5 mAP) indicates audio modality weakness
  - Minimal improvement from multimodal fusion (33.1 mAP vs 30.5 mAP) suggests audio provides limited complementary information
  - If training loss plateaus early, it may indicate insufficient model capacity or poor feature quality

- First 3 experiments:
  1. Ablation study: Train with video-only features to verify the baseline improvement from VideoMAE V2 over TSN features
  2. Feature analysis: Visualize attention weights in the Transformer to understand how the model processes temporal patterns
  3. Fusion timing: Test late fusion (concatenating video and audio features after separate Transformer processing) to compare with early fusion performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed multimodal fusion approach compare to unimodal approaches when using different visual feature extraction methods?
- Basis in paper: The paper states that visual features significantly improve performance compared to audio alone, and multimodal fusion provides additional gains. However, it does not provide a detailed comparison of different visual feature extraction methods.
- Why unresolved: The paper focuses on the effectiveness of the multimodal fusion approach but does not explore the impact of using different visual feature extraction methods on the overall performance.
- What evidence would resolve it: A detailed ablation study comparing the performance of the multimodal fusion approach using different visual feature extraction methods, such as VideoMAE V2 and other state-of-the-art methods, would provide insights into the importance of the choice of visual feature extraction method.

### Open Question 2
- Question: What is the impact of the window size in local self-attention on the performance of the ActionFormer model?
- Basis in paper: The paper mentions that a window size of 11 was used for local self-attention, but it does not explore the effect of varying this parameter on the model's performance.
- Why unresolved: The choice of window size for local self-attention is an important hyperparameter that can significantly affect the model's ability to capture relevant information from the input sequence.
- What evidence would resolve it: An ablation study varying the window size for local self-attention and measuring its impact on the model's performance metrics, such as mean Average Precision (mAP), would provide insights into the optimal window size for this task.

### Open Question 3
- Question: How does the proposed multimodal fusion approach generalize to other temporal sound localization datasets or domains?
- Basis in paper: The paper evaluates the proposed approach on a specific dataset provided by the ICCV 1st Perception Test Challenge 2023. However, it does not discuss the generalizability of the approach to other datasets or domains.
- Why unresolved: The performance of the proposed approach on the specific dataset used in the challenge does not guarantee its effectiveness on other datasets or domains with different characteristics.
- What evidence would resolve it: Evaluating the proposed approach on multiple temporal sound localization datasets from different domains, such as music, speech, or environmental sounds, and comparing its performance across these datasets would provide insights into its generalizability and robustness.

## Limitations
- The marginal improvement from multimodal fusion (33.1 mAP vs 30.5 mAP) suggests audio provides limited complementary information for this task
- Limited experimental validation through ablation studies to quantify the contribution of each component
- Single GPU training with small batch size may limit model performance and generalization

## Confidence
- Multimodal fusion effectiveness: Medium - Limited ablation studies to validate the approach
- VideoMAE V2 superiority: Medium - Assumed but not empirically demonstrated for sound localization
- Audio modality contribution: Low - Audio-only performance is significantly worse than video, raising questions about its value

## Next Checks
1. Conduct comprehensive ablation study testing early fusion vs late fusion, and audio-only vs video-only baselines
2. Test the approach on a separate validation set using the full 17 sound classes to verify robustness
3. Compare VideoMAE V2 visual features against other state-of-the-art video encoders (e.g., TSN, SlowFast)