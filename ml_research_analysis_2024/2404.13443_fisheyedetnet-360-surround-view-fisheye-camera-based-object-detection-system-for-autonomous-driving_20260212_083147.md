---
ver: rpa2
title: "FisheyeDetNet: 360\xB0 Surround view Fisheye Camera based Object Detection\
  \ System for Autonomous Driving"
arxiv_id: '2404.13443'
source_url: https://arxiv.org/abs/2404.13443
tags:
- detection
- object
- segmentation
- bounding
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of object detection in fisheye
  cameras for autonomous driving, where standard bounding box representations fail
  due to heavy radial distortion. The authors propose extending the output representation
  of object detection to include rotated bounding boxes, ellipses, and polygons.
---

# FisheyeDetNet: 360° Surround view Fisheye Camera based Object Detection System for Autonomous Driving

## Quick Facts
- arXiv ID: 2404.13443
- Source URL: https://arxiv.org/abs/2404.13443
- Authors: Ganesh Sistu; Senthil Yogamani
- Reference count: 40
- Primary result: Proposed FisheyeDetNet with polygon representation achieves 49.5% mAP on Valeo fisheye surround-view dataset

## Executive Summary
This paper addresses the challenge of object detection in fisheye cameras for autonomous driving, where standard bounding box representations fail due to heavy radial distortion. The authors propose extending the output representation of object detection to include rotated bounding boxes, ellipses, and polygons. They introduce a new metric, instance segmentation mIOU, to evaluate these representations. The proposed model, FisheyeDetNet, uses a polygon representation and achieves a mAP score of 49.5% on the Valeo fisheye surround-view dataset. This dataset contains 60K images captured from 4 surround-view cameras across Europe, North America, and Asia. The results show that polygon representation outperforms other representations and can solve practical problems like missing parking spots, which are common in fisheye-based visual navigation systems.

## Method Summary
The paper addresses the challenge of object detection on fisheye cameras for autonomous driving, where standard bounding box representations fail due to radial distortion. The authors propose a modified YOLOv3 architecture with ResNet18 encoder and introduce four different object representation formats: bounding box, oriented box, ellipse, and polygon. The model is trained on the Valeo fisheye surround-view dataset with 60K images from 4 cameras, using Ranger optimizer and one-cycle learning rate scheduler for 80 epochs. The key innovation is the polygon representation, which samples object boundaries in polar coordinates at varying rates (12, 24, 36, 60, and 120 points per 360°). The authors introduce a new metric, instance segmentation mIOU, to evaluate the performance of different representations.

## Key Results
- FisheyeDetNet with polygon representation achieves 49.5% mAP on the Valeo fisheye surround-view dataset
- Polygon representation outperforms bounding box, oriented box, and ellipse representations in terms of mAP score
- The dataset contains 60K images from 4 surround-view cameras across Europe, North America, and Asia
- Polygon representation can solve practical problems like missing parking spots in fisheye-based visual navigation systems

## Why This Works (Mechanism)
The polygon representation works better than traditional bounding boxes because it can accurately capture the complex shapes of objects in fisheye images, which suffer from heavy radial distortion. By sampling object boundaries in polar coordinates at multiple points around the object, the polygon representation can better handle the non-linear distortion effects that occur in fisheye cameras. This allows for more precise localization of objects, especially at the edges of the image where distortion is most severe.

## Foundational Learning
- Fisheye distortion: Heavy radial distortion that causes standard bounding boxes to fail - needed to understand the problem space; quick check: visualize fisheye images with standard bounding boxes
- Polar coordinate sampling: Sampling object boundaries in polar coordinates instead of Cartesian - needed to understand polygon representation; quick check: convert a few polygon annotations to polar coordinates
- Instance segmentation mIOU: New metric for evaluating object detection in fisheye images - needed to compare different representations; quick check: calculate mIOU for a few example detections
- Anchor box configurations: Specific anchor boxes used for each representation type - needed for fair comparison between models; quick check: verify anchor box sizes and aspect ratios

## Architecture Onboarding

**Component Map:**
Input (544x288 fisheye image) -> Modified YOLOv3 with ResNet18 encoder -> Representation blocks (bbox, oriented box, ellipse, polygon) -> Output layer

**Critical Path:**
Image input -> Feature extraction (ResNet18) -> Object detection head -> Representation-specific output (bbox/oriented box/ellipse/polygon) -> Post-processing and evaluation

**Design Tradeoffs:**
- Polygon representation provides better accuracy but requires more complex annotations and processing
- Using YOLOv3 architecture balances speed and accuracy but may limit performance compared to more advanced detectors
- Single-stage detection simplifies the pipeline but may sacrifice some accuracy compared to two-stage methods

**Failure Signatures:**
- Poor performance on polygon representation suggests issues with annotation format or sampling strategy
- Overfitting on fisheye dataset indicates limited augmentation and complex distortion patterns
- Low mAP scores across all representations suggest problems with feature extraction or detection head

**First Experiments:**
1. Verify polygon annotation format by checking polar coordinate conversion and point distribution
2. Reconstruct loss function implementation for all four representation types to ensure fair comparison
3. Test model performance on a small subset of the dataset to validate training pipeline

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the proposed polygon representation handle occlusions or overlapping objects in fisheye images compared to other representations?
- Basis in paper: The paper mentions that polygon representation outperforms other representations in solving practical problems like missing parking spots, which suggests it handles object boundaries better.
- Why unresolved: The paper does not provide a detailed analysis of how polygon representation specifically addresses occlusions or overlapping objects, which are common challenges in object detection.
- What evidence would resolve it: Experimental results comparing the performance of polygon representation versus other representations in scenarios with occlusions or overlapping objects would provide clarity.

### Open Question 2
- Question: What is the impact of varying the number of sampling points in the polygon representation on the accuracy and computational efficiency of the model?
- Basis in paper: The paper mentions using different sampling rates (12, 24, 36, 60, and 120 points) and their effect on performance, but does not explore the trade-off between accuracy and computational efficiency.
- Why unresolved: While the paper shows that more sampling points lead to better performance, it does not analyze how this affects the computational cost or real-time applicability of the model.
- What evidence would resolve it: A detailed analysis of the trade-off between accuracy and computational efficiency for different sampling rates would help determine the optimal number of points for practical applications.

### Open Question 3
- Question: How does the FisheyeDetNet perform in real-world autonomous driving scenarios compared to standard object detection models on rectified fisheye images?
- Basis in paper: The paper discusses the limitations of bounding box representations on fisheye images and the advantages of the proposed polygon representation, but does not compare the performance of FisheyeDetNet with standard models on rectified images in real-world scenarios.
- Why unresolved: The paper focuses on the performance of FisheyeDetNet on raw fisheye images but does not provide a direct comparison with standard models on rectified images, which are commonly used in practice.
- What evidence would resolve it: Comparative experiments evaluating FisheyeDetNet and standard models on rectified fisheye images in real-world autonomous driving scenarios would provide insights into the practical benefits of the proposed approach.

## Limitations
- Missing details about exact polygon annotation format and sampling strategy
- Limited augmentation (only horizontal flip) may lead to overfitting on fisheye dataset
- No comparison with standard models on rectified fisheye images in real-world scenarios

## Confidence
- Methodology: High - detailed specifications of dataset and evaluation framework
- Technical implementation: Medium - missing details about polygon annotation format and anchor configurations
- Practical application: Medium - results support claims but require validation of annotation methodology

## Next Checks
1. Verify the polygon annotation format and sampling strategy by checking the polar coordinate conversion and point distribution
2. Reconstruct the complete loss function implementation for all four representation types to ensure fair comparison
3. Test the model on additional fisheye datasets to verify generalization beyond the Valeo dataset