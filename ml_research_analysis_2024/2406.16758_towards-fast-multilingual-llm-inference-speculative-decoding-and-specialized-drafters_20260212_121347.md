---
ver: rpa2
title: 'Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized
  Drafters'
arxiv_id: '2406.16758'
source_url: https://arxiv.org/abs/2406.16758
tags:
- arxiv
- speculative
- decoding
- drafter
- speedup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high inference latency in
  multilingual large language model (LLM) deployment, particularly for translation
  tasks. The authors propose a specialized training strategy for drafter models used
  in speculative decoding, employing a pretrain-and-finetune approach on language-specific
  datasets.
---

# Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters

## Quick Facts
- arXiv ID: 2406.16758
- Source URL: https://arxiv.org/abs/2406.16758
- Reference count: 38
- Primary result: Specialized drafter models trained with pretrain-and-finetune strategy achieve up to 2.42x speedup in multilingual translation tasks

## Executive Summary
This paper addresses the challenge of high inference latency in multilingual large language model (LLM) deployment, particularly for translation tasks. The authors propose a specialized training strategy for drafter models used in speculative decoding, employing a pretrain-and-finetune approach on language-specific datasets. Experiments show that this method significantly improves inference speed compared to existing baselines, achieving up to 2.42x speedup on average across multiple translation tasks using Vicuna 7B as the target model. The speedup scales logarithmically with the number of training tokens and demonstrates strong generalization to out-of-domain tasks. GPT-4o evaluation confirms the method maintains high translation quality.

## Method Summary
The authors propose a pretrain-and-finetune strategy for drafter models in speculative decoding. First, drafters are pretrained on broad language modeling datasets (C4 and ShareGPT), then finetuned on language-specific translation tasks. This creates compact models with comprehensive language modeling capabilities that better capture target language nuances. The training uses self-distillation from target LLMs to generate aligned training data. Speedup is measured as the ratio of standard autoregressive greedy decoding time to speculative decoding time on RTX3090 GPU with batch size 1.

## Key Results
- Specialized drafters achieve up to 2.42x average speedup across multiple translation tasks
- Speedup scales logarithmically with training token count, showing diminishing returns
- Input domain consistency (source language) is more important than output domain consistency for achieving high speedup
- Strong generalization to out-of-domain translation tasks while maintaining quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized drafter models trained with a pretrain-and-finetune strategy significantly improve speculative decoding speedup compared to instruction-tuned models.
- Mechanism: The pretrain-and-finetune approach first exposes drafters to broad language modeling capabilities through pretraining on C4 and ShareGPT datasets, then refines them with language-specific translation tasks. This creates compact models with comprehensive language modeling capabilities that better capture the nuances of target languages.
- Core assumption: Drafting requires broader language modeling capabilities beyond simple token prediction to handle complex linguistic structures and context variations effectively.

### Mechanism 2
- Claim: Speedup scales logarithmically with the number of training tokens used in drafter training.
- Mechanism: As the drafter is exposed to more training tokens, it develops better language modeling capabilities and alignment with the target LLM's token distribution, leading to higher acceptance rates during speculative decoding. The logarithmic relationship suggests diminishing returns as token count increases.
- Core assumption: More training tokens lead to better alignment with target LLM behavior, but the relationship follows a logarithmic curve rather than linear.

### Mechanism 3
- Claim: Input domain consistency with training data is more important than output domain consistency for achieving high speedup in translation tasks.
- Mechanism: Drafters perform better when the source language of the translation task matches the source language used during training. This suggests the model has learned source language patterns and structures that enable better drafting, regardless of the target language.
- Core assumption: The drafter's effectiveness depends more on understanding the source language patterns than on generating the target language, as verification is handled by the target LLM.

## Foundational Learning

- Concept: Speculative decoding draft-verify-accept paradigm
  - Why needed here: Understanding how assistant models draft tokens that are then verified by the target LLM is fundamental to grasping why specialized drafters improve performance
  - Quick check question: What are the three phases of speculative decoding and what happens in each phase?

- Concept: Language model alignment and token distribution matching
  - Why needed here: The paper's core contribution relies on aligning drafter outputs with target LLM predictions, which requires understanding how language models generate token distributions
  - Quick check question: How does the rejection sampling mechanism ensure that only tokens aligned with the target LLM's predictions are accepted?

- Concept: Pretraining and finetuning methodology
  - Why needed here: The paper's novel approach uses a two-stage training process that differs from standard instruction tuning, so understanding when and why to use each stage is crucial
  - Quick check question: What is the difference between pretraining and finetuning in the context of this paper's drafter training approach?

## Architecture Onboarding

- Component map: Input text → Drafter generation → Parallel verification by target LLM → Token acceptance/rejection → Output generation
- Critical path: Source text → Drafter model → Speculative tokens → Target LLM verification → Final output
- Design tradeoffs:
  - Model size vs. drafting speed (smaller models are faster but may have lower quality)
  - Training data diversity vs. task specificity (broad pretraining vs. focused finetuning)
  - Number of drafts vs. computational overhead (multiple drafts increase acceptance but add latency)
  - Hardware constraints vs. performance targets (RTX3090 vs. A100 differences)
- Failure signatures:
  - Low acceptance rates indicate poor drafter-target alignment
  - Inconsistent speedup across languages suggests training data mismatch
  - Hardware-specific performance variations point to memory bandwidth bottlenecks
  - Degradation in translation quality indicates loss of semantic understanding
- First 3 experiments:
  1. Compare speedup of pretrain-and-finetune drafter vs. instruction-tuned drafter on WMT16 De-En task to validate the core hypothesis
  2. Vary the number of training tokens used in finetuning to confirm the logarithmic scaling relationship
  3. Test drafter performance on out-of-domain translation pairs to validate input domain consistency importance

## Open Questions the Paper Calls Out
- Theoretical limit of speedup achievable with specialized drafters and scaling with model size/token count
- Impact of different pretraining strategies (C4 vs. multilingual corpora) on drafter performance
- Extension to non-translation multilingual tasks like cross-lingual summarization or dialogue generation

## Limitations
- Logarithmic scaling suggests diminishing returns before reaching optimal performance
- Limited transferability across languages not present in training data
- Hardware-specific measurements may not generalize to other GPU architectures

## Confidence
- High confidence: Pretrain-and-finetune approach improves speedup compared to instruction-tuned models
- Medium confidence: Logarithmic scaling relationship between training tokens and speedup
- Medium confidence: Input domain consistency being more important than output domain consistency

## Next Checks
1. Cross-architecture validation: Reproduce speedup measurements on different GPU architectures (A100, H100) to verify hardware independence
2. Domain transfer evaluation: Test drafter performance on non-translation generation tasks to determine generalizability
3. Capacity scaling study: Vary drafter model sizes to quantify tradeoff between capacity and speedup performance