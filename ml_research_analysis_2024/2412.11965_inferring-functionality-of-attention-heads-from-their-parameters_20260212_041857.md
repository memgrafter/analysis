---
ver: rpa2
title: Inferring Functionality of Attention Heads from their Parameters
arxiv_id: '2412.11965'
source_url: https://arxiv.org/abs/2412.11965
tags:
- heads
- head
- relation
- word
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAPS (Mapping Attention head ParameterS),
  a framework that infers the functionality of attention heads in large language models
  directly from their parameters, without model training or inference. The approach
  casts attention heads as matrices mapping between vocabulary tokens, then analyzes
  patterns in these mappings to identify predefined relations (e.g., country-to-capital,
  word-to-synonym) or salient operations.
---

# Inferring Functionality of Attention Heads from their Parameters

## Quick Facts
- arXiv ID: 2412.11965
- Source URL: https://arxiv.org/abs/2412.11965
- Reference count: 40
- This paper introduces MAPS (Mapping Attention head ParameterS), a framework that infers the functionality of attention heads in large language models directly from their parameters, without model training or inference.

## Executive Summary
This paper presents MAPS, a framework that efficiently infers the functionality of attention heads in large language models directly from their parameters, without requiring model training or inference. The approach represents attention heads as matrices mapping between vocabulary tokens, enabling analysis of predefined relations (e.g., country-to-capital, word-to-synonym) and salient operations. The framework demonstrates strong correlations (0.71-0.95) between static parameter analysis and dynamic head outputs during inference, and shows that removing relation heads causally affects model predictions.

## Method Summary
MAPS works by representing attention heads as matrices mapping between vocabulary tokens through the product E(WV O)U, where E is the embedding matrix, WV O is the attention head's output matrix, and U is the unembedding matrix. This creates a vocabulary-to-vocabulary mapping matrix M that captures the head's functional behavior. The framework measures predefined relations by analyzing top-k token rankings in M, identifies salient operations through saliency scores and GPT-4o pattern recognition, and validates results through human evaluation and causal experiments.

## Key Results
- MAPS estimations correlate strongly with head outputs during inference (0.71-0.95 correlation)
- Causal experiments show relation heads significantly affect model predictions when removed (≥32% accuracy drop)
- Automatic pipeline using GPT-4o successfully characterizes prominent operations for 62% of heads with human-verified accuracy
- Analysis reveals diverse operations including previously overlooked heads and shows architecture biases (e.g., grouped-query attention heads sharing functions)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention heads can be represented as vocabulary-to-vocabulary mappings without model inference
- Mechanism: The WV O weight matrix is multiplied by embedding and unembedding matrices to create a matrix M where each entry (i,j) represents the mapping strength from token i to token j. This transforms attention head parameters into interpretable vocabulary-space operations.
- Core assumption: The product E(WV O)U faithfully captures the attention head's functional behavior in the vocabulary space
- Evidence anchors:
  - [abstract]: "We propose MAPS (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference"
  - [section 2]: "Elhage et al. (2021); Dar et al. (2023) interpret each of the attention head matrices – WQK and WV O – as a matrix that maps between pairs of tokens from the vocabulary"
  - [corpus]: Weak - no direct corpus evidence found for this specific matrix multiplication approach
- Break condition: If the embedding/unembedding matrices are not aligned with the attention head's actual computation path, or if the head's behavior is highly context-dependent rather than vocabulary-driven

### Mechanism 2
- Claim: Patterns in the mapping matrix M reveal attention head functionality
- Mechanism: By analyzing submatrices of M corresponding to specific token groups (like countries or words), we can identify how strongly a head implements particular operations. The top-k scoring tokens for each source token reveal the head's priorities.
- Core assumption: Salient mappings in M correspond to the head's actual operational priorities during inference
- Evidence anchors:
  - [abstract]: "Evaluating MAPS on 20 operations across 6 popular LLMs shows its estimations correlate with the head's outputs during inference"
  - [section 3]: "Considering the scores of certain submatrices of M may reveal how the attention head operates on different sets of inputs"
  - [corpus]: Moderate - the DHA paper (arxiv:2406.06567) uses similar attention head analysis approaches
- Break condition: If the head's functionality depends on complex contextual interactions that aren't captured by static vocabulary mappings, or if the mapping scores are dominated by embedding norm effects

### Mechanism 3
- Claim: GPT-4o can automatically characterize attention head operations from salient mappings
- Mechanism: The pipeline identifies the most salient tokens for each head, collects their top mappings, and queries GPT-4o to describe patterns. This automates the interpretation of attention head functionality.
- Core assumption: Large language models can reliably identify patterns in attention head mappings that humans would recognize
- Evidence anchors:
  - [abstract]: "Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment"
  - [section 5.2]: "A human study shows that our automated pipeline performs reasonably well, and GPT-4o reliably detects observable operations"
  - [corpus]: Weak - no direct corpus evidence for GPT-4o-based attention head interpretation
- Break condition: If GPT-4o fails to recognize patterns that are semantically meaningful but not obvious, or if it hallucinates patterns that aren't actually present in the mappings

## Foundational Learning

- Concept: Attention head as matrix multiplication
  - Why needed here: Understanding how attention heads transform inputs is fundamental to interpreting their parameters
  - Quick check question: What are the two matrices (WQK and WV O) in an attention head and what roles do they play in the computation?

- Concept: Vocabulary space projection
  - Why needed here: The framework relies on projecting attention head parameters into the vocabulary space to make them interpretable
  - Quick check question: How does multiplying the WV O matrix by embedding and unembedding matrices transform attention head parameters into vocabulary-to-vocabulary mappings?

- Concept: Pattern recognition in high-dimensional spaces
  - Why needed here: Identifying functional patterns in the mapping matrix requires understanding how to extract meaningful information from large matrices
  - Quick check question: What are different ways to identify patterns in a large matrix - by rows, columns, submatrices, or statistical properties?

## Architecture Onboarding

- Component map:
  - Input layer: Raw text → Tokenization → Token embeddings
  - Attention heads: Query-Key (WQK) matrix + Value-Output (WV O) matrix → Attention weights → Contextualized representations
  - MAPS framework: WV O matrix → Vocabulary mapping matrix M → Pattern analysis → Functionality characterization
  - GPT-4o pipeline: Salient mappings → Pattern description → Automated interpretation

- Critical path:
  1. Extract WV O matrices from pre-trained models
  2. Compute vocabulary mapping matrix M = E(WV O)U
  3. Analyze patterns in M for predefined relations
  4. Identify salient tokens and their top mappings
  5. Use GPT-4o to describe observed patterns
  6. Validate with human judgment and causal experiments

- Design tradeoffs:
  - Static vs dynamic analysis: Parameter-based inference is faster but may miss context-dependent behaviors
  - Vocabulary coverage: Large vocabularies enable more precise mappings but increase computational cost
  - Pattern complexity: Simple patterns are easier to detect but may miss nuanced functionality
  - Automation vs human oversight: GPT-4o speeds up analysis but requires validation

- Failure signatures:
  - Low correlation between static and dynamic relation scores
  - GPT-4o consistently fails to identify patterns that humans can see
  - Salient mappings don't correspond to interpretable operations
  - High input skewness but low output space size (or vice versa)

- First 3 experiments:
  1. Implement basic MAPS framework on a small GPT-2 model and verify correlation with dynamic analysis
  2. Test GPT-4o pattern recognition on a small set of attention heads with known functionality
  3. Compare MAPS-identified functionality with known circuits from literature (e.g., Wang et al. 2023)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the analysis of attention heads be extended to include the WQK matrix and bias terms that are currently omitted in MAPS?
- Basis in paper: [explicit] The paper explicitly states that MAPS primarily focuses on analyzing the WV O component and mentions that "it is still valuable to examine the head's computation responsible for contextualization and for creating ˜X, i.e., the matrix WQK" and that "these bias terms may influence" the estimations.
- Why unresolved: The authors acknowledge that extending MAPS to include WQK and bias terms would provide a more comprehensive analysis but leave this as future work, noting that incorporating these components could improve the framework's coverage of early layers and account for potential influences on functionality estimations.
- What evidence would resolve it: Experimental results comparing the correlation between head outputs and estimations when including WQK and bias terms versus the current implementation would demonstrate whether this extension improves the framework's accuracy and coverage.

### Open Question 2
- Question: What is the optimal method to craft prompts that consistently trigger specific functionalities in attention heads, given that heads may exhibit different behaviors depending on context?
- Basis in paper: [inferred] The paper notes that while MAPS estimations strongly correlate with head outputs for task-related inputs, correlation is lower for contextualized inputs in some cases, and mentions that "the attention weight assigned to the name token may change depending on the context" when discussing name-copying heads.
- Why unresolved: The authors observe that heads can switch operations based on context and that different prompts can significantly affect dynamic relation scores, but they do not provide a systematic approach for crafting prompts that reliably trigger specific functionalities across different contexts and head types.
- What evidence would resolve it: A study demonstrating a methodology for prompt engineering that consistently elicits specific head functionalities across various contexts, with quantitative comparisons of dynamic relation scores across different prompt types for the same heads.

### Open Question 3
- Question: How can MAPS be extended to analyze operations that cannot be expressed through pairs of tokens, such as idioms and positional features?
- Basis in paper: [explicit] The paper explicitly states that "its expressivity is bounded by the model's vocabulary" and that "there are likely to be operations that this framework would overlook, such as idioms and positional features."
- Why unresolved: The authors recognize this fundamental limitation of the framework's reliance on token-pair mappings but do not propose concrete solutions for extending the analysis to capture these more complex operations that require broader contextual understanding.
- What evidence would resolve it: Development and validation of an extended framework that can analyze multi-token operations and positional features, with demonstrated success in identifying previously overlooked functionalities and maintaining strong correlation with head outputs during inference.

## Limitations

- The framework's expressivity is bounded by the model's vocabulary, potentially overlooking operations like idioms and positional features
- Static parameter analysis may miss context-dependent behaviors that only emerge during actual inference
- GPT-4o-based automated pipeline has 62% human-verified accuracy, meaning 38% of characterizations could be incorrect or incomplete

## Confidence

**Confidence: Medium** - While MAPS shows strong correlation with dynamic inference results (0.71-0.95), the framework's effectiveness depends heavily on the assumption that attention head parameters directly encode functional relationships in vocabulary space.

**Confidence: Medium** - The GPT-4o automated pipeline shows promise with 62% human-verified accuracy, but this still means nearly 40% of characterizations could be incorrect or incomplete.

**Confidence: Low** - The paper identifies architectural biases like grouped-query attention heads sharing functions, but the underlying reasons for these patterns remain unclear.

## Next Checks

1. **Cross-architecture validation**: Apply MAPS to attention heads from non-LLM architectures (e.g., vision transformers, multimodal models) to test the universality claim and identify architecture-specific limitations.

2. **Ablation studies on M matrix construction**: Systematically vary the embedding/unembedding matrix transformations (including alternatives like MLP layer application or layer norm) to determine which construction most faithfully captures attention head functionality.

3. **Temporal stability analysis**: Track attention head functionality across training checkpoints to understand how operations evolve and whether MAPS can detect emerging or decaying functionality before it becomes apparent through inference-based methods.