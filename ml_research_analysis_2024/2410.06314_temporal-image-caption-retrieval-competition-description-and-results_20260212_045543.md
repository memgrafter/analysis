---
ver: rpa2
title: Temporal Image Caption Retrieval Competition -- Description and Results
arxiv_id: '2410.06314'
source_url: https://arxiv.org/abs/2410.06314
tags:
- dataset
- temporal
- image
- caption
- competition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Temporal Image Caption Retrieval Competition
  (TICRC), which introduces a multimodal task combining visual, textual, and temporal
  information. The competition uses a dataset derived from the Chronicling America
  project, containing 3,902 instances of newspaper images with captions and publication
  dates spanning 274 years.
---

# Temporal Image Caption Retrieval Competition -- Description and Results

## Quick Facts
- arXiv ID: 2410.06314
- Source URL: https://arxiv.org/abs/2410.06314
- Reference count: 22
- Five teams participated; winning solution achieved MRR 0.3444 using EVA-CLIP without fine-tuning

## Executive Summary
This paper presents the Temporal Image Caption Retrieval Competition (TICRC), introducing a novel multimodal task that combines visual, textual, and temporal information for image-caption retrieval. The competition utilized a dataset from the Chronicling America project containing 3,902 newspaper images with captions spanning 274 years. Participants were tasked with retrieving correct captions for given images and dates from candidate sets. Five teams participated, with three surpassing the baseline performance. The winning approach used EVA-CLIP without fine-tuning to achieve an MRR of 0.3444 on the test-B dataset.

## Method Summary
The competition used a dataset derived from the Chronicling America project containing 3,902 instances of newspaper images paired with captions and publication dates. Participants received train, validation, and test splits to develop retrieval models. The evaluation metric was Mean Reciprocal Rank (MRR), which measures the rank position of the correct caption in retrieval results. Five teams submitted solutions, with three achieving scores above the baseline. The winning team employed the EVA-CLIP model without any fine-tuning, demonstrating the model's strong zero-shot capabilities for this multimodal retrieval task.

## Key Results
- Five teams participated in the competition
- Three teams achieved scores above the baseline
- Winning solution achieved MRR of 0.3444 on test-B dataset
- EVA-CLIP model used without fine-tuning proved effective for this task

## Why This Works (Mechanism)
The competition demonstrates that multimodal retrieval benefits from incorporating temporal context alongside visual and textual information. The EVA-CLIP model's strong zero-shot performance suggests that pre-trained vision-language models can effectively capture newspaper imagery patterns across centuries without task-specific adaptation. The temporal dimension adds a crucial retrieval constraint that helps disambiguate visually similar but temporally distinct content.

## Foundational Learning
1. **Mean Reciprocal Rank (MRR)** - why needed: standard retrieval evaluation metric that rewards correct answers at higher ranks; quick check: verify MRR calculation handles ties appropriately
2. **Multimodal representation learning** - why needed: enables joint modeling of images, text, and temporal features; quick check: confirm embeddings support similarity computation across modalities
3. **Zero-shot transfer learning** - why needed: allows leveraging pre-trained models without task-specific fine-tuning; quick check: measure performance degradation when removing temporal context

## Architecture Onboarding
Component map: Input (Image, Date, Captions) -> Feature Extraction (Visual, Temporal, Textual) -> Fusion Layer -> Similarity Scoring -> MRR Evaluation
Critical path: Image and caption features are extracted and compared, with temporal information serving as an additional filtering constraint
Design tradeoffs: Using pre-trained models without fine-tuning prioritizes generalization over task-specific optimization; temporal integration remains challenging for current models
Failure signatures: Low MRR scores indicate poor visual-textual matching or ineffective temporal filtering
First experiments: 1) Evaluate individual modality contributions through ablation, 2) Test temporal generalization on unseen date ranges, 3) Compare zero-shot vs fine-tuned EVA-CLIP performance

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 3,902 instances is relatively small for training complex multimodal models
- Competition format may not represent real-world open-ended retrieval scenarios
- Limited participant pool (5 teams) may not capture full potential of state-of-the-art approaches

## Confidence
- High Confidence: Competition design and MRR evaluation methodology
- Medium Confidence: Performance claims based on limited participant pool
- Low Confidence: Lack of systematic failure analysis and temporal generalization studies

## Next Checks
1. Test whether temporal pretraining on the full 274-year span improves performance
2. Conduct ablation studies to quantify relative contributions of visual, temporal, and textual features
3. Evaluate model robustness on out-of-distribution temporal ranges not seen during training