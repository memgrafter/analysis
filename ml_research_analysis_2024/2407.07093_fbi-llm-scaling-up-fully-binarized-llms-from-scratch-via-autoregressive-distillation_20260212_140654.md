---
ver: rpa2
title: 'FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation'
arxiv_id: '2407.07093'
source_url: https://arxiv.org/abs/2407.07093
tags:
- training
- fbi-llm
- llms
- binarized
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for training fully binarized large
  language models (FBI-LLMs) from scratch using autoregressive distillation. The approach
  achieves performance comparable to full-precision models while maintaining binary
  weights, with 1.01 average bit-width across model sizes of 130M, 1.3B, and 7B.
---

# FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation

## Quick Facts
- arXiv ID: 2407.07093
- Source URL: https://arxiv.org/abs/2407.07093
- Authors: Liqun Ma; Mingjie Sun; Zhiqiang Shen
- Reference count: 40
- This paper presents a method for training fully binarized large language models (FBI-LLMs) from scratch using autoregressive distillation, achieving performance comparable to full-precision models while maintaining binary weights.

## Executive Summary
This paper introduces FBI-LLM, a method for training fully binarized large language models from scratch using autoregressive distillation. The approach achieves performance comparable to full-precision models while maintaining binary weights with an average bit-width of 1.01 across model sizes of 130M, 1.3B, and 7B parameters. The key innovation is using autoregressive distillation with a full-precision teacher model to guide the binarization process, along with column-wise scaling factors to preserve representational capacity. Notably, the authors demonstrate that training from scratch performs equivalently to continuing from pretrained weights, challenging the assumption that initialization from pretrained weights is necessary for optimal performance in binarized models.

## Method Summary
FBI-LLMs use autoregressive distillation to train fully binarized models from scratch. The method replaces standard Linear layers with FBI-Linear modules (except in the causal head), while keeping embedding and layer norm layers in full-precision. During training, the model gradually distills from a full-precision teacher using an autoregressive distillation scheme that matches the predicted probabilities of the teacher at each token location. The cross-entropy between student and teacher outputs serves as the loss function. Column-wise scaling factors are applied to reduce quantization error and preserve representational capacity. The approach uses the Straight-Through Estimator (STE) to enable gradient propagation through non-differentiable binarization operations.

## Key Results
- FBI-LLMs achieve perplexity comparable to full-precision models while maintaining binary weights with 1.01 average bit-width
- The 7B FBI-LLM outperforms existing binarized models on perplexity and downstream tasks
- Analysis shows training from scratch yields similar results to continuing from pretrained weights, challenging conventional wisdom about binarization initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive distillation enables stable training of fully binarized LLMs from scratch
- Mechanism: Instead of using one-hot labels, the model matches probability distributions from a full-precision teacher at each token position, providing richer supervision signals
- Core assumption: Soft probability distributions from the teacher contain sufficient information to guide binarization without explicit one-hot supervision
- Evidence anchors:
  - [abstract] "we propose a novel training procedure based on distillation. Specifically, during training, we gradually distill from a full-precision teacher and adopt an autoregressive distillation-based scheme to match the predicted probabilities of the teacher model at each token location"
  - [section 3.3] "The cross-entropy between the outputs of the student model and the teacher model is calculated as the final loss function at each step of predicting the next token"
  - [corpus] Weak - no direct evidence about distillation effectiveness in binarization context
- Break condition: If teacher probabilities become too smooth or teacher and student distributions diverge significantly, the distillation signal becomes ineffective

### Mechanism 2
- Claim: Training from scratch performs equivalently to continuing from pretrained weights
- Mechanism: Binarization creates a fundamentally different parameter space pattern that requires distinct optimization, making initialization source irrelevant
- Core assumption: Full-precision and binarized models use different parameter configurations to encode semantics
- Evidence anchors:
  - [abstract] "by analyzing the training trajectory, we find that the pretrained weight is not necessary for training binarized LLMs from scratch"
  - [section 5.1] "Throughout the training process, the FFratio for both methods are on similar scales and the values are relatively small"
  - [corpus] Weak - limited evidence on initialization independence in binarization
- Break condition: If initialization significantly affects early training stability or final performance metrics

### Mechanism 3
- Claim: Column-wise scaling with learnable parameters improves binarization accuracy
- Mechanism: Scaling factors reduce quantization error between binarized and original parameters while maintaining representational capacity
- Core assumption: Adding learnable scaling parameters provides sufficient flexibility to compensate for information loss in binarization
- Evidence anchors:
  - [section 3.2] "Scale factors can effectively reduce the error between the binarized and original parameters, thereby preserving the more representational capacity"
  - [section 3.2] "In the FBI-linear, we apply scaling at the granularity of the matrix columns"
  - [corpus] Weak - limited evidence on scaling effectiveness specifically for binarization
- Break condition: If scaling parameters fail to converge or become numerically unstable during training

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Provides soft supervision signals that guide binarization process more effectively than hard labels
  - Quick check question: What's the key difference between using teacher probabilities versus one-hot labels in distillation?

- Concept: Straight-Through Estimator (STE)
  - Why needed here: Enables gradient propagation through non-differentiable binarization operation
  - Quick check question: How does STE approximate gradients for non-differentiable functions?

- Concept: Flip-Flop Ratio
  - Why needed here: Measures parameter stability during binarization training
  - Quick check question: What does a high flip-flop ratio indicate about training stability?

## Architecture Onboarding

- Component map: FBI-Linear modules replace standard Linear layers (except causal head), embedding and layer norm layers remain full-precision
- Critical path: Autoregressive distillation loss computation → STE gradient estimation → FBI-Linear parameter updates
- Design tradeoffs: Full binarization maximizes efficiency but requires distillation; keeping some layers full-precision maintains accuracy but reduces compression
- Failure signatures: Training loss spikes, flip-flop ratio instability, gradient norm explosions
- First 3 experiments:
  1. Verify STE gradient computation matches expected patterns
  2. Test distillation loss effectiveness on small binarized model
  3. Measure parameter stability during initial training epochs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the binarization process fundamentally alter the parameter space structure compared to full-precision models, and if so, how does this affect optimization dynamics?
- Basis in paper: [explicit] The paper states "binarized and full-precision LLMs employ different parameter combinations and configurations to encode semantics, which results in substantial divergences in their parameter space pattern."
- Why unresolved: While the paper identifies this divergence, it does not provide a detailed analysis of how the binarization affects the geometry of the parameter space or how this impacts optimization landscapes.
- What evidence would resolve it: A comparative analysis of the parameter space topology between binarized and full-precision models, including visualization of loss landscapes and gradient flow patterns during training.

### Open Question 2
- Question: What is the optimal trade-off between model size and performance when scaling fully binarized LLMs beyond 7B parameters?
- Basis in paper: [inferred] The paper demonstrates effectiveness at 130M, 1.3B, and 7B scales, but explicitly states "limited by computational resources, the current results for FBI-LLM 7B are not final."
- Why unresolved: The paper only provides results for models up to 7B parameters, leaving open questions about how performance scales with larger binarized models.
- What evidence would resolve it: Training and evaluation of fully binarized LLMs at scales of 13B, 30B, and 70B parameters, comparing perplexity and downstream task performance against their full-precision counterparts.

### Open Question 3
- Question: How does the proposed autoregressive distillation objective compare to other knowledge distillation approaches for training binarized LLMs?
- Basis in paper: [explicit] The paper states "using autoregressive distillation objective is more effective in training binarized LLMs" compared to standard autoregressive loss, but does not compare against other distillation methods.
- Why unresolved: The paper only compares autoregressive distillation against the standard autoregressive loss, without exploring other distillation techniques like response-based distillation or multi-teacher distillation.
- What evidence would resolve it: A comprehensive comparison of autoregressive distillation against other knowledge distillation methods (e.g., response-based, feature-based, multi-teacher) on the same binarized LLM architecture and training setup.

## Limitations

- Training instability for larger models (1.3B and 7B) requiring multiple restarts to achieve good results
- Limited evaluation scope focusing primarily on perplexity and a small set of zero-shot downstream tasks
- Computational efficiency claims not empirically validated with comprehensive inference benchmarks

## Confidence

**High Confidence Claims**:
- The autoregressive distillation method can train binarized LLMs from scratch
- Column-wise scaling with learnable parameters improves binarization accuracy
- Training from scratch achieves comparable results to continuing from pretrained weights

**Medium Confidence Claims**:
- FBI-LLMs achieve comparable perplexity to full-precision models
- The 1.01 average bit-width across model sizes is achievable
- Flip-flop ratio analysis provides meaningful insights into training stability

**Low Confidence Claims**:
- FBI-LLMs outperform all existing binarized models across all metrics
- The approach generalizes well to all types of language tasks
- Training from scratch is always preferable to using pretrained weights

## Next Checks

1. **Training Stability Analysis**: Conduct controlled experiments to measure training stability across different random seeds and hardware configurations for all three model sizes. Track flip-flop ratio evolution and loss curves to identify conditions that lead to instability.

2. **Comprehensive Downstream Evaluation**: Test FBI-LLMs on a broader range of tasks including few-shot learning, fine-tuning scenarios, and more complex reasoning benchmarks. Compare performance against both full-precision models and other binarization approaches under identical evaluation conditions.

3. **Inference Efficiency Benchmarking**: Measure actual inference latency, memory usage, and throughput on different hardware platforms (GPU, CPU, specialized accelerators). Compare these metrics against both full-precision and other quantized models to validate the claimed efficiency benefits.