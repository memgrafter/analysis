---
ver: rpa2
title: Direct Preference Knowledge Distillation for Large Language Models
arxiv_id: '2406.19774'
source_url: https://arxiv.org/abs/2406.19774
tags:
- reward
- dpkd
- distillation
- function
- divergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method called Direct Preference Knowledge
  Distillation (DPKD) for knowledge distillation of large language models. The key
  idea is to use an implicit reward function and preference modeling to supplement
  the traditional KL divergence in KD.
---

# Direct Preference Knowledge Distillation for Large Language Models

## Quick Facts
- arXiv ID: 2406.19774
- Source URL: https://arxiv.org/abs/2406.19774
- Reference count: 22
- Primary result: DPKD outperforms baseline methods in both output response precision and exact match percentage for LLM distillation

## Executive Summary
This paper proposes Direct Preference Knowledge Distillation (DPKD), a novel method for knowledge distillation of large language models that supplements traditional KL divergence with an implicit reward function and preference modeling. The approach addresses limitations of KL divergence when distilling from significantly larger teacher models to smaller student models. DPKD is formulated as a two-stage optimization process that first maximizes an objective combining implicit reward and reverse KL divergence, then improves the preference probability of teacher outputs over student outputs. Experiments across various LLM sizes (120M to 13B parameters) and instruction tuning datasets demonstrate superior performance compared to baseline methods.

## Method Summary
DPKD reformulates knowledge distillation by incorporating an implicit reward function that measures preference between outputs using the Bradley-Terry model. The method operates in two stages: first optimizing an objective combining the implicit reward with reverse KL divergence, then maximizing the probability that teacher outputs are preferred over student outputs. The approach includes length normalization to prevent bias toward shorter responses and preserves language modeling capabilities through an additional loss term. Experiments use teacher models GPT-2 (1.5B) and OPT (13B) with student models ranging from 120M to 1.3B parameters, trained on instruction tuning datasets including Dolly, Self-Inst, and SUPER NATURAL-INSTRUCTIONS.

## Key Results
- DPKD outperforms baseline methods (SFT, KD, SeqKD, MiniLLM, AKL) in both output response precision and exact match percentage
- Superior performance maintained across various generation lengths on test subsets
- Demonstrates effectiveness for distilling from 13B parameter teacher models to 120M-1.3B parameter student models
- Shows robustness across different instruction tuning datasets and model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL divergence is insufficient for LLM distillation when teacher models are significantly larger than student models
- Mechanism: Traditional KL divergence fails to capture full distributional differences between large teacher and smaller student models, especially for complex distributions
- Core assumption: KL divergence measures distributional difference but becomes asymmetric and less effective for large model distillation
- Evidence anchors: [abstract] "KL divergence is insufficient under the condition of a stronger teacher model"; [section] "KL divergence measures the difference between two distributions, but it is not really a distance because it is asymmetric"
- Break condition: When student and teacher models are of similar size, KL divergence may suffice

### Mechanism 2
- Claim: Implicit reward function acts as a supplement to KL divergence by measuring preference between outputs
- Mechanism: The reward function estimates the gain of choosing one output over another using Bradley-Terry model, creating preference-based optimization
- Core assumption: The implicit reward can be estimated from the ratio of student and teacher output probabilities
- Evidence anchors: [abstract] "LLMs can serve as an implicit reward function, which we define as a supplement to KL divergence"; [section] "From the BT model, we can obtain the following probability: Pr(y1≻y2) = σ(r(y1)−r(y2))"
- Break condition: When the reward estimation becomes unstable or noisy

### Mechanism 3
- Claim: The two-stage optimization process improves preference probability of teacher outputs over student outputs
- Mechanism: First stage optimizes implicit reward plus reverse KL divergence, second stage maximizes probability that teacher outputs are preferred over student outputs
- Core assumption: The two-stage process can effectively capture both distributional and preference-based learning objectives
- Evidence anchors: [abstract] "We re-formulate KD of LLMs into two stages: first optimizing an objective consisting of implicit reward and reverse KL divergence and then improving the preference probability"; [section] "The purpose of KD is to fit the distribution of the student model to teacher model, which can also be understood as we expect the student model to have a greater probability of outputting results similar to the teacher model"
- Break condition: When the two-stage optimization becomes computationally prohibitive or overfits

## Foundational Learning

- Concept: Kullback-Leibler divergence
  - Why needed here: KL divergence is the traditional metric for knowledge distillation that measures distributional difference between teacher and student models
  - Quick check question: What makes KL divergence asymmetric and why does this matter for knowledge distillation?

- Concept: Bradley-Terry preference model
  - Why needed here: The BT model provides the theoretical foundation for measuring preference between two outputs, which is crucial for the preference-based distillation approach
  - Quick check question: How does the BT model calculate the probability that one output is preferred over another?

- Concept: Markov Decision Process (MDP)
  - Why needed here: The MDP framework connects the sequence generation task to reinforcement learning concepts, allowing the reward function to be interpreted as a Q-function
  - Quick check question: In the MDP perspective of sequence generation, what are the states, actions, and transitions?

## Architecture Onboarding

- Component map:
  Teacher model → Generate outputs → Compute implicit reward → Student model optimization → Preference improvement → Final distilled model

- Critical path: Teacher model → Generate outputs → Compute implicit reward → Student model optimization → Preference improvement → Final distilled model

- Design tradeoffs:
  - Adding implicit reward increases complexity but addresses KL divergence limitations
  - Two-stage optimization improves preference learning but requires more computation
  - Length normalization prevents bias toward shorter outputs but adds hyperparameter tuning
  - Language modeling loss maintains general capabilities but dilutes distillation focus

- Failure signatures:
  - Poor performance despite training indicates insufficient reward estimation or preference modeling
  - Model collapse to short outputs suggests missing length normalization
  - Inability to generalize beyond training data indicates missing language modeling loss
  - High computational cost suggests inefficient two-stage optimization

- First 3 experiments:
  1. Compare KL divergence-only distillation vs DPKD on small dataset to verify effectiveness of implicit reward
  2. Test different preference objective variants (SimPO, CPO, IPO) to identify optimal formulation
  3. Analyze training curves of reward, KLD, and reverse KLD to understand convergence behavior and identify optimal checkpoints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DPKD scale with different reward function formulations beyond the basic form presented in the paper?
- Basis in paper: [explicit] The paper mentions that different preference expressions related to reward functions yield significantly different results, and that various forms like IPO, CPO, and SimPO were tested with promising but not superior results to the basic DPKD formulation
- Why unresolved: The paper only provides preliminary experiments on a few alternative reward formulations without comprehensive exploration or optimization of these variants for the specific task of knowledge distillation
- What evidence would resolve it: Systematic experiments comparing DPKD performance across a wider range of reward function formulations, including both established preference optimization methods and novel variants specifically designed for knowledge distillation contexts

### Open Question 2
- Question: What is the optimal value of the hyperparameter β that balances the contribution of the implicit reward and reverse KL divergence components in DPKD?
- Basis in paper: [inferred] The paper mentions that the magnitude of the reward function may be related to the hyperparameter β used during training, which adjusts the relative proportions of the KL divergence and the reward function
- Why unresolved: The paper does not provide an analysis of how different β values affect the final performance, nor does it discuss methods for determining the optimal β value
- What evidence would resolve it: Experiments showing DPKD performance across a range of β values, along with analysis of the trade-off between reward maximization and KL divergence minimization at different β settings

### Open Question 3
- Question: How does DPKD perform in black-box knowledge distillation scenarios where the teacher model's internal parameters are not accessible?
- Basis in paper: [explicit] The paper focuses on white-box distillation using output distributions, but mentions that black-box distillation (using only teacher-generated text) is another approach to KD for LLMs
- Why unresolved: The paper does not test DPKD in black-box settings or explore how the preference-based approach would need to be modified when teacher logits are unavailable
- What evidence would resolve it: Experiments comparing DPKD performance in both white-box and black-box settings, along with analysis of how the implicit reward function can be estimated or approximated without access to teacher model parameters

## Limitations

- The paper relies heavily on ablation studies without comprehensive comparisons to state-of-the-art preference optimization methods like DPO
- Performance metrics focus primarily on Rouge-L scores and exact match percentages, lacking human evaluation or qualitative analysis
- The implicit reward function estimation may become unstable or noisy during training, especially with significantly different model sizes

## Confidence

**High confidence**: The theoretical framework connecting knowledge distillation to reinforcement learning concepts (Q-functions, MDPs) is well-established and correctly applied

**Medium confidence**: The experimental results showing improved performance over baseline methods are convincing, but comparisons may not be comprehensive enough to establish definitive superiority

**Low confidence**: The scalability of the approach to extremely large language models (beyond 13B parameters) is not demonstrated, and potential limitations in handling diverse instruction types remain unexplored

## Next Checks

1. **Comprehensive baseline comparison**: Conduct head-to-head comparisons with state-of-the-art preference optimization methods (DPO, IPO, CPO) on the same datasets and model configurations, including both automatic metrics and human evaluation

2. **Ablation of two-stage optimization**: Systematically evaluate each component of the two-stage optimization process through controlled ablation studies, measuring individual contributions of implicit reward, reverse KL divergence, and preference probability improvement

3. **Stability analysis of reward estimation**: Monitor the implicit reward function throughout training to quantify its stability and identify conditions under which it becomes unreliable, testing alternative reward estimation methods and implementing regularization techniques