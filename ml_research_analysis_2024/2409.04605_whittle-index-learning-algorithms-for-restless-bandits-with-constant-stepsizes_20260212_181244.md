---
ver: rpa2
title: Whittle Index Learning Algorithms for Restless Bandits with Constant Stepsizes
arxiv_id: '2409.04605'
source_url: https://arxiv.org/abs/2409.04605
tags:
- learning
- index
- q-learning
- algorithm
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the Whittle index learning algorithm for restless
  multi-armed bandits. The authors consider Q-learning with constant stepsizes and
  various exploration policies (epsilon-greedy, softmax, and epsilon-softmax) for
  a single-armed restless bandit.
---

# Whittle Index Learning Algorithms for Restless Bandits with Constant Stepsizes

## Quick Facts
- **arXiv ID**: 2409.04605
- **Source URL**: https://arxiv.org/abs/2409.04605
- **Reference count**: 32
- **Primary result**: Proposed two-timescale index learning algorithms successfully learn Whittle indices for restless bandits using Q-learning, DQN, and linear function approximation with constant stepsizes

## Executive Summary
This paper presents index learning algorithms for restless multi-armed bandits that learn Whittle indices without knowing model parameters. The authors develop a two-timescale stochastic approximation approach where Q-learning updates occur on a faster timescale while index updates occur on a slower timescale. The algorithms incorporate constant stepsizes and various exploration policies (epsilon-greedy, softmax, and epsilon-softmax), and are extended to work with deep Q-networks and linear function approximation. Numerical experiments demonstrate that the proposed algorithms can effectively learn Whittle indices, with re-initialization strategies improving convergence speed.

## Method Summary
The paper proposes a two-timescale stochastic approximation algorithm for learning Whittle indices in restless bandits. On the faster timescale, Q-learning updates improve Q-function estimates assuming a fixed index value using constant step size α. On the slower timescale, the Whittle index is updated using step size γ (where γ << α) based on the difference between Q-values for actions a=1 and a=0. The algorithm incorporates exploration policies (epsilon-greedy, softmax, epsilon-softmax) and re-initialization strategies to ensure all state-action pairs are sufficiently visited. The approach is extended to deep Q-networks with experience replay and target networks, as well as linear function approximation through state aggregation for larger state spaces.

## Key Results
- The two-timescale index learning algorithm successfully learns Whittle indices for single-armed restless bandits
- Re-initialization strategies significantly improve convergence speed by ensuring exploration of all state-action pairs
- Performance varies with exploration policy choice, with epsilon-greedy showing fastest convergence in tested examples
- The algorithm scales to larger state spaces using DQN and linear function approximation methods
- Error metric (max_s |Q(s,1,λ) - Q(s,0,λ)|) converges to near-zero values across different implementations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-timescale stochastic approximation approach ensures convergence of both the Q-function estimates and the Whittle index estimates.
- Mechanism: On the faster timescale, Q-learning updates improve the Q-function estimates assuming a fixed index. On the slower timescale, the index is updated based on the difference between Q-values for actions a=1 and a=0. The separation of timescales (γ << α) allows the slower index updates to track the limiting ODE while the faster Q-learning tracks its own stable equilibrium.
- Core assumption: The Q-learning updates converge to a stable equilibrium for a fixed index, and the index updates are slow enough to track the limiting ODE.
- Evidence anchors:
  - [abstract] "The algorithm of index learning is two-timescale variant of stochastic approximation, on slower timescale we update index learning scheme and on faster timescale we update Q-learning assuming fixed index value."
  - [section] "We study constant stepsizes two timescale stochastic approximation algorithm. We provide analysis of two-timescale stochastic approximation for index learning with constant stepsizes."
  - [corpus] Weak - no direct citation supporting this mechanism.
- Break condition: If the stepsize ratio γ/α is not sufficiently small, or if the Q-learning updates do not converge to a stable equilibrium for fixed indices.

### Mechanism 2
- Claim: Re-initialization improves convergence speed by ensuring all state-action pairs are sufficiently visited.
- Mechanism: Periodic re-initialization of the current state forces exploration of all state-action pairs, preventing the algorithm from getting stuck in suboptimal regions of the state space. This is particularly important in larger state spaces where some state-action pairs may be infrequently visited.
- Core assumption: The underlying MDP has sufficient connectivity to allow exploration from any state.
- Evidence anchors:
  - [section] "To accelerate convergence of learning learning algorithm, we use we use re-initialization approach after periodic interval which can ensures that all state action pair sufficiently visited."
  - [section] "Re-initialization allows exploration of all state-action pair which did not happen in earlier example."
  - [corpus] Weak - no direct citation supporting this mechanism.
- Break condition: If the re-initialization period is too long, or if the MDP structure prevents effective exploration even with re-initialization.

### Mechanism 3
- Claim: Different exploration policies (epsilon-greedy, softmax, epsilon-softmax) affect the convergence rate of the Q-learning component.
- Mechanism: The choice of exploration policy determines how the agent balances exploration and exploitation during the Q-learning updates. Some policies may explore more effectively in certain MDP structures, leading to faster convergence of the Q-function estimates.
- Core assumption: The optimal exploration strategy depends on the specific structure of the MDP.
- Evidence anchors:
  - [section] "We also studied exploration schemes ϵ-greedy, softmax and ϵ-softmax policies. We observed that the performance of exploration policies is model dependent."
  - [section] "We illustrate the significance of action-selection policy for faster convergence of algorithm."
  - [corpus] Weak - no direct citation supporting this mechanism.
- Break condition: If the exploration policy is too greedy, the agent may not explore enough; if too exploratory, it may not exploit sufficiently.

## Foundational Learning

- Concept: Restless Multi-Armed Bandits (RMAB)
  - Why needed here: The paper studies learning algorithms specifically for RMAB problems, which require handling arms that evolve independently regardless of whether they are pulled.
  - Quick check question: What distinguishes a restless bandit from a standard multi-armed bandit?

- Concept: Whittle Index
  - Why needed here: The Whittle index is the target of the learning algorithms - the goal is to learn this index without knowing the model parameters.
  - Quick check question: Under what conditions is the Whittle index policy applicable?

- Concept: Two-Timescale Stochastic Approximation
  - Why needed here: The convergence analysis relies on the theory of two-timescale stochastic approximation to prove that both the Q-function and index estimates converge.
  - Quick check question: What is the key condition that must hold between the two stepsizes in a two-timescale algorithm?

## Architecture Onboarding

- Component map:
  - Environment (MDP) -> Action selection (exploration policy) -> State transition/reward -> Q-learning update -> Index update

- Critical path: State → Action selection (exploration policy) → Environment response (reward, next state) → Q-learning update → Index update (periodic)

- Design tradeoffs:
  - Stepsize ratio (γ/α): Smaller ratios ensure better timescale separation but may slow convergence
  - Re-initialization frequency: More frequent re-initialization improves exploration but may disrupt learning progress
  - Exploration policy choice: Different policies perform better on different MDP structures
  - Function approximation vs tabular: DQN handles larger state spaces but is computationally expensive

- Failure signatures:
  - Q-function estimates not converging: May indicate inappropriate stepsize ratio or poor exploration
  - Index estimates oscillating: May indicate stepsize ratio too large or insufficient timescale separation
  - Slow convergence: May benefit from re-initialization or different exploration policy
  - Poor performance in large state spaces: May require function approximation (DQN or linear)

- First 3 experiments:
  1. Run Q-learning with epsilon-greedy policy on a simple RMAB (e.g., 5-state random walk) and verify Q-function convergence
  2. Implement the two-timescale index learning algorithm on the same RMAB and verify index convergence
  3. Compare convergence rates using different exploration policies (epsilon-greedy, softmax, epsilon-softmax) on the same RMAB

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of index learning with different exploration schemes (epsilon-greedy, softmax, epsilon-softmax) vary across different restless bandit models and structures?
- Basis in paper: [explicit] The paper explicitly discusses and compares these exploration schemes, noting that performance is model-dependent.
- Why unresolved: While the paper provides numerical examples showing varying performance, it doesn't offer a comprehensive theoretical analysis or guidelines for selecting the optimal exploration scheme for different model structures.
- What evidence would resolve it: A systematic study comparing the performance of these exploration schemes across a wide range of restless bandit models with varying structures, along with theoretical analysis explaining the observed differences.

### Open Question 2
- Question: What are the theoretical convergence guarantees for the two-timescale stochastic approximation algorithm used in index learning with constant stepsizes?
- Basis in paper: [explicit] The paper mentions that the analysis of two-timescale stochastic approximation for index learning with constant stepsizes is provided in the appendix, but the main text does not elaborate on these guarantees.
- Why unresolved: The paper only mentions the existence of this analysis in the appendix without providing details or results in the main text, leaving readers without insight into the convergence properties.
- What evidence would resolve it: A detailed explanation of the convergence proof and results for the two-timescale algorithm with constant stepsizes, including conditions for convergence and rate of convergence.

### Open Question 3
- Question: How does the performance of index learning with linear function approximation (state-aggregation) compare to other function approximation methods (e.g., neural networks) for larger state spaces?
- Basis in paper: [inferred] The paper mentions studying index learning with deep Q-networks (DQN) and linear function approximation, noting that DQN is computationally expensive. It also presents results for linear function approximation with a specific example.
- Why unresolved: The paper provides limited comparison between different function approximation methods, focusing mainly on Q-learning and DQN. It doesn't explore the performance of linear function approximation relative to other methods for various state space sizes.
- What evidence would resolve it: A comprehensive comparison of index learning performance using different function approximation methods (linear, neural networks, etc.) across a range of state space sizes, including computational efficiency and accuracy metrics.

## Limitations

- The analysis assumes finite-state MDPs with known state spaces, limiting applicability to environments with very large or continuous state spaces
- The convergence guarantees rely on the stepsize ratio γ/α being sufficiently small, but no explicit bound is provided
- The performance of exploration policies is described as "model dependent" without systematic analysis of which policies work best under which conditions

## Confidence

- **High confidence**: The two-timescale stochastic approximation framework and its theoretical foundation are well-established in the literature. The algorithm's basic structure and convergence mechanism are sound.
- **Medium confidence**: The empirical results showing learning of Whittle indices across different exploration policies and function approximation methods are convincing, but the lack of comparison with baseline algorithms makes it difficult to assess relative performance.
- **Low confidence**: The claims about re-initialization improving convergence are based on limited numerical examples without systematic study of re-initialization frequency or alternative exploration strategies.

## Next Checks

1. Conduct systematic experiments varying the stepsize ratio γ/α to identify the threshold below which convergence is guaranteed, and measure the impact on convergence speed.
2. Compare the proposed index learning algorithm against model-free RL baselines (e.g., direct Q-learning without index structure) and model-based approaches on the same benchmark problems.
3. Extend experiments to continuous or very large state spaces where function approximation is necessary, and evaluate the sample efficiency and computational requirements compared to tabular methods.