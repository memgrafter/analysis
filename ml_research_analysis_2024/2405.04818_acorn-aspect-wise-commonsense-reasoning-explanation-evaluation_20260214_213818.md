---
ver: rpa2
title: 'ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation'
arxiv_id: '2405.04818'
source_url: https://arxiv.org/abs/2405.04818
tags:
- human
- explanation
- explanations
- llms
- raters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLMs show promise for evaluating free-text explanations, but their\
  \ ratings diverge from human consensus in many cases. Using a new dataset of 3,500\
  \ explanations with aspect-wise human ratings, we found that replacing human raters\
  \ with LLMs often decreased inter-annotator agreement, suggesting the models\u2019\
  \ judgments conflict with human ones."
---

# ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation

## Quick Facts
- arXiv ID: 2405.04818
- Source URL: https://arxiv.org/abs/2405.04818
- Reference count: 40
- Primary result: LLMs show promise for evaluating free-text explanations but diverge from human consensus in many cases

## Executive Summary
This paper introduces ACORN, a dataset of 3,500 free-text commonsense reasoning explanations annotated with aspect-wise human ratings. The authors evaluate whether large language models (LLMs) can replace or supplement human raters in assessing explanation quality across multiple aspects (plausibility, relevance, sufficiency, etc.). Using a zero-shot evaluation framework with GPT-3.5, GPT-4, and Claude, they find that LLMs generally perform worse than human raters, with most models decreasing inter-annotator agreement when replacing humans. GPT-4 is an exception, maintaining or improving agreement in most cases, though its correlation with human ratings remains imperfect (0.53-0.95 range). The study concludes that while LLMs are not yet reliable replacements for human raters, they may be useful in targeted configurations with human oversight.

## Method Summary
The authors developed a dataset of 3,500 commonsense reasoning explanations covering three tasks: abductive reasoning, commonsense validation, and counterfactual reasoning. Each explanation was rated by 4-7 human annotators across five aspects: plausibility, relevance, sufficiency, clarity, and specificity. For LLM evaluation, they employed a zero-shot approach using carefully crafted prompts that presented human-rated explanations alongside candidate explanations, asking models to rate them. The evaluation framework compared LLM ratings against human consensus ratings, measuring correlation and inter-annotator agreement across different configurations (LLM replacing humans vs. LLM as additional rater). The study used three state-of-the-art models (GPT-3.5, GPT-4, Claude) and systematically varied the number of human raters in the ensemble to assess the impact of adding LLMs.

## Key Results
- Replacing human raters with LLMs decreased inter-annotator agreement in most cases (except GPT-4, which maintained or improved agreement)
- GPT-4 correlation with majority-voted human ratings ranged from 0.53 to 0.95 across aspects, averaging 0.72
- Adding GPT-4 as an additional rater only improved outcomes when there were two human raters; with three or more humans, it was neutral or detrimental
- Most LLMs failed to capture the nuanced aspect-wise distinctions that human raters made

## Why This Works (Mechanism)
The evaluation mechanism works by leveraging LLMs' ability to understand natural language and perform comparative reasoning tasks. The zero-shot approach relies on the model's inherent understanding of quality dimensions (plausibility, relevance, etc.) without requiring task-specific training. By presenting the model with human-rated examples as reference points, the framework enables relative scoring that aligns with established human judgment patterns. The multi-aspect evaluation captures different dimensions of explanation quality that LLMs must simultaneously reason about, revealing their strengths and limitations in handling complex, multi-faceted assessment tasks.

## Foundational Learning

**Commonsense reasoning** - The ability to make logical inferences based on everyday knowledge and experience. Needed because the evaluation targets commonsense explanations. Quick check: Can the model distinguish between logically valid and commonsense-valid inferences?

**Zero-shot learning** - Evaluating models without task-specific training examples. Needed because the study aims to assess general reasoning capabilities. Quick check: Does the model perform well without any fine-tuning on the specific task?

**Inter-annotator agreement** - A measure of consistency between different raters' judgments. Needed to quantify the reliability of ratings and the impact of LLM inclusion. Quick check: What is the Fleiss' kappa or similar metric for human-only ratings?

**Aspect-wise evaluation** - Assessing different quality dimensions separately rather than using a single holistic score. Needed to capture the multi-faceted nature of explanation quality. Quick check: Are the individual aspect ratings internally consistent?

## Architecture Onboarding

Component map: Human annotators -> ACORN dataset -> LLM evaluation framework -> Correlation metrics -> Agreement analysis

Critical path: Human-rated explanations → LLM zero-shot evaluation → Statistical comparison with human consensus → Performance assessment

Design tradeoffs: Zero-shot approach vs. fine-tuning (zero-shot preserves generality but may underperform specialized models); comprehensive multi-aspect evaluation vs. simpler single-score assessment (more nuanced but more complex to analyze)

Failure signatures: Low correlation with human ratings (0.53-0.95 range indicates substantial disagreement); decreased inter-annotator agreement when replacing humans; inconsistent performance across different aspects

3 first experiments:
1. Baseline: Compare inter-annotator agreement among 4-7 human raters on the same explanations
2. LLM replacement: Replace human raters one-by-one with GPT-4 and measure agreement changes
3. LLM augmentation: Add GPT-4 as an additional rater to existing human ensembles of varying sizes

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The study relies on a single dataset (ACORN) which may not generalize to other domains or explanation types
- The moderate correlation (average 0.72) between GPT-4 ratings and human majority votes indicates substantial disagreement remains
- The conclusion that LLMs are "not yet reliable replacements" is based on specific experimental conditions that may not reflect fundamental limitations

## Confidence

High confidence:
- LLMs generally decrease inter-annotator agreement when replacing human raters
- GPT-4 maintains or improves agreement in most cases
- Adding GPT-4 as an additional rater helps only with two human raters

Medium confidence:
- GPT-4 correlation with human ratings (0.53-0.95 range)
- Overall utility of LLMs depends on specific use cases and human oversight

Low confidence:
- Generalizability to other explanation domains
- Whether findings hold for newer LLM versions
- Impact of different prompt engineering approaches

## Next Checks

1. Test the same evaluation framework on diverse explanation datasets (e.g., technical, scientific, or narrative explanations) to assess generalizability

2. Compare newer LLM versions (post-GPT-4) using identical prompts and evaluation protocols to determine if performance has improved

3. Conduct ablation studies varying prompt templates, few-shot examples, and temperature settings to identify optimal configurations for human-AI agreement