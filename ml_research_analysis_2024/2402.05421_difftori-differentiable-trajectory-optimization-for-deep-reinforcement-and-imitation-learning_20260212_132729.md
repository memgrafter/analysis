---
ver: rpa2
title: 'DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and
  Imitation Learning'
arxiv_id: '2402.05421'
source_url: https://arxiv.org/abs/2402.05421
tags:
- difftori
- policy
- learning
- optimization
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffTORI introduces differentiable trajectory optimization as a
  policy representation for deep reinforcement and imitation learning. The method
  leverages recent advances in differentiable trajectory optimization to compute gradients
  of the loss with respect to cost and dynamics function parameters, enabling end-to-end
  learning.
---

# DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning

## Quick Facts
- arXiv ID: 2402.05421
- Source URL: https://arxiv.org/abs/2402.05421
- Authors: Weikang Wan; Ziyu Wang; Yufei Wang; Zackory Erickson; David Held
- Reference count: 40
- Key outcome: DiffTORI introduces differentiable trajectory optimization as a policy representation for deep reinforcement and imitation learning, achieving state-of-the-art performance across 15 model-based RL tasks and 35 imitation learning tasks with high-dimensional sensory inputs.

## Executive Summary
DiffTORI introduces differentiable trajectory optimization as a policy representation for deep reinforcement and imitation learning. The method leverages recent advances in differentiable trajectory optimization to compute gradients of the loss with respect to cost and dynamics function parameters, enabling end-to-end learning. For reinforcement learning, DiffTORI addresses the "objective mismatch" problem by directly optimizing task performance through policy gradient loss backpropagated via trajectory optimization. For imitation learning, it generates actions through test-time optimization with learned cost functions.

The approach achieves state-of-the-art performance across 15 model-based RL tasks and 35 imitation learning tasks with high-dimensional sensory inputs. In model-based RL, DiffTORI outperforms prior methods on 14 out of 15 DeepMind Control Suite tasks. For imitation learning, it achieves higher success rates than baselines on Robomimic, MetaWorld, and ManiSkill benchmarks, particularly excelling on tasks where other methods struggle. The method demonstrates the effectiveness of differentiable trajectory optimization as a policy class for complex robotic tasks with image and point cloud observations.

## Method Summary
DiffTORI combines differentiable trajectory optimization with model-based reinforcement learning and imitation learning frameworks. The core innovation is using differentiable trajectory optimization as a policy representation, allowing end-to-end learning through backpropagation. For model-based RL, it extends TD-MPC by replacing non-differentiable planning with differentiable optimization and including policy gradient loss to directly optimize task performance. For imitation learning, it uses a CVAE architecture where the decoder is a trajectory optimizer that generates actions conditioned on sampled latent vectors. The method is implemented using Theseus for differentiable trajectory optimization and tested across multiple benchmarks with high-dimensional sensory inputs.

## Key Results
- Outperforms prior methods on 14 out of 15 DeepMind Control Suite tasks in model-based RL
- Achieves higher success rates than baselines on Robomimic, MetaWorld, and ManiSkill benchmarks for imitation learning
- Demonstrates the effectiveness of differentiable trajectory optimization as a policy class for complex robotic tasks
- Particularly excels on tasks where other methods struggle, showing strong generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DiffTORI directly optimizes task performance by differentiating the policy gradient loss through the trajectory optimization process, solving the "objective mismatch" problem.
- Mechanism: The policy gradient loss is computed on actions generated by differentiable trajectory optimization. Backpropagation through this process updates the dynamics and cost functions to maximize task reward directly, rather than optimizing surrogate losses.
- Core assumption: Differentiable trajectory optimization can compute gradients of the loss with respect to policy parameters effectively.
- Evidence anchors:
  - [abstract]: "DiffTORI addresses the 'objective mismatch' issue of prior model-based RL algorithms, as the dynamics model in DiffTORI is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process."
  - [section]: "The key to DiffTORI is to change the non-differentiable MPPI planning algorithm in TD-MPC to a differentiable trajectory optimization, and include the policy gradient loss on the generated actions to optimize the model parameters θ directly for task performance."
  - [corpus]: Weak evidence - corpus neighbors do not directly address objective mismatch or differentiable trajectory optimization.
- Break condition: If the differentiable trajectory optimization fails to compute accurate gradients, or if the policy gradient loss becomes too noisy to provide useful signals.

### Mechanism 2
- Claim: Using trajectory optimization as a policy representation enables test-time optimization for imitation learning, outperforming explicit policies.
- Mechanism: Instead of generating actions through forward passes of a neural network, DiffTORI solves a trajectory optimization problem at test time using a learned cost function. This allows the policy to adapt actions based on the current state and learned objectives.
- Core assumption: The learned cost function captures the essential features needed to generate appropriate actions for the task.
- Evidence anchors:
  - [abstract]: "For imitation learning, it generates actions through test-time optimization with learned cost functions."
  - [section]: "In contrast to explicit policies that generate actions at test-time by forward passes of the policy network, DiffTORI generates the actions via test-time trajectory optimization with a learned cost function."
  - [corpus]: Weak evidence - corpus neighbors focus on differentiable optimization but don't specifically address test-time optimization for imitation learning.
- Break condition: If the learned cost function is insufficient to capture task requirements, or if test-time optimization becomes computationally prohibitive.

### Mechanism 3
- Claim: The CVAE architecture enables DiffTORI to capture multimodal action distributions in imitation learning.
- Mechanism: The CVAE encoder maps state-action pairs to a latent space, and the decoder uses trajectory optimization to generate actions conditioned on sampled latent vectors. This allows sampling different latent vectors to generate diverse, multimodal actions.
- Core assumption: The latent space learned by the CVAE can effectively represent the multimodal structure of expert actions.
- Evidence anchors:
  - [abstract]: "We use a Conditional Variational Auto-Encoder (CV AE) as the policy architecture, which has the ability to capture a multi-modal action distribution."
  - [section]: "The CV AE encoder encodes the state si and the expert action a* i into a latent state vector zi. The key idea in our approach is that the decoder in CV AE takes the form of a trajectory optimization algorithm..."
  - [corpus]: Weak evidence - corpus neighbors don't specifically address CVAE for multimodal action generation in trajectory optimization.
- Break condition: If the CVAE fails to learn a meaningful latent representation, or if the trajectory optimization cannot effectively utilize the sampled latent vectors.

## Foundational Learning

- Concept: Differentiable trajectory optimization
  - Why needed here: Enables backpropagation through the trajectory optimization process to learn cost and dynamics functions end-to-end.
  - Quick check question: Can you explain how gradients flow from the loss through the trajectory optimization to update the policy parameters?

- Concept: Policy gradient methods
  - Why needed here: DiffTORI uses policy gradient loss to update the model parameters by differentiating through the trajectory optimization.
  - Quick check question: What is the difference between the policy gradient loss used in DiffTORI and the surrogate losses used in traditional model-based RL?

- Concept: Conditional Variational Auto-Encoder (CVAE)
  - Why needed here: CVAE enables DiffTORI to capture multimodal action distributions by sampling from the latent space during test-time optimization.
  - Quick check question: How does the CVAE architecture in DiffTORI differ from a standard CVAE, and why is this modification necessary?

## Architecture Onboarding

- Component map: Encoder (hθ) -> Latent dynamics model (dθ) -> Cost/reward function (cθ, Rθ, fθ) -> Trajectory optimizer -> CVAE (for imitation)
- Critical path: 1. Encode observation to latent state, 2. Initialize actions (from policy or zero/random), 3. Solve trajectory optimization problem, 4. Compute loss (policy gradient or imitation), 5. Backpropagate through trajectory optimization, 6. Update model parameters
- Design tradeoffs:
  - Computational cost vs. sample efficiency: Differentiable trajectory optimization is slower than non-differentiable methods but can achieve better performance with fewer samples.
  - Planning horizon: Longer horizons may capture more complex dependencies but increase computational cost and potential for error accumulation.
  - Action initialization: Using a base policy for initialization can improve performance but adds complexity; zero/random initialization is simpler but may converge to suboptimal solutions.
- Failure signatures:
  - Poor performance despite long training: Check if trajectory optimization is solving the correct problem or if gradients are vanishing/exploding.
  - High variance in results: Investigate the stability of the differentiable trajectory optimization or the quality of the learned cost function.
  - Slow convergence: Profile the computational cost of the trajectory optimization and consider using a more efficient solver or reducing the planning horizon.
- First 3 experiments:
  1. Train DiffTORI on a simple control task (e.g., cartpole) with known dynamics to verify the basic mechanism works.
  2. Compare DiffTORI with and without the policy gradient loss on a model-based RL task to isolate the contribution of end-to-end optimization.
  3. Test DiffTORI with different action initialization strategies (base policy, zero, random) on an imitation learning task to understand the impact of initialization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DiffTORI's performance scale with increasing planning horizon H in imitation learning, and what is the optimal horizon value?
- Basis in paper: [explicit] The paper mentions ablation experiments on planning horizon H, noting that performance remains nearly the same when increasing from H=1 to H=3, but slightly declines when H=5.
- Why unresolved: The paper only tests three specific horizon values and doesn't explore the full range or provide a theoretical explanation for the observed behavior.
- What evidence would resolve it: Comprehensive ablation studies testing a wider range of horizon values (e.g., H=2, H=4, H=6) and analysis of computational trade-offs versus performance gains would clarify the optimal planning horizon.

### Open Question 2
- Question: What is the relationship between the learned cost function fθ in imitation learning and traditional inverse reinforcement learning (IRL) methods?
- Basis in paper: [inferred] The paper notes that the learned objective function f is not necessarily a "reward" function as those learned via IRL, and mentions this connection as a direction for future work.
- Why unresolved: The paper doesn't explore the theoretical connections or differences between the learned cost function and IRL reward functions, nor does it compare performance with IRL methods.
- What evidence would resolve it: Formal analysis of the relationship between fθ and IRL reward functions, plus experimental comparisons between DiffTORI and state-of-the-art IRL methods on the same tasks, would clarify this connection.

### Open Question 3
- Question: How does the computational efficiency of DiffTORI compare to model-free methods when considering wall-clock time to convergence across different task complexities?
- Basis in paper: [explicit] The paper acknowledges that DiffTORI requires more wall-clock time for training due to solving and differentiating through trajectory optimization, and provides some comparative results showing environment-dependent trade-offs.
- Why unresolved: The paper only provides wall-clock time comparisons for a limited set of environments and doesn't analyze how this trade-off varies with task complexity or provide systematic analysis of when DiffTORI becomes more efficient than model-free methods.
- What evidence would resolve it: Systematic experiments measuring wall-clock time to convergence across a spectrum of task complexities (simple to complex) and analysis of how computational costs scale with state/action dimensionality would clarify when DiffTORI is preferable to model-free approaches.

## Limitations
- Computational cost: Differentiable trajectory optimization is slower than non-differentiable methods, increasing wall-clock training time.
- Implementation complexity: Requires specialized differentiable optimization frameworks and careful gradient flow implementation.
- Test-time optimization: Computational overhead during inference due to solving trajectory optimization problems at test time.

## Confidence
- High confidence in the core differentiable optimization framework based on Theseus implementation
- Medium confidence in the CVAE architecture's ability to capture multimodal action distributions
- Low confidence in the exact network architectures and hyperparameters beyond high-level descriptions

## Next Checks
1. Implement a minimal working version on a simple control task (e.g., cartpole) to verify gradient flow through differentiable trajectory optimization
2. Conduct ablation studies comparing DiffTORI with different action initialization strategies to quantify the impact on performance
3. Measure the computational overhead of test-time optimization vs. traditional explicit policies on a representative task