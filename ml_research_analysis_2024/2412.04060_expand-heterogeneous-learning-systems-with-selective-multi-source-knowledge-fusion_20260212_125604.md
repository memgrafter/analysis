---
ver: rpa2
title: Expand Heterogeneous Learning Systems with Selective Multi-Source Knowledge
  Fusion
arxiv_id: '2412.04060'
source_url: https://arxiv.org/abs/2412.04060
tags:
- knowledge
- target
- source
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses learning system expansion for new domains with
  limited labeled data and heterogeneous data/device constraints. It proposes HaT,
  a framework that selects high-quality source models, fuses their knowledge with
  sample-wise weights via an attention-based mixer, and adaptively injects this knowledge
  into target models.
---

# Expand Heterogeneous Learning Systems with Selective Multi-Source Knowledge Fusion

## Quick Facts
- arXiv ID: 2412.04060
- Source URL: https://arxiv.org/abs/2412.04060
- Reference count: 39
- One-line primary result: HaT outperforms state-of-the-art baselines by up to 16.5% accuracy and reduces communication traffic by up to 39%

## Executive Summary
This paper addresses the challenge of expanding heterogeneous learning systems to new domains with limited labeled data and diverse data/device constraints. The proposed HaT framework selects high-quality source models, fuses their knowledge with sample-wise attention weights, and adaptively injects this knowledge into target models. The method reduces communication overhead by transmitting only selected models and uses low-cost joint training. Experiments across four datasets spanning six modalities show HaT achieves up to 16.5% accuracy improvement over baselines while reducing communication traffic by up to 39%.

## Method Summary
HaT operates through a multi-stage process: first, an efficient model selection protocol uses coarse feature-based filtering followed by fine-grained centroid-accuracy joint selection to identify high-quality source models while minimizing communication. Second, an attention-based mixer fuses predictions from selected source models using sample-wise weights computed from feature similarity between target and source representations. Third, adaptive knowledge injection selectively stores high-quality fused predictions in a knowledge dictionary and scales the distillation loss weight based on mixer accuracy. Finally, low-cost joint training simultaneously updates the mixer and target model using separate optimizers, with selective storage ensuring stable learning objectives.

## Key Results
- Achieves up to 16.5% accuracy improvement over state-of-the-art baselines
- Reduces communication traffic by up to 39% compared to transmitting all source models
- Demonstrates effectiveness across four datasets spanning six modalities and three tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sample-wise weighted knowledge fusion resolves model conflicts better than static fusion.
- Mechanism: An attention-based mixer assigns per-sample weights to each source model's prediction based on feature similarity, producing a fused prediction that adapts to sample-specific reliability.
- Core assumption: The similarity between target and source model features is a valid proxy for prediction quality.
- Evidence anchors:
  - [abstract]: "fuses their knowledge by assigning sample-wise weights to their predictions"
  - [section]: "an attention-based mixer is trained to assign sample-wise weights to each model's predictions by measuring the representations similarity between models"
  - [corpus]: No direct evidence; no corpus neighbor directly describes sample-wise weighting.
- Break condition: If feature similarity does not correlate with accuracy, the attention weights will misalign knowledge fusion.

### Mechanism 2
- Claim: Adaptive knowledge injection improves target model training by selectively learning from high-quality fused predictions.
- Mechanism: Fused predictions are stored in a knowledge dictionary only when the mixer's accuracy improves; the target model's distillation loss weight is scaled by the mixer's training accuracy.
- Core assumption: Knowledge quality fluctuates across training epochs, and filtering unstable predictions improves convergence.
- Evidence anchors:
  - [abstract]: "the fused knowledge is selectively injected into the customized models based on the knowledge quality"
  - [section]: "The knowledge dictionary is introduced to provide more stable learning objectives. After each model update, the fused predictions...are stored in the KD only if the accuracy of the mixer improves."
  - [corpus]: No corpus neighbor directly addresses adaptive distillation or selective storage.
- Break condition: If the mixer's accuracy is not a reliable indicator of fused prediction quality, the dictionary will store poor samples.

### Mechanism 3
- Claim: Coarse-then-fine source model selection reduces communication cost without sacrificing quality.
- Mechanism: First, statistical features filter source domains; then centroids and accuracy jointly rank the remaining models, limiting full model transfers to the most promising candidates.
- Core assumption: Coarse feature similarity is a strong early filter, and centroid similarity with accuracy is a good fine-grained selector.
- Evidence anchors:
  - [abstract]: "It first selects multiple high-quality models from the system at a low cost"
  - [section]: "Lightweight features...are extracted...These features provide a coarse-grained description...centroids are computed...centroid similarity scores...is then multiplied by the accuracy Acci to rank the source domains"
  - [corpus]: No corpus neighbor directly addresses multi-stage selection; corpus shows diverse data fusion approaches but not model selection pipelines.
- Break condition: If either selection stage is inaccurate, high-quality models may be excluded, degrading performance.

## Foundational Learning

- Concept: Domain adaptation and data heterogeneity
  - Why needed here: The paper addresses non-IID data between source and target domains; understanding why source models may fail on target data is essential.
  - Quick check question: What is the difference between in-distribution and out-of-distribution data, and why does this matter for model reuse?

- Concept: Knowledge distillation and teacher-student training
  - Why needed here: HaT relies on distilling knowledge from multiple source models into a target model; grasping the core idea of transferring soft predictions is critical.
  - Quick check question: In knowledge distillation, why might a teacher's soft label be more informative than its hard label?

- Concept: Attention mechanisms and sample weighting
  - Why needed here: The mixer uses attention to compute sample-wise weights; familiarity with query-key-value attention is needed to follow the design.
  - Quick check question: How does self-attention differ from cross-attention, and when would you use each?

## Architecture Onboarding

- Component map:
  Efficient Model Selection Protocol -> Attention-based Mixer -> Adaptive Knowledge Injection -> Low-cost Joint Training

- Critical path:
  1. Feature extraction and coarse filtering on target domain
  2. Centroid computation and fine ranking on selected models
  3. Joint training loop: encoder forward -> mixer update -> target update -> KD update
  4. Inference with frozen target encoder and classifier

- Design tradeoffs:
  - Coarse selection reduces communication but risks excluding good models if features are too coarse.
  - Joint training saves cost but complicates gradient management; separate optimizers are used.
  - Selective KD storage reduces noise but may delay learning if good samples are discarded.

- Failure signatures:
  - Poor accuracy despite many source models: likely coarse selection over-filtered or attention weights are off.
  - High training instability: likely KD is storing too many noisy samples or b threshold is too low.
  - Excessive communication: likely Î· is too high, preventing coarse filtering.

- First 3 experiments:
  1. Verify coarse selection reduces model count while retaining accuracy on a small domain set.
  2. Test attention-based mixer weights vs. equal weighting on a fixed source-target pair.
  3. Compare KD-augmented training vs. vanilla distillation on a small labeled dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework perform when the number of source models increases beyond what was tested in the experiments?
- Basis in paper: [inferred] The paper mentions varying the number of selected models (Np) in the experiments but does not explore scenarios with a very large number of source models.
- Why unresolved: The paper does not provide results for scenarios with a significantly larger number of source models, which could reveal potential scalability issues or performance plateaus.
- What evidence would resolve it: Experiments with a larger number of source models to determine if the framework's performance continues to improve or if it plateaus or degrades.

### Open Question 2
- Question: How does the framework handle scenarios where the source and target domains have significantly different label spaces?
- Basis in paper: [explicit] The paper mentions that the label sets of source and target domains may not fully overlap, introducing additional complexity during expansion.
- Why unresolved: The paper does not provide specific results or analysis on how the framework performs when the label spaces differ significantly between source and target domains.
- What evidence would resolve it: Experiments with source and target domains having significantly different label spaces to evaluate the framework's robustness and effectiveness in such scenarios.

### Open Question 3
- Question: What is the impact of using different similarity functions in the feature-based coarse selection stage?
- Basis in paper: [inferred] The paper uses cosine similarity in the feature-based coarse selection stage but does not explore the impact of using other similarity functions.
- Why unresolved: The paper does not compare the performance of different similarity functions, which could affect the quality of the selected source models.
- What evidence would resolve it: Experiments comparing the performance of different similarity functions (e.g., Euclidean distance, Jaccard similarity) in the feature-based coarse selection stage to determine the optimal choice.

### Open Question 4
- Question: How does the framework perform when the target domain has a very limited amount of labeled data?
- Basis in paper: [explicit] The paper mentions that the target domains have limited labeled data due to high labeling costs.
- Why unresolved: The paper does not provide results for scenarios with extremely limited labeled data, which could reveal the framework's limitations in such cases.
- What evidence would resolve it: Experiments with target domains having very limited labeled data (e.g., less than 5%) to evaluate the framework's performance and identify potential challenges.

## Limitations
- The framework relies heavily on feature similarity as a proxy for prediction quality, which may not hold across all domain pairs
- Key hyperparameters are not fully specified, making faithful reproduction challenging
- Model architecture specifications are incomplete, requiring assumptions for implementation
- Joint training approach may introduce gradient management complexity not fully addressed

## Confidence
- **High confidence**: Experimental results showing 16.5% accuracy improvement and 39% communication reduction on the four tested datasets
- **Medium confidence**: The theoretical framework of sample-wise attention weighting and adaptive knowledge injection
- **Low confidence**: Generalization claims beyond the four specific datasets and six modalities tested

## Next Checks
1. **Feature similarity validation**: Measure correlation between feature similarity scores and actual prediction accuracy across diverse domain pairs to verify the attention weighting assumption.

2. **Selection protocol sensitivity**: Test how varying the coarse selection threshold affects final performance to quantify the risk of excluding high-quality models.

3. **Mixer accuracy reliability**: Compare the mixer's accuracy metric against direct evaluation of fused prediction quality to validate the knowledge dictionary filtering mechanism.