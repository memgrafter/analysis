---
ver: rpa2
title: Dataset Growth
arxiv_id: '2405.18347'
source_url: https://arxiv.org/abs/2405.18347
tags:
- data
- dataset
- gain
- infogrowth
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoGrowth is an online algorithm that improves data quality for
  training by cleaning and selecting informative samples in streaming data. It uses
  a multimodal encoder to embed samples, identifies noisy and redundant samples via
  neighborhood analysis, relabels noisy samples, and retains only informative ones
  based on a submodular-like gain function.
---

# Dataset Growth

## Quick Facts
- arXiv ID: 2405.18347
- Source URL: https://arxiv.org/abs/2405.18347
- Reference count: 31
- Primary result: InfoGrowth achieves comparable or better retrieval performance using only 0.4M samples versus 2.7M in the original CC3M dataset

## Executive Summary
InfoGrowth is an online algorithm designed to improve data quality for training by cleaning and selecting informative samples from streaming data. It leverages a multimodal encoder to embed samples and uses neighborhood analysis to identify noisy and redundant samples, which are then relabeled and filtered based on a submodular-like gain function. The method is applicable to both single-modal and multi-modal tasks and scales efficiently to large datasets. InfoGrowth demonstrated significant improvements in data efficiency and training cost reduction while maintaining or enhancing performance on various downstream tasks.

## Method Summary
InfoGrowth processes streaming data by first encoding samples using a multimodal backbone like BLIP. It then identifies noisy and redundant samples through neighborhood analysis in the embedding space, using HNSW for efficient nearest neighbor search. Noisy samples are relabeled using a cleaner module that leverages cross-modal alignment and recaptioning. The algorithm calculates a submodular-like gain for each sample based on its neighborhood, retaining only those with high information value. This process supports both static and dynamic sampling strategies, allowing for efficient data selection and improved downstream task performance.

## Key Results
- Achieved comparable or better retrieval performance on CC3M using only 0.4M samples versus 2.7M in the original dataset
- Reduced training cost by up to 55% while improving classification accuracy on ImageNet-1K
- Outperformed static and dynamic pruning baselines on multiple tasks including classification, VQA, and image captioning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noisy and redundant samples are identified via neighborhood analysis in the embedding space.
- Mechanism: Each new sample is embedded using a multimodal backbone (BLIP), then its nearest k neighbors in the current dataset are found using HNSW. The cosine distances to these neighbors are averaged to estimate the sample's "information gain". Noisy samples are detected by low cross-modal alignment (cosine similarity below threshold δ), and redundant samples have small gain (near neighbors are similar).
- Core assumption: In the embedding space, noisy samples are out-of-distribution and redundant samples have near neighbors with similar conditions; thus, neighborhood statistics reveal both problems.
- Evidence anchors:
  - [abstract] "identifies noisy and redundant samples via neighborhood analysis"
  - [section 3.3] "we estimate its noisy degree and redundancy according to its neighborhood in the already collected dataset"
  - [section 3.5] "we exploit an online approximate nearest neighbor search architecture HNSW [19] to query the nearest k neighbors of a given data point"
- Break condition: If the embedding space does not separate classes well (poor backbone), neighborhood analysis will misidentify samples.

### Mechanism 2
- Claim: The gain function provides a submodular-like property for efficient online selection.
- Mechanism: Gain is defined as the mean cosine distance to k nearest neighbors, which satisfies submodularity (U ⊆ V ⇒ f(x,U) ≥ f(x,V)) while being computed in O(log n) time via HNSW, instead of O(n) for full set operations.
- Core assumption: Submodular gain estimation can be approximated efficiently by local neighborhood queries without sacrificing the diminishing returns property.
- Evidence anchors:
  - [abstract] "retains only informative ones based on a submodular-like gain function"
  - [section 3.5] "This design provides a submodular-like property (U ⊆ V ⇒ f(x, U) ≥ f(x, V )) with higher efficiency (O(logn) compared to set operation O(n))"
  - [corpus] Weak: No direct citation of submodular theory; relies on author's claim.
- Break condition: If the local neighborhood is not representative of global structure, the gain estimate will be inaccurate.

### Mechanism 3
- Claim: Cleaner module improves label quality by recaptioning and filtering low-alignment samples.
- Mechanism: A BLIP model encodes image and text; samples with multimodal cosine similarity below δ are considered misaligned and are recaptioned by MiniGPT-4. For single-modal tasks, a classifier predicts P(c|d) and relabels low-confidence samples.
- Core assumption: Cross-modal alignment is a reliable proxy for label quality in multimodal data, and large language models can produce better captions than noisy web labels.
- Evidence anchors:
  - [abstract] "identifies noisy and redundant samples via neighborhood analysis, relabels noisy samples"
  - [section 3.4] "we use the cosine similarity of two modalities’ encoded features to filter samples whose multimodal cosine similarity is smaller than δ"
  - [section 3.4] "the cleaning process mainly uses filter and recaption to improve the data alignment and reduce noise"
- Break condition: If the cleaner model itself is noisy or the recaptioning introduces new errors, quality may degrade.

## Foundational Learning

- Concept: Submodular functions and diminishing returns
  - Why needed here: To justify the gain function's design and its efficiency properties.
  - Quick check question: Does the mean cosine distance to k nearest neighbors satisfy the submodular property U ⊆ V ⇒ f(x,U) ≥ f(x,V)?

- Concept: Approximate nearest neighbor search (HNSW)
  - Why needed here: To enable efficient neighborhood queries at scale (O(log n) vs O(n)).
  - Quick check question: How does HNSW balance search speed and accuracy compared to brute-force search?

- Concept: Cross-modal embedding alignment
  - Why needed here: To detect and correct noisy labels in multimodal data.
  - Quick check question: Why is cosine similarity between image and text embeddings a good proxy for label correctness?

## Architecture Onboarding

- Component map: Encoder (BLIP backbone) -> Neighborhood search (HNSW) -> Cleaner (BLIP + MiniGPT-4) -> Gain calculator (mean cosine distance) -> Sampler (static or dynamic) -> Dataset store
- Critical path: Encoder -> HNSW query -> Gain calculation -> Sampler decision (accept/reject/relabel)
- Design tradeoffs:
  - Static sampling: Simpler, better diversity control, but fixed cost.
  - Dynamic sampling: Lower training cost, but two-phase logic adds complexity.
  - Gain function: Local approximation trades exactness for speed.
- Failure signatures:
  - High false positive rate in noisy detection -> low δ or poor cleaner.
  - Slow growth of dataset -> gain function not discriminating.
  - HNSW memory blowup -> need distributed ANN or lower embedding dimension.
- First 3 experiments:
  1. Verify neighborhood statistics: embed a small dataset, query k=4 neighbors, plot distribution of mean cosine distances.
  2. Test cleaner accuracy: run BLIP encoding and cosine filtering on a held-out noisy subset, measure precision/recall of alignment detection.
  3. Evaluate gain submodularity: for a sample, compute gain with increasing neighbor set sizes and check diminishing returns property.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the information gain calculated by InfoGrowth correlate with actual downstream task performance improvements, and is there a theoretical model that can predict this relationship?
- Basis in paper: [inferred] The paper mentions that InfoGrowth uses a submodular-like gain function based on neighborhood analysis in the embedding space to estimate the information gain of samples, and that this gain is used for sampling. However, the direct correlation between this gain and actual task performance is not explicitly established.
- Why unresolved: While the paper shows that InfoGrowth improves performance on downstream tasks, it does not provide a theoretical framework or empirical evidence directly linking the calculated information gain to the magnitude of performance improvement. Establishing such a relationship would be crucial for optimizing the gain calculation and predicting the impact of adding specific samples.
- What evidence would resolve it: A detailed analysis correlating the calculated information gain values of individual samples or batches with the subsequent changes in validation metrics on downstream tasks would be needed. This could involve regression analysis or developing a predictive model that estimates performance improvement based on the gain distribution of added data.

### Open Question 2
- Question: What is the optimal neighborhood size (k) for the near-neighbor search in InfoGrowth, and how does it depend on the dataset size and distribution in the embedding space?
- Basis in paper: [explicit] The paper includes an ablation study (Table 5) that shows the influence of different k choices on retrieval performance, indicating that k=4 with arithmetic mean performs best in their experiments. However, it does not provide a general guideline or theoretical justification for choosing k.
- Why unresolved: The optimal k value is likely to depend on factors such as the density of the dataset in the embedding space, the dimensionality of the embeddings, and the specific task. A small k might be too sensitive to noise, while a large k might smooth out important distinctions. Determining the optimal k for different scenarios requires further investigation.
- What evidence would resolve it: Extensive experiments varying k across different dataset sizes, embedding dimensions, and task types, along with a theoretical analysis of the trade-offs involved in choosing k, would help establish guidelines for selecting the optimal neighborhood size.

### Open Question 3
- Question: How does the choice of backbone model (e.g., BLIP vs. CLIP) affect the effectiveness of InfoGrowth in cleaning and selecting informative samples?
- Basis in paper: [inferred] The paper states that InfoGrowth works for both single-modal and multi-modal tasks across different model architectures, and provides results using BLIP and ResNet-50. However, it does not directly compare the performance of InfoGrowth when using different backbone models for the same dataset and task.
- Why unresolved: The effectiveness of InfoGrowth relies on the quality of the embedding space provided by the backbone model. Different models might encode information differently, leading to variations in the neighborhood structure and, consequently, the calculated information gain. Understanding how the choice of backbone model influences InfoGrowth's performance is crucial for its practical application.
- What evidence would resolve it: Experiments comparing the performance of InfoGrowth using different backbone models (e.g., BLIP, CLIP, other multimodal models) on the same dataset and task, along with an analysis of the resulting embedding spaces and gain distributions, would clarify the impact of the backbone model choice.

## Limitations

- The cleaner module's effectiveness depends on the quality of the cross-modal alignment proxy, which could fail if the backbone model is noisy or if the data distribution shifts significantly.
- The evaluation focuses primarily on CC3M and ImageNet-1K, leaving generalization to other domains uncertain.
- The static sampling strategy, while simpler, may lead to suboptimal diversity control compared to dynamic sampling, especially in highly imbalanced datasets.

## Confidence

- **High Confidence**: The overall effectiveness of InfoGrowth in reducing dataset size while maintaining or improving performance on CC3M and ImageNet-1K. The use of HNSW for efficient neighborhood queries is well-established.
- **Medium Confidence**: The submodular-like gain function's efficiency and effectiveness in online selection, as this relies on the neighborhood approximation and the specific design of the gain metric.
- **Low Confidence**: The generalizability of the cleaner module's cross-modal alignment proxy to other datasets or domains, and the dynamic sampling strategy's two-phase logic, which adds complexity and may introduce new failure modes.

## Next Checks

1. **Validate Gain Function Submodularity**: For a sample, compute the gain with increasing neighbor set sizes (k=1,2,4,8) and verify the diminishing returns property (f(x,U) ≥ f(x,V) for U ⊆ V). This will confirm if the mean cosine distance is a valid submodular-like gain metric.

2. **Test Cleaner Robustness**: Run the cleaner on a held-out noisy subset of CC3M or another multimodal dataset, measuring precision/recall of alignment detection and the quality of recaptioned samples. This will assess the reliability of the cross-modal alignment proxy.

3. **Evaluate Dynamic Sampling Efficiency**: Compare the training cost and performance of static vs. dynamic sampling strategies on a large-scale dataset (e.g., LAION-400M). Measure the reduction in training cost and the impact on downstream task performance to validate the claimed 55% reduction.