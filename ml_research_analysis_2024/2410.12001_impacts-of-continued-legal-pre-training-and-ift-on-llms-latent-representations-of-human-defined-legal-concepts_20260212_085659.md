---
ver: rpa2
title: Impacts of Continued Legal Pre-Training and IFT on LLMs' Latent Representations
  of Human-Defined Legal Concepts
arxiv_id: '2410.12001'
source_url: https://arxiv.org/abs/2410.12001
tags:
- legal
- attention
- concepts
- training
- human-defined
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares the attention patterns of three models\u2014\
  Mistral 7B, SaulLM-7B-Base (continued pre-training on legal corpora), and SaulLM-7B-Instruct\
  \ (with additional instruction fine-tuning)\u2014to understand how legal training\
  \ affects their utilization of human-defined legal concepts. The study analyzed\
  \ 7 text sequences from recent AI & Law literature, each containing a legal concept,\
  \ by measuring the proportion of attention allocated to tokens representing these\
  \ concepts and visualizing attention score alterations."
---

# Impacts of Continued Legal Pre-Training and IFT on LLMs' Latent Representations of Human-Defined Legal Concepts

## Quick Facts
- arXiv ID: 2410.12001
- Source URL: https://arxiv.org/abs/2410.12001
- Authors: Shaun Ho
- Reference count: 0
- Primary result: Legal training stabilizes model attention but does not align attention patterns with human-defined legal concepts

## Executive Summary
This paper investigates how continued pre-training on legal corpora and instruction fine-tuning (IFT) affect large language models' (LLMs) attention to human-defined legal concepts. Using three model variants (Mistral 7B, SaulLM-7B-Base, and SaulLM-7B-Instruct), the study analyzes attention patterns across 32 layers when processing text sequences containing legal concepts. The research reveals that while legal training creates more stable attention distributions, it does not introduce novel attention structures corresponding to legal concepts, instead modifying existing patterns in ways that sometimes degrade legal feature representation.

## Method Summary
The study compares attention patterns of three models: Mistral 7B (base), SaulLM-7B-Base (continued pre-training on legal corpora), and SaulLM-7B-Instruct (additional instruction fine-tuning on legal tasks). Researchers measured the proportion of attention allocated to tokens representing legal concepts across all layers and heads. They visualized attention score alterations between models and analyzed distribution metrics including skewness, kurtosis, and entropy. The analysis focused on 7 text sequences from recent AI & Law literature, each containing a specific legal concept.

## Key Results
- Legal training stabilizes model attention but does not align attention patterns with human-defined legal concepts
- Legal training unevenly impacts attention to different legal concepts, often reducing attention in later layers while slightly increasing it in earlier layers
- IFT attenuates instabilities introduced during continued pre-training and distributes effects more uniformly across attention heads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal training stabilizes model attention but does not align attention patterns with human-defined legal concepts
- Mechanism: Legal training modifies existing attention patterns rather than introducing novel structures that correspond to human-defined legal concepts. The continued pre-training on legal corpora and additional IFT modulate the model's attention allocation across layers and heads, creating more uniform distribution but not aligning with legal concept boundaries
- Core assumption: Attention patterns learned during legal training can be meaningfully analyzed to understand how models utilize legal concepts
- Evidence anchors:
  - [abstract] "legal training stabilizes model attention but does not align model attention patterns with human-defined legal concepts"
  - [section] "Visualizations revealed that the patterns of alterations learned during legal training targeted pre-existing attention structures in Mistral 7B, rather than introducing new structures corresponding to human-defined legal concepts"
- Break condition: If attention visualizations show novel structures corresponding to legal concepts emerge after legal training, or if model performance on legal tasks improves significantly despite lack of aligned attention patterns

### Mechanism 2
- Claim: Legal training unevenly impacts attention to different legal concepts, often reducing attention in later layers while slightly increasing it in earlier layers
- Mechanism: The legal training process affects attention distribution across layers differently - earlier layers show increased attention to legal concepts while later layers show decreased attention, possibly due to the hierarchical nature of feature processing in transformer models
- Core assumption: The hierarchical processing of features in transformer layers (higher-level concepts in early layers, lower-level in later layers) is preserved and modified by legal training
- Evidence anchors:
  - [abstract] "legal training unevenly impacts attention to different legal concepts"
  - [section] "Legal training therefore (1) improved Mistral 7B's utilization of human-defined legal concepts when developing broader, longer-ranged, and higher-level contextual representations of input sequences, but (2) often diminished its reliance on legal concepts when developing narrower, recent-ranged, and lower-level representations"
- Break condition: If attention patterns across layers show uniform increase or decrease in legal concept attention after training, or if hierarchical processing is disrupted entirely

### Mechanism 3
- Claim: IFT attenuates instabilities introduced during continued pre-training and distributes effects more uniformly across attention heads
- Mechanism: Instruction fine-tuning modulates the extreme attention shifts caused by continued pre-training, reducing kurtosis (fat tails) and increasing entropy (more even spread) of attention distribution, creating more stable model behavior
- Core assumption: IFT can correct instabilities introduced during pre-training by redistributing attention effects across more heads
- Evidence anchors:
  - [section] "Attention shifts brought about by continued pre-training of Mistral 7B demonstrated moderate to strong negative skewness in most instances... Additional IFT modulated this behavior, resulting in skewness values that were consistently closer to zero or even positive"
  - [section] "The distribution of attention shifts toward legal concepts exhibited lower kurtosis after IFT, further suggesting that IFT modulated instabilities introduced during continued pre-training. Entropy was slightly higher following IFT"
- Break condition: If IFT shows no significant effect on attention distribution metrics, or if it introduces new instabilities rather than correcting existing ones

## Foundational Learning

- Concept: Attention mechanism in transformer models
  - Why needed here: The entire analysis is based on how models allocate attention to different tokens representing legal concepts
  - Quick check question: What is the relationship between query vectors and key/value matrices in computing attention scores?

- Concept: Continued pre-training vs fine-tuning
  - Why needed here: The paper compares three models with different training regimes (general, pre-trained on legal, pre-trained + IFT on legal)
  - Quick check question: How does continued pre-training differ from instruction fine-tuning in terms of objectives and typical datasets?

- Concept: Tokenization and concept boundaries
  - Why needed here: Legal concepts are often represented by contiguous token sequences, but attention may only target subsets of these tokens
  - Quick check question: How might tokenization strategies affect a model's ability to recognize and attend to multi-token legal concepts?

## Architecture Onboarding

- Component map: Mistral 7B base model -> Continued legal pre-training -> SaulLM-7B-Base -> Instruction fine-tuning -> SaulLM-7B-Instruct
- Critical path: Compute attention scores → Filter to legal concept tokens → Calculate proportion of attention → Compare across models and layers → Visualize attention matrices → Analyze patterns and shifts
- Design tradeoffs: Using attention patterns as proxy for concept utilization vs direct performance metrics; analyzing raw attention scores vs normalized attention; focusing on specific legal concepts vs broader legal knowledge
- Failure signatures: If attention patterns show no consistent differences between models, if visualizations are uninformative, if legal concept tokens are not reliably identified, or if layer-wise patterns contradict established hierarchical processing theories
- First 3 experiments:
  1. Replicate the attention proportion analysis on a different set of legal concepts to test generalizability of findings
  2. Compare attention patterns on legal vs non-legal sequences within the same model to isolate legal-specific effects
  3. Visualize attention matrices for individual legal concept tokens to understand which tokens within multi-token concepts receive attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of base model architecture influence the effectiveness of legal pre-training and instruction fine-tuning across different legal domains and jurisdictions?
- Basis in paper: [explicit] "Further work should explore the extent to which the choice of base model (e.g. Mistral 7B) influences the outcomes of continued legal pre-training and IFT."
- Why unresolved: The paper only compares models based on the same Mistral 7B architecture, leaving open whether architectural differences would lead to different patterns of legal concept attention.
- What evidence would resolve it: Comparative studies using different base architectures (e.g., GPT-4, Claude, Llama) with identical legal training protocols and evaluation methods.

### Open Question 2
- Question: Would custom tokenization strategies that highlight legal keywords improve the alignment between LLM attention patterns and human-defined legal concepts?
- Basis in paper: [explicit] "This may call for new tokenization strategies, such as that of Nguyen et al. [31] which improved legal task performance by using custom tokens to highlight legal keywords in input sequences."
- Why unresolved: The paper demonstrates misalignment between learned attention patterns and human-defined legal concepts but does not test alternative tokenization approaches.
- What evidence would resolve it: Experiments comparing standard tokenization with keyword-highlighting tokenization on the same legal concept attention analysis tasks.

### Open Question 3
- Question: To what extent does legal pre-training introduce superficial improvements in performance at the cost of model stability, and how does this vary across different legal tasks?
- Basis in paper: [explicit] "The wider literature has also called into question whether fine-tuning introduces superficial improvements at the cost of model stability [11,12]."
- Why unresolved: While the paper notes attention pattern changes, it does not assess whether these changes translate to genuine capability improvements or merely performance on benchmark tasks.
- What evidence would resolve it: Longitudinal studies comparing model performance and stability across diverse real-world legal applications before and after legal training.

## Limitations

- The findings are based on a small set of 7 text sequences from AI & Law literature, which may not represent the full diversity of legal language and concepts
- The study focuses exclusively on attention patterns as a proxy for concept utilization, without examining downstream task performance or other behavioral indicators
- The paper does not address potential confounding factors such as the specific legal corpora used for continued pre-training or the exact instruction fine-tuning methodology

## Confidence

- Legal training stabilizes attention without aligning to concepts: High confidence
- Legal training unevenly affects attention across layers: Medium confidence
- IFT stabilizes attention distributions: Medium confidence

## Next Checks

1. Replicate the attention proportion analysis using a larger and more diverse set of legal concepts from different domains (criminal, civil, constitutional law) to test generalizability
2. Compare model performance on legal reasoning tasks with attention pattern analysis to determine if misaligned attention correlates with task performance
3. Conduct ablation studies removing legal training from the pre-trained models to isolate which attention patterns are truly learned versus inherited from base architecture