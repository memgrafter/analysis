---
ver: rpa2
title: Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation
  Models
arxiv_id: '2406.10507'
source_url: https://arxiv.org/abs/2406.10507
tags:
- speech
- finetuning
- data
- child
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks child automatic speech recognition (ASR)
  using various speech foundation models (SFMs) like Whisper, Wav2vec2.0, HuBERT,
  and WavLM. It compares supervised and self-supervised SFMs, finding that supervised
  models like Canary and Parakeet outperform Whisper despite less training data, suggesting
  data quality is crucial.
---

# Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models

## Quick Facts
- **arXiv ID**: 2406.10507
- **Source URL**: https://arxiv.org/abs/2406.10507
- **Reference count**: 0
- **One-line primary result**: Supervised SFMs like Canary and Parakeet outperform Whisper on child speech despite less training data, suggesting data quality is more important than quantity.

## Executive Summary
This paper benchmarks child automatic speech recognition (ASR) using various speech foundation models (SFMs) including Whisper, Wav2vec2.0, HuBERT, and WavLM. The study compares supervised and self-supervised SFMs, finding that supervised models like Canary and Parakeet outperform Whisper despite less training data, suggesting data quality is crucial. The research investigates data augmentation and parameter-efficient finetuning (PEFT) methods, proposing perturbation invariant finetuning (PIF) to stabilize finetuning with augmented data. Results show that adapter tuning matches full finetuning for large models but not small ones, and that model size impacts PEFT performance.

## Method Summary
The study benchmarks child ASR using two datasets: My Science Tutor (MyST) spontaneous speech corpus (133 hours after filtering) and CSLU OGI scripted read speech corpus (50 hours). Various SFMs are finetuned on these datasets using standard CTC loss for self-supervised models and Whisper's objective for supervised models. Data augmentation methods include pitch perturbation, speed perturbation, vocal tract length perturbation, and SpecAugment. Parameter-efficient finetuning techniques such as LoRA, adapter tuning, prompt tuning, and prefix tuning are compared to full finetuning. A perturbation invariant finetuning (PIF) loss is proposed as regularization to stabilize training with augmented data.

## Key Results
- Supervised SFMs (Canary, Parakeet) outperform Whisper on child speech despite using less training data, indicating data quality is more important than quantity.
- Adapter tuning matches full finetuning performance for large models but performs worse for small models.
- The proposed PIF technique stabilizes finetuning with augmented data by adding a distance loss between original and perturbed encoder outputs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised SFMs like Canary and Parakeet outperform Whisper on child speech despite less training data, suggesting data quality is more important than data quantity.
- Mechanism: The training data for Canary and Parakeet is likely cleaner, more curated, and better matched to child speech characteristics, enabling better generalization even with fewer hours.
- Core assumption: Weakly-supervised data in Whisper introduces noise that degrades performance on the specific domain of child speech.
- Evidence anchors:
  - [abstract] "the Canary and Parakeet models are better than Whisper models on child speech with much less training data, indicating the data quality is sometimes more important than the data quantity."
  - [section] "the Whisper models are trained with more data than Canary and Parakeet... we conclude that data quality is sometimes more important than the size of data for obtaining a robust supervised speech foundation model"
- Break condition: If the Whisper training data included a significant proportion of child speech, the quality advantage might be negated.

### Mechanism 2
- Claim: Adapter tuning matches full finetuning performance for large models but performs worse for small models in child ASR.
- Mechanism: Large models have sufficient capacity that lightweight adapters can effectively learn domain-specific features without overfitting, while small models lack the representational headroom to compensate for the frozen base parameters.
- Core assumption: The representational capacity of large models allows adapters to capture task-specific information without needing to modify all parameters.
- Evidence anchors:
  - [section] "We observe that the behaviors of these methods are different when the model size increases. For example, PEFT matches the performance of full finetuning for large models but worse for small models."
  - [section] "the adapter tuning does not work as well as the full finetuning for small models. However, when the model size increases, the gap between adapter tuning and full finetuning decreases."
- Break condition: If the child speech domain requires significant architectural changes, even large models might need full finetuning.

### Mechanism 3
- Claim: Perturbation Invariant Finetuning (PIF) stabilizes finetuning with augmented data by adding a distance loss between original and perturbed encoder outputs.
- Mechanism: The regularization term prevents the model from overfitting to augmentation artifacts while still learning robust features, addressing instability in finetuning with pitch and vocal tract length perturbations.
- Core assumption: Augmentation methods like pitch and VTLP introduce variability that can destabilize training if not properly regularized.
- Evidence anchors:
  - [section] "To stabilize finetuning using augmented data, we propose a perturbation invariant finetuning (PIF) loss as a regularization."
  - [section] "We propose a perturbation invariant finetuning (PIF) technique to stabilize the VTLP and PP. Specifically, an additional distance loss between the encoder outputs of original and perturbed utterance is added as a regularization for finetuning."
- Break condition: If the base model is already highly robust to the specific perturbations used, the PIF regularization might not provide additional benefit.

## Foundational Learning

- Concept: Self-supervised vs supervised learning paradigms in speech models
  - Why needed here: Understanding why supervised models like Whisper, Canary, and Parakeet behave differently from self-supervised models like Wav2vec2.0 and HuBERT is crucial for selecting appropriate model families for child ASR.
  - Quick check question: What is the fundamental difference in how supervised and self-supervised speech foundation models are trained?

- Concept: Parameter-efficient finetuning (PEFT) techniques
  - Why needed here: The paper extensively compares adapter tuning, LoRA, prompt tuning, and prefix tuning to understand when lightweight finetuning strategies are appropriate for child ASR.
  - Quick check question: How does adapter tuning differ from LoRA in terms of where and how parameters are modified?

- Concept: Data augmentation for low-resource speech recognition
  - Why needed here: Child ASR suffers from data scarcity, making data augmentation critical; understanding which methods work best with foundation models is essential.
  - Quick check question: Why might data augmentation methods that work well for training from scratch behave differently when used with pretrained foundation models?

## Architecture Onboarding

- Component map: Speech foundation models (supervised like Whisper/Canary/Parakeet or self-supervised like Wav2vec2.0/HuBERT/WavLM) -> Data augmentation pipelines (pitch perturbation, speed perturbation, VTLP, SpecAugment) -> PEFT modules (adapters, LoRA, prompts, prefixes) -> Finetuning framework with CTC or encoder-decoder loss -> Output transcriptions
- Critical path: Input speech → Feature extraction by SFM → Augmentation (if enabled) → PEFT adaptation (if using PEFT) → Finetuning loss computation → Parameter updates → Output transcriptions
- Design tradeoffs: Full finetuning provides best performance but requires high memory; PEFT reduces memory but may underperform on small models; supervised SFMs need less finetuning but may have domain mismatch; self-supervised SFMs are more flexible but require careful finetuning.
- Failure signatures: PEFT underperforming on small models (insufficient capacity); data augmentation instability (requires PIF); supervised models failing on child speech (domain mismatch); self-supervised models requiring longer finetuning.
- First 3 experiments:
  1. Compare zero-shot performance of Whisper-tiny through Whisper-largeV3 on child speech to establish baseline without finetuning
  2. Full finetuning of Whisper-small with and without each data augmentation method to assess augmentation impact
  3. Adapter tuning vs full finetuning on Whisper-largeV3 to verify PEFT effectiveness scales with model size

## Open Questions the Paper Calls Out

- How do different model sizes impact the effectiveness of data augmentation methods in child ASR?
- What is the relative performance of Whisper-largeV3 compared to other large-scale models like SeamlessM4T and OWSM on child ASR tasks?
- How does the Perturbation Invariant Finetuning (PIF) technique affect the stability and performance of finetuning when using other data augmentation methods beyond pitch and vocal tract length perturbation?

## Limitations
- The benchmark only covers English child speech datasets, limiting generalizability to other languages.
- Several technical details are underspecified, including exact PIF loss implementation and specific PEFT hyperparameters.
- The filtering process using Whisper-largeV2 to select high-quality utterances introduces potential bias.

## Confidence

- **High confidence**: Supervised SFMs (Canary, Parakeet) outperform Whisper on child speech despite less training data
- **Medium confidence**: PEFT performance varies with model size
- **Low confidence**: The effectiveness of the proposed PIF technique

## Next Checks

1. Cross-linguistic validation: Test the same benchmark pipeline on child speech datasets from different languages and cultural contexts.
2. Ablation study on PIF: Conduct controlled experiments removing the PIF regularization from finetuning with data augmentation.
3. Extended PEFT comparison: Evaluate additional PEFT variants and conduct systematic hyperparameter sweeps for each method.