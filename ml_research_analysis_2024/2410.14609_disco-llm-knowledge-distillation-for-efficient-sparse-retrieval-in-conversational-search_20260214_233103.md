---
ver: rpa2
title: 'DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational
  Search'
arxiv_id: '2410.14609'
source_url: https://arxiv.org/abs/2410.14609
tags:
- retrieval
- disco
- distillation
- conversational
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiSCo introduces a relaxed distillation approach for conversational
  search that shifts from constraining student representations to match teacher representations,
  to instead aligning similarity scores between queries and documents. This relaxation
  provides more representational freedom and better leverages the contrastive nature
  of ranking.
---

# DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search

## Quick Facts
- arXiv ID: 2410.14609
- Source URL: https://arxiv.org/abs/2410.14609
- Authors: Simon Lupart; Mohammad Aliannejadi; Evangelos Kanoulas
- Reference count: 40
- Key outcome: DiSCo achieves up to six-point gains in recall for out-of-domain tasks through relaxed distillation that aligns similarity scores rather than constraining representations

## Executive Summary
DiSCo introduces a novel approach to conversational search that addresses the challenge of sparse retrieval through relaxed knowledge distillation from large language models (LLMs). The key innovation shifts from traditional distillation that constrains student representations to match teacher representations, to instead aligning similarity scores between queries and documents. This relaxation provides more representational freedom and better leverages the contrastive nature of ranking tasks. The method also enables multi-teacher distillation, combining knowledge from multiple LLMs to improve robustness and performance. Experiments across five datasets show state-of-the-art performance with significant gains in recall, particularly for out-of-domain tasks.

## Method Summary
The DiSCo framework addresses conversational search by using LLMs as teachers to distill knowledge into sparse retrieval models. Unlike traditional distillation approaches that force student representations to match teacher representations directly, DiSCo relaxes this constraint by aligning the similarity scores between queries and documents. This approach allows the student model more freedom in how it represents information while still learning effective retrieval patterns from the LLM teacher. The framework also supports multi-teacher distillation, where knowledge from multiple LLMs can be combined through various aggregation strategies. This is particularly valuable for conversational search where different LLMs might excel at different aspects of context modeling and relevance assessment.

## Key Results
- Achieves up to six-point gains in recall for out-of-domain tasks compared to existing methods
- Provides improved control over model sparsity, especially for longer conversational contexts
- Demonstrates state-of-the-art performance across five conversational search datasets
- Enables efficient single-model approach that unifies context modeling and retrieval

## Why This Works (Mechanism)
DiSCo's relaxed distillation objective works by aligning similarity scores rather than forcing exact representation matching. This provides more representational freedom in the high-dimensional sparse space, allowing the student model to develop more effective retrieval patterns while still learning from the teacher's knowledge. The approach leverages the contrastive nature of ranking tasks, where the relative similarity between relevant and non-relevant documents matters more than exact representation alignment. Multi-teacher distillation further improves robustness by combining knowledge from multiple LLMs, reducing the risk of overfitting to a single teacher's biases or limitations.

## Foundational Learning

### Knowledge Distillation
- Why needed: Transfer knowledge from powerful but expensive LLMs to efficient student models
- Quick check: Verify student model can achieve comparable performance to teacher with significantly lower computational cost

### Sparse Retrieval
- Why needed: Enable interpretable, controllable retrieval with explicit term matching
- Quick check: Confirm sparse representations maintain semantic understanding while enabling exact match capabilities

### Conversational Search
- Why needed: Handle context carryover and ambiguity in multi-turn information seeking
- Quick check: Validate model can maintain conversation context across multiple turns

### Contrastive Learning
- Why needed: Learn effective representations through relative comparisons rather than absolute targets
- Quick check: Ensure model learns to distinguish relevant from non-relevant documents effectively

### Multi-teacher Distillation
- Why needed: Combine diverse knowledge sources to improve robustness and coverage
- Quick check: Verify aggregation strategies preserve and enhance teacher knowledge effectively

## Architecture Onboarding

### Component Map
User Query -> Context Encoder -> Sparse Retriever -> Document Scoring -> Ranked Results

### Critical Path
The critical path involves encoding the conversational context, generating sparse representations, computing similarity scores with document representations, and producing a ranked list of relevant documents. The relaxed distillation occurs during training, where similarity scores are aligned rather than representations.

### Design Tradeoffs
The primary tradeoff is between representation fidelity and efficiency. Traditional distillation enforces strict representation matching, which can be overly restrictive in high-dimensional sparse spaces. DiSCo's relaxation provides more freedom but requires careful alignment of similarity scores. The multi-teacher approach adds robustness but increases computational overhead during training.

### Failure Signatures
- Over-sparsification leading to loss of semantic information
- Poor similarity score alignment causing degraded retrieval performance
- Aggregation failures in multi-teacher settings leading to inconsistent knowledge transfer
- Context modeling failures resulting in broken conversation threads

### First Experiments
1. Baseline sparse retrieval performance without distillation
2. Traditional distillation with representation matching constraint
3. DiSCo with single teacher and similarity score alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DiSCo's relaxed distillation objective affect the interpretability of the learned sparse representations compared to traditional methods?
- Basis in paper: The paper mentions that DiSCo allows "more effective control over the sparsity of the trained models" and provides "more freedom in the representation space."
- Why unresolved: The paper demonstrates improved sparsity control but does not explicitly analyze how this affects the interpretability of which dimensions are activated or the semantic meaning of sparse representations.
- What evidence would resolve it: Detailed analysis of which vocabulary dimensions are activated for different types of queries and contexts, and whether these activations align with interpretable linguistic features.

### Open Question 2
- Question: Can DiSCo's approach be extended to dense retrieval models, and what would be the theoretical implications of relaxing distillation in dense versus sparse spaces?
- Basis in paper: The paper states "Both dense and sparse methods could be subject to this relaxation" but focuses exclusively on sparse architectures.
- Why unresolved: The authors chose to focus on sparse retrieval due to "more degrees of freedom due to the high dimensionality of the representations," but don't explore why dense models might or might not benefit similarly.
- What evidence would resolve it: Experiments applying DiSCo's similarity-based distillation to dense models like ConvDR, comparing performance and analyzing the differences in how relaxation affects dense versus sparse representations.

### Open Question 3
- Question: What is the optimal strategy for selecting and combining multiple LLM teachers in DiSCo, and how does teacher diversity impact student performance?
- Basis in paper: The paper experiments with "mean, min, and max aggregation methods" but finds "no significant differences in performance" and uses mean aggregation.
- Why unresolved: The authors don't analyze why different aggregation strategies perform similarly, or explore more sophisticated methods for teacher selection based on teacher quality, diversity, or domain expertise.
- What evidence would resolve it: Systematic experiments varying teacher combinations, analyzing teacher performance correlations, and testing adaptive aggregation strategies that weight teachers based on their individual effectiveness for different query types or conversation contexts.

## Limitations
- Experimental evaluation focuses on five datasets, limiting generalizability to other conversational search scenarios
- Six-point recall gains lack detailed statistical significance analysis and confidence intervals
- Computational overhead of multi-teacher distillation compared to single-teacher approaches is not thoroughly quantified
- Scalability to very large document collections (100M+ documents) and real-time applications is not addressed

## Confidence

### High
- The core technical contribution of shifting from representation matching to similarity score alignment in knowledge distillation

### Medium
- The empirical improvements over existing methods across the five evaluated datasets
- The efficiency gains for longer conversational contexts

## Next Checks
1. Conduct ablation studies to isolate the contribution of relaxed distillation versus multi-teacher knowledge combination
2. Evaluate statistical significance of performance improvements across all reported metrics
3. Test scalability on larger document collections (100M+ documents) to verify real-world applicability claims