---
ver: rpa2
title: 'BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce
  Domain'
arxiv_id: '2409.20075'
source_url: https://arxiv.org/abs/2409.20075
tags:
- retrieval
- generation
- retriever
- which
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting Retrieval-Augmented
  Generation (RAG) systems to the e-commerce domain, where existing approaches with
  separate retrieval and generation modules limit mutual benefits. To solve this,
  the authors propose a Backbone Shared RAG (BSharedRAG) framework that uses a shared
  domain-specific backbone model with task-specific Low-Rank Adaptation (LoRA) modules
  for retrieval and generation.
---

# BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain

## Quick Facts
- arXiv ID: 2409.20075
- Source URL: https://arxiv.org/abs/2409.20075
- Authors: Kaisi Guan; Qian Cao; Yuchong Sun; Xiting Wang; Ruihua Song
- Reference count: 33
- Primary result: BSharedRAG achieves 5-17% improvement in Hit@3 for retrieval and 23% improvement in BLEU-3 for generation compared to baselines

## Executive Summary
This paper addresses the challenge of adapting Retrieval-Augmented Generation (RAG) systems to the e-commerce domain, where existing approaches with separate retrieval and generation modules limit mutual benefits. To solve this, the authors propose a Backbone Shared RAG (BSharedRAG) framework that uses a shared domain-specific backbone model with task-specific Low-Rank Adaptation (LoRA) modules for retrieval and generation. The framework employs domain-specific continual pre-training, hard negative mining for retrieval, and retrieval-augmented instruction tuning for generation. Experiments on the WorthBuying dataset show BSharedRAG achieves significant improvements over baseline models while maintaining comparable inference efficiency.

## Method Summary
BSharedRAG is a novel framework for adapting RAG systems to specific domains, particularly e-commerce. The method consists of three stages: (1) domain-specific continual pre-training of a base LLM on e-commerce data to create a shared knowledge base, (2) training a retriever using hard negative mining with LoRA modules on top of the pre-trained backbone, and (3) training a generator using retrieval-augmented instruction tuning with LoRA, incorporating retrieved documents into instruction-response pairs. The framework is evaluated on the WorthBuying dataset using standard retrieval metrics (Hit@3, nDCG@3, nDCG@5) and generation metrics (BLEU-3, ROUGE-L, BERTScore, Accuracy).

## Key Results
- BSharedRAG achieves 5-13% improvement in Hit@3 for retrieval compared to baseline models
- BSharedRAG achieves 23% improvement in BLEU-3 for generation compared to baseline models
- The framework maintains comparable inference efficiency to baseline models while providing significant performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing a backbone model between retrieval and generation enables effective knowledge transfer without negative interference.
- Mechanism: The backbone model is continually pre-trained on domain-specific data, creating a shared representation space. Retrieval and generation LoRA modules are then trained independently on this frozen backbone, allowing each task to benefit from the domain knowledge encoded in the backbone without conflicting optimization signals.
- Core assumption: Continual pre-training on domain data creates a representation space that is beneficial for both retrieval and generation tasks.
- Evidence anchors:
  - [abstract]: "Experimental results indicate that our proposed BSharedRAG outperforms baseline models by 5% and 13% in Hit@3 upon two datasets in retrieval evaluation and by 23% in terms of BLEU-3 in generation evaluation."
  - [section]: "This framework may suffer from negative transfer, i.e., performance decrease in both tasks due to the potential conflicts between them, as verified in our experiments."
  - [corpus]: Weak - the corpus contains related papers but none directly analyze negative transfer in shared backbone RAG frameworks.
- Break condition: If continual pre-training creates a representation space that is highly specialized for one task but detrimental to the other, or if the LoRA modules introduce significant task-specific bias that conflicts with the shared backbone knowledge.

### Mechanism 2
- Claim: Hard negative mining during retriever training improves the model's ability to distinguish relevant from irrelevant documents.
- Mechanism: The retriever is fine-tuned using in-batch contrastive learning with hard negatives - documents that are ranked highly by a baseline model but are actually irrelevant. This forces the model to learn finer distinctions between similar documents.
- Core assumption: Hard negatives provide more informative gradients for training than randomly sampled negatives.
- Evidence anchors:
  - [section]: "We also propose a strategy for identifying hard negatives during contrastive learning. Specifically, we utilize a baseline retrieval model to retrieve a preliminary set of potential negatives, then the high-ranking yet erroneously matched documents, i.e., hard negatives, which helps refine the model's discrimination capabilities."
  - [section]: "Experimental results show that this strategy enhances the effectiveness of retrieval."
  - [corpus]: Weak - the corpus contains related RAG papers but none specifically analyze hard negative mining strategies in shared backbone frameworks.
- Break condition: If the hard negative mining strategy becomes too aggressive and starts including borderline relevant documents, or if the baseline model used for mining is too weak and the hard negatives are not truly challenging.

### Mechanism 3
- Claim: Retrieval-augmented instruction tuning improves the generator's ability to use retrieved information effectively.
- Mechanism: The generator is fine-tuned on instruction-response pairs augmented with retrieved documents, learning to incorporate external knowledge into its responses. This creates a preference alignment where the generator naturally favors documents that contain information useful for answering questions.
- Core assumption: Fine-tuning with retrieved documents as context teaches the generator to recognize and utilize relevant information patterns.
- Evidence anchors:
  - [abstract]: "Our proposed BSharedRAG outperforms base models by 5% and 13% in Hit@3 upon two datasets in retrieval evaluation and by 23% in terms of BLEU-3 in generation evaluation."
  - [section]: "To ensure that the generator can effectively utilize retrieved information, we augment the instruction-response pairs by incorporating the retrieved articles, forming triples (q, d, a)."
  - [corpus]: Weak - the corpus contains related RAG papers but none specifically analyze instruction tuning with retrieved documents in shared backbone frameworks.
- Break condition: If the retrieved documents are consistently of poor quality or irrelevant to the questions, or if the generator overfits to the specific format of the augmented training data and fails to generalize.

## Foundational Learning

- Concept: Continual pre-training
  - Why needed here: Adapts a general domain LLM to the specific characteristics of e-commerce data, creating a shared knowledge base for both retrieval and generation tasks.
  - Quick check question: What is the difference between continual pre-training and fine-tuning, and why is continual pre-training preferred for adapting to a new domain?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Allows efficient adaptation of the large backbone model with minimal additional parameters, enabling task-specific modifications for retrieval and generation while keeping the shared backbone frozen.
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning, and what are the trade-offs?

- Concept: Contrastive learning for retrieval
  - Why needed here: Trains the retriever to distinguish between relevant and irrelevant documents by optimizing a similarity metric, which is essential for effective information retrieval.
  - Quick check question: What is the InfoNCE loss, and how does it encourage the model to assign higher similarity scores to relevant document-query pairs?

## Architecture Onboarding

- Component map: Domain-specific backbone LLM (frozen) -> Retriever LoRA module -> Generator LoRA module -> E-commerce knowledge base (735K documents)

- Critical path: Document retrieval → Document ranking → Document selection → Answer generation

- Design tradeoffs:
  - Sharing backbone vs. separate models: Sharing enables knowledge transfer but requires careful design to avoid negative interference
  - LoRA vs. full fine-tuning: LoRA is more parameter-efficient but may limit adaptation capacity
  - Hard negatives vs. random negatives: Hard negatives provide better gradients but require additional computation for mining

- Failure signatures:
  - Retrieval performance drops significantly compared to baseline models
  - Generation quality deteriorates despite good retrieval performance
  - Model shows signs of catastrophic forgetting during continual pre-training
  - Hard negative mining strategy produces too many false positives

- First 3 experiments:
  1. Compare retrieval performance with and without continual pre-training on the backbone
  2. Test different negative sampling strategies (random vs. hard negatives) during retriever training
  3. Evaluate the impact of retrieval-augmented instruction tuning on generator performance with different amounts of training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between retrieval and generation losses in the Fully Shared RAG framework, and how does this balance vary across different domains?
- Basis in paper: [explicit] The paper mentions that finding an appropriate λ to balance retrieval and generation losses in Fully Shared RAG is non-trivial and may require effort-taking hyperparameter search.
- Why unresolved: The paper does not provide specific guidance on how to determine the optimal λ value, and it may vary depending on the domain and specific tasks.
- What evidence would resolve it: Experiments comparing Fully Shared RAG performance across different λ values and domains, along with an analysis of how the optimal λ varies.

### Open Question 2
- Question: How does the performance of BSharedRAG compare to other domain adaptation methods for RAG systems, such as fine-tuning the retriever and generator separately or using multi-task learning with shared parameters?
- Basis in paper: [inferred] The paper presents BSharedRAG as a novel approach for domain adaptation in RAG systems, but does not compare it to other domain adaptation methods.
- Why unresolved: The paper focuses on comparing BSharedRAG to baseline models but does not explore other domain adaptation strategies.
- What evidence would resolve it: Comparative experiments between BSharedRAG and other domain adaptation methods for RAG systems, evaluating their performance on domain-specific tasks.

### Open Question 3
- Question: What are the long-term effects of using hard negative mining in the retriever training process, and how does it impact the model's ability to generalize to unseen data?
- Basis in paper: [explicit] The paper introduces hard negative mining as a strategy to improve retrieval performance, but does not discuss its long-term effects or impact on generalization.
- Why unresolved: The paper focuses on the immediate benefits of hard negative mining but does not explore its potential drawbacks or long-term implications.
- What evidence would resolve it: Longitudinal studies tracking the performance of BSharedRAG over time with and without hard negative mining, as well as experiments testing the model's generalization ability to unseen data.

## Limitations

- The evaluation relies entirely on a single e-commerce dataset (WorthBuying), making generalizability to other domains uncertain.
- The hard negative mining strategy depends on a baseline retrieval model that is not clearly specified, raising questions about the quality and consistency of mined hard negatives.
- The computational efficiency claims lack direct comparison metrics - while inference speed is mentioned as comparable, detailed runtime and memory usage comparisons with baseline models are absent.

## Confidence

**High Confidence**: The retrieval performance improvements (5-13% Hit@3) are well-supported by experimental results and the mechanism of continual pre-training plus hard negative mining is theoretically sound. The use of LoRA for efficient task-specific adaptation is a well-established technique with predictable outcomes.

**Medium Confidence**: The generation improvements (23% BLEU-3) are demonstrated but rely on a single dataset and metric. The effectiveness of retrieval-augmented instruction tuning is plausible but the exact contribution of this component versus the shared backbone remains unclear from ablation studies.

**Low Confidence**: The claims about avoiding negative interference between tasks are based on comparative experiments rather than direct analysis of representation spaces or gradient conflicts. The scalability to larger models and datasets is asserted but not empirically verified.

## Next Checks

1. **Ablation Study on Pre-training**: Conduct experiments isolating the contribution of continual pre-training by comparing BSharedRAG against versions where the backbone is not pre-trained on e-commerce data. This would clarify whether the performance gains stem from domain adaptation or task-specific LoRA modules.

2. **Cross-Domain Generalization Test**: Evaluate BSharedRAG on non-e-commerce retrieval and generation tasks (e.g., academic Q&A or general knowledge base) to assess the framework's adaptability and potential domain-specific limitations.

3. **Hard Negative Quality Analysis**: Implement and compare multiple hard negative mining strategies (e.g., using different baseline models or ranking methods) and analyze the distribution and quality of mined negatives to understand their impact on retrieval performance.