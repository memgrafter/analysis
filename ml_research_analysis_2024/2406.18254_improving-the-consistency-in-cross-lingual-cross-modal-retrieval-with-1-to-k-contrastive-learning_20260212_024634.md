---
ver: rpa2
title: Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K
  Contrastive Learning
arxiv_id: '2406.18254'
source_url: https://arxiv.org/abs/2406.18254
tags:
- languages
- language
- learning
- contrastive
- cross-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two consistency problems in cross-lingual
  cross-modal retrieval (CCR): intra-modal error propagation and inter-modal optimization
  direction bias. To solve these, the authors propose 1-to-K contrastive learning,
  where each image is aligned with K texts in different languages simultaneously,
  eliminating error propagation and optimization bias.'
---

# Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning

## Quick Facts
- arXiv ID: 2406.18254
- Source URL: https://arxiv.org/abs/2406.18254
- Reference count: 40
- Primary result: Achieves state-of-the-art results in cross-lingual cross-modal retrieval with improved consistency using 1-to-K contrastive learning

## Executive Summary
This paper addresses two critical consistency problems in cross-lingual cross-modal retrieval (CCR): intra-modal error propagation and inter-modal optimization direction bias. The authors propose 1-to-K contrastive learning, where each image is aligned with K texts in different languages simultaneously, eliminating error propagation and optimization bias. They also introduce Mean Rank Variance (MRV) as a new metric to evaluate rank consistency across languages. Experiments on four datasets show that their method, CCRk, achieves new state-of-the-art results with smaller-scale pre-trained data, improving both recall rates and MRV.

## Method Summary
The proposed CCRk framework combines 1-to-K contrastive learning, Multi-lingual Image-Text Matching (MITM), and Cross-modal Masked Language Modeling (CMLM). In 1-to-K contrastive learning, each image is simultaneously aligned with K texts in different languages, eliminating error propagation and optimization bias. The model uses a Swin Transformer for image encoding, XLM-R for multi-lingual text encoding, and a fusion encoder with cross-attention. Training involves AdamW optimizer with 1e-4 learning rate, 0.01 weight decay, batch size 64, and 30 epochs on pre-training data including CC3M, SBU Caption, Visual Genome, and COCO with 10 languages.

## Key Results
- Achieves new state-of-the-art recall rates on xFlickr&CO, WIT, Multi30K, and COCO datasets
- Demonstrates significant improvements in Mean Rank Variance (MRV), indicating better rank consistency across languages
- Shows effectiveness with smaller-scale pre-trained data compared to previous methods
- Outperforms baselines by 3.1% on average in recall@1 for 10-language experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning one image with K texts in different languages simultaneously eliminates optimization direction bias.
- Mechanism: In traditional 1-to-1 contrastive learning, the image is aligned with only one language at a time, creating directional bias. With 1-to-K, the image representation is pulled toward the centroid of all language representations simultaneously, ensuring balanced alignment across all languages.
- Core assumption: The semantic space across languages is roughly symmetric around the image representation when all languages are aligned together.
- Evidence anchors:
  - [abstract]: "treats each language equally and eliminates error propagation and optimization bias"
  - [section 4.2]: "each image is aligned simultaneously with K texts in different languages"

### Mechanism 2
- Claim: 1-to-K contrastive learning reduces intra-modal error propagation compared to using English as a bridge.
- Mechanism: By aligning all languages directly with images rather than through English, the method eliminates the intermediate alignment step where errors can compound. Each language has a direct alignment path to the visual modality.
- Core assumption: Direct alignment paths have lower error accumulation than multi-hop alignment through a bridge language.
- Evidence anchors:
  - [abstract]: "eliminates error propagation and optimization bias"
  - [section 3.2]: "if English texts are used to connect images and texts in other languages, there will be a risk of error propagation"

### Mechanism 3
- Claim: The Mean Rank Variance (MRV) metric captures rank inconsistency across languages that Recall@K misses.
- Mechanism: MRV measures the variance in ranking positions of the same image when queried with semantically equivalent texts in different languages. This reveals local inconsistencies that average performance metrics obscure.
- Core assumption: Rank consistency within instances is meaningful for practical applications where users expect consistent results across languages.
- Evidence anchors:
  - [abstract]: "proposed a new evaluation metric, Mean Rank Variance (MRV), to reflect the rank inconsistency across languages within each instance"
  - [section 4.4]: "MRV for K languages... can be expressed as... |Rank_jk - Rank_j|"

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Forms the basis of how image-text pairs are aligned in semantic space
  - Quick check question: What happens to the loss when the anchor and positive sample are perfectly aligned?

- Concept: Cross-modal alignment optimization
  - Why needed here: Understanding how alignment direction affects semantic space geometry
  - Quick check question: In a 2-language setup, what should be the optimal alignment direction for an image relative to two text representations?

- Concept: Multi-lingual text encoder representations
  - Why needed here: The quality of cross-lingual alignment depends on how well different languages are aligned in the text encoder's semantic space
  - Quick check question: What does it mean if text representations from different languages are not semantically aligned in the encoder's space?

## Architecture Onboarding

- Component map: Image → Image encoder → [1-to-K contrastive] ← Text encoder ← Text in K languages
- Critical path: Image → Image encoder → [1-to-K contrastive] ← Text encoder ← Text in K languages
- Design tradeoffs:
  - More languages → better cross-lingual consistency but higher computational cost
  - No fusion encoder → faster training but potentially lower recall rates
  - Hard negative sampling → better discrimination but more complex implementation
- Failure signatures:
  - High MRV with good Recall@K → rank inconsistency despite good overall performance
  - English consistently outperforms other languages → error propagation from bridge language
  - MRV increases with more languages → capacity limitations or optimization difficulty
- First 3 experiments:
  1. Compare 1-to-1 vs 1-to-K contrastive learning on xFlickr&CO with English and one other language
  2. Measure MRV vs Recall@K trade-off on a dataset with 4 languages
  3. Test ablation of fusion encoder to validate its contribution to recall rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would CCRk's performance change with more than 10 languages during pre-training?
- Basis in paper: [explicit] The authors note that adding more languages increases difficulty and may hurt performance due to model capacity constraints and optimization challenges
- Why unresolved: The paper only evaluates up to 10 languages, leaving open whether performance plateaus or degrades significantly with further language additions
- What evidence would resolve it: Experiments pre-training CCRk with 15-20+ languages and comparing recall rates and MRV across all language pairs

### Open Question 2
- Question: What is the impact of varying the text masking ratio in the CMLM task on CCRk's retrieval performance?
- Basis in paper: [inferred] The paper uses a fixed 15% masking ratio following BERT, but doesn't explore how different ratios affect the learned representations
- Why unresolved: The masking ratio is treated as a hyperparameter without systematic evaluation of its impact on downstream retrieval tasks
- What evidence would resolve it: Experiments training CCRk variants with masking ratios ranging from 5% to 50% and measuring performance differences

### Open Question 3
- Question: How does the choice of negative sampling strategy affect CCRk's performance compared to random negative sampling?
- Basis in paper: [explicit] The authors use hard negative sampling based on cosine similarity, but don't compare against random sampling baselines
- Why unresolved: The paper assumes hard negatives are superior but doesn't empirically validate this assumption
- What evidence would resolve it: Head-to-head comparison of CCRk trained with random negatives versus hard negatives, measuring both recall and MRV metrics

## Limitations
- Evaluation focuses on recall rates and MRV but lacks direct evidence of eliminated error propagation and optimization bias
- Computational cost implications of scaling to more languages are not thoroughly explored
- Real-world impact of rank inconsistencies captured by MRV is not demonstrated

## Confidence
- **High Confidence**: Core experimental results showing improved recall rates and MRV across multiple datasets are well-documented and reproducible
- **Medium Confidence**: Theoretical claims about eliminating error propagation and optimization bias are plausible but lack direct validation
- **Low Confidence**: Practical significance of MRV as a metric is not fully established

## Next Checks
1. Design an experiment measuring error accumulation across different alignment paths to quantitatively verify 1-to-K reduces error propagation
2. Analyze geometric properties of semantic space by measuring variance in alignment distances from images to different language representations
3. Conduct user studies to determine whether rank inconsistencies captured by MRV actually affect practical retrieval performance