---
ver: rpa2
title: 'Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based
  on Subword-Level Semantic Concepts?'
arxiv_id: '2411.04530'
source_url: https://arxiv.org/abs/2411.04530
tags:
- semantic
- word
- embedding
- miracl
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether multilingual language models (mLMs)
  understand text based on shared subword-level semantic concepts rather than just
  superficial forms. The authors group semantically similar subwords and their embeddings
  into "semantic tokens" and evaluate the updated mLMs on five multilingual downstream
  tasks across 30 languages.
---

# Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?

## Quick Facts
- arXiv ID: 2411.04530
- Source URL: https://arxiv.org/abs/2411.04530
- Reference count: 40
- Key outcome: Multilingual models can preserve >90% effectiveness on classification tasks with only 5% of original vocabulary size

## Executive Summary
This paper investigates whether multilingual language models (mLMs) understand text based on shared subword-level semantic concepts rather than just superficial forms. The authors group semantically similar subwords and their embeddings into "semantic tokens" and evaluate the updated mLMs on five multilingual downstream tasks across 30 languages. The key finding is that mLMs can preserve over 90% effectiveness on classification tasks with only 5% of the original vocabulary size and over 85% effectiveness on embedding tasks with 20% of the original vocabulary size. This suggests that general shared semantics can get models a long way in making predictions. Further analysis shows these findings generalize across different tokenizers, model sizes, and are not confounded by reduced embedding parameters.

## Method Summary
The authors form "semantic tokens" by clustering semantically similar subwords using K-Means on word embeddings, then evaluate updated models on five multilingual downstream tasks. The method involves continual pretraining with MLM objective after vocabulary and embedding modifications. They test across mBERT, XLM-R (base/large), and XLM-V models on MasakhaNER (NER), TyDiQA (QA), XNI (NLI), and MIRACL (Retrieval/Reranking) tasks. Cross-lingual subword alignment (CLSA) is applied using InfoNCE loss with bilingual dictionaries and concept lists to enhance semantic similarity among grouped subwords.

## Key Results
- mLMs preserve over 90% effectiveness on classification tasks with only 5% of original vocabulary size
- mLMs maintain over 85% effectiveness on embedding tasks with 20% of original vocabulary size
- Zero-shot results on certain classification tasks with semantic tokens are on par with or better than original models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping semantically similar subwords preserves most of the model's downstream effectiveness.
- Mechanism: By clustering subwords based on their embedding similarity and sharing the same embedding vector, the model retains coarse semantic representations sufficient for classification tasks while reducing vocabulary size.
- Core assumption: Subword-level semantic similarity is sufficient to maintain task performance; fine-grained distinctions are less critical for many tasks.
- Evidence anchors: [abstract] "Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes."
- Break condition: Tasks requiring fine-grained lexical distinctions (e.g., embedding tasks) show more sensitivity to semantic grouping.

### Mechanism 2
- Claim: Semantic grouping is complementary to embedding dimension reduction, allowing further parameter savings without performance loss.
- Mechanism: Semantic grouping reduces vocabulary size, while dimension reduction reduces embedding size; together they achieve parameter reduction beyond what either can do alone.
- Core assumption: The model's performance is bottlenecked by vocabulary size rather than embedding dimension beyond a certain point.
- Evidence anchors: [section 4.2] "SG could further push down the overall word embedding parameters to a level that could not be achieved by reducing the word embedding dimension alone."
- Break condition: If embedding dimension becomes too small (e.g., below 32), performance drops sharply regardless of grouping.

### Mechanism 3
- Claim: Cross-lingual subword alignment (CLSA) enhances semantic similarity among grouped subwords, improving downstream performance.
- Mechanism: Training word embeddings with InfoNCE loss using cross-lingual word pairs aligns semantically equivalent subwords across languages, strengthening shared semantic representations.
- Core assumption: Cross-lingual word pairs from dictionaries and concept lists provide reliable semantic anchors for alignment.
- Evidence anchors: [section 2.2] "we gather cross-lingual word pairs from bilingual dictionaries... and concept lists... train using InfoNCE loss."
- Break condition: If alignment data is sparse or noisy, improvements may not materialize.

## Foundational Learning

- Concept: K-Means clustering on embedding space
  - Why needed here: Used to group semantically similar subwords based on cosine distance of their embeddings.
  - Quick check question: How does cosine distance differ from Euclidean distance when clustering high-dimensional embeddings?

- Concept: Cross-lingual word alignment via InfoNCE loss
  - Why needed here: Aligns word embeddings across languages to enhance semantic similarity among grouped subwords.
  - Quick check question: What role do in-batch negatives play in the InfoNCE loss formulation?

- Concept: Continual pretraining with MLM objective
  - Why needed here: Realigns model parameters after vocabulary and embedding modifications to restore performance.
  - Quick check question: Why is continual pretraining necessary after semantic grouping or dimension reduction?

## Architecture Onboarding

- Component map: Tokenizer → Vocabulary → Word embeddings → Model layers → Downstream heads
- Critical path: Tokenizer → Vocabulary → Word embeddings → Model layers → Downstream heads
- Design tradeoffs:
  - Vocabulary size vs. model size: Smaller vocabulary reduces memory but may lose information
  - Embedding dimension vs. capacity: Lower dimensions save memory but may lose nuance
  - Semantic grouping granularity: Coarser groups save more parameters but may lose task-specific details
- Failure signatures:
  - Performance drops sharply after grouping → Vocabulary too small or grouping too coarse
  - No improvement from CLSA → Alignment data insufficient or noisy
  - Memory usage unchanged after dimension reduction → Bottleneck in activations, not embeddings
- First 3 experiments:
  1. Apply K-Means grouping on mBERT embeddings with 20% grouping ratio, evaluate on XNLI
  2. Apply dimension reduction to 128D, then semantic grouping at 10% ratio, evaluate on MasakhaNER
  3. Apply CLSA with MUSE data, then grouping at 5% ratio, evaluate on TyDiQA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do semantic grouping effects vary when applied to monolingual versus multilingual models?
- Basis in paper: [inferred] The paper extensively tests multilingual models (mBERT, XLM-R, XLM-V) but does not compare with monolingual baselines.
- Why unresolved: The paper's focus is on cross-lingual semantic sharing, so monolingual comparisons are outside the scope but would provide important context for how much cross-lingual transfer contributes versus monolingual semantic redundancy.
- What evidence would resolve it: A direct comparison showing effectiveness retention rates for monolingual models under semantic grouping at various vocabulary reduction levels.

### Open Question 2
- Question: What is the impact of semantic grouping on model robustness to adversarial or out-of-distribution inputs?
- Basis in paper: [inferred] The paper evaluates on standard benchmark tasks but doesn't test robustness to perturbations, adversarial examples, or OOD data.
- Why unresolved: Semantic grouping may improve robustness by reducing vocabulary sparsity, or it may harm it by conflating distinct meanings, but this remains unexplored.
- What evidence would resolve it: Experiments measuring performance drops on adversarial examples or OOD test sets for semantically grouped versus baseline models.

### Open Question 3
- Question: How does semantic grouping affect the model's ability to handle polysemy and word sense disambiguation?
- Basis in paper: [explicit] The paper acknowledges limitations with polysemous situations in the Conclusion section.
- Why unresolved: The paper groups based on overall embedding similarity without distinguishing different senses of the same word, which could degrade performance on tasks requiring fine-grained sense distinctions.
- What evidence would resolve it: Analysis of model performance on word sense disambiguation benchmarks before and after semantic grouping, or inspection of grouped subwords containing multiple senses.

## Limitations
- Semantic grouping relies on embedding similarity which may not capture all semantic nuances, particularly for polysemous words
- The study focuses on transformer-based models and may not generalize to other architectures
- Cross-lingual alignment quality depends on the completeness and accuracy of bilingual dictionaries and concept lists, which may be limited for low-resource languages

## Confidence
- **High confidence**: The core finding that mLMs can maintain >90% classification effectiveness with only 5% of original vocabulary is well-supported by consistent results across multiple tasks, models, and tokenizers.
- **Medium confidence**: The claim that shared subword-level semantics serve as anchors for cross-lingual transfer is supported by zero-shot results but requires more rigorous ablation studies.
- **Low confidence**: The assertion that semantic grouping reveals how mLMs "understand" language at a conceptual level overstates the findings, as the grouping is based on distributional similarity rather than true semantic comprehension.

## Next Checks
1. **Ablation on CLSA impact**: Run the semantic grouping experiment without cross-lingual alignment (InfoNCE training) to quantify the exact contribution of CLSA versus grouping alone on downstream performance.
2. **Fine-grained task analysis**: Systematically vary grouping ratios (5%, 10%, 20%, 50%) across all five tasks to identify task-specific breakpoints where semantic granularity becomes critical for performance.
3. **Low-resource language stress test**: Evaluate the semantic grouping approach on languages with minimal cross-lingual alignment data to determine whether the method's benefits degrade as alignment quality decreases.