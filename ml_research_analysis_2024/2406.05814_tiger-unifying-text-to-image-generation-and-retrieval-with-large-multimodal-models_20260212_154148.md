---
ver: rpa2
title: 'TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal
  Models'
arxiv_id: '2406.05814'
source_url: https://arxiv.org/abs/2406.05814
tags:
- retrieval
- generation
- image
- arxiv
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a unified framework called TIGeR-ONE for text-to-image
  generation and retrieval using large multimodal models (LMMs). The key idea is to
  explore the intrinsic discriminative abilities of LMMs and introduce an efficient
  generative retrieval method for text-to-image retrieval in a training-free manner.
---

# TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models

## Quick Facts
- arXiv ID: 2406.05814
- Source URL: https://arxiv.org/abs/2406.05814
- Authors: Leigang Qu; Haochuan Li; Tan Wang; Wenjie Wang; Yongqi Li; Liqiang Nie; Tat-Seng Chua
- Reference count: 39
- Key outcome: Unified framework TIGeR-ONE for text-to-image generation and retrieval using LMMs, achieving superior performance on creative and knowledge-intensive domains

## Executive Summary
This paper proposes TIGeR-ONE, a unified framework that leverages large multimodal models (LMMs) to perform both text-to-image generation and retrieval in a training-free manner. The framework exploits intrinsic discriminative abilities of LMMs by using bidirectional likelihoods for both generation (forward p(Y|X)) and retrieval (reverse p(X|Y)), unified within an autoregressive framework. An autonomous decision mechanism selects between generated and retrieved images based on likelihood comparisons, and the authors construct TIGeR-Bench, a comprehensive benchmark spanning creative and knowledge-intensive domains.

## Method Summary
TIGeR-ONE is a model-agnostic framework that unifies text-to-image generation and retrieval using large multimodal models. The method exploits bidirectional likelihoods - forward p(Y|X) for generation and reverse p(X|Y) for retrieval - within an autoregressive framework. For efficient retrieval, it introduces forward beam search constrained to database tokens combined with reverse re-ranking. An autonomous decision mechanism compares similarity scores between generated and retrieved images to select the optimal output. The framework operates without separate training by leveraging intrinsic cross-modal alignment capabilities of pretrained LMMs.

## Key Results
- TIGeR-ONE achieves higher CLIP-T and CLIP-I scores compared to separate generation or retrieval baselines
- The unified method demonstrates improved retrieval performance with R@1 scores exceeding existing methods
- Generative retrieval via forward beam search with reverse re-ranking shows superior efficiency compared to dense retrieval approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TIGeR-ONE leverages intrinsic discriminative abilities of LMMs to perform both generation and retrieval without separate training
- Mechanism: The framework exploits bidirectional likelihoods—forward p(Y|X) for generation and reverse p(X|Y) for retrieval—allowing unified autoregressive processing
- Core assumption: LMMs trained on next-token prediction inherently develop semantic matching capabilities across modalities
- Evidence anchors:
  - [abstract] "explore the intrinsic discriminative abilities of LMMs and introduce an efficient generative retrieval method"
  - [section 3.2] "we propose another option to circumvent the unbalanced prior distribution...estimate the semantic similarity in a reverse way by predicting the conditional likelihood of X given Y"
  - [corpus] Weak: No direct corpus evidence of this specific mechanism; only related benchmarks exist
- Break condition: If LMMs lack sufficient cross-modal alignment during pretraining, bidirectional likelihoods will be unreliable

### Mechanism 2
- Claim: Generative retrieval via forward beam search with reverse re-ranking achieves efficient and accurate retrieval
- Mechanism: Beam search constrains decoding to database tokens for efficiency, then re-ranking uses reverse likelihood to correct semantic mismatches
- Core assumption: Forward proxies are efficient but less accurate, while reverse proxies are accurate but computationally expensive for full database traversal
- Evidence anchors:
  - [section 3.3] "we introduce forward beam search and reverse re-ranking...significantly improve the efficiency since it only requires N times of forward propagation"
  - [section 5.3] "RRR could help FBS break through the ceiling and significantly improve recall"
  - [corpus] Moderate: Visual-RAG paper suggests similar two-stage retrieval but not identical
- Break condition: If beam size is too small, retrieval recall will suffer regardless of re-ranking

### Mechanism 3
- Claim: Autonomous decision mechanism selects between generated and retrieved images based on likelihood comparisons
- Mechanism: Compute s(X, Y^G) and s(X, Y^R) using chosen proxy, select higher-scoring image without additional computation
- Core assumption: LMM likelihood scores directly correlate with human preference for image relevance
- Evidence anchors:
  - [section 3.4] "calculate two similarities s(X, Y^G) and s(X, Y^R) using one of the three proxies and choose the image with the higher similarity"
  - [section 5.2] "Our proposed unified framework significantly improves the performance of both LaVIT and SEED-LLaMA"
  - [corpus] Moderate: Retrieval-augmented generation papers support likelihood-based selection
- Break condition: If LMM likelihood distributions are poorly calibrated, decision quality will degrade

## Foundational Learning

- Concept: Autoregressive generation and conditional probability
  - Why needed here: TIGeR-ONE relies on next-token prediction framework for both generation and retrieval
  - Quick check question: Can you explain how p(Y|X) = ∏ p(y_i|Y<i,X) enables both image generation and retrieval?
- Concept: Multimodal representation alignment
  - Why needed here: LMMs must align text and visual tokens in shared semantic space for cross-modal retrieval
  - Quick check question: What makes CLIP-like alignment crucial for TIGeR-ONE's discriminative abilities?
- Concept: Beam search and constrained decoding
  - Why needed here: Forward beam search enables efficient retrieval by limiting search space to database tokens
  - Quick check question: How does maintaining a Trie structure enable constraint generation during beam search?

## Architecture Onboarding

- Component map: LMM core -> Discrete visual tokenizer -> Forward beam search engine -> Reverse re-ranker -> Decision module -> Output selector
- Critical path: Prompt -> LMM forward pass -> Beam search generation -> Reverse likelihood computation -> Image selection -> Output
- Design tradeoffs: Efficiency vs recall (beam size), accuracy vs computation (forward vs reverse proxies), generation vs retrieval preference in decision
- Failure signatures: Low retrieval accuracy (beam size too small), poor decision quality (likelihood miscalibration), slow inference (inefficient token processing)
- First 3 experiments:
  1. Compare forward ranking vs reverse ranking on MS-COCO to verify proxy effectiveness
  2. Test beam size impact on retrieval recall vs inference time trade-off
  3. Evaluate decision accuracy by comparing selected vs ground-truth images on TIGeR-Bench

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the decision-making mechanism in TIGeR-ONE be improved to reduce generation preference bias, especially in knowledge-intensive domains?
- Basis in paper: [explicit] The paper identifies a generation preference problem where LaVIT consistently chooses generated images even when retrieved ones are correct, and attributes this to potential gaps between pre-trained/fine-tuned data and TIGeR-Bench.
- Why unresolved: The current decision mechanism relies solely on similarity scores from forward and reverse proxies, which may not capture nuanced domain-specific requirements or the quality of factual accuracy needed for knowledge-intensive images.
- What evidence would resolve it: Comparative experiments testing different decision strategies (e.g., incorporating retrieval confidence scores, domain-specific weighting, or human preference learning) and their impact on %Retr. accuracy across creative vs. knowledge-intensive domains.

### Open Question 2
- What are the fundamental causes of visual modality bias in LMMs, and how can they be systematically addressed to improve both generation and retrieval performance?
- Basis in paper: [explicit] The paper discusses visual bias arising from unbalanced distribution p(Y) and shows that debiasing with PMI-like approaches helps, but the problem persists especially in forward proxies.
- Why unresolved: While debiasing strategies like adjusting η in Eqn. 3 improve ranking, they don't fully eliminate bias, and the paper suggests deeper issues may stem from data distribution, model architecture, or optimization objectives.
- What evidence would resolve it: Ablation studies varying training data composition (captioning vs. text-to-image ratios), architectural modifications to balance cross-modal interactions, and quantitative analysis of bias reduction across multiple LMMs and benchmarks.

### Open Question 3
- How can retrieval-augmented generation and generation-augmented retrieval be effectively integrated within the TIGeR framework to create synergistic benefits beyond the current decision mechanism?
- Basis in paper: [inferred] The paper briefly explores augmented generation for better retrieval and augmented retrieval for better generation in qualitative examples (Fig. 11-12), showing potential improvements but not systematically evaluating the integration.
- Why unresolved: Current TIGeR-ONE uses a single-step decision between generated and retrieved images, but iterative or bidirectional augmentation strategies could leverage complementary strengths more effectively.
- What evidence would resolve it: Systematic evaluation comparing single-step decision vs. iterative augmentation strategies on TIGeR-Bench metrics (CLIP-T/I scores, R@K) and qualitative analysis of generated/retrieved image quality improvements.

### Open Question 4
- What is the optimal balance between generation and retrieval in unified frameworks for different application scenarios (e.g., creative vs. knowledge-intensive domains)?
- Basis in paper: [explicit] The paper shows that models exhibit generation preference in creative domains but need stronger retrieval for knowledge-intensive domains, yet the unified decision mechanism doesn't adapt based on domain or user intent.
- Why unresolved: The current framework treats all prompts uniformly without considering the nature of the request or user preferences, leading to suboptimal trade-offs between novelty and factual accuracy.
- What evidence would resolve it: Domain-specific ablation studies varying the generation-retrieval balance, user studies measuring preference satisfaction across scenarios, and development of adaptive decision mechanisms that infer user intent from prompts.

## Limitations

- Unification Effectiveness: Performance gains are modest, with incremental improvements that may not justify the added complexity for all applications
- Decision Mechanism Reliability: Limited evidence that LMM likelihood scores correlate well with human preferences for image selection
- Computational Efficiency Claims: Actual computational costs are not fully characterized, and reverse re-ranking could become a bottleneck for large-scale deployments

## Confidence

**High Confidence** (Well-supported by evidence):
- The technical feasibility of using bidirectional likelihoods (p(Y|X) and p(X|Y)) for cross-modal matching
- The basic effectiveness of forward beam search with reverse re-ranking for improving retrieval recall
- The necessity of creating TIGeR-Bench as a more challenging evaluation benchmark

**Medium Confidence** (Partially supported):
- The unified framework consistently outperforms separate generation and retrieval baselines across all metrics
- The decision mechanism reliably selects the better image (generated vs retrieved) in practice
- Generative retrieval is substantially more efficient than dense retrieval methods in real-world scenarios

**Low Confidence** (Weak or missing evidence):
- The specific likelihood thresholds or calibration methods that would optimize the autonomous decision mechanism
- The scalability of the approach to much larger image databases without significant performance degradation
- The actual user preference for images selected by the unified framework versus human-curated results

## Next Checks

1. **Decision Quality Validation**: Conduct human evaluation studies where annotators rate the quality of images selected by the autonomous decision mechanism versus ground truth preferences to verify likelihood-based selection correlates with human judgment.

2. **Scalability Benchmark**: Test TIGeR-ONE on progressively larger image databases (10K, 100K, 1M images) to empirically measure how beam search efficiency and reverse re-ranking costs scale, validating claimed computational advantages.

3. **Ablation on Decision Thresholds**: Systematically vary the decision threshold and decision criteria (e.g., confidence intervals, different likelihood proxies) to identify optimal settings for different types of queries and determine if the current mechanism is truly optimal.