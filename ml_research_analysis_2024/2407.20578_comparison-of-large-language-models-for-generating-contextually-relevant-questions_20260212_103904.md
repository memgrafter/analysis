---
ver: rpa2
title: Comparison of Large Language Models for Generating Contextually Relevant Questions
arxiv_id: '2407.20578'
source_url: https://arxiv.org/abs/2407.20578
tags:
- questions
- llms
- slide
- question
- evaluations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared three large language models (GPT-3.5 Turbo,
  Flan T5 XXL, and Llama 2-Chat 13B) for generating educationally relevant questions
  from university slide text without fine-tuning. Using a two-step pipeline, Llama
  2-Chat 13B extracted answer phrases from slides, which the three models then used
  to generate questions.
---

# Comparison of Large Language Models for Generating Contextually Relevant Questions

## Quick Facts
- arXiv ID: 2407.20578
- Source URL: https://arxiv.org/abs/2407.20578
- Reference count: 10
- Primary result: GPT-3.5 Turbo slightly outperformed other models in generating educationally relevant questions without fine-tuning

## Executive Summary
This study evaluates three large language models (GPT-3.5 Turbo, Flan T5 XXL, and Llama 2-Chat 13B) for generating contextually relevant educational questions from university slide text. Using a two-step pipeline where Llama 2-Chat extracts answer phrases and all three models generate questions, the research found that all models performed well without fine-tuning. GPT-3.5 Turbo and Llama 2-Chat 13B slightly outperformed Flan T5 XXL, particularly in clarity and question-answer alignment. The findings suggest these LLMs have potential for educational applications like personalized learning and diagnostic quizzes.

## Method Summary
The study employed a two-step pipeline for automatic question generation. First, Llama 2-Chat 13B extracted answer phrases from 21 Pattern Recognition course slides at Kyushu University. Then, three LLMs (GPT-3.5 Turbo, Flan T5 XXL, and Llama 2-Chat 13B) generated questions based on these extracted answers. A survey of 46 students evaluated 246 questions across five metrics: clarity, relevance, difficulty, slide relation, and question-answer alignment. The evaluation used a 1-5 scale for each metric, with higher scores indicating better performance.

## Key Results
- All three LLMs successfully generated educationally relevant questions without fine-tuning
- GPT-3.5 Turbo excelled at tailoring questions to match input answers
- Flan T5 XXL encountered more challenges with clarity compared to other models
- Llama 2-Chat 13B provided effective answer extraction that enhanced downstream question quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5 Turbo's superior performance stems from its fine-tuned ability to align generated questions with input answers.
- Mechanism: The model's training on instruction-following tasks enables it to interpret answer phrases and construct semantically consistent questions, reducing misalignment.
- Core assumption: GPT-3.5's instruction tuning directly improves QA alignment over base models like Llama 2-Chat.
- Evidence anchors: [abstract] "GPT-3.5 especially excels at tailoring questions to match the input answers."
- Break condition: If answer phrases are highly ambiguous or multi-topic, GPT-3.5's alignment advantage may diminish due to overgeneralization.

### Mechanism 2
- Claim: Llama 2-Chat 13B's strong performance in answer extraction enables higher-quality downstream question generation.
- Mechanism: By extracting precise, context-rich answer phrases, Llama 2-Chat provides cleaner input for question generation, improving overall clarity and relevance.
- Core assumption: Answer extraction quality directly correlates with final question quality.
- Evidence anchors: [abstract] "Llama 2-Chat 13B extracted answer phrases from slides."
- Break condition: If slide text is poorly structured, even good extraction may yield ambiguous answer phrases that hurt downstream performance.

### Mechanism 3
- Claim: Flan T5 XXL's architecture and pretraining make it more sensitive to context complexity, leading to lower clarity scores.
- Mechanism: As an encoder-decoder model, Flan T5 may struggle with the nuanced context in slide text, producing more verbose or less precise questions.
- Core assumption: Encoder-decoder models like Flan T5 are inherently less effective than decoder-only models like GPT-3.5 in handling complex educational contexts.
- Evidence anchors: [abstract] "Flan T5 XXL encountered slightly more challenges... some of its questions were rated lower for clarity."
- Break condition: If prompts are simplified or fine-tuned, Flan T5's clarity scores could improve significantly.

## Foundational Learning

- Concept: Natural Language Processing (NLP) and Large Language Models (LLMs)
  - Why needed here: Understanding model architectures, training paradigms, and evaluation metrics is essential for interpreting results and designing experiments.
  - Quick check question: What is the key architectural difference between GPT-3.5 and Flan T5 XXL, and how might it affect their question generation quality?

- Concept: Automatic Question Generation (AQG) pipeline
  - Why needed here: The two-step pipeline (answer extraction → question generation) is central to the study's methodology and performance differences.
  - Quick check question: Why is answer extraction considered a crucial first step in the proposed pipeline?

- Concept: Educational content evaluation metrics
  - Why needed here: Understanding clarity, relevance, difficulty, slide relation, and QA alignment is critical for interpreting student survey results.
  - Quick check question: Which metric directly measures how well the generated question matches the provided answer?

## Architecture Onboarding

- Component map: Slide text input → Llama 2-Chat 13B (answer extraction) → Three LLMs (question generation) → Student survey evaluation
- Critical path: Slide text → Answer extraction → Question generation → Evaluation
- Design tradeoffs:
  - Open vs. closed models: Llama 2-Chat and Flan T5 are open but may underperform GPT-3.5 in QA alignment.
  - Fine-tuning vs. zero-shot: Zero-shot works well, but fine-tuning could further improve alignment and clarity.
  - Model size vs. latency: Larger models (e.g., Llama 2-Chat 13B) may offer better quality but increase generation time.
- Failure signatures:
  - Poor answer extraction → Generic or irrelevant questions
  - Low QA alignment → Confusing or misleading questions
  - Low clarity scores → Questions that are hard to understand
- First 3 experiments:
  1. Swap answer extraction models (e.g., use GPT-3.5 for extraction) and compare downstream question quality.
  2. Fine-tune Flan T5 XXL on a small dataset of educational QG examples and measure changes in clarity and QA alignment.
  3. Test different prompt templates for each model to optimize clarity and relevance scores.

## Open Questions the Paper Calls Out

- Question: How would fine-tuning the LLMs impact the quality of generated questions compared to using them without fine-tuning?
  - Basis in paper: [explicit] The paper states "it would be interesting to explore whether fine-tuning would significantly improve the results" and mentions this as future work.
  - Why unresolved: The current study only evaluated the LLMs without any fine-tuning, so there is no empirical evidence on how fine-tuning would affect performance.
  - What evidence would resolve it: Comparative experiments measuring question quality metrics before and after fine-tuning the LLMs on educational question-answer datasets.

- Question: Which prompt formulations lead to the best question quality across different LLMs for educational question generation?
  - Basis in paper: [explicit] The paper states "future work can compare different prompts and describe which of them lead to better or worse questions."
  - Why unresolved: The current study used fixed prompts without exploring variations, so optimal prompt designs remain unknown.
  - What evidence would resolve it: Systematic comparison of multiple prompt variations measuring their impact on question quality metrics across different LLMs.

- Question: How can LLMs be optimized to avoid generating generic "what is" questions and reduce bias towards certain question types?
  - Basis in paper: [explicit] The paper mentions "limiting bias towards certain types of questions (e.g., avoiding generic 'what is' questions)" as a limitation.
  - Why unresolved: The current study observed this tendency but did not explore methods to mitigate it.
  - What evidence would resolve it: Experiments testing different prompting strategies, fine-tuning approaches, or post-processing techniques to diversify question types and reduce generic patterns.

## Limitations

- The study used only 21 slides from a single Pattern Recognition course, limiting generalizability across subjects
- Evaluation relied solely on student surveys rather than expert assessment or objective quality metrics
- The study did not explore prompt optimization or fine-tuning, leaving questions about potential performance improvements
- Lack of detailed prompt specifications and model configurations makes exact reproduction challenging

## Confidence

- **High Confidence**: The finding that all three models can generate educationally relevant questions without fine-tuning is well-supported by the experimental results and student evaluations.
- **Medium Confidence**: The ranking of models (GPT-3.5 ≈ Llama 2-Chat > Flan T5) is supported but could shift with different prompts, datasets, or evaluation methods.
- **Low Confidence**: The attribution of specific performance differences to particular mechanisms (e.g., GPT-3.5's alignment ability, Llama 2-Chat's extraction quality) lacks direct experimental validation and relies on post-hoc interpretation.

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary prompts across all three models while keeping the dataset constant to determine how much performance differences stem from model architecture versus prompt engineering.

2. **Expert Validation**: Re-evaluate the same question sets using subject matter experts rather than students to assess whether the student ratings align with educational quality standards.

3. **Cross-Domain Generalization**: Test the same pipeline on slides from different academic disciplines (e.g., humanities, social sciences) to evaluate whether the observed performance patterns hold across varied educational content.