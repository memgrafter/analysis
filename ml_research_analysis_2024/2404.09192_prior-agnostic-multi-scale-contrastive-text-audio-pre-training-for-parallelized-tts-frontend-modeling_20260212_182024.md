---
ver: rpa2
title: Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized
  TTS Frontend Modeling
arxiv_id: '2404.09192'
source_url: https://arxiv.org/abs/2404.09192
tags:
- audio
- frontend
- text
- learning
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving text-to-speech (TTS)
  frontend modeling, which involves tasks like text normalization, prosody boundary
  prediction, and polyphone disambiguation. The proposed method, TAP-FM, introduces
  a two-stage pipeline that incorporates multi-scale contrastive text-audio pre-training
  (MC-TAP) to learn fine-grained representations from both text and audio data.
---

# Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling

## Quick Facts
- arXiv ID: 2404.09192
- Source URL: https://arxiv.org/abs/2404.09192
- Reference count: 0
- Primary result: State-of-the-art performance on TTS frontend multi-task predictions (TN, PBP, PD) with significant improvements in F1-scores and accuracy

## Executive Summary
This paper addresses the challenge of improving text-to-speech (TTS) frontend modeling by proposing a two-stage pipeline called TAP-FM. The first stage employs multi-scale contrastive text-audio pre-training (MC-TAP) to learn fine-grained representations from both text and audio data through span-level and sentence-level contrastive learning. The second stage introduces a parallelized frontend model that leverages the pre-trained text encoder and employs a modified dynamic weight averaging (DWA+) technique to handle imbalanced and non-uniform data across multiple tasks. Experimental results on benchmark datasets demonstrate the superiority of TAP-FM, achieving state-of-the-art performance with significant improvements in accuracy and F1-scores across all tasks.

## Method Summary
TAP-FM introduces a two-stage pipeline for TTS frontend modeling. In the first stage, MC-TAP pre-trains a model using span-level and sentence-level contrastive learning to align text and audio representations without manual alignment. The span-level approach uses overlapping sliding windows and dynamic programming to find monotonic alignments, while sentence-level learning contrasts representations across speakers. In the second stage, a parallelized frontend model leverages the pre-trained text encoder with ResConformer for improved gradient propagation, and uses modified DWA+ to balance learning across three tasks: text normalization (TN), prosody boundary prediction (PBP), and polyphone disambiguation (PD).

## Key Results
- TAP-FM achieves state-of-the-art performance on TTS frontend multi-task predictions
- Significant improvements in F1-scores and accuracy across TN, PBP, and PD tasks
- Superior performance compared to baseline models including BERT-based, SpeechT5, and UFEF approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MC-TAP learns fine-grained text-audio representations by automatically aligning spans using dynamic programming, eliminating manual alignment overhead
- Mechanism: The span-level contrastive learning uses overlapping sliding windows on both text and audio features, then employs a dynamic programming algorithm to find the longest increasing subsequence (LIS) of pairing indices, generating alignment labels without forced alignment tools
- Core assumption: Monotonic alignment exists between text subwords and audio segments, allowing LIS-based automatic labeling to approximate true alignment
- Evidence anchors:
  - [abstract]: "Instead of using data preprocessing to align text & audio segments in previous works, we propose a novel fully-automatic labeling alignment mechanism by resorting to the dynamic programming algorithm"
  - [section]: "Inspired by CLIP (Radford et al., 2021), a span-level pairing matrix... Our objective is to integrate text and audio characteristics... Thankfully, we can leverage the monotonic increasing alignment relationship between text subwords and audio segments"
  - [corpus]: Weak - no direct mention of CLIP in corpus titles/abstracts, but CLIP-style contrastive learning is implied
- Break condition: Monotonic alignment assumption fails in cases of significant audio-text misalignment (e.g., disfluencies, non-speech audio), causing LIS to produce incorrect alignments

### Mechanism 2
- Claim: Sentence-level contrastive learning enriches speaker-discriminative features, improving generalization across different speakers
- Mechanism: For each text-audio pair, MC-TAP samples a positive audio example from the same speaker and a negative audio example from a different speaker, then contrasts their global representations against the text representation using cosine similarity loss
- Core assumption: Speaker identity is a meaningful dimension for contrastive learning in TTS, and sampling from different speakers provides useful negative examples
- Evidence anchors:
  - [abstract]: "Sentence-level contrastive learning endows the model with higher discriminability among different speakers and stronger generalizability"
  - [section]: "To gain a more extensive and diverse understanding of different speakers, we have designed a global contrastive learning task at the sentence level... This approach allows us to effectively contrast and compare different speakers"
  - [corpus]: Weak - corpus focuses on TTS frontend modeling but doesn't explicitly mention speaker-level contrastive learning
- Break condition: If speaker identity is not a meaningful dimension for the target TTS application, or if speaker sampling is not representative

### Mechanism 3
- Claim: The parallelized frontend model with ResConformer and DWA+ effectively handles imbalanced multi-task learning across TN, PBP, and PD tasks
- Mechanism: The model uses a pre-trained TextEncoder from MC-TAP for all tasks, adds ResConformer to improve gradient propagation, and employs modified DWA+ with convergence coefficients to balance task-specific learning rates during joint fine-tuning
- Core assumption: The imbalance and non-uniformity across tasks can be addressed through adaptive weighting, and residual connections improve convergence for the multi-task setting
- Evidence anchors:
  - [abstract]: "Furthermore, a parallelized TTS frontend model is delicately devised to execute TN, PD, and PBP prediction tasks, respectively in the second stage"
  - [section]: "To deal with TN, PBP, and PD tasks in parallel, we establish a framework based on the TextEncoder in MC-TAP... the improved Dynamic Weight Averaging (DWA) (Liu et al., 2019) is introduced to suppress the negative effects brought from distinct data sources for diverse tasks"
  - [corpus]: Weak - corpus contains related TTS papers but doesn't specifically mention DWA+ or ResConformer adaptations
- Break condition: If task imbalance is extreme or task relationships are incompatible, DWA+ may fail to find effective weightings

## Foundational Learning

- Concept: Contrastive learning for multimodal representation alignment
  - Why needed here: The core innovation relies on learning text-audio alignments without manual supervision, requiring understanding of contrastive learning principles and how they apply to multimodal data
  - Quick check question: How does the span-level contrastive loss differ from standard image-text contrastive learning like CLIP?

- Concept: Dynamic programming for sequence alignment
  - Why needed here: The automatic alignment mechanism uses LIS finding via dynamic programming, which is critical to understanding how the model generates training labels without manual alignment
  - Quick check question: What properties of the text-audio alignment problem make the LIS approach valid?

- Concept: Multi-task learning with imbalanced datasets
  - Why needed here: The parallelized frontend handles three tasks with different data characteristics, requiring knowledge of techniques like DWA+ for balancing task-specific learning
  - Quick check question: How does DWA+ differ from standard DWA, and why is the convergence coefficient necessary?

## Architecture Onboarding

- Component map: Text/audio input → MC-TAP pre-training (span-level + sentence-level + MLM) → TextEncoder weights frozen → Frontend model training with DWA+ → Task predictions

- Critical path: Text/audio input → MC-TAP pre-training (span-level + sentence-level + MLM) → TextEncoder weights frozen → Frontend model training with DWA+ → Task predictions

- Design tradeoffs:
  - Using WavLM frozen vs. fine-tuning: Faster training but potentially less task-specific audio features
  - Span-level vs. segment-level alignment: More fine-grained but computationally heavier
  - Parallel vs. sequential frontend: More efficient but requires careful balancing of task interactions

- Failure signatures:
  - Poor TN performance despite good MC-TAP pre-training: Indicates text features aren't capturing NSW patterns effectively
  - PBP/PD tasks not improving: Suggests audio features aren't providing useful prosody/pronunciation information
  - Training instability with DWA+: Imbalanced task weights or incompatible task gradients

- First 3 experiments:
  1. Validate automatic alignment quality: Compare cosine similarity matrices between BERT and MC-TAP on held-out text-audio pairs
  2. Test component ablation: Train frontend without span-level contrastive learning, sentence-level contrastive learning, and MLM separately
  3. Evaluate task-specific performance: Measure F1/accuracy for each task independently to identify which components most impact which tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the TAP-FM model compare when using different pre-trained language models, such as RoBERTa or XLNet, as the TextEncoder instead of BERT?
- Basis in paper: [explicit] The paper mentions using BERT as the TextEncoder in MC-TAP, but does not explore the use of other pre-trained language models
- Why unresolved: The paper does not provide a comparison of the performance of TAP-FM when using different pre-trained language models as the TextEncoder
- What evidence would resolve it: Conducting experiments using different pre-trained language models as the TextEncoder in MC-TAP and comparing their performance in the TAP-FM model would provide evidence to resolve this question

### Open Question 2
- Question: How does the performance of the TAP-FM model change when using different types of audio encoders, such as Wav2Vec 2.0 or HuBERT, instead of WavLM?
- Basis in paper: [explicit] The paper mentions using WavLM as the AudioEncoder in MC-TAP, but does not explore the use of other audio encoders
- Why unresolved: The paper does not provide a comparison of the performance of TAP-FM when using different audio encoders
- What evidence would resolve it: Conducting experiments using different audio encoders in MC-TAP and comparing their performance in the TAP-FM model would provide evidence to resolve this question

### Open Question 3
- Question: How does the performance of the TAP-FM model change when using different loss weighting strategies, such as dynamic loss scaling or focal loss, instead of the modified DWA+ technique?
- Basis in paper: [explicit] The paper mentions using a modified DWA+ technique to address imbalanced and non-uniform data across multiple tasks, but does not explore other loss weighting strategies
- Why unresolved: The paper does not provide a comparison of the performance of TAP-FM when using different loss weighting strategies
- What evidence would resolve it: Conducting experiments using different loss weighting strategies in the TAP-FM model and comparing their performance would provide evidence to resolve this question

## Limitations

- The automatic alignment mechanism relies on the monotonic alignment assumption between text and audio, which may not hold in all real-world scenarios involving disfluencies, non-speech audio, or significant speaker variation
- The effectiveness of the span-level contrastive learning depends on the quality of automatically generated alignment labels, which could be compromised if the LIS-based approach produces incorrect alignments
- The modified DWA+ approach for handling task imbalance requires careful tuning of convergence coefficients, and its effectiveness may vary depending on the specific task distribution and data characteristics

## Confidence

- High confidence: The two-stage pipeline architecture (MC-TAP pre-training followed by parallelized frontend) is well-established in the literature
- Medium confidence: The span-level contrastive learning mechanism with automatic alignment using dynamic programming is innovative but depends critically on the monotonic alignment assumption
- Medium confidence: The modified DWA+ for multi-task learning is theoretically sound but requires empirical validation for the specific task combination

## Next Checks

1. **Alignment quality validation**: Implement a human evaluation study comparing automatic alignments generated by the LIS-based method against ground truth alignments on a subset of the dataset to quantify alignment accuracy
2. **Ablation study of contrastive components**: Systematically remove each contrastive learning component (span-level, sentence-level, MLM) and measure the impact on each frontend task to understand component contributions
3. **Task balance sensitivity analysis**: Vary the convergence coefficients in DWA+ across a range of values and measure how task performance trade-offs change, identifying optimal balance points for different task importance weights