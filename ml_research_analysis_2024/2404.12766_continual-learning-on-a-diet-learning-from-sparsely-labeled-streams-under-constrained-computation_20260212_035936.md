---
ver: rpa2
title: 'Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under
  Constrained Computation'
arxiv_id: '2404.12766'
source_url: https://arxiv.org/abs/2404.12766
tags:
- learning
- data
- budget
- time
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel continual learning setting, termed
  "CL on Diet," where algorithms operate under constrained computational budgets and
  sparse label availability. Existing methods, both supervised and semi-supervised,
  struggle in this regime due to overfitting and insufficient budget allocation.
---

# Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under Constrained Computation

## Quick Facts
- arXiv ID: 2404.12766
- Source URL: https://arxiv.org/abs/2404.12766
- Reference count: 19
- Key outcome: DietCL outperforms existing methods by 2-4% accuracy on large-scale datasets under computational budget constraints and sparse labels.

## Executive Summary
This paper introduces a novel continual learning setting termed "CL on Diet," where algorithms must learn from sparsely labeled data streams under strict computational budget constraints. The authors identify that existing continual learning methods fail in this regime due to insufficient budget allocation and overfitting to sparse labeled data. To address these challenges, they propose DietCL, a simple yet effective baseline that jointly optimizes labeled and unlabeled data while employing a budget allocation mechanism to balance learning from current and previous distributions. DietCL leverages masked modeling (MAE) for efficient feature learning and demonstrates robust performance across varying label rates, computational budgets, and stream lengths.

## Method Summary
DietCL addresses continual learning under constrained computation by jointly training on labeled data (current task classification and buffer replay) and unlabeled data (MAE reconstruction) simultaneously. The method employs a budget allocation mechanism that divides computational resources equally among labeled data, unlabeled data, and buffer data when the total budget is below a threshold B. When budget exceeds B, extra steps are dedicated to buffer data to balance learning of current and previous classes. The approach uses a Vision Transformer encoder with MAE decoder for reconstruction, a classification head with masking for current task classes, and a balanced buffer storing labeled data from all previous tasks.

## Key Results
- DietCL outperforms existing methods by 2-4% in accuracy on large-scale datasets (ImageNet10k, CLOC, CGLM)
- Demonstrates robustness across varying label rates, computational budgets, and stream lengths
- Shows that joint optimization prevents catastrophic forgetting better than two-stage pre-training + fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Budget allocation between labeled and unlabeled data prevents overfitting while maintaining learning efficiency.
- Mechanism: DietCL allocates equal computational budget initially to labeled data, unlabeled data, and balanced buffer. When budget exceeds threshold B, extra steps are dedicated to buffer data to balance learning of current and previous classes.
- Core assumption: Labeled data is sparse and prone to overfitting; unlabeled data provides regularization and distribution information.
- Evidence anchors: [abstract] "DietCL employs a budget allocation mechanism to balance learning from current and previous distributions"; [section 4] "We divide it equally among the labeled (Lm), unlabeled (Lr), and buffer data (Lb) if the budget is less than a threshold B"
- Break condition: If unlabeled data does not provide meaningful regularization or if label rate becomes high enough that overfitting is no longer a primary concern.

### Mechanism 2
- Claim: Masked modeling (MAE) is more budget-efficient than contrastive learning for capturing current distribution.
- Mechanism: DietCL uses MAE reconstruction loss instead of contrastive learning because contrastive methods require two augmented views and two separate backbone updates, halving the effective budget utilization.
- Core assumption: Computational budget is the primary constraint, not the quality of self-supervised learning method.
- Evidence anchors: [section 4] "contrastive learning algorithms typically necessitate augmenting input images into dual views and updating gradients for two distinct backbones. Consequently, with a budget of Bu for unlabeled data, only Bu/2 gradient steps are feasible"
- Break condition: If computational budget is not constrained or if contrastive methods become more efficient.

### Mechanism 3
- Claim: Joint optimization of labeled and unlabeled data provides better generalization than two-stage pre-training + fine-tuning.
- Mechanism: DietCL jointly trains on labeled data (current task classification and buffer replay) and unlabeled data (MAE reconstruction) simultaneously, rather than separating pre-training and fine-tuning stages.
- Core assumption: Two-stage approaches cause representation drift that harms previous task performance.
- Evidence anchors: [section 4] "we jointly learn from the current unlabeled and labeled data, where the dense unlabeled data acts as a regularizer to prevent overfitting to the sparse labeled data"; [section C.2] "TwoStage method improves the classification accuracy of the samples from the new classes... However, we did not observe any advantages of the pre-training stage when measuring the performance among all seen classes"
- Break condition: If label rate becomes high enough that two-stage approaches no longer cause significant representation drift.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper addresses continual learning where models must retain knowledge from previous tasks while learning new ones
  - Quick check question: What happens to a neural network's performance on previous tasks when trained on new tasks without any mitigation strategy?

- Concept: Self-supervised learning (SSL) methods
  - Why needed here: DietCL uses MAE (a masked modeling SSL method) to learn from unlabeled data efficiently
  - Quick check question: How does masked autoencoding differ from contrastive learning in terms of computational requirements?

- Concept: Budget-constrained optimization
  - Why needed here: The core challenge is learning under restricted computational resources per time step
  - Quick check question: What is the relationship between FLOPs, gradient steps, and effective learning time in a constrained budget scenario?

## Architecture Onboarding

- Component map: Vision Transformer encoder -> MAE decoder -> Classification head with masking -> Balanced buffer -> Budget allocation controller

- Critical path:
  1. Data loading and preprocessing (labeled and unlabeled streams)
  2. Budget allocation decision (based on total available budget and threshold B)
  3. Joint forward pass through encoder and decoder
  4. Computation of three losses: reconstruction (unlabeled), current task classification (labeled), and buffer replay (labeled)
  5. Backpropagation and parameter update
  6. Buffer management and update

- Design tradeoffs:
  - Joint training vs two-stage pre-training/fine-tuning: Joint training prevents representation drift but may be harder to optimize
  - MAE vs contrastive learning: MAE is more budget-efficient but may capture different features
  - Fixed vs adaptive budget allocation: Fixed allocation is simpler but may not adapt to changing task difficulty

- Failure signatures:
  - Poor performance on recent tasks: May indicate insufficient budget allocation to current task learning
  - Catastrophic forgetting: May indicate buffer management issues or insufficient replay budget
  - Slow convergence: May indicate inefficient SSL method choice or suboptimal learning rate

- First 3 experiments:
  1. Ablation study removing the reconstruction loss (Lr) to verify unlabeled data contribution
  2. Varying the budget threshold B to find optimal allocation between current and buffer learning
  3. Comparison with two-stage pre-training + fine-tuning approach to validate joint optimization benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal computational budget threshold for transitioning from joint training to buffer-only fine-tuning in different continual learning scenarios?
- Basis in paper: [explicit] The authors mention that they choose a threshold B by cross-validation, but the selection criteria and sensitivity to different datasets or stream characteristics are not fully explored.
- Why unresolved: The paper only briefly mentions cross-validation for threshold selection without providing detailed guidelines or analyzing how this threshold varies across different experimental conditions.
- What evidence would resolve it: A comprehensive analysis showing how the optimal threshold changes with factors like label rate, stream length, dataset characteristics, and computational budget would help establish guidelines for threshold selection.

### Open Question 2
- Question: How does the performance of DietCL scale with increasing number of classes and dataset complexity?
- Basis in paper: [inferred] While the authors demonstrate DietCL's effectiveness on large-scale datasets like ImageNet10k, CLOC, and CGLM, the paper does not explicitly explore performance trends as the number of classes or dataset complexity increases beyond these benchmarks.
- Why unresolved: The experiments focus on specific dataset configurations without systematically varying class counts or complexity to understand scalability limits.
- What evidence would resolve it: Experiments varying the number of classes, dataset complexity, and stream characteristics beyond the current benchmarks would provide insights into DietCL's scalability and potential limitations.

### Open Question 3
- Question: What is the impact of different self-supervised learning methods on DietCL's performance under varying computational budgets?
- Basis in paper: [explicit] The authors choose MAE for masked modeling due to its efficiency, but acknowledge that contrastive learning could be an alternative despite higher computational costs.
- Why unresolved: The paper does not provide a direct comparison between different SSL methods or explore how their performance varies under different budget constraints.
- What evidence would resolve it: A systematic comparison of various SSL methods (e.g., MAE, contrastive learning, SimCLR) across different computational budgets and label rates would clarify the trade-offs and optimal choices for different scenarios.

### Open Question 4
- Question: How does DietCL's performance change with different buffer management strategies and buffer sizes?
- Basis in paper: [inferred] While the authors mention maintaining a task-balanced buffer, they do not explore alternative buffer management strategies or analyze the impact of buffer size on performance.
- Why unresolved: The paper assumes an unlimited buffer size but does not investigate how different buffer management strategies or size constraints might affect DietCL's effectiveness.
- What evidence would resolve it: Experiments varying buffer sizes, implementing different sampling strategies, and comparing fixed-size versus growing buffers would provide insights into the role of buffer management in DietCL's performance.

## Limitations
- The computational efficiency claims are primarily theoretical, with limited empirical validation of actual FLOPs savings versus contrastive methods.
- The budget allocation mechanism uses a fixed threshold (B) without justification for why this specific value was chosen or how it performs across different dataset characteristics.
- The buffer management strategy is not fully specified, leaving questions about how samples are selected for storage and replacement.

## Confidence
- High confidence: The core observation that existing methods struggle in low-budget, sparse-label regimes is well-supported by experimental results showing 2-4% accuracy gaps.
- Medium confidence: The mechanism that joint optimization prevents overfitting is supported by ablation studies but lacks detailed analysis of why the two-stage approach fails.
- Low confidence: The claim that MAE is inherently more budget-efficient than contrastive learning is based on theoretical arguments rather than measured computational savings.

## Next Checks
1. Conduct ablation studies varying the budget threshold B across multiple orders of magnitude to understand its impact on learning dynamics and identify optimal allocation strategies.
2. Measure actual FLOPs consumed by MAE versus contrastive learning implementations to validate the theoretical computational efficiency claims with concrete data.
3. Analyze buffer content over training time to verify that stored samples represent the most informative examples and that the current sampling strategy effectively prevents catastrophic forgetting.