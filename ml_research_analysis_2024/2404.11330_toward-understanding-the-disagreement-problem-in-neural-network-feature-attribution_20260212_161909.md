---
ver: rpa2
title: Toward Understanding the Disagreement Problem in Neural Network Feature Attribution
arxiv_id: '2404.11330'
source_url: https://arxiv.org/abs/2404.11330
tags:
- methods
- feature
- attribution
- neural
- effects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the "disagreement problem" in feature attribution
  methods for neural networks, where different methods produce inconsistent explanations.
  The authors categorize feature attribution methods into four groups based on their
  underlying principles and analyze their behavior through comprehensive simulation
  studies.
---

# Toward Understanding the Disagreement Problem in Neural Network Feature Attribution

## Quick Facts
- arXiv ID: 2404.11330
- Source URL: https://arxiv.org/abs/2404.11330
- Authors: Niklas Koenen; Marvin N. Wright
- Reference count: 40
- Key outcome: This paper addresses the "disagreement problem" in feature attribution methods for neural networks, where different methods produce inconsistent explanations.

## Executive Summary
This study investigates why different feature attribution methods produce inconsistent explanations for neural network predictions. The authors categorize methods into four groups based on their underlying principles and demonstrate that disagreements stem primarily from different baseline values used by the methods rather than differences in explanation quality. Through comprehensive simulation studies, they show that most state-of-the-art methods produce explanations that correlate well with ground-truth effects when proper preprocessing is applied, and that Shapley-based methods consistently outperform others in both effect attribution and feature importance identification.

## Method Summary
The study uses synthetic datasets generated through additive models with independent variables, where ground-truth effects are known. Neural networks with three dense layers (256, 128, 64 neurons) using ReLU activations and dropout are trained on these datasets. Feature attribution methods are implemented using the R package innsight, and performance is evaluated using Pearson correlation between attributions and ground-truth effects, along with F1-scores for feature importance discrimination. The analysis focuses on understanding how different methods answer fundamentally different questions about feature importance based on their choice of baseline/reference value.

## Key Results
- Most state-of-the-art methods produce explanations that correlate well with ground-truth effects when using z-score scaling for continuous variables and one-hot encoding for categorical variables
- Prediction-sensitive methods (Grad, SmoothGrad) excel at distinguishing important and unimportant features globally but fail at attributing specific effect magnitudes locally
- Shapley-based methods, particularly DeepLIFT-RC, consistently outperform other methods in both attributing effects and identifying important features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The disagreement problem in feature attribution stems primarily from different baseline values used by methods rather than differences in explanation quality.
- **Mechanism:** Different feature attribution methods answer fundamentally different questions about feature importance based on their choice of baseline/reference value. Methods using zero baseline (GxI, LRP-0) measure effects relative to zero, while methods using mean baseline (IntGrad with mean, DeepSHAP) measure effects relative to average feature values. Shapley-based methods measure effects relative to the marginal effect across the entire feature distribution.
- **Core assumption:** The same feature can have different relevance magnitudes depending on the baseline, but the relative importance ranking should remain consistent if methods are answering the same underlying question.
- **Evidence anchors:**
  - [abstract] "Key findings include: 1) Most state-of-the-art methods, when using z-score scaling for continuous variables and one-hot encoding for categorical variables, produce explanations that correlate well with ground-truth effects as effect strength increases."
  - [section 3] "This shows, in particular, that features with a high assigned magnitude can generally be less relevant or even irrelevant for another baseline."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- **Break condition:** When feature distributions are highly skewed or contain outliers, different baselines may lead to fundamentally different relative importance rankings, not just magnitude differences.

### Mechanism 2
- **Claim:** Prediction-sensitive methods (Grad, SmoothGrad) excel at distinguishing important and unimportant features globally but fail at attributing specific effect magnitudes locally.
- **Mechanism:** These methods calculate output sensitivity (partial derivatives) rather than effect decomposition. They highlight features that cause large changes in predictions when perturbed, making them excellent for binary classification of importance but poor for quantifying contribution magnitudes.
- **Core assumption:** Global feature importance can be determined by sensitivity to perturbations even when local effect magnitudes are incorrect.
- **Evidence anchors:**
  - [abstract] "Prediction-sensitive methods (e.g., Grad, SmoothGrad) are not suitable for effect decomposition but excel at distinguishing important and unimportant features."
  - [section 4.3] "While prediction-sensitive methods may falter in correctly attributing relevances, they performed well in determining whether or not a feature is important."
  - [corpus] Weak - corpus doesn't provide direct evidence for this specific mechanism
- **Break condition:** When features have non-linear relationships with predictions or when feature importance depends on interactions with other features.

### Mechanism 3
- **Claim:** Proper data preprocessing (z-score scaling for continuous, one-hot encoding for categorical) enables most state-of-the-art methods to produce proportional explanation distributions that correlate with ground-truth effects.
- **Mechanism:** Standardization removes arbitrary scale differences between features and creates a neutral reference point. One-hot encoding avoids introducing artificial ordinal relationships. These preprocessing steps ensure that different methods are comparing features on the same scale and answering comparable questions.
- **Core assumption:** Feature attribution methods are scale-dependent and sensitive to the choice of encoding scheme.
- **Evidence anchors:**
  - [abstract] "Most state-of-the-art methods, when using z-score scaling for continuous variables and one-hot encoding for categorical variables, produce explanations that correlate well with ground-truth effects as effect strength increases."
  - [section 4.1] "We found that, except for prediction-sensitive methods, standard normalization techniques such as z-score scaling for continuous and one-hot encoding for categorical variables provide the most accurate attributions of local effects."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- **Break condition:** When features have fundamentally different distributions that cannot be adequately standardized, or when encoding choices interact with model architecture in complex ways.

## Foundational Learning

- **Concept: Taylor approximation in feature attribution**
  - Why needed here: Many feature attribution methods (GxI, LRP) rely on first-order Taylor approximations to decompose predictions, making understanding their limitations crucial.
  - Quick check question: Why do fixed-reference methods struggle with non-linear effects, and how does this relate to the quality of Taylor approximation?

- **Concept: Shapley values and their approximation in neural networks**
  - Why needed here: DeepSHAP and ExpGrad approximate Shapley values efficiently for neural networks, and understanding their theoretical foundation explains their superior performance.
  - Quick check question: How do Shapley-based methods incorporate multiple baseline values to avoid out-of-distribution problems that plague single-reference methods?

- **Concept: Feature importance vs. feature attribution distinction**
  - Why needed here: The paper distinguishes between measuring local effects (attribution) and global importance (ranking), which is critical for interpreting results correctly.
  - Quick check question: Why can a feature with zero relevance under one baseline still be globally important, and how does this affect interpretation?

## Architecture Onboarding

- **Component map:** Data preprocessing pipeline (scaling, encoding) -> Neural network training module -> Feature attribution method implementation -> Ground-truth effect generation -> Correlation and ranking evaluation metrics -> Visualization and comparison tools

- **Critical path:**
  1. Generate synthetic data with known ground-truth effects
  2. Preprocess data using appropriate scaling/encoding
  3. Train neural network to learn the ground-truth relationships
  4. Apply multiple feature attribution methods
  5. Compare attributions to ground-truth effects using correlation metrics
  6. Evaluate global importance using ranking-based metrics

- **Design tradeoffs:**
  - Single-reference vs. multiple-reference methods: Single-reference methods are computationally efficient but sensitive to baseline choice; multiple-reference methods are more robust but computationally expensive
  - Prediction sensitivity vs. effect decomposition: Sensitivity methods excel at importance detection but fail at magnitude attribution
  - Computational cost vs. explanation quality: Shapley-based methods provide the most accurate attributions but require significant computational resources

- **Failure signatures:**
  - Inconsistent attributions across different baselines suggest the method is baseline-dependent
  - Poor correlation with ground-truth effects indicates the method fails to capture the true relationships
  - Excellent global importance detection but poor local attribution suggests a prediction-sensitive method
  - Method performance degradation with non-linear effects indicates reliance on linear approximations

- **First 3 experiments:**
  1. Implement the linear data-generating process (Y = X1 + X2 + X3² + X4 + ε) and verify that all methods except prediction-sensitive ones produce proportional distributions
  2. Test the impact of different baselines (zero vs. mean) on the same method to demonstrate baseline sensitivity
  3. Compare method performance on z-score scaled vs. raw continuous features to demonstrate preprocessing importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Shapley-based feature attribution methods perform on neural networks with interaction effects between features?
- Basis in paper: [inferred] The paper mentions that investigating methods for interaction effects remains an attractive direction for future work, building upon the theoretical groundwork already explored by Deng et al. [61].
- Why unresolved: The paper focuses on simulations with independent variables and does not explore the performance of Shapley-based methods on neural networks with interaction effects.
- What evidence would resolve it: Conducting simulations with neural networks that include interaction effects between features and comparing the performance of Shapley-based methods to other feature attribution methods.

### Open Question 2
- Question: How do feature attribution methods perform on neural networks with correlated features?
- Basis in paper: [inferred] The paper mentions that the simulations are based on simple synthetic data without correlated features.
- Why unresolved: The paper does not explore the performance of feature attribution methods on neural networks with correlated features, which is a common scenario in real-world datasets.
- What evidence would resolve it: Conducting simulations with neural networks that include correlated features and comparing the performance of different feature attribution methods.

### Open Question 3
- Question: How do feature attribution methods perform on different types of neural network architectures, such as convolutional neural networks or attention modules?
- Basis in paper: [inferred] The paper mentions that dense layers serve as a fundamental building block for many modern deep neural networks, such as convolutional neural networks or attention modules.
- Why unresolved: The paper focuses on dense neural networks and does not explore the performance of feature attribution methods on different types of neural network architectures.
- What evidence would resolve it: Conducting experiments with different types of neural network architectures, such as convolutional neural networks or attention modules, and comparing the performance of feature attribution methods across these architectures.

## Limitations

- The study relies on synthetic data with controlled ground-truth effects, which may not fully capture the complexity of real-world data distributions and model behaviors
- Results are based on a specific neural network architecture (three-layer dense network) and may vary with different architectures, particularly those with convolutional or recurrent layers
- The computational cost of Shapley-based methods, while acknowledged, may be prohibitive for large-scale applications

## Confidence

- **High confidence:** The categorization of feature attribution methods into four groups based on underlying principles is well-founded and supported by the theoretical foundations of each method.
- **Medium confidence:** The superiority of Shapley-based methods is demonstrated through correlation metrics but requires validation on more diverse datasets and architectures.
- **Medium confidence:** The importance of proper preprocessing (z-score scaling and one-hot encoding) is supported by experimental results but may not generalize to all feature distributions.

## Next Checks

1. **Architecture generalization test**: Apply the same experimental protocol to a CNN architecture for image data and a Transformer architecture for text data to assess method performance across different model types.

2. **Real-world data validation**: Replicate key experiments using established benchmark datasets (e.g., UCI datasets, ImageNet) with ground-truth feature importance established through domain expertise or ablation studies.

3. **Computational efficiency analysis**: Systematically measure the runtime and memory requirements of different methods across various dataset sizes to quantify the practical tradeoffs between accuracy and computational cost.