---
ver: rpa2
title: Towards Generalist Prompting for Large Language Models by Mental Models
arxiv_id: '2402.18252'
source_url: https://arxiv.org/abs/2402.18252
tags:
- mental
- prompting
- reasoning
- llms
- memo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of generalist prompting for large
  language models (LLMs), aiming to achieve optimal or near-optimal performance across
  diverse tasks without requiring manual prompt selection or customization. The proposed
  MeMo (Mental Models) method distills the cores of various prompting techniques into
  individual mental models and enables LLMs to autonomously select the most suitable
  ones for a given problem.
---

# Towards Generalist Prompting for Large Language Models by Mental Models

## Quick Facts
- arXiv ID: 2402.18252
- Source URL: https://arxiv.org/abs/2402.18252
- Authors: Haoxiang Guan; Jiyan He; Shuxin Zheng; En-Hong Chen; Weiming Zhang; Nenghai Yu
- Reference count: 40
- Primary result: MeMo method achieves state-of-the-art or near-state-of-the-art results on diverse tasks including STEM, logical reasoning, and commonsense reasoning in zero-shot settings

## Executive Summary
This paper introduces MeMo (Mental Models), a generalist prompting approach that enables large language models to autonomously select and apply appropriate mental models for diverse tasks without manual prompt customization. The method distills various prompting techniques into individual mental models and provides LLMs with definitions and examples of these models. By doing so, MeMo allows LLMs to leverage their pre-trained knowledge and problem context to infer the best mental models to apply, achieving or closely approaching state-of-the-art performance across different task domains.

## Method Summary
MeMo introduces the concept of mental models to LLMs by providing them with definitions and examples of various cognitive frameworks. When presented with a problem, the LLM identifies the most suitable mental models from the provided set and applies them to solve the task. The approach is evaluated across multiple task types including logical reasoning, STEM, and commonsense reasoning using GPT-3.5, GPT-4, and Llama-2 models in zero-shot settings. The method is compared against baseline prompting techniques such as Direct Query, Chain-of-Thought, Take a Deep Breath, and Step-Back.

## Key Results
- MeMo demonstrates or closely approaches state-of-the-art results on diverse tasks including STEM, logical reasoning, and commonsense reasoning
- The method shows superior generalization capabilities compared to other prompting methods, with improvements of up to 13% on certain tasks
- Component ablation studies reveal that both the definition and examples of mental models contribute to performance, though their significance varies across different tasks

## Why This Works (Mechanism)

### Mechanism 1
LLMs can autonomously select and apply mental models to diverse tasks without manual prompt customization. The MeMo method provides LLMs with definitions and examples of mental models, enabling them to recognize patterns in problems and match them to appropriate cognitive frameworks. Core assumption: LLMs have sufficient pre-trained knowledge to understand the definitions of mental models and can map problem contexts to these mental models.

### Mechanism 2
Using diverse mental models improves generalization across different task types compared to single-prompting methods. By providing access to multiple problem-solving frameworks (first principles thinking, analogy, cause-and-effect analysis, etc.), LLMs can adapt their reasoning approach to task-specific requirements. Core assumption: Different types of problems benefit from different cognitive approaches, and a single prompting method cannot optimize performance across all domains.

### Mechanism 3
The component ablation study demonstrates that both the definition and examples of mental models are necessary for optimal performance. Definitions provide conceptual understanding while examples show practical application, creating a complete learning framework for the LLM. Core assumption: LLMs learn mental models through both conceptual definitions and concrete examples, similar to human learning patterns.

## Foundational Learning

- Concept: Mental models as cognitive frameworks for problem-solving
  - Why needed here: Understanding what mental models are and how they function is essential to grasp why MeMo works
  - Quick check question: What distinguishes mental models from other prompting techniques?

- Concept: Chain-of-Thought and Step-Back prompting as specific mental models
  - Why needed here: These existing methods serve as examples of how specific cognitive approaches can be encoded as prompts
  - Quick check question: How do CoT and SB prompting differ in their problem-solving approaches?

- Concept: Zero-shot versus few-shot prompting
  - Why needed here: MeMo is a zero-shot method, so understanding the distinction is important for evaluating its design
  - Quick check question: What key difference distinguishes zero-shot from few-shot prompting in terms of required input?

## Architecture Onboarding

- Component map: Provide mental model definitions and examples -> Present problem to LLM -> LLM identifies relevant mental models -> LLM applies selected mental models to solve problem
- Critical path: The core workflow is: provide mental model definitions → present problem → LLM identifies relevant mental models → LLM applies selected mental models to solve problem
- Design tradeoffs: Longer prompts with comprehensive mental model definitions improve performance but increase computational cost; simpler prompts reduce cost but may limit effectiveness
- Failure signatures: Poor performance on tasks where no suitable mental model exists in the provided set; inconsistent mental model selection across similar problems; excessive computation time due to prompt length
- First 3 experiments:
  1. Ablation study comparing performance with only definitions, only examples, and both components combined
  2. Comparison of MeMo versus specialized prompting methods on tasks where each specialized method excels
  3. Analysis of mental model selection patterns across different task domains to verify appropriate matching

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational cost of MeMo scale with the number of mental models included, and what is the optimal number of mental models to balance performance and efficiency? The paper acknowledges the computational cost issue but does not provide specific data on how the number of mental models affects performance or the optimal number of models to use.

### Open Question 2
How can MeMo be adapted to handle domain-specific tasks that require specialized knowledge beyond general reasoning abilities? The paper focuses on general tasks but does not address how MeMo would perform on highly specialized domains requiring specific knowledge.

### Open Question 3
What mechanisms can be implemented to verify and refine the mental models that LLMs generate, ensuring their correctness and consistency across different tasks? The paper mentions that LLMs may identify suitable mental models but use them incorrectly, and suggests future work could investigate verification and refinement methods.

## Limitations

- The exact prompt structure and content used to introduce mental models to LLMs is not specified, making exact reproduction difficult
- Performance improvements show high variability across different task types and LLM models (ranging from 1% to 13%), suggesting potential task-specific limitations
- The method's effectiveness may depend heavily on the comprehensiveness of the mental model definitions and examples provided, but optimal prompt length/complexity tradeoffs are not quantified

## Confidence

- **High Confidence**: The core mechanism of providing mental models to LLMs and enabling autonomous selection is technically sound and well-explained
- **Medium Confidence**: The reported performance improvements are credible but may be sensitive to prompt engineering details not fully disclosed
- **Medium Confidence**: The component ablation results showing both definitions and examples contribute to performance are plausible but lack comparison to alternative component combinations

## Next Checks

1. **Prompt Replication Study**: Replicate the exact mental model introduction prompt and test whether performance improvements are consistent across different implementations
2. **Cross-Domain Generalization Test**: Evaluate MeMo on a new set of tasks outside the tested domains (e.g., creative writing, code generation) to verify true generalist capabilities
3. **Mental Model Coverage Analysis**: Systematically identify tasks where MeMo underperforms and analyze whether this correlates with missing mental model types in the prompt