---
ver: rpa2
title: 'Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs'
arxiv_id: '2409.05558'
source_url: https://arxiv.org/abs/2409.05558
tags:
- diamond
- square
- circle
- mask
- masks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using visible geometric masks to create adversarial
  examples for CAPTCHA security, specifically addressing the limitations of traditional
  imperceptible adversarial attacks that are often model-specific and ineffective
  against multiple models. The authors propose applying simple geometric masks (circles,
  diamonds, squares, and custom patterns) at various intensities to images, aiming
  to preserve semantic information for human solvers while degrading machine classifier
  performance.
---

# Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs

## Quick Facts
- arXiv ID: 2409.05558
- Source URL: https://arxiv.org/abs/2409.05558
- Authors: Yahya Jabary; Andreas Plesner; Turlan Kuzhagaliyev; Roger Wattenhofer
- Reference count: 10
- Primary result: Geometric masks can degrade image classifier accuracy by 50-80% while remaining human-solvable

## Executive Summary
This paper explores using visible geometric masks as adversarial examples for CAPTCHA security, addressing limitations of traditional imperceptible attacks that are often model-specific. The authors propose applying simple geometric masks (circles, diamonds, squares, and custom patterns) at various intensities to images, aiming to preserve semantic information for human solvers while degrading machine classifier performance. They evaluate state-of-the-art models across three datasets, showing that mask intensity significantly reduces accuracy across all models, with drops exceeding 50%-points and up to 80%-points for Vision Transformers. The approach demonstrates that aggressive, semantically-preserving perturbations can effectively fool multiple models simultaneously, offering a practical direction for robust CAPTCHA design.

## Method Summary
The authors apply geometric masks (circle, diamond, square, knit patterns) to images at various opacity levels (20%, 30%, 40%, 50%) and evaluate their impact on state-of-the-art image classifiers. They test on ImageNet-1K, ImageNette, and resized variants using models including Vision Transformers, ConvNeXt, and ResNet. The evaluation measures accuracy drops (Acc@1, Acc@5) and perceptual quality metrics (cosine similarity, PSNR, SSIM, LPIPS) to assess the trade-off between adversarial effectiveness and human interpretability.

## Key Results
- Circle masks proved particularly effective, causing Acc@1 drops exceeding 50%-points across all models
- Vision Transformers showed the highest vulnerability with up to 80%-point drops in accuracy
- Diamond masks showed less impact compared to circle and square patterns
- Lower resolution images combined with masks created a synergistic effect, further degrading model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visible geometric masks degrade machine classifier performance while preserving human interpretability.
- Mechanism: Geometric overlays introduce structured, non-random perturbations that disrupt the non-robust features machines rely on, while humans can still recognize the underlying semantic content.
- Core assumption: The difference between human and machine vision is that machines rely more heavily on non-robust features that geometric perturbations can disrupt.
- Evidence anchors:
  - [abstract] "by adding masks of various intensities the Accuracy @ 1 (Acc@1) drops by more than 50%-points for all models, and supposedly robust models such as vision transformers see an Acc@1 drop of 80%-points"
  - [section] "by allowing for more significant changes to the images while preserving the semantic information and keeping it solvable by humans, we can fool many state-of-the-art models"
  - [corpus] Weak - corpus papers focus on spatial reasoning CAPTCHAs and multimodal models, not geometric mask effectiveness specifically.

### Mechanism 2
- Claim: Circle masks are particularly effective at degrading model accuracy compared to other geometric shapes.
- Mechanism: The circular pattern creates a disruption in the feature space that is particularly confusing to current vision models, possibly due to how convolutional filters respond to circular patterns.
- Core assumption: The effectiveness of circle masks is not uniform across all geometric shapes, indicating shape-specific vulnerabilities in current models.
- Evidence anchors:
  - [section] "The circle mask proved particularly effective, while diamond masks showed less impact" and Table 1 showing circle masks consistently causing larger drops in Acc@1 than diamond or square masks.
  - [corpus] Weak - corpus papers do not specifically address geometric mask shape effectiveness.

### Mechanism 3
- Claim: Lower resolution images combined with masks create a synergistic effect that further degrades model performance.
- Mechanism: Image resizing reduces the resolution and detail available to models, and when combined with geometric masks, this creates a compounded degradation effect that is more effective than either alone.
- Core assumption: The combination of resolution reduction and geometric masking creates a multiplicative rather than additive effect on model performance degradation.
- Evidence anchors:
  - [section] "it is evident that in this setting, masks at much lower opacity ratios are more successful in distorting models' performance" and "the scaling of images combines very well with masks"
  - [corpus] Weak - corpus papers focus on spatial reasoning and multimodal models, not resolution-masking interactions.

## Foundational Learning

- Concept: Adversarial examples and their relationship to robust vs non-robust features
  - Why needed here: Understanding why geometric masks work requires knowing how adversarial examples exploit model vulnerabilities and the difference between features humans and machines rely on
  - Quick check question: What is the key difference between robust and non-robust features, and how does this relate to why geometric masks can fool models while remaining interpretable to humans?

- Concept: Image classification metrics (Acc@1, Acc@5) and perceptual quality metrics
  - Why needed here: The paper evaluates success using both accuracy metrics and perceptual quality measures, requiring understanding of what these metrics measure and their trade-offs
  - Quick check question: How do Acc@1 and Acc@5 differ in what they measure, and why might a high Acc@5 with low Acc@1 indicate partial model confusion?

- Concept: Vision model architectures (CNNs vs Transformers) and their vulnerabilities
  - Why needed here: The paper tests multiple model architectures, requiring understanding of their structural differences and how these might affect vulnerability to geometric perturbations
  - Quick check question: How do the architectural differences between CNNs (like ResNet) and Transformers (like ViT) potentially influence their susceptibility to geometric mask attacks?

## Architecture Onboarding

- Component map: Image → Mask application (with opacity setting) → Model inference → Accuracy calculation → Perceptual quality evaluation → Analysis of trade-offs
- Critical path: Image → Mask application (with opacity setting) → Model inference → Accuracy calculation → Perceptual quality evaluation → Analysis of trade-offs
- Design tradeoffs: The key tradeoff is between adversarial effectiveness (mask strength) and perceptual quality (how recognizable the image remains to humans). More aggressive masks are more effective but may become less human-solvable.
- Failure signatures: If accuracy doesn't drop significantly despite high mask opacity, it suggests models are robust to the specific mask patterns. If perceptual quality drops too low, masks become unsolvable for humans. If certain models show minimal degradation, they may have specific robustness mechanisms.
- First 3 experiments:
  1. Apply each mask type at 20%, 40%, and 60% opacity to a small subset of correctly classified images and measure Acc@1 drops across all models.
  2. Test the same masks on resized images (128x128) to evaluate the resolution-masking interaction effect.
  3. Compare the confidence scores for ground truth labels before and after mask application to understand the degree of model uncertainty introduced.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do geometric masks compare to traditional imperceptible adversarial attacks in terms of model robustness across diverse architectures?
- Basis in paper: [explicit] The paper discusses that traditional imperceptible adversarial attacks are often model-specific, whereas geometric masks show generalizability across state-of-the-art models.
- Why unresolved: While the paper demonstrates effectiveness across various models, it does not provide a direct comparative analysis with traditional imperceptible attacks in terms of robustness across different architectures.
- What evidence would resolve it: A comprehensive benchmark comparing the effectiveness of geometric masks versus traditional imperceptible attacks across a wide range of model architectures and datasets.

### Open Question 2
- Question: Can fine-tuning models on images with geometric masks improve their robustness against such attacks?
- Basis in paper: [inferred] The paper mentions the potential for expanding into other masks and determining how effectively models can be fine-tuned on images with masks applied.
- Why unresolved: The paper does not explore the impact of fine-tuning models on mask-augmented images, leaving the question of whether this approach can enhance model robustness unanswered.
- What evidence would resolve it: Experimental results showing the performance of models fine-tuned on mask-augmented images compared to those trained on standard datasets when subjected to geometric mask attacks.

### Open Question 3
- Question: How do geometric masks affect human perception and usability in CAPTCHA systems?
- Basis in paper: [explicit] The paper notes that while geometric masks are effective in fooling models, they aim to preserve semantic information and keep images solvable by humans.
- Why unresolved: Although the paper mentions the importance of maintaining human solvability, it does not provide detailed human evaluation results to assess the impact of geometric masks on user experience.
- What evidence would resolve it: Results from user studies evaluating the difficulty and user experience of solving CAPTCHA tasks with geometric masks applied, comparing them to traditional CAPTCHA methods.

## Limitations
- Lack of systematic exploration of mask parameters, making exact reproduction challenging
- Unclear weighting scheme and computation details for perceptual quality metrics
- Limited investigation of why circle masks are particularly effective compared to other shapes

## Confidence

- **High confidence**: The core finding that geometric masks can effectively degrade model accuracy across multiple architectures simultaneously (50-80% accuracy drops are substantial and consistent)
- **Medium confidence**: The claim that circle masks are universally superior to other shapes (may be dataset-dependent, mechanism not fully explained)
- **Medium confidence**: The resolution-masking synergy effect (demonstrated but not thoroughly isolated to rule out other factors)

## Next Checks

1. Parameter sensitivity analysis: Systematically vary mask opacity scaling parameters and measure their impact on both accuracy degradation and perceptual quality to establish robust parameter ranges.

2. Cross-dataset generalization test: Apply the same mask types to entirely different image datasets (e.g., medical imaging, satellite imagery) to verify the shape-specific effectiveness isn't dataset-dependent.

3. Adversarial training robustness: Train models on masked images and test whether this confers robustness to geometric perturbations, helping establish whether the vulnerability is fundamental or can be easily mitigated.