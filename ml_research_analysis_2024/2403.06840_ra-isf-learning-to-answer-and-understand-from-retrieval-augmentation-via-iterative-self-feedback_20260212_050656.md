---
ver: rpa2
title: 'RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via
  Iterative Self-Feedback'
arxiv_id: '2403.06840'
source_url: https://arxiv.org/abs/2403.06840
tags:
- question
- knowledge
- answer
- problem
- ra-isf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RA-ISF, a retrieval-augmented generation\
  \ framework that improves answer accuracy by iteratively processing questions through\
  \ three modules: self-knowledge, passage relevance, and question decomposition.\
  \ The method first checks if a question can be answered using the model\u2019s internal\
  \ knowledge, then retrieves relevant passages, and if none are found, decomposes\
  \ the question into sub-questions to be solved recursively."
---

# RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback

## Quick Facts
- arXiv ID: 2403.06840
- Source URL: https://arxiv.org/abs/2403.06840
- Reference count: 40
- RA-ISF achieves up to 55.0% exact match accuracy on GPT-3.5 and 46.0% on Llama-2 13B, outperforming existing methods like Self-RAG

## Executive Summary
RA-ISF introduces a novel retrieval-augmented generation framework that improves answer accuracy by iteratively processing questions through three specialized modules: self-knowledge, passage relevance, and question decomposition. The method first checks if a question can be answered using the model's internal knowledge, then retrieves relevant passages, and if none are found, decomposes the question into sub-questions to be solved recursively. Experiments on five datasets show that RA-ISF outperforms existing methods, including Self-RAG, with up to 55.0% exact match accuracy on GPT-3.5 and 46.0% on Llama-2 13B, and helps reduce hallucination issues common in RAG-based approaches.

## Method Summary
RA-ISF is a three-stage iterative question answering framework that uses a retriever, an LLM, and three sub-models (Mknow, Mrel, Mdecom) to process questions. First, Mknow determines if the question can be answered using the model's internal knowledge. If not, the retriever fetches relevant passages from a corpus. Mrel then filters these passages for relevance, and if no relevant passages are found, Mdecom decomposes the question into sub-questions. The framework recursively applies this process to sub-questions up to a threshold (default Dth=3), and answers are synthesized at the end. The sub-models are trained on automatically generated data using GPT-4 and fine-tuned on Llama2-7B with cross-entropy loss.

## Key Results
- RA-ISF achieves up to 55.0% exact match accuracy on GPT-3.5 and 46.0% on Llama-2 13B across five datasets
- RA-ISF outperforms Self-RAG and other RAG-based methods by significant margins
- The framework reduces hallucination issues by filtering irrelevant passages and using internal knowledge when possible

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-Knowledge Module (Mknow) reduces unnecessary retrieval by checking if the LLM already knows the answer.
- **Mechanism**: Mknow first judges if the question can be solved using the model's internal knowledge. If yes, the question is answered directly without retrieval.
- **Core assumption**: The model can reliably determine whether it has sufficient knowledge to answer a question.
- **Evidence anchors**:
  - [abstract] states RA-ISF "first uses a self-knowledge module to determine whether the current question could be answered on its own knowledge."
  - [section 3.3] shows Mknow(qt)=Know branch returns direct answer without retrieval.
  - [corpus] shows related work on self-knowledge guided retrieval (Wang et al. 2023b), supporting this assumption.
- **Break condition**: If Mknow's judgment is unreliable, it may skip retrieval when it should not, leading to incorrect answers.

### Mechanism 2
- **Claim**: Passage Relevance Module (Mrel) filters irrelevant retrieved passages to avoid hallucination.
- **Mechanism**: After retrieval, Mrel evaluates each retrieved paragraph for relevance to the question. Only relevant paragraphs are used for answering.
- **Core assumption**: The model can accurately distinguish relevant from irrelevant passages.
- **Evidence anchors**:
  - [abstract] mentions RA-ISF uses "passage relevance module will assess the relevance of each retrieved paragraph to the problem."
  - [section 3.3] describes Mrel(pi)=relevant filtering step.
  - [corpus] shows Shi et al. 2023a discusses how irrelevant retrieval impairs performance, validating the need for this mechanism.
- **Break condition**: If Mrel incorrectly classifies irrelevant passages as relevant, it may introduce noise and cause hallucinations.

### Mechanism 3
- **Claim**: Question Decomposition Module (Mdecom) enables solving complex questions through iterative sub-question solving.
- **Mechanism**: When no relevant passages are found, Mdecom decomposes the question into sub-questions. These are solved recursively using the same RA-ISF pipeline, and answers are synthesized.
- **Core assumption**: Decomposing complex questions into simpler sub-questions helps the model solve problems it cannot answer directly.
- **Evidence anchors**:
  - [abstract] states "the question decomposition module will break down the questions into sub-questions and repeat the aforementioned steps for these sub-questions."
  - [section 3.3] shows Qsub = {q1, ..., qn} ← Mdecom(qt) and recursive problem-solving.
  - [corpus] references Least-to-Most (Zhou et al. 2023) and task decomposition literature, supporting this approach.
- **Break condition**: Excessive decomposition may lead to irrelevant sub-questions or infinite recursion if not properly bounded.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG)
  - Why needed here: RA-ISF builds on RAG but improves it by adding self-knowledge and decomposition modules to handle cases where RAG fails.
  - Quick check question: What is the main limitation of standard RAG approaches that RA-ISF addresses?

- **Concept**: Iterative Self-Feedback
  - Why needed here: RA-ISF uses iterative self-feedback to refine answers through multiple modules, unlike single-pass RAG systems.
  - Quick check question: How does iterative self-feedback differ from standard RAG's approach to answering questions?

- **Concept**: Question Decomposition for Complex Reasoning
  - Why needed here: RA-ISF uses question decomposition to handle questions that cannot be answered directly or with retrieved knowledge.
  - Quick check question: What is the purpose of decomposing questions into sub-questions in the RA-ISF framework?

## Architecture Onboarding

- **Component map**: Question → Mknow → (if needed) R → Mrel → (if needed) Mdecom → Recursive solving → Synthesis
- **Critical path**: Question → Mknow → (if needed) R → Mrel → (if needed) Mdecom → Recursive solving → Synthesis
- **Design tradeoffs**:
  - Adding Mknow reduces unnecessary retrieval but requires accurate self-knowledge judgment
  - Mrel filtering reduces hallucination but may filter out useful information if too strict
  - Mdecom enables complex reasoning but adds computational overhead and recursion depth limits
  - Default iteration threshold Dth=3 balances thoroughness and efficiency
- **Failure signatures**:
  - Mknow incorrectly judges self-knowledge → Skips retrieval when should not
  - Mrel incorrectly filters passages → Loses useful information or keeps noise
  - Mdecom produces irrelevant sub-questions → Recursive solving goes off-track
  - Dth too low → Complex questions not fully decomposed
  - Dth too high → Excessive computation without benefit
- **First 3 experiments**:
  1. Test Mknow alone on questions the model should know vs. should not know to verify self-knowledge accuracy
  2. Test Mrel filtering by retrieving passages for known questions and verifying it keeps relevant ones
  3. Test full RA-ISF pipeline on simple questions that require retrieval to verify end-to-end functionality

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies heavily on the accuracy of its three sub-models (Mknow, Mrel, Mdecom), but the paper does not provide detailed error analysis for individual modules
- The training data generation process using GPT-4 is a potential source of bias, and the quality of generated labels directly impacts downstream performance
- The default iteration threshold of 3 is somewhat arbitrary and may not be optimal for all question types or dataset characteristics

## Confidence
- **High Confidence**: The overall framework architecture and experimental methodology are sound. The ablation studies clearly demonstrate that each module contributes to performance improvements, and the results show consistent gains across five diverse datasets.
- **Medium Confidence**: The specific implementation details of the sub-models and their training procedures are well-described, but the quality of the automatically generated training data from GPT-4 remains a concern. The comparison with Self-RAG is valid but may not capture all potential baseline variations.
- **Low Confidence**: The generalizability of the iteration threshold and the potential for cascading errors through the three-module pipeline have not been thoroughly explored. The computational overhead of the iterative process compared to simpler alternatives is not quantified.

## Next Checks
1. **Module Isolation Testing**: Run controlled experiments where each sub-model (Mknow, Mrel, Mdecom) is tested in isolation with ground truth labels to quantify individual module accuracy and identify potential failure points in the pipeline.
2. **Training Data Quality Analysis**: Evaluate the consistency and accuracy of the GPT-4 generated training labels by having human annotators verify a sample, particularly for edge cases where modules may disagree.
3. **Iteration Threshold Sensitivity**: Systematically vary the default iteration threshold (Dth) from 1 to 5 and measure the trade-off between performance gains and computational overhead to determine if the current default is optimal or dataset-dependent.