---
ver: rpa2
title: Overcoming Domain Drift in Online Continual Learning
arxiv_id: '2405.09133'
source_url: https://arxiv.org/abs/2405.09133
tags:
- tasks
- learning
- task
- continual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting and domain drift in
  online continual learning by introducing Drift-Reducing Rehearsal (DRR), a novel
  rehearsal-based approach that anchors old task knowledge. The core idea is to store
  more representative samples in memory using a Centroid-based Online Selection (COS)
  strategy, which selects data points based on their distance from centroids constructed
  in a data stream.
---

# Overcoming Domain Drift in Online Continual Learning

## Quick Facts
- arXiv ID: 2405.09133
- Source URL: https://arxiv.org/abs/2405.09133
- Authors: Fan Lyu; Daofeng Liu; Linglan Zhao; Zhang Zhang; Fanhua Shang; Fuyuan Hu; Wei Feng; Liang Wang
- Reference count: 40
- Primary result: Introduces Drift-Reducing Rehearsal (DRR) with Centroid-based Online Selection and contrastive margin loss to combat catastrophic forgetting and domain drift in online continual learning.

## Executive Summary
This paper tackles catastrophic forgetting and domain drift in online continual learning by introducing Drift-Reducing Rehearsal (DRR), a novel rehearsal-based approach that anchors old task knowledge. The core idea is to store more representative samples in memory using a Centroid-based Online Selection (COS) strategy, which selects data points based on their distance from centroids constructed in a data stream. To further mitigate domain drift, DRR employs a two-level angular cross-task Contrastive Margin Loss (CML) that enforces intra-class/task compactness and inter-class/task discrepancy, and optionally uses a Centroid Distillation Loss (CDL) to preserve old task representations. Experiments on four benchmark datasets—Permuted MNIST, Split CIFAR, Split CUB, and Split AWA—demonstrate that DRR significantly outperforms state-of-the-art methods in average accuracy, forgetting measure, and long-term remembering, achieving up to 94.43% accuracy on Permuted MNIST and 72.47% on Split CIFAR. The approach effectively reduces continual domain drift and enhances both stability and plasticity in online continual learning.

## Method Summary
The paper introduces Drift-Reducing Rehearsal (DRR) to address catastrophic forgetting and domain drift in online continual learning. DRR combines a Centroid-based Online Selection (COS) strategy with a two-level angular cross-task Contrastive Margin Loss (CML). COS dynamically selects and stores representative samples by measuring their distance from task-specific centroids in a streaming data context. The CML loss enforces intra-class/task compactness and inter-class/task discrepancy using angular margins, thereby mitigating domain drift. Optionally, DRR incorporates a Centroid Distillation Loss (CDL) to further stabilize old task representations. Experiments validate DRR on four datasets, showing substantial gains in accuracy, forgetting, and long-term remembering compared to baselines.

## Key Results
- DRR achieves up to 94.43% accuracy on Permuted MNIST and 72.47% on Split CIFAR, significantly outperforming state-of-the-art methods.
- The approach reduces catastrophic forgetting, as evidenced by improved forgetting measures and long-term remembering metrics.
- DRR effectively mitigates continual domain drift, enhancing both stability and plasticity in online continual learning scenarios.

## Why This Works (Mechanism)
DRR addresses catastrophic forgetting by anchoring old task knowledge through selective memory storage (COS) and contrastive loss (CML), which enforce task and class compactness and separation. Domain drift is mitigated by dynamically updating centroids and using angular margins to maintain consistent task representations across the data stream. CDL further stabilizes old task representations, reducing drift-induced forgetting. The combination of selective memory, contrastive regularization, and distillation preserves both stability and plasticity, enabling effective continual learning under domain shift.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to forget previous tasks when learning new ones. *Why needed*: Central problem DRR aims to solve.
- **Domain drift**: Gradual changes in data distribution over time in online learning. *Why needed*: DRR explicitly targets this to improve stability.
- **Contrastive learning**: Learning representations by contrasting similar and dissimilar pairs. *Why needed*: CML uses contrastive principles to maintain task boundaries.
- **Centroid-based selection**: Choosing representative samples based on distance from cluster centroids. *Why needed*: COS leverages this to store informative samples in memory.
- **Angular margin loss**: Loss functions that enforce angular separation between classes/tasks. *Why needed*: CML uses angular margins for robust task discrimination.

## Architecture Onboarding

**Component Map**: Online data stream -> Feature extractor -> Centroid-based Online Selection (COS) -> Memory buffer -> Contrastive Margin Loss (CML) -> Model update -> (Optional) Centroid Distillation Loss (CDL)

**Critical Path**: Data stream → Feature extractor → COS → Memory buffer → CML → Model update

**Design Tradeoffs**: COS trades memory for representativeness; CML balances plasticity (learning new tasks) and stability (retaining old knowledge); CDL adds regularization at the cost of extra computation.

**Failure Signatures**: Poor centroid initialization may lead to suboptimal sample selection; overly rigid angular margins may hinder adaptation to new tasks; insufficient memory buffer size may limit DRR's effectiveness.

**First Experiments**: 1) Ablation study: Remove CDL to assess its contribution to drift mitigation. 2) Memory buffer sensitivity: Vary buffer size to measure impact on performance. 3) Cross-domain evaluation: Test DRR on non-image modalities to evaluate generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- The Centroid-based Online Selection (COS) strategy's reliance on Euclidean distance in feature space may not align with task-specific semantic similarity, especially for highly diverse datasets.
- The contrastive margin loss assumes angular margins between task clusters will prevent drift, but this has not been validated on non-image or non-classification tasks.
- The ablation study on CDL is incomplete; the extent to which CDL contributes to drift mitigation versus general regularization is unclear.

## Confidence
- High confidence for claims on Permuted MNIST and Split CIFAR based on reported metrics.
- Medium confidence for claims on Split CUB and Split AWA due to lack of detailed class imbalance or domain shift analysis.
- Low confidence in memory efficiency claims due to absence of wall-clock or memory usage comparisons to baselines.

## Next Checks
1. Evaluate DRR on non-image modalities (e.g., text or time-series) to test generalizability of the centroid and contrastive loss assumptions.
2. Perform a sensitivity analysis on memory buffer size and its interaction with COS selection criteria to quantify scalability limits.
3. Conduct a failure case analysis where domain drift is artificially accelerated to test whether DRR's two-level contrastive loss can adapt or if it over-constrains plasticity.