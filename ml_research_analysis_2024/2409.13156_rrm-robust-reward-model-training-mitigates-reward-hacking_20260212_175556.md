---
ver: rpa2
title: 'RRM: Robust Reward Model Training Mitigates Reward Hacking'
arxiv_id: '2409.13156'
source_url: https://arxiv.org/abs/2409.13156
tags:
- reward
- responses
- arxiv
- training
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward hacking in reward model
  training, where models can exploit spurious context-free artifacts (like response
  length or format) instead of learning true contextual preferences. The authors propose
  a causal framework and data augmentation technique to mitigate this issue.
---

# RRM: Robust Reward Model Training Mitigates Reward Hacking

## Quick Facts
- arXiv ID: 2409.13156
- Source URL: https://arxiv.org/abs/2409.13156
- Authors: Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasiia Makarova, Jeremiah Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, Mohammad Saleh
- Reference count: 22
- One-line primary result: Reward model trained with data augmentation that balances context-free artifacts improves RewardBench accuracy from 80.61% to 84.15% and policy performance on MT-Bench and AlpacaEval-2

## Executive Summary
This paper addresses reward hacking in reward model training, where models exploit spurious context-free artifacts (like response length or format) instead of learning true contextual preferences. The authors propose a causal framework and data augmentation technique to mitigate this issue. Their approach involves creating additional training data by combining responses from different prompts, effectively balancing context-free artifacts between chosen and rejected responses. This method helps the reward model focus on genuine contextual signals rather than artifacts.

## Method Summary
The authors propose a causal framework with data augmentation to mitigate reward hacking. They create augmented training data by combining responses from different prompts, ensuring context-free artifacts appear with similar frequencies in both chosen and rejected responses. The approach trains a pairwise reward model on Gemma-2-9b-it using the RLHFlow preference dataset with 14X additional examples generated through permutation of responses. A filtering strategy selects examples where the reward model's predicted probability differs by ≥0.2 from ground truth, resulting in 2.4M total training examples. Policies are trained using DPO with on-policy responses labeled by both baseline and robust reward models.

## Key Results
- RRM improves RewardBench accuracy from 80.61% to 84.15%
- Policies trained with RRM achieve higher MT-Bench scores (8.31 vs 7.27)
- Length-controlled win-rates in AlpacaEval-2 increase from 33.46% to 52.49%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The robust reward model (RRM) effectively decouples contextual preference signals from context-free artifacts by balancing artifact frequencies between chosen and rejected responses through data augmentation.
- **Mechanism:** By creating augmented training data that includes response pairs from different prompts, the method ensures that context-free artifacts appear with similar frequencies in both chosen and rejected responses. This forces the reward model to learn preferences based on prompt-dependent contextual signals rather than artifact-based shortcuts.
- **Core assumption:** Context-free artifacts are independent of the prompt and can be balanced across response pairs through permutation of responses from different examples.
- **Evidence anchors:**
  - [abstract]: "We propose a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them."
  - [section 3.2]: "With these, we have the following claim: If the reward model is trained with Dhf and augmented triplets in Equation 5, there is no causal edge from A to C in DAG G."
  - [corpus]: Weak - no direct evidence in corpus about balancing artifact frequencies specifically.
- **Break condition:** If artifacts are not independent of prompts or if the augmentation doesn't sufficiently balance artifact frequencies, the mechanism fails.

### Mechanism 2
- **Claim:** The causal framework distinguishes between genuine contextual preferences and spurious artifact-based preferences by identifying d-separation relationships in the causal graph.
- **Mechanism:** The causal graph (X→S→C and X→Y1→A→C, X→Y2→A→C) establishes that under H0 (no causal edge from A to C), artifacts A should be d-separated from preference C given either the responses (Y1,Y2) or the contextual signal S. The data augmentation exploits these d-separation properties to eliminate artifact-based learning.
- **Core assumption:** The causal relationships in the graph accurately represent how preferences are formed and that d-separation in the graph corresponds to statistical independence in the data.
- **Evidence anchors:**
  - [section 3.1]: "We formulate a DAG G to model the causal relationships among different quantities... We assume the distribution of (X, Y1, Y2, S, A, C) to be faithful to the DAG."
  - [section 3.1]: "We say response y is contextual to x if they are from the same triplet in Dhf... Then for (x, y1, y2), we have the following rules..."
  - [corpus]: Weak - corpus doesn't directly address the causal graph formulation.
- **Break condition:** If the causal assumptions are violated or the d-separation relationships don't hold in the actual data distribution, the mechanism breaks.

### Mechanism 3
- **Claim:** The RRM improves policy performance by training on data that has been corrected for artifact-based bias, leading to better alignment with human preferences.
- **Mechanism:** By training the reward model on augmented data that eliminates artifact-based shortcuts, the resulting RRM provides more accurate preference signals during policy training. This leads to policies that generate higher-quality responses without exploiting spurious correlations.
- **Core assumption:** The improved reward model quality directly translates to better policy performance, and the augmented data effectively corrects for the artifact-based biases present in the original training data.
- **Evidence anchors:**
  - [abstract]: "Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM)."
  - [section 4.2]: "RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%."
  - [section 4.2]: "policies trained using RRM significantly outperform those trained with the baseline reward model, achieving higher MT-Bench scores (8.31 vs 7.27) and length-controlled win-rates in AlpacaEval-2 (52.49% vs 33.46%)."
  - [corpus]: Weak - corpus papers discuss related reward modeling approaches but don't specifically address this augmentation mechanism.
- **Break condition:** If the relationship between reward model quality and policy performance is not direct, or if the augmentation introduces other biases, the mechanism fails.

## Foundational Learning

- **Concept:** Causal inference and d-separation
  - **Why needed here:** The paper's core mechanism relies on understanding causal relationships and using d-separation to identify when artifacts should be independent of preferences. Without this foundation, the data augmentation approach wouldn't have theoretical justification.
  - **Quick check question:** If two variables are d-separated in a causal graph, what does this imply about their statistical relationship in the data?

- **Concept:** Bradley-Terry model and pairwise ranking
  - **Why needed here:** The paper uses pairwise reward models as the baseline approach being improved. Understanding how these models work is essential for grasping what the RRM is improving upon.
  - **Quick check question:** How does the Bradley-Terry model estimate the probability that one response is preferred over another?

- **Concept:** Data augmentation techniques in machine learning
  - **Why needed here:** The paper's novel contribution involves a specific form of data augmentation. Understanding general data augmentation principles helps contextualize this approach.
  - **Quick check question:** What is the primary purpose of data augmentation in machine learning, and how might it help with the reward hacking problem?

## Architecture Onboarding

- **Component map:**
  Original preference dataset (Dhf) -> Data augmentation pipeline -> Reward model training (baseline and RRM) -> Policy training using DPO -> Evaluation on benchmarks

- **Critical path:**
  1. Load original preference dataset
  2. Generate augmented data using permutation rules
  3. Train baseline reward model
  4. Train RRM on combined original + augmented data
  5. Generate responses using base LLM
  6. Apply DPO using both reward models
  7. Evaluate policies on benchmark datasets

- **Design tradeoffs:**
  - Data augmentation increases training data size by ~14x, which increases computational cost but improves robustness
  - The permutation-based augmentation may introduce some unrealistic response pairs, but the filtering step mitigates this
  - The approach assumes artifacts can be balanced, which may not hold for all types of artifacts

- **Failure signatures:**
  - If RRM doesn't improve over baseline on RewardBench, the augmentation may not be effective
  - If policies trained with RRM don't show improved win-rates in AlpacaEval-2, the reward model may not be translating to better policies
  - If the augmented data introduces significant noise, both reward model accuracy and policy quality may degrade

- **First 3 experiments:**
  1. Train baseline reward model on original data and evaluate on RewardBench to establish baseline performance
  2. Apply data augmentation and train RRM, then compare RewardBench accuracy with baseline
  3. Train DPO policies using both reward models and evaluate on AlpacaEval-2 to verify policy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between contextual and non-contextual response pairs in the augmented training data?
- Basis in paper: [inferred] The paper describes creating 14X additional examples through data augmentation, but notes that "most of which can be too easy for RM to learn" and applies a filtering strategy based on prediction confidence thresholds.
- Why unresolved: The authors only use a heuristic filtering approach (removing examples where |ˆP(A ≻ B) − P∗(A ≻ B)| < 0.2) rather than systematically studying the trade-off between training data composition and model performance.
- What evidence would resolve it: Systematic ablation studies varying the proportion of contextual vs. non-contextual pairs in the augmented dataset, measuring downstream policy performance and reward model accuracy across different ratios.

### Open Question 2
- Question: How does the RRM approach generalize to reward hacking patterns beyond length, format, and explicit artifacts like emoji prefixes?
- Basis in paper: [explicit] The authors acknowledge "there are more reward hacking patterns beyond length, such as format (markdowns, bold-faces) and patterns (certain n-grams or emojis)" and state "it is challenging to identify and mitigate all potential exploitation patterns."
- Why unresolved: While the paper demonstrates effectiveness against specific known artifacts, it doesn't test the method's robustness against a comprehensive range of potential reward hacking patterns that might emerge in real-world applications.
- What evidence would resolve it: Experiments systematically introducing various types of artifacts (syntactic patterns, semantic patterns, multi-turn conversation artifacts) and measuring RRM's robustness compared to baseline methods across diverse test sets.

### Open Question 3
- Question: What is the long-term effect of RRM training on reward model generalization to out-of-distribution prompts and responses?
- Basis in paper: [inferred] The paper focuses on improving reward model accuracy on RewardBench and downstream policy performance, but doesn't address how RRM affects generalization to prompts and responses that differ substantially from the training distribution.
- Why unresolved: The evaluation primarily uses datasets similar to the training data (UltraFeedback prompts, AlpacaEval-2), without testing performance on truly out-of-distribution scenarios that might reveal overfitting to the augmentation strategy.
- What evidence would resolve it: Extended evaluation on diverse, out-of-distribution prompt-response pairs, including adversarial examples, cross-domain prompts, and prompts requiring novel reasoning, comparing RRM generalization to baseline models.

### Open Question 4
- Question: How does the causal framework extend to multi-turn conversations where artifacts might span across response turns?
- Basis in paper: [inferred] The current causal framework models single-turn interactions with X → (Y1, Y2) → C structure, but real conversations involve sequential dependencies and potential artifacts that persist across turns.
- Why unresolved: The paper focuses on single-turn response pairs and doesn't address how artifacts or contextual signals might propagate across multiple conversation turns, which is crucial for practical chatbot applications.
- What evidence would resolve it: Development and testing of an extended causal framework for multi-turn conversations, with corresponding data augmentation strategies and evaluation on multi-turn dialogue datasets showing improved reward modeling across conversation contexts.

## Limitations

- The causal framework's effectiveness relies on the assumption that context-free artifacts are independent of prompts, which may not hold for all artifact types
- Data augmentation increases training data by ~14x, potentially introducing noise through unrealistic response pairings despite filtering
- Results are based on a single model family (Gemma-2-9b-it), limiting generalizability across different architectures

## Confidence

- **High confidence:** The improvement in RewardBench accuracy (80.61% → 84.15%) and policy performance metrics (MT-Bench 7.27 → 8.31, AlpacaEval-2 win-rate 33.46% → 52.49%) are directly measured and reported.
- **Medium confidence:** The causal mechanism's effectiveness in decoupling artifacts from preferences, as this relies on theoretical assumptions about d-separation that may not fully hold in practice.
- **Low confidence:** Generalization of results across different model families and domains, given the single-model evaluation.

## Next Checks

1. **Cross-model validation:** Test RRM on a different model family (e.g., Llama or Mistral) to verify that improvements in reward model accuracy and policy performance generalize beyond Gemma-2-9b-it.

2. **Domain-specific artifact analysis:** Evaluate whether RRM over-corrects for artifacts that are actually contextually relevant in specific domains (e.g., longer responses in reasoning tasks) by analyzing performance on domain-specific benchmarks.

3. **Ablation study on filtering threshold:** Systematically vary the filtering threshold for augmented data (|P̂(A≻B) - P*(A≻B)| ≥ 0.2) to determine its impact on reward model quality and identify whether the current threshold is optimal or could be relaxed to include more training data.