---
ver: rpa2
title: 'FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs'' Responsiveness
  to Human Feedback'
arxiv_id: '2410.09412'
source_url: https://arxiv.org/abs/2410.09412
tags:
- llms
- feedback
- user
- response
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FB-Bench is a fine-grained, multi-task benchmark designed to evaluate
  Large Language Models' (LLMs) responsiveness to human feedback in Chinese. The benchmark
  comprises 591 samples across eight task types, five deficiency types, and nine feedback
  types, organized into error correction and response maintenance scenarios.
---

# FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs' Responsiveness to Human Feedback

## Quick Facts
- arXiv ID: 2410.09412
- Source URL: https://arxiv.org/abs/2410.09412
- Reference count: 30
- 27 LLMs evaluated on Chinese responsiveness benchmark with 591 samples

## Executive Summary
FB-Bench is a fine-grained, multi-task benchmark designed to evaluate Large Language Models' (LLMs) responsiveness to human feedback in Chinese. The benchmark comprises 591 samples across eight task types, five deficiency types, and nine feedback types, organized into error correction and response maintenance scenarios. Using a LLM-as-a-Judge framework with weighted checklists, the evaluation achieves over 90% human-LLM agreement. Experiments with 27 LLMs reveal that DeepSeek-V3 performs best overall (74.86%), with most models showing balanced error correction and response maintenance abilities.

## Method Summary
The benchmark uses a three-tier hierarchical taxonomy (task types, model response deficiencies, user feedback) to organize 591 curated samples. Model responses are evaluated using GPT-4o as an automated judge against human-curated weighted checklists, where criteria are assigned partial credit based on importance weights. The evaluation protocol achieves 90.91% human-LLM agreement through careful judge model selection. Model follow-up responses are generated using different temperature settings for different task types, then scored on their ability to address the feedback while maintaining response quality.

## Key Results
- DeepSeek-V3 achieved the highest overall score at 74.86% across all models tested
- Most models demonstrated balanced capabilities in both error correction and response maintenance scenarios
- Open-source models showed superior performance in response maintenance compared to closed-source alternatives
- Hinting guidance significantly improved LLM responses while misinformation often led to misleading outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained weighted checklist evaluation enables more accurate and differentiated scoring of LLM responsiveness.
- Mechanism: Instead of binary pass/fail evaluation, the weighted checklist assigns partial credit based on how well the follow-up response addresses specific aspects of the feedback. Higher weights indicate more critical criteria. This allows nuanced differentiation between models that might both "pass" under binary evaluation but differ significantly in quality.
- Core assumption: The weighted checklist criteria accurately capture the dimensions of effective responsiveness to human feedback.
- Evidence anchors:
  - [abstract] "To precisely assess the performance of LLMs in a detailed manner, we employ GPT-4o to act as a judge, scoring the model-generated follow-up responses based on the human-curated weighted checklist."
  - [section] "To get a more fine-grained evaluation in error correction scenarios, we further set different weights for different criteria in the checklist, where a higher weight signifies greater importance and the sum of these weights equals 1."
- Break condition: If the checklist criteria are poorly designed or misaligned with actual quality dimensions, the weighted scoring becomes meaningless or misleading.

### Mechanism 2
- Claim: LLM-as-a-judge framework with GPT-4o achieves high agreement with human evaluators (>90%), providing reliable automated assessment.
- Mechanism: By using GPT-4o as an automated judge that evaluates responses against the weighted checklist, the system can scale evaluation across many models while maintaining consistency with human judgment. The high human-LLM agreement rate (90.91%) validates this approach.
- Core assumption: GPT-4o's evaluation quality is comparable to human experts for this task.
- Evidence anchors:
  - [abstract] "This evaluation protocol achieves a human-LLM agreement rate exceeding 90%, demonstrating significant robustness."
  - [section] "To determine the most suitable judge model, we randomly select 194 samples... We found that gpt-4o-2024-08-06 achieved the highest consistency rate, at 90.91%."
- Break condition: If GPT-4o's evaluation quality degrades over time or if the task domain shifts beyond its capabilities, the agreement rate would drop significantly.

### Mechanism 3
- Claim: The three-tier hierarchical taxonomy captures the essential elements of human-LLM interaction, enabling comprehensive evaluation across real-world scenarios.
- Mechanism: By organizing data into query tasks (8 types), model response deficiencies (5 types), and user feedback (9 types), the benchmark covers the full spectrum of interaction dynamics. This structure allows analysis of how different combinations affect LLM performance.
- Core assumption: The selected task types, deficiency types, and feedback types comprehensively represent real-world human-LLM interactions.
- Evidence anchors:
  - [abstract] "Drawing from the two main interaction scenarios, FB-Bench comprises 591 meticulously curated samples, encompassing eight task types, five deficiency types of response, and nine feedback types."
  - [section] "A typical human-LLM interaction process comprises three components: the user's query, the model's response, and the user's feedback."
- Break condition: If real-world interactions involve types not covered by the taxonomy, or if the distribution of types doesn't match actual usage patterns, the benchmark's external validity would be compromised.

## Foundational Learning

- Concept: Three-tier hierarchical taxonomy for human-LLM interactions
  - Why needed here: Provides the structural framework for organizing benchmark data and ensuring comprehensive coverage of interaction scenarios
  - Quick check question: Can you explain why the benchmark separates query tasks, model response deficiencies, and user feedback into distinct tiers rather than combining them?

- Concept: LLM-as-a-judge evaluation methodology
  - Why needed here: Enables scalable, consistent evaluation of model responses while maintaining high agreement with human judgment
  - Quick check question: What is the key metric used to validate that the LLM judge is performing equivalently to human evaluators?

- Concept: Weighted checklist scoring system
  - Why needed here: Allows fine-grained differentiation between models by assigning partial credit based on how well responses address specific evaluation criteria
  - Quick check question: How does the weighted checklist differ from binary pass/fail evaluation in terms of information provided about model performance?

## Architecture Onboarding

- Component map: Data curation pipeline → Taxonomy definition → Weighted checklist creation → LLM-as-a-judge evaluation → Result analysis
- Critical path: Sample collection → Human annotation → Difficulty filtering → Model response generation → Automated evaluation → Performance analysis
- Design tradeoffs: 
  - Chinese-only dataset limits language applicability but enables deeper cultural context understanding
  - LLM-as-a-judge provides scalability but introduces potential LLM-specific biases
  - Weighted checklist enables nuanced scoring but requires careful criterion design
- Failure signatures:
  - Low human-LLM agreement rates indicate judge model inadequacy
  - Uniform high scores across all models suggest checklist criteria are too easy
  - Significant performance gaps between tasks indicate potential benchmark imbalance
- First 3 experiments:
  1. Validate judge model agreement by having humans evaluate 100 random samples and comparing to LLM judge results
  2. Test checklist effectiveness by having different annotators create checklists for same samples and measuring inter-annotator agreement
  3. Evaluate difficulty calibration by running models of known capabilities through the benchmark and verifying expected performance patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of instruction-following ability on LLMs' performance in error correction scenarios, and how does this ability vary across different task types?
- Basis in paper: [explicit] The paper mentions that "Stronger LLMs outperform less capable LLMs in rectifying all categories of deficiencies identified in prior dialogues, especially when addressing logical errors and failures to follow user instructions."
- Why unresolved: The paper does not provide a detailed analysis of how instruction-following ability specifically affects error correction performance across different task types. It only mentions that this ability is particularly important for addressing certain deficiency types.
- What evidence would resolve it: A detailed breakdown of LLMs' performance in error correction scenarios, categorized by task type and deficiency type, with a focus on instruction-following ability.

### Open Question 2
- Question: How does the effectiveness of different feedback types (e.g., hinting guidance, misinformation, credibility support) vary across different task types in both error correction and response maintenance scenarios?
- Basis in paper: [explicit] The paper states that "Hinting guidance significantly helps LLMs enhance the quality of responses, while exposing LLMs to misinformation or challenging them with fabricated credentials often leads to misleading outputs."
- Why unresolved: The paper provides a general overview of the impact of feedback types but does not offer a detailed analysis of how these effects vary across different task types in both error correction and response maintenance scenarios.
- What evidence would resolve it: A comprehensive analysis of the effectiveness of different feedback types across all task types in both error correction and response maintenance scenarios, including specific examples and quantitative data.

### Open Question 3
- Question: What are the key factors that contribute to the narrowing performance gap between open-source and closed-source LLMs in response maintenance scenarios?
- Basis in paper: [explicit] The paper notes that "The performance gap between open-source and closed-source LLMs is narrowing" and that "open-source LLMs demonstrate superior response maintenance capabilities."
- Why unresolved: The paper does not delve into the specific factors that contribute to this narrowing gap or explain why open-source models excel in response maintenance.
- What evidence would resolve it: A detailed investigation into the architectural, training, and deployment differences between open-source and closed-source LLMs that lead to their performance in response maintenance scenarios, including comparative studies and case analyses.

## Limitations
- Chinese-only dataset limits generalizability to other languages and cultures
- Benchmark effectiveness depends heavily on the quality and completeness of the three-tier taxonomy design
- LLM-as-a-judge framework may introduce model-specific biases that systematically favor certain response patterns

## Confidence
- **High Confidence**: The weighted checklist methodology and its implementation (90.91% human-LLM agreement rate is empirically validated)
- **Medium Confidence**: The three-tier hierarchical taxonomy comprehensively captures real-world interaction scenarios (based on careful curation but limited external validation)
- **Medium Confidence**: Open-source models demonstrate superior response maintenance capabilities (supported by experimental results but may reflect specific dataset characteristics)

## Next Checks
1. **Cross-cultural validation**: Replicate the benchmark with English and other language datasets to test generalizability of the taxonomy and evaluation framework across cultural contexts
2. **Longitudinal stability test**: Evaluate model performance over time to assess whether GPT-4o judge agreement rates remain stable as models evolve and training data shifts
3. **Real-world deployment correlation**: Conduct field studies where human feedback is applied to deployed LLM systems, comparing benchmark predictions with actual user satisfaction and task completion rates