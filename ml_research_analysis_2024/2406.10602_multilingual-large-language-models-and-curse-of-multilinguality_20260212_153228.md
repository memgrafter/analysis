---
ver: rpa2
title: Multilingual Large Language Models and Curse of Multilinguality
arxiv_id: '2406.10602'
source_url: https://arxiv.org/abs/2406.10602
tags:
- multilingual
- language
- languages
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive overview of multilingual Large
  Language Models (LLMs), examining their architectures, pre-training objectives,
  data sources, tokenization methods, and the challenges posed by the curse of multilinguality.
  It explores encoder-only models like mBERT and XLM-R, decoder-only models such as
  XGLM, PALM, BLOOM, and GPT-3, as well as encoder-decoder models like mBART and mT5.
---

# Multilingual Large Language Models and Curse of Multilinguality

## Quick Facts
- arXiv ID: 2406.10602
- Source URL: https://arxiv.org/abs/2406.10602
- Authors: Daniil Gurgurov; Tanja Bäumel; Tatiana Anikina
- Reference count: 11
- Primary result: Comprehensive overview of multilingual LLM architectures, pre-training methods, and the curse of multilinguality

## Executive Summary
This paper provides a comprehensive survey of multilingual Large Language Models, examining encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder (mBART, mT5) architectures. It explores how these models leverage diverse training data sources and tokenization techniques to handle multiple languages while discussing the fundamental challenge of curse of multilinguality. The paper identifies how model performance degrades as language count increases due to imbalanced data and linguistic diversity, and proposes modular architectures and adapter-based approaches as potential solutions.

## Method Summary
The paper synthesizes existing research on multilingual LLMs by examining their architectural designs, pre-training objectives (MLM, CLM, NSP, TLM), data sources (Wikipedia, Common Crawl, ROOTS), and tokenization methods (BPE, WordPiece, SentencePiece). It analyzes how models handle multilingual challenges through weighted sampling techniques and discusses modular approaches like X-MOD that combine shared and language-specific parameters. The survey methodology involves reviewing published works and technical documentation to provide a comprehensive overview of the field.

## Key Results
- Multilingual LLMs use diverse tokenization methods (BPE, WordPiece, SentencePiece) to handle different morphological structures across languages
- The curse of multilinguality causes performance degradation as the number of supported languages increases
- Exponentially smoothed weighting and modular architectures (X-MOD) are proposed solutions to mitigate language imbalance issues
- Different model architectures (encoder-only, decoder-only, encoder-decoder) serve distinct multilingual tasks and capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual models can learn language-specific subword tokenization to handle diverse languages efficiently.
- Mechanism: Subword tokenization techniques like BPE, WordPiece, and SentencePiece iteratively merge frequent character pairs to create a vocabulary that represents both common words and rare subword units. SentencePiece treats the input as a raw stream, making it suitable for languages without spaces.
- Core assumption: Subword tokenization effectively balances vocabulary size and coverage across languages with different morphological structures.
- Evidence anchors:
  - [section] "SentencePiece (Kudo and Richardson, 2018) is a text tokenizer that works at the subword level... It employs a unigram language model to tokenize text into pieces, making it suitable for various languages."
  - [section] "WordPiece (Schuster and Nakajima, 2012) is another subword tokenization algorithm that starts with a vocabulary of individual characters and iteratively merges the most frequent character pairs."
  - [corpus] Weak corpus evidence - related papers mention data mixtures but don't provide direct evidence for tokenization effectiveness.
- Break condition: Languages with extremely low resource data may not have enough frequency statistics to build effective subword vocabularies.

### Mechanism 2
- Claim: Modular architectures with language-specific parameters can mitigate the curse of multilinguality by reducing inter-language interference.
- Mechanism: X-MOD architecture combines shared attention/feed-forward components with language-specific modules at every transformer layer. This allows each language to have dedicated parameters while maintaining shared knowledge across languages.
- Core assumption: Language-specific modules can capture language-specific patterns without negatively impacting other languages.
- Evidence anchors:
  - [section] "Pfeiffer et al. propose X-MOD, a modular multilingual architecture that combines shared and language-specific parameters as a way of uplifting curse of multilinguality"
  - [section] "X-MOD initializes modular models during pre-training, facilitating inexpensive expansion to new languages afterwards"
  - [corpus] Weak corpus evidence - related papers discuss modular approaches but lack specific empirical validation for X-MOD.
- Break condition: When the number of languages becomes extremely large, the overhead of maintaining language-specific modules may become prohibitive.

### Mechanism 3
- Claim: Exponentially smoothed weighting ensures fair representation of low-resource languages in pre-training data.
- Mechanism: Models apply weighted sampling where languages with less data receive higher weights during training, preventing high-resource languages from dominating the training process.
- Core assumption: Weighted sampling can effectively balance the training distribution without introducing significant training instability.
- Evidence anchors:
  - [section] "To address these imbalances, multilingual LLMs often employ exponentially smoothed weighting (Devlin et al., 2018). This approach ensures relatively fair representation of low-resource languages in the model's training data"
  - [section] "Managing the imbalance in pre-training data among languages, especially when dealing with a large number of them, is a challenge"
  - [corpus] Weak corpus evidence - related papers discuss data mixtures but don't provide specific evidence for exponentially smoothed weighting effectiveness.
- Break condition: Extreme imbalance between high-resource and low-resource languages may make it difficult to find appropriate smoothing parameters.

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding how encoder-only, decoder-only, and encoder-decoder architectures work is essential for grasping multilingual LLM design
  - Quick check question: What is the key difference between masked self-attention in encoders and causal self-attention in decoders?

- Concept: Subword tokenization and vocabulary building
  - Why needed here: Different tokenization methods directly impact how well models handle linguistic diversity and vocabulary size
  - Quick check question: How does BPE differ from WordPiece in terms of vocabulary construction approach?

- Concept: Masked Language Modeling and Causal Language Modeling objectives
  - Why needed here: These pre-training objectives determine what linguistic knowledge the models learn during training
  - Quick check question: Why is CLM particularly suited for autoregressive generation tasks while MLM is better for understanding tasks?

## Architecture Onboarding

- Component map: Core transformer layers → Language-specific adapter modules (X-MOD) → Shared attention/feed-forward components → Tokenization layer → Pre-training objective functions → Data loading pipeline with weighted sampling
- Critical path: Data → Tokenization → Model architecture → Pre-training objective → Training loop with weighted sampling → Evaluation across languages
- Design tradeoffs: Larger models can handle more languages but suffer more from curse of multilinguality; modular approaches add complexity but improve language-specific performance
- Failure signatures: Performance degradation in low-resource languages; increased training instability with weighted sampling; memory issues with language-specific modules
- First 3 experiments:
  1. Implement a simple multilingual model with SentencePiece tokenization and compare performance across 10 languages with different morphological structures
  2. Add exponentially smoothed weighting to the data loader and measure impact on low-resource language performance
  3. Implement a basic X-MOD-style architecture with one language-specific adapter and compare against a fully shared model

## Open Questions the Paper Calls Out
None

## Limitations
- The survey nature limits empirical validation of proposed solutions like X-MOD and exponentially smoothed weighting
- Specific implementation details and hyperparameters for models are not provided, making exact reproduction difficult
- Claims about the curse of multilinguality and mitigation strategies rely heavily on secondary sources rather than original experiments

## Confidence
**High Confidence**: Claims about existing multilingual model architectures (mBERT, XLM-R, XGLM, PALM, BLOOM, mBART, mT5) and their basic design patterns are well-established in the literature and widely replicated.

**Medium Confidence**: Claims about tokenization methods (BPE, WordPiece, SentencePiece) and their general effectiveness across languages are supported by implementation details in the original papers, though specific performance differences across languages are not deeply analyzed.

**Low Confidence**: Claims about the specific effectiveness of exponentially smoothed weighting and modular architectures like X-MOD in mitigating the curse of multilinguality lack direct empirical validation in this paper and rely on secondary sources.

## Next Checks
1. **Implement and benchmark exponential smoothing**: Create a controlled experiment comparing standard uniform sampling against exponentially smoothed weighting across a multilingual corpus with known language distribution imbalances. Measure per-language performance to quantify the impact on low-resource language representation.

2. **Test modular architecture scalability**: Implement a simplified X-MOD-style architecture with shared attention/feed-forward components and language-specific adapter modules. Systematically increase the number of supported languages while measuring performance degradation patterns and memory overhead compared to a fully shared baseline.

3. **Validate tokenization robustness**: Conduct a comparative analysis of BPE, WordPiece, and SentencePiece on morphologically diverse language pairs (e.g., Chinese vs Turkish vs English). Measure vocabulary efficiency and downstream task performance to establish which tokenization approach handles specific linguistic challenges most effectively.