---
ver: rpa2
title: 'Through the Thicket: A Study of Number-Oriented LLMs derived from Random Forest
  Models'
arxiv_id: '2406.04926'
source_url: https://arxiv.org/abs/2406.04926
tags:
- language
- data
- llms
- classi
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for training Large Language
  Models (LLMs) using knowledge transfer from Random Forest (RF) classifiers. The
  approach converts RF decision paths into natural language statements to generate
  training data for LLMs, enabling them to classify numerical data and explain their
  decisions.
---

# Through the Thicket: A Study of Number-Oriented LLMs derived from Random Forest Models

## Quick Facts
- arXiv ID: 2406.04926
- Source URL: https://arxiv.org/abs/2406.04926
- Reference count: 9
- Key outcome: Novel method trains LLMs using knowledge transfer from Random Forest classifiers, converting decision paths to natural language statements for classification and explanation of numerical data

## Executive Summary
This paper presents a novel approach to training Large Language Models (LLMs) to classify numerical data and provide interpretable explanations by leveraging knowledge transfer from Random Forest (RF) classifiers. The method converts RF decision paths into natural language statements, which serve as training data for LLMs. The approach includes preprocessing techniques like integer normalization, verbal descriptions of values, and relation encoding to improve LLM understanding of numerical data. The researchers validate the correctness of LLM-generated explanations by applying them as logical filters to the training data and measuring classification performance.

## Method Summary
The method trains LLMs to classify numerical data using knowledge transfer from Random Forest classifiers. RF decision paths are converted into natural language statements and used as training data for LLMs. The approach includes preprocessing steps like integer normalization, verbal descriptions, and relation encoding to improve LLM comprehension of numerical data. LLMs are fine-tuned using LoRA to map preprocessed inputs to decision paths and labels. The correctness of LLM-generated explanations is verified by parsing them into logical conditions and applying them to the training set, measuring precision and recall metrics.

## Key Results
- High label accuracy (typically above 95%) across tested datasets
- Statement accuracy above 80% when relation encoding is enabled
- Successful application to classification tasks with interpretable explanations
- Demonstrates potential for using LLMs as explainable classification models in various applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting RF decision paths into natural language statements enables LLMs to classify numerical data while providing interpretable explanations.
- Mechanism: RF generates decision paths for each training example, which are converted into conditional statements (e.g., "petal length 64 (upper quantile) is less than 64 (upper quantile)"). These statements, paired with correct labels, serve as training data for the LLM, which learns to reproduce both classification and reasoning.
- Core assumption: LLMs can generalize from structured decision paths to unseen numerical data, and RF decision paths sufficiently describe the data space for classification.
- Evidence anchors:
  - [abstract] "By converting RF decision paths into natural language statements, we generate outputs for LLM fine-tuning, enhancing the model's ability to classify and explain its decisions."
  - [section 2.1.2] "We define an example x âˆˆ X as belonging to the path if: ... This is used to create the LLM output representing a decision path for any labelled example in the form: statement1 and . . . and statement H Label: class label"
- Break condition: LLM fails to parse or generate valid logical statements, or RF decision paths don't capture true decision boundaries.

### Mechanism 2
- Claim: Preprocessing numerical data using integer normalization, verbal description, and relation encoding improves LLM comprehension and reasoning.
- Mechanism: Floating-point numbers are transformed into 2-digit integers (IN), augmented with quartile-based labels (VD), and relational operators are converted to words (RE). This reduces tokenization complexity and provides richer context about data distribution.
- Core assumption: LLMs struggle with raw numerical values and benefit from simplified, context-rich representations.
- Evidence anchors:
  - [section 2.1] "Tokenization converts such statements, including the floating-point values, into a series of tokens... To facilitate LLM understanding and reasoning with such statements, we perform several preprocessing steps, including: 1. Integer normalisation (IN)... 2. Verbal description of values (VD)... 3. Relation encoding (RE)"
  - [section 3] "Enabling RE sharply increased the number of correct (parsable) statements. Without RE, common errors included missing relation (e.g., inequality) signs, which were rare with RE enabled."
- Break condition: Integer normalization introduces too much error, or verbal descriptions are misleading due to dataset shifts.

### Mechanism 3
- Claim: LLM-generated explanations can be objectively validated by applying them as logical filters to the training data and measuring classification performance.
- Mechanism: Each generated explanation (sequence of conditional statements) is parsed into Python code and applied to the training set. The subset of examples satisfying conditions is checked for label purity using precision and recall metrics.
- Core assumption: If an explanation correctly identifies a region of feature space dominated by a single class, it is a valid explanation of the model's decision.
- Evidence anchors:
  - [section 2.2] "We verify the correctness of LLM explanation using precision and recall metrics applied to the set Tpredicted, using the predicted label from LLM output and true labels from the training set."
  - [section 3] "For LLM explanations, we observed high average statement accuracy and recall with RE+IN+VN preprocessing, both above 80% and 74%, respectively."
- Break condition: Parsed conditions are ambiguous or overly broad, leading to low precision/recall, or LLM generates contradictory statements.

## Foundational Learning

- Concept: Random Forest decision paths
  - Why needed here: The method relies on sampling individual decision trees from the RF and extracting their decision paths to form explanations.
  - Quick check question: What is a decision path in a decision tree, and how does it relate to feature thresholds and class labels?

- Concept: Tokenization and numerical representation in LLMs
  - Why needed here: The method modifies how numbers are represented in prompts to improve LLM comprehension (e.g., integer normalization, verbal descriptions).
  - Quick check question: Why might splitting a floating-point number into multiple tokens hinder an LLM's ability to reason about it?

- Concept: Sequence-to-sequence fine-tuning with LoRA
  - Why needed here: The LLM is fine-tuned using LoRA to adapt its weights efficiently for the classification and explanation task.
  - Quick check question: What is LoRA, and how does it differ from full fine-tuning in terms of parameter efficiency and training speed?

## Architecture Onboarding

- Component map:
  Random Forest Classifier -> Preprocessing Module -> LLM (FLAN-T5-base) -> Parser -> Validator

- Critical path:
  1. Train RF on dataset
  2. Sample decision paths for each example
  3. Preprocess features and decision paths
  4. Fine-tune LLM with preprocessed data
  5. Validate LLM outputs by parsing and applying to training set

- Design tradeoffs:
  - Depth of RF trees vs. output length: Deeper trees yield more complex explanations but risk exceeding LLM context limits
  - Preprocessing granularity vs. information loss: Integer normalization simplifies tokens but may lose precision
  - Number of trees sampled per example vs. dataset size: More trees increase training data but may introduce redundancy

- Failure signatures:
  - LLM outputs contain unparsable text or missing relational operators
  - Validation metrics (precision/recall) drop significantly, indicating poor explanation quality
  - Label accuracy is high but statement accuracy is low, suggesting the model memorizes labels without understanding

- First 3 experiments:
  1. Train RF and generate decision paths on a simple dataset (e.g., Iris) without preprocessing; fine-tune LLM and evaluate label accuracy
  2. Add integer normalization preprocessing and retrain; compare label and statement accuracy
  3. Add relation encoding and retrain; verify if parsability of LLM outputs improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the depth of the Random Forest classifier affect the performance of the fine-tuned LLM in terms of classification accuracy and explanation quality?
- Basis in paper: [explicit] The authors mention that the depth of the RF is the most critical parameter and that more complex datasets require deeper decision trees, resulting in more extended outputs.
- Why unresolved: The paper only briefly touches on this aspect and does not provide a detailed analysis of how varying the depth of the RF impacts the LLM's performance.
- What evidence would resolve it: Conducting experiments with different depths of the RF and analyzing the corresponding changes in the LLM's classification accuracy and the quality of its explanations would provide insights into this relationship.

### Open Question 2
- Question: Can the proposed method be effectively applied to non-numerical data, such as text or images, and what modifications would be necessary?
- Basis in paper: [inferred] The authors mention the potential application of the methodology to Computer Vision problems, such as hyperspectral classification, but do not explore this in detail.
- Why unresolved: The paper focuses on numerical data and does not explore the application of the method to other types of data.
- What evidence would resolve it: Testing the method on datasets with text or image data and identifying the necessary preprocessing steps and adaptations would determine its effectiveness for non-numerical data.

### Open Question 3
- Question: How does the choice of preprocessing techniques (e.g., integer normalization, verbal description, relation encoding) impact the LLM's ability to generate accurate and interpretable explanations?
- Basis in paper: [explicit] The authors discuss the impact of preprocessing techniques on the representation of numerical data and their influence on classification accuracy and rule correctness.
- Why unresolved: While the paper presents results for different preprocessing combinations, it does not provide a comprehensive analysis of how each technique individually affects the LLM's performance.
- What evidence would resolve it: Conducting experiments with each preprocessing technique individually and in various combinations, while measuring the impact on the LLM's classification accuracy and explanation quality, would provide insights into their relative importance.

## Limitations
- Evaluation limited to three small tabular datasets (Iris, Wine, Breast Cancer), constraining generalizability to more complex numerical data
- Method's performance on high-dimensional datasets or continuous-valued regression tasks remains unknown
- Study does not address computational costs or runtime efficiency when scaling to larger datasets or deeper RF models
- Validation mechanism relies on training data, which may not capture generalization to unseen data distributions

## Confidence
- High Confidence: The core mechanism of converting RF decision paths to natural language for LLM training is well-specified and validated through multiple preprocessing variations. The validation framework using precision and recall on training subsets is clearly defined.
- Medium Confidence: The claim that relation encoding significantly improves parsability is supported by the results, but the relative contribution of each preprocessing step (IN, VD, RE) is not systematically isolated.
- Low Confidence: The paper's suggestion that this approach could extend to computer vision tasks lacks any experimental validation or theoretical justification for how RF-derived explanations would translate to image data.

## Next Checks
1. **Cross-dataset generalization test**: Evaluate the approach on at least five additional datasets with varying dimensionality, feature types, and decision boundary complexity to assess robustness beyond the initial three datasets.

2. **Ablation study on preprocessing components**: Systematically disable each preprocessing step (IN, VD, RE) individually and in combinations to quantify their specific contributions to label accuracy, statement accuracy, and parsability.

3. **Out-of-distribution validation**: Apply the trained LLM to held-out test sets or synthetic data with distribution shifts to verify that explanations remain accurate and meaningful when faced with data the RF model did not directly generate paths for.