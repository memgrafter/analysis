---
ver: rpa2
title: Are LLMs good pragmatic speakers?
arxiv_id: '2411.01562'
source_url: https://arxiv.org/abs/2411.01562
tags:
- pragmatic
- meaning
- language
- function
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) exhibit
  pragmatic reasoning similar to humans, using the Rational Speech Act (RSA) framework
  to compare LLM (Llama3-8B-Instruct) and RSA model scoring in reference game tasks.
  The study constructs utterance spaces using both top-k LLM sampling and logical
  rules, evaluating scores against RSA models with prompt-based and rule-based meaning
  functions.
---

# Are LLMs good pragmatic speakers?

## Quick Facts
- arXiv ID: 2411.01562
- Source URL: https://arxiv.org/abs/2411.01562
- Reference count: 40
- LLMs show limited pragmatic reasoning ability in reference games, with positive correlation to RSA models (PCC = 0.291-0.303, SRCC = 0.606-0.736)

## Executive Summary
This paper investigates whether large language models exhibit pragmatic reasoning similar to humans using the Rational Speech Act (RSA) framework. The study compares LLM (Llama3-8B-Instruct) and RSA model scoring in reference game tasks constructed from the TUNA corpus. While the LLM demonstrates ability to distinguish utterance types (literal vs. pragmatic), it struggles particularly with ranking pragmatic utterances within groups, showing limited evidence of pragmatic speaker behavior.

## Method Summary
The study constructs reference games from the TUNA dataset (furniture domain) with 2,940 games containing 7 objects each. Utterance spaces are generated using both top-k LLM sampling and logical rules based on feature combinations. The Llama3-8B-Instruct model scores utterances using logit probabilities, which are then compared against RSA models using two meaning functions: prompt-based (3-shot prompting) and rule-based (feature exclusion). Correlation analysis using PCC and SRCC measures alignment between LLM and RSA scores across different α values.

## Key Results
- Positive correlation between LLM and RSA scores (PCC = 0.291-0.303, SRCC = 0.606-0.736)
- Stronger alignment for logic-constructed utterances compared to top-k generated ones
- Rule-based meaning functions outperform prompt-based ones for structured reference games
- LLM struggles with ranking pragmatic utterances within groups despite distinguishing utterance types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can distinguish utterance types (literal vs. pragmatic) in reference games
- Mechanism: LLM scoring shows correlation with RSA models, indicating ability to differentiate utterance categories
- Core assumption: Top-k and logic-constructed utterances are sufficiently distinct for LLM to recognize
- Evidence anchors:
  - [abstract]: "LLM demonstrates limited pragmatic reasoning ability, struggling particularly with ranking pragmatic utterances within groups"
  - [section]: "Correlation plots for each α are provided in Appendix E. Our results indicate that varying α does not alter our primary finding: the scores from LLM do not exhibit strong alignment with those of the RSA models"
  - [corpus]: "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog" (FMR=0.0) - limited corpus evidence for this specific mechanism
- Break condition: When utterance space becomes too diverse or ambiguous, LLM fails to maintain type distinctions

### Mechanism 2
- Claim: Rule-based meaning functions outperform prompt-based ones for structured reference games
- Mechanism: Logical exclusion rules capture literal relationships more reliably than LLM-based prompt scoring
- Core assumption: Feature-based exclusion is sufficient for determining utterance-object relationships in controlled settings
- Evidence anchors:
  - [section]: "the rule-based meaning function consistently identifies literal relationships for the logical-constructed sequences, and most of the top-k generated sequences (mean Acc = 99.9%)"
  - [section]: "the rule-based MF outperforms the prompt-based approach in factual judgement tasks, highlighting the LLM's strength in structured reasoning"
  - [corpus]: "Non-literal Understanding of Number Words by Language Models" (FMR=0.0) - limited corpus support for structured reasoning
- Break condition: When utterances require complex natural language understanding beyond feature exclusion

### Mechanism 3
- Claim: LLMs struggle with pragmatic ranking within utterance groups
- Mechanism: While LLMs can separate literal from pragmatic utterances, they fail to rank pragmatic options effectively
- Core assumption: Pragmatic ranking requires deeper reasoning than type classification
- Evidence anchors:
  - [abstract]: "the LLM demonstrates limited pragmatic reasoning ability, struggling particularly with ranking pragmatic utterances within groups"
  - [section]: "SRCC scores ranging from -0.25 to 0.50 for both SRCC graphs" indicating poor ranking ability
  - [corpus]: "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts" (FMR=0.0) - no corpus evidence for this mechanism
- Break condition: When pragmatic utterances are too similar or require context beyond the reference game

## Foundational Learning

- Concept: Rational Speech Act (RSA) framework
  - Why needed here: RSA provides the theoretical foundation for modeling pragmatic reasoning in human communication
  - Quick check question: How does RSA model the recursive reasoning between speakers and listeners?

- Concept: Reference game paradigm
  - Why needed here: Reference games provide a controlled setting to evaluate pragmatic communication
  - Quick check question: What distinguishes a literal utterance from a pragmatic one in reference games?

- Concept: Meaning function design
  - Why needed here: Different meaning functions (rule-based vs. prompt-based) significantly impact evaluation results
  - Quick check question: How does the choice of meaning function affect the correlation between LLM and RSA scores?

## Architecture Onboarding

- Component map: TUNA corpus -> Utterance space generation -> LLM scoring -> RSA model implementation -> Correlation analysis pipeline

- Critical path:
  1. Construct reference game world
  2. Generate alternative utterances
  3. Score utterances using LLM
  4. Score utterances using RSA models
  5. Compare scores using PCC and SRCC

- Design tradeoffs:
  - Top-k sampling vs. logical construction: flexibility vs. control
  - Prompt-based vs. rule-based meaning functions: generalization vs. reliability
  - Single α vs. multiple α values: simplicity vs. robustness

- Failure signatures:
  - Low PCC/SRCC scores indicating poor alignment
  - Inconsistent scoring across similar utterances
  - Hallucinations in top-k generated utterances

- First 3 experiments:
  1. Test correlation with different α values (0.2, 1.0, 3.0)
  2. Compare performance on top-k vs. logic-constructed utterances
  3. Evaluate meaning function accuracy on held-out test cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the alignment between LLM and RSA scores improve with more advanced LLMs like GPT models?
- Basis in paper: [explicit] The paper states "Testing on other LLMs, especially those with advanced pragmatic reasoning like GPT models trained on large datasets, would provide deeper insights into handling pragmatic tasks."
- Why unresolved: The study only tested Llama3-8B-Instruct, limiting generalizability to other LLM architectures.
- What evidence would resolve it: Direct comparison of multiple LLM models (GPT, Claude, etc.) using the same RSA framework and evaluation metrics.

### Open Question 2
- Question: How does iterative reasoning (multiple RSA iterations) affect the alignment between LLM and RSA model scores?
- Basis in paper: [explicit] The paper suggests "Further research could also compare LLM alignment with the RSA model when iterated multiple times, rather than a single interaction."
- Why unresolved: The current study only uses single-step RSA reasoning without exploring multi-step iterations.
- What evidence would resolve it: Experiments comparing single-step vs multi-step RSA iterations across various α values and measuring correlation changes.

### Open Question 3
- Question: Does scaling the cost function or α parameter differently affect LLM-RSA alignment?
- Basis in paper: [explicit] The paper mentions "examine the effects of scaling parameters and cost functions on alignment" and notes that "increasing α sharpens the probability distribution."
- Why unresolved: The study only tested a limited range of α values (0.2-3.0) without exploring extreme values or alternative cost functions.
- What evidence would resolve it: Systematic experiments varying α and cost functions across multiple orders of magnitude to identify optimal alignment conditions.

## Limitations

- The study uses a controlled experimental setting with the TUNA corpus, which represents a narrow furniture domain with predefined attributes
- Results are based on correlation metrics that cannot definitively establish genuine pragmatic reasoning versus pattern matching
- The study only tests Llama3-8B-Instruct, limiting generalizability to other LLM architectures and scales

## Confidence

**High Confidence**: The finding that LLM scores show positive correlation with RSA models (PCC = 0.291-0.303, SRCC = 0.606-0.736) is well-supported by the experimental data and robust across different α values. The observation that rule-based meaning functions outperform prompt-based ones in structured reference games is consistently demonstrated.

**Medium Confidence**: The conclusion that LLMs demonstrate limited pragmatic reasoning ability, particularly in ranking pragmatic utterances within groups, is supported but requires cautious interpretation due to the constrained experimental setting and the complexity of measuring pragmatic competence.

**Low Confidence**: The broader claim that these findings indicate LLMs lack sufficient evidence of pragmatic speaker behavior should be viewed with skepticism, as the study's scope and methodology may not capture the full range of pragmatic capabilities that LLMs might possess in less constrained settings.

## Next Checks

1. **Cross-Domain Validation**: Replicate the experiment using reference games from different domains (e.g., animals, vehicles, or abstract concepts) to test the generalizability of the findings beyond the furniture domain.

2. **Model Size Comparison**: Conduct the same evaluation using larger LLM variants (e.g., Llama3-70B) to determine if model scale influences pragmatic reasoning capabilities and correlation with RSA models.

3. **Natural Language Utterance Test**: Replace the logically constructed utterances with naturally occurring pragmatic expressions from human conversations to assess whether the observed limitations persist in more realistic communication scenarios.