---
ver: rpa2
title: 'From Tokens to Words: On the Inner Lexicon of LLMs'
arxiv_id: '2410.05864'
source_url: https://arxiv.org/abs/2410.05864
tags:
- words
- word
- token
- tokens
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Large Language Models (LLMs) internally
  process words that are split into sub-word tokens by their tokenizer. The authors
  demonstrate that LLMs reconstruct whole-word representations at the final token
  of multi-token words through a process they term "detokenization," primarily occurring
  in early-to-middle layers.
---

# From Tokens to Words: On the Inner Lexicon of LLMs

## Quick Facts
- arXiv ID: 2410.05864
- Source URL: https://arxiv.org/abs/2410.05864
- Reference count: 40
- This paper demonstrates that LLMs reconstruct whole-word representations at the final token of multi-token words through detokenization.

## Executive Summary
This paper investigates how Large Language Models (LLMs) internally process words that are split into sub-word tokens by their tokenizer. The authors demonstrate that LLMs reconstruct whole-word representations at the final token of multi-token words through a process they term "detokenization," primarily occurring in early-to-middle layers. They provide evidence that this is achieved by first aggregating information from previous sub-word tokens via attention, then retrieving a complete word concept from the feedforward network layers, which act as an internal lexicon. Their method enables expanding an LLM's vocabulary with new words without finetuning, by detecting words the model already "knows" and assigning them new token embeddings based on their internal representations.

## Method Summary
The authors use probing classifiers, logit lens method, Patchscopes technique, attention analysis, and ablation experiments to investigate how LLMs process multi-token words. They extract detokenized representations from hidden states and learn linear maps to project these into embedding/unembedding spaces for vocabulary expansion, all without modifying core model parameters.

## Key Results
- LLMs build internal word-level representations at the final token of multi-token words through attention aggregation and feedforward retrieval
- The internal lexicon is more exhaustive than the tokenizer's vocabulary, allowing recognition of out-of-vocabulary words
- The method reduces sequence length by 10-15% across datasets while maintaining or slightly improving model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The model builds internal word-level representations even though its tokenizer outputs sub-word tokens.
- **Mechanism:** At the last token of a multi-token word, attention aggregates information from preceding sub-word tokens, then feedforward layers retrieve the complete word representation from an internal lexicon.
- **Core assumption:** The internal lexicon is structured as a soft, non-unique mapping from sub-word sequences to word vectors, rather than a strict key-value dictionary.
- **Evidence anchors:**
  - [abstract] "LLMs engage in an intrinsic detokenization process, where sub-word sequences are combined into coherent whole-word representations at their last token."
  - [section] "Our results hint that language models hold a latent vocabulary or inner lexicon, which they access to identify words from token sequences."
  - [corpus] Weak—most corpus neighbors discuss tokenization, not internal lexicon reconstruction.
- **Break condition:** If attention does not preferentially attend to preceding sub-word tokens in early layers, or if feedforward updates do not consistently retrieve word concepts before they emerge in hidden states.

### Mechanism 2
- **Claim:** The internal lexicon is more exhaustive than the tokenizer's vocabulary, allowing recognition of out-of-vocabulary words.
- **Mechanism:** Multi-token words absent from the tokenizer's BPE vocabulary are still processed as cohesive units because the model can retrieve their internal representations from the feedforward layers.
- **Core assumption:** The feedforward layers act as key-value memories that store word-level concepts beyond the tokenizer's scope.
- **Evidence anchors:**
  - [abstract] "when feeding the last token internal representations of such words to the model as input, it can 'understand' them as the complete word despite never seeing such representations as input during training."
  - [section] "Interestingly, we observe that 22.6% of the multi-token words are never successfully decoded from any internal layer, hinting that they might not be represented in the model's inner lexicon."
  - [corpus] Weak—corpus neighbors focus on tokenization rather than internal word representations.
- **Break condition:** If feedforward layers do not store or retrieve word-level representations, or if the model fails to recognize multi-token words not present in its vocabulary.

### Mechanism 3
- **Claim:** The model can use its internal lexicon to expand vocabulary without finetuning by assigning new token embeddings based on internal representations.
- **Mechanism:** By extracting detokenized representations from the model's hidden states and projecting them into embedding/unembedding spaces, new vocabulary entries can be created that the model can use as inputs and outputs.
- **Core assumption:** The model can generalize to new token embeddings derived from its own internal representations without additional training.
- **Evidence anchors:**
  - [abstract] "Our findings suggest that LLMs maintain a latent vocabulary beyond the tokenizer's scope. These insights provide a practical, finetuning-free application for expanding the vocabulary of pre-trained models."
  - [section] "By enabling the addition of new vocabulary words, we reduce input length and inference iterations, which reduces both space and model latency, with little to no loss in model accuracy."
  - [corpus] Weak—corpus neighbors discuss vocabulary expansion methods that require finetuning.
- **Break condition:** If the model cannot generalize to new token embeddings derived from its internal representations, or if performance degrades significantly when using expanded vocabulary.

## Foundational Learning

- **Concept:** Byte-Pair Encoding (BPE) tokenization
  - Why needed here: Understanding how words are split into sub-word tokens is fundamental to grasping why LLMs need detokenization mechanisms.
  - Quick check question: Why does BPE tokenization sometimes split words like "unhappiness" into "un," "h," and "appiness"?

- **Concept:** Transformer attention mechanism
  - Why needed here: The attention mechanism is crucial for aggregating information from preceding sub-word tokens into the final token of a word.
  - Quick check question: How does multi-head attention enable the model to selectively focus on relevant sub-word tokens when reconstructing word representations?

- **Concept:** Feedforward networks as memory
  - Why needed here: The feedforward layers are shown to store and retrieve word-level representations, functioning as an internal lexicon.
  - Quick check question: What evidence suggests that feedforward layers in transformers can act as key-value memories for linguistic knowledge?

## Architecture Onboarding

- **Component map:** Tokenizer -> Embedding layer -> Transformer layers -> Output layer
- **Critical path:**
  1. Tokenization of input text into sub-word tokens
  2. Initial embedding of tokens
  3. Layer-by-layer processing where attention aggregates sub-word information and feedforward layers retrieve word representations
  4. Final token outputs that can represent complete words
  5. Potential vocabulary expansion using internal representations

- **Design tradeoffs:**
  - Fixed vocabulary vs. dynamic expansion: The method trades some complexity for the ability to add new words without retraining
  - Computational overhead: Vocabulary expansion requires additional projection matrices but saves tokens in inference
  - Coverage vs. efficiency: The internal lexicon covers many words but not all, affecting which words can be expanded

- **Failure signatures:**
  - Words not recognized as cohesive units (attention doesn't aggregate sub-word tokens)
  - Feedforward layers fail to retrieve word representations (logit lens shows poor retrieval)
  - New vocabulary entries don't improve or degrade performance
  - Token reduction doesn't materialize despite successful expansion

- **First 3 experiments:**
  1. Verify word vs. nonword classification across layers using k-NN probe on hidden states
  2. Test logit lens retrieval for artificially split single-token words across model layers
  3. Apply Patchscopes to multi-token words to see if the model can regenerate the full word from the last token's hidden state

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the detokenization process extend to syntactic words that are split into multiple tokens due to morphological complexity (e.g., agglutinative languages)?
- Basis in paper: [explicit] The paper notes that our method shows potential for morphologically rich languages, particularly in the Arabic dataset, but does not explore this systematically.
- Why unresolved: The experiments primarily focus on English and Arabic, and do not directly test languages with extreme morphological complexity where a single word can contain multiple morphemes.
- What evidence would resolve it: Conducting experiments on languages like Turkish, Finnish, or Hungarian to measure the success rate of detokenization and vocabulary expansion for highly agglutinative words.

### Open Question 2
- Question: How does the detokenization mechanism interact with the model's attention heads that perform specific sub-word merging operations?
- Basis in paper: [inferred] The paper mentions Ferrando & V oita (2024) observed attention heads that promote sub-word merging, but does not investigate whether these heads are responsible for the detokenization process.
- Why unresolved: The analysis focuses on aggregate attention patterns rather than identifying specific attention heads involved in detokenization.
- What evidence would resolve it: Using causal tracing or attention head ablation to determine which specific attention heads are necessary for successful detokenization of multi-token words.

### Open Question 3
- Question: What is the relationship between the internal lexicon and the model's factual recall capabilities?
- Basis in paper: [explicit] The ablation experiments show that removing FFN updates carrying full-word representations dramatically reduces retrieval rates and affects the model's ability to retrieve capital cities of countries.
- Why unresolved: While the paper demonstrates that FFN updates are important for both detokenization and factual recall, it does not explore whether the internal lexicon stores factual associations or if factual knowledge is stored separately.
- What evidence would resolve it: Analyzing whether factual knowledge about multi-token entities (like country names) is retrieved from the same FFN layers that store word representations, or if there is a separate mechanism for factual recall.

## Limitations

- The corpus analysis reveals weak semantic similarity between cited papers and core claims about internal lexicon reconstruction, suggesting mechanism identification relies heavily on direct experimental evidence rather than established theoretical frameworks
- The method assumes a relatively straightforward attention-to-feedforward pipeline for word reconstruction, but doesn't fully explore alternative explanations such as distributed representations across multiple tokens
- The 22.6% of multi-token words that cannot be successfully decoded from internal layers raises questions about the completeness and reliability of the proposed inner lexicon mechanism

## Confidence

**High Confidence**: The empirical demonstrations of detokenization at the final token of multi-token words through attention aggregation and feedforward retrieval are well-supported by multiple experimental approaches including k-NN probing, logit lens, and Patchscopes. The layer-wise progression from early aggregation to middle-layer retrieval is consistently observed across different model architectures.

**Medium Confidence**: The characterization of the feedforward network as functioning as an "internal lexicon" with key-value memory properties is supported by evidence but remains somewhat speculative. The paper shows that representations can be retrieved but doesn't definitively prove the underlying storage mechanism or whether this constitutes a true vocabulary-like structure versus distributed processing.

**Low Confidence**: The vocabulary expansion application's practical utility and long-term stability are less well-established. While token-level accuracy and sequence length metrics are positive, the lack of comprehensive downstream evaluation and exploration of potential failure modes or semantic quality issues limits confidence in the approach's robustness.

## Next Checks

1. **Cross-linguistic verification**: Test the detokenization mechanism across languages with different morphological structures (highly agglutinative vs. analytic) to determine whether the attention-aggregation pattern is universal or language-dependent.

2. **Ablation of attention patterns**: Systematically ablate attention heads that show strong multi-token word aggregation patterns to determine whether these specific attention mechanisms are necessary and sufficient for detokenization.

3. **Long-term stability evaluation**: Conduct a longitudinal study tracking the semantic drift and performance stability of models with expanded vocabulary over extended inference sessions and across multiple domains.