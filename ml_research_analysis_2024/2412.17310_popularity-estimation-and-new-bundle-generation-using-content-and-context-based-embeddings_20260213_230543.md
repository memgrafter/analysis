---
ver: rpa2
title: Popularity Estimation and New Bundle Generation using Content and Context based
  Embeddings
arxiv_id: '2412.17310'
source_url: https://arxiv.org/abs/2412.17310
tags:
- bundle
- bundles
- embeddings
- games
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to product bundling by defining
  new popularity metrics and using content/context-aware embeddings to generate optimized
  bundles. It models bundle popularity as a classification problem, evaluates various
  embeddings (SBERT, Fasttext, Prod2Vec, MetaProd2Vec), and employs iterative strategies
  (add/replace/delete) to improve existing bundles.
---

# Popularity Estimation and New Bundle Generation using Content and Context based Embeddings

## Quick Facts
- arXiv ID: 2412.17310
- Source URL: https://arxiv.org/abs/2412.17310
- Authors: Ashutosh Nayak; Prajwal NJ; Sameeksha Keshav; Kavitha S. N.; Roja Reddy; Rajasekhara Reddy Duvvuru Muni
- Reference count: 23
- One-line primary result: Novel approach to product bundling using popularity metrics and embeddings achieves up to 52% improvement in bundle popularity metrics

## Executive Summary
This paper introduces a novel approach to product bundling by defining new popularity metrics and using content/context-aware embeddings to generate optimized bundles. The methodology models bundle popularity as a classification problem, evaluates various embeddings (SBERT, Fasttext, Prod2Vec, MetaProd2Vec), and employs iterative strategies (add/replace/delete) to improve existing bundles. Experiments on Steam Games data show up to 52% improvement in popularity metrics including explicit purchases, playtime, and diversity, with strong AUC and F1-scores.

## Method Summary
The methodology uses Steam Games dataset to build embeddings from game titles, tags, genres, and specifications. It trains classification models to predict bundle popularity categories using these embeddings along with game metadata features like price, age, and discount. The approach employs iterative greedy sampling strategies to generate new bundles by selecting games that improve predicted popularity metrics based on embedding similarity. The system evaluates improvements using multiple popularity metrics and compares performance across different embedding types.

## Key Results
- Up to 52% improvement in popularity metrics across explicit/implicit purchases, playtime, and diversity measures
- Content-aware embeddings (SBERT + title+genre) outperform context-aware embeddings for bundle popularity prediction
- Strong classification performance with AUC and F1-scores validating the effectiveness of the popularity classification approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Content-aware embeddings (SBERT + title+genre) outperform context-aware embeddings for bundle popularity prediction because they encode domain-specific semantics that better reflect bundle quality.
- Mechanism: The model uses SBERT to generate dense vector representations from concatenated title and genre information, capturing semantic relationships between game titles and genres. This embedding captures thematic coherence within bundles better than co-purchase patterns alone.
- Core assumption: The textual descriptors (title, genre) provided by game developers accurately represent the game's content and thematic relationships with other games.
- Evidence anchors:
  - [abstract] "use these metrics in the methodology proposed in this paper to generate new bundles for mobile games using content aware and context aware embeddings"
  - [section] "Content aware embeddings perform better than context aware embeddings on average, therefore M-P2V outperforms P2V"
  - [corpus] Weak - no direct corpus evidence comparing content vs context embeddings for bundling
- Break condition: If game titles and genres are sparse, inconsistent, or fail to capture the actual gameplay experience, the embeddings will not reflect true bundle compatibility.

### Mechanism 2
- Claim: Bundle popularity can be modeled as a classification problem using a linear logistic regression that combines game embeddings, price, age, discount, and aggregated playtime metrics.
- Mechanism: The model treats bundle quality as a binary classification task, where the input features include mean game embeddings, bundle price, age, discount, total purchases, and playtime per download. The linear decision boundary effectively separates popular from unpopular bundles.
- Core assumption: Bundle popularity follows a linear relationship with the combined features, and the dataset size (615 bundles) is sufficient for logistic regression without overfitting.
- Evidence anchors:
  - [abstract] "model product bundling as classification problem, thus we categorize bundles as Unpopular (Cat 1), Popular (Cat 2) and Very popular (Cat 3)"
  - [section] "Due to skewness in the dataset, we model the problem of identifying the popularity of a bundle as classification problem"
  - [corpus] Missing - no corpus evidence on linear vs non-linear classification performance for bundle popularity
- Break condition: If the relationship between bundle features and popularity is highly non-linear or if important features are missing, the linear model will fail to capture the true popularity signal.

### Mechanism 3
- Claim: Iterative greedy sampling strategies (replace, add, delete) can improve existing bundles by selecting games that increase predicted popularity metrics based on embedding similarity.
- Mechanism: The algorithm samples games from the embedding space based on distance to the bundle centroid, then evaluates whether adding/replacing/deleting games improves popularity metrics using the trained classifier. This greedy approach converges to locally optimal bundles.
- Core assumption: Small, iterative changes to bundles can systematically improve popularity metrics, and the embedding space preserves meaningful similarity relationships between games.
- Evidence anchors:
  - [abstract] "employs iterative strategies (add/replace/delete) to improve existing bundles"
  - [section] "use an iterative greedy sampling approach to create new bundles from the existing bundles"
  - [corpus] Weak - no corpus evidence on greedy vs other optimization strategies for bundle generation
- Break condition: If the embedding space is noisy or if local improvements don't lead to global optima, the greedy approach may get stuck in suboptimal solutions.

## Foundational Learning

- Concept: Embeddings and similarity metrics
  - Why needed here: The methodology relies on using embeddings to represent games and measure similarity between games for bundle construction
  - Quick check question: How does cosine similarity between game embeddings relate to the likelihood that two games should be bundled together?

- Concept: Classification modeling and evaluation metrics
  - Why needed here: The paper models bundle popularity as a classification problem and uses metrics like AUC and F1-score to evaluate model performance
  - Quick check question: Why is AUC a more appropriate metric than accuracy for this imbalanced bundle popularity classification task?

- Concept: Iterative optimization and greedy algorithms
  - Why needed here: The bundle generation uses iterative strategies to improve existing bundles by sampling and evaluating changes
  - Quick check question: What are the potential drawbacks of using a greedy algorithm for bundle optimization compared to global optimization methods?

## Architecture Onboarding

- Component map: Data preprocessing → Embedding generation (SBERT/FastText/Prod2Vec/MetaProd2Vec) → Feature engineering (aggregated metrics) → Classification model training (logistic regression) → Bundle optimization (iterative sampling) → Evaluation (popularity metrics)
- Critical path: Embedding generation → Classification model training → Bundle optimization
- Design tradeoffs: Linear logistic regression chosen for simplicity and to avoid overfitting on small dataset vs. complex neural networks that might overfit; SBERT embeddings chosen for semantic richness vs. computational efficiency of FastText
- Failure signatures: Poor classification performance (low AUC/F1) indicates embedding quality issues or feature engineering problems; minimal bundle improvement indicates optimization strategy issues or embedding space problems
- First 3 experiments:
  1. Train classification model using only SBERT title embeddings vs. using full feature set to validate feature importance
  2. Compare greedy iterative optimization vs. random bundle generation to establish baseline improvement
  3. Test different distance thresholds for game sampling in embedding space to optimize the trade-off between exploration and exploitation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed popularity metrics be validated across different bundling domains (e.g., music, e-commerce) to ensure generalizability?
- Basis in paper: [explicit] The paper mentions the methodology is generic and can be extended to other bundling problems, but does not provide cross-domain validation.
- Why unresolved: The study focuses on Steam Games data, and no experiments are conducted on other domains like music or e-commerce.
- What evidence would resolve it: Conducting experiments on datasets from different domains (e.g., Spotify playlists, Amazon product bundles) and comparing the effectiveness of the proposed metrics across these domains.

### Open Question 2
- Question: How do dynamic bundling strategies (session-based) compare to static bundling in terms of consumer engagement and revenue generation?
- Basis in paper: [inferred] The paper focuses on static bundling and mentions dynamic bundling as a future research direction, but does not explore it.
- Why unresolved: The study does not include experiments or analysis on dynamic bundling strategies.
- What evidence would resolve it: Implementing and testing dynamic bundling strategies on a dataset with session-based interactions and comparing the results with static bundling in terms of engagement and revenue.

### Open Question 3
- Question: What are the computational trade-offs of using more sophisticated optimization models (e.g., non-greedy approaches) for bundle generation?
- Basis in paper: [explicit] The paper uses a naive greedy approach and suggests that more sophisticated optimization models could improve bundle generation.
- Why unresolved: The study does not explore alternative optimization models or their computational costs.
- What evidence would resolve it: Implementing and comparing different optimization models (e.g., genetic algorithms, reinforcement learning) for bundle generation and analyzing their computational efficiency and effectiveness.

## Limitations

- Small bundle dataset size (615 bundles) may not capture full diversity of bundle popularity patterns
- Effectiveness of content-aware embeddings depends heavily on quality and consistency of game metadata
- Greedy iterative optimization may converge to local optima rather than globally optimal bundles

## Confidence

- High confidence in classification performance metrics (AUC, F1-scores) due to established evaluation methods
- Medium confidence in embedding effectiveness, as superiority demonstrated but not rigorously compared against alternatives
- Medium confidence in bundle improvement claims, as 52% improvement metric combines multiple measures without statistical significance testing

## Next Checks

1. Perform ablation studies to quantify individual contribution of each feature (price, age, discount, playtime) to classification performance
2. Test generalization of bundling approach on different dataset (e.g., mobile app bundles) to validate domain transferability
3. Compare greedy iterative optimization strategy against sophisticated optimization methods (e.g., genetic algorithms, reinforcement learning) to establish baseline improvement bounds