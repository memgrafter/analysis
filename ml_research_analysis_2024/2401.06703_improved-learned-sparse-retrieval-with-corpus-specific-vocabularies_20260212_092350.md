---
ver: rpa2
title: Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies
arxiv_id: '2401.06703'
source_url: https://arxiv.org/abs/2401.06703
tags:
- retrieval
- vocabulary
- sparse
- document
- expansion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the efficiency-effectiveness trade-offs in
  learned sparse retrieval systems by proposing corpus-specific vocabularies (CSV).
  The core idea involves pre-training BERT on the target corpus with custom vocabulary
  sizes and incorporating these vocabularies into document expansion and sparse retrieval
  models.
---

# Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies

## Quick Facts
- arXiv ID: 2401.06703
- Source URL: https://arxiv.org/abs/2401.06703
- Reference count: 0
- Primary result: Corpus-specific vocabularies improve learned sparse retrieval quality by up to 12% and reduce latency by up to 50%

## Executive Summary
This paper addresses the efficiency-effectiveness trade-offs in learned sparse retrieval systems by proposing corpus-specific vocabularies (CSV). The core idea involves pre-training BERT on the target corpus with custom vocabulary sizes and incorporating these vocabularies into document expansion and sparse retrieval models. Experiments on MS MARCO datasets show that CSV improves retrieval quality by up to 12% and reduces latency by up to 50%. The approach is applicable to various models like SPLADE and uniCOIL, offering a simple yet effective method to enhance learned sparse retrieval systems.

## Method Summary
The approach involves creating corpus-specific vocabularies using WordPiece tokenization on the target corpus, pre-training BERT with these vocabularies using masked language modeling, and incorporating them into document expansion (via TILDE) and sparse retrieval models (uniCOIL, SPLADE). The method is evaluated on MS MARCO v1 and v2 datasets, comparing different vocabulary sizes (30k, 100k, 300k) against default BERT vocabularies.

## Key Results
- Corpus-specific vocabularies improve retrieval quality by up to 12% on MS MARCO datasets
- Larger vocabulary sizes reduce latency by up to 50% due to shorter postings lists
- The approach works across multiple sparse retrieval models including uniCOIL and SPLADE
- Vocabulary size of 100k was sufficient for effectiveness, while 300k provided additional latency benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Custom vocabularies from the target corpus improve retrieval quality by aligning model vocabulary with corpus usage patterns.
- Mechanism: Training WordPiece tokenizers on the target corpus creates vocabularies that reflect actual term distributions and reduce vocabulary mismatch during indexing.
- Core assumption: Corpus-specific vocabularies better represent query terms and document tokens than general pre-trained vocabularies.
- Evidence anchors: [abstract] "We find that pre-training the underlying BERT model on the target corpus, specifically targeting different vocabulary sizes incorporated into the document expansion process, improves retrieval quality by up to 12%"
- Break condition: If the target corpus contains many out-of-domain terms or if the corpus-specific vocabulary doesn't cover query terms adequately.

### Mechanism 2
- Claim: Larger vocabulary sizes reduce average postings list length, improving latency.
- Mechanism: More vocabulary entries distribute term frequencies across more tokens, reducing the number of documents per token and shortening postings lists.
- Core assumption: Shorter postings lists directly translate to faster query processing due to reduced I/O and processing overhead.
- Evidence anchors: [abstract] "Our experiments show that adopting corpus-specific vocabulary and increasing vocabulary size decreases average postings list length which in turn reduces latency."
- Break condition: If the additional overhead of managing larger vocabularies (e.g., more inverted index lists) outweighs the benefits of shorter postings lists.

### Mechanism 3
- Claim: Document expansion techniques benefit from corpus-specific vocabularies through better token prediction.
- Mechanism: Models like TILDE predict expansion tokens from the vocabulary space; using a corpus-specific vocabulary improves the quality of predicted expansion tokens.
- Core assumption: The quality of predicted expansion tokens directly impacts retrieval effectiveness.
- Evidence anchors: [section 3.4] "TILDE can also leverage CSV as it predicts additional document tokens over the underlying LM vocabulary space which we adjust and fine-tune to the target corpus."
- Break condition: If the document expansion doesn't meaningfully improve retrieval quality or if the expansion tokens are mostly irrelevant.

## Foundational Learning

- Concept: WordPiece tokenization and vocabulary learning
  - Why needed here: The paper relies on custom vocabulary creation using WordPiece for different vocabulary sizes.
  - Quick check question: How does WordPiece tokenization differ from byte-pair encoding, and why is it suitable for this application?

- Concept: Inverted index data structures and postings lists
  - Why needed here: Understanding how vocabulary size affects postings list length and query processing is central to the latency improvements.
  - Quick check question: What is the relationship between the number of unique terms and the average length of postings lists in an inverted index?

- Concept: Document expansion techniques in sparse retrieval
  - Why needed here: The paper uses TILDE for document expansion and proposes modifications that leverage corpus-specific vocabularies.
  - Quick check question: How does document expansion mitigate the vocabulary mismatch problem in information retrieval?

## Architecture Onboarding

- Component map: BERT model → Vocabulary selection → Pre-training → Document expansion (TILDE) → Sparse retrieval model (uniCOIL/SPLADE) → Inverted index
- Critical path: Corpus-specific vocabulary creation → BERT pre-training on target corpus → Document expansion with TILDE → Sparse retrieval model training → Index creation
- Design tradeoffs: Larger vocabularies improve quality and latency but increase model size and pre-training cost; document expansion improves quality but may increase latency
- Failure signatures: No improvement in retrieval quality despite corpus-specific vocabulary; increased latency despite shorter postings lists; document expansion tokens being irrelevant
- First 3 experiments:
  1. Create corpus-specific vocabularies of different sizes (30k, 100k, 300k) and measure basic statistics (token coverage, average postings list length)
  2. Pre-train BERT on the target corpus using these vocabularies and compare MLM loss convergence
  3. Train a simple sparse retrieval model (e.g., uniCOIL) with each vocabulary and measure retrieval quality and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does corpus-specific vocabulary selection affect retrieval effectiveness for non-English corpora?
- Basis in paper: [inferred] The paper demonstrates benefits of CSV on MS MARCO datasets but does not explore multilingual or non-English corpora.
- Why unresolved: The paper focuses exclusively on English language datasets, leaving the generalizability to other languages unexplored.
- What evidence would resolve it: Experiments comparing CSV performance across multiple languages and corpora would establish whether the approach transfers to non-English search scenarios.

### Open Question 2
- Question: What is the optimal vocabulary size for balancing retrieval effectiveness and latency across different corpus sizes?
- Basis in paper: [explicit] The paper experiments with 30K, 100K, and 300K vocabulary sizes but notes that 100K was sufficient for MS MARCO v1 while 300K provided latency improvements.
- Why unresolved: The paper shows varying results across different vocabulary sizes but does not provide a systematic framework for determining optimal vocabulary size based on corpus characteristics.
- What evidence would resolve it: A comprehensive study varying corpus sizes and measuring the trade-off between effectiveness and latency for different vocabulary sizes would establish guidelines for vocabulary selection.

### Open Question 3
- Question: How do corpus-specific vocabularies interact with other pre-training strategies like coCondenser?
- Basis in paper: [explicit] The paper mentions that search-specific pre-training tasks like coCondenser provide orthogonal benefits to vocabulary changes but leaves exploring potential interactions to future work.
- Why unresolved: The paper treats vocabulary selection and pre-training strategies as independent factors without examining their combined effects.
- What evidence would resolve it: Experiments combining CSV with various pre-training strategies would reveal whether there are synergistic effects or whether certain combinations are particularly effective.

## Limitations

- Corpus generalization concerns: Effectiveness may not transfer to specialized domains with different term distributions
- Vocabulary size tradeoffs not fully explored: No systematic framework for determining optimal vocabulary size
- Limited ablation studies: Difficulty quantifying individual contributions of vocabulary, pre-training, and expansion components

## Confidence

- High confidence: Retrieval quality improvements (up to 12%) are well-supported by experimental results across multiple baselines and metrics
- Medium confidence: Latency improvements (up to 50%) are supported but relationship between postings list length and actual query latency could be more rigorously established
- Low confidence: Document expansion benefits from corpus-specific vocabularies lack strong empirical support regarding relevance of expansion tokens

## Next Checks

1. Cross-domain validation: Test CSV approach on specialized corpora (biomedical, legal, scientific) to assess generalizability to domains with different term distributions

2. Vocabulary size sensitivity analysis: Conduct experiments varying vocabulary sizes across a wider range (10k, 50k, 200k, 500k) and across different corpus sizes to identify optimal sizing strategies

3. Component ablation study: Design experiments isolating effects of (a) corpus-specific vocabulary without BERT pre-training, (b) BERT pre-training without corpus-specific vocabulary, and (c) standard document expansion without corpus-specific vocabulary