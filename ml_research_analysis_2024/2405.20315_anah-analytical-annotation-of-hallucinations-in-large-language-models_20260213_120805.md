---
ver: rpa2
title: 'ANAH: Analytical Annotation of Hallucinations in Large Language Models'
arxiv_id: '2405.20315'
source_url: https://arxiv.org/abs/2405.20315
tags:
- reference
- hallucination
- question
- annotation
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ANAH provides a fine-grained dataset for hallucination annotation
  in LLMs, addressing the challenge of coarse-grained benchmarks. It involves sentence-level
  annotation with reference retrieval, hallucination type judgment, and correction.
---

# ANAH: Analytical Annotation of Hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2405.20315
- Source URL: https://arxiv.org/abs/2405.20315
- Authors: Ziwei Ji; Yuzhe Gu; Wenwei Zhang; Chengqi Lyu; Dahua Lin; Kai Chen
- Reference count: 40
- Primary result: Sentence-level hallucination annotation reveals progressive accumulation patterns in LLM responses

## Executive Summary
ANAH introduces a fine-grained dataset for annotating hallucinations in large language models, addressing the limitations of coarse-grained benchmarks. The system employs sentence-level annotation with reference retrieval, hallucination type judgment, and correction to capture progressive hallucination accumulation. Through multi-task training with data augmentation, ANAH-trained annotators achieve 81.01% accuracy, surpassing open-source models and rivaling GPT-4 performance. The dataset spans bilingual content across diverse topics, enabling systematic analysis of hallucination patterns and generalization capabilities.

## Method Summary
ANAH constructs a bilingual dataset for hallucination annotation through a human-in-the-loop pipeline. The process involves topic selection and reference retrieval, question generation and selection, answer generation by LLMs, and fine-grained annotation at the sentence level. Each annotation includes reference fragment retrieval, hallucination type classification (No/Contradictory/Unverifiable Hallucinations, No Fact), and correction of hallucinated content. The system trains both generative and discriminative annotators using InternLM models with multi-task learning and data augmentation techniques, evaluating performance on unseen topics and questions.

## Key Results
- Hallucinations accumulate progressively in LLM responses, with P(Ht|H[0:t−1]) significantly exceeding P(Ht|∼H[0:t−1])
- Multi-task training with data augmentation improves generalization for hallucination annotation
- Generative annotators outperform discriminative ones on imbalanced hallucination type distributions
- Trained annotators achieve 81.01% accuracy, surpassing open-source models and rivaling GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained sentence-level hallucination annotation allows detection of progressive hallucination accumulation in LLM responses.
- **Mechanism:** By annotating each sentence individually with reference retrieval, hallucination type judgment, and correction, the dataset captures how hallucinations build up across response sentences rather than just flagging entire responses as hallucinatory or not.
- **Core assumption:** Sentence-level granularity is sufficient to capture hallucination progression patterns.
- **Evidence anchors:**
  - [abstract]: "Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer"
  - [section 2.5]: "The hallucination probability is 58.51% for English and 52.54% for Chinese, while the hallucination probability when the previous sentences don't contain, P(Ht|∼H[0:t−1]), is 14.61% for English and 17.2% for Chinese. P(Ht|H[0:t−1]) is significantly higher than P(Ht|∼H[0:t−1])"

### Mechanism 2
- **Claim:** Multi-task training with data augmentation improves hallucination annotator generalization.
- **Mechanism:** Incorporating related tasks (question generation, selection, answer generation) and using prompt augmentation creates shared representations that help the model better understand and follow hallucination annotation instructions.
- **Core assumption:** Shared representations from related tasks transfer to hallucination annotation capability.
- **Evidence anchors:**
  - [section 4.4]: "results are superior in the mix-task setting compared to the single-task setting. This suggests that LLMs benefit from the multi-task shared representations"

### Mechanism 3
- **Claim:** Generative annotators outperform discriminative ones on imbalanced hallucination type distributions.
- **Mechanism:** Generative models can produce corrections and reference fragments, providing richer supervision signals that help navigate the class imbalance problem where "No Hallucination" dominates.
- **Core assumption:** The additional output modalities (correction, reference) provide useful regularization signals.
- **Evidence anchors:**
  - [section 4.3]: "Fig. 4 shows the discriminative annotator is more prone to misjudge into the largest category (No Hallucination), with the 2nd to 4th row of the 1st column totaling 255, exceeding 147 for generative annotator"

## Foundational Learning

- **Concept:** Reference retrieval and matching
  - Why needed here: The annotation process requires finding exact supporting fragments from reference documents to determine if sentences are hallucinated
  - Quick check question: Can you explain the difference between hard matching (exact) and soft matching (semantic similarity) in reference retrieval?

- **Concept:** Hallucination type classification
  - Why needed here: Annotators must distinguish between No Hallucination, Contradictory, Unverifiable, and No Fact categories
  - Quick check question: What's the key difference between "Contradictory" and "Unverifiable" hallucination types?

- **Concept:** Multi-task learning benefits
  - Why needed here: The model is trained on multiple related tasks to improve generalization for hallucination annotation
  - Quick check question: How does training on question generation help with hallucination annotation specifically?

## Architecture Onboarding

- **Component map:** Topic selection -> Reference retrieval -> Question generation -> Answer generation -> Fine-grained hallucination annotation (reference retrieval, type judgment, correction)
- **Critical path:** The annotation pipeline is the bottleneck - each sentence requires reference retrieval, hallucination type judgment, and correction. This determines the overall throughput and quality of the dataset.
- **Design tradeoffs:** The system trades annotation speed for granularity - sentence-level annotation is much slower than response-level but provides the progressive accumulation insights. Using GPT-4 for initial annotation speeds up the process but requires careful human verification.
- **Failure signatures:** If reference retrieval fails consistently, hallucination classification accuracy drops. If human verification is insufficient, annotation quality degrades. If the multi-task training is unbalanced, generalization suffers.
- **First 3 experiments:**
  1. Verify the progressive hallucination accumulation effect by computing P(Ht|H[0:t−1]) vs P(Ht|∼H[0:t−1]) on a small subset
  2. Compare single-task vs multi-task training on a validation set to confirm the benefit of data augmentation
  3. Test the robustness of the trained annotator on out-of-domain questions to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the hallucination accumulation effect observed in this study hold across different LLM architectures and sizes, or is it specific to the models used in this work?
- Basis in paper: [inferred] The paper mentions the hallucination accumulation effect and provides evidence of its existence in the analyzed LLM responses, but does not investigate its generalizability across different model types.
- Why unresolved: The study focuses on a specific set of LLM models for generating answers. It is unclear if the observed accumulation effect is a universal characteristic of LLM behavior or if it varies depending on the model architecture, size, or training data.
- What evidence would resolve it: Conducting a similar analysis on a diverse range of LLM models, including different architectures (e.g., transformer variants, encoder-decoder models), sizes (e.g., small, medium, large), and training paradigms (e.g., supervised learning, reinforcement learning from human feedback) would provide insights into the generalizability of the hallucination accumulation effect.

### Open Question 2
- Question: How does the fine-grained hallucination annotation approach proposed in this paper compare to other hallucination detection methods, such as those based on n-gram overlap or semantic similarity, in terms of accuracy and efficiency?
- Basis in paper: [explicit] The paper introduces a novel fine-grained hallucination annotation approach that involves sentence-level analysis with reference retrieval, hallucination type judgment, and correction. However, it does not directly compare this approach to existing methods.
- Why unresolved: While the proposed approach offers a detailed analysis of hallucinations, it is unclear how it performs compared to other established methods in terms of accuracy, computational efficiency, and applicability to different types of text generation tasks.
- What evidence would resolve it: Conducting a comparative study that evaluates the proposed fine-grained approach against other hallucination detection methods on a common benchmark dataset would provide insights into its relative strengths and weaknesses. Metrics such as precision, recall, F1-score, and computational time could be used for comparison.

### Open Question 3
- Question: Can the hallucination annotators trained on ANAH be effectively used to guide the training of LLMs towards reducing hallucination, and if so, what are the best practices for incorporating their feedback into the training process?
- Basis in paper: [inferred] The paper demonstrates that the hallucination annotators trained on ANAH can achieve high accuracy in detecting and classifying hallucinations. However, it does not explore their potential use in actively reducing hallucinations during LLM training.
- Why unresolved: While the trained annotators show promise in identifying hallucinations, it is unclear how their feedback can be effectively utilized to guide the training of LLMs towards generating more factual and reliable outputs.
- What evidence would resolve it: Investigating different strategies for incorporating the feedback from the hallucination annotators into the LLM training process, such as fine-tuning with augmented data, reinforcement learning with human feedback, or adversarial training, would provide insights into their effectiveness in reducing hallucination. Ablation studies and comparisons with baseline models would help determine the best practices for leveraging the annotators' feedback.

## Limitations

- Dataset generalization uncertainty to real-world LLM deployment scenarios where users pose unpredictable queries
- Potential calibration bias in the 81.01% human annotator accuracy benchmark established by the research team itself
- Reported advantages of generative approaches may be specific to the InternLM architecture and training methodology used

## Confidence

**High Confidence:**
- Fine-grained sentence-level annotation provides more granular insights than coarse-grained response-level annotation
- Hallucinations accumulate progressively in LLM responses

**Medium Confidence:**
- Multi-task training with data augmentation improves generalization for hallucination annotation
- Generative annotators outperform discriminative ones on imbalanced hallucination type distributions

**Low Confidence:**
- The 81.01% human annotator accuracy represents the true upper bound for hallucination annotation
- Topic coverage is more important than question diversity for generalization

## Next Checks

1. **External Annotator Validation:** Recruit independent annotators unfamiliar with the ANAH methodology to annotate a held-out sample of 100-200 sentences. Compare their inter-annotator agreement and accuracy against the reported 81.01% benchmark to assess calibration bias.

2. **Open-Domain Deployment Test:** Apply the trained annotator to a dataset of user queries from real LLM applications (e.g., customer support logs, educational tutoring sessions). Measure performance degradation and identify failure modes that weren't captured in the controlled test sets.

3. **Cross-Architecture Transfer:** Train comparable generative and discriminative annotators using different model architectures (e.g., LLaMA, Mistral) on the same ANAH dataset. Compare whether the observed advantages of generative approaches and multi-task learning persist across architectures.