---
ver: rpa2
title: A Survey of Music Generation in the Context of Interaction
arxiv_id: '2402.15294'
source_url: https://arxiv.org/abs/2402.15294
tags:
- music
- generation
- which
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The survey provides a comprehensive review of music generation
  approaches suitable for interactive human-machine co-creation, focusing on real-time
  interaction. It covers data formats, transformed representations, statistical models
  (Markov chains, Hidden Markov Models, formal grammars), parametric models (RNNs,
  CNNs, VAEs, GANs, attention-based models), and evolutionary computation.
---

# A Survey of Music Generation in the Context of Interaction

## Quick Facts
- arXiv ID: 2402.15294
- Source URL: https://arxiv.org/abs/2402.15294
- Reference count: 0
- Primary result: Comprehensive review of music generation approaches suitable for interactive human-machine co-creation, focusing on real-time interaction and evaluation challenges

## Executive Summary
This survey provides a comprehensive review of music generation approaches suitable for interactive human-machine co-creation, focusing on real-time interaction capabilities. It covers data formats, transformed representations, statistical models (Markov chains, Hidden Markov Models, formal grammars), parametric models (RNNs, CNNs, VAEs, GANs, attention-based models), and evolutionary computation. The authors emphasize the need for models that can adapt to live input and evaluate their co-creativity potential, addressing a gap in the literature where most studies focus on style replication without considering interactive scenarios.

## Method Summary
The survey synthesizes existing literature on music generation approaches, categorizing them by data formats, modeling techniques, and their suitability for interactive scenarios. It examines both symbolic (MIDI, piano-rolls, ABC notation) and digital audio formats, transformed representations (spectrograms, chromagrams, embeddings), and various modeling approaches ranging from statistical models to deep learning architectures. The methodology emphasizes the trade-offs between different approaches, particularly their ability to adapt to live input and generate musically coherent output in real-time contexts.

## Key Results
- Statistical models offer fast adaptation but limited context, while deep learning models can generate longer structures but have less real-time interaction flexibility
- Most existing music generation models are not suitable for human-machine co-creation through live interaction
- Clear evaluation criteria are needed for assessing interactive music generation systems and their co-creativity potential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey's focus on real-time interaction distinguishes it from prior reviews by emphasizing live human-machine co-creation rather than batch generation.
- Mechanism: By explicitly addressing the need for models that can adapt to live input and evaluating their co-creativity potential, the survey fills a gap in the literature where most studies focus on style replication or transfer without considering interactive scenarios.
- Core assumption: Real-time interaction requires fundamentally different evaluation criteria and model architectures than offline generation.
- Evidence anchors:
  - [abstract] states that "most of these models are not suitable for human-machine co-creation through live interaction, neither is clear, how such models and resulting creations would be evaluated."
  - [section 6.1] discusses "Co-creative Process" and notes that "Statistical modeling facilitates fast and steerable training mechanisms, enabling rapid adaptation during a human-machine improvisation."

### Mechanism 2
- Claim: The survey provides comprehensive coverage of data formats and transformed representations essential for understanding music generation in interactive contexts.
- Mechanism: By covering symbolic formats (MIDI, piano-rolls, ABC notation), digital audio formats, automated transcription, and transformed representations (spectrograms, chromagrams, embeddings), the survey equips readers with the foundational knowledge needed to implement interactive systems.
- Core assumption: Understanding data representation is prerequisite to building effective interactive music generation systems.
- Evidence anchors:
  - [section 2] covers "Data and Formats" including "Symbolic" and "Digital Audio" formats.
  - [section 3] discusses "Transformed Representations" including spectrograms and chromagrams.

### Mechanism 3
- Claim: The survey's balanced treatment of statistical and parametric modeling approaches helps readers understand the tradeoffs between real-time adaptability and long-term structure generation.
- Mechanism: By presenting statistical models (Markov chains, HMMs, formal grammars) alongside parametric models (RNNs, CNNs, VAEs, GANs, attention-based models), the survey enables readers to make informed decisions about which approach suits their interactive requirements.
- Core assumption: Different modeling approaches have distinct strengths and weaknesses in interactive contexts.
- Evidence anchors:
  - [section 4] covers "Statistical Modeling" including Markov chains and HMMs.
  - [section 5] covers "Parametric Modeling" including various deep learning approaches.
  - [abstract] notes the trade-off that "statistical models offer fast adaptation but limited context, while deep learning models can generate longer structures but have less real-time interaction flexibility."

## Foundational Learning

- Concept: Music representation formats (symbolic vs. digital audio)
  - Why needed here: Different formats require different processing pipelines and model architectures, directly impacting interactive capabilities
  - Quick check question: What are the key differences between MIDI and WAV formats in terms of structure and processing requirements?

- Concept: Spectrogram and chromagram transformations
  - Why needed here: These representations capture essential musical information for analysis and generation, affecting how models learn musical patterns
  - Quick check question: How does a chromagram differ from a spectrogram in terms of what musical information it preserves?

- Concept: Markov chain and HMM fundamentals
  - Why needed here: These statistical models form the basis for many interactive music generation approaches due to their computational efficiency
  - Quick check question: What is the key difference between a Markov chain and a Hidden Markov Model in terms of state observability?

## Architecture Onboarding

- Component map:
  Data input layer -> Format conversion (MIDI, audio to internal representation) -> Feature extraction: Spectrogram/chromagram computation -> Model selection: Statistical vs. parametric approaches -> Real-time processing: Sampling and adaptation mechanisms -> Output synthesis: MIDI/audio generation and playback

- Critical path:
  1. Receive user input (MIDI/audio)
  2. Convert to appropriate representation
  3. Update model state (statistical or parametric)
  4. Generate next output sequence
  5. Convert to playback format
  6. Stream to output device

- Design tradeoffs:
  - Latency vs. quality: Statistical models offer lower latency but may sacrifice long-term coherence
  - Flexibility vs. complexity: Parametric models can capture complex patterns but require more computational resources
  - Interpretability vs. performance: Some approaches (formal grammars) offer clearer structure but may limit creative possibilities

- Failure signatures:
  - Audio glitches: May indicate buffer underruns in real-time processing
  - Repetitive output: Could suggest insufficient model adaptation or limited state space
  - Unresponsive behavior: Might indicate computational bottlenecks or model architecture issues

- First 3 experiments:
  1. Implement a simple Markov chain for melody generation and test real-time adaptation to user input
  2. Compare spectrogram vs. chromagram representations for chord recognition accuracy
  3. Benchmark different model architectures (HMM vs. RNN) for latency in interactive scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively evaluate the creativity of music generation systems that interact with humans in real-time?
- Basis in paper: [explicit] The paper discusses evaluation challenges in creative systems and mentions that no ground truth is available for objective assessment of generative music systems.
- Why unresolved: Current evaluation methods like Turing Test style surveys do not account for the interactive aspect of music generation, and internal evaluation components like discriminators in GANs only measure stylistic conformity rather than true creativity.
- What evidence would resolve it: Development of new evaluation metrics that assess the interactive and co-creative aspects of music generation systems, potentially involving real-time human feedback or analysis of the system's ability to generate novel and aesthetically pleasing musical content in response to live input.

### Open Question 2
- Question: How can attention-based models be adapted to allow for more interactive control during live music generation?
- Basis in paper: [explicit] The paper notes that attention-based models' ability to generate longer coherent structures limits their potential for interactive scenarios, as the influence of a human player can only go beyond providing a primer token.
- Why unresolved: Current attention-based models are primarily designed for generation from scratch, and their architecture does not easily allow for real-time user interaction during the generation process.
- What evidence would resolve it: Development of new attention-based architectures that can incorporate live human input during generation, potentially through mechanisms that allow for dynamic adjustment of attention weights or integration of real-time user feedback into the model's decision-making process.

### Open Question 3
- Question: What is the optimal balance between statistical modeling (for fast adaptation) and deep learning models (for longer context) in interactive music generation systems?
- Basis in paper: [explicit] The paper discusses the trade-offs between different approaches, noting that statistical models offer fast adaptation but limited context, while deep learning models can generate longer structures but have less real-time interaction flexibility.
- Why unresolved: Finding the right balance depends on the specific requirements of the interactive music generation task and the desired level of user control, which may vary depending on the application and user preferences.
- What evidence would resolve it: Comparative studies of interactive music generation systems using different combinations of statistical and deep learning models, evaluating their performance in terms of both musical quality and responsiveness to user input in various real-time scenarios.

## Limitations

- The survey's emphasis on real-time interaction as a distinguishing factor assumes this remains a significant challenge in music generation, though recent advances in transformer architectures may be reducing this gap
- Limited empirical validation of the claimed trade-offs between statistical and parametric models, as the survey is primarily conceptual rather than experimental
- Potential bias toward Western music traditions in the representation formats and evaluation criteria discussed

## Confidence

- High confidence: The comprehensive coverage of data formats and transformed representations forms a solid foundation for understanding interactive music generation
- Medium confidence: The comparative analysis of statistical versus parametric approaches is well-reasoned but would benefit from more empirical validation
- Medium confidence: The proposed evaluation criteria for co-creativity potential are theoretically sound but lack standardized implementation across the field

## Next Checks

1. **Empirical Benchmark**: Implement representative statistical (HMM) and parametric (transformer) models for the same interactive music generation task and measure actual latency, adaptation speed, and output quality under identical conditions
2. **Cross-Cultural Validation**: Test whether the proposed evaluation criteria for interactive systems hold when applied to non-Western musical traditions and alternative creative practices
3. **Real-Time Feasibility Study**: Conduct a systematic analysis of computational requirements for attention-based models in true real-time scenarios, measuring the gap between theoretical capability and practical implementation constraints