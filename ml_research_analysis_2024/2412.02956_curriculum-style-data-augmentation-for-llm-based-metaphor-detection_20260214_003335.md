---
ver: rpa2
title: Curriculum-style Data Augmentation for LLM-based Metaphor Detection
arxiv_id: '2412.02956'
source_url: https://arxiv.org/abs/2412.02956
tags:
- data
- metaphor
- word
- computational
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Curriculum-style Data Augmentation (CDA) for
  fine-tuning open-source LLMs on metaphor detection. The method iteratively augments
  training data by generating diverse samples using a teacher LLM and selecting those
  correctly predicted by the student model for fine-tuning, while incorrectly predicted
  instances are used as seeds for the next augmentation iteration.
---

# Curriculum-style Data Augmentation for LLM-based Metaphor Detection

## Quick Facts
- arXiv ID: 2412.02956
- Source URL: https://arxiv.org/abs/2412.02956
- Authors: Kaidi Jia; Yanxia Wu; Ming Liu; Rongsheng Li
- Reference count: 25
- Key outcome: CDA with Llama 3.1 8B achieves SOTA performance on metaphor detection datasets

## Executive Summary
This paper proposes Curriculum-style Data Augmentation (CDA) for fine-tuning open-source LLMs on metaphor detection. The method iteratively augments training data by generating diverse samples using a teacher LLM and selecting those correctly predicted by the student model for fine-tuning, while incorrectly predicted instances are used as seeds for the next augmentation iteration. This approach enables progressive learning from simpler to more complex instances. Experiments on VUA Verb, MOH-X, and TroFi datasets show that CDA with Llama 3.1 8B achieves state-of-the-art performance, outperforming both BERT-based and other LLM-based methods.

## Method Summary
CDA fine-tunes Llama 3.1 8B on the VUA Verb dataset through three iterative augmentation cycles. The method uses GPT-4o as a teacher model to generate diverse metaphorical and literal sentences using three augmentation methods: direct generation, target word replacement, and context replacement. Each iteration evaluates generated data using the student model itself, selecting correctly predicted instances for fine-tuning while using incorrectly predicted instances as seeds for the next augmentation cycle. The approach employs LoRA fine-tuning with rank 8, alpha 16, and learning rate 2e-4.

## Key Results
- Llama 3.1 8B with CDA achieves 81.5% accuracy on VUA Verb dataset
- Zero-shot transfer to MOH-X and TroFi datasets achieves state-of-the-art performance
- CDA reduces inference costs by requiring only a single model call versus multi-step prompting
- Performance degradation observed after 3 iterations due to data distribution imbalance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CDA enables progressive learning by selecting data based on model confidence
- Mechanism: The method evaluates generated data using the student model itself. Correctly predicted instances are used for fine-tuning while incorrectly predicted instances are used as seeds for generating more challenging data in subsequent iterations. This creates a curriculum where the model first learns simpler knowledge and progressively tackles more complex examples.
- Core assumption: The model's own predictions can reliably distinguish between data it can learn from versus data that is too difficult at its current capability level
- Evidence anchors:
  - [abstract] "Specifically, before fine-tuning, we evaluate the training data to identify correctly predicted instances for fine-tuning, while incorrectly predicted instances are used as seed data for data augmentation"
  - [section 4.2] "Specifically, after performing data augmentation, we use the model to be trained to evaluate the generated data. The correctly predicted data is selected for fine-tuning, while the incorrectly predicted data is used as seed data for generating the data for next iteration"
  - [corpus] Weak - no direct corpus evidence found supporting this specific curriculum selection mechanism
- Break condition: If the model consistently misclassifies its own predictions, it will select inappropriate data for fine-tuning, potentially learning from noisy examples or missing important learning opportunities

### Mechanism 2
- Claim: Data augmentation specifically designed for metaphor detection improves performance
- Mechanism: Three distinct augmentation methods are used: direct generation of metaphorical sentences, context replacement while preserving metaphorical usage, and target word replacement while maintaining metaphorical meaning. This creates diverse training examples that expose the model to various metaphorical constructions.
- Core assumption: The teacher LLM (GPT-4o) has sufficient understanding of metaphorical relationships to generate meaningful and diverse examples
- Evidence anchors:
  - [section 4.1] "For each sentence, we use three different methods for data augmentation. Specifically, we design three different prompts for metaphorical and literal data to ensure the diversity of generated data"
  - [section 4.1] "To ensure that the augmented data is of high quality, we introduce a powerful LLM as the teacher model MT, which is responsible for generating data for the student model MS to be trained"
  - [corpus] Moderate - related work on LaiDA mentions linguistics-aware data augmentation for metaphor components, supporting the general approach
- Break condition: If the teacher model lacks understanding of metaphor theory, it will generate noisy or irrelevant examples that could harm training

### Mechanism 3
- Claim: Iterative fine-tuning on progressively harder data improves overall performance
- Mechanism: The model undergoes multiple rounds of fine-tuning, where each iteration focuses on data the model previously struggled with. This allows the model to build foundational knowledge before tackling more complex examples, similar to curriculum learning principles.
- Core assumption: The model's capability improves monotonically with each iteration, allowing it to handle increasingly difficult data
- Evidence anchors:
  - [abstract] "This approach enables the model to quickly learn simpler knowledge and progressively acquire more complex knowledge, thereby improving performance incrementally"
  - [section 4.2] "With this approach, we enable the model to quickly learn knowledge from relatively simple data, and as the model's capability improves, gradually learn more difficult data, continuously enhancing its performance"
  - [section 5.5] "However, we observed a downward trend in model performance as the number of iterations increased, which is primarily caused by the imbalance in the training data distribution"
- Break condition: Performance degradation occurs when data distribution becomes too imbalanced across iterations, as observed in experiments where metaphorical instances dominated training data

## Foundational Learning

- Concept: Curriculum learning
  - Why needed here: Metaphor detection suffers from severe data scarcity, making it difficult for models to learn complex patterns without structured progression from simple to difficult examples
  - Quick check question: How does curriculum learning differ from random sampling in training data selection?

- Concept: Data augmentation principles
  - Why needed here: Limited metaphor detection datasets require synthetic data generation to provide sufficient training examples for effective fine-tuning
  - Quick check question: What are the key considerations when designing data augmentation strategies for specialized NLP tasks?

- Concept: Fine-tuning vs prompt engineering
  - Why needed here: The paper contrasts fine-tuning open-source LLMs with prompt-based approaches to reduce inference costs and latency
  - Quick check question: What are the computational tradeoffs between fine-tuning and prompt-based inference for LLM applications?

## Architecture Onboarding

- Component map:
  - GPT-4o (teacher) -> Data augmentation pipeline -> Llama 3.1 8B (student) -> Evaluation module -> Selected training data

- Critical path:
  1. Initial evaluation of training data
  2. Selection of correctly predicted instances for fine-tuning
  3. Data augmentation using incorrectly predicted instances
  4. Iterative refinement through multiple training cycles

- Design tradeoffs:
  - Single-step inference vs multi-step prompting: CDA reduces inference costs but requires upfront computational investment in fine-tuning
  - Data quality vs quantity: Selecting only correctly predicted instances ensures quality but may limit diversity
  - Iteration depth vs data imbalance: More iterations provide better learning but risk imbalanced training data

- Failure signatures:
  - Performance degradation across iterations indicates data distribution imbalance
  - High variance in zero-shot test results suggests overfitting to training distribution
  - Low accuracy on original test sets indicates poor generalization

- First 3 experiments:
  1. Run baseline: Fine-tune Llama 3.1 8B on VUA Verb without CDA for comparison
  2. Single iteration CDA: Apply one round of data augmentation and evaluate performance improvement
  3. Ablation study: Test each augmentation method individually to identify the most effective approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of iterations for CDA before performance plateaus or degrades due to data distribution imbalance?
- Basis in paper: [inferred] The paper shows performance degradation in later iterations (Figure 2) and mentions stopping at 3 iterations, but doesn't explore the full iteration space.
- Why unresolved: The paper only tested up to 3 iterations and observed degradation, but didn't systematically explore different iteration counts to find the optimal stopping point.
- What evidence would resolve it: Experiments testing 1, 2, 3, 4, and 5+ iterations with detailed analysis of data distribution imbalance and performance metrics at each step.

### Open Question 2
- Question: How does CDA performance scale with different LLM sizes beyond Llama 3.1 8B, such as the larger Llama 3.1 70B and 405B models?
- Basis in paper: [explicit] "We did not conduct experiments on larger models such as Llama 3.1 70B and Llama 3.1 405B, mainly due to computational resource limitations."
- Why unresolved: The paper acknowledges computational limitations prevented testing larger models that might have different learning dynamics with CDA.
- What evidence would resolve it: Experiments using the same CDA methodology on larger LLM variants, comparing performance gains and iteration behavior.

### Open Question 3
- Question: What is the impact of using incorrectly predicted data (instead of discarding it) as seed data for augmentation in early iterations?
- Basis in paper: [inferred] The paper discards incorrectly predicted data in early iterations because "the teacher model lacks sufficient understanding," but doesn't test whether including this data might be beneficial.
- Why unresolved: The paper makes an assumption about data quality without empirical testing of whether including "difficult" data might help model learning.
- What evidence would resolve it: Comparative experiments using three approaches: discarding wrong cases, including all cases, and selectively including wrong cases based on confidence scores.

## Limitations
- Performance degradation after 3 CDA iterations due to data distribution imbalance
- Reliance on GPT-4o teacher model creates computational cost and accessibility barriers
- Limited to verb-level metaphor detection without validation for other parts of speech

## Confidence

**High confidence:** The core claim that CDA improves metaphor detection performance on VUA Verb dataset (supported by experimental results showing 81.5% accuracy)

**Medium confidence:** The assertion that CDA reduces inference costs compared to prompt-based approaches (the computational tradeoff between fine-tuning and inference is reasonable but not extensively validated)

**Medium confidence:** The claim of state-of-the-art performance on MOH-X and TroFi datasets (zero-shot results are strong but may be influenced by dataset-specific factors)

**Low confidence:** The specific mechanisms of how curriculum-style selection improves learning (theoretical basis is sound but empirical evidence is limited to performance metrics)

## Next Checks
1. Implement ablation studies to quantify the individual contributions of each data augmentation method and verify that the combination provides synergistic benefits rather than additive improvements
2. Conduct experiments with varying iteration counts to identify the optimal number of CDA iterations and characterize the point at which performance degradation begins
3. Test the model's robustness by evaluating on datasets with different metaphorical distribution patterns to assess generalization beyond the specific datasets used in the study