---
ver: rpa2
title: Deep MMD Gradient Flow without adversarial training
arxiv_id: '2405.06780'
source_url: https://arxiv.org/abs/2405.06780
tags:
- gradient
- flow
- training
- discriminator
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Diffusion-MMD-Gradient Flow (DMMD), a method
  for generative modeling that transports particles from an initial source distribution
  to a target distribution using a noise-adaptive Wasserstein gradient of the Maximum
  Mean Discrepancy (MMD). The key innovation is training the MMD discriminator on
  data distributions corrupted by increasing levels of noise via a forward diffusion
  process, eliminating the need for adversarial training required in GANs.
---

# Deep MMD Gradient Flow without adversarial training

## Quick Facts
- arXiv ID: 2405.06780
- Source URL: https://arxiv.org/abs/2405.06780
- Reference count: 40
- One-line primary result: Achieves FID of 8.31 on CIFAR10 using noise-adaptive MMD gradient flow without adversarial training

## Executive Summary
This paper introduces Diffusion-MMD-Gradient Flow (DMMD), a method for generative modeling that transports particles from an initial source distribution to a target distribution using a noise-adaptive Wasserstein gradient of the Maximum Mean Discrepancy (MMD). The key innovation is training the MMD discriminator on data distributions corrupted by increasing levels of noise via a forward diffusion process, eliminating the need for adversarial training required in GANs. The approach generalizes MMD Gradient Flow by adapting the MMD discriminator to different noise levels during sampling. Experimental results show competitive performance in unconditional image generation on CIFAR10, MNIST, CELEB-A (64x64), and LSUN Church (64x64).

## Method Summary
The method uses a forward diffusion process to generate noisy versions of clean data across multiple noise levels. For each level, it trains a noise-conditional MMD discriminator that distinguishes clean from noisy data. At sampling time, it performs noise-adaptive MMD gradient flow, starting from Gaussian noise and gradually decreasing noise levels while following the learned discriminator gradients. The method also proposes a scalable approximate sampling procedure using linear kernels with pre-computed features, enabling single samples to be generated at cost independent of the number of particles used in training.

## Key Results
- Achieves FID of 8.31 on CIFAR10, competitive with state-of-the-art diffusion models
- Demonstrates successful image generation on MNIST, CELEB-A (64x64), and LSUN Church (64x64)
- Shows that DMMD works when MMD is replaced by a lower bound on the KL divergence
- Provides scalable sampling with linear kernel approximation that maintains quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DMMD achieves competitive generative modeling performance by training noise-adaptive MMD discriminators via forward diffusion, avoiding adversarial training.
- Mechanism: The method uses a forward diffusion process to generate noisy versions of clean data across multiple noise levels. For each level, it trains a noise-conditional MMD discriminator that distinguishes clean from noisy data. At sampling time, it performs noise-adaptive MMD gradient flow, starting from Gaussian noise and gradually decreasing noise levels while following the learned discriminator gradients.
- Core assumption: Training discriminators on progressively noisier data mimics the behavior of discriminators in GAN training, where discriminators start by distinguishing random noise from data and gradually become more refined.
- Evidence anchors:
  - [abstract]: "The noise-adaptive MMD is trained on data distributions corrupted by increasing levels of noise, obtained via a forward diffusion process... The result is a generalization of MMD Gradient Flow, which we call Diffusion-MMD-Gradient Flow or DMMD."
  - [section 4.1]: "Using this insight, for each noise level t ∈ [0, 1], we define a discriminator MMD2(Pt, P; t, θ) using the kernel of type (3) with noise-conditional discriminator features ϕ(x, t; θ)."
- Break condition: If the forward diffusion process doesn't adequately span the space from clean data to random noise, or if the noise-adaptive discriminator fails to capture meaningful gradients at different noise levels.

### Mechanism 2
- Claim: Adaptive kernel widths during gradient flow lead to faster convergence than fixed kernels, especially when source and target distributions are far apart.
- Mechanism: For Gaussian distributions, there exists an optimal kernel width α⋆ that maximizes the gradient magnitude of MMD2. This optimal width depends on the distance between distributions and is zero when distributions are close enough. The method adapts kernel widths based on noise level during sampling.
- Core assumption: The optimal kernel width for MMD gradient flow depends on the relative position between source and target distributions, and can be learned or approximated through noise adaptation.
- Evidence anchors:
  - [section 3]: "Proposition 3.1. For any µ0 ∈ Rd and σ > 0, let α⋆ be given by α⋆ = argmaxα≥0∥∇µ0MMD2α(π0,σ, πµ0,σ)∥. Then, we have that α⋆ = ReLU(∥µ0∥2/(d + 2) − 2σ2)1/2."
- Break condition: If the adaptive kernel mechanism doesn't properly scale with dimensionality or if the learned adaptation doesn't reflect true optimal kernel widths.

### Mechanism 3
- Claim: Using linear kernels with pre-computed features enables scalable sampling independent of particle count.
- Mechanism: When using linear base kernels, the witness function computation becomes O(N) instead of O(N²). By pre-computing average features across the dataset, single particles can be sampled independently without requiring interaction with multiple particles.
- Core assumption: For large numbers of particles, the interaction term in MMD gradient flow can be approximated by the average features from the entire dataset, allowing independent sampling.
- Evidence anchors:
  - [section 5]: "Using linear kernel (12) allows us to use these average features (13) in the second term of (11). In practice, we can precompute these features for T timesteps and store them in memory in order to use them for sampling purposes."
- Break condition: If the approximation of particle interactions by dataset averages introduces significant bias, or if linear kernels are insufficient for capturing complex data distributions.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD serves as the divergence measure that quantifies the difference between generated and target distributions, and its gradient provides the direction for particle movement.
  - Quick check question: What is the relationship between MMD and integral probability metrics, and why is MMD well-defined even for distributions with non-overlapping support?

- Concept: Wasserstein Gradient Flow
  - Why needed here: The method transports particles from an initial distribution to a target distribution by following the gradient of a divergence measure in Wasserstein space.
  - Quick check question: How does the discretization of Wasserstein gradient flow relate to the particle-based sampling procedure used in DMMD?

- Concept: Forward Diffusion Process
  - Why needed here: The forward diffusion process generates noisy versions of clean data across multiple noise levels, which are used to train noise-adaptive discriminators.
  - Quick check question: What is the mathematical relationship between the forward diffusion process parameters (αt, βt) and the noise level t in the generated samples?

## Architecture Onboarding

- Component map:
  Forward Diffusion Module -> Noise-Conditional MMD Discriminator -> Adaptive Kernel Module -> Gradient Flow Sampler -> Linear Kernel Approximation

- Critical path:
  1. Train noise-conditional MMD discriminator using forward diffusion
  2. Pre-compute average features for linear kernel approximation
  3. Sample initial particles from Gaussian distribution
  4. Perform noise-adaptive gradient flow with decreasing noise levels
  5. Apply final denoising step if needed

- Design tradeoffs:
  - Adaptive vs. fixed kernels: Adaptive kernels converge faster but require additional learning and hyperparameter tuning
  - Linear vs. nonlinear kernels: Linear kernels enable scalability but may lose expressive power
  - Number of noise levels vs. computational cost: More levels provide better adaptation but increase training time

- Failure signatures:
  - Poor FID scores despite training convergence: Likely indicates inadequate kernel adaptation or linear approximation issues
  - Mode collapse in generated samples: May indicate insufficient noise levels or overly aggressive gradient flow steps
  - Training instability: Could result from improper gradient penalty or ℓ2 regularization settings

- First 3 experiments:
  1. Verify MMD discriminator training on synthetic 2D data with known optimal kernel widths
  2. Test noise-adaptive gradient flow on simple Gaussian mixtures to confirm faster convergence
  3. Validate linear kernel approximation by comparing sampling quality with and without the approximation on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DMMD scale when applied to high-dimensional datasets like ImageNet compared to diffusion models and GANs?
- Basis in paper: [inferred] The paper demonstrates competitive performance on CIFAR-10, MNIST, CELEB-A, and LSUN Church but acknowledges that diffusion models still outperform DMMD. The authors note that "The empirical performance of DMMD will be of interest in regimes where diffusion models could be ill-behaved, such as in generative modeling on Riemannian manifolds; as well as on larger datasets such as ImageNet."
- Why unresolved: The paper does not provide experimental results on datasets as large and complex as ImageNet. The scalability of DMMD to high-dimensional data remains an open question.
- What evidence would resolve it: Comprehensive experiments comparing DMMD with state-of-the-art diffusion models and GANs on high-dimensional datasets like ImageNet, including metrics such as FID, Inception Score, and computational efficiency.

### Open Question 2
- Question: Can the noise-adaptive discriminator training approach used in DMMD be effectively applied to other domains beyond image generation, such as audio or protein modeling?
- Basis in paper: [explicit] The authors state, "DMMD provides a way of training a discriminator, which may be applicable in other areas where a domain-adaptive discriminator might be required."
- Why unresolved: The paper focuses exclusively on image generation tasks and does not explore applications in other domains. The generalizability of the noise-adaptive training approach is not tested.
- What evidence would resolve it: Successful application and evaluation of DMMD or similar noise-adaptive discriminator training methods on tasks in audio generation, protein modeling, or other domains, demonstrating improved performance over existing methods.

### Open Question 3
- Question: What are the theoretical guarantees for the convergence of the DMMD gradient flow, and under what conditions can we ensure that the flow converges to the target distribution?
- Basis in paper: [explicit] The authors mention, "Finally, it will be of interest to establish theoretical foundations for DMMD in general settings, and to derive convergence results for the associated flow."
- Why unresolved: The paper provides empirical evidence of DMMD's effectiveness but does not offer theoretical convergence guarantees. The conditions under which the gradient flow converges to the target distribution are not established.
- What evidence would resolve it: Rigorous mathematical proofs demonstrating the convergence of DMMD gradient flow under various conditions, including assumptions on the kernel, noise levels, and initial distributions. Theoretical bounds on the rate of convergence would also be valuable.

## Limitations
- The relationship between optimal kernel widths and noise levels in high-dimensional spaces remains theoretically underexplored, with current analysis limited to Gaussian distributions
- The linear kernel approximation, while enabling scalability, may limit the method's ability to capture complex data distributions
- The method relies heavily on the assumption that noise-adaptive MMD discriminators can effectively approximate the behavior of discriminators in GAN training without requiring adversarial objectives

## Confidence

- Mechanism 1 (Noise-adaptive training): Medium - supported by theoretical framework and experimental results, but limited direct empirical validation
- Mechanism 2 (Adaptive kernel optimization): Low - primarily theoretical with limited empirical evidence, especially for high-dimensional data
- Mechanism 3 (Linear kernel scalability): Medium - practical implementation details provided, but trade-offs with expressiveness not fully characterized

## Next Checks

1. **Kernel adaptation validation**: Systematically vary noise levels and measure actual kernel widths during training to verify that the learned adaptation reflects theoretical predictions from Proposition 3.1

2. **Linear approximation impact**: Generate samples using full MMD gradient flow (O(N²)) vs. linear approximation on CIFAR-10 to quantify the quality loss and computational benefits

3. **Distribution coverage analysis**: Visualize particle trajectories in 2D projection space to verify that the noise-adaptive gradient flow properly explores the space between source and target distributions without mode collapse