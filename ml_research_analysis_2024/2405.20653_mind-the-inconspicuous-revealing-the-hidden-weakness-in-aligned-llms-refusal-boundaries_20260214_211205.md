---
ver: rpa2
title: 'Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs'' Refusal
  Boundaries'
arxiv_id: '2405.20653'
source_url: https://arxiv.org/abs/2405.20653
tags:
- tokens
- harmful
- arxiv
- jailbreak
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors reveal that simply appending multiple end-of-sequence
  (eos) tokens to prompts can cause a phenomenon called context segmentation, shifting
  both harmful and benign inputs closer to the refusal boundary in the hidden space
  of aligned large language models (LLMs). They propose BOOST, a method that enhances
  jailbreak attacks by appending eos tokens, which significantly increases attack
  success rates across 8 representative jailbreak techniques and 16 open-source LLMs
  (2B to 72B parameters).
---

# Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries

## Quick Facts
- **arXiv ID**: 2405.20653
- **Source URL**: https://arxiv.org/abs/2405.20653
- **Reference count**: 38
- **Primary result**: Appending end-of-sequence tokens to prompts creates context segmentation that shifts harmful and benign inputs closer to refusal boundaries, significantly increasing jailbreak success rates

## Executive Summary
This paper reveals a critical vulnerability in aligned large language models where appending multiple end-of-sequence (eos) tokens to prompts creates a phenomenon called context segmentation. This segmentation shifts both harmful and benign inputs closer to the refusal boundary in the model's hidden space, making aligned models more susceptible to jailbreak attacks. The authors demonstrate that this vulnerability affects both open-source models (ranging from 2B to 72B parameters) and major commercial API providers like OpenAI, Anthropic, and Qwen. They propose BOOST, a method that leverages eos tokens to enhance jailbreak attacks, achieving significant improvements across 8 representative attack techniques. This finding exposes a blind spot in current alignment and content filtering approaches, suggesting that models may be more vulnerable than previously thought despite alignment efforts.

## Method Summary
The authors systematically investigate how appending multiple eos tokens to prompts affects the hidden space representations of both harmful and benign inputs in aligned LLMs. They conduct experiments across 16 open-source models with varying parameter sizes and test 8 different jailbreak techniques. The BOOST method specifically appends eos tokens to standard jailbreak prompts to exploit the context segmentation vulnerability. The evaluation includes both open-source models and commercial API providers, testing whether these providers filter eos tokens at the input level. The methodology involves measuring attack success rates with and without eos token augmentation, analyzing shifts in hidden space representations relative to refusal boundaries, and comparing vulnerability across different model sizes and alignment approaches.

## Key Results
- Appending eos tokens causes context segmentation that shifts both harmful and benign inputs closer to refusal boundaries in hidden space
- BOOST method significantly increases jailbreak success rates across 8 representative techniques and 16 open-source LLMs (2B-72B parameters)
- Major commercial API providers (OpenAI, Anthropic, Qwen) do not filter eos tokens, showing similar vulnerability to open-source models

## Why This Works (Mechanism)
The context segmentation vulnerability works by appending multiple eos tokens to prompts, which causes the model to treat the prompt as a distinct segment in its processing pipeline. This segmentation creates a boundary effect where the hidden representations of both harmful and benign inputs are shifted closer to the refusal boundary in the model's internal space. The mechanism exploits the fact that aligned models learn to separate harmful and benign inputs based on their proximity to this boundary, and by artificially creating a segmentation point, attackers can manipulate the effective distance between inputs and the refusal boundary. This works because the model's alignment mechanisms don't account for artificial segmentation that can be induced through simple token manipulation, making the alignment boundaries less robust than assumed.

## Foundational Learning
- **Context segmentation**: The process by which appending eos tokens causes the model to treat input as separate segments, affecting hidden space representations
  - *Why needed*: Understanding this phenomenon is crucial for grasping how simple token manipulation can undermine alignment
  - *Quick check*: Verify that adding eos tokens changes how the model processes subsequent input tokens

- **Refusal boundary**: The decision boundary in hidden space that separates harmful from benign inputs, which aligned models learn during training
  - *Why needed*: The attack exploits proximity to this boundary, so understanding its nature is essential
  - *Quick check*: Confirm that harmful inputs are positioned closer to this boundary than benign inputs in aligned models

- **Hidden space representations**: The intermediate vector representations that LLMs use for decision-making before generating output
  - *Why needed*: The vulnerability operates by manipulating these representations through segmentation
  - *Quick check*: Verify that hidden representations shift when eos tokens are appended

- **Jailbreak techniques**: Methods designed to bypass content filters and elicit harmful outputs from aligned LLMs
  - *Why needed*: The study tests eos token enhancement across multiple existing jailbreak approaches
  - *Quick check*: Confirm that standard jailbreak techniques work before testing eos-enhanced versions

- **Alignment boundaries**: The learned separation between acceptable and unacceptable content that alignment processes establish
  - *Why needed*: The attack specifically targets the robustness of these boundaries
  - *Quick check*: Verify that alignment processes create measurable separation between harmful and benign inputs

## Architecture Onboarding

**Component map**: Prompt generation -> EOS token appending -> Model input processing -> Hidden space representation -> Output generation

**Critical path**: The sequence from prompt construction through hidden space manipulation to final output generation is critical, as the vulnerability exploits the transition between prompt segments and their representation in hidden space.

**Design tradeoffs**: The vulnerability highlights a fundamental tradeoff between model flexibility (accepting eos tokens as valid input) and security (preventing their exploitation for context manipulation). Models must balance permissive input handling with robust boundary enforcement.

**Failure signatures**: When the vulnerability is exploited, harmful prompts produce harmful outputs at higher rates, benign prompts may be incorrectly flagged as harmful, and the model's confidence in refusal decisions decreases for inputs near the manipulated boundary.

**Three first experiments**:
1. Test whether appending varying numbers of eos tokens (1, 5, 10) creates progressively stronger context segmentation effects
2. Compare hidden space distances between harmful and benign inputs with and without eos token augmentation
3. Evaluate whether different types of eos tokens (standard vs. custom) produce similar segmentation effects

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on single-turn interactions, leaving multi-turn conversation vulnerabilities unexplored
- The research assumes all alignment approaches are equally susceptible without comparing different alignment methodologies
- Evaluation metrics focus on attack success rates without measuring downstream harm potential or qualitative differences in outputs

## Confidence
- **High**: Context segmentation vulnerability in open-source LLMs (rigorous experimental methodology, consistent reproducibility across models and techniques)
- **Medium**: Vulnerability in commercial API providers (only tests input-level filtering, not deeper architectural defenses)

## Next Checks
1. Test context segmentation attacks in multi-turn conversation settings with stateful interactions
2. Evaluate whether commercial API providers have implemented hidden defenses that mitigate eos token effects despite accepting them as input
3. Assess whether different alignment methodologies (RLHF, constitutional AI, supervised fine-tuning) exhibit varying susceptibility to context segmentation vulnerabilities