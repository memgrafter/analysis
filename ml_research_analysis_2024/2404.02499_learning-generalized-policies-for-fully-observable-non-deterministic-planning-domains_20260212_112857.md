---
ver: rpa2
title: Learning Generalized Policies for Fully Observable Non-Deterministic Planning
  Domains
arxiv_id: '2404.02499'
source_url: https://arxiv.org/abs/2404.02499
tags:
- state
- fond
- policy
- general
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends combinatorial methods for learning generalized
  policies from classical planning domains to fully observable non-deterministic (FOND)
  domains. The key insight is that FOND planning can be reduced to classical planning
  plus dead-end detection by leveraging the all-outcome relaxation.
---

# Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains

## Quick Facts
- arXiv ID: 2404.02499
- Source URL: https://arxiv.org/abs/2404.02499
- Reference count: 26
- Key outcome: This work extends combinatorial methods for learning generalized policies from classical planning domains to fully observable non-deterministic (FOND) domains, producing correct generalized policies for 7 out of 12 benchmark domains.

## Executive Summary
This paper presents a method for learning generalized policies for FOND planning domains by extending combinatorial approaches from classical planning. The key insight is that FOND planning can be reduced to classical planning plus dead-end detection using the all-outcome relaxation. The approach learns policies expressed as rules and constraints over learned features, where rules define action selection and constraints avoid dead-ends. The method solves a minimum-cost SAT problem that enforces policy correctness on training instances, with experimental results showing successful generalization on 7 out of 12 benchmark domains.

## Method Summary
The method learns generalized FOND policies by reducing the problem to a minimum-cost SAT formulation. It leverages the all-outcome relaxation to convert FOND actions into deterministic classical actions, then learns rules for action selection and constraints for dead-end avoidance. Features are generated using description logic grammar from domain predicates, and the SAT theory enforces that alive states have good transitions, dead-ends are avoided, and features distinguish between dead, alive, and goal states. The approach uses an incremental learning strategy starting from smallest instances and building up to larger ones.

## Key Results
- The approach produces correct generalized policies for 7 out of 12 benchmark FOND domains
- Policies successfully generalize to larger problem instances in domains like blocks-clear, doors, and spiky-tireworld
- The method fails to generalize for 5 domains, particularly on larger problem sizes
- The transition constraint variant outperforms the state constraint variant on tireworld and triangle-tireworld domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FOND planning can be reduced to classical planning plus dead-end detection
- Mechanism: The all-outcome relaxation converts FOND actions with non-deterministic effects into deterministic classical actions. This creates a classical problem where dead-ends in the FOND problem may become alive states, but the reverse is true: dead-ends in the classical relaxation are also dead-ends in the FOND problem.
- Core assumption: The target class Q of FOND problems is closed under taking reachable alive states
- Evidence anchors:
  - [abstract]: "FOND planning can be reduced to classical planning plus dead-end detection by leveraging the all-outcome relaxation"
  - [section 3.4]: "If s is a dead-end state in PD, s will be a dead-end state in the FOND problem P"
- Break condition: If the closedness assumption fails, the reduction breaks because dead-ends in the classical relaxation may not correspond to dead-ends in the original FOND problem

### Mechanism 2
- Claim: General policies for FOND problems can be constructed from safe classical policies plus dead-end constraints
- Mechanism: A general classical policy that is safe (avoids transitions leading to FOND dead-ends) can be combined with a sound set of dead-end constraints to create a general FOND policy. The rules define action selection and the constraints ensure dead-ends are avoided.
- Core assumption: The classical policy is B-safe (avoids dead-ends) and the constraint set B is sound (covers all FOND dead-ends)
- Evidence anchors:
  - [abstract]: "General FOND policies are expressed using rules and constraints over learned features, where rules define action selection and constraints avoid dead-ends"
  - [section 4.2]: "Theorem 2: If the rules R encode a general classical policy that solves QD which is B-safe, then the general FOND policy πR,B that follows from Definition 2 solves Q"
- Break condition: If either the B-safety or soundness conditions fail, the resulting FOND policy may visit dead-ends

### Mechanism 3
- Claim: Learning generalized FOND policies reduces to a minimum-cost SAT problem over state transitions
- Mechanism: The SAT formulation enforces that (1) alive states have good transitions, (2) transitions leading to dead-ends are not good, (3) features distinguish dead, alive, and goal states, and (4) good transitions lead toward goals. The minimum-cost aspect ensures simple, generalizable policies.
- Core assumption: The SAT theory is satisfiable iff there exists a general FOND policy with the desired properties
- Evidence anchors:
  - [abstract]: "The method learns these policies by solving a minimum-cost SAT problem that enforces policy correctness on training instances"
  - [section 5.1]: "Theorem 3: The theory T(S,F) is satisfiable iff there is a general FOND policy πR,B over the features in the pool F that solves the set of sampled FOND problems Q′"
- Break condition: If the SAT theory is unsatisfiable, no policy with the given features can solve the training instances

## Foundational Learning

- Concept: All-outcome relaxation
  - Why needed here: This is the key transformation that converts FOND problems into classical planning problems by treating each non-deterministic action as a set of deterministic actions
  - Quick check question: What is the relationship between dead-ends in the all-outcome relaxation and dead-ends in the original FOND problem?

- Concept: Dead-end detection
  - Why needed here: FOND policies must avoid dead-ends, which requires identifying states from which no solution is possible. The paper uses an iterative algorithm that marks states as dead-ends when all actions lead to dead-ends.
  - Quick check question: How does the iterative dead-end detection algorithm ensure soundness and completeness?

- Concept: Generalized planning
  - Why needed here: The goal is to learn policies that work across families of problems (domains) rather than individual instances. This requires learning features and rules that generalize beyond the training set.
  - Quick check question: What makes a policy "generalized" versus just being a policy for a single instance?

## Architecture Onboarding

- Component map:
  PDDL parsing and domain analysis → DLPlan feature generation → SAT theory construction → Min-cost SAT solver (clingo) → Policy extraction → Dead-end detection → Policy validation

- Critical path:
  1. Parse PDDL domain and generate features using DLPlan
  2. Construct SAT theory with state transitions and feature constraints
  3. Solve min-cost SAT problem with clingo
  4. Extract rules and constraints from satisfying assignment
  5. Validate policy on training and test instances

- Design tradeoffs:
  - Feature complexity vs. policy simplicity: Higher complexity features may enable better policies but risk overfitting
  - State constraints vs. transition constraints: State constraints are simpler but may be less expressive than transition constraints
  - Incremental learning vs. batch learning: Incremental approach avoids manual training set selection but may fail if early policies don't generalize

- Failure signatures:
  - SAT theory unsatisfiable: No policy with given features can solve training instances
  - Out of memory during SAT solving: Feature pool too large or instances too complex
  - Policy fails on test instances: Overfitting or insufficient training data
  - No features distinguish dead/alive states: Feature generation or selection failed

- First 3 experiments:
  1. Run on smallest acrobatics instance to verify basic pipeline works
  2. Test incremental learning on blocks-clear domain to verify generalization
  3. Compare state constraints vs. transition constraints on tireworld domain to evaluate expressiveness tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the learned FOND policies scale with problem size beyond the tested benchmarks?
- Basis in paper: [explicit] The paper notes that the approach fails to generalize to larger problems in 5 out of 12 domains, indicating potential scalability limitations.
- Why unresolved: The paper only tests up to a certain problem size, leaving the performance on larger instances unclear.
- What evidence would resolve it: Experiments on significantly larger problem instances across all domains, measuring success rates and computational resources.

### Open Question 2
- Question: Can the transition constraint variant consistently outperform the state constraint variant across all domains?
- Basis in paper: [explicit] The paper presents both variants and shows the transition constraint variant solves all instances of tireworld and triangle-tireworld, where the state constraint variant fails.
- Why unresolved: The paper does not provide a comprehensive comparison across all domains to determine which variant is generally superior.
- What evidence would resolve it: A systematic comparison of both variants on all benchmark domains, measuring success rates, computational resources, and policy quality.

### Open Question 3
- Question: How robust are the learned policies to variations in the feature pool generation process?
- Basis in paper: [inferred] The paper relies on a description logic grammar to generate the feature pool, but does not explore the impact of different feature generation strategies.
- Why unresolved: The paper does not experiment with alternative feature generation methods or analyze the sensitivity of the learned policies to the feature pool.
- What evidence would resolve it: Experiments comparing policies learned with different feature generation strategies, measuring success rates and policy complexity.

## Limitations
- The approach depends critically on the closedness assumption for the all-outcome relaxation, which is not proven for all FOND domains
- The feature generation method using description logic grammar may produce overly complex features that overfit to training instances
- The incremental learning approach can fail when early policies don't generalize, potentially requiring manual intervention to select training instances

## Confidence
- **High Confidence**: The reduction of FOND planning to classical planning plus dead-end detection (Mechanism 1) - supported by clear theoretical foundations and experimental validation
- **Medium Confidence**: The SAT formulation correctly captures the generalized policy learning problem (Mechanism 3) - theoretically sound but depends on feature generation quality
- **Medium Confidence**: The policy correctness guarantees when B-safety and soundness conditions hold (Mechanism 2) - relies on assumptions about feature selection and dead-end detection completeness

## Next Checks
1. **Closedness Verification**: Test the closedness assumption by constructing counterexamples where dead-ends in the all-outcome relaxation do not correspond to dead-ends in the original FOND problem, and analyze failure modes when this assumption breaks.

2. **Feature Generalization Analysis**: Systematically vary feature complexity bounds and measure overfitting by comparing training vs. test performance across domains, particularly for domains like miner where generalization was poor.

3. **Dead-End Detection Completeness**: Implement and test the iterative dead-end detection algorithm on domains with known dead-end structures to verify soundness and completeness, and measure impact on policy correctness when detection is incomplete.