---
ver: rpa2
title: A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models
arxiv_id: '2407.00436'
source_url: https://arxiv.org/abs/2407.00436
tags:
- parallel
- language
- corpora
- tasks
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how to effectively exploit parallel corpora
  to enhance multilingual large language models (mLLMs) across diverse languages and
  tasks. The study focuses on four key factors: data quality, data quantity, training
  objectives, and model size.'
---

# A Recipe of Parallel Corpora Exploitation for Multilingual Large Language Models

## Quick Facts
- arXiv ID: 2407.00436
- Source URL: https://arxiv.org/abs/2407.00436
- Authors: Peiqin Lin; André F. T. Martins; Hinrich Schütze
- Reference count: 24
- This paper investigates how to effectively exploit parallel corpora to enhance multilingual large language models (mLLMs) across diverse languages and tasks

## Executive Summary
This paper systematically investigates how to optimally exploit parallel corpora for enhancing multilingual large language models (mLLMs) across diverse languages and tasks. The authors conduct extensive experiments using BLOOM models and various evaluation benchmarks to identify critical factors that influence mLLM performance. Through systematic ablation studies, they reveal that filtering noisy translations is essential, even small datasets of 10K parallel sentences can yield competitive results, and larger models benefit more from parallel corpora than smaller ones. The study provides actionable insights for researchers and practitioners seeking to leverage parallel corpora effectively in multilingual settings.

## Method Summary
The authors use BLOOM models with LoRA adapters for instruction tuning on parallel corpora from the OPUS100 dataset across five target languages (Arabic, Spanish, Hindi, Vietnamese, and Chinese). They systematically vary four key factors: data quality (using COMETKIWI scores to filter translations), data quantity (testing 1K, 10K, and 25K sentence pairs), training objectives (MT, TLM, and XSS patterns), and model size (BLOOM-7B1, 3B, and 1B7). Models are evaluated on translation tasks (FLORES, MUSE) and classification tasks (SIB, MLQA, XQuAD) using a 2-shot in-context learning approach.

## Key Results
- Filtering noisy translations (τc = 0.75) is essential, improving average performance by 0.8%
- Using only 10K parallel sentences achieves optimal performance (54.0% average score), comparable to larger datasets
- Machine Translation (MT) objective alone produces the best results across tasks
- Larger mLLMs (BLOOM-7B1) benefit more from parallel corpora than smaller models due to superior cross-task transferability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering noisy translations is essential for effectively exploiting parallel corpora
- Mechanism: When translation quality is low, the model learns incorrect cross-lingual mappings, leading to degraded performance across tasks
- Core assumption: COMETKIWI score threshold τc = 0.75 effectively identifies and filters low-quality translations
- Evidence anchors:
  - [abstract] "filtering noisy translations is essential for effectively exploiting parallel corpora"
  - [section 4.2] "Filtering out noisy translations leads to notable improvements. When τc = 0.5, the average performance rises from 53.2% to 53.7%. Further refinement to τc = 0.75 achieves an additional 0.3% improvement."
  - [corpus] Weak - COMETKIWI is a proxy metric but not direct performance evidence
- Break condition: If τc threshold becomes too high (>0.9), may filter out useful noisy data that helps with generalization

### Mechanism 2
- Claim: Larger mLLMs benefit more from parallel corpora than smaller models
- Mechanism: Larger models have better cross-task transferability, allowing insights from parallel corpora in one task to improve performance in others
- Core assumption: Model size correlates with cross-task transferability capacity
- Evidence anchors:
  - [abstract] "larger multilingual language models benefit more from parallel corpora than smaller models"
  - [section 7] "Larger models excel in diverse tasks. Conversely, larger models generally demonstrate greater enhancements in tasks beyond machine translation"
  - [corpus] Weak - No ablation studies on cross-task transfer specifically
- Break condition: When task-specific fine-tuning is required rather than general enhancement

### Mechanism 3
- Claim: Even a small dataset of just 10K parallel sentences can yield results comparable to much larger datasets
- Mechanism: Beyond a certain threshold, additional parallel sentences provide diminishing returns while 10K provides sufficient coverage for effective learning
- Core assumption: 10K sentences provide adequate diversity and coverage for effective instruction tuning
- Evidence anchors:
  - [abstract] "even a corpus with just 10K parallel sentences can yield results comparable to those obtained from much larger datasets"
  - [section 5.1] "Using 10K parallel sentences leads to the optimal performance. The best overall performance is achieved with 10K parallel sentences, resulting in an average score of 54.0%"
  - [corpus] Weak - No analysis of diversity or coverage metrics
- Break condition: For highly specialized domains requiring domain-specific terminology

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: The study uses instruction tuning with parallel corpora to enhance mLLMs rather than traditional fine-tuning
  - Quick check question: What's the difference between instruction tuning and standard fine-tuning in terms of data format and learning objectives?

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge transfers between languages is crucial for interpreting why larger models benefit more from parallel corpora
  - Quick check question: How does cross-lingual transfer differ from zero-shot learning in multilingual models?

- Concept: Quality estimation metrics
  - Why needed here: COMETKIWI scores are used to filter parallel corpora quality, requiring understanding of what these metrics measure
  - Quick check question: What does a COMETKIWI score of 0.75 actually indicate about translation quality?

## Architecture Onboarding

- Component map: BLOOM mLLM → LoRA adapter → parallel corpora → instruction tuning → evaluation benchmarks
- Critical path: Data quality filtering → 10K sentence selection → MT objective training → performance evaluation
- Design tradeoffs: LoRA vs full fine-tuning (computational efficiency vs potential performance)
- Failure signatures: Poor performance on bilingual tasks but good on in-language tasks indicates data quality issues
- First 3 experiments:
  1. Test τc threshold variations (0.5, 0.75, 0.9) on a single language pair
  2. Compare 1K, 10K, 25K sentence quantities on BLOOM-7B1
  3. Test all three objectives (MT, TLM, XSS) individually on BLOOM-1B7

## Open Questions the Paper Calls Out
The paper identifies several open questions that require further investigation, including the optimal LoRA rank for different languages and tasks, how different types of noise affect mLLM performance across tasks, the relationship between parallel corpus quality and model size regarding diminishing returns, the impact of domain-specific parallel corpora versus general-domain corpora, and how different tokenization strategies affect parallel corpus effectiveness. The authors also call for research on the optimal balance between parallel corpus quantity and quality under computational constraints, the impact of parallel corpus exploitation on zero-shot and few-shot performance across different language families, and how translation direction (en→xx vs. xx→en) affects code-switching capabilities.

## Limitations
- The study relies on COMETKIWI scores as a proxy for translation quality without direct human validation
- Limited exploration of hybrid training objectives combining MT, TLM, and XSS patterns
- Focus on BLOOM models limits generalizability to other mLLM architectures
- No analysis of diversity or coverage metrics for the parallel corpora used
- Implementation details for negative example construction in XSS objective were not fully specified

## Confidence
- Data quality filtering (τc = 0.75): High confidence
- 10K sentence threshold: High confidence  
- Larger models benefit more: Medium confidence
- MT objective superiority: Medium confidence
- Cross-task transferability mechanism: Medium confidence
- LoRA implementation details: Low confidence

## Next Checks
1. **Ablation study on cross-task transfer**: Train separate models on translation tasks only versus general instruction tuning, then measure transfer to classification tasks to isolate the cross-task benefit claimed for larger models.

2. **Quality threshold sensitivity analysis**: Systematically vary τc thresholds (0.5, 0.6, 0.7, 0.75, 0.8, 0.9) across all language pairs and measure both COMETKIWI scores and downstream task performance to identify optimal filtering strategies per language.

3. **Objective combination experiments**: Test hybrid training objectives (MT+TLM, MT+XSS, all three combined) with varying weight ratios to determine if the single-objective approach is truly optimal or if combinations could yield better performance.