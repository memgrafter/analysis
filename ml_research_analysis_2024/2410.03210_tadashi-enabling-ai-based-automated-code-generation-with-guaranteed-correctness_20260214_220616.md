---
ver: rpa2
title: 'Tadashi: Enabling AI-Based Automated Code Generation With Guaranteed Correctness'
arxiv_id: '2410.03210'
source_url: https://arxiv.org/abs/2410.03210
tags: []
core_contribution: This paper introduces Tadashi, a Python library that enables machine
  learning practitioners to generate code with guaranteed correctness using the polyhedral
  model. The key challenge addressed is that existing ML approaches for code generation
  cannot provide formal guarantees of functional correctness.
---

# Tadashi: Enabling AI-Based Automated Code Generation With Guaranteed Correctness

## Quick Facts
- **arXiv ID**: 2410.03210
- **Source URL**: https://arxiv.org/abs/2410.03210
- **Reference count**: 40
- **Primary result**: Tadashi is a Python library that uses the polyhedral model to enable ML-based code generation with formal correctness guarantees

## Executive Summary
This paper introduces Tadashi, a Python library that bridges the gap between machine learning-based code generation and formal correctness guarantees. Existing ML approaches for code generation cannot provide functional correctness guarantees, which limits their applicability in production systems. Tadashi solves this by leveraging the polyhedral model to verify the legality of transformations applied to loop nests, enabling AI-based code generation while maintaining correctness. The library provides a simple Python interface that allows users to sample legal transformations, check their validity, and generate code with guaranteed correctness.

## Method Summary
Tadashi uses the polyhedral model as a formal framework to verify code transformations. The approach involves representing loop nests as mathematical objects (polyhedra) and using this representation to check whether transformations preserve program semantics. The library provides a Python interface where users can specify transformations, which are then verified using polyhedral analysis before code generation. This ensures that only semantically equivalent transformations are applied, providing formal guarantees of correctness while maintaining the flexibility of AI-based approaches.

## Key Results
- Demonstrates 100-800 transformations per second while maintaining correctness guarantees
- Code generation overhead is negligible compared to compilation and execution times
- Successfully implemented a mock reinforcement learning system using the library
- Verified performance using Polybench benchmarks

## Why This Works (Mechanism)
The polyhedral model provides a mathematical framework for representing and analyzing loop nests as geometric objects. This allows formal verification of whether transformations preserve program semantics. By integrating this verification step into the ML-based code generation pipeline, Tadashi ensures that only transformations that maintain correctness are applied. The Python interface abstracts the complexity of polyhedral analysis while maintaining the formal guarantees.

## Foundational Learning

**Polyhedral Model**
- Why needed: Provides mathematical framework for representing loop nests as geometric objects
- Quick check: Can represent loop iteration spaces as polyhedra and apply affine transformations

**Loop Nest Transformations**
- Why needed: Core operations for optimizing and parallelizing code
- Quick check: Understand legality conditions for loop interchange, tiling, and skewing

**Formal Verification**
- Why needed: Ensures generated code maintains functional correctness
- Quick check: Can distinguish between legal and illegal transformations using polyhedral analysis

**Affine Transformations**
- Why needed: Mathematical operations that preserve polyhedral structure
- Quick check: Understand how affine mappings affect iteration spaces

## Architecture Onboarding

**Component Map**
Python Interface -> Polyhedral Analysis Engine -> Code Generator -> Output Code

**Critical Path**
User specifies transformation → Polyhedral legality check → Code generation → Output

**Design Tradeoffs**
- Accuracy vs. performance: Polyhedral analysis is computationally expensive but provides strong guarantees
- Flexibility vs. simplicity: Python interface simplifies usage but may limit advanced features
- Verification overhead vs. correctness: Additional checks ensure correctness but add processing time

**Failure Signatures**
- Illegal transformation attempts are rejected by polyhedral analysis
- Performance degradation when analyzing complex loop nests
- Generation failures when polyhedral representation cannot be constructed

**First Experiments**
1. Apply simple loop interchange to a basic loop nest and verify correctness
2. Test performance with increasingly complex loop structures
3. Compare generated code against manually optimized versions

## Open Questions the Paper Calls Out
None

## Limitations

- Evaluation relies on Polybench benchmarks, which may not represent real-world ML workloads
- Performance comparison lacks benchmarks against ML-only approaches without correctness guarantees
- "Negligible" overhead claim is context-dependent and may vary in latency-critical applications

## Confidence

**High confidence**: The polyhedral model for correctness verification is a well-established compiler theory concept
**Medium confidence**: Performance claims are reasonable but practical significance depends on specific use cases
**Medium confidence**: Usability claims are supported by demonstration but lack broader user studies

## Next Checks

1. Benchmark against ML-only code generation approaches to quantify the performance trade-off
2. Test with real-world ML workloads beyond Polybench to assess practical applicability
3. Conduct a user study with ML practitioners to evaluate library usability and integration complexity