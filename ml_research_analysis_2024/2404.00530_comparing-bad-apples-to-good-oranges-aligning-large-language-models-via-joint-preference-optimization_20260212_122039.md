---
ver: rpa2
title: 'Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint
  Preference Optimization'
arxiv_id: '2404.00530'
source_url: https://arxiv.org/abs/2404.00530
tags:
- response
- joint
- preference
- preferences
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the traditional paradigm of preference acquisition
  for aligning large language models (LLMs) by proposing a framework that elicits
  preferences jointly over instruction-response pairs, rather than just pairwise comparisons
  on identical contexts. The authors introduce Joint Preference Optimization (JPO),
  a novel preference optimization objective that upweights the joint probability of
  chosen instruction-response pairs over rejected ones, allowing the model to leverage
  diverse preference signals from non-identical instructions.
---

# Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization

## Quick Facts
- arXiv ID: 2404.00530
- Source URL: https://arxiv.org/abs/2404.00530
- Authors: Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, Aditya Grover
- Reference count: 20
- One-line primary result: JPO outperforms DPO by 5.2% and 3.3% win-rate for summarization and open-ended dialogue tasks, respectively

## Executive Summary
This paper introduces Joint Preference Optimization (JPO), a novel approach to aligning large language models by leveraging joint preferences over instruction-response pairs rather than traditional pairwise comparisons on identical contexts. JPO upweights the joint probability of chosen instruction-response pairs over rejected ones, enabling the model to extract richer preference signals from diverse comparisons. The method demonstrates significant improvements over Direct Preference Optimization (DPO), achieving a 5.2% win-rate improvement on summarization tasks and 3.3% on dialogue tasks, while also improving performance on the AlpacaEval2 leaderboard.

## Method Summary
The JPO framework involves collecting joint preference data by pairing chosen and rejected instruction-response pairs from existing datasets, then optimizing a language model using an objective that upweights the joint probability of preferred pairs. The approach uses supervised fine-tuned models as reference points and can work with both AI-generated and human-annotated preferences. JPO generalizes DPO as a special case when instructions are identical, allowing it to be more flexible while maintaining compatibility with existing preference optimization methods.

## Key Results
- JPO outperforms DPO by 5.2% win-rate on TL;DR summarization tasks
- JPO achieves 3.3% win-rate improvement on Anthropic-Helpful dialogue tasks
- JPO shows 1.8 percentage point improvement over DPO on AlpacaEval2 leaderboard

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JPO captures richer human preference signals by allowing comparisons across non-identical instructions, revealing decision-making heuristics that are obfuscated in traditional pairwise ranking protocols.
- Mechanism: By upweighting the joint probability of chosen instruction-response pairs over rejected ones, JPO leverages the full context of the comparison, including the quality of both the instruction and response.
- Core assumption: Human preferences are context-dependent and can be more accurately elicited by comparing responses across diverse instructions rather than identical ones.
- Evidence anchors: [abstract] "joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs"; [section 5.4] "humans can provide decisive preferences in joint preferences protocol"
- Break condition: If instructions are too dissimilar (e.g., comparing cooking recipes to harmful instructions), comparisons become meaningless.

### Mechanism 2
- Claim: JPO subsumes prior preference optimization methods like DPO as a special case when instructions are identical.
- Mechanism: When instructions in joint preference pairs are the same, JPO objective reduces to DPO objective, which optimizes conditional probability of chosen response given instruction.
- Core assumption: DPO objective is valid and effective for aligning LLMs when instructions are identical.
- Evidence anchors: [section 4] "JPO subsumes prior preference optimizations as conditional rankings are a special case of joint preferences"
- Break condition: If DPO is not effective for aligning LLMs in identical instruction case, JPO's generalization may not be beneficial.

### Mechanism 3
- Claim: JPO achieves better alignment by effectively leveraging diverse preference signals present in existing instruction-response data without requiring additional data collection.
- Mechanism: By using joint preferences over instruction-response pairs, JPO extracts more information from the same amount of data compared to DPO and KTO, which only use conditional preferences.
- Core assumption: Additional preference signals captured by joint preferences are valuable for aligning LLMs and lead to better performance than using only conditional preferences.
- Evidence anchors: [section 6.2] "JPO outperforms DPO by 5.2% and 3.3% win-rate points on the summarization and helpfulness datasets"
- Break condition: If additional preference signals are not useful for aligning LLMs or if JPO is more sensitive to data quality and diversity.

## Foundational Learning

- **Preference acquisition and ranking protocols**: Understanding how human preferences are traditionally acquired and the limitations of existing ranking protocols is crucial for appreciating the novelty and benefits of joint preference acquisition framework.
  - Why needed: To understand the motivation behind moving beyond traditional pairwise comparisons
  - Quick check: What are the limitations of traditional pairwise ranking protocols for acquiring human preferences for LLM alignment?

- **Direct Preference Optimization (DPO) and its variants**: JPO is presented as a generalization of DPO that can handle joint preferences over non-identical instructions.
  - Why needed: To understand how DPO works and its limitations as motivation for JPO
  - Quick check: How does the DPO objective optimize a language model using conditional preferences?

- **Reinforcement Learning from Human Feedback (RLHF) and its alternatives**: The paper mentions RLHF as a common approach for aligning LLMs with human preferences, and JPO is presented as an alternative.
  - Why needed: To understand the context and motivation for JPO as an alternative to RLHF
  - Quick check: What are the key steps involved in the RLHF approach for aligning LLMs with human preferences?

## Architecture Onboarding

- **Component map**: Supervised fine-tuned language model (SFT) -> Joint preference dataset (chosen/rejected instruction-response pairs) -> JPO objective function -> Optimized language model
- **Critical path**: (1) Obtain SFT model, (2) Construct joint preference dataset from existing data, (3) Implement JPO objective function, (4) Optimize language model using JPO objective and joint preference dataset
- **Design tradeoffs**: JPO trades simplicity and compatibility with existing methods for ability to capture richer human preference signals. Requires constructing joint preference datasets but can extract more information from same data amount.
- **Failure signatures**: JPO may fail if joint preference comparisons are not meaningful (comparing responses to very different instructions) or if additional preference signals are not actually useful for aligning LLMs. May be more sensitive to data quality and diversity compared to DPO.
- **First 3 experiments**:
  1. Implement JPO objective function and verify it reduces to DPO objective when instructions are identical.
  2. Construct small joint preference dataset from existing conditional preference data and verify JPO can align language model using this dataset.
  3. Compare performance of JPO and DPO on small-scale alignment task using same amount of data and verify JPO achieves better performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Joint Preference Optimization (JPO) scale with the size of the joint preference dataset when compared to Direct Preference Optimization (DPO) under the same conditions?
- Basis in paper: [inferred] The paper discusses the impact of dataset size on JPO's performance, noting improvements with increased data, but does not directly compare this scaling effect to DPO.
- Why unresolved: The paper provides data on JPO's scaling with dataset size but lacks a comparative analysis with DPO.
- What evidence would resolve it: Conducting experiments where both JPO and DPO are trained on datasets of varying sizes and comparing their win-rates against gold responses.

### Open Question 2
- Question: Can the introduction of instruction similarity metrics improve the quality and consistency of joint preference acquisition?
- Basis in paper: [explicit] The paper mentions the potential difficulty in comparing responses from instructions with significantly dissimilar distributions, suggesting the use of instruction similarity metrics.
- Why unresolved: While the paper suggests the possibility of using instruction similarity metrics, it does not explore or implement such a system.
- What evidence would resolve it: Implementing and evaluating a system that uses instruction similarity metrics to guide joint preference acquisition, followed by measuring the consistency and quality of resulting preferences.

### Open Question 3
- Question: How do joint preferences acquired from diverse demographic groups affect the alignment and performance of LLMs?
- Basis in paper: [explicit] The paper acknowledges the limitation of using annotators predominantly from U.S. or Canada regions, highlighting the need for studying diverse group impacts.
- Why unresolved: The paper identifies the limitation but does not conduct experiments or analyses with annotators from diverse demographic backgrounds.
-