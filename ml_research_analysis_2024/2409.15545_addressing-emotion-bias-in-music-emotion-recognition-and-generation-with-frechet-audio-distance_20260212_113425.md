---
ver: rpa2
title: Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet
  Audio Distance
arxiv_id: '2409.15545'
source_url: https://arxiv.org/abs/2409.15545
tags:
- emotion
- music
- audio
- bias
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates emotion bias in Music Emotion Recognition
  (MER) and Emotional Music Generation (EMG), proposing solutions using Frechet Audio
  Distance (FAD) and diverse audio encoders. The research benchmarks MER performance
  using various encoders, revealing that valence is more challenging to predict than
  arousal, and that performance varies significantly across encoders and emotion dimensions.
---

# Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance

## Quick Facts
- arXiv ID: 2409.15545
- Source URL: https://arxiv.org/abs/2409.15545
- Reference count: 36
- Key outcome: The study proposes using Frechet Audio Distance (FAD) and diverse audio encoders to address emotion bias in Music Emotion Recognition (MER) and Emotional Music Generation (EMG), demonstrating improved emotional realism in generated music through combined categorical and dimensional emotion conditioning.

## Executive Summary
This paper addresses emotion bias in Music Emotion Recognition (MER) and Emotional Music Generation (EMG) by introducing Frechet Audio Distance (FAD) as an objective evaluation metric and proposing a multi-encoder approach to reduce bias. The authors benchmark MER performance using various audio encoders, revealing that valence is more challenging to predict than arousal, and that performance varies significantly across encoders and emotion dimensions. For EMG, they introduce an enhanced model that combines categorical and dimensional emotion conditioning, achieving more realistic emotional expression in generated music. Comparative evaluations show that their approach produces synthetic music with FAD scores closer to real music than baseline models, particularly improving differentiation between emotion quadrants while maintaining natural variation.

## Method Summary
The method involves benchmarking MER performance using multiple audio encoders (VGGish, CLAP, CLAP-LAION, MERT, CDPAM, EnCodec, DAC) on datasets like Emomusic, MMirex, and EMOPIA, then proposing FAD as an objective metric by averaging scores across encoders to reduce bias. For EMG, the authors enhance a Music Transformer backbone by combining categorical (quadrant-based) and dimensional (continuous V-A values) emotion conditioning through weighted embeddings and cross-attention mechanisms. The approach aims to balance emotion prominence with variation while maintaining realism, validated through FAD comparisons between real and synthetic music across all emotion quadrants.

## Key Results
- Valence prediction is significantly more challenging than arousal prediction across all encoders and datasets
- Multi-encoder FAD averaging reduces emotion bias compared to single-encoder approaches
- The enhanced EMG model combining categorical and dimensional conditioning produces music with FAD scores closer to real music than baseline models
- The proposed approach improves differentiation between emotion quadrants while maintaining natural variation in generated music

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FAD scores provide a reference-free, objective measure for comparing emotional content across music datasets
- Mechanism: FAD computes the distance between multivariate Gaussians fitted to audio embeddings from different music sets, allowing comparison without ground truth labels
- Core assumption: Audio embeddings capture sufficient emotional information to distinguish between different emotion categories
- Evidence anchors:
  - [abstract] "We then propose assessing MER performance using FAD derived from multiple encoders to provide a more objective measure of musical emotion"
  - [section] "FAD was initially proposed for evaluating music enhancement quality and has shown a close correlation with human perception as a reference-free metric"
  - [corpus] Weak evidence - corpus contains related papers on music emotion but none specifically validate FAD for emotion comparison
- Break condition: If audio encoders fail to capture emotion-relevant features, FAD scores will not correlate with perceived emotional differences

### Mechanism 2
- Claim: Combining categorical and dimensional emotion conditioning produces more realistic emotional expression than either approach alone
- Mechanism: The model uses weighted embeddings from both quadrant-based (categorical) and continuous V-A (dimensional) representations, allowing emotion to be anchored while maintaining variation
- Core assumption: Music emotion can be effectively represented as a combination of discrete quadrant positioning and continuous V-A values
- Evidence anchors:
  - [abstract] "Furthermore, we introduce an enhanced EMG approach designed to improve both the variability and prominence of generated musical emotion"
  - [section] "Instead of using either continuous V A values or discrete emotion quadrants alone, we combine them in the conditioning process"
  - [corpus] Weak evidence - corpus lacks papers specifically testing combined categorical/dimensional approaches in EMG
- Break condition: If the weighted sum cannot balance precision and variation effectively, generated music will either be too similar within emotions or too ambiguous

### Mechanism 3
- Claim: Multiple audio encoders reduce emotion bias compared to single-encoder approaches
- Mechanism: Using diverse encoders with different training objectives (contrastive, compression, self-supervised) captures complementary aspects of emotional expression, reducing reliance on any single encoder's bias
- Core assumption: Different encoders capture different aspects of emotional information in music
- Evidence anchors:
  - [abstract] "Our study begins with a benchmark evaluation of MER, highlighting the limitations of using a single audio encoder"
  - [section] "We include models trained with various objectives, such as VGGish (convolutional embeddings), CLAP (contrastive embeddings), MERT (self-supervised embeddings)"
  - [corpus] Weak evidence - corpus contains related papers on music emotion but none specifically validate multi-encoder approaches
- Break condition: If encoders are too similar in their representation capabilities, averaging provides no benefit over single-encoder approaches

## Foundational Learning

- Concept: Frechet Distance and Multivariate Gaussians
  - Why needed here: FAD calculates distance between two sets of audio embeddings using their mean and covariance matrices
  - Quick check question: How would you compute the Frechet distance between two sets of 2D points with different means and covariances?

- Concept: Dimensional vs Categorical Emotion Models
  - Why needed here: The paper addresses both Russell's circumplex model (valence-arousal) and categorical quadrant approaches
  - Quick check question: What are the key differences between representing emotion as continuous V-A values versus discrete quadrant labels?

- Concept: Self-Supervised Learning in Audio Encoders
  - Why needed here: Various encoders use different training strategies (contrastive, self-supervised, compression) that affect their ability to capture emotional information
  - Quick check question: How might an encoder trained for audio compression differ in emotional representation capability from one trained with contrastive learning?

## Architecture Onboarding

- Component map: Audio encoder selection → Feature extraction → FAD computation → Multi-encoder averaging → MER/EMG evaluation
- Critical path: Audio encoding → FAD calculation → Bias reduction in emotion recognition/generation
- Design tradeoffs: Single encoder provides consistency but introduces bias; multiple encoders reduce bias but increase computational cost
- Failure signatures: High variance in FAD scores across encoders suggests inconsistent emotional representation; low FAD scores between emotion categories indicate poor discrimination
- First 3 experiments:
  1. Compute FAD scores between all pairs of emotion categories in a single dataset using one encoder
  2. Repeat experiment with multiple encoders and compare variance in results
  3. Generate music with baseline and proposed models, then compute FAD scores against real music to validate emotional realism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further improve the generalizability of music emotion recognition across different musical cultures and genres?
- Basis in paper: [explicit] The paper mentions that MER models trained on Western music often perform poorly on non-Western music due to differences in emotional interpretation.
- Why unresolved: While the paper acknowledges this limitation, it does not provide specific solutions or techniques to address cross-cultural differences in MER.
- What evidence would resolve it: Experimental results comparing MER models trained on diverse, culturally balanced datasets versus traditional Western-centric datasets, along with cross-cultural evaluation metrics.

### Open Question 2
- Question: What is the optimal weighting strategy for combining categorical and dimensional emotion embeddings in the proposed EMG model?
- Basis in paper: [explicit] The paper uses a fixed weight of 0.5 for combining quadrant and V-A embeddings, but acknowledges this is adjustable.
- Why unresolved: The paper does not explore how different weighting strategies affect the balance between emotion prominence and variation in generated music.
- What evidence would resolve it: Systematic evaluation of generated music quality across different weight values, measuring both emotional accuracy and naturalness through human ratings and objective metrics.

### Open Question 3
- Question: How does the proposed FAD-based evaluation approach compare to human perception in assessing emotional realism in generated music?
- Basis in paper: [inferred] The paper advocates for FAD as an objective alternative to subjective human ratings but does not validate its correlation with human perception.
- Why unresolved: While FAD shows promise as a reference-free metric, its effectiveness in capturing the nuances of emotional expression that humans perceive remains untested.
- What evidence would resolve it: Comparative studies measuring correlation between FAD scores and human ratings for emotional realism across various music generation models.

## Limitations

- Limited dataset diversity - the study primarily uses Western music datasets (Emomusic, MMirex, EMOPIA) which may not generalize to other cultural music traditions or broader emotional expression patterns
- Encoder bias persistence - while using multiple encoders reduces bias compared to single-encoder approaches, the fundamental assumption that these encoders capture complementary emotional information remains untested across diverse music genres
- Evaluation metric constraints - FAD scores, while objective, may not fully correlate with human perception of emotional authenticity in generated music, particularly for complex emotional expressions that blend multiple quadrants

## Confidence

- High confidence: The benchmark findings showing valence is more challenging to predict than arousal, and that performance varies significantly across encoders and emotion dimensions
- Medium confidence: The effectiveness of FAD as an objective measurement for emotion bias, pending validation against human perception studies
- Medium confidence: The enhanced EMG approach combining categorical and dimensional conditioning produces more realistic emotional expression, based on FAD score improvements over baselines

## Next Checks

1. Conduct human perception studies comparing FAD scores with subjective emotional ratings for both recognized and generated music across all emotion quadrants
2. Test the multi-encoder approach on diverse cultural music datasets to validate generalizability beyond Western music traditions
3. Perform ablation studies on the proposed EMG model to quantify the individual contributions of categorical vs dimensional conditioning components to overall emotional realism