---
ver: rpa2
title: 'Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs
  and Transformers'
arxiv_id: '2408.12568'
source_url: https://arxiv.org/abs/2408.12568
tags:
- pruning
- relevance
- layers
- attribution
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper improves model pruning by optimizing attribution method
  hyperparameters specifically for the task of pruning. The authors extend attribution-based
  pruning to transformers, revealing that transformers exhibit higher over-parameterization
  than CNNs.
---

# Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers

## Quick Facts
- arXiv ID: 2408.12568
- Source URL: https://arxiv.org/abs/2408.12568
- Authors: Sayed Mohammad Vakilzadeh Hatefi; Maximilian Dreyer; Reduan Achtibat; Thomas Wiegand; Wojciech Samek; Sebastian Lapuschkin
- Reference count: 40
- Primary result: Attribution method hyperparameters optimized for pruning achieve higher compression rates than heuristic choices, with transformers showing higher over-parameterization than CNNs

## Executive Summary
This paper demonstrates that optimizing Layer-wise Relevance Propagation (LRP) hyperparameters specifically for pruning tasks significantly outperforms heuristic attribution methods. The authors extend attribution-based pruning to transformers, revealing that vision transformers are more over-parameterized than convolutional networks. By systematically searching LRP rule combinations and absolute value usage, they achieve superior compression rates while maintaining accuracy on ImageNet classification. The work challenges the assumption that attribution methods optimized for explanations are suitable for pruning tasks.

## Method Summary
The approach extends attribution-based structured pruning by introducing hyperparameter optimization for LRP methods. Rather than using fixed LRP configurations, the method performs a grid search over LRP rule combinations (LLR, MLL, HLL, FCL groups) and whether to use absolute values, followed by Bayesian optimization to maximize area under the sparsity-accuracy curve. The framework computes component relevances using reference samples, optimizes LRP hyperparameters, and prunes components based on sorted relevance scores. The method is applied to both CNNs (VGG, ResNet) and transformers (ViT), with specific attention to handling attention heads and linear layers in transformers using appropriate LRP rules (CP-LRP and AttnLRP).

## Key Results
- Optimized LRP hyperparameters achieve higher compression rates than heuristic LRP choices while maintaining accuracy
- Vision transformers exhibit higher over-parameterization than convolutional networks, enabling greater pruning potential
- Attribution methods optimized for explanations (like LRP-z+) perform poorly for pruning compared to configurations like LRP-ϵ
- The method outperforms previous approaches including heuristically chosen LRP composites from prior work

## Why This Works (Mechanism)

### Mechanism 1
Optimizing LRP hyperparameters for pruning yields higher compression rates than heuristic LRP choices. The method uses relevance scores to identify least important components, systematically searching LRP rule combinations and absolute value usage to maximize area under the sparsity-accuracy curve. Core assumption: Relevance scores that optimize for pruning performance also preserve model functionality. Evidence shows higher model compression rates for VGG, ResNet, and ViT architectures. Break condition: If optimized LRP configurations fail to generalize across different network architectures or tasks.

### Mechanism 2
Vision transformers exhibit higher over-parameterization than CNNs, enabling greater pruning potential. ViTs contain more neurons (approximately 46,000) compared to CNNs (few thousand filters), and experiments show ViTs maintain performance at higher pruning rates. Core assumption: Neuron count correlates with over-parameterization potential. Evidence anchors show transformers maintain accuracy at pruning rates up to 20% while CNNs show earlier degradation. Break condition: If pruning experiments on other datasets show similar performance between CNNs and ViTs.

### Mechanism 3
Attributions optimized for explanations are not necessarily optimal for pruning. LRP configurations that produce faithful input explanations (like LRP-z+ for ViTs) perform poorly for pruning, while configurations like LRP-ϵ excel at pruning despite being noisy for input explanations. Core assumption: Different optimization objectives require different attribution characteristics. Evidence shows a faithful LRP composite designed for ViT explanations is not ideal for pruning. Break condition: If future work demonstrates attribution methods that excel at both explanation and pruning simultaneously.

## Foundational Learning

- Concept: Layer-wise Relevance Propagation (LRP) rules and hyperparameters
  - Why needed here: The entire pruning framework depends on computing component relevances using specific LRP configurations
  - Quick check question: What is the difference between LRP-ϵ and LRP-z+ rules in terms of how they handle relevance distribution?

- Concept: Attribution-based structured pruning methodology
  - Why needed here: The paper extends existing attribution-based pruning by adding hyperparameter optimization
  - Quick check question: How does the paper's approach differ from the original LRP-based pruning method [48]?

- Concept: Vision transformer architecture components (attention heads, linear layers)
  - Why needed here: The paper extends pruning to transformers by targeting these specific structures
  - Quick check question: Why does the paper need to treat attention heads differently from linear layer neurons when computing relevances?

## Architecture Onboarding

- Component map: (1) Attribution method (LRP with tunable hyperparameters) -> (2) Reference sample set for computing component relevances -> (3) Hyperparameter optimization procedure (grid search + Bayesian optimization) -> (4) Pruning execution that removes components based on sorted relevance scores
- Critical path: Compute relevances → Optimize hyperparameters → Apply pruning → Evaluate performance
- Design tradeoffs: Using more reference samples increases relevance estimation accuracy but also computational cost; optimizing for higher pruning rates may sacrifice model robustness
- Failure signatures: If pruning leads to rapid accuracy degradation, check whether the LRP configuration is appropriate for the architecture; if hyperparameter optimization fails, verify the search space covers relevant configurations
- First 3 experiments:
  1. Implement basic LRP-based pruning on a small CNN (like ResNet-18) with a simple LRP-ϵ configuration and 10 reference samples
  2. Compare performance of different LRP rules (ϵ, z+, αβ) on the same CNN to understand their impact on pruning
  3. Extend to a small ViT and experiment with the attention-specific LRP rules (CP-LRP vs AttnLRP) to observe differences in pruning behavior

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of reference sample size affect the stability and reliability of attribution-based pruning across different architectures? The paper shows CNNs require more reference samples than transformers for stable pruning results, but didn't systematically explore the relationship between sample size and pruning performance across a broader range of models and tasks. Resolution would require a comprehensive study varying reference sample sizes across multiple architectures and tasks.

### Open Question 2
Why do transformer models exhibit higher over-parameterization than convolutional networks? The authors observe transformers show higher pruning potential than CNNs in their experiments, but don't provide a theoretical explanation for this difference, only empirical observations. Resolution would require analysis of fundamental architectural differences between transformers and CNNs.

### Open Question 3
What is the relationship between attribution methods optimized for explanations and those optimized for pruning? The authors find that LRP composites good for explanations are not necessarily optimal for pruning, but don't explore the underlying reasons for this discrepancy or whether there's a unified framework that could optimize for both tasks. Resolution would require research into the mathematical relationship between explanation quality and pruning effectiveness.

### Open Question 4
How does pruning affect the interpretability and reliability of model explanations? The authors show that optimized pruning maintains explanation quality better than random pruning, but don't fully explore the implications. The study focuses on accuracy metrics rather than a comprehensive analysis of how pruning impacts explanation faithfulness, completeness, and stability. Resolution would require systematic evaluation of explanation quality metrics before and after pruning.

## Limitations
- Generalizability of optimized LRP configurations across different datasets and model architectures remains unclear
- Computational cost of hyperparameter optimization via grid search and Bayesian optimization may limit practical applicability
- Claim about transformer over-parameterization lacks comparative evidence across diverse architectures and tasks

## Confidence

- High confidence: The core claim that optimizing LRP hyperparameters improves pruning performance
- Medium confidence: The finding that ViTs exhibit higher over-parameterization than CNNs
- Medium confidence: The assertion that explanation-optimized attributions perform poorly for pruning

## Next Checks

1. Test the optimized LRP configurations on non-ImageNet datasets (e.g., CIFAR-10, medical imaging) to assess generalizability
2. Compare pruning performance across different transformer variants (DeiT, Swin, etc.) to verify the over-parameterization claim
3. Evaluate the computational efficiency of the hyperparameter optimization pipeline against practical deployment constraints