---
ver: rpa2
title: Improving Sequential Recommendations with LLMs
arxiv_id: '2402.01339'
source_url: https://arxiv.org/abs/2402.01339
tags:
- item
- recommendation
- recommendations
- embeddings
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) can be
  leveraged to improve sequential recommendation accuracy. The authors propose three
  orthogonal approaches: (1) LLMSeqSim - using LLM embeddings to compute session similarity
  for recommendations; (2) LLMSeqPrompt - fine-tuning LLMs to generate next-item recommendations;
  and (3) LLM2Sequential - enhancing existing sequential models with LLM embeddings.'
---

# Improving Sequential Recommendations with LLMs

## Quick Facts
- arXiv ID: 2402.01339
- Source URL: https://arxiv.org/abs/2402.01339
- Reference count: 40
- Primary result: LLM embeddings improve sequential recommendation performance, with 45% average improvement in NDCG@20 over vanilla models

## Executive Summary
This paper explores how large language models (LLMs) can enhance sequential recommendation systems. The authors propose three distinct approaches: using LLM embeddings for session similarity (LLMSeqSim), fine-tuning LLMs for next-item prediction (LLMSeqPrompt), and augmenting existing sequential models with LLM embeddings (LLM2Sequential). Through extensive experiments on Amazon Beauty, Delivery Hero, and Steam datasets, the research demonstrates that LLM-enhanced models consistently outperform their vanilla counterparts, with particular success in improving NDCG@20 scores, catalog coverage, and serendipity metrics.

## Method Summary
The paper introduces three orthogonal approaches to leverage LLMs for sequential recommendation. LLMSeqSim uses LLM embeddings to compute session similarity for recommendations, capturing semantic relationships between items. LLMSeqPrompt fine-tunes LLMs to generate next-item recommendations directly. LLM2Sequential enhances existing sequential models like SASRec and BERT4Rec by incorporating LLM embeddings into their architecture. The methods are evaluated on three datasets using standard recommendation metrics including NDCG@20, catalog coverage, and serendipity.

## Key Results
- LLM2SASRec and LLM2BERT4Rec achieved 45% average improvement in NDCG@20 over vanilla models on Amazon Beauty
- 9% improvement on Delivery Hero dataset with LLM-enhanced approaches
- LLM2SASRec nearly doubled catalog coverage and increased serendipity by 21% compared to SASRec

## Why This Works (Mechanism)
LLM embeddings capture rich semantic relationships between items that traditional sequential models might miss. By leveraging pre-trained language models, the system can understand contextual similarities beyond simple co-occurrence patterns. The fine-tuning process allows models to learn domain-specific concepts while benefiting from the broader knowledge encoded in LLMs. This combination of semantic understanding and domain adaptation enables more accurate and diverse recommendations.

## Foundational Learning
- **Sequential Recommendation**: Why needed - captures user behavior patterns over time; Quick check - models predict next item based on previous interactions
- **LLM Embeddings**: Why needed - provide semantic understanding beyond item IDs; Quick check - represent items in continuous vector space capturing meaning
- **Fine-tuning**: Why needed - adapts pre-trained models to specific recommendation tasks; Quick check - improves performance on domain-specific data
- **Catalog Coverage**: Why needed - measures diversity of recommendations; Quick check - percentage of unique items recommended
- **Serendipity**: Why needed - captures unexpected but relevant recommendations; Quick check - recommendations that surprise but satisfy users

## Architecture Onboarding

Component Map: User Sessions -> LLM Embedding Layer -> Sequential Model (SASRec/BERT4Rec) -> Next Item Prediction

Critical Path: Input sequence → LLM embedding generation → Sequential model processing → Item scoring → Top-K recommendation

Design Tradeoffs:
1. Pre-trained vs. from-scratch LLMs: Pre-trained offers better semantic understanding but higher computational cost
2. Embedding fusion methods: Early fusion (before sequential model) vs. late fusion (after) affects model capacity and training stability
3. Fine-tuning scope: Full model vs. last layers impacts adaptation speed and risk of catastrophic forgetting

Failure Signatures:
1. Over-reliance on LLM embeddings causing loss of sequential patterns
2. Catastrophic forgetting during fine-tuning
3. Computational bottlenecks from LLM inference in real-time systems

First Experiments:
1. Compare LLM-enhanced vs. vanilla SASRec on Amazon Beauty with varying K values
2. Ablation study: Remove LLM embeddings to measure contribution
3. Stress test: Evaluate performance with limited training data

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of LLM-enhanced sequential recommendation models scale with model size and parameter count?
- Basis in paper: [inferred] The paper discusses the potential of leveraging LLMs for sequential recommendation but does not explore the impact of varying LLM sizes or parameter counts.
- Why unresolved: The authors acknowledge this as a limitation and suggest future work to investigate the effect of larger model sizes on recommendation accuracy.
- What evidence would resolve it: Conducting experiments with different LLM sizes and parameter counts to measure their impact on recommendation performance across various datasets and approaches.

### Open Question 2
- Question: How effective are LLM-enhanced sequential recommendation models in cold-start scenarios with limited user-item interaction data?
- Basis in paper: [inferred] The paper focuses on sequential recommendation but does not specifically analyze the performance of LLM-enhanced models in cold-start situations.
- Why unresolved: Cold-start scenarios present a unique challenge for recommendation systems, and the authors suggest future work to explore the effectiveness of LLM-enhanced models in such cases.
- What evidence would resolve it: Evaluating the performance of LLM-enhanced models on datasets with limited user-item interaction data or simulating cold-start scenarios to assess their effectiveness.

### Open Question 3
- Question: Can LLM-enhanced sequential recommendation models be effectively applied to cross-domain recommendation settings?
- Basis in paper: [inferred] The paper explores the potential of LLMs for sequential recommendation but does not investigate their applicability to cross-domain scenarios.
- Why unresolved: Cross-domain recommendation involves leveraging knowledge from multiple domains, and the authors suggest future work to study the use of LLM-enhanced models in such settings.
- What evidence would resolve it: Conducting experiments on datasets from multiple domains to evaluate the performance of LLM-enhanced models in cross-domain recommendation tasks.

## Limitations
- Limited domain coverage (Amazon Beauty, Delivery Hero, Steam) may not generalize to other recommendation contexts
- Resource requirements and computational costs of LLM-based approaches not thoroughly analyzed
- Fine-tuning data efficiency and required labeled data amounts remain unclear

## Confidence

**High Confidence**: Experimental methodology is sound with proper baseline comparisons and evaluation metrics. Reproducibility on tested datasets appears solid.

**Medium Confidence**: Claims about consistent improvement across datasets may be overstated given limited domain coverage. 45% improvement figure represents combination of different datasets and models.

**Medium Confidence**: Assertion that LLMs learn domain concepts beyond task instructions is plausible but not thoroughly validated. Evidence shows improved performance but doesn't conclusively demonstrate meaningful domain concept learning.

## Next Checks
1. **Cross-Domain Validation**: Test proposed approaches on at least two additional diverse recommendation domains (e.g., news recommendations, music streaming) to assess generalizability beyond current domains.

2. **Ablation Studies**: Conduct comprehensive ablation studies to isolate contribution of LLM embeddings versus base sequential models, particularly examining performance scaling with embedding quality and model size.

3. **Resource Efficiency Analysis**: Perform detailed analysis of computational costs including inference time, memory usage, and fine-tuning requirements to evaluate practical feasibility of deploying approaches at scale.