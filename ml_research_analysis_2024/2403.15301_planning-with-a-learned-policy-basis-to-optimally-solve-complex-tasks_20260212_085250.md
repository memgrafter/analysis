---
ver: rpa2
title: Planning with a Learned Policy Basis to Optimally Solve Complex Tasks
arxiv_id: '2403.15301'
source_url: https://arxiv.org/abs/2403.15301
tags:
- policy
- optimal
- learning
- task
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for optimal policy transfer in stochastic
  environments with non-Markovian reward specifications using successor features and
  finite state automata (FSA). The approach learns a convex coverage set (CCS) of
  policies where each policy solves a subproblem optimally.
---

# Planning with a Learned Policy Basis to Optimally Solve Complex Tasks

## Quick Facts
- arXiv ID: 2403.15301
- Source URL: https://arxiv.org/abs/2403.15301
- Reference count: 33
- Primary result: Achieves global optimality through planning on FSA task specifications rather than learning from scratch for each new task

## Executive Summary
This paper introduces a method for optimal policy transfer in stochastic environments with non-Markovian reward specifications. The approach learns a convex coverage set (CCS) of policies using successor features, where each policy solves a subproblem optimally. At test time, dynamic programming on a finite state automaton (FSA) combines these policies into a globally optimal solution for the full task without additional learning. Experiments on Office and Delivery domains demonstrate faster convergence and better generalization to stochastic environments compared to baselines that use single policies per subgoal.

## Method Summary
The method learns a CCS of policies where each policy optimally solves a subproblem within a larger non-Markovian task specified by an FSA. Successor features are used to represent policies, allowing linear combinations to maintain optimality for their respective subproblems. The SFOLS algorithm iteratively builds the CCS by prioritizing weight vectors that maximize the expected value difference between current and optimal policies. At test time, SF-FSA-VI performs dynamic programming on the FSA to find the globally optimal policy combination. This approach avoids learning from scratch for each new task by leveraging the precomputed CCS and planning based on the task specification.

## Key Results
- Converges faster than baselines (LOF, flat Q-learning) during both learning and composition phases
- Achieves optimal performance on sequential, disjunction, and composite task specifications
- Demonstrates better generalization to stochastic environments compared to single-policy approaches

## Why This Works (Mechanism)
The method works by decomposing complex non-Markovian tasks into simpler subproblems, each solved optimally by a separate policy. Successor features enable these policies to be combined linearly while preserving their optimality properties. The FSA structure encodes the non-Markovian task specification, allowing dynamic programming to efficiently find the globally optimal combination of subpolicies. This planning-based approach avoids the need for additional learning when faced with new task specifications, as the CCS contains policies that can be recombined to solve any task within the same environment.

## Foundational Learning
- **Successor Features**: Linearly combine policies while preserving optimality for their respective subproblems
  - Why needed: Enables efficient composition of subpolicies into globally optimal solutions
  - Quick check: Verify that linear combinations of policies maintain optimality for their corresponding weight vectors

- **Convex Coverage Set (CCS)**: Set of policies where each solves a subproblem optimally and spans the space of achievable value functions
  - Why needed: Provides a complete basis for composing solutions to any task in the environment
  - Quick check: Confirm that the CCS contains policies for all relevant weight vectors in the task specification

- **Finite State Automata (FSA)**: Specifies non-Markovian task structure through propositional symbols and transitions
  - Why needed: Encodes complex task dependencies and constraints that cannot be captured by Markovian rewards
  - Quick check: Verify that the FSA correctly represents the intended task logic and constraints

## Architecture Onboarding

**Component Map**
SFOLS learning -> CCS of policies -> SF-FSA-VI planning on FSA -> Globally optimal policy

**Critical Path**
1. Learn CCS using SFOLS algorithm
2. Encode task specification as FSA
3. Apply SF-FSA-VI to find optimal policy combination
4. Execute resulting policy in environment

**Design Tradeoffs**
- CCS size vs. planning efficiency: Larger CCS provides better coverage but increases planning complexity
- Sample budget per policy: Higher budgets improve policy quality but slow down CCS learning
- FSA complexity: More complex FSAs can specify richer tasks but require more sophisticated planning

**Failure Signatures**
- Premature CCS convergence: Limited policy diversity leading to suboptimal task solutions
- Planning errors: Incorrect dynamic programming updates resulting in suboptimal or invalid policies
- Stochastic environment issues: Policies fail to generalize when environment transitions are non-deterministic

**First Experiments**
1. Verify CCS learning on simple sequential task with deterministic transitions
2. Test SF-FSA-VI planning on disjunction task with known optimal solution
3. Evaluate generalization to stochastic environment by gradually increasing transition noise

## Open Questions the Paper Calls Out
None

## Limitations
- Requires a priori knowledge of subgoals and task structure to define FSA specifications
- CCS learning complexity grows with the number of subgoals and task complexity
- Performance depends on quality of successor feature learning and proper handling of exit states

## Confidence

**Confidence Assessment**
The reproduction plan is **Medium** confidence due to several critical uncertainties. The core algorithmic framework (successor features + FSA planning) is well-specified, but key implementation details are missing. The most significant gap is the lack of hyperparameter specifications for SFOLS and the baseline implementations. Additionally, the transition to stochastic environments and the proper handling of exit states in the successor feature framework require careful implementation.

**Major Uncertainties and Limitations**
1. **Hyperparameter Sensitivity**: The SFOLS algorithm's performance heavily depends on sample budget, learning rates, and discount factors, none of which are specified. This could significantly impact the convergence and quality of the learned CCS.
2. **Stochastic Environment Handling**: While the paper claims better generalization to stochastic environments, the specific mechanisms for handling stochastic transitions in both learning and planning phases are not detailed. The interaction between successor features and stochasticity requires careful implementation.
3. **FSA Specification Complexity**: The proper encoding of FSA task specifications, particularly for composite tasks, and their integration with the successor feature framework is not fully elaborated. The dynamic programming updates on the FSA structure need precise implementation to ensure optimality.

**Concrete Next Validation Checks**
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the sample budget, learning rates, and discount factors in SFOLS to determine their impact on CCS quality and convergence speed. Compare the resulting performance against the reported results.
2. **Stochastic Transition Testing**: Implement a controlled experiment where the environment transitions are systematically varied between deterministic and stochastic regimes. Verify that the learned policies maintain performance and that the planning algorithm correctly handles stochasticity.
3. **FSA Composition Verification**: For the composite task in the Delivery domain, manually verify that the dynamic programming algorithm correctly combines the individual policies to achieve the globally optimal solution. Check that the Bellman optimality equations are satisfied for the augmented FSA states.

## Next Checks
1. Hyperparameter sensitivity analysis for SFOLS algorithm
2. Controlled experiment with varying transition stochasticity
3. Manual verification of FSA composition for composite tasks