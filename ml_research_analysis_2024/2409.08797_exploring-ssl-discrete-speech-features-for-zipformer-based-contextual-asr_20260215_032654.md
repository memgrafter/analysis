---
ver: rpa2
title: Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR
arxiv_id: '2409.08797'
source_url: https://arxiv.org/abs/2409.08797
tags:
- speech
- context
- utterance
- discrete
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of SSL discrete speech features extracted
  from WavLM models as cross-utterance acoustic context features in Zipformer-Transducer
  ASR systems. The approach demonstrates improved performance on the Gigaspeech 1000-hr
  corpus by incorporating cross-utterance contexts from preceding and future segments,
  achieving statistically significant word error rate (WER) reductions of 0.32% to
  0.41% absolute (2.78% to 3.54% relative) on dev and test data.
---

# Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR

## Quick Facts
- arXiv ID: 2409.08797
- Source URL: https://arxiv.org/abs/2409.08797
- Authors: Mingyu Cui; Yifan Yang; Jiajun Deng; Jiawen Kang; Shujie Hu; Tianzi Wang; Zhaoqing Li; Shiliang Zhang; Xie Chen; Xunying Liu
- Reference count: 40
- Primary result: Achieved new state-of-the-art WER of 11.15% (dev) and 11.14% (test) on Gigaspeech 1000-hr corpus

## Executive Summary
This paper explores the use of SSL discrete speech features extracted from WavLM models as cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The approach demonstrates improved performance on the Gigaspeech 1000-hr corpus by incorporating cross-utterance contexts from preceding and future segments, achieving statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on dev and test data. The best system achieved a new state-of-the-art WER of 11.15% and 11.14% on the dev and test sets, respectively.

## Method Summary
The paper uses a neural Transducer model composed of an audio "Encoder," text "Predictor," and "Joint Network." SSL pre-trained discrete token sequences are fed into the encoder to produce acoustic representations. Two approaches are used to fuse cross-utterance contexts: utterance-level concatenation of Zipformer encoder context embeddings and more compact subspace projection via attention pooling. The model is trained using pruned RNN-T loss on 4 x NVIDIA A100 40GB GPUs with the Gigaspeech M size corpus containing 1000 hours of speech.

## Key Results
- Achieved new state-of-the-art WER of 11.15% (dev) and 11.14% (test) on Gigaspeech corpus
- Statistically significant WER reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative)
- Utterance-level concatenation outperformed attention pooling for context fusion
- SSL discrete tokens outperformed Fbank features when used for both internal and cross-utterance contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-utterance context from both preceding and future segments improves ASR performance more than internal context alone.
- Mechanism: The Zipformer encoder fuses discrete token features from surrounding utterances with current utterance encoding via multi-head self-attention (MHSA), allowing the model to leverage long-range acoustic dependencies.
- Core assumption: Cross-utterance discrete tokens preserve semantic information that is complementary to the current utterance's internal features.
- Evidence anchors:
  - [abstract] states "incorporating cross-utterance contexts in ASR systems... has been widely shown to improve speech recognition performance."
  - [section] describes that "cross-utterance SSL discrete speech context features are fused with utterance internal acoustic contexts via two approaches."
  - [corpus] shows average neighbor FMR 0.494, indicating related works are topically aligned.
- Break condition: If cross-utterance features contain mostly redundant information already captured internally, the performance gain disappears.

### Mechanism 2
- Claim: SSL discrete tokens outperform Fbank features when used for both internal and cross-utterance contexts.
- Mechanism: WavLM-based discrete tokens capture richer semantic embeddings than raw Fbank coefficients, providing better input representation for context fusion.
- Core assumption: The clustering of WavLM embeddings into 2000 discrete tokens preserves more meaningful speech structure than spectral envelope features.
- Evidence anchors:
  - [abstract] notes "discrete tokens based pre-trained features provide compact speech representations and have been successfully applied to both ASR and TTS tasks."
  - [section] explains "we apply k-means clustering directly on the extracted embeddings" and that "using more discrete tokens can improve ASR performance."
  - [corpus] neighbors include studies on SSL tokens for ASR, supporting topical relevance.
- Break condition: If token granularity is too coarse or too fine, semantic information is lost or overfitted.

### Mechanism 3
- Claim: Utterance-level concatenation of context embeddings is more effective than attention pooling for cross-utterance context fusion.
- Mechanism: Direct concatenation preserves the full sequence structure of preceding/future contexts, which is more informative for the Zipformer encoder than compressed pooled vectors.
- Core assumption: The Zipformer's multi-head attention can effectively attend across the longer concatenated context sequences without prohibitive computational cost.
- Evidence anchors:
  - [section] states "The utterance-level concatenation method generally performs better than pooling projection" with lower WER in the results.
  - [abstract] mentions both concatenation and attention pooling as fusion approaches, with concatenation shown superior.
  - [corpus] related works on contextual ASR support the general principle.
- Break condition: If computational overhead outweighs accuracy gains, or if sequence length becomes too long for efficient MHSA.

## Foundational Learning

- Concept: Self-supervised learning (SSL) speech representations
  - Why needed here: SSL models like WavLM provide pre-trained embeddings that capture speech structure without labeled data, enabling better discrete token generation.
  - Quick check question: How do SSL models like WavLM differ from supervised feature extractors in terms of domain adaptability?

- Concept: Multi-head self-attention (MHSA) fusion
  - Why needed here: MHSA in the Zipformer encoder integrates current and cross-utterance features, allowing dynamic context weighting.
  - Quick check question: In what way does MHSA handle variable-length cross-utterance contexts differently from simple concatenation?

- Concept: K-means clustering for discretization
  - Why needed here: K-means converts continuous WavLM embeddings into discrete tokens, enabling efficient modeling and compact representation.
  - Quick check question: Why might 2000 clusters be chosen over a smaller or larger number for this task?

## Architecture Onboarding

- Component map: WavLM SSL encoder → k-means discrete tokens → Zipformer encoder with MHSA → Stateless predictor → Joint network with ReLU + softmax
- Critical path: Input: discrete token sequence (current, preceding, future) → Encoder: Zipformer stacks with context concatenation or pooling → Output: logits for RNN-T loss
- Design tradeoffs:
  - Token granularity vs. model complexity (2000 clusters)
  - Concatenation vs. pooling for context fusion (accuracy vs. speed)
  - Full vs. partial cross-utterance context (RTF overhead)
- Failure signatures:
  - WER degradation when using only future or only preceding context
  - Increased RTF without WER improvement
  - Overfitting if discrete tokens are too domain-specific
- First 3 experiments:
  1. Replace Fbank with WavLM discrete tokens for internal context only (baseline comparison).
  2. Add preceding utterance context via utterance-level concatenation.
  3. Add both preceding and future contexts, compare concatenation vs. pooling fusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Zipformer-Transducer with SSL discrete speech features for cross-utterance context modeling compare to other advanced ASR architectures like Transformers or Conformer-based systems?
- Basis in paper: [explicit] The paper states that the best-performing Zipformer-Transducer system using SSL discrete token features for cross-utterance context modeling achieves a new state-of-the-art WER of 11.15% and 11.14% on the Gigaspeech dev and test sets, respectively. However, it does not directly compare its performance with other advanced ASR architectures.
- Why unresolved: The paper does not provide a direct comparison of the proposed method with other advanced ASR architectures like Transformers or Conformer-based systems.
- What evidence would resolve it: Conducting experiments to compare the performance of the proposed Zipformer-Transducer system with SSL discrete speech features for cross-utterance context modeling against other advanced ASR architectures like Transformers or Conformer-based systems on the same benchmark dataset.

### Open Question 2
- Question: What is the impact of using different SSL models (other than WavLM) for generating discrete speech features on the performance of Zipformer-Transducer ASR systems?
- Basis in paper: [inferred] The paper uses WavLM for generating discrete speech features, but it does not explore the impact of using different SSL models on the performance of the Zipformer-Transducer ASR systems.
- Why unresolved: The paper does not investigate the performance of the proposed method using discrete speech features generated by SSL models other than WavLM.
- What evidence would resolve it: Conducting experiments to compare the performance of the Zipformer-Transducer system using discrete speech features generated by different SSL models (e.g., Wav2Vec 2.0, HuBERT) on the same benchmark dataset.

### Open Question 3
- Question: How does the proposed method handle streaming scenarios, where future context is not available?
- Basis in paper: [explicit] The paper mentions that future studies will explore SSL discrete token based cross-utterance contexts for streaming Zipformer-Transducers, indicating that the current method is not designed for streaming scenarios.
- Why unresolved: The paper does not address the performance of the proposed method in streaming scenarios where future context is not available.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the proposed method in streaming scenarios and comparing it with other streaming ASR systems.

## Limitations
- No ablation studies isolating SSL pretraining versus discretization contributions
- K-means clustering parameters not specified, potentially impacting discrete representation quality
- Computational overhead analysis incomplete despite similar RTF claims
- Evaluation limited to Gigaspeech corpus without cross-domain generalization testing

## Confidence

**High confidence**: The Zipformer architecture's ability to fuse current and cross-utterance contexts via MHSA is well-established in the methodology. The statistical significance of WER improvements (0.32-0.41% absolute reduction) is properly demonstrated through rigorous testing. The state-of-the-art claim on Gigaspeech is supported by direct comparisons with published baselines.

**Medium confidence**: The superiority of utterance-level concatenation over attention pooling for context fusion is demonstrated but based on a limited comparison within this single system configuration. The claim that SSL discrete tokens outperform Fbank features requires further validation through additional ablation studies.

**Low confidence**: The mechanism by which cross-utterance contexts improve ASR performance is inferred rather than empirically validated. The optimal granularity of 2000 discrete clusters is presented as effective but not systematically explored across different values.

## Next Checks

1. **Ablation study on context sources**: Train and evaluate separate systems using only preceding utterance context, only future utterance context, and internal context only, to quantify the individual contribution of each context type to the observed WER improvements.

2. **Discrete token granularity sweep**: Systematically vary the number of k-means clusters (500, 1000, 2000, 3000, 4000) to determine the optimal granularity for SSL discrete tokens and assess sensitivity to this hyperparameter.

3. **Computational overhead analysis**: Measure actual wall-clock time and memory usage for training and inference with cross-utterance contexts versus baseline systems, including data loading and preprocessing costs for neighboring utterances.