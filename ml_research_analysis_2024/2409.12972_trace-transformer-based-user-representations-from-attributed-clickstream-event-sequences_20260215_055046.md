---
ver: rpa2
title: 'TRACE: Transformer-based user Representations from Attributed Clickstream
  Event sequences'
arxiv_id: '2409.12972'
source_url: https://arxiv.org/abs/2409.12972
tags:
- user
- trace
- page
- embeddings
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRACE introduces a novel transformer-based approach for generating
  user embeddings from multi-session clickstream data in travel e-commerce. Unlike
  prior methods focused on single sessions, TRACE leverages site-wide page view sequences
  spanning multiple sessions, employing a multi-task learning framework to capture
  comprehensive user preferences and intents.
---

# TRACE: Transformer-based user Representations from Attributed Clickstream Event sequences

## Quick Facts
- arXiv ID: 2409.12972
- Source URL: https://arxiv.org/abs/2409.12972
- Authors: William Black; Alexander Manlove; Jack Pennington; Andrea Marchini; Ercument Ilhan; Vilda Markeviciute
- Reference count: 30
- Primary result: Transformer-based model achieves +7.23% AUROC and +13.58% AUPRC uplift over baseline for multi-session user embedding generation

## Executive Summary
TRACE introduces a novel transformer-based approach for generating user embeddings from multi-session clickstream data in travel e-commerce. Unlike prior methods focused on single sessions, TRACE leverages site-wide page view sequences spanning multiple sessions, employing a multi-task learning framework to capture comprehensive user preferences and intents. The model uses a lightweight transformer encoder trained on five diverse engagement targets, incorporating event-level attributes and learnable positional encodings for session and event positions. Experiments on a large-scale dataset show TRACE outperforms vanilla transformers and LSTM-based models, achieving mean AUROC uplift of +7.23% and AUPRC uplift of +13.58% over a myopic baseline. Embeddings reveal meaningful clusters corresponding to user behaviors, demonstrating strong generalization to unseen tasks and effectiveness for real-time recommendation applications.

## Method Summary
TRACE processes multi-session clickstream data using a transformer encoder with learnable event and session position encodings. The model extracts page view events with categorical attributes and timestamps, creating sessions based on 2-hour gaps. Time-based features and session IDs are engineered, then encoded into fixed-length sequences. A single transformer block with 8 attention heads and embedding dimension 32 processes the sequences, with outputs passed through global max pooling and a shared dense layer before reaching five task-specific dense layers. The model is trained using class-weighted binary cross-entropy loss across five binary classification tasks representing different user engagement signals.

## Key Results
- TRACE achieves mean AUROC uplift of +7.23% and AUPRC uplift of +13.58% over myopic baseline across evaluation tasks
- The model demonstrates strong generalization, outperforming baselines on three unseen tasks not used during training
- Multi-task training with five diverse engagement targets produces more comprehensive user representations than single-task approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning encourages the model to learn generalized user representations by leveraging diverse engagement signals.
- Mechanism: By predicting five different future user engagement targets simultaneously, the shared transformer encoder is forced to capture a wide range of user intents and behaviors, rather than specializing in just one task.
- Core assumption: Different user engagement tasks share underlying latent factors that can be jointly learned.
- Evidence anchors:
  - [abstract] "Employing a multi-task learning framework, TRACE captures comprehensive user preferences and intents distilled into low-dimensional representations."
  - [section] "The motivation behind the MTL approach is that by jointly predicting a diverse set of user engagement signals, the model is encouraged to learn comprehensive and generalizable representations that can be effectively utilized across a variety of downstream applications."
- Break condition: If the tasks are too dissimilar or uncorrelated, the shared representation may become diluted and lose task-specific discriminative power.

### Mechanism 2
- Claim: Event and session position encodings enable the model to capture temporal dynamics across multiple sessions.
- Mechanism: Learnable embeddings for both event position within a journey and session position provide the transformer with explicit information about where events occur, allowing it to model patterns within and across sessions.
- Core assumption: The position of an event within a multi-session journey contains meaningful information about user intent and behavior patterns.
- Evidence anchors:
  - [section] "We also enumerate the event position... and session position indexes are independently embedded... acting as an event-session position encoding. This was designed to allow the model to learn representations specific to session and position combinations, enabling it to capture dynamics both within and across multiple sessions more effectively."
  - [corpus] No direct evidence found in corpus papers about event-session position encoding for multi-session clickstream data.
- Break condition: If position information is noisy or irrelevant for certain domains, these encodings may add unnecessary complexity without performance gains.

### Mechanism 3
- Claim: The transformer architecture effectively captures long-range dependencies in multi-session clickstream sequences.
- Mechanism: Self-attention mechanisms in the transformer encoder can attend to any position in the input sequence, allowing it to model complex patterns across sessions that might be missed by sequential models like LSTMs.
- Core assumption: User journey patterns involve dependencies that span across sessions and cannot be adequately captured by fixed-window or sequential approaches.
- Evidence anchors:
  - [abstract] "TRACE leverages site-wide page view sequences spanning multiple user sessions to model long-term engagement."
  - [section] "We use a transformer encoder architecture to process the input sequences of pages, and train it in a multi-task regime across five different targets, representing a variety of future user engagement signals."
- Break condition: If sequences become extremely long, transformer computational complexity may become prohibitive, or if local sequential patterns are more important than long-range dependencies.

## Foundational Learning

- Concept: Multi-task learning and task relationship modeling
  - Why needed here: Understanding how to design tasks that share meaningful information and how joint training affects representation quality
  - Quick check question: What happens to model performance when tasks are too dissimilar or when class imbalance exists across tasks?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The model relies on transformer self-attention to capture complex patterns across multi-session clickstreams
  - Quick check question: How does the number of attention heads and layers affect the model's ability to capture session-level patterns?

- Concept: Position encoding techniques for sequence models
  - Why needed here: TRACE uses learnable event and session position encodings to capture temporal dynamics
  - Quick check question: What's the difference between learnable position encodings and fixed sinusoidal encodings, and when would each be preferable?

## Architecture Onboarding

- Component map: Input features → Feature engineering layer → Event/session position encodings → Transformer encoder (1 block, 8 heads) → Global max pooling → Shared dense layer → 5 task-specific dense layers → Multi-task loss
- Critical path: Feature engineering and position encoding → Transformer processing → Task-specific predictions
- Design tradeoffs: Single transformer block vs deeper architectures (simpler, faster, sufficient for data complexity); Learnable position encodings vs fixed encodings (adaptable to domain); Multi-task vs single-task training (generalization vs specialization)
- Failure signatures: Poor performance on all tasks suggests feature engineering issues; Good performance on some tasks but not others suggests task imbalance or model capacity issues; Slow inference suggests architectural inefficiencies
- First 3 experiments:
  1. Compare single-task vs multi-task training on a subset of tasks to verify MTL benefits
  2. Test with and without position encodings to validate their contribution to performance
  3. Compare transformer encoder depth (1-4 blocks) to find optimal balance between performance and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TRACE's performance compare to large language model-based approaches for user representation learning?
- Basis in paper: [inferred] The paper mentions LLMs as a future direction for exploration but does not compare TRACE to LLM-based methods.
- Why unresolved: The authors only compare TRACE to simpler transformer and LSTM-based models, leaving the potential advantages or disadvantages of LLM-based approaches unexplored.
- What evidence would resolve it: Direct comparison of TRACE against LLM-based user representation methods on the same dataset and evaluation metrics.

### Open Question 2
- Question: How do different session boundary definitions impact TRACE's performance?
- Basis in paper: [explicit] The paper defines sessions based on a fixed time interval T but does not explore how varying this parameter affects model performance.
- Why unresolved: The authors use a fixed session definition but do not investigate the sensitivity of their model to different session boundary definitions.
- What evidence would resolve it: Ablation studies varying the session boundary parameter T and measuring its impact on TRACE's performance.

### Open Question 3
- Question: How well does TRACE generalize to different e-commerce domains beyond travel?
- Basis in paper: [inferred] The paper demonstrates TRACE's effectiveness on travel e-commerce data but does not test its performance on other domains.
- Why unresolved: The authors only evaluate TRACE on travel data, leaving its applicability to other e-commerce domains unexplored.
- What evidence would resolve it: Applying TRACE to clickstream data from different e-commerce domains (e.g., retail, fashion) and comparing its performance to domain-specific baselines.

### Open Question 4
- Question: What is the optimal balance between model complexity and inference latency for real-time applications?
- Basis in paper: [explicit] The authors mention a self-imposed 100ms latency limit but do not explore the trade-off between model complexity and latency in detail.
- Why unresolved: While the authors demonstrate that their chosen model architecture meets latency requirements, they do not systematically investigate how different model complexities affect both performance and latency.
- What evidence would resolve it: Comprehensive benchmarking of TRACE variants with varying model complexities, measuring both performance and inference latency to identify the optimal balance for real-time applications.

## Limitations

- Limited real-world deployment validation: The paper demonstrates strong offline metrics but lacks evaluation of actual business impact in live production environments
- Narrow domain evaluation: TRACE is only tested on travel e-commerce data, leaving its generalizability to other domains unverified
- Incomplete latency analysis: While meeting a 100ms requirement, the paper doesn't systematically explore the performance-latency tradeoff across different model complexities

## Confidence

- **High confidence** in the transformer architecture's effectiveness for capturing multi-session dependencies, supported by consistent performance improvements over baseline models
- **Medium confidence** in the multi-task learning framework's benefits, as the paper shows gains but doesn't fully explore task relationships or potential negative transfer effects
- **Medium confidence** in the position encoding approach, since the paper demonstrates improvements but doesn't compare against alternative encoding strategies or conduct ablation studies on positional information importance

## Next Checks

1. Deploy the model in a live A/B test to measure actual business impact on key metrics like conversion rates, average order value, and user engagement time, comparing against the myopic baseline in real-world conditions

2. Conduct an ablation study specifically isolating the contribution of event and session position encodings by training variants without these features and measuring performance degradation on both seen and unseen tasks

3. Test model robustness to data distribution shifts by evaluating on clickstream data from different time periods, user segments, or product categories not represented in the training data to assess generalization limits