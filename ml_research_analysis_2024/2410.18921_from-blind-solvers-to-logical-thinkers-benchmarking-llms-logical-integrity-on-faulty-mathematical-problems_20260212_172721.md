---
ver: rpa2
title: 'From Blind Solvers to Logical Thinkers: Benchmarking LLMs'' Logical Integrity
  on Faulty Mathematical Problems'
arxiv_id: '2410.18921'
source_url: https://arxiv.org/abs/2410.18921
tags:
- math
- problem
- llms
- 'false'
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new dataset, FaultyMath, containing 363
  faulty mathematical problems across multiple categories, difficulty levels, and
  types of logical inconsistencies. The authors evaluate 10 LLMs on three tasks: detecting
  faulty problems without hints, adapting to correct or misleading hints, and generating
  explanations for identified flaws.'
---

# From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical Integrity on Faulty Mathematical Problems

## Quick Facts
- arXiv ID: 2410.18921
- Source URL: https://arxiv.org/abs/2410.18921
- Authors: A M Muntasir Rahman; Junyi Ye; Wei Yao; Sierra S. Liu; Jesse Yu; Jonathan Yu; Wenpeng Yin; Guiling Wang
- Reference count: 40
- Primary result: Current LLMs function as Blind Solvers, with even top models like Gemini 1.5 Pro only identifying 33.33% of faulty problems without hints

## Executive Summary
This paper introduces FaultyMath, a novel dataset of 363 faulty mathematical problems designed to test LLMs' ability to detect logical inconsistencies rather than simply solve problems. The authors evaluate 10 LLMs across three tasks: detecting faulty problems without hints, adapting to correct or misleading hints, and generating explanations for identified flaws. Results reveal that current LLMs predominantly function as Blind Solvers, exhibiting high confirmational bias when given hints and struggling to identify logical contradictions in mathematical problems.

The study makes a significant contribution by shifting the evaluation paradigm from problem-solving ability to logical integrity assessment. By demonstrating that mathematical reasoning skills don't necessarily translate to faulty problem detection, the authors highlight a critical gap in current LLM capabilities. The findings suggest that improving logical verification mechanisms, rather than just expanding mathematical knowledge, is essential for developing truly logical mathematical reasoning in LLMs.

## Method Summary
The authors developed a three-stage pipeline to create the FaultyMath dataset: (1) GPT-4 modified valid MATH problems into flawed versions, (2) GPT-4 performed self-evaluation to identify likely faulty problems, and (3) human annotators verified the remaining problems. They evaluated 10 LLMs using three dimensions: detection accuracy without hints, adaptation to hints (correct or misleading), and explanation quality for identified faulty problems. Auto-evaluation with GPT-4 as evaluator was used to scale the assessment process.

## Key Results
- Gemini 1.5 Pro achieved only 33.33% accuracy in identifying faulty problems without hints
- LLMs showed high confirmational bias, accepting both correct and misleading hints without independent verification
- When correctly identifying faulty problems, models like Gemini and Qwen-1.5 consistently provided accurate explanations
- Mathematical categories showed varying difficulty, with geometry problems being particularly challenging for faulty detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FaultyMath dataset construction leverages multi-stage filtering to increase the proportion of genuinely unsolvable problems while maintaining diversity across mathematical categories and difficulty levels.
- **Mechanism:** The dataset creation pipeline first uses GPT-4 to convert valid MATH problems into faulty versions with explanations, then applies self-filtering where GPT-4 attempts to solve the problem and identify its faultiness, and finally human annotators verify the remaining problems. This staged approach progressively eliminates solvable or ambiguous problems while preserving challenging faulty ones.
- **Core assumption:** GPT-4's self-evaluation capability is sufficiently reliable to identify solvable problems among faulty problem candidates, and human verification can effectively distinguish genuine faults from ambiguous cases.
- **Evidence anchors:**
  - [abstract] "We developed a three-stage pipeline: (i) We used GPT-4 to modify valid problems from the MATH dataset into flawed ones, generating 2,000 potential faulty math problems; (ii) we then prompted GPT-4 to perform self-evaluation, identifying 635 of these as likely faulty; (iii) finally, human annotators reviewed these problems, confirming 363 as genuinely faulty."
  - [section] "This approach significantly improved the likelihood of generating genuinely false math problems" and "Through this manual verification process, 363 problems were confirmed as genuine false math problems by both annotators."
  - [corpus] Weak - The corpus neighbors don't directly address multi-stage dataset filtering approaches.
- **Break condition:** If GPT-4's self-evaluation becomes unreliable or human annotators cannot consistently identify genuine faults, the quality of the dataset would degrade significantly.

### Mechanism 2
- **Claim:** LLMs exhibit confirmational bias when provided with hints about problem validity, accepting correct or misleading hints without performing independent logical verification.
- **Mechanism:** When hints are provided about whether problems are valid or faulty, LLMs tend to accept these hints at face value rather than conducting their own analysis. Correct hints improve performance, but misleading hints cause models to incorrectly classify faulty problems as valid, demonstrating a tendency to confirm existing information rather than challenge it.
- **Core assumption:** LLMs prioritize hint information over their own reasoning capabilities when both are available, indicating a fundamental limitation in independent logical verification.
- **Evidence anchors:**
  - [abstract] "While factually correct hints improve performance, models remain vulnerable to misleading hints, demonstrating confirmational bias."
  - [section] "The LLMs in this experiment exhibited high confirmational bias, where they agreed to the instruction easily, without rationalizing and reaching a different conclusion."
  - [case study] GPT-4 and Gemini-1.5-Pro both failed to identify the faulty problem when provided with misleading hints suggesting it was valid.
- **Break condition:** If models were explicitly trained to verify hints independently or if hint format was modified to encourage critical thinking, this confirmational bias might be reduced.

### Mechanism 3
- **Claim:** LLMs can provide accurate explanations when they correctly identify faulty problems, suggesting that their reasoning capabilities exist but are inconsistently triggered.
- **Mechanism:** When LLMs successfully identify a problem as faulty, they often provide correct and detailed explanations for why the problem is unsolvable. This indicates that the underlying reasoning mechanisms are present but not consistently activated across all faulty problem detection attempts.
- **Core assumption:** The ability to explain faulty problems correctly implies that the reasoning knowledge exists within the model, even when the detection capability fails.
- **Evidence anchors:**
  - [abstract] "How accurate are the explanations provided by LLMs when they correctly identify a faulty problem? Our findings reveal that models such as Gemini and Qwen-1.5 consistently produced correct explanations when they recognized falsehoods, highlighting their ability to articulate reasoning effectively in such cases."
  - [section] "Our findings revealed that both Gemini and Qwen1.5 provided accurate explanations for all 21 samples (21/21), while GPT-4 offered correct explanations for 20 out of 21 samples (20/21)."
  - [case study] When Gemini-1.5-Pro identified the faulty problem with true hints, it provided a correct explanation about why no consecutive even integers could satisfy the given conditions.
- **Break condition:** If the explanation quality deteriorates or becomes inconsistent even when problems are correctly identified, this would suggest the reasoning capability is not as robust as the data indicates.

## Foundational Learning

- **Concept:** Mathematical problem validity verification
  - Why needed here: LLMs need to distinguish between solvable and unsolvable problems, which requires understanding mathematical constraints and common sense violations that make problems impossible.
  - Quick check question: Given the problem "A square has a perimeter of 10 units. What is the length of one side?", can you identify if this is valid or if additional information is needed?

- **Concept:** Logical contradiction detection in mathematical contexts
  - Why needed here: Many faulty problems contain mathematical contradictions (like requiring even numbers that don't exist), and LLMs must detect these inconsistencies to function as Logical Thinkers rather than Blind Solvers.
  - Quick check question: Consider "The sum of two positive numbers is 10, and their product is 30. What are the numbers?" Is this problem valid or does it contain a mathematical contradiction?

- **Concept:** Pattern recognition across mathematical domains
  - Why needed here: FaultyMath includes problems from algebra, geometry, number theory, and other domains, requiring models to apply validity checking across different mathematical contexts and recognize domain-specific impossibility patterns.
  - Quick check question: For geometry problems, what type of information would make a triangle problem impossible to solve?

## Architecture Onboarding

- **Component map:** Dataset curation pipeline (GPT-4 problem generation → GPT-4 self-filtering → human verification) → LLM evaluation framework (prompt templates, auto-evaluation with GPT-4 as evaluator) → Performance metrics (accuracy, F1 score, category-based analysis) → Experimental design (no-hint, true-hint, misleading-hint conditions) → Result analysis tools (heatmaps, category breakdowns, explanation quality assessment)

- **Critical path:** Problem generation and filtering to create the dataset → Auto-evaluation prompt design and validation against human evaluation → LLM evaluation across all experimental conditions → Statistical analysis of results by difficulty, category, and fault type → Explanation quality assessment for correctly identified faulty problems

- **Design tradeoffs:**
  - Dataset size vs. quality: 363 high-quality faulty problems vs. larger but less reliable dataset
  - Auto-evaluation vs. human evaluation: scalable but potentially less accurate vs. accurate but labor-intensive
  - Hint conditions: revealing experimental design vs. naturalistic assessment of model capabilities
  - Mathematical categories: breadth of coverage vs. depth in specific areas

- **Failure signatures:**
  - Low accuracy across all conditions suggests fundamental limitations in logical reasoning
  - High performance with correct hints but poor performance with misleading hints indicates confirmational bias
  - Good explanations when problems are identified but poor identification rates suggests reasoning exists but is inconsistently applied
  - Category-specific failures may indicate domain knowledge gaps

- **First 3 experiments:**
  1. Replicate the auto-evaluation prompt with a different LLM as evaluator to test consistency of the evaluation framework
  2. Test a smaller subset of problems with human evaluation to validate auto-evaluation accuracy
  3. Conduct ablation study on the filtering pipeline to determine which stage contributes most to dataset quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design LLM architectures that better distinguish between valid and invalid mathematical problems without relying on external hints?
- Basis in paper: [explicit] The paper demonstrates that even top-performing models like Gemini 1.5 Pro only correctly identify 33.33% of faulty problems without hints, suggesting fundamental architectural limitations.
- Why unresolved: Current LLM architectures are primarily designed for pattern matching and solving valid problems rather than detecting logical inconsistencies. The paper shows that mathematical reasoning capabilities don't directly translate to identifying flawed problems.
- What evidence would resolve it: Development and evaluation of new LLM architectures specifically trained on faulty problem detection, with performance metrics showing significant improvement over current models on the FaultyMath dataset.

### Open Question 2
- Question: What is the relationship between an LLM's mathematical problem-solving ability and its capacity to identify faulty problems?
- Basis in paper: [explicit] The authors observe that strong performance on general math datasets (MATH) does not correlate with effectiveness in identifying faulty problems, suggesting these are distinct capabilities.
- Why unresolved: The paper provides preliminary evidence of a disconnect but doesn't establish the underlying reasons or mechanisms. It's unclear whether problem-solving skill actually hinders faulty problem detection.
- What evidence would resolve it: Systematic correlation studies between problem-solving accuracy and faulty problem detection across multiple models and datasets, potentially revealing whether these skills compete or complement each other.

### Open Question 3
- Question: How can we create more robust evaluation methods for faulty problem detection that account for LLM biases toward certain answer patterns?
- Basis in paper: [explicit] The authors demonstrate that LLMs exhibit confirmational bias when given misleading hints, and show vulnerability to prompt design that favors "Yes" or "No" answers.
- Why unresolved: Current evaluation methods may not adequately capture the complexity of faulty problem detection, as demonstrated by the significant differences in performance based on minor prompt variations.
- What evidence would resolve it: Development and validation of evaluation protocols that control for prompt bias and measure genuine faulty problem detection ability rather than pattern matching or prompt compliance.

## Limitations

- Dataset construction relies heavily on GPT-4's self-evaluation capability, which may introduce systematic biases
- Auto-evaluation using GPT-4 as evaluator raises concerns about circular evaluation methodology
- Human annotation process lacks detailed inter-annotator agreement metrics and specific verification criteria
- Limited testing of prompt engineering approaches to reduce confirmational bias

## Confidence

- Dataset construction methodology: Medium
- Confirmational bias findings: High
- Explanation quality assessment: Medium
- Generalizability across LLMs: Low

## Next Checks

1. Conduct a systematic comparison between auto-evaluation results and human evaluation on a stratified sample of 50-100 problems to quantify evaluation bias and establish confidence intervals for performance metrics.

2. Test the dataset construction pipeline with different LLMs (Claude, Llama) in place of GPT-4 for the self-filtering stage to assess robustness of the multi-stage filtering approach.

3. Design and test prompt modifications that explicitly encourage critical thinking and hint verification, measuring whether specific prompt engineering can reduce confirmational bias in model responses.