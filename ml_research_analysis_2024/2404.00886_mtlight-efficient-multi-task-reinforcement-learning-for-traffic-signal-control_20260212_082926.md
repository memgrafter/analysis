---
ver: rpa2
title: 'MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control'
arxiv_id: '2404.00886'
source_url: https://arxiv.org/abs/2404.00886
tags:
- traffic
- learning
- control
- state
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MTLight, a multi-task reinforcement learning
  framework for traffic signal control. It enhances the agent observation with a latent
  state learned from numerous traffic indicators.
---

# MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control

## Quick Facts
- **arXiv ID**: 2404.00886
- **Source URL**: https://arxiv.org/abs/2404.00886
- **Reference count**: 40
- **Primary result**: MTLight achieves leading convergence speed and asymptotic performance in multi-task traffic signal control, outperforming baselines like Individual RL, MetaLight, and PressLight with mean improvements of 693.46, 461.80, and 432.38 in average travel time reduction.

## Executive Summary
MTLight is a multi-task reinforcement learning framework for traffic signal control that enhances agent observations with latent states learned from numerous traffic indicators. The method constructs multiple auxiliary and supervisory tasks to learn task-specific and task-shared features, improving convergence speed and asymptotic performance. Experiments on CityFlow demonstrate MTLight's high adaptability under peak-hour patterns, achieving superior results compared to state-of-the-art baselines.

## Method Summary
MTLight proposes a multi-task reinforcement learning framework for traffic signal control that learns latent states from traffic indicators. The method uses a shared hidden representation learned from global state features, branching into four auxiliary predictors (flow distribution, travel time distribution, queue length, and vehicles on road). This shared representation is distilled into task-specific and task-shared latent features, which are concatenated with raw observations to form the enhanced state. The approach employs hard parameter sharing in the multi-task network, reducing overfitting and encouraging generalization across intersections.

## Key Results
- MTLight achieves leading convergence speed and asymptotic performance in traffic signal control experiments.
- Outperforms Individual RL, MetaLight, and PressLight with mean improvements of 693.46, 461.80, and 432.38 in average travel time reduction.
- Demonstrates high adaptability under peak-hour traffic patterns in CityFlow simulations.

## Why This Works (Mechanism)

### Mechanism 1
Multi-task learning of auxiliary tasks provides informative priors that accelerate policy convergence in dynamic multi-agent traffic control. The shared hidden representation learned from global state features branches into auxiliary predictors, distilled into task-shared and task-specific latent features. These features capture general traffic dynamics and intersection-specific policy-relevant information, respectively.

### Mechanism 2
Hard parameter sharing in the Multi-Task network reduces overfitting and encourages policy generalization across intersections. Early layers are shared across all auxiliary tasks, forcing the network to learn a common representation of traffic patterns while keeping task-specific output layers. This reduces total parameters compared to separate models, improving sample efficiency.

### Mechanism 3
Combining task-shared and task-specific latent states yields a richer representation than either alone. The ablation study shows each latent feature type alone improves over raw observation, but their combination achieves the best results. Task-shared states encode stable long-term traffic patterns, while task-specific states capture dynamic intersection-tailored policy signals.

## Foundational Learning

- **Markov Decision Process (MDP) formulation of traffic signal control**: Essential for understanding how each intersection is modeled as an RL agent in a joint MDP, with state transitions, rewards, and policies. *Quick check*: In this MDP, what constitutes the global state S versus the local observation o_i?

- **Multi-task learning and hard parameter sharing**: Critical for understanding MTLight's core innovation of sharing early layers across multiple auxiliary tasks. *Quick check*: What is the key difference between hard and soft parameter sharing in MTL?

- **Latent variable models and hierarchical feature extraction**: Important for understanding how the latent state is learned from a shared encoder and how priors are encoded. *Quick check*: In a single latent variable model, how are coarse- vs. fine-grained features typically separated?

## Architecture Onboarding

- **Component map**: Simulator (CityFlow) -> Multi-Task Network -> enhanced observation -> Policy Network (DQN) -> action -> Simulator
- **Critical path**: Simulator provides raw observations and rewards, Multi-Task Network learns latent states, Policy Network takes enhanced observation to produce Q-values and select actions, which are executed in the Simulator
- **Design tradeoffs**: Sharing parameters across tasks reduces overfitting but may limit specialization; two latent feature types increase representation richness but add complexity; queue length reward is simple but may not capture all objectives
- **Failure signatures**: Latent features are static (multi-task predictions fail to improve, policy reverts to raw observation performance); latent features are noisy (policy performance degrades, training instability); auxiliary tasks mismatch (auxiliary losses dominate, policy underfits)
- **First 3 experiments**: 1) Run MTLight on a small 4Ã—4 Hangzhou scenario with real traffic flow; compare convergence curves to Individual RL. 2) Enable only task-shared latent state (BASE + SHR) and observe if performance matches full MTLight. 3) Replace queue-length reward with negative travel time reward; check if auxiliary tasks still provide useful priors.

## Open Questions the Paper Calls Out

- How does the performance of MTLight vary with different numbers of auxiliary tasks beyond the four used in the paper?
- How does MTLight perform under different traffic flow patterns not covered in the experiments, such as those with sudden changes or disruptions?
- What is the impact of using different neural network architectures for the multi-task network on the performance of MTLight?

## Limitations
- The paper does not fully specify architectural details of the multi-task network (layer sizes, activation functions, output dimensions), affecting reproducibility.
- Effectiveness of task-shared and task-specific latent states is demonstrated through ablation studies, but scenarios where these might fail are not explored.
- Queue length as reward signal is a simplification that may not capture all traffic control objectives like emissions or pedestrian safety.

## Confidence
- **High**: The core mechanism of using multi-task learning with auxiliary tasks to learn informative latent states is well-supported by ablation study results.
- **Medium**: The claim that hard parameter sharing reduces overfitting and improves generalization is plausible but relies on the assumption that intersections share underlying traffic dynamics.
- **Low**: The assertion that combining task-shared and task-specific latent states yields superior performance is based on limited experimental evidence and may not hold in all traffic scenarios.

## Next Checks
1. **Latent feature stability**: Evaluate the stability of task-shared and task-specific latent states across different traffic patterns and intersection types. Check if latent features remain informative in highly heterogeneous scenarios.
2. **Auxiliary task sensitivity**: Test the robustness of MTLight by varying the choice and number of auxiliary tasks. Assess whether performance degrades when auxiliary tasks are poorly chosen or noisy.
3. **Reward signal generalization**: Replace queue length reward with alternative objectives (e.g., travel time or emissions) and evaluate if MTLight's performance remains consistent. This tests generalizability of learned latent states to different traffic control goals.