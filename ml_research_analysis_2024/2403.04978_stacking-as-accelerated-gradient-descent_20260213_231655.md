---
ver: rpa2
title: Stacking as Accelerated Gradient Descent
arxiv_id: '2403.04978'
source_url: https://arxiv.org/abs/2403.04978
tags:
- stacking
- training
- initialization
- deep
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical explanation for the effectiveness
  of stacking initialization in stagewise training of deep neural networks and boosting
  methods. The core idea is that stacking initialization enables accelerated gradient
  descent (AGD) in function space, as opposed to vanilla gradient descent without
  initialization or stochastic gradient descent with random initialization.
---

# Stacking as Accelerated Gradient Descent

## Quick Facts
- arXiv ID: 2403.04978
- Source URL: https://arxiv.org/abs/2403.04978
- Reference count: 11
- This paper provides a theoretical explanation for the effectiveness of stacking initialization in stagewise training of deep neural networks and boosting methods.

## Executive Summary
This paper provides a theoretical explanation for the effectiveness of stacking initialization in stagewise training of deep neural networks and boosting methods. The core idea is that stacking initialization enables accelerated gradient descent (AGD) in function space, as opposed to vanilla gradient descent without initialization or stochastic gradient descent with random initialization. The authors develop a general theoretical framework that unifies additive ensembles (boosting) and residual compositional models (deep residual networks). They prove that stacking initialization in additive models exactly recovers Nesterov's AGD, resulting in a convergence rate of O(T^-2) for smooth losses compared to O(T^-1) for zero initialization. For residual compositional models, stacking updates resemble AGD, and the authors prove accelerated convergence for deep linear networks by analyzing stacking as a perturbation of AGD with bounded errors. Experimental results on synthetic data and BERT base model validate the theoretical findings, showing that stacking initialization consistently outperforms zero and random initialization, especially for ill-conditioned problems.

## Method Summary
The paper analyzes stacking initialization as a form of functional gradient descent in two settings: additive ensembles (boosting) and residual compositional models (deep residual networks). For additive ensembles, stacking initialization exactly recovers Nesterov's accelerated gradient descent, proving O(T^-2) convergence for smooth, strongly convex losses. For residual compositional models, the authors prove accelerated convergence for deep linear networks by showing stacking updates resemble AGD despite differences in form. The analysis models early stopping as ℓ2 regularization and requires L-smooth, μ-strongly convex losses. The key insight is that initializing each new stage with β times the previous stage's function (f⁰_{t+1} = βf_t) introduces momentum-like behavior that accelerates convergence.

## Key Results
- Stacking initialization in additive models exactly recovers Nesterov's accelerated gradient descent with O(T^-2) convergence rate
- For deep linear residual networks, stacking updates resemble AGD and achieve accelerated convergence
- Experimental validation on synthetic data and BERT Base model shows stacking consistently outperforms zero and random initialization
- Stacking initialization is particularly effective for ill-conditioned problems where the condition number κ is large

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stacking initialization enables accelerated gradient descent by initializing each new layer with a scaled version of the previous layer's weights.
- Mechanism: When initializing a new layer with β times the previous layer's weights, the update step mimics Nesterov's accelerated gradient descent in function space, replacing the standard gradient descent step with a momentum-based step.
- Core assumption: The stacking initialization (f⁰_{t+1} = βf_t) exactly recovers Nesterov's accelerated functional gradient descent.
- Evidence anchors:
  - [abstract]: "stacking implements a form of Nesterov's accelerated gradient descent"
  - [section 2]: "the use of the popular stacking initialization exactly recovers Nesterov's accelerated functional gradient descent"
  - [corpus]: No direct evidence found in corpus; this is a novel theoretical contribution.
- Break condition: If β ≠ √κ-1/√κ+1 or if the loss function is not smooth and strongly convex, the accelerated convergence rate may not hold.

### Mechanism 2
- Claim: For deep linear residual networks, stacking updates resemble accelerated gradient descent despite differences in form.
- Mechanism: Even though stacking updates differ from Nesterov's method (using f_t ◦ F_t instead of f_t ◦ F_{t-1}), the similarity is sufficient to prove accelerated convergence by showing the perturbation is bounded.
- Core assumption: The perturbation term Δ_t in the stacking update is bounded relative to the previous layer's weight change.
- Evidence anchors:
  - [abstract]: "stacking updates resemble AGD, and the authors prove accelerated convergence for deep linear networks"
  - [section 3]: "we show that stacking initialization results in updates that look remarkably similar to Nesterov's accelerated functional gradient descent"
  - [corpus]: No direct evidence found in corpus; this is a novel theoretical contribution.
- Break condition: If the network weights diverge significantly from the optimal solution or if the perturbation term exceeds the bound, accelerated convergence may not hold.

### Mechanism 3
- Claim: Random initialization recovers stochastic functional gradient descent on a smoothed loss.
- Mechanism: When initializing with a random function independent of previous stages, the update step becomes equivalent to stochastic gradient descent on an expected loss where the randomness averages out.
- Core assumption: E[f⁰_{t+1}] = 0, where the zero function has zero mean under the data distribution.
- Evidence anchors:
  - [section 2]: "random initialization recovers stochastic functional gradient descent (on a smoothed loss) for both types of models"
  - [abstract]: "random initialization recovers stochastic functional gradient descent on a smoothed version of the loss"
  - [corpus]: No direct evidence found in corpus; this is a novel theoretical contribution.
- Break condition: If the random initialization is not zero-mean or if the smoothing effect is insufficient, the stochastic gradient descent equivalence may not hold.

## Foundational Learning

- Concept: Functional gradient descent
  - Why needed here: Understanding how gradient descent operates in function space is crucial for analyzing stacking as an optimization method.
  - Quick check question: What is the update rule for functional gradient descent in the additive ensemble setting?

- Concept: Nesterov's accelerated gradient descent
  - Why needed here: The paper's core contribution is showing stacking implements Nesterov's acceleration in function space.
  - Quick check question: How does Nesterov's update rule differ from standard gradient descent?

- Concept: Smoothness and strong convexity
  - Why needed here: These properties are required for the convergence rate analysis and to define the condition number κ.
  - Quick check question: What are the mathematical definitions of L-smoothness and μ-strong convexity for a function?

## Architecture Onboarding

- Component map:
  - Stacking initialization: f⁰_{t+1} = βf_t
  - Ensemble aggregation: Addition or residual composition
  - Loss function: L-smooth and μ-strongly convex
  - Optimization method: Functional gradient descent with early stopping modeled as ℓ2 regularization

- Critical path:
  1. Initialize first layer randomly or with zero weights
  2. Train for a few steps (early stopping)
  3. For each subsequent layer: initialize with β times previous layer, train for a few steps
  4. Continue until desired depth is reached

- Design tradeoffs:
  - Zero initialization: Simpler but slower convergence (O(T^-1))
  - Random initialization: Adds noise but may help escape local minima
  - Stacking initialization: Faster convergence (O(T^-2)) but requires careful β tuning

- Failure signatures:
  - Loss plateauing: May indicate β is too large or too small
  - Diverging weights: May indicate ill-conditioning or insufficient regularization
  - No improvement over zero initialization: May indicate loss function is not sufficiently smooth

- First 3 experiments:
  1. Compare stacking vs zero initialization on a simple additive model with known optimal solution
  2. Test different β values to find optimal momentum parameter
  3. Verify accelerated convergence rate empirically by plotting log(loss) vs log(stage number)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does stacking initialization provide accelerated convergence for deep non-linear networks?
- Basis in paper: [explicit] The paper proves accelerated convergence for deep linear networks but notes that proving this for general non-parametric functional settings seems intractable.
- Why unresolved: The authors acknowledge that extending the proof to non-linear networks would require additional assumptions or techniques beyond their current framework.
- What evidence would resolve it: Rigorous mathematical proofs or empirical demonstrations showing accelerated convergence rates for deep non-linear networks using stacking initialization.

### Open Question 2
- Question: What is the optimal initialization scheme that exactly recovers Nesterov's AGD updates for deep residual networks?
- Basis in paper: [explicit] The paper mentions that "a very intriguing and potentially impactful question is whether it is possible to come up with an efficiently implementable initialization scheme that leads to Nesterov's AGD updates exactly for deep residual networks."
- Why unresolved: While the paper shows that stacking initialization resembles AGD updates, it does not provide an exact implementation method that matches Nesterov's updates precisely.
- What evidence would resolve it: An algorithm that initializes new layers in a way that exactly produces Nesterov's AGD updates, validated both theoretically and empirically.

### Open Question 3
- Question: How does the β parameter affect stacking initialization in practice?
- Basis in paper: [explicit] The authors perform experiments with a trainable β parameter multiplying the output of newly added transformer blocks, finding it doesn't hurt efficacy and may improve final performance.
- Why unresolved: The experiments are limited to BERT Base, and the theoretical implications of β in general deep networks remain unexplored.
- What evidence would resolve it: Systematic experiments across various architectures and theoretical analysis of β's role in convergence rates and final performance.

### Open Question 4
- Question: Can stacking initialization be extended to other optimization problems beyond supervised learning?
- Basis in paper: [inferred] The paper focuses on supervised learning but presents stacking as a general framework that could potentially apply to other function optimization problems.
- Why unresolved: The authors do not explore applications beyond the supervised learning setting described in the paper.
- What evidence would resolve it: Proofs or empirical results demonstrating accelerated convergence for stacking in unsupervised learning, reinforcement learning, or other optimization domains.

## Limitations
- Theoretical analysis is limited to specific model classes (additive ensembles and deep linear residual networks)
- Extension to general deep nonlinear networks remains heuristic, though supported by BERT experiments
- Optimal β parameter depends on unknown condition number κ, which must be estimated in practice

## Confidence
- Mechanism 1 (Additive ensembles → Nesterov AGD): High - Formal proof provided with clear mathematical derivation
- Mechanism 2 (Residual networks → AGD-like updates): Medium - Proof for deep linear case; extension to nonlinear networks is heuristic
- Mechanism 3 (Random initialization → SFGD): High - Formal proof provided with clear mathematical derivation

## Next Checks
1. **Empirical validation of convergence rates**: Plot log(loss) vs log(stage number) for stacking vs zero initialization on synthetic data to verify the theoretical O(T^-2) vs O(T^-1) rates hold empirically.
2. **Sensitivity analysis for β parameter**: Systematically vary β around the theoretical value √κ-1/√κ+1 to quantify the impact of misestimation on convergence speed and stability.
3. **Extension to nonlinear activation functions**: Test stacking initialization on deep networks with ReLU or GELU activations to assess whether the AGD-like behavior extends beyond the linear case, measuring both convergence speed and final accuracy.