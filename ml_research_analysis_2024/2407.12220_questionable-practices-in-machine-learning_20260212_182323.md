---
ver: rpa2
title: Questionable practices in machine learning
arxiv_id: '2407.12220'
source_url: https://arxiv.org/abs/2407.12220
tags:
- arxiv
- data
- test
- https
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper catalogs 44 questionable research practices (QRPs) in
  machine learning evaluation, focusing on large language models (LLMs) and public
  benchmarks. QRPs include data contamination (e.g., training on test data), cherrypicking
  (selecting favorable results), misreporting (e.g., understating model size), and
  irreproducible research practices (e.g., hiding training data).
---

# Questionable practices in machine learning

## Quick Facts
- arXiv ID: 2407.12220
- Source URL: https://arxiv.org/abs/2407.12220
- Reference count: 40
- Key outcome: This paper catalogs 44 questionable research practices (QRPs) in machine learning evaluation, focusing on large language models (LLMs) and public benchmarks.

## Executive Summary
This paper identifies and categorizes 44 questionable research practices (QRPs) that undermine the validity of machine learning evaluation, particularly for large language models on public benchmarks. The authors classify these practices into contamination, cherrypicking, misreporting, and irreproducible research practices, providing concrete examples from the field. The work highlights the tension between scientific rigor and industrial incentives, emphasizing the need for transparency and reproducibility in ML research. While the paper catalogs these issues extensively, it acknowledges that actual prevalence and effectiveness of proposed defenses remain open questions.

## Method Summary
The paper employs a qualitative methodology, reviewing existing literature and providing concrete examples to catalog 44 questionable research practices in machine learning evaluation. It focuses on LLMs and public benchmarks, examining how researchers exploit flexibility in experimental design to obtain favorable results. The authors propose defenses including standardized evaluation harnesses, semantic decontamination of datasets, and preregistration of experiments. The work synthesizes observations from various sources to create a comprehensive taxonomy of QRPs, though it does not quantify their prevalence or empirically validate proposed mitigation strategies.

## Key Results
- ML researchers exploit researcher degrees of freedom to make methods appear superior through selective reporting
- Large language models can memorize test data, making contamination a severe threat to evaluation validity
- Selective reporting of results creates false impressions of method superiority
- Proposed defenses include standardized evaluation harnesses and semantic decontamination of datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Researchers exploit researcher degrees of freedom (RDOFs) to make their methods appear superior
- Mechanism: Multiple free choices in experimental design and analysis allow researchers to test many configurations and report only the most favorable results
- Core assumption: Researchers have strong incentives to publish SOTA results and weak disincentives for QRP
- Evidence anchors:
  - [abstract] "The strong incentive for researchers and companies to report a state-of-the-art result on some metric often leads to questionable research practices (QRPs)"
  - [section] "ML researchers run experiments to compare the effectiveness of methods. An evaluation usually has a main method...and a set of baselines. To publish usually requires that their method be statistically significantly better than the baselines."
  - [corpus] Weak - related papers don't discuss RDOFs directly
- Break condition: When researchers face strong reputational costs for QRP or when evaluation processes become standardized and automated

### Mechanism 2
- Claim: Large language models can memorize test data, making contamination a severe threat to evaluation validity
- Mechanism: High-capacity models exposed to test data during training can memorize exact or near-exact examples, inflating benchmark scores
- Core assumption: Modern LLMs have sufficient capacity to memorize arbitrary training examples
- Evidence anchors:
  - [abstract] "The strong incentive for researchers and companies to report a state-of-the-art result on some metric often leads to questionable research practices (QRPs)"
  - [section] "LLMs can memorise arbitrary pretraining examples (Ippolito et al., 2023) and are known to be exposed to test data during training (Brown et al., 2020; Achiam et al., 2023; Zhang et al., 2024)"
  - [corpus] Weak - related papers don't discuss LLM memorization
- Break condition: When evaluation datasets are rigorously decontaminated or when benchmarks use entirely new data

### Mechanism 3
- Claim: Selective reporting of results creates false impressions of method superiority
- Mechanism: Researchers run multiple experiments with different parameters, prompts, or benchmarks, then report only the most favorable outcomes
- Core assumption: The research community accepts selective reporting as normal practice
- Evidence anchors:
  - [abstract] "We describe 44 such practices which can undermine reported results"
  - [section] "Most ways to mislead others, or delude yourself, in ML evaluation fall into one of the following categories: 1. Contamination: 2. Cherrypicking: 3. Misreporting"
  - [corpus] Weak - related papers don't discuss selective reporting mechanisms
- Break condition: When preregistration becomes standard or when all experimental results are mandatorily disclosed

## Foundational Learning

- Concept: Researcher degrees of freedom (RDOFs)
  - Why needed here: Understanding RDOFs explains how researchers can exploit experimental flexibility to obtain favorable results
  - Quick check question: What is the difference between legitimate optimization of method hyperparameters and QRP cherrypicking?
- Concept: Contamination vs. memorization in LLMs
  - Why needed here: Distinguishing between actual generalization and memorized test data is crucial for interpreting benchmark results
  - Quick check question: How can a model appear to generalize when it has simply memorized training examples?
- Concept: Statistical significance and multiple comparisons
  - Why needed here: Understanding how p-hacking works requires knowledge of statistical testing and correction for multiple comparisons
  - Quick check question: Why does conducting multiple hypothesis tests without correction increase false positive rates?

## Architecture Onboarding

- Component map: Data collection → Training → Validation → Test → Reporting with decision points at each stage
- Critical path: Benchmark selection → contamination checking → baseline implementation → prompt optimization → metric selection → result reporting
- Design tradeoffs: Transparency vs. competitive advantage (dataset hiding), computational cost vs. robustness (multiple runs), standardization vs. flexibility (evaluation harnesses)
- Failure signatures: Suspiciously perfect results on known benchmarks, inconsistent results across different evaluation harnesses, missing implementation details
- First 3 experiments:
  1. Replicate a known benchmark score using the same evaluation harness and compare to reported results
  2. Test a model on both original and decontaminated versions of a benchmark to measure contamination effects
  3. Run multiple seeds of the same evaluation to measure variance and check if reported results fall within expected ranges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the actual prevalence of data contamination in large language models across major benchmarks?
- Basis in paper: [explicit] The paper discusses contamination as a major issue but notes "This paper answers the limited question 'what could make a model's reported performance to some extent spurious?'" and does not quantify actual prevalence
- Why unresolved: The paper catalogs QRPs and provides examples but does not systematically measure how widespread contamination is across the field
- What evidence would resolve it: Large-scale analysis of training corpora for test data overlap across multiple major LLM releases, using semantic decontamination techniques

### Open Question 2
- Question: How much do different QRPs affect benchmark scores in practice?
- Basis in paper: [explicit] The paper states "we do not claim that most performance is spurious" and does not quantify effect sizes of different QRPs
- Why unresolved: While the paper describes many QRPs, it does not measure their individual or combined impact on reported performance
- What evidence would resolve it: Controlled experiments measuring score changes when specific QRPs are applied versus when they are prevented

### Open Question 3
- Question: What is the optimal balance between transparency (preventing IRPs) and competitive advantage for AI companies?
- Basis in paper: [inferred] The paper discusses dataset hiding and other IRPs as being motivated by competitive advantage, but doesn't explore optimal disclosure policies
- Why unresolved: The paper identifies the tension between reproducibility and industrial incentives but doesn't propose or evaluate specific disclosure frameworks
- What evidence would resolve it: Empirical studies comparing innovation rates and scientific progress under different transparency regimes, or economic modeling of disclosure impacts

## Limitations

- The paper lacks quantitative data on the actual prevalence of QRPs in published research
- Effectiveness of proposed defenses like semantic decontamination has not been empirically validated
- The tension between scientific rigor and industrial competitiveness creates structural barriers to adopting recommended practices

## Confidence

- **High confidence** in the catalog of 44 specific QRPs and their classification into contamination, cherrypicking, and misreporting categories
- **Medium confidence** in the prevalence estimates and impact assessments of these practices across the ML research community
- **Low confidence** in the effectiveness of proposed mitigation strategies without empirical validation studies

## Next Checks

1. Conduct a systematic review of recent ML papers to quantify the frequency of different QRPs in practice
2. Implement controlled experiments comparing evaluation results using standard vs. decontaminated datasets to measure contamination effects
3. Survey ML researchers and reviewers about their awareness of QRPs and their willingness to adopt proposed defenses in their workflows