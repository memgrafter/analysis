---
ver: rpa2
title: Demystifying Verbatim Memorization in Large Language Models
arxiv_id: '2407.17817'
source_url: https://arxiv.org/abs/2407.17817
tags:
- verbatim
- memorized
- sequences
- memorization
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how large language models memorize training data
  verbatim. The authors introduce a controlled sequence injection framework that continues
  training from model checkpoints with novel strings inserted at controlled frequencies,
  enabling causal analysis of memorization.
---

# Demystifying Verbatim Memorization in Large Language Models

## Quick Facts
- arXiv ID: 2407.17817
- Source URL: https://arxiv.org/abs/2407.17817
- Reference count: 40
- This work studies how large language models memorize training data verbatim through controlled sequence injection and causal analysis.

## Executive Summary
This paper investigates verbatim memorization in large language models by introducing a controlled sequence injection framework. The authors continue training from model checkpoints with novel strings inserted at controlled frequencies, enabling causal analysis of memorization mechanisms. Key findings reveal that memorization is not a specialized module but emerges from general language modeling capabilities, relying on distributed abstract states rather than token-level information. The study also demonstrates that better language models memorize more, and that unlearning methods often fail while degrading overall model quality.

## Method Summary
The authors develop a sequence injection framework to study verbatim memorization by continuing pre-training from Pythia checkpoints with novel sequences inserted at controlled frequencies. They use interchange interventions to analyze causal dependencies between trigger representations and memorized tokens, and evaluate unlearning methods through stress tests with position and semantic perturbations. The framework enables controlled experimentation to identify which model components are reused during memorization and whether memorization is intertwined with general language modeling capabilities.

## Key Results
- Non-trivial repetition is required for verbatim memorization; very low frequency insertion doesn't produce memorization
- Later checkpoints memorize more, even for out-of-distribution sequences
- Memorization relies on distributed, abstract model states encoding high-level features rather than token-level information
- Unlearning methods often fail to remove memorized content and degrade model quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verbatim memorization is not a specialized memorization module but emerges from general language modeling capabilities distributed across multiple layers.
- Mechanism: Memorization uses high-level semantic features encoded in middle-layer representations, which are reused by the model during decoding. Tokens that depend on the trigger are reconstructed using these distributed features, while others follow standard LM patterns.
- Core assumption: The model encodes abstract states instead of token-level information, and these states can be activated by semantically similar inputs.
- Evidence anchors:
  - [abstract] "the generation of memorized sequences is triggered by distributed model states that encode high-level features"
  - [section] "Only some tokens in verbatim memorized sequences causally depend on a set of distributed triggering states that encode high-level semantic features, with the rest produced by regular LM decoding"
  - [corpus] Weak - corpus evidence primarily shows distributional effects rather than causal mechanisms
- Break condition: If interventions show that all tokens in a memorized sequence depend on a single localized representation, the distributed abstraction hypothesis fails.

### Mechanism 2
- Claim: Better language models (lower perplexity on training data