---
ver: rpa2
title: 'ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of
  Text Through Knowledge Graphs'
arxiv_id: '2403.09724'
source_url: https://arxiv.org/abs/2403.09724
tags:
- text
- claim
- triplets
- arxiv
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClaimVer is a human-centric framework for claim-level text verification
  and evidence attribution using knowledge graphs. It decomposes input text into individual
  claims, verifies each claim against Wikidata, and provides granular predictions
  (Attributable, Extrapolatory, or Contradictory) with supporting triplets and rationales.
---

# ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs

## Quick Facts
- **arXiv ID:** 2403.09724
- **Source URL:** https://arxiv.org/abs/2403.09724
- **Reference count:** 30
- **Primary result:** Achieves classification F1 scores above 89% for claim-level text verification using knowledge graphs

## Executive Summary
ClaimVer is a human-centric framework that performs claim-level text verification and evidence attribution using knowledge graphs. The system decomposes input text into individual claims, verifies each claim against Wikidata, and provides granular predictions with supporting evidence triplets and rationales. By fine-tuning multiple open-source LLMs on synthetic data, ClaimVer achieves high classification accuracy while maintaining explainability through clear, claim-specific attributions. The framework introduces a KG Attribution Score (KAS) that quantifies overall text validity, making it applicable across diverse domains and reducing user cognitive load.

## Method Summary
ClaimVer operates through a preprocessing pipeline that combines named entity recognition, coreference resolution, and KG entity linking to extract discrete claims from input text. It retrieves supporting evidence using a multi-node BFS algorithm to find relevant Wikidata triplets, then employs fine-tuned open-source LLMs to classify each claim as Attributable, Extrapolatory, or Contradictory. The framework generates explanations and calculates a KG Attribution Score using a modified sigmoid function that penalizes errors more heavily than rewards valid claims. Models are fine-tuned using LoRA on 4-bit quantized versions with a synthetic dataset generated by GPT-4.

## Key Results
- Achieves classification F1 scores above 89% across multiple open-source LLM models
- Introduces KG Attribution Score (KAS) for quantifying overall text validity
- Demonstrates effectiveness across diverse topics with clear, claim-specific explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ClaimVer can break down complex input text into individual claims for granular verification.
- **Mechanism:** Uses preprocessing pipeline combining NER, coreference resolution, and KG entity linking to identify and extract discrete claims.
- **Core assumption:** Claims are self-contained units that can be evaluated independently using KG triplets.
- **Evidence anchors:**
  - [abstract]: "Decomposes input text into individual claims, verifies each claim against Wikidata"
  - [section 3.3]: "First, decomposing the input text into claims"
  - [corpus]: Weak - no direct evidence of claim decomposition effectiveness
- **Break condition:** If preprocessing fails to correctly identify claim boundaries, the entire granularity advantage collapses.

### Mechanism 2
- **Claim:** Fine-tuned LLMs can operationalize the claim-level attribution objective function.
- **Mechanism:** Trains open-source models on synthetic dataset generated by GPT-4 to perform claim decomposition and attribution simultaneously.
- **Core assumption:** Small open-source models can approximate GPT-4 performance when fine-tuned on high-quality synthetic data.
- **Evidence anchors:**
  - [abstract]: "fine-tunes multiple open-source LLMs to operationalize this objective function"
  - [section 3.4]: "We selected eight open-source LLMs... The models were fine-tuned using LoRA"
  - [section 5]: "Solar-10.7B-Chat, with 1031 exact matches out of 1677 claims"
- **Break condition:** If synthetic dataset quality is insufficient, fine-tuned models will fail to generalize to real-world inputs.

### Mechanism 3
- **Claim:** KG Attribution Score (KAS) provides a continuous validity metric for downstream applications.
- **Mechanism:** Combines claim scores and triplet match scores through a modified sigmoid function that penalizes errors more heavily than rewards valid claims.
- **Core assumption:** A modified sigmoid with asymmetric penalty factor (γ=3 for negative scores) better reflects the relative importance of avoiding false claims versus confirming true claims.
- **Evidence anchors:**
  - [abstract]: "framework introduces an attribution score, enhancing applicability across a wide range of downstream tasks"
  - [section 3.5.3]: "We propose the KG Attribution Score (KAS), which accomplishes this task with a high level of granularity"
  - [corpus]: Weak - no direct evidence of KAS effectiveness in downstream tasks
- **Break condition:** If the penalty factor is set incorrectly, KAS will either be too lenient on false claims or too harsh on valid but partially supported claims.

## Foundational Learning

- **Concept: Knowledge Graph structure and triplet representation**
  - Why needed here: ClaimVer queries Wikidata using triplet format (subject, predicate, object) to find supporting evidence
  - Quick check question: Can you explain the difference between a node and an edge in a knowledge graph?

- **Concept: Natural Language Inference (NLI) task formulation**
  - Why needed here: Claim-level attribution is framed similarly to NLI, requiring models to determine if claims are supported, contradicted, or neither
  - Quick check question: What are the three standard NLI labels and how do they map to ClaimVer's attribution categories?

- **Concept: Named Entity Recognition (NER) for knowledge graph entities**
  - Why needed here: Preprocessing step identifies which text spans correspond to KG nodes for evidence retrieval
  - Quick check question: Why would you need a specialized NER model for Wiki entities rather than a general-purpose NER?

## Architecture Onboarding

- **Component map:** Preprocessing module (NER + coreference + entity linking) -> KG triplet retrieval engine (multi-node BFS algorithm) -> Fine-tuned LLM inference pipeline -> Attribution scoring module (claim scores + TMS + KAS computation) -> Output formatter (claim-level results with rationales)

- **Critical path:** Input text → Preprocessing → KG Triplet Retrieval → LLM Inference → Scoring → Output

- **Design tradeoffs:**
  - Granularity vs. computational cost (claim-level vs. sentence/paragraph)
  - Single vs. multiple LLM queries (batch processing vs. per-claim)
  - Open vs. specialized KG (Wikidata vs. domain-specific)

- **Failure signatures:**
  - Incorrect claim decomposition → Misaligned predictions and evidence
  - Poor triplet retrieval → High extrapolatory scores despite available evidence
  - LLM attribution errors → Contradictory claims marked as attributable

- **First 3 experiments:**
  1. Test preprocessing pipeline on diverse text samples to verify claim extraction quality
  2. Validate KG triplet retrieval with known entities to ensure correct paths are found
  3. Run LLM inference on synthetic dataset to confirm classification performance matches reported metrics

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of ClaimVer change when using specialized knowledge graphs (e.g., commonsense KG) instead of Wikidata for non-factoid claims?
- **Basis in paper:** [inferred] The authors mention that specialized KGs may be necessary to support a broader range of topics, particularly for non-factoid answers involving logic.
- **Why unresolved:** The paper primarily uses Wikidata and does not explore the impact of using different types of KGs on performance.
- **What evidence would resolve it:** Experiments comparing ClaimVer's performance using various specialized KGs versus Wikidata on diverse datasets containing non-factoid claims.

### Open Question 2
- **Question:** What is the impact of claim decomposition granularity on the accuracy and usability of ClaimVer?
- **Basis in paper:** [explicit] The authors acknowledge that decomposing text into multiple claims is subjective and complex, with multiple valid decompositions possible. They also note challenges in evaluating model performance due to this variability.
- **Why unresolved:** The paper does not systematically explore how different levels of decomposition granularity affect performance or user experience.
- **What evidence would resolve it:** Controlled experiments varying the level of claim decomposition granularity and measuring their impact on accuracy, computational efficiency, and user comprehension.

### Open Question 3
- **Question:** How effective are the proposed validation techniques in mitigating LLM reasoning errors in ClaimVer?
- **Basis in paper:** [explicit] The authors acknowledge LLM reasoning errors as a limitation and describe implementing membership checks and string matching to minimize mistakes, but note that validating reasoning remains an open problem.
- **Why unresolved:** The paper does not provide quantitative analysis of how effective these validation techniques are in practice.
- **What evidence would resolve it:** Detailed error analysis comparing ClaimVer's performance with and without the proposed validation techniques, including both quantitative metrics and qualitative case studies of error types.

## Limitations
- Claim decomposition quality is critical but not directly validated in the paper
- Synthetic dataset generation process and potential biases remain unclear
- KG Attribution Score's asymmetric penalty (γ=3) lacks empirical validation for optimality
- Woolnet multi-node BFS algorithm for triplet retrieval is mentioned but not fully specified

## Confidence
- **High Confidence:** The claim-level verification approach and KG triplet retrieval mechanism are well-supported by methodology and performance metrics
- **Medium Confidence:** The LLM fine-tuning process and synthetic dataset generation are described but lack detailed implementation specifications
- **Low Confidence:** The effectiveness of KAS in downstream applications and the optimal selection of penalty factor γ remain largely theoretical without empirical validation

## Next Checks
1. **Claim Decomposition Validation:** Test the preprocessing pipeline on a diverse set of texts to verify that claim boundaries are correctly identified and that the resulting claims are indeed self-contained and verifiable units.
2. **Triplet Retrieval Accuracy:** Conduct systematic validation of the Woolnet algorithm by testing known entity relationships to ensure the multi-hop BFS correctly retrieves relevant supporting evidence from Wikidata.
3. **KAS Penalty Factor Sensitivity:** Empirically test different values of γ (the penalty factor in KAS computation) to determine whether γ=3 is optimal or if alternative values better balance the tradeoff between penalizing false claims and recognizing valid ones.