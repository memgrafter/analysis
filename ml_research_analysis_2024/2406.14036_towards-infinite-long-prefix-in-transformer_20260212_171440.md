---
ver: rpa2
title: Towards Infinite-Long Prefix in Transformer
arxiv_id: '2406.14036'
source_url: https://arxiv.org/abs/2406.14036
tags:
- follows
- step
- definition
- prefix
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the scaling law in prefix learning and provides
  theoretical insights into why and how prefix learning operates. The authors analyze
  the optimization of ultra-long prefixes in a stylized attention network using the
  Neural Tangent Kernel (NTK) framework and show that when the prefix length is sufficiently
  large, the training can be analyzed via NTK, leading to a convergence guarantee.
---

# Towards Infinite-Long Prefix in Transformer

## Quick Facts
- arXiv ID: 2406.14036
- Source URL: https://arxiv.org/abs/2406.14036
- Authors: Yingyu Liang; Zhenmei Shi; Zhao Song; Chiwun Yang
- Reference count: 40
- This paper studies the scaling law in prefix learning and provides theoretical insights into why and how prefix learning operates, proposing NTK-Attention as an efficient approximation method.

## Executive Summary
This paper investigates prefix learning in transformers, providing theoretical insights into why and how it works through the lens of Neural Tangent Kernel (NTK) analysis. The authors show that when prefix length is sufficiently large, training can be analyzed via NTK, leading to convergence guarantees. Based on this theoretical framework, they propose NTK-Attention, which approximates prefix attention using two trainable parameters instead of a large prefix matrix, significantly reducing memory usage and computational cost while maintaining or improving performance.

## Method Summary
The paper introduces NTK-Attention, a method that replaces the large prefix matrix P with two trainable parameters Z and k using a polynomial method. The approach leverages the NTK framework to analyze ultra-long prefix learning, showing that when the prefix length is sufficiently large, the network becomes over-parameterized and training converges to small errors. The polynomial method approximates the exponential attention computation through a feature mapping, enabling efficient implementation while maintaining approximation quality within polynomial bounds.

## Key Results
- NTK-Attention achieves superior or competitive performance compared to full parameters fine-tuning, P-Tuning V2, and LoRA on vision, natural language, and math datasets
- The method significantly reduces memory usage from O(md) to O(rd + r) parameters and computational cost from O(mL + L²) to O(L²)
- Theoretical analysis shows loss decreases exponentially with computational cost, establishing a scaling law in prefix learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ultra-long prefixes enable strong generalization because the model becomes over-parameterized, allowing NTK-based convergence guarantees.
- Mechanism: When prefix length m is sufficiently large, the NTK matrix H(W) becomes close to a positive definite H* at initialization, and the loss landscape is nearly convex. Gradient descent can then converge to small errors with proper learning rate.
- Core assumption: The minimum eigenvalue λ of H* is positive, and m grows fast enough to ensure kernel convergence (R = λ/poly(n,d,exp(B))).
- Evidence anchors:
  - [abstract] "...when the prefix length is sufficiently large, the training can be analyzed via NTK, which leads to our theoretical guarantee of convergence to small errors."
  - [section 3.3] "After initializing the prefix matrix from a normal distribution, assuming the minimum eigenvalue of NTK λ > 0, setting m to be a large enough value so that the network is sufficiently over-parameterized. Then with proper learning rate, the loss can be minimized in finite training time to an arbitrarily small error ϵ."
  - [corpus] No direct citations; assumption supported only by theoretical NTK literature.
- Break condition: If λ ≤ 0 or m is not large enough, the kernel is not positive definite and the convergence guarantee fails.

### Mechanism 2
- Claim: NTK-Attention approximates prefix attention with polynomial-small error by using a low-rank polynomial mapping ϕ to replace the large prefix matrix.
- Mechanism: The prefix matrix P is decomposed into two trainable parameters Z and k via the polynomial method. ϕ maps query vectors into a higher-dimensional space such that exp(Q⊤K⊤/√d) ≈ ϕ(Q)⊤ϕ(K), allowing efficient approximation of the prefix attention without storing P.
- Core assumption: The polynomial mapping error is bounded by 1/poly(m), and input/query/key/value norms satisfy ∥·∥∞ ≤ o(√log m).
- Evidence anchors:
  - [section 4.1] "Therefore, checking the training process of P, we observe that P is updating iff Z and k are updating. Hence, we can replace P by utilizing trainable parameters Z and k in Eq. (7) to re-parameterize the prefix attention."
  - [section 4.3] "If the condition Eq. (7), ∥Q∥∞ ≤ o(√log m), ∥KC∥∞ ≤ o(√log m), ∥VC∥∞ ≤ o(√log m) and d = O(log m) holds, then Algorithm 2 outputs a matrix T ∈ RL×d within time complexity of O(L2d) that satisfies: ∥T − PrefixAttn(X, P)∥∞ ≤ 1/ poly(m)."
  - [corpus] No corpus citations; claim relies on polynomial method literature (AS23).
- Break condition: If input norms are too large, the Taylor approximation fails and error grows beyond polynomial bound.

### Mechanism 3
- Claim: The scaling law in prefix learning emerges from the relationship between computational cost and loss reduction, confirmed by NTK analysis.
- Mechanism: As m grows, the number of parameters N and training time T increase, leading to higher total compute cost Ccpt. Theorem 3.2 shows loss decreases exponentially with Ccpt, producing a scaling law: L ≈ α/[exp(ηλCcpt)]^(1/α).
- Core assumption: Training follows gradient descent with fixed η and the dataset size D is fixed; thus Ccpt ≈ NDT.
- Evidence anchors:
  - [section 3.3] "Proposition 3.3 shows that the training loss of the prefix learning converges exponentially as we increase the computational cost Ccpt, which primarily depends on the number of parameters and the training time in prefix learning, further indicating a possible relationship for formulating scaling law in prefix learning."
  - [section 1] "A rich line of studies... have reported a common observation that as the prefix length increases, the model's ability to master complex skills also improves."
  - [corpus] No direct corpus citations; relies on empirical scaling law literature (KMH+20).
- Break condition: If the learning rate is too large/small or the dataset size grows with m, the simple Ccpt formula breaks.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its use for analyzing over-parameterized networks.
  - Why needed here: NTK provides a framework to analyze the training dynamics of ultra-long prefix learning, which is impractical to train directly.
  - Quick check question: Why does NTK analysis become valid when m is large enough, and what property of H(W) ensures this?

- Concept: Polynomial method for attention approximation (Taylor expansion of exp).
  - Why needed here: NTK-Attention uses the polynomial method to replace the large prefix matrix with trainable parameters Z and k, enabling efficient approximation.
  - Quick check question: How does the polynomial method approximate exp(Q⊤K⊤/√d) and why does this reduce memory/time complexity?

- Concept: Gradient descent convergence analysis in the NTK regime.
  - Why needed here: To prove that prefix learning converges to small error, the paper analyzes the gradient flow of F(W, x, a) under NTK conditions.
  - Quick check question: What role does the minimum eigenvalue λ of H* play in ensuring convergence of gradient descent?

## Architecture Onboarding

- Component map:
  - Input: X ∈ RL×d
  - Frozen backbone: WQ, WK, WV ∈ Rd×d
  - Prefix learning: P ∈ Rm×d (standard) or Z ∈ Rr×d, k ∈ Rr (NTK-Attention)
  - Output: Attention matrix T ∈ RL×d

- Critical path:
  1. Compute Q = XW Q, K = XWK, V = XWV
  2. For NTK-Attention: Compute bA = exp(QK⊤/√d), bD = diag(bA1L + Φ(Q)k), T = bD−1(bAV + Φ(Q)Z)

- Design tradeoffs:
  - Memory: NTK-Attention reduces O(md) → O(rd + r) parameters
  - Time: Prefix attention O(mL + L²) → NTK-Attention O(L²)
  - Approximation quality vs. efficiency: Larger r improves accuracy but increases cost

- Failure signatures:
  - Gradient explosion if η is too large
  - Divergence if kernel H(W) is not positive definite (λ ≤ 0)
  - High approximation error if input norms violate ∥·∥∞ ≤ o(√log m)

- First 3 experiments:
  1. Verify NTK-Attention vs. prefix attention on random data: measure ∥T − PrefixAttn∥∞ for varying m, d, L
  2. Train NTK-Attention on CIFAR-100: compare accuracy to LoRA and full fine-tuning
  3. Test scaling law empirically: plot loss vs. compute cost (mdT) for different prefix lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prefix length required for LLMs to solve NP problems scale with problem complexity?
- Basis in paper: [inferred] The paper discusses scaling laws in prefix learning and mentions the theoretical possibility of solving any advanced reasoning skill through self-planning and prompting, but does not address the specific relationship between prefix length and problem complexity.
- Why unresolved: The paper focuses on proving convergence guarantees for ultra-long prefixes and introducing NTK-Attention as an approximation method, but does not explore the practical implications of prefix length scaling for different problem complexities.
- What evidence would resolve it: Empirical studies comparing prefix lengths required for LLMs to solve problems of varying complexity (e.g., P vs NP problems) would provide concrete data on the scaling relationship.

### Open Question 2
- Question: What is the relationship between prefix length and the complexity of problems that LLMs can solve?
- Basis in paper: [explicit] The paper mentions that "we don't know the answers" regarding the relationship between prefix length and problem complexity, and that explaining prefix learning, particularly Chain-of-Thought (CoT), is still a fascinating and challenging problem for future work.
- Why unresolved: While the paper provides theoretical insights into prefix learning and introduces NTK-Attention, it does not explore the practical implications of prefix length on the complexity of solvable problems.
- What evidence would resolve it: Experimental studies varying prefix lengths and measuring LLM performance on problems of different complexities would help establish a clear relationship between these factors.

### Open Question 3
- Question: How does NTK-Attention compare to LoRA in terms of parameter efficiency when both methods are re-parameterized for low-rank adaptation?
- Basis in paper: [explicit] The paper discusses the parameter-efficiency comparison with LoRA, stating that NTK-Attention seems less efficient initially but can be improved by re-parameterizing the matrix Z into two trainable matrices, potentially making it more efficient than LoRA.
- Why unresolved: The paper provides a theoretical analysis of NTK-Attention and mentions the possibility of improving its parameter efficiency, but does not present experimental results comparing the re-parameterized NTK-Attention to LoRA.
- What evidence would resolve it: Empirical studies comparing the performance and parameter efficiency of re-parameterized NTK-Attention and LoRA on various tasks would provide concrete data on their relative effectiveness.

## Limitations

- The theoretical convergence guarantees rely on idealized assumptions that may not hold in practical settings, including sufficiently large prefix length and positive minimum eigenvalue λ
- The polynomial approximation error bound depends on input norm constraints that are not empirically verified across different datasets and problem types
- The scaling law analysis assumes fixed dataset size, which may not reflect real-world scenarios where larger models are trained on larger datasets

## Confidence

**High confidence**: The memory and computational complexity improvements of NTK-Attention are mathematically well-founded and directly verifiable through implementation. The reduction from O(md) to O(rd + r) parameters is straightforward.

**Medium confidence**: The empirical performance claims are supported by experiments on multiple datasets, but the paper lacks ablation studies on critical hyperparameters like r (rank of Z) and comparisons with recent prompt tuning methods beyond LoRA.

**Low confidence**: The theoretical convergence guarantees in the NTK regime are mathematically sound but may not translate to practical settings where m is large but not infinite, and where real-world data distributions differ from theoretical assumptions.

## Next Checks

1. **Empirical verification of NTK assumptions**: Measure the minimum eigenvalue λ of H(W) during training on actual datasets to verify that it remains positive and that the kernel approximation error decreases as m increases.

2. **Scaling law validation**: Replicate the computational cost vs. loss reduction experiments across multiple model scales (small, medium, large transformers) to confirm the exponential relationship holds beyond the single experimental setup shown.

3. **Approximation quality analysis**: Systematically evaluate NTK-Attention's performance across different r values and input norm ranges to determine the practical limits of the polynomial approximation method and identify failure modes.