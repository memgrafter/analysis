---
ver: rpa2
title: Do LLMs write like humans? Variation in grammatical and rhetorical styles
arxiv_id: '2410.16107'
source_url: https://arxiv.org/abs/2410.16107
tags:
- llms
- llama
- text
- human
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examined whether large language models (LLMs) write like
  humans by comparing grammatical and rhetorical styles. Researchers used several
  LLM variants to generate text from prompts drawn from a large corpus, then analyzed
  the output using linguistic features identified by Douglas Biber.
---

# Do LLMs write like humans? Variation in grammatical and rhetorical styles

## Quick Facts
- arXiv ID: 2410.16107
- Source URL: https://arxiv.org/abs/2410.16107
- Reference count: 0
- The study found that instruction-tuned LLMs use certain grammatical features and vocabulary at significantly higher rates than human writing, making them more distinguishable from humans than base models.

## Executive Summary
This study investigates whether large language models (LLMs) write like humans by comparing grammatical and rhetorical styles. Researchers analyzed LLM-generated text from multiple model variants against human writing samples, using Douglas Biber's 66 linguistic features to measure stylistic differences. They discovered that instruction-tuned LLMs systematically overuse certain features and vocabulary, while base models more closely match human style patterns. Random forest classifiers achieved 66% accuracy in distinguishing between human and LLM text, with instruction-tuned models being easier to detect than base models.

## Method Summary
The researchers created parallel corpora by splitting human texts into chunks and using one chunk to prompt LLMs to generate matching-style text. They then extracted 66 linguistic features from Douglas Biber's tagset from both human and LLM texts. The analysis included calculating feature usage rates, performing statistical comparisons using Wilcoxon signed-rank tests with Bonferroni correction, and training random forest classifiers to distinguish between human and LLM sources. Vocabulary frequency analysis was also conducted to identify overused words in LLM outputs.

## Key Results
- Instruction-tuned LLMs use present participial clauses and nominalizations at 2-5 times the rate of human text
- GPT-4o and GPT-4o Mini overused specific vocabulary at 100+ times the human rate
- Random forest classifiers achieved 66% accuracy in distinguishing between human and LLM text overall, with 93-98% accuracy when comparing single LLM to human text
- Base models more closely match human stylistic patterns than instruction-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning introduces systematic stylistic shifts that make LLM output more distinguishable from human text.
- Mechanism: Base LLMs produce text at feature rates closer to humans; instruction tuning amplifies use of informationally dense features (e.g., nominalizations, present participial clauses) and specific vocabulary choices.
- Core assumption: Training data and fine-tuning objectives drive stylistic preferences that persist in generation.
- Evidence anchors:
  - [abstract] "differences persist when moving from smaller models to larger ones, and are larger for instruction-tuned models than base models"
  - [section] "the Llama 3 base models use these features at rates more closely matching humans... instruction tuning, rather than training the models to write even more like humans, instead trains them in a particular informationally dense, noun-heavy style"
  - [corpus] Limited corpus evidence; study compared parallel corpora but did not report feature distribution stability across different training datasets explicitly.
- Break condition: If new instruction-tuned models are trained on genre-diverse, human-mimicking objectives and evaluated across varied domains, the stylistic gap narrows or reverses.

### Mechanism 2
- Claim: Linguistic feature-based classifiers can reliably detect LLM-generated text with moderate accuracy.
- Mechanism: Random forest classifiers trained on Douglas Biber's 66 linguistic features achieve ~66% accuracy distinguishing humans from LLMs; higher accuracy when comparing single LLM to human text (~93–98%).
- Core assumption: LLM-generated texts exhibit measurable deviations in grammatical, lexical, and rhetorical features that are captured by Biber's tagset.
- Evidence anchors:
  - [abstract] "random forest classifiers achieved 66% accuracy in distinguishing between human and LLM text"
  - [section] "A random forest classifier using the Biber features to distinguish between the seven text sources in HAP-E... achieved a test accuracy of 66%"
  - [corpus] Study used HAP-E corpus (n=66,320 chunks) and CAP corpus (n=76,920 chunks) for training/testing, but no explicit cross-corpus stability metrics reported.
- Break condition: If LLMs are trained or fine-tuned explicitly to minimize detectable linguistic deviations, classifier accuracy drops below chance levels.

### Mechanism 3
- Claim: Feature usage rates and vocabulary choices differ markedly between base and instruction-tuned models.
- Mechanism: Instruction-tuned models overuse certain grammatical constructions (e.g., present participial clauses 2–5× human rate) and vocabulary items (e.g., "tapestry," "camaraderie" at >100× human rate), whereas base models align more closely with human distributions.
- Core assumption: The fine-tuning process biases models toward specific linguistic patterns independent of input prompt style.
- Evidence anchors:
  - [abstract] "GPT-4o and GPT-4o Mini also overused specific vocabulary at 100+ times the human rate"
  - [section] "instruction-tuned LLMs used present participial clauses at 2 to 5 times the rate of human text... GPT-4o uses present participial clauses at 5.3 times the rate of humans"
  - [corpus] HAP-E corpus used to measure feature rates, but no explicit report on whether corpus sampling or preprocessing introduced bias.
- Break condition: If new base models are trained on stylistically diverse data and evaluated without instruction tuning, vocabulary and construction overuse diminishes.

## Foundational Learning

- Concept: Douglas Biber's linguistic feature taxonomy (66 features spanning lexical, grammatical, and rhetorical categories).
  - Why needed here: Provides the quantitative framework to measure and compare stylistic variation between human and LLM text.
  - Quick check question: Which Biber feature categories would best capture genre-specific stylistic shifts in academic versus spoken language?

- Concept: Instruction tuning versus base model training.
  - Why needed here: Explains why stylistic differences exist between model variants and how fine-tuning objectives influence feature usage rates.
  - Quick check question: How might instruction tuning objectives bias a model toward noun-heavy, information-dense prose compared to base model completions?

- Concept: Random forest classification mechanics.
  - Why needed here: Underpins the method used to distinguish human from LLM text based on aggregated linguistic feature vectors.
  - Quick check question: What feature importance patterns would you expect if a classifier could not differentiate between human and LLM text?

## Architecture Onboarding

- Component map: Prompt generator → LLM inference (base/instruction-tuned variants) → Text chunk extraction → Feature extraction (Biber tagset) → Statistical analysis (Wilcoxon test, random forest) → Classification evaluation
- Critical path: Generate parallel human-LLM corpora → Extract linguistic features → Train classifier → Evaluate accuracy and feature importance
- Design tradeoffs: Use of fixed 500-word chunks simplifies alignment but may miss cross-chunk stylistic continuity; choice of Biber features provides linguistic depth but may not capture all stylistic nuances
- Failure signatures: Low classification accuracy could indicate either stylistic convergence or insufficient feature sensitivity; high accuracy on instruction-tuned models but low on base models suggests instruction tuning drives detectability
- First 3 experiments:
  1. Replicate Biber feature extraction on a small held-out validation set to verify feature stability
  2. Train a logistic regression classifier on the same feature set to compare with random forest performance
  3. Generate LLM outputs using varied prompt styles (formal vs. informal) to test prompt sensitivity of stylistic deviations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does instruction tuning alter writing style at the architectural or algorithmic level in LLMs?
- Basis in paper: [explicit] The authors note that instruction tuning "pushes models to produce text that reads unlike a human" and speculate this may be due to differences in human preferences in rating responses or task types used for tuning.
- Why unresolved: The paper cannot determine the exact cause because "the instruction tuning processes are not publicly documented."
- What evidence would resolve it: Access to detailed documentation or research on the instruction tuning process, including specific training data, human feedback mechanisms, and task diversity used across different LLM variants.

### Open Question 2
- Question: Do base model LLMs truly match human stylistic variation across all genres, or do they only appear to do so in specific contexts?
- Basis in paper: [inferred] The authors observe that base models use linguistic features at rates more closely matching humans than instruction-tuned models, but this comparison is limited to specific prompts and genres.
- Why unresolved: The study's controlled comparison using fixed prompts may not capture the full range of stylistic variation humans exhibit across diverse writing situations.
- What evidence would resolve it: Direct comparisons of base models and human writing across a broader range of naturally occurring genres and communicative contexts, without controlled prompts.

### Open Question 3
- Question: Can LLMs be fine-tuned to better match human genre conventions while maintaining their instruction-following capabilities?
- Basis in paper: [explicit] The authors conclude that instruction-tuned models produce "notable misalignment" with genre conventions and suggest that "linguistic expertise and functional conceptions of language" could improve LLM output.
- Why unresolved: The study identifies the problem but does not explore potential solutions or whether models can be trained to balance both capabilities.
- What evidence would resolve it: Experiments testing whether targeted fine-tuning on genre-specific corpora can reduce the stylistic discrepancies identified in this study without degrading task performance.

## Limitations
- The use of fixed 500-word chunks may introduce artificial stylistic artifacts not present in natural writing
- Analysis is limited to English language texts from specific corpora, limiting generalizability to other languages and domains
- The Biber feature set, while comprehensive, may not capture all dimensions of rhetorical style, particularly emergent patterns in digital communication

## Confidence
- **High Confidence**: Instruction-tuned models show systematically higher usage rates of specific grammatical features and vocabulary items compared to human writing; classification accuracy results are robust across different LLM variants
- **Medium Confidence**: Base models more closely match human stylistic patterns than instruction-tuned models; observed differences persist across model size variations
- **Low Confidence**: Claims about vocabulary overuse may be sensitive to corpus sampling methods; generalizability to other languages, genres, or future LLM versions remains uncertain

## Next Checks
1. **Cross-Corpus Validation**: Replicate the feature extraction and classification pipeline on an independent corpus of human and LLM texts from different domains (e.g., scientific writing, creative fiction) to test the robustness of observed stylistic differences.

2. **Temporal Stability Analysis**: Generate texts using multiple versions of the same LLM models over time to determine whether stylistic patterns are stable or evolve with model updates and fine-tuning changes.

3. **Human Judgment Validation**: Conduct a blind evaluation where human readers rate the naturalness and stylistic similarity of LLM-generated texts versus human writing, comparing these subjective assessments with the automated feature-based classifications.