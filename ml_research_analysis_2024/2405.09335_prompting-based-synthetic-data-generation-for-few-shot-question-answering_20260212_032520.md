---
ver: rpa2
title: Prompting-based Synthetic Data Generation for Few-Shot Question Answering
arxiv_id: '2405.09335'
source_url: https://arxiv.org/abs/2405.09335
tags:
- data
- question
- mrqa
- prompting
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a synthetic data generation method for few-shot
  Question Answering using large language models (LMs) and the Prompting framework.
  The core idea is to use the linguistic knowledge encoded in pre-trained LMs to generate
  synthetic question-answer pairs for the target domain, thereby reducing the need
  for expensive data annotation.
---

# Prompting-based Synthetic Data Generation for Few-Shot Question Answering

## Quick Facts
- arXiv ID: 2405.09335
- Source URL: https://arxiv.org/abs/2405.09335
- Authors: Maximilian Schmidt; Andrea Bartezzaghi; Ngoc Thang Vu
- Reference count: 0
- Achieves F1 score of 85.5% on SQuAD without labeled data

## Executive Summary
This paper presents a synthetic data generation method for few-shot Question Answering using large language models (LMs) and the prompting framework. The approach leverages linguistic knowledge encoded in pre-trained LMs to generate synthetic question-answer pairs for target domains, significantly reducing the need for expensive data annotation. The method demonstrates competitive performance across various datasets in few-shot settings, achieving strong F1 scores without any labeled data and showing good generalization to other domains.

## Method Summary
The method consists of two main steps: first, sampling answer candidates from context using Named Entity Recognition (NER); second, generating questions conditioned on the context and sampled answers using a pre-trained LM. The generated synthetic data is then used to train an MRQA model, which is subsequently fine-tuned on available labeled data. This two-stage approach effectively combines the strengths of pre-trained LMs with task-specific fine-tuning to achieve robust performance in low-resource scenarios.

## Key Results
- Achieves F1 score of 85.5% on SQuAD without any labeled data
- Outperforms many state-of-the-art approaches in few-shot settings across various datasets
- Demonstrates strong generalization to other domains through the few-shot MRQA benchmark
- User study shows generated question-answer pairs are of comparable quality to human-annotated data when 128 labeled samples are used

## Why This Works (Mechanism)
The method works by leveraging the rich linguistic knowledge already encoded in pre-trained language models, bypassing the need for extensive domain-specific training data. By using NER to identify potential answer candidates and then prompting the LM to generate questions conditioned on both context and these candidates, the approach creates realistic and contextually appropriate QA pairs. This synthetic data augmentation effectively simulates the distribution of human-annotated data, allowing models to learn robust question-answering patterns even with minimal labeled examples.

## Foundational Learning

**Named Entity Recognition (NER)**
- Why needed: Identifies potential answer candidates within context documents
- Quick check: Evaluate NER model's precision/recall on domain-specific text

**Prompting Framework**
- Why needed: Guides language models to generate contextually appropriate questions
- Quick check: Test different prompt templates on small validation set

**MRQA Model**
- Why needed: Provides base QA architecture for fine-tuning on synthetic and real data
- Quick check: Verify model can learn from synthetic data by training on small synthetic-only dataset

## Architecture Onboarding

**Component Map**
NER System -> Answer Candidate Extraction -> LM Prompting -> Question Generation -> MRQA Training -> Fine-tuning

**Critical Path**
Context + NER → Answer Candidates → LM Prompted Question Generation → Synthetic Data → MRQA Model Training → Fine-tuning on Limited Real Data

**Design Tradeoffs**
- NER precision vs recall tradeoff affects synthetic data quality
- LM generation parameters (temperature, max tokens) impact question diversity
- Balance between synthetic data quantity and quality

**Failure Signatures**
- Low-quality NER leads to poor answer candidates and irrelevant questions
- Over-reliance on synthetic data causes hallucination or pattern overfitting
- Prompting strategy too narrow limits question diversity

**3 First Experiments**
1. Test NER performance on target domain text to establish baseline answer candidate quality
2. Generate small synthetic dataset with different LM prompting strategies to evaluate question quality
3. Train MRQA model on synthetic data only to verify learning capability before fine-tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on NER systems may introduce domain-specific biases or miss valid answers
- Quality assessment limited to 128 labeled samples, raising robustness questions
- No detailed analysis of potential artifacts or hallucinations in generated questions
- Computational cost of generating large synthetic datasets not addressed

## Confidence

| Claim | Confidence |
|-------|------------|
| F1 score of 85.5% on SQuAD without labeled data | Medium |
| Outperforms state-of-the-art in few-shot settings | Medium |
| Generated data quality comparable to human-annotated data | Medium |

## Next Checks
1. Systematic evaluation of NER impact by testing with multiple NER models and analyzing effect on downstream QA performance
2. Comprehensive error analysis comparing model mistakes on synthetic vs human-generated training data to identify potential artifacts
3. Scalability testing to measure quality and diversity of generated data as synthetic dataset size increases, particularly for extremely low-resource scenarios (1-5 samples)