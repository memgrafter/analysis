---
ver: rpa2
title: Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for Fine-Tuning
  Large Language Models
arxiv_id: '2410.09823'
source_url: https://arxiv.org/abs/2410.09823
tags:
- lezo
- mezo
- fine-tuning
- sparse
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LeZO, a layer-wise sparse and efficient zeroth-order
  optimizer for fine-tuning large language models. The authors identify that perturbation
  and updating processes in the previous MeZO optimizer account for over 50% of the
  overall fine-tuning time cost.
---

# Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for Fine-Tuning Large Language Models

## Quick Facts
- arXiv ID: 2410.09823
- Source URL: https://arxiv.org/abs/2410.09823
- Authors: Fei Wang; Li Shen; Liang Ding; Chao Xue; Ye Liu; Changxing Ding
- Reference count: 40
- Primary result: LeZO achieves over 3× speedup compared to MeZO on SST-2, BoolQ, and Copa tasks while maintaining or improving performance

## Executive Summary
This paper introduces LeZO, a layer-wise sparse and efficient zeroth-order optimizer designed to address the high computational cost of MeZO, where perturbation and updating processes account for over 50% of fine-tuning time. LeZO achieves simultaneous computation and memory efficiency by dynamically selecting parameter subsets in each step, allowing full-parameter fine-tuning while accelerating computation. The approach incorporates layer-wise sparsity into SPSA and ZO-SGD without additional memory overhead, demonstrating significant speedups on OPT models across SuperGLUE benchmark and generative tasks.

## Method Summary
LeZO is a layer-wise sparse zeroth-order optimizer that improves upon MeZO by introducing dynamic parameter sparsity during optimization. The method partitions model parameters into layer units and randomly selects a subset of layers to perturb and update in each training step, while skipping these operations for untuned layers. This reduces floating-point operations per step without introducing extra memory overhead. The algorithm maintains full-parameter coverage over multiple steps through dynamic layer selection, ensuring all parameters are eventually updated. LeZO integrates seamlessly with parameter-efficient fine-tuning methods like LoRA and prefix tuning.

## Key Results
- Achieves over 3× speedup compared to MeZO on SST-2, BoolQ, and Copa tasks
- Maintains or improves performance while accelerating computation
- Better integration with parameter-efficient fine-tuning methods
- Improved convergence characteristics with layer-wise sparsity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-wise sparse perturbation reduces the number of parameters updated per step while maintaining convergence quality.
- Mechanism: LeZO dynamically selects a subset of layers in each optimization step to perturb and update, reducing floating-point operations. Different layer subsets are chosen over multiple steps, ensuring all parameters are eventually updated.
- Core assumption: The model's loss landscape allows partial parameter updates without degrading convergence.
- Evidence anchors: [abstract] "LeZO incorporates layer-wise parameter sparsity in the process of SPSA and ZO-SGD"; [section 4.1] "We use layers as the fundamental unit for sparsity."
- Break condition: If sparsity rate is too high, convergence slows or fails.

### Mechanism 2
- Claim: Skipping perturbation and update steps for untuned layers reduces computational overhead without extra memory usage.
- Mechanism: Only parameters in selected layers are perturbed and updated during each forward pass, allowing the algorithm to skip gradient estimation and parameter update operations for untuned layers.
- Core assumption: Perturbing only a subset of layers is sufficient to estimate the overall gradient direction.
- Evidence anchors: [section 3.2] "Simplifying these steps can accelerate MeZO"; [section 4.1] "Compared to MeZO, LeZO does not introduce extra memory overhead."
- Break condition: If untuned layers are structurally important for the current task, skipping their updates could degrade performance.

### Mechanism 3
- Claim: Dynamic layer selection across steps ensures full-parameter coverage while keeping per-step computation low.
- Mechanism: Random seed-based selection of different layer subsets in each optimization step ensures every layer is eventually perturbed and updated.
- Core assumption: Random layer selection provides sufficient coverage over the model's parameters to maintain convergence quality.
- Evidence anchors: [section 4.1] "By maintaining a subset a to store pruned layers, these layers are skipped during perturbation and updating processes"; [section 4.2, Lemma 3] "As the sparsity ratio ρ decreases, the upper bound on time required to converge also decreases."
- Break condition: If the number of steps is too low relative to the sparsity rate, some layers may not be updated often enough.

## Foundational Learning

- Concept: Zeroth-order optimization (ZO)
  - Why needed here: LeZO is a ZO optimizer; understanding how ZO works (gradient estimation via perturbations) is essential to grasp the mechanism.
  - Quick check question: How does a zeroth-order optimizer estimate gradients without backpropagation?

- Concept: Simultaneous Perturbation Stochastic Approximation (SPSA)
  - Why needed here: SPSA is the core gradient estimation technique used in LeZO; knowing its two-sided perturbation formula is key to understanding efficiency gains.
  - Quick check question: What is the formula for SPSA gradient estimation and how does it differ from finite differences?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: LeZO integrates with PEFT methods; understanding LoRA and prefix tuning helps explain how LeZO can be combined with these methods.
  - Quick check question: What is the difference between LoRA and prefix tuning in terms of trainable parameters?

## Architecture Onboarding

- Component map: Input parameters θ, loss function L, layer structure, sparsity rate ρ, random seed -> Layer-wise selection module -> Perturbation module -> Forward pass -> Gradient estimation (SPSA) -> Parameter update module -> Updated parameters θ

- Critical path:
  1. Select subset of layers using random seed and sparsity rate
  2. Perturb parameters in selected layers
  3. Forward pass to compute loss
  4. Estimate gradient using SPSA on perturbed parameters
  5. Update selected layer parameters

- Design tradeoffs:
  - Higher sparsity (fewer layers per step) → faster per-step computation but slower overall convergence
  - Lower sparsity → slower per-step computation but faster overall convergence
  - Fixed vs. adaptive layer selection: fixed is simpler, adaptive could be more efficient but adds complexity

- Failure signatures:
  - Training diverges: likely too high sparsity or poor learning rate
  - Convergence is extremely slow: sparsity rate may be too high or random selection not diverse
  - No memory savings: check if layer selection is actually being applied

- First 3 experiments:
  1. Compare training speed and convergence of LeZO vs. MeZO on a small model (e.g., OPT-1.3B) on SST-2
  2. Vary sparsity rate (e.g., 0.5, 0.75, 0.9) and measure impact on convergence speed and final accuracy
  3. Combine LeZO with LoRA and measure whether memory savings and speed gains are additive

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LeZO's layer-wise sparsity scheme perform on tasks with highly variable input lengths or token distributions compared to uniform tasks?
- Basis in paper: [inferred] The paper mentions that LeZO's computational speedup is influenced by the average input token length of different datasets.
- Why unresolved: The paper only provides a correlation between input token length and speedup without exploring how LeZO handles tasks with highly variable or skewed token distributions.
- What evidence would resolve it: Experiments comparing LeZO's performance and speedup on tasks with uniform vs. highly variable token distributions.

### Open Question 2
- Question: Can LeZO's layer-wise sparsity strategy be extended to more granular parameter-level or attention-head-level sparsity without introducing additional memory overhead?
- Basis in paper: [inferred] The paper discusses layer-wise sparsity as the basic unit, but doesn't explore finer-grained sparsity strategies.
- Why unresolved: The authors only consider layer-wise sparsity for computational efficiency and simplicity.
- What evidence would resolve it: Comparative experiments between layer-wise and attention-head-level or parameter-level sparsity approaches.

### Open Question 3
- Question: How does LeZO's performance scale with extremely large models (e.g., 100B+ parameters) compared to smaller models, and are there diminishing returns?
- Basis in paper: [explicit] The paper shows LeZO achieves more pronounced performance gains on larger LLMs but doesn't explore models beyond 30B parameters.
- Why unresolved: The authors only test up to OPT-30B and observe better performance on larger models.
- What evidence would resolve it: Experiments on models with 100B+ parameters comparing LeZO's relative performance and speedup to MeZO.

## Limitations

- Weak empirical evidence for the sparsity mechanism, with only 3.4× speedup on SST-2 well-documented
- No ablation studies showing how sparsity rate affects convergence or performance degradation
- Theoretical analysis of convergence bounds is not empirically validated across different sparsity rates

## Confidence

**High confidence**: The layer-wise sparsity mechanism is correctly described and the 3.4× speedup on SST-2 with 75% sparsity is well-documented. The integration with LoRA and prefix tuning is straightforward and the memory efficiency claims are supported by the algorithm design.

**Medium confidence**: The claims about improved performance on BoolQ and Copa tasks are supported but not as robustly as the SST-2 results. The theoretical analysis of convergence bounds is sound but not empirically validated across different sparsity rates.

**Low confidence**: The claim that "extensive experiments" demonstrate consistent performance across all tasks is not fully supported. The paper lacks ablation studies showing the impact of different sparsity rates on convergence speed and final performance.

## Next Checks

1. **Ablation study on sparsity rate**: Run experiments with different sparsity rates (e.g., 0.5, 0.75, 0.9) on SST-2 to measure the tradeoff between computation speed and convergence quality. Plot convergence curves for each sparsity rate.

2. **Layer coverage analysis**: Track which layers are selected for update across training steps to verify that dynamic selection provides sufficient coverage. Measure the variance in update frequency across layers and correlate with performance.

3. **Memory overhead validation**: Profile memory usage during training to confirm that LeZO introduces no additional memory overhead compared to MeZO. Measure memory consumption with and without sparsity applied.