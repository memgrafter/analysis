---
ver: rpa2
title: Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs
arxiv_id: '2405.11880'
source_url: https://arxiv.org/abs/2405.11880
tags:
- effects
- reasoning
- memorization
- interactions
- emily
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an axiomatic system to quantify and categorize
  memorization and in-context reasoning effects in large language models (LLMs). The
  method formulates these effects as non-linear interactions between tokens/words
  encoded by the LLM, and decomposes the model's confidence score into foundational
  memorization, chaotic memorization, and in-context reasoning effects.
---

# Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs

## Quick Facts
- **arXiv ID**: 2405.11880
- **Source URL**: https://arxiv.org/abs/2405.11880
- **Reference count**: 40
- **Key outcome**: Proposed axiomatic system quantifies and categorizes memorization and in-context reasoning effects in LLMs through non-linear token interactions, revealing that reasoning effects range from 39-45% while chaotic memorization is relatively weak at 7-8%.

## Executive Summary
This paper introduces an axiomatic framework to systematically quantify and disentangle memorization and in-context reasoning effects in large language models. The method decomposes LLM inference into foundational memorization, chaotic memorization, and in-context reasoning effects through mathematical analysis of token interactions. Experiments on multiple models and tasks reveal detailed inference patterns, showing that LLMs use many incorrect interactions despite producing seemingly reasonable outputs. The framework enables semantic debugging by identifying incorrect inference logic and quantifying the relative contributions of different reasoning mechanisms.

## Method Summary
The method formulates memorization and in-context reasoning effects as non-linear interactions between tokens encoded by LLMs, using an axiomatic system to decompose the model's confidence score. The framework extracts AND-OR interactions from LLM outputs and categorizes them based on condition dependence and variable independence axioms. It constructs logically equivalent prompts to isolate chaotic memorization effects, then quantifies foundational memorization, chaotic memorization, and in-context reasoning effects. The approach enables examination of detailed inference patterns across different model sizes and tasks.

## Key Results
- In-context reasoning effects constitute 39-45% of total inference effects, while chaotic memorization remains relatively weak at 7-8%
- Reasoning effects systematically eliminate high-order complex memorization patterns while enhancing simpler patterns
- Despite seemingly reasonable outputs, LLMs use many incorrect interaction patterns, with detailed analysis revealing specific inference failures
- The clear disentanglement enables identification of incorrect inference logic and semantic debugging of LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The axiomatic system enables decomposition of LLM inference into foundational memorization, chaotic memorization, and in-context reasoning effects through non-linear token interactions.
- Mechanism: The system defines AND-OR interactions between tokens and categorizes their effects based on axioms of condition dependence and variable independence, allowing precise quantification of memorization vs. reasoning contributions.
- Core assumption: LLMs encode inference logic as a combination of AND-OR interactions that can be mathematically decomposed and categorized according to their dependence on context.
- Evidence anchors:
  - [abstract]: "These effects are formulated as non-linear interactions between tokens/words encoded by the LLM"
  - [section 2.3]: "Iand(S|x) decompose = Jand(S|x) + Kand(S|x), Ior(S|x) decompose = Jor(S|x) + Kor(S|x)"
  - [corpus]: Weak evidence - related work focuses on benchmarking reasoning abilities but lacks direct mathematical decomposition frameworks
- Break condition: If the sparsity property doesn't hold (too many salient interactions) or if the universal matching property fails (interactions cannot approximate output), the decomposition becomes intractable.

### Mechanism 2
- Claim: In-context reasoning effects primarily eliminate high-order complex memorization patterns while enhancing simpler patterns, making inference more reliable.
- Mechanism: When premise information is added, reasoning effects systematically reduce the influence of complex, potentially incorrect memorization patterns (higher orders) while reinforcing simpler, more generalizable patterns.
- Core assumption: High-order interactions have lower generalization power than low-order interactions, so reasoning effects should target complex patterns for elimination.
- Evidence anchors:
  - [section 2.4]: "reasoning effects mainly eliminated some memorization effects, especially those corresponding to high-order interactions"
  - [section 2.4]: "the LLM uses the premise to filter out irrelevant memorization effects"
  - [corpus]: Moderate evidence - LogiDynamics paper investigates dynamics of different inference types, supporting the idea that reasoning can systematically modify memorization patterns
- Break condition: If reasoning effects don't show preferential elimination of high-order patterns, or if elimination is random rather than systematic, the claimed mechanism breaks down.

### Mechanism 3
- Claim: Chaotic memorization effects represent logically irrelevant patterns that vary with context changes, while foundational memorization remains stable across logically equivalent prompts.
- Mechanism: By constructing logically equivalent prompts that differ only in irrelevant details, the system can isolate and quantify chaotic memorization as the variation in interaction effects that should theoretically be invariant.
- Core assumption: Logically equivalent prompts should produce the same foundational memorization and reasoning effects, with only chaotic memorization varying based on irrelevant context changes.
- Evidence anchors:
  - [section 2.3]: "axioms to define and extract tens/hundreds of explicit interactions corresponding to context-agnostic memorization effect and interactions corresponding to in-context reasoning effects"
  - [section 2.3]: "chaotic memorization effect represents the interaction effects that are affected by the logical-irrelevant textual changes in two logically equivalent prompts"
  - [corpus]: Strong evidence - Detecting Data Contamination in LLMs paper uses similar in-context learning approaches to distinguish memorized vs. novel patterns
- Break condition: If foundational memorization shows variation across logically equivalent prompts, or if reasoning effects vary with irrelevant context changes, the separation between foundational and chaotic memorization fails.

## Foundational Learning

- Concept: AND-OR interaction decomposition
  - Why needed here: Forms the mathematical foundation for representing LLM inference as symbolic patterns that can be analyzed and categorized
  - Quick check question: Can you explain why v(x) = P I(S|x) for all possible interaction sets S, and what the sparsity property guarantees?

- Concept: Axiomatic system design
  - Why needed here: Provides the theoretical framework for distinguishing between memorization and reasoning effects based on their mathematical properties
  - Quick check question: What are the two core axioms (condition dependence and variable independence) and how do they enable categorization of interaction effects?

- Concept: Logically equivalent prompt construction
  - Why needed here: Enables isolation of chaotic memorization effects by creating controlled variations that shouldn't affect reasoning
  - Quick check question: What are the three types of logically equivalent prompts (irrelevant background, semantic paraphrasing, renaming) and why does each help isolate different aspects of memorization?

## Architecture Onboarding

- Component map: Interaction extraction module -> Axiomatic categorization system -> Logical equivalence generator -> Effect quantification engine -> Visualization and analysis layer
- Critical path: Prompt → LLM inference → Interaction extraction → Axiom application → Effect categorization → Quantification → Analysis
- Design tradeoffs:
  - Computational cost vs. granularity: Extracting all 2^n interactions is expensive; sparsity property allows focusing on salient ones
  - Prompt complexity vs. analysis accuracy: More logically equivalent prompts improve chaotic memorization isolation but increase computation
  - Token selection vs. completeness: Selecting key words enables tractable analysis but may miss important interactions
- Failure signatures:
  - Too many salient interactions (> O(n) per theory): Sparsity property violation, decomposition becomes intractable
  - Large approximation errors between LLM output and interaction-based reconstruction: Universal matching property failure
  - Chaotic memorization ratios remain high (>20%): Suggests LLM has poor generalization, fundamental representation issues
- First 3 experiments:
  1. Validate sparsity: Run interaction extraction on sample prompts, verify only ~O(n) interactions have significant effects
  2. Test logical equivalence: Create logically equivalent prompts, verify foundational memorization effects remain stable while chaotic effects vary
  3. Measure reasoning impact: Compare interaction patterns with and without premise, verify high-order patterns are eliminated while low-order patterns are enhanced

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of interactions to use for accurate approximation of LLM outputs across all masked samples?
- Basis in paper: [explicit] The paper discusses using top-ranked 50, 100, 150, and 200 interactions to measure matching errors, and shows that the confidence scores on all masked samples can be well approximated by the decomposed effects.
- Why unresolved: The paper does not provide a definitive answer on the optimal number of interactions, only showing that using top-ranked interactions can approximate the LLM's output. The optimal number may depend on the specific LLM and task.
- What evidence would resolve it: Experiments comparing the approximation errors using different numbers of interactions (e.g., 50, 100, 150, 200) on a variety of LLMs and tasks, to determine the point of diminishing returns in terms of accuracy vs. computational cost.

### Open Question 2
- Question: How do the ratios of in-context reasoning effects (ρr) and chaotic memorization effects (ρc) vary across different types of LLM tasks (e.g., question-answering, text generation, translation)?
- Basis in paper: [explicit] The paper reports the ratios ρr and ρc for three LLMs (OPT-1.3B, LLaMA-7B, GPT-3.5-Turbo) on a question-answer task, but does not explore how these ratios vary across different task types.
- Why unresolved: The paper only examines the ratios on a single task type (question-answering), so it is unclear how these ratios would generalize to other LLM tasks that may rely more or less on in-context reasoning and memorization.
- What evidence would resolve it: Experiments measuring the ratios ρr and ρc for the same LLMs on a diverse set of task types (e.g., question-answering, text generation, translation, summarization), to understand how the reliance on in-context reasoning and memorization varies by task.

### Open Question 3
- Question: Can the proposed axiomatic system be extended to quantify the exact reasoning and memorization effects used by LLMs for more complex reasoning tasks (e.g., multi-step reasoning, causal reasoning)?
- Basis in paper: [inferred] The paper focuses on quantifying in-context reasoning and memorization effects for relatively simple question-answering tasks, but does not explore more complex reasoning scenarios.
- Why unresolved: The paper's axiomatic system is designed to decompose LLM outputs into memorization and in-context reasoning effects, but it is unclear whether this approach can be extended to handle more complex reasoning patterns that may require multiple reasoning steps or causal inferences.
- What evidence would resolve it: Experiments applying the axiomatic system to more complex reasoning tasks (e.g., multi-step reasoning, causal reasoning) and evaluating whether the decomposed effects accurately capture the LLM's reasoning process in these scenarios.

## Limitations
- The framework relies heavily on the sparsity property assumption, which may not hold for more complex reasoning tasks or larger models
- The method's effectiveness depends on successful identification of logically equivalent prompts, but edge cases where such construction might be ambiguous are not fully addressed
- The paper doesn't fully explore how background knowledge affects logical equivalence or how the framework extends to multi-step reasoning scenarios

## Confidence
- **High confidence**: The core methodology for extracting AND-OR interactions and the basic framework for decomposing memorization vs. reasoning effects
- **Medium confidence**: The quantitative findings about effect ratios (39-45% reasoning, 7-8% chaotic memorization) and the three-way categorization of reasoning patterns
- **Medium confidence**: The claim that reasoning effects systematically eliminate high-order complex memorization patterns

## Next Checks
1. **Sparsity Property Validation**: Test the interaction extraction method on more complex reasoning tasks (beyond simple QA) to verify that the sparsity assumption holds and that only O(n) interactions remain significant.

2. **Logical Equivalence Robustness**: Systematically vary the construction of logically equivalent prompts across different domains and measure the stability of foundational memorization effects to test the separation between foundational and chaotic memorization.

3. **Cross-Model Generalization**: Apply the methodology to a wider range of model sizes (including larger models than tested) and architectures to verify that the observed effect ratios and patterns are consistent across the LLM landscape.