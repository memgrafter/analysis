---
ver: rpa2
title: '$\textit{BenchIE}^{FL}$ : A Manually Re-Annotated Fact-Based Open Information
  Extraction Benchmark'
arxiv_id: '2407.16860'
source_url: https://arxiv.org/abs/2407.16860
tags:
- benchie
- information
- annotation
- example
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses issues in BenchIE, the latest OIE benchmark,
  by creating BenchIEFL, a manually re-annotated corpus that enforces better annotation
  guidelines and introduces a more flexible matching function. The core method involves
  applying principles of informativity, minimality, exhaustivity, relation completeness,
  and inference to re-annotate 300 sentences, resulting in more precise and concise
  facts.
---

# $\textit{BenchIE}^{FL}$ : A Manually Re-Annotated Fact-Based Open Information Extraction Benchmark

## Quick Facts
- arXiv ID: 2407.16860
- Source URL: https://arxiv.org/abs/2407.16860
- Authors: Fabrice Lamarche; Philippe Langlais
- Reference count: 26
- Primary result: BenchIEFL achieves 0.997 correlation with manual rankings vs BenchIE's 0.961

## Executive Summary
BenchIEFL addresses limitations in the BenchIE OIE benchmark by creating a manually re-annotated corpus with stricter annotation guidelines and a more flexible matching function. The re-annotation process applies principles of informativity, minimality, exhaustivity, relation completeness, and inference to create cleaner, more precise reference clusters. A new matching function combines exact matching with heuristics for alternative formulations, level of detail, and punctuation handling. BenchIEFL demonstrates significantly higher correlation with downstream task performance, making it a more reliable benchmark for evaluating OIE systems.

## Method Summary
The method involves manually re-annotating 300 sentences from the BenchIE corpus using strict guidelines that enforce minimal, complete, and non-redundant facts. The re-annotation process identified and corrected five error types: missing facts, false facts, irrelevant facts, double annotations, and double meanings. A new matching function was developed that combines exact matching with heuristics for handling alternative formulations, level of detail differences, and punctuation variations. The benchmark was validated by comparing system rankings on BenchIEFL against their performance on three downstream tasks (ABQA, C-QA, KBP), showing improved correlation compared to BenchIE.

## Key Results
- BenchIEFL achieves 0.997 correlation with manual rankings vs BenchIE's 0.961
- Annotator validation found more than double the errors in BenchIE compared to BenchIEFL for most error types
- BenchIEFL contains 9400 extraction-annotation pairs with 96.8% having no associated cluster
- System rankings on BenchIEFL show significantly higher correlation with downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The new matching function increases recall by relaxing exact-match constraints while preserving precision.
- Mechanism: It introduces heuristics that detect semantically equivalent tuples that differ in optional words, word order, or punctuation, and match them to the same reference cluster.
- Core assumption: Most valid system outputs differ from reference tuples only in surface-level features (punctuation, optional words, or linearization) rather than core meaning.
- Evidence anchors:
  - [abstract]: "A new matching function combines exact matching with heuristics for alternative formulations, level of detail, and punctuation handling."
  - [section]: "The F1 score of matching function variants on BenchIEF L match is reported in Table 8. We observe that the exact match function is outperformed by each addition of the heuristics we described, and that using them all leads to the best performance."
  - [corpus]: "BenchIEF L match contains 9400 extraction-annotation pairs, 96.8% of which have no associated cluster." (implies many matches require heuristics)
- Break condition: If heuristics incorrectly match semantically distinct tuples, precision will drop and downstream task performance will degrade.

### Mechanism 2
- Claim: Re-annotation using strict principles (informativity, minimality, exhaustivity, relation completeness, inference) creates cleaner, more precise reference clusters.
- Mechanism: By enforcing that each cluster contain only one minimal fact and that relations be complete (not modified by arguments), the reference set becomes less ambiguous and more consistent.
- Core assumption: Previous annotations contained redundant, imprecise, or incomplete facts that confused matching and evaluation.
- Evidence anchors:
  - [section]: "By inspecting a (random) sample of 50 sentences of BenchIE... we noticed a number of issues... Five error types... Missing fact, False fact, Irrelevant fact, Double annotation, Double meaning."
  - [section]: "We observe that for all criterion, the annotators found our annotations to follow the guidelines much more closely. Both annotators find more than double the errors in BenchIE compared to our annotation for almost all error types."
  - [corpus]: "BenchIEF L Avg cluster/sentence 6 vs BenchIE Avg cluster/sentence 4.5" (more granular, minimal facts)
- Break condition: If strict principles lead to overly fragmented annotations that are harder to match or less useful for downstream tasks.

### Mechanism 3
- Claim: BenchIEF L's higher correlation with downstream task performance proves it is a better benchmark.
- Mechanism: By creating a more accurate evaluation of system output (better annotations + better matching), the benchmark ranking better reflects actual utility in downstream applications.
- Core assumption: Downstream task performance depends on the quality and informativeness of extracted facts, which BenchIEF L measures more accurately.
- Evidence anchors:
  - [abstract]: "BenchIEF L achieves a higher correlation (0.997) with manual rankings compared to BenchIE (0.961) and significantly improves alignment with downstream task performance."
  - [section]: "We compute Pearson product-moment correlation coefficients between these rankings and the ones obtained using OIE benchmarks and observe (see Table 10) that BenchIEF L has the highest correlation on these varied tasks."
  - [corpus]: "System ABQA C-QA KBP scores" (downstream task scores) correlate better with BenchIEF L than with other benchmarks.
- Break condition: If downstream tasks rely on factors not captured by BenchIEF L's principles, the correlation may not hold across all applications.

## Foundational Learning

- Concept: Open Information Extraction (OIE)
  - Why needed here: Understanding OIE is essential to grasp why benchmarks and matching functions matter.
  - Quick check question: What is the primary goal of OIE systems, and how does it differ from traditional relation extraction?

- Concept: Synsets/Clusters in OIE
  - Why needed here: BenchIE and BenchIEF L use clusters to group equivalent fact formulations; understanding this is key to the matching function.
  - Quick check question: Why do OIE benchmarks use clusters instead of single fact tuples, and how does this affect matching?

- Concept: Token-level vs Exact vs Custom Matching
  - Why needed here: Different matching functions have different trade-offs; understanding them is crucial for evaluating BenchIEF L's improvements.
  - Quick check question: What are the main differences between token-level, exact, and custom matching in OIE evaluation?

## Architecture Onboarding

- Component map: Sentences → Manual re-annotation (strict guidelines) → Fact clusters → Matching function (heuristics) → System scores → Downstream task correlation
- Critical path: Annotation → Matching → Downstream correlation (the chain that proves BenchIEF L is better)
- Design tradeoffs:
  - Granularity vs usability: More clusters (BenchIEF L) may be more precise but harder to annotate and match.
  - Strictness vs flexibility: Strict guidelines improve quality but may miss valid facts; heuristics improve recall but risk false matches.
- Failure signatures:
  - Low correlation with downstream tasks: Indicates benchmark is not measuring useful facts.
  - Precision drop in matching: Heuristics are matching wrong tuples.
  - High annotation disagreement: Guidelines are unclear or too strict.
- First 3 experiments:
  1. Run the new matching function on BenchIE's annotations and compare scores/rankings to BenchIE's original evaluation.
  2. Manually annotate a small sample of system outputs using BenchIEF L guidelines and test the matching heuristics for precision/recall.
  3. Run systems on a downstream task (e.g., ABQA) and correlate scores with BenchIEF L vs BenchIE rankings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BenchIEFL's annotation methodology compare to traditional OIE benchmarks in terms of inter-annotator agreement?
- Basis in paper: [explicit] The paper mentions that a single annotator created BenchIEFL and that validation was conducted by two other annotators who found more errors in BenchIE than in BenchIEFL.
- Why unresolved: The paper does not provide detailed statistics on inter-annotator agreement or the degree of consensus among annotators for BenchIEFL.
- What evidence would resolve it: Detailed inter-annotator agreement scores and a comparison with agreement scores from other benchmarks.

### Open Question 2
- Question: What are the specific linguistic features or characteristics of sentences that lead to higher rates of missing facts in OIE benchmarks like BenchIE?
- Basis in paper: [inferred] The paper notes that BenchIE had a high frequency of missing facts and attributes some of these to inference issues, suggesting certain sentence structures or linguistic features might be problematic.
- Why unresolved: The paper does not identify specific linguistic features that correlate with higher rates of missing facts.
- What evidence would resolve it: An analysis identifying specific linguistic features (e.g., sentence complexity, types of clauses) that correlate with higher rates of missing facts in OIE benchmarks.

### Open Question 3
- Question: How does the performance of neural OIE systems on BenchIEFL compare to their performance on other downstream tasks beyond the three mentioned in the paper?
- Basis in paper: [explicit] The paper evaluates neural systems on three downstream tasks and finds that BenchIEFL correlates better with these tasks than other benchmarks.
- Why unresolved: The paper only tests three downstream tasks, leaving open the question of how BenchIEFL performs across a broader range of applications.
- What evidence would resolve it: Performance data for neural OIE systems on a wider variety of downstream tasks, showing correlation with BenchIEFL scores.

## Limitations

- The exact implementation details of the matching heuristics remain unclear, particularly how alternative formulations and level of detail matching handle edge cases
- We cannot verify whether the heuristics might incorrectly match semantically distinct tuples, potentially lowering precision
- While the paper claims improved downstream task alignment, we lack full details on how downstream scoring was modified, making it difficult to assess whether performance differences are due to benchmark improvements or scoring changes

## Confidence

- High confidence: The mechanism of strict annotation guidelines improving reference quality is well-supported by error analysis and annotator agreement results
- Medium confidence: The matching function improvements are logically sound and tested, but exact heuristic implementations are unspecified
- Medium confidence: Downstream task correlation improvements are demonstrated but depend on scoring modifications we cannot fully verify

## Next Checks

1. Implement the matching heuristics independently and test on a small sample of system outputs to verify precision/recall trade-offs
2. Compare downstream task scores using both the original and modified scoring procedures to isolate benchmark effects
3. Conduct ablation studies removing individual heuristics to quantify their individual contributions to the matching function's performance