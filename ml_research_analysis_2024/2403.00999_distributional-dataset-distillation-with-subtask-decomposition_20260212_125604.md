---
ver: rpa2
title: Distributional Dataset Distillation with Subtask Decomposition
arxiv_id: '2403.00999'
source_url: https://arxiv.org/abs/2403.00999
tags:
- distillation
- distilled
- dataset
- training
- storage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new dataset distillation method called Distributional
  Dataset Distillation (D3), which encodes data using minimal sufficient per-class
  statistics and a decoder to achieve more memory-efficient representation compared
  to prototype-based methods. To scale the process, the authors propose federated
  distillation, which decomposes the dataset into subsets, distills them in parallel
  using sub-task experts, and then re-aggregates them.
---

# Distributional Dataset Distillation with Subtask Decomposition

## Quick Facts
- arXiv ID: 2403.00999
- Source URL: https://arxiv.org/abs/2403.00999
- Reference count: 40
- One-line primary result: Introduces Distributional Dataset Distillation (D3) with federated distillation, achieving state-of-the-art results on ImageNet-1K

## Executive Summary
This paper introduces Distributional Dataset Distillation (D3), a novel approach to dataset distillation that encodes data using minimal sufficient per-class statistics and a decoder, achieving more memory-efficient representation compared to prototype-based methods. The authors propose federated distillation to scale the process by decomposing the dataset into subsets, distilling them in parallel using sub-task experts, and then re-aggregating them. The method is evaluated on TinyImageNet and ImageNet-1K, achieving state-of-the-art results, outperforming prior art by 6.9% on ImageNet-1K under the storage budget of 2 images per class.

## Method Summary
The method combines distributional representation learning with federated distillation. It encodes data using Gaussian priors per class and pairs it with a decoder to create a compact distributional representation. To scale to large datasets, the method decomposes the dataset into subsets based on class labels, distills them in parallel using local experts, and re-aggregates them. The learning objective combines Matching Training Trajectories (MTT) and Maximum Mean Discrepancy (MMD) losses to ensure both training dynamics and feature distribution matching between original and distilled data.

## Key Results
- Achieves 48.3% top-1 accuracy on ImageNet-1K using only 2 synthetic images per class
- Outperforms state-of-the-art by 6.9% on ImageNet-1K under the same storage budget
- Demonstrates effective scaling to large datasets through federated distillation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The distributional representation with Gaussian priors is more memory-efficient than prototype-based methods.
- Mechanism: By modeling each class with one or multiple Gaussian distributions, the method stores only mean and covariance parameters per prototype, avoiding the need to store soft labels or augmentation parameters.
- Core assumption: The Gaussian distribution adequately captures the variation within each class for downstream training purposes.
- Evidence anchors:
  - [abstract]: "encodes the data using minimal sufficient per-class statistics and paired with a decoder, we distill dataset into a compact distributional representation"
  - [section]: "we argue that one can further achieve information compression by distilling into a latent distribution"

### Mechanism 2
- Claim: Federated distillation decomposes the dataset into subtasks, enabling parallel distillation and scaling to large datasets.
- Mechanism: By splitting the dataset into subsets based on class labels, each subset can be distilled independently using local experts, then reaggregated to form the full dataset.
- Core assumption: Data distilled on simpler subtasks generalizes well to the full task.
- Evidence anchors:
  - [abstract]: "decomposes the dataset into subsets, distills them in parallel using sub-task experts and then re-aggregates them"
  - [section]: "We then aggregate the locally distilled datasets to form the distilled data for the full task. We show that data distilled on subtasks generalize well to the full task"

### Mechanism 3
- Claim: The combination of Matching Training Trajectories (MTT) and Maximum Mean Discrepancy (MMD) objectives provides superior distillation quality.
- Mechanism: MTT ensures the distilled data follows similar training dynamics as the original data, while MMD ensures the feature distributions match, leading to better generalization.
- Core assumption: Both training dynamics and feature distribution matching are necessary for effective dataset distillation.
- Evidence anchors:
  - [section]: "Building on the foundations of existing data distillation techniques, we introduce a learning objective compromised of two distinct terms"
  - [section]: "we use a mixture of Radial Basis Function (RBF) kernels k(x, x′) =PK q=1 kσq(x, x′), where kσq represents a Gaussian kernel with bandwidth σq"

## Foundational Learning

- Concept: Deep Latent Variable Models (DLVMs)
  - Why needed here: The distributional representation is based on DLVMs, which combine variational inference with deep neural networks to map latent space to data space.
  - Quick check question: How do DLVMs enable flexible generation of data samples from learned distributions?

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD is used to measure the distance between feature distributions of the original and distilled data, ensuring distribution matching.
  - Quick check question: What is the role of the Reproducing Hilbert Kernel in MMD computation?

- Concept: Federated Learning
  - Why needed here: Federated distillation is used to parallelize the distillation process by decomposing the dataset into subsets.
  - Quick check question: How does federated learning ensure that models trained on subsets generalize to the full dataset?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Distillation network -> Distributional representation -> Decoder -> MTT and MMD objectives -> Aggregation

- Critical path:
  1. Preprocess data with mean-variance scaling
  2. Train experts on full dataset for MTT loss
  3. Initialize student network from expert trajectories
  4. Optimize distributional representation using MTT and MMD losses
  5. Aggregate distilled subsets from subtasks

- Design tradeoffs:
  - Storage vs. compute: More latent priors per class increase storage but improve accuracy
  - Decoder size: Larger decoders improve generation quality but increase storage cost
  - Number of subtasks: More subtasks enable better parallelization but may reduce generalization

- Failure signatures:
  - Poor generalization to unseen architectures (e.g., Vision Transformers)
  - High storage cost due to excessive latent priors or decoder size
  - Long downstream training time due to complex generation process

- First 3 experiments:
  1. Test distributional representation on CIFAR-10 with varying number of latent priors per class
  2. Evaluate federated distillation on TinyImageNet with different subtask sizes
  3. Compare storage and accuracy tradeoffs between prototype-based and distributional methods on ImageNet-1K

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of kernel bandwidths in the MMD loss impact the performance of D3, and what is the optimal selection strategy?
- Basis in paper: [explicit] The paper mentions using a mixture of 5 RBF kernels with bandwidths {1, 2, 4, 8, 16} for the MMD computation, inspired by MMD GANs.
- Why unresolved: The paper does not provide an ablation study or detailed analysis on the impact of different kernel bandwidth choices on the distillation performance.
- What evidence would resolve it: An ablation study comparing the performance of D3 using different sets of kernel bandwidths (e.g., {1, 2, 4, 8, 16} vs. {2, 4, 8, 16, 32}) on a benchmark dataset like TinyImageNet.

### Open Question 2
- Question: What is the impact of the number of subtasks in federated distillation on the scalability and performance of D3, and is there an optimal number of subtasks for a given dataset size?
- Basis in paper: [explicit] The paper mentions that increasing the number of subtasks has a negative impact on the performance of federated distillation, but does not provide a detailed analysis on the relationship between the number of subtasks and the scalability of the method.
- Why unresolved: The paper only experiments with three division sizes (10, 5, and 2 subtasks) on ImageNet-1K, without exploring a wider range of subtask numbers or providing a theoretical analysis on the trade-off between scalability and performance.
- What evidence would resolve it: An extensive ablation study on ImageNet-1K with a wide range of subtask numbers (e.g., 20, 15, 10, 8, 6, 4, 3, 2) and a theoretical analysis on the scalability and performance trade-off.

### Open Question 3
- Question: How does the choice of decoder architecture (e.g., depth, width, latent dimension) impact the performance of D3, and what is the optimal architecture for a given dataset and storage budget?
- Basis in paper: [explicit] The paper mentions using three sizes of decoders (S, M, L) with varying latent dimensions and total parameters, but does not provide a detailed analysis on the impact of decoder architecture on the distillation performance.
- Why unresolved: The paper only uses three fixed decoder architectures without exploring the design space or providing an ablation study on the impact of decoder depth, width, and latent dimension on the performance.
- What evidence would resolve it: An ablation study on TinyImageNet or ImageNet-1K comparing the performance of D3 using different decoder architectures (e.g., varying depth, width, and latent dimension) for a given storage budget.

## Limitations

- Scalability to extremely large datasets (e.g., JFT-300M or LAION-400M) remains unproven
- Performance on non-ImageNet-like distributions is not thoroughly explored
- Computational overhead of the MMD objective with RBF kernels could become prohibitive for higher-resolution datasets

## Confidence

- **High confidence**: The core mechanism of distributional representation with Gaussian priors is well-supported by the experimental results, showing consistent improvements over prototype-based methods.
- **Medium confidence**: The effectiveness of federated distillation for scaling to larger datasets is demonstrated on TinyImageNet but requires additional validation on more diverse and larger-scale datasets.
- **Low confidence**: The claim that the proposed evaluation protocols provide a more accurate characterization of distillation quality is not fully substantiated, as the paper does not compare against alternative evaluation metrics.

## Next Checks

1. Evaluate D3 on a dataset significantly larger than ImageNet-1K (e.g., JFT-300M) to assess the practical scalability of federated distillation.
2. Test the method on datasets with different characteristics from ImageNet (e.g., medical imaging or satellite imagery) to validate generalization across domains.
3. Measure the training time and memory usage of D3 compared to baseline methods, particularly focusing on the impact of the MMD objective with RBF kernels.