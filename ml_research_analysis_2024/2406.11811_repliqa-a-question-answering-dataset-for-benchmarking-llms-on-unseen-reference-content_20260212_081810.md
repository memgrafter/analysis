---
ver: rpa2
title: 'RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference
  Content'
arxiv_id: '2406.11811'
source_url: https://arxiv.org/abs/2406.11811
tags:
- repliqa
- content
- document
- answer
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REPLIQA, a new benchmark dataset for evaluating
  large language models on unseen reference content for question-answering and topic
  retrieval tasks. The dataset consists of 89,770 question-answer pairs based on 17,954
  reference documents, all created by human annotators to ensure the content is not
  present on the internet.
---

# RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content

## Quick Facts
- arXiv ID: 2406.11811
- Source URL: https://arxiv.org/abs/2406.11811
- Reference count: 40
- Primary result: Introduces REPLIQA, a benchmark dataset that evaluates LLMs on unseen reference content to distinguish between memorization and true reading comprehension

## Executive Summary
This paper introduces REPLIQA, a new benchmark dataset designed to evaluate large language models on unseen reference content for question-answering and topic retrieval tasks. The dataset consists of 89,770 question-answer pairs based on 17,954 reference documents, all created by human annotators to ensure the content is not present on the internet. The authors conduct a large-scale benchmark with 18 state-of-the-art LLMs, revealing that models tend to rely more on internal memory acquired during pre-training than on reference documents provided via prompting. The best-performing model on REPLIQA is MISTRAL LARGE, with a recall of 72.29%, but all models perform significantly worse than on TRIVIA QA, a well-known dataset with factual information.

## Method Summary
The REPLIQA dataset was created using a third-party vendor to generate synthetic reference documents depicting imaginary scenarios, with each document accompanied by five questions and corresponding answers. The dataset is released in staggered splits to minimize contamination risk, with the first split released in June 2024 and subsequent splits planned for every two months. The evaluation protocol involves providing models with reference documents and questions, measuring their ability to answer correctly using only the provided context versus their internal knowledge. The study benchmarks 18 state-of-the-art LLMs across question-answering and topic retrieval tasks using standard evaluation metrics.

## Key Results
- MISTRAL LARGE achieves the highest performance on REPLIQA with 72.29% recall
- All models perform significantly worse on REPLIQA compared to TRIVIA QA, suggesting reliance on memorization
- The staggered release schedule helps mitigate data contamination risks
- Models consistently fail to utilize provided reference documents effectively, instead defaulting to pre-trained knowledge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The dataset forces LLMs to rely on provided context instead of memorized facts.
- **Mechanism**: All reference documents are synthetic and never existed on the internet before the dataset was created, so models cannot use pre-trained knowledge to answer questions.
- **Core assumption**: The synthetic documents contain enough information to answer the questions if the model correctly reads and interprets the context.
- **Evidence anchors**:
  - [abstract]: "Each sample in REPLIQA comprises (1) a reference document crafted by a human annotator and depicting an imaginary scenario (e.g., a news article) absent from the internet"
  - [section]: "REPLIQA is designed to assess open-domain question answering based on reference documents and document topic retrieval"
  - [corpus]: Weak - the corpus only provides neighbor paper metadata, not direct evidence about the dataset's synthetic nature
- **Break condition**: If the synthetic documents accidentally contain information from real-world events or entities that were present in the training data, models could potentially answer using memorization.

### Mechanism 2
- **Claim**: The staggered release schedule delays potential data contamination.
- **Mechanism**: By releasing the dataset in five separate splits over time, the authors reduce the window during which the data could be scraped and used for model training.
- **Core assumption**: The gradual release provides enough time to evaluate models on clean data before any contamination occurs.
- **Evidence anchors**:
  - [abstract]: "we have opted for merely delaying the risk of leakage by staggered dataset releases"
  - [section]: "REPLIQA is composed of a total 89, 770 question-answer pairs based on 17, 954 reference documents"
  - [corpus]: Weak - corpus doesn't provide evidence about the release schedule strategy
- **Break condition**: If the first split (REPLIQA 0) was exposed to the web before the staggered release began, subsequent splits could still be contaminated.

### Mechanism 3
- **Claim**: The dataset reveals the true reading comprehension capabilities of LLMs by removing the advantage of memorization.
- **Mechanism**: By comparing performance on REPLIQA (unseen content) versus TriviaQA (potentially memorized content), the authors can quantify how much models rely on memory versus actual reading skills.
- **Core assumption**: The performance gap between the two datasets accurately reflects the difference between memorization and comprehension.
- **Evidence anchors**:
  - [abstract]: "models tend to rely more on internal memory acquired during pre-training than on reference documents provided via prompting"
  - [section]: "we show that models tend to rely more on internal memory acquired during pre-training than on reference documents provided via prompting"
  - [corpus]: Weak - corpus doesn't provide direct evidence about the comparison methodology
- **Break condition**: If the synthetic documents in REPLIQA are easier or harder than TriviaQA documents, the performance gap might reflect document difficulty rather than memorization vs comprehension.

## Foundational Learning

- **Concept**: Data contamination in LLM evaluation
  - **Why needed here**: Understanding how pre-training data overlap with test benchmarks can lead to misleading evaluation results
  - **Quick check question**: What is the primary risk when evaluating LLMs on datasets that might have been used during training?

- **Concept**: Question answering with context
  - **Why needed here**: The dataset evaluates models' ability to find answers within provided reference documents
  - **Quick check question**: How does the model determine whether to answer a question or output "UNANSWERABLE"?

- **Concept**: Text classification for topic retrieval
  - **Why needed here**: The dataset includes a task where models must identify document topics from a predefined list
  - **Quick check question**: What evaluation metric would be most appropriate for the topic retrieval task?

## Architecture Onboarding

- **Component map**: Synthetic reference documents -> Question generation (5 per document) -> Answer extraction from context -> Topic labeling (17 categories) -> Model evaluation with context provision -> Performance comparison against TriviaQA baseline
- **Critical path**: Create synthetic documents → Generate questions and answers → Release dataset in staggered splits → Evaluate models on question answering and topic retrieval → Compare performance with baseline datasets to assess memorization vs comprehension
- **Design tradeoffs**: The dataset prioritizes unseen content over natural language variety, which may make the synthetic documents feel less authentic. The staggered release reduces contamination risk but delays full dataset availability.
- **Failure signatures**: Poor performance on REPLIQA but good performance on TriviaQA suggests memorization. Conversely, good performance on both might indicate strong reading comprehension skills. Unexpected performance patterns could indicate issues with the synthetic document quality or question generation process.
- **First 3 experiments**:
  1. Run a subset of models on REPLIQA 0 without context to verify they cannot answer questions using only memorization
  2. Test the same models on TriviaQA without context to establish baseline memorization performance
  3. Compare model performance on both datasets with context to measure the impact of reading comprehension skills

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions beyond the general challenges of dataset contamination and evaluation methodology. The authors focus primarily on presenting their solution (REPLIQA) and demonstrating its effectiveness in distinguishing between memorization and comprehension.

## Limitations

- The synthetic nature of documents may make them feel less authentic than real-world content
- Staggered release strategy delays full dataset availability and may not completely prevent contamination
- Performance gaps between REPLIQA and TriviaQA could reflect document difficulty differences rather than pure memorization effects

## Confidence

- **REPLIQA dataset design and synthetic document creation**: High confidence - The methodology for creating unseen content is clearly specified and addresses the contamination problem directly.
- **Performance degradation on REPLIQA indicates memorization**: Medium confidence - While the evidence is strong, document difficulty differences and the artificial nature of synthetic content could partially explain the performance gap.
- **Staggered release effectively prevents contamination**: Medium confidence - The approach reduces risk but doesn't eliminate it, and the effectiveness depends on implementation details not fully specified in the abstract.

## Next Checks

1. Run ablation studies comparing model performance on synthetic vs natural documents of similar complexity to isolate the memorization effect from document difficulty confounds.

2. Test models on REPLIQA subsets with varying document types (e.g., news articles vs technical documents) to determine if performance degradation is consistent across content domains or specific to certain document styles.

3. Analyze model attention patterns during context reading to verify that poor performance stems from inadequate context utilization rather than other factors like token processing limitations or prompt adherence issues.