---
ver: rpa2
title: 'An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual
  Pixels'
arxiv_id: '2406.09415'
source_url: https://arxiv.org/abs/2406.09415
tags:
- locality
- pixels
- size
- image
- pixel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the necessity of the locality inductive
  bias in modern computer vision architectures. Through multiple case studies, the
  authors find that vanilla Transformers can achieve highly performant results by
  directly treating each individual pixel as a token, without assuming any spatial
  relationship in the architecture.
---

# An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels

## Quick Facts
- arXiv ID: 2406.09415
- Source URL: https://arxiv.org/abs/2406.09415
- Reference count: 23
- Primary result: Vanilla Transformers can achieve highly performant results by treating individual pixels as tokens, challenging the conventional belief that locality is a fundamental inductive bias for vision tasks.

## Executive Summary
This work investigates whether locality inductive bias is necessary in modern computer vision architectures. Through multiple case studies, the authors demonstrate that vanilla Transformers can achieve competitive performance by directly treating each individual pixel as a token, without assuming any spatial relationship in the architecture. This challenges the conventional wisdom that locality is a fundamental inductive bias for vision tasks. The authors evaluate this approach across three well-studied computer vision tasks: supervised learning for classification and regression, self-supervised learning via masked autoencoding, and image generation with diffusion models.

## Method Summary
The method replaces standard patch-based input with individual pixels, where each pixel is projected to an embedding space and combined with learned position embeddings. The architecture uses a standard Transformer encoder with self-attention and MLP layers, but operates on the much longer sequence of pixel tokens rather than patches. The approach is tested with four model sizes (T, S, B, L) across multiple datasets including CIFAR-100, ImageNet, Oxford-102-Flower, NYU-v2, and VQGAN latent tokens for image generation.

## Key Results
- Pixel-based Transformers achieve competitive performance on classification, depth estimation, and image generation tasks without locality bias
- Decreasing patch size (increasing sequence length) consistently improves performance across tasks
- Learned position embeddings can effectively replace fixed positional encodings, and even removing position embeddings entirely only causes minor performance degradation

## Why This Works (Mechanism)

### Mechanism 1
Direct pixel-level attention enables global context without local inductive bias by capturing global dependencies while remaining permutation-equivariant at the pixel level. The model learns spatial relationships directly from data rather than through predefined locality assumptions. This works under the assumption that quadratic complexity can be managed at pixel resolution and learned position embeddings are sufficient to encode spatial structure.

### Mechanism 2
Increased resolution and sequence length provide more information than locality bias. When input size is fixed, decreasing patch size consistently improves performance, suggesting that more detailed input information outweighs the benefits of locality bias. This assumes that additional information from higher resolution inputs compensates for the loss of explicit locality encoding.

### Mechanism 3
Transformers can learn shape-based representations more effectively than texture-based ones when locality bias is removed. The pixel-based transformer shows stronger shape bias compared to standard ViT, suggesting that removing locality allows the model to focus on structural rather than textural features. This assumes that shape-based representations are more robust and generalizable than texture-based ones for object recognition.

## Foundational Learning

- **Self-attention mechanism and computational complexity**: Understanding why pixel-level processing is computationally expensive (quadratic complexity) and how it affects feasibility. Quick check: What is the computational complexity of self-attention, and how does it scale with sequence length?

- **Position embeddings and their role**: Understanding how learned position embeddings can replace fixed positional encodings and encode spatial structure without locality bias. Quick check: How do learned position embeddings differ from fixed positional encodings like sin-cos, and what are their advantages/disadvantages?

- **Inductive biases in neural networks**: Understanding the concept of inductive biases and why removing locality bias is significant for vision architectures. Quick check: What are inductive biases, and why are they important in neural network design?

## Architecture Onboarding

- **Component map**: Image preprocessing -> Pixel projection to embedding space -> Addition of learned position embeddings -> Transformer processing -> Task-specific output generation

- **Critical path**: 1) Image preprocessing and resizing, 2) Pixel projection to embedding space, 3) Addition of learned position embeddings, 4) Transformer processing, 5) Task-specific output generation

- **Design tradeoffs**: Resolution vs. computational cost (higher resolution provides more information but increases computational complexity), Learned vs. fixed position embeddings (learned provide flexibility but may require more data), Pixel-level vs. patch-level processing (pixel-level provides more detail but is computationally expensive)

- **Failure signatures**: Training instability with high learning rates, Poor performance on texture-heavy tasks, Memory issues with high-resolution images, Difficulty converging without sufficient data

- **First 3 experiments**: 1) Implement basic pixel-based transformer on CIFAR-100 with learned position embeddings, 2) Compare performance with patch-based ViT at different resolutions, 3) Test the effect of removing position embeddings entirely

## Open Questions the Paper Calls Out

1. Does the elimination of locality bias in vision transformers fundamentally improve model generalization across diverse tasks beyond those tested in this paper? The paper only tests three specific tasks and does not explore how these findings might generalize to other vision tasks like object detection, segmentation, or video understanding.

2. What is the theoretical explanation for why transformers can effectively learn spatial relationships from individual pixels without explicit locality bias? The paper presents empirical results showing effectiveness but does not investigate the underlying mechanisms that enable transformers to capture spatial structure without locality priors.

3. How does the performance trade-off between efficiency and accuracy scale as we move from extreme locality-free approaches to partially locality-aware architectures? The paper establishes that locality-free transformers work well but does not systematically explore the continuum between fully locality-free and fully locality-biased approaches to understand where the optimal balance lies.

## Limitations
- Computational feasibility is severely limited by quadratic self-attention complexity at pixel resolution, making the approach impractical for real-world applications
- Position embedding reliability has not been thoroughly validated across diverse vision tasks and more complex datasets
- Generalizability of findings is limited to classification and generation tasks, with unexplored performance on detection, segmentation, and video processing

## Confidence

**High Confidence**: The core finding that pixel-based transformers can achieve competitive performance on standard vision benchmarks is well-supported by experimental results.

**Medium Confidence**: The claim that learned position embeddings can effectively replace fixed positional encodings is supported by results, but generalizability to more complex tasks and larger datasets is uncertain.

**Low Confidence**: The broader implications for next-generation vision architectures and suggestions that locality bias may not be necessary are speculative due to computational impracticality and limited evidence.

## Next Checks

1. Test the pixel-based transformer approach on higher-resolution images (e.g., 512×512 or 1024×1024) to evaluate computational feasibility and performance degradation as image size increases.

2. Apply the pixel-based transformer to vision tasks beyond classification and generation, such as object detection, semantic segmentation, or video processing, to validate generalizability across the broader vision task spectrum.

3. Investigate the performance of pixel-based transformers when trained on limited data compared to patch-based approaches to address whether learned position embeddings and global attention mechanisms require more data to achieve comparable performance.