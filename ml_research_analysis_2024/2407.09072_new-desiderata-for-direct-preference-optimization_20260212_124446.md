---
ver: rpa2
title: New Desiderata for Direct Preference Optimization
arxiv_id: '2407.09072'
source_url: https://arxiv.org/abs/2407.09072
tags:
- preference
- loss
- policy
- arxiv
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces new desiderata for direct preference optimization\
  \ (DPO) methods, identifying limitations in existing approaches like DPO, IPO, and\
  \ f-DPO regarding selective policy preservation, interpolation behavior, and constraint\
  \ effects. The authors propose a new loss function, \u2113TYPO, that satisfies these\
  \ desiderata by combining supervised and unsupervised terms."
---

# New Desiderata for Direct Preference Optimization

## Quick Facts
- arXiv ID: 2407.09072
- Source URL: https://arxiv.org/abs/2407.09072
- Reference count: 40
- This paper introduces new desiderata for direct preference optimization (DPO) methods, identifying limitations in existing approaches like DPO, IPO, and f-DPO regarding selective policy preservation, interpolation behavior, and constraint effects. The authors propose a new loss function, ℓTYPO, that satisfies these desiderata by combining supervised and unsupervised terms. ℓTYPO achieves smooth interpolation between reference models and optimal policies while preserving performance in regions where the reference model excels. Empirical validation on synthetic and real-world data demonstrates ℓTYPO's superiority over existing methods in meeting the proposed evaluation criteria. The work provides theoretical insights into DPO-based methods and offers a practical alternative for preference optimization.

## Executive Summary
This paper identifies fundamental limitations in existing direct preference optimization (DPO) methods and proposes new evaluation desiderata for alignment techniques. The authors introduce ℓTYPO, a novel loss function that satisfies these desiderata by combining supervised and unsupervised terms with KL divergence objectives. ℓTYPO achieves smooth interpolation between reference models and optimal policies while preserving performance in regions where the reference model excels. The method is theoretically grounded and empirically validated, demonstrating superiority over existing approaches in meeting the proposed evaluation criteria.

## Method Summary
The authors propose ℓTYPO, a new loss function for preference optimization that combines supervised and unsupervised terms using KL divergence. The supervised term KL[p*(z|·)||pθ(z|·)] measures the divergence between true preference probabilities and model-predicted probabilities, while the unsupervised term KL[πref||πθ] regularizes the model to stay close to a reference policy. The trade-off parameter λ balances these competing objectives. Unlike existing DPO methods, ℓTYPO avoids reparameterization dependencies and constraint-induced failures while achieving selective policy preservation and strong interpolation criteria.

## Key Results
- ℓTYPO satisfies new desiderata for preference optimization including selective policy preservation and strong interpolation criteria
- Theoretical analysis shows ℓTYPO avoids constraint-induced reparameterization failures that plague existing DPO methods
- Empirical validation demonstrates ℓTYPO's superiority over DPO, IPO, and f-DPO in meeting proposed evaluation criteria
- ℓTYPO achieves smooth interpolation between reference models and optimal policies while preserving performance in regions where the reference model excels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ℓTYPO preserves optimal policies in regions where the reference model is strong while improving elsewhere
- Mechanism: Separable supervised and unsupervised terms allow independent optimization of preservation and improvement objectives
- Core assumption: Preference data can be partitioned into regions where reference policy is optimal vs suboptimal
- Evidence anchors:
  - [abstract] "ℓTYPO achieves smooth interpolation between reference models and optimal policies while preserving performance in regions where the reference model excels"
  - [section 4.2] Proposition 3 shows minimizers of ℓTYPO preserve reference policy in dgood regions while improving in dbad regions
  - [corpus] Weak - related papers focus on DPO variants but not selective preservation
- Break condition: When prompt partitioning assumption fails or reference policy has no optimal regions

### Mechanism 2
- Claim: ℓTYPO satisfies strong interpolation criteria (SIC) unlike existing QPO methods
- Mechanism: Supervised term KL divergence forces convergence to BT-optimal policy as λ→0, unsupervised term KL divergence preserves reference policy as λ→∞
- Core assumption: KL divergence minimization recovers optimal distributions in limit
- Evidence anchors:
  - [abstract] "ℓTYPO satisfies our evaluation desiderata while avoiding any dependency on constraint-dependent reparameterizations"
  - [section 4.2] Proposition 4 proves ℓTYPO satisfies SIC through separable KL terms
  - [corpus] Weak - related papers discuss DPO interpolation but not SIC satisfaction
- Break condition: When KL divergence assumptions break down or separable terms interfere

### Mechanism 3
- Claim: ℓTYPO avoids constraint-induced reparameterization failures
- Mechanism: Derived without reference to RLHF reparameterizations, so constraint introduction doesn't invalidate derivation
- Core assumption: Objective independence from RLHF equivalence allows constraint robustness
- Evidence anchors:
  - [section 3.3] "TYPO is not based on any implicit association with RLHF in the first place, adding constraints...pose no issue"
  - [section 4.2] "none of the derivations used to motivate ℓTYPO...rely on unconstrained optimization"
  - [corpus] Weak - related papers focus on DPO constraint issues but not alternative derivations
- Break condition: When constraints fundamentally alter optimization landscape beyond separable term structure

## Foundational Learning

- Concept: Bradley-Terry preference model
  - Why needed here: Forms the theoretical foundation for how human preferences are modeled and optimized
  - Quick check question: What probability does the BT model assign when two responses have equal rewards?

- Concept: KL divergence properties
  - Why needed here: Central to both supervised and unsupervised terms in ℓTYPO
  - Quick check question: What distribution minimizes KL divergence to a target distribution?

- Concept: Quasi-convex optimization
  - Why needed here: Understanding QPO family limitations that ℓTYPO addresses
  - Quick check question: What property must a function have to be quasi-convex?

## Architecture Onboarding

- Component map:
  - Supervised term: KL divergence between true and model preference probabilities
  - Unsupervised term: KL divergence between reference and trainable policies
  - Trade-off parameter λ: Balances preservation vs improvement objectives
  - Reference policy πref: Fixed starting point for optimization
  - Trainable policy πθ: Model being optimized

- Critical path:
  1. Sample preference pairs from reference policy
  2. Compute supervised KL divergence term
  3. Sample responses from reference policy for unsupervised term
  4. Compute unsupervised KL divergence term
  5. Combine with λ weighting
  6. Optimize using gradient descent

- Design tradeoffs:
  - λ too small: Over-prioritizes BT-optimal policy, may lose reference policy benefits
  - λ too large: Over-prioritizes reference policy preservation, may not improve suboptimal regions
  - Separate terms: Allows independent control but requires careful λ tuning

- Failure signatures:
  - Reference policy performance degrades in previously good regions
  - Insufficient improvement in suboptimal regions
  - Optimization instability when preference data is sparse
  - Numerical issues with KL divergence computation

- First 3 experiments:
  1. Synthetic interpolation test: Verify convergence to BT-optimal policy with small λ
  2. Preservation test: Verify maintenance of reference policy in good regions while improving bad regions
  3. Constraint sensitivity: Compare performance with various regularization strengths against DPO baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the TYPO objectives compare with RLHF in practical scenarios where data distributions are mismatched?
- Basis in paper: [explicit] The paper mentions TYPO's independence from derivation/required equivalence/reparameterization that no longer holds upon introduction of constraints.
- Why unresolved: The paper doesn't provide empirical comparisons of TYPO vs RLHF in settings with distribution shifts.
- What evidence would resolve it: Experiments showing TYPO's performance relative to RLHF under various data distribution mismatches.

### Open Question 2
- Question: How sensitive are the TYPO objectives to the choice of the unsupervised term's distribution?
- Basis in paper: [explicit] The unsupervised term in TYPO uses samples from the reference model, and the paper mentions the potential to exploit out-of-preference data.
- Why unresolved: The paper doesn't explore the impact of using different distributions for the unsupervised term.
- What evidence would resolve it: Experiments varying the distribution used for the unsupervised term and measuring the impact on TYPO's performance.

### Open Question 3
- Question: How does the TYPO loss handle the trade-off between preference preservation and generation diversity?
- Basis in paper: [inferred] The paper mentions TYPO's ability to preserve optimal policies in ideal regimes while improving performance elsewhere, suggesting a trade-off between preservation and improvement.
- Why unresolved: The paper doesn't explicitly analyze how TYPO balances preference preservation with maintaining diversity in generated responses.
- What evidence would resolve it: Experiments measuring TYPO's ability to preserve preferences while maintaining diversity in generated responses, compared to other methods.

## Limitations

- Theoretical framework assumes clean partitioning of input space into optimal and suboptimal regions, which may not exist in practice
- Empirical validation relies heavily on synthetic experiments with limited testing on diverse real-world preference datasets
- Claims about superiority over existing methods are based on limited empirical comparison across relatively narrow task domains

## Confidence

- **High confidence**: The mathematical derivation of ℓTYPO and its satisfaction of the proposed desiderata (interpolation behavior, selective preservation)
- **Medium confidence**: Claims about ℓTYPO's superiority over existing methods, based on limited empirical comparison
- **Low confidence**: Generalization of ℓTYPO's benefits to all preference optimization scenarios, particularly given the synthetic nature of most experiments

## Next Checks

1. **Real-world robustness test**: Apply ℓTYPO to diverse real-world preference datasets (e.g., from multiple domains) and evaluate whether selective preservation holds when preference data is noisier and less clearly partitioned
2. **Scaling behavior analysis**: Investigate how ℓTYPO performs as model size increases and whether the λ hyperparameter requires scaling adjustments
3. **Constraint sensitivity study**: Systematically evaluate ℓTYPO's behavior under various regularization schemes and constraints to verify the claimed robustness compared to DPO-based methods