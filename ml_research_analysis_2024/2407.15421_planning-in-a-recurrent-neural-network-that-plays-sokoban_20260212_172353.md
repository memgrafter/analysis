---
ver: rpa2
title: Planning in a recurrent neural network that plays Sokoban
arxiv_id: '2407.15421'
source_url: https://arxiv.org/abs/2407.15421
tags:
- steps
- levels
- probe
- thinking
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes a Deep Repeating ConvLSTM (DRC) trained on Sokoban,
  uncovering that the network maintains a causal plan representation predicting future
  actions up to 50 steps in advance. The plan quality and length improve with computation,
  and the network deliberately "paces" in cycles to refine its plan early in levels.
---

# Planning in a recurrent neural network that plays Sokoban

## Quick Facts
- arXiv ID: 2407.15421
- Source URL: https://arxiv.org/abs/2407.15421
- Reference count: 40
- Key outcome: A DRC network maintains causal plan representations predicting future actions up to 50 steps ahead, with plan quality improving through deliberate pacing cycles during computation.

## Executive Summary
This paper analyzes a Deep Repeating ConvLSTM (DRC) trained on Sokoban to understand how it represents and refines plans during gameplay. The researchers discovered that the network maintains a causal plan representation encoding future actions about 50 steps in advance, and deliberately paces in cycles during early game steps to allocate extra computation time for plan refinement. Using linear probes trained on hidden states, they achieved 83% accuracy on training levels and 77.9% on validation levels, demonstrating that the ConvLSTM core can generalize to puzzles up to 3-4× larger than training data. The study provides insights into learned planning mechanisms and releases open-source resources for further interpretability research.

## Method Summary
The researchers trained DRC(3,3) and ResNet models on the Boxoban dataset using IMPALA V-trace actor-critic. They collected hidden state activations on hard levels and trained linear regression probes (Agent-Directions, Box-Directions, Next-Action) with L1 regularization to interpret plan representations. Causal interventions were performed using adaptive scaling methods to test probe influence on agent actions, and plan improvement was evaluated by forcing thinking steps. Generalization was tested by replacing the MLP with probes on out-of-distribution level sizes up to 31x31.

## Key Results
- The DRC maintains a causal plan representation predicting future actions up to 50 steps in advance
- Plan quality and length improve over the first few computation steps through deliberate pacing cycles
- Training with reduced NOOP penalties confirms pacing behavior is incentivized and serves plan refinement
- ConvLSTM core generalizes to puzzles 3-4× larger than training data with 83% probe accuracy on training levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ConvLSTM core encodes a causal plan representation that predicts future actions up to 50 steps in advance.
- Mechanism: Hidden state channels (e.g., L3C17, L3C7, L3C3) store spatially aligned information about box movement directions, agent movement, and goal states. Probes trained on these channels can decode this information with high accuracy.
- Core assumption: The network's planning capability is primarily contained in the convolutional core rather than the MLP output layer.
- Evidence anchors:
  - [abstract]: "The RNN has a causal plan representation which predicts its future actions about 50 steps in advance."
  - [section]: "Using layer 3 next-action probes (Table 10), we predict at each spatial location the next action of the DRC."
  - [corpus]: "Found 25 related papers... Average neighbor FMR=0.561" - weak correlation with planning-focused papers.

### Mechanism 2
- Claim: The network refines its plan over computation steps, improving plan quality and length.
- Mechanism: During thinking steps (cycles of NOOPs), the ConvLSTM iteratively updates its hidden state, which is reflected in increasing probe F1 scores and plan length. This refinement is more pronounced during deliberate cycles than accidental ones.
- Core assumption: Additional computation allows the network to develop longer-term solutions and avoid myopic errors.
- Evidence anchors:
  - [abstract]: "The quality and length of the represented plan increases over the first few steps."
  - [section]: "Plan improvement. If DRC refines its plan with extra computation (Finding 2), probe-extracted plans should become more predictive of the agent's actions as more computation time is provided."
  - [corpus]: "Dynamic Gated Recurrent Neural Network for Compute-efficient Speech Enhancement" - relevant to compute-efficient planning.

### Mechanism 3
- Claim: The network deliberately "paces" to allocate extra computation time for plan refinement.
- Mechanism: When the network detects its plan is suboptimal, it enters cycles of NOOPs (or pacing movements) to give itself more computation time. Training with reduced NOOP penalties increases cycle frequency while maintaining total cycle steps.
- Core assumption: The pacing behavior is incentivized by training and serves the purpose of plan refinement rather than being accidental.
- Evidence anchors:
  - [abstract]: "We uncover a surprising behavior: the RNN 'paces' in cycles to give itself extra computation at the start of a level."
  - [section]: "We hypothesize (3) that this behavior is incentivized by training, and its purpose is to give the DRC enough time to develop the plan."
  - [corpus]: "RTify: Aligning Deep Neural Networks with Human Behavioral Decisions" - relevant to behavioral decision-making in neural networks.

## Foundational Learning

- Concept: ConvLSTM architecture and recurrent computation
  - Why needed here: Understanding how the network processes sequential information and maintains hidden states across time steps is crucial for interpreting plan representations.
  - Quick check question: How does a ConvLSTM differ from a standard LSTM, and why is this important for spatial reasoning tasks?

- Concept: Causal intervention methodology
  - Why needed here: To verify that the network actually uses the information encoded in its hidden states rather than just having it passively represented.
  - Quick check question: What distinguishes a causal intervention from a simple predictive probe, and why is this distinction important for understanding network behavior?

- Concept: Probe training and evaluation
  - Why needed here: Probes are the primary tool for interpreting the network's internal representations, so understanding how to train and evaluate them is essential.
- Quick check question: What factors determine probe quality, and how can we distinguish between genuine representations and coincidental correlations?

## Architecture Onboarding

- Component map: Observation → Encoder (2×4×4 conv layers) → ConvLSTM core (3 layers, each applied 3× with pool-and-inject) → MLP (256 hidden units) → Action selection
- Critical path: Observation → Encoder → ConvLSTM layers (with recurrent connections) → MLP → Action selection
- Design tradeoffs: The DRC trades computational efficiency for planning capability through its recurrent architecture, while the ResNet baseline offers faster inference but limited planning ability.
- Failure signatures:
  - Low probe accuracy on hidden states may indicate poor plan representation
  - High probe accuracy without causal effects suggests the network isn't using the information
  - Oscillating performance during training could indicate instability in the recurrent connections
- First 3 experiments:
  1. Train box-direction probes on different layers and compare accuracy to identify where plan information is encoded
  2. Perform causal interventions on hidden states and measure changes in agent behavior
  3. Test generalization by replacing MLP with probes and evaluating on out-of-distribution level sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the DRC perform search by generating and evaluating multiple plans before selecting the one with the highest predicted value?
- Basis in paper: [inferred] The paper states that the DRC does not perform search by generating and evaluating multiple plans, and that the evidence is compatible with both search and iterative refinement with heuristics. The authors were unsuccessful at finding a representation of the value of each plan using a value probe.
- Why unresolved: The authors could not find clear evidence of multiple simultaneous plan representations or a mechanism to evaluate and decide between plans. The network's behavior could be explained by iterative refinement rather than explicit search.
- What evidence would resolve it: Finding a representation of the value of each plan in the network's activations, or correlating thinking steps with the number of plans generated and evaluated, would provide evidence for search. Conversely, if all plan refinement could be explained by iterative heuristics without explicit evaluation, it would suggest the network is not performing search.

### Open Question 2
- Question: What is the exact mechanism by which the DRC learns to "pace" and how does this behavior emerge during training?
- Basis in paper: [explicit] The paper shows that the DRC learns to pace in cycles to give itself extra computation time, and that this behavior is incentivized by training. Training with reduced NOOP penalties confirms this behavior is incentivized.
- Why unresolved: While the paper demonstrates that pacing occurs and is beneficial, the exact mechanism by which the network learns this behavior during training is not explained. The authors show that the pacing behavior is not an artifact and is deliberately performed, but the learning process remains unclear.
- What evidence would resolve it: Analyzing the network's activations and gradients during training could reveal how the pacing behavior emerges. Comparing networks trained with different reward structures or training schedules might also provide insights into the learning mechanism.

### Open Question 3
- Question: Can the DRC's planning representations generalize to tasks beyond Sokoban that require different types of sequential reasoning?
- Basis in paper: [explicit] The paper demonstrates that the DRC's ConvLSTM core can generalize to larger, out-of-distribution Sokoban puzzles by replacing the MLP with interpretable probes, achieving success on puzzles up to 3-4× larger than training data.
- Why unresolved: While the paper shows successful generalization within the Sokoban domain, it does not test whether the planning representations can transfer to fundamentally different tasks requiring sequential reasoning, such as maze navigation or puzzle games with different mechanics.
- What evidence would resolve it: Testing the DRC on tasks outside the Sokoban domain, such as different puzzle games or navigation tasks, while maintaining the same probing approach would demonstrate whether the planning representations are task-specific or more general. Success on diverse tasks would suggest more general planning capabilities.

## Limitations

- The interpretability claims rely heavily on probe accuracy, which may capture spurious correlations rather than genuine plan representations
- Causal intervention methodology depends on adaptive scaling parameters that could artificially inflate effect sizes
- Generalization results are limited to 3-4× larger puzzles, leaving uncertainty about performance on much larger or structurally different Sokoban variants

## Confidence

- **High Confidence**: The DRC architecture's superior performance on Boxoban levels (relative to ResNet) is well-established through direct comparison
- **Medium Confidence**: Probe accuracy metrics (83% on training, 77.9% on validation) suggest genuine plan representations, though correlation vs. causation concerns remain
- **Low Confidence**: Claims about pacing behavior being incentivized by training are indirect, based on reduced NOOP penalties rather than controlled ablation studies

## Next Checks

1. **Probe Robustness Test**: Evaluate probe performance on scrambled input observations to distinguish between genuine plan representations and superficial correlations with input features
2. **Intervention Power Analysis**: Systematically vary intervention strength across multiple orders of magnitude to establish whether observed behavioral changes are robust or artifacts of adaptive scaling
3. **Cross-Architecture Generalization**: Train similar probes on alternative recurrent architectures (e.g., vanilla LSTM, GRU) to determine whether ConvLSTM-specific properties are essential for plan representation