---
ver: rpa2
title: Do Large Language Models Align with Core Mental Health Counseling Competencies?
arxiv_id: '2410.22446'
source_url: https://arxiv.org/abs/2410.22446
tags:
- mental
- health
- counseling
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CounselingBench, a new benchmark for evaluating large language
  models on core mental health counseling competencies, was introduced. Based on the
  NCMHCE exam, it assesses 22 general-purpose and medical-finetuned models across
  five competencies: Counseling Skills & Interventions, Intake, Assessment & Diagnosis,
  Professional Practice & Ethics, Treatment Planning, and Core Counseling Attributes.'
---

# Do Large Language Models Align with Core Mental Health Counseling Competencies?

## Quick Facts
- arXiv ID: 2410.22446
- Source URL: https://arxiv.org/abs/2410.22446
- Reference count: 40
- Large language models fall short of expert-level performance in mental health counseling competencies

## Executive Summary
This study introduces CounselingBench, a benchmark designed to evaluate large language models (LLMs) on core mental health counseling competencies based on the National Clinical Mental Health Counseling Examination (NCMHCE). The benchmark assesses 22 general-purpose and medical-finetuned models across five competency domains. While frontier models surpass minimum passing thresholds, they fail to reach expert-level performance, particularly struggling with competencies requiring empathy and nuanced reasoning such as Core Counseling Attributes and Professional Practice & Ethics. Interestingly, medical LLMs do not outperform generalist models in accuracy despite providing better justifications, highlighting the challenges of developing AI systems for mental health applications.

## Method Summary
The researchers developed CounselingBench by curating 127 questions from the NCMHCE exam across five core mental health counseling competency domains. They evaluated 22 LLMs including both general-purpose and medical-finetuned models, measuring performance on multiple-choice accuracy and justification quality. Expert evaluators assessed model responses against minimum passing thresholds and expert-level standards. The evaluation framework compared generalist and medical LLMs while examining performance variations across different competency areas.

## Key Results
- Frontier models exceed minimum passing thresholds but fall short of expert-level performance
- Models perform best in Intake, Assessment & Diagnosis but struggle with Core Counseling Attributes and Professional Practice & Ethics
- Medical LLMs provide better justifications than generalist models but make more context-related errors and don't achieve higher accuracy

## Why This Works (Mechanism)
The benchmark leverages standardized clinical assessment frameworks (NCMHCE) to provide objective, quantifiable measures of LLM performance across well-defined competency domains. The multiple-choice format enables systematic comparison while expert evaluation of justifications offers insight into reasoning quality. Medical LLMs' better justifications suggest enhanced domain knowledge, though this doesn't translate to improved accuracy, indicating that raw knowledge isn't sufficient for complex counseling tasks requiring empathy and contextual understanding.

## Foundational Learning
- NCMHCE exam structure - why needed: Provides standardized framework for evaluating counseling competencies; quick check: 127 questions mapped to 5 competency domains
- Mental health counseling competencies - why needed: Defines the specific skills and knowledge areas to be assessed; quick check: 5 core domains including counseling skills, ethics, and treatment planning
- LLM evaluation metrics - why needed: Establishes objective measures for comparing model performance; quick check: multiple-choice accuracy and justification quality assessment

## Architecture Onboarding
- Component map: Question bank -> LLM models -> Multiple-choice response -> Expert evaluation -> Competency domain scores
- Critical path: Benchmark question → Model response → Expert scoring → Competency assessment
- Design tradeoffs: Multiple-choice format enables objective scoring but may not capture nuanced counseling skills
- Failure signatures: Models struggle with empathy-based competencies, context-related errors increase with medical finetuning
- First experiments: 1) Compare generalist vs medical LLMs across all competency domains; 2) Test model performance at different temperature settings; 3) Evaluate justification quality independently from accuracy

## Open Questions the Paper Calls Out
- How can LLMs be better aligned with empathy-based competencies like Core Counseling Attributes?
- What specific aspects of medical finetuning lead to increased context-related errors?
- Can alternative evaluation formats (e.g., open-ended scenarios) better capture nuanced counseling skills?
- How do model limitations in mental health counseling impact real-world deployment considerations?

## Limitations
- Benchmark based on subset of NCMHCE exam may not capture full breadth of real-world counseling scenarios
- Sample size of 127 questions and small expert panel introduces potential sampling bias
- Assessment focuses on multiple-choice accuracy rather than practical counseling effectiveness
- Evaluation framework may not fully capture the interpersonal and contextual nuances of real counseling interactions

## Confidence
- Confidence in frontier models falling short of expert-level performance: **High**
- Confidence in medical LLMs not outperforming generalist models: **Medium**
- Confidence in models struggling most with Core Counseling Attributes: **Medium**

## Next Checks
1. Expand benchmark to include open-ended scenario-based questions for better assessment of nuanced counseling competencies
2. Conduct blinded evaluations comparing LLM responses to actual expert counselor responses in real counseling transcripts
3. Test additional fine-tuning on diverse counseling datasets to improve performance in weakest competency areas