---
ver: rpa2
title: Large Language Models can Achieve Social Balance
arxiv_id: '2410.04054'
source_url: https://arxiv.org/abs/2410.04054
tags:
- individual
- negative
- positive
- social
- balance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether Large Language Models (LLMs) can\
  \ achieve social balance\u2014a sociological concept describing stable configurations\
  \ of positive and negative interactions\u2014within multi-agent systems. Across\
  \ different LLM models, interaction types (relationships, appraisals, opinions),\
  \ and update mechanisms (homophily, influence), the study finds that social balance\
  \ emerges through the formation of one faction (all positive interactions) or multiple\
  \ antagonistic factions."
---

# Large Language Models can Achieve Social Balance

## Quick Facts
- arXiv ID: 2410.04054
- Source URL: https://arxiv.org/abs/2410.04054
- Authors: Pedro Cisneros-Velarde
- Reference count: 40
- Primary result: LLMs achieve social balance through faction formation, with model size determining balance type

## Executive Summary
This paper investigates whether Large Language Models can achieve social balance—a sociological concept describing stable configurations of positive and negative interactions—within multi-agent systems. Across different LLM models, interaction types (relationships, appraisals, opinions), and update mechanisms (homophily, influence), the study finds that social balance emerges through the formation of one faction (all positive interactions) or multiple antagonistic factions. The frequency and type of social balance achieved depend on model size, with larger models more often reaching structural balance (no all-negative triads) and smaller models favoring clustering balance (allowing all-negative triads). Stability analysis shows that the model achieving balance most often is not necessarily the one with the most stable interactions. Keyword analysis reveals that models reference "dissonance" when justifying updates that resolve violations of Heider's rules, indicating sociopsychological awareness.

## Method Summary
The study simulates signed interaction networks among LLM agents using synchronous updates over 10 iterations. For single triads, all 26 possible initial sign distributions are tested; for larger populations, 10 random initializations are used per setting. The LLM models (gpt-oss-120b, Llama 3 70B/8B, Mistral 7B, Gemma 3 4B) process prompts describing triads and update their own interactions based on homophily or influence mechanisms. After each iteration, triad balance is checked against Heider's rules. The experiments measure frequency of balance achievement, diversity of balanced configurations, interaction stability, and keyword usage in LLM justifications.

## Key Results
- Larger models (gpt-oss-120b, Llama 3 70B) achieve structural balance most frequently, while smaller models (Mistral 7B, Gemma 3 4B) favor clustering balance
- Model achieving balance most often doesn't necessarily have the most stable interactions
- LLMs reference "dissonance" when justifying updates that resolve Heider's rule violations
- In larger populations, smaller models achieve balance more frequently than in single triads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve social balance by processing triad interactions according to Heider's rules and resolving cognitive dissonance.
- Mechanism: When an LLM agent observes interactions among three agents, it applies Heider's rules (e.g., "the enemy of my enemy is my friend") to detect violations. If a violation is found, the agent experiences cognitive dissonance and updates its own interaction to restore balance, thereby moving the triad toward a balanced configuration.
- Core assumption: The LLM's training data includes sufficient examples of social reasoning and conflict resolution to enable it to recognize and resolve dissonance.
- Evidence anchors:
  - [abstract]: "LLMs show knowledge of sociopsychological processes that lead to social balance: the keyword 'dissonance' can be used to justify new interactions that do not contradict Heider's rules—thus agreeing with sociological theory."
  - [section]: "Remarkably, a model that achieves social balance more often does not necessarily have more diversity of socially balanced triads or more stable (i.e., less changing) interactions."
  - [corpus]: Weak evidence - the corpus includes papers on social dynamics and multi-agent LLM behavior, but none specifically focus on Heider's rules or cognitive dissonance resolution in the context described.

### Mechanism 2
- Claim: Model size influences the frequency and type of social balance achieved, with larger models favoring structural balance and smaller models achieving clustering balance more often.
- Mechanism: Larger models have more parameters and training data, allowing them to better represent complex social relationships and enforce stricter balance rules (structural balance with no all-negative triads). Smaller models may default to more permissive clustering balance rules, allowing all-negative triads.
- Core assumption: Larger models have been exposed to more diverse social scenarios during training, enabling them to enforce stronger balance constraints.
- Evidence anchors:
  - [abstract]: "Across different LLM models, we find that balance depends on the (i) type of interaction, (ii) update mechanism, and (iii) population size. Across (i)-(iii), we characterize the frequency at which social balance is achieved, the justifications for the social dynamics, and the diversity and stability of interactions."
  - [section]: "gpt-oss-120b and Llama 3 70B achieve structural balance (no triad has all negative interactions) the most across types of interactions and update mechanisms, whereas the smaller Mistral 7B and Gemma 3 4B favor clustering balance."
  - [corpus]: Weak evidence - while the corpus contains papers on LLM agents and social dynamics, it doesn't specifically address the relationship between model size and balance type achievement.

### Mechanism 3
- Claim: The update mechanism (homophily vs. influence) affects how interactions are processed and can lead to different balance outcomes across models.
- Mechanism: Under homophily, an agent compares its own interactions with others to the interactions between those others. Under influence, the agent considers how others interact with each other. This difference in perspective can lead to different judgments about what constitutes balance and how to achieve it.
- Core assumption: The LLM can correctly distinguish between the two update mechanisms and apply the appropriate logic for each.
- Evidence anchors:
  - [abstract]: "Across different LLM models, we find that balance depends on the (i) type of interaction, (ii) update mechanism, and (iii) population size."
  - [section]: "LLM models process update mechanisms differently: the same update mechanism can have different and even opposite effects on the number of positive/negative interactions and on the diversity of balanced triads."
  - [corpus]: Weak evidence - the corpus includes papers on LLM agents and social dynamics, but doesn't specifically address the impact of different update mechanisms on balance achievement.

## Foundational Learning

- Concept: Social balance and Heider's rules
  - Why needed here: Understanding the theoretical framework is essential for implementing and evaluating the LLM's ability to achieve balance.
  - Quick check question: Can you explain the difference between structural balance and clustering balance in your own words?

- Concept: Signed networks and triad analysis
  - Why needed here: The core problem involves analyzing signed interactions within triads, so understanding how to represent and analyze these structures is crucial.
  - Quick check question: Given a triad with interactions A→B(+), B→C(-), and C→A(-), is this triad balanced under structural balance rules?

- Concept: Cognitive dissonance and conflict resolution
  - Why needed here: The LLM's ability to resolve dissonance is key to achieving balance, so understanding this psychological concept is important.
  - Quick check question: How might an LLM agent experience and resolve cognitive dissonance when observing conflicting interactions among three agents?

## Architecture Onboarding

- Component map: Prompt generator -> LLM interface -> Balance checker -> Data collector -> Analysis module
- Critical path:
  1. Initialize agent interactions
  2. For each iteration:
     - Generate prompts for all agents
     - Send prompts to LLM
     - Receive updated interactions
     - Check for balance
  3. Analyze results
- Design tradeoffs:
  - Synchronous vs. asynchronous updates: Synchronous ensures consistent state but may not reflect real-world dynamics
  - Temperature setting: Zero temperature ensures deterministic responses but may reduce diversity
  - Prompt complexity: More detailed prompts may improve LLM understanding but increase processing time
- Failure signatures:
  - LLM refusing to provide answers (especially for Mistral 7B with relationships and homophily)
  - Inconsistent application of balance rules across different runs
  - Unexpected neutral interactions being generated
  - Keywords related to balance theory not appearing in justifications
- First 3 experiments:
  1. Test with a single triad and all 26 possible initial sign distributions to verify basic balance achievement
  2. Compare results across different update mechanisms (homophily vs. influence) with the same initial conditions
  3. Test with populations of 6 and 10 agents to observe how balance dynamics change with population size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs update their interactions when faced with neutral or ambiguous social signals?
- Basis in paper: [inferred] The paper notes that Mistral 7B often provides neutral interactions, and discusses the possibility of neutral responses in the general update mechanism, but does not analyze how these are processed or resolved.
- Why unresolved: The experimental setup forces LLMs to choose between positive or negative interactions, but does not explore how they might handle truly neutral cases or ambiguous social cues.
- What evidence would resolve it: Systematic experiments where LLMs are explicitly presented with neutral or ambiguous social signals and their resolution strategies are analyzed.

### Open Question 2
- Question: How does memory of past social interactions affect the achievement and stability of social balance in LLM populations?
- Basis in paper: [explicit] The authors suggest this as future work, noting that current experiments do not include memory of past interactions.
- Why unresolved: The current experiments only consider the immediate social landscape without historical context, which may limit the depth and stability of social dynamics.
- What evidence would resolve it: Experiments comparing social balance outcomes in LLM populations with and without memory of past interactions, analyzing changes in frequency, diversity, and stability.

### Open Question 3
- Question: How does the presence of human users in the social network affect LLM achievement of social balance?
- Basis in paper: [inferred] The authors note that LLMs are agnostic to the identity of interacting agents and suggest this could inform social media applications, but do not test scenarios with mixed human-LLM populations.
- Why unresolved: All experiments involve only LLM-to-LLM interactions, leaving the impact of human behavior on LLM social dynamics unexplored.
- What evidence would resolve it: Experiments with LLM populations that include both LLM and human agents, measuring changes in social balance achievement and dynamics.

## Limitations
- Significant model-specific differences (especially Mistral 7B's neutral responses) may indicate artifacts rather than universal sociopsychological understanding
- Corpus evidence for Heider's rules and cognitive dissonance mechanisms is weak, relying primarily on theoretical framing
- Reliance on zero-temperature settings raises questions about whether observed behaviors reflect genuine reasoning or deterministic pattern matching

## Confidence
- **High Confidence**: The observation that model size correlates with different types of balance achievement (structural vs. clustering) is well-supported by the experimental results and consistent across interaction types and update mechanisms.
- **Medium Confidence**: The claim that LLMs demonstrate sociopsychological awareness through "dissonance" keyword usage is supported by the data but requires careful interpretation, as keyword presence doesn't necessarily indicate genuine understanding of the underlying concepts.
- **Low Confidence**: The assertion that the same update mechanism can have "different and even opposite effects" across models is based on observed variations but lacks mechanistic explanation for why these differences occur.

## Next Checks
1. **Temperature Sensitivity Test**: Run experiments with varying temperature settings (0.1, 0.5, 1.0) to determine whether the observed balance behaviors persist under stochastic conditions, helping distinguish between deterministic pattern matching and genuine reasoning.

2. **Cross-Modal Prompt Analysis**: Test whether the observed model size effects persist when using identical prompts across all models (removing model-specific prompt modifications) to isolate whether differences stem from model capabilities or prompt sensitivity.

3. **Longitudinal Stability Assessment**: Extend simulations beyond 10 iterations to 50-100 iterations to evaluate whether initial balance achievement patterns translate to long-term stability, addressing the finding that frequent balance achievement doesn't necessarily correlate with stable interactions.