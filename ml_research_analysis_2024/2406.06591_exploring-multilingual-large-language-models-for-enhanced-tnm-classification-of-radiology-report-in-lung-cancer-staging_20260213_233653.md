---
ver: rpa2
title: Exploring Multilingual Large Language Models for Enhanced TNM classification
  of Radiology Report in lung cancer staging
arxiv_id: '2406.06591'
source_url: https://arxiv.org/abs/2406.06591
tags:
- accuracy
- reports
- radiology
- definitions
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the efficacy of GPT3.5-turbo, a multilingual
  LLM by OpenAI, in generating TNM classifications from radiology reports in both
  English and Japanese languages. Classification accuracy was significantly improved
  by providing the TNM definition with GPT3.5 as prompts, the classification accuracies
  were significantly improved.
---

# Exploring Multilingual Large Language Models for Enhanced TNM classification of Radiology Report in lung cancer staging

## Quick Facts
- arXiv ID: 2406.06591
- Source URL: https://arxiv.org/abs/2406.06591
- Reference count: 0
- GPT3.5-turbo significantly improves TNM classification accuracy when provided with definitions as prompts, but shows reduced accuracy for Japanese reports compared to English

## Executive Summary
This study investigates the use of GPT3.5-turbo for automatic TNM classification from radiology reports in both English and Japanese. The researchers employed zero-shot learning with prompt engineering, providing TNM definitions alongside radiology reports to improve classification accuracy. The study found that including TNM definitions significantly enhanced performance, though Japanese reports showed notably lower accuracy for N and M classifications compared to English reports.

## Method Summary
The researchers used the NTCIR-17 MedNLP-SC dataset containing 162 Japanese radiology reports paired with ground truth TNM classifications, translated to English. They implemented zero-shot learning with GPT3.5-turbo, varying prompts by including different combinations of TNM definitions in both languages. Classification accuracy for T, N, M, and combined ALL factors was evaluated using Generalized Linear Mixed Models (GLMM) with odds ratios to account for random effects across report IDs.

## Key Results
- Providing TNM definitions to GPT3.5-turbo significantly improved classification accuracy across all factors
- Japanese reports showed a significant decrease in N and M accuracies compared to English reports (OR = 0.21 for M accuracy)
- Zero-shot learning without model retraining was sufficient to achieve performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing TNM definitions to GPT3.5-turbo significantly improves classification accuracy.
- Mechanism: The LLM leverages provided definitions as context to reduce ambiguity and align predictions with standardized criteria, acting as in-context learning rather than fine-tuning.
- Core assumption: GPT3.5-turbo retains sufficient pre-trained knowledge of medical terminology to benefit from contextual definitions.
- Evidence anchors:
  - [abstract] "Classification accuracy was significantly improved by providing the TNM definition with GPT3.5 as prompts"
  - [section] "providing TNM definitions led to better performance compared to when definitions were not provided"
  - [corpus] Weak corpus support; no direct citations of definition-prompting in radiology LLMs.
- Break condition: If the model lacks foundational knowledge of TNM, definitions may not bridge the gap, leading to inconsistent predictions.

### Mechanism 2
- Claim: Language of input reports affects N and M factor accuracy but not T or overall accuracy.
- Mechanism: The multilingual model shows bias toward English-language training data, reducing precision for Japanese inputs in more complex classification subtasks (N and M).
- Core assumption: The model's multilingual training corpus is imbalanced toward English, particularly for specialized medical terminology.
- Evidence anchors:
  - [abstract] "There was a significant decrease in N and M accuracies for Japanese reports compared with English Reports"
  - [section] "documenting reports in Japanese led to a significant decrease in performance (OR = 0.21) for M accuracy"
  - [corpus] No direct evidence of multilingual performance variance in radiology datasets.
- Break condition: If translation quality degrades subtle clinical nuance, accuracy drops across all factors, not just N and M.

### Mechanism 3
- Claim: Zero-shot learning suffices for TNM classification without model retraining.
- Mechanism: GPT3.5-turbo's broad pretraining captures general reasoning patterns that transfer to structured classification tasks when supplemented with definitions.
- Core assumption: Zero-shot performance is stable and reproducible across similar datasets without domain-specific fine-tuning.
- Evidence anchors:
  - [section] "Even without additional training, performance improvements were evident with the provided TNM definitions"
  - [section] "This study uses zero-shot learning, which mitigates concerns regarding overfitting"
  - [corpus] No corpus evidence on zero-shot radiology task generalization.
- Break condition: If task specificity exceeds zero-shot capability, accuracy plateaus or degrades with scale.

## Foundational Learning

- Concept: TNM staging system
  - Why needed here: Core target for classification; without understanding T, N, M definitions, model predictions lack clinical grounding.
  - Quick check question: What are the five categories for T, four for N, and two for M in lung cancer staging?

- Concept: Zero-shot learning in LLMs
  - Why needed here: Determines whether task-specific fine-tuning is required or if in-context prompting suffices.
  - Quick check question: How does zero-shot learning differ from few-shot learning in LLM behavior?

- Concept: Generalized Linear Mixed Models (GLMM)
  - Why needed here: Statistical framework for analyzing classification accuracy with fixed and random effects across multiple conditions.
  - Quick check question: What is the role of random effects in GLMM when analyzing repeated measurements across report IDs?

## Architecture Onboarding

- Component map: GPT3.5-turbo API → Prompt assembly (definitions + report) → Response parsing → Accuracy evaluation → GLMM analysis
- Critical path: Prompt construction → LLM inference → Result extraction → Statistical validation
- Design tradeoffs: Zero-shot simplicity vs. fine-tuning accuracy; multilingual breadth vs. depth; prompt size limits vs. definition completeness
- Failure signatures: Inconsistent TNM predictions; low M accuracy for Japanese reports; prompt-response parsing errors
- First 3 experiments:
  1. Vary prompt definition inclusion (T only, N only, M only, full) to isolate impact per factor.
  2. Translate Japanese reports to English and compare classification accuracy to native English reports.
  3. Test with a larger, balanced dataset to validate statistical significance and generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GPT3.5-turbo's performance in TNM classification change if it were fine-tuned on a larger, domain-specific dataset of radiology reports?
- Basis in paper: [inferred] The paper used zero-shot learning without additional model training, suggesting that fine-tuning could potentially improve performance.
- Why unresolved: The study only examined zero-shot learning performance and did not explore the effects of fine-tuning the model.
- What evidence would resolve it: A study comparing the zero-shot performance with fine-tuned GPT3.5-turbo on a large, domain-specific dataset of radiology reports.

### Open Question 2
- Question: Would the performance of GPT3.5-turbo in TNM classification be affected by the use of radiology reports written in other languages, such as Chinese or Spanish?
- Basis in paper: [explicit] The paper examined the performance of GPT3.5-turbo on radiology reports in English and Japanese, finding a significant decrease in N and M accuracies for Japanese reports.
- Why unresolved: The study only tested two languages, and it is unclear how the model would perform with other languages.
- What evidence would resolve it: A study testing GPT3.5-turbo's performance on radiology reports in multiple languages, including Chinese, Spanish, and other widely spoken languages.

### Open Question 3
- Question: How would the inclusion of additional clinical information, such as patient history and laboratory results, impact GPT3.5-turbo's performance in TNM classification?
- Basis in paper: [inferred] The study only used radiology reports for TNM classification, suggesting that additional clinical information could potentially improve the model's performance.
- Why unresolved: The study did not explore the effects of incorporating additional clinical information into the TNM classification task.
- What evidence would resolve it: A study comparing the performance of GPT3.5-turbo on TNM classification using only radiology reports versus a combination of radiology reports and additional clinical information.

## Limitations
- Small dataset size (162 reports) may limit generalizability of findings
- Translation artifacts may affect accuracy comparisons between English and Japanese reports
- Zero-shot approach may not achieve optimal performance compared to fine-tuned models

## Confidence
- High confidence in: GPT3.5-turbo's ability to perform TNM classification from radiology reports using zero-shot learning with prompt-based definitions
- Medium confidence in: The specific accuracy improvements from providing TNM definitions
- Medium confidence in: The observed language-dependent performance differences (Japanese vs. English)
- Low confidence in: Claims about the mechanism underlying language-dependent performance differences

## Next Checks
1. Test the classification system on a larger, balanced multilingual dataset with native language reports in both English and Japanese to validate the observed language-dependent performance patterns.

2. Conduct an ablation study comparing zero-shot performance with fine-tuned models on the same task to quantify the performance tradeoff and determine if fine-tuning would be beneficial.

3. Evaluate the impact of different prompt engineering strategies, including varying the level of detail in TNM definitions and testing alternative prompt structures, to optimize classification accuracy.