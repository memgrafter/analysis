---
ver: rpa2
title: 'TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning
  in Real-world Use-cases'
arxiv_id: '2408.07579'
source_url: https://arxiv.org/abs/2408.07579
tags:
- adversarial
- standard
- lcld
- wids
- tabnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TabularBench is the first comprehensive benchmark for adversarial
  robustness of tabular deep learning models. It evaluates models using CAA, an ensemble
  of gradient and search attacks shown to be highly effective.
---

# TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases

## Quick Facts
- arXiv ID: 2408.07579
- Source URL: https://arxiv.org/abs/2408.07579
- Reference count: 40
- Key outcome: First comprehensive benchmark for adversarial robustness of tabular deep learning models

## Executive Summary
TabularBench introduces the first standardized benchmark for evaluating adversarial robustness in tabular deep learning, addressing the unique challenges posed by feature constraints and complex relationships in structured data. The benchmark evaluates 200+ models across five real-world datasets using CAA, an ensemble of gradient and search attacks shown to be highly effective against tabular models. Key findings reveal that in-distribution performance is misleading for robustness assessment, domain constraints are critical for reliable evaluation, and data augmentation effectiveness varies significantly across tasks.

## Method Summary
TabularBench provides a comprehensive framework for evaluating adversarial robustness in tabular deep learning through standardized datasets, pre-trained models, and evaluation procedures. The benchmark uses CAA (Constrained Adaptive Attack), combining gradient-based CAPGD and search-based MOEV A attacks to handle feature constraints and optimize for classification error, perturbation minimization, and constraint satisfaction. The evaluation includes both standard training and adversarial training with six data augmentation techniques across five real-world datasets spanning finance, healthcare, and security domains.

## Key Results
- ID performance is misleading for robustness assessment - models with similar clean accuracy show vastly different robust performance
- Domain constraints are critical for reliable evaluation - unconstrained attacks overestimate model robustness
- Data augmentation effectiveness is task-specific - no single technique consistently improves both ID and robust performance
- No single architecture or augmentation consistently outperforms others across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAA (Constrained Adaptive Attack) is the most effective attack against tabular models because it combines gradient-based and search-based approaches.
- Mechanism: The attack uses CAPGD (gradient-based) to maximize classification error and minimize constraint violations through regularization and projection, while MOEV A (search-based) uses a genetic algorithm to optimize for classification error, perturbation minimization, and constraint violations simultaneously.
- Core assumption: Tabular data exhibits complex feature relationships and constraints that gradient-only attacks cannot effectively handle.
- Evidence anchors:
  - [abstract]: "CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model."
  - [section]: "CAA combines two attacks, CAPGD and MOEV A. CAPGD is an iterative gradient attack that maximizes the error and minimizes the features' constraint violations with regularization losses and projection mechanisms. MOEV A is a genetic algorithm attack that considers the three adversarial objectives: (1) classifier's error maximization, (2) perturbation minimization, and (3) constraint violations minimization, in its fitness function."
  - [corpus]: Weak evidence - corpus contains related work on adversarial attacks but no direct evidence of CAA's effectiveness.
- Break condition: If the feature relationships are simple or if constraints can be easily satisfied through projection alone, the search-based component may be unnecessary overhead.

### Mechanism 2
- Claim: Domain constraints are critical for reliable robustness evaluation because unconstrained attacks overestimate model robustness.
- Mechanism: When constraints are ignored, attacks can generate adversarial examples that violate domain rules, which would be rejected by real-world systems. This creates a false sense of security.
- Core assumption: Real-world tabular ML systems have monitoring mechanisms that reject inputs violating domain constraints.
- Evidence anchors:
  - [abstract]: "Importance of domain constraints: Disregarding domain constraints overestimates robustness and leads to selection of sub-optimal architectures and defenses when considering the domain constraints."
  - [section]: "One significant challenge is that tabular data exhibit feature constraints, which are complex relationships and interactions between features. Satisfying these feature constraints can be a non-convex or even nondifferentiable problem, making established evasion attack algorithms relying on gradient descent ineffective in generating valid adversarial examples."
  - [corpus]: Weak evidence - corpus contains related work on adversarial attacks but no direct evidence of domain constraint importance.
- Break condition: If the domain constraints are minimal or if the model is already robust to unconstrained attacks, the additional complexity of constrained evaluation may not be necessary.

### Mechanism 3
- Claim: Data augmentation effectiveness is task-specific, meaning no single augmentation technique consistently improves both ID and robust performance across all tasks.
- Mechanism: Different augmentation techniques (CTGAN, Cutmix, etc.) capture different aspects of the data distribution. The effectiveness depends on the specific characteristics of the task and dataset.
- Core assumption: The data distribution and feature relationships vary significantly across different tabular tasks.
- Evidence anchors:
  - [abstract]: "Data augmentation effectiveness is task-specific. There is no data augmentation that is optimal for both ID and robust performance across all tasks. Some simpler augmentations (like Cutmix) can outperform complex generative approaches."
  - [section]: "With data augmentation alone, ID and robust performances are not aligned. In Figure 2 we study the impact of data augmentation on ID and robust performance, both in standard and adversarial training."
  - [corpus]: Weak evidence - corpus contains related work on data augmentation but no direct evidence of task-specific effectiveness.
- Break condition: If the data distribution is simple or if the task characteristics are similar across datasets, a single augmentation technique might be sufficient.

## Foundational Learning

- Concept: Adversarial robustness
  - Why needed here: Understanding how to evaluate and improve the resistance of ML models to adversarial attacks is central to this work.
  - Quick check question: What is the difference between clean accuracy and robust accuracy?

- Concept: Feature constraints and relationships
  - Why needed here: Tabular data often has complex relationships between features that must be satisfied for valid inputs, which is crucial for realistic adversarial attacks.
  - Quick check question: Why are feature constraints important in tabular data but less so in image data?

- Concept: Data augmentation techniques
  - Why needed here: Various data augmentation methods are evaluated for their impact on both clean and robust performance, requiring understanding of their mechanisms.
  - Quick check question: How does Cutmix differ from generative approaches like CTGAN in terms of data augmentation?

## Architecture Onboarding

- Component map: Dataset Zoo -> Model Zoo -> Benchmark -> Attacks
- Critical path: Load dataset → Preprocess with constraints → Select model from zoo → Evaluate with benchmark → Analyze results
- Design tradeoffs:
  - Support for multiple architectures vs. depth of implementation for each
  - Constraint satisfaction vs. attack effectiveness
  - Computational cost of search-based attacks vs. gradient-based attacks
- Failure signatures:
  - Performance collapse when using certain data augmentations (e.g., WGAN in LCLD)
  - Misalignment between ID and robust performance
  - Overfitting to specific constraint sets
- First 3 experiments:
  1. Evaluate a pre-trained model on a dataset with constraints to verify the constraint satisfaction mechanism
  2. Compare clean accuracy vs. robust accuracy for a model trained with and without adversarial training
  3. Test the impact of different data augmentation techniques on both ID and robust performance for a single architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do feature constraints impact the robustness of tabular deep learning models across different domains and attack scenarios?
- Basis in paper: [explicit] The paper highlights the importance of domain constraints in evaluating robustness and finds that disregarding them overestimates robustness, but does not provide a comprehensive analysis of how constraint complexity affects robustness across various domains.
- Why unresolved: While the paper identifies that domain constraints are critical for reliable robustness assessment, it does not systematically explore how varying levels of constraint complexity (e.g., linear vs. non-linear, number of constraints) impact model robustness across different domains (finance, healthcare, security).
- What evidence would resolve it: A systematic study comparing model robustness under varying constraint complexities and types across multiple domains would clarify the relationship between constraint characteristics and robustness.

### Open Question 2
- Question: Which data augmentation techniques are most effective for improving both in-distribution and adversarial robustness in tabular deep learning?
- Basis in paper: [explicit] The paper finds that data augmentation effectiveness is task-specific and that no single technique consistently outperforms others across all tasks, with Cutmix often performing well but no clear winner identified.
- Why unresolved: The paper shows that data augmentation impacts robustness differently across tasks and architectures, but does not provide clear guidance on which techniques to prioritize for specific use cases or how to balance ID and robust performance improvements.
- What evidence would resolve it: Comparative studies of data augmentation techniques across diverse tabular tasks, measuring both ID and robust performance, would identify optimal strategies for different scenarios.

### Open Question 3
- Question: How does the choice of distance metric affect the evaluation of adversarial robustness in tabular deep learning?
- Basis in paper: [inferred] The paper uses L2 distance for imperceptibility but acknowledges that other metrics exist (e.g., L∞, SSIM) and that the choice of metric can vary by domain, without providing a comprehensive comparison of their impact on robustness assessment.
- Why unresolved: The paper focuses on L2 distance for imperceptibility but does not explore how different distance metrics might affect the perceived robustness of models or the effectiveness of attacks.
- What evidence would resolve it: Evaluating model robustness and attack effectiveness using multiple distance metrics (e.g., L∞, SSIM) would reveal how metric choice influences robustness assessment and attack strategies.

## Limitations

- The evaluation framework relies heavily on CAA as the primary attack method, but lacks extensive ablation studies showing how individual attack components contribute to overall effectiveness
- Domain constraint implementation details are not fully specified, making it difficult to assess whether constraints accurately reflect real-world deployment scenarios
- The generalizability of task-specific data augmentation findings is limited by the selection of only 5 datasets

## Confidence

- **High confidence**: The benchmark design and methodology are sound, with clear APIs and reproducible evaluation procedures. The identification of the ID-robustness gap is well-supported by empirical results.
- **Medium confidence**: The CAA attack effectiveness claims are reasonable but not extensively validated against alternative approaches. The domain constraint importance is theoretically sound but lacks comprehensive empirical validation across diverse constraint types.
- **Low confidence**: The generalizability of task-specific data augmentation findings is limited by the selection of only 5 datasets, which may not represent the full diversity of tabular ML applications.

## Next Checks

1. **Ablation study of CAA components**: Evaluate the relative contribution of gradient-based (CAPGD) vs search-based (MOEV A) attack components on the benchmark datasets to quantify their individual effectiveness.

2. **Constraint sensitivity analysis**: Systematically relax domain constraints in the evaluation pipeline to measure the impact on robustness estimates and identify which constraint types are most critical for reliable assessment.

3. **Augmentation transferability test**: Apply data augmentation techniques that showed strong performance on one dataset to other datasets to assess whether effectiveness is truly task-specific or if some augmentations generalize better than others.