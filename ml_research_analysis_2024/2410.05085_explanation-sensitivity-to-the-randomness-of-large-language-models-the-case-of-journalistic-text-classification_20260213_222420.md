---
ver: rpa2
title: 'Explanation sensitivity to the randomness of large language models: the case
  of journalistic text classification'
arxiv_id: '2410.05085'
source_url: https://arxiv.org/abs/2410.05085
tags:
- explanations
- which
- camembert
- attention
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the sensitivity of large language
  model (LLM) explanations to random elements in their training is significant enough
  to affect their explainability. The authors focus on the task of opinionated journalistic
  text classification in French using a fine-tuned CamemBERT model and Layer-wise
  Relevance Propagation (LRP) explanations.
---

# Explanation sensitivity to the randomness of large language models: the case of journalistic text classification

## Quick Facts
- arXiv ID: 2410.05085
- Source URL: https://arxiv.org/abs/2410.05085
- Reference count: 0
- Key outcome: Large language model explanations show significant sensitivity to random training elements, necessitating characterization of explanation distributions for reliable explainability.

## Executive Summary
This paper investigates whether the sensitivity of large language model (LLM) explanations to random elements in their training is significant enough to affect their explainability. The authors focus on the task of opinionated journalistic text classification in French using a fine-tuned CamemBERT model and Layer-wise Relevance Propagation (LRP) explanations. They find that training with different random seeds produces models with similar accuracy but variable explanations, demonstrating that characterizing the statistical distribution of explanations is needed for LLM explainability. The authors also explore a simpler logistic regression model based on textual features, which offers stable but less accurate explanations. By enriching this simpler model with features derived from CamemBERT's explanations, they achieve improved accuracy while retaining deterministic results. The study highlights the need to characterize the randomness of LLM explanations and suggests directions for future research on reducing this sensitivity and improving explainability.

## Method Summary
The authors fine-tune CamemBERT on French journalistic text classification data using different random seeds to generate equivalent models with similar accuracy. They generate LRP explanations for each model and analyze the correlation between explanations across models. They also develop a feature-based logistic regression model (LING-LR) and enrich it with features derived from CamemBERT explanations to create LING-LR-E, comparing accuracy and explanation stability across approaches.

## Key Results
- Models trained with different random seeds achieve similar accuracy but produce variable explanations for the same inputs
- Characterizing the statistical distribution of explanations is necessary for LLM explainability
- Enriching simpler models with features from LLM explanations improves accuracy while maintaining deterministic results
- The sensitivity to randomness is observed specifically in complex models rather than simpler feature-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training with different random seeds produces models with similar accuracy but variable explanations.
- Mechanism: Randomness in model training (weight initialization, data shuffling, dropout) leads to different learned representations that can still achieve comparable performance on the same task.
- Core assumption: The random elements in the training process are significant enough to affect the learned representations but not the overall accuracy.
- Evidence anchors:
  - [abstract] "We find that training with different random seeds produces models with similar accuracy but variable explanations."
  - [section] "We therefore claim that characterizing the explanations' statistical distribution is needed for the explainability of LLMs."
  - [corpus] Weak evidence - the corpus neighbors do not directly address the impact of randomness on explanations.
- Break condition: If the random elements in the training process do not significantly affect the learned representations, or if the models trained with different seeds have significantly different accuracies.

### Mechanism 2
- Claim: Characterizing the statistical distribution of explanations is needed for the explainability of LLMs.
- Mechanism: Since different models with similar accuracy can produce different explanations, a single explanation is insufficient and potentially arbitrary. Characterizing the distribution provides a more complete understanding of the model's decision-making process.
- Core assumption: The variability in explanations across models with similar accuracy is significant and cannot be ignored for explainability purposes.
- Evidence anchors:
  - [abstract] "We therefore claim that characterizing the explanations' statistical distribution is needed for the explainability of LLMs."
  - [section] "In this context, we claim that if a learning method leads to a non-negligible set of equivalent models whose explanations differ, then limiting ourselves to the explanation of a single model obtained with that learning method is insufficient."
  - [corpus] Weak evidence - the corpus neighbors do not directly address the need for characterizing the statistical distribution of explanations.
- Break condition: If the variability in explanations across models with similar accuracy is negligible or if a single explanation is sufficient for the intended use case.

### Mechanism 3
- Claim: Enriching a simpler, stable model with features derived from LLM explanations can improve accuracy while retaining deterministic results.
- Mechanism: The simpler model provides stable explanations, and the features derived from LLM explanations capture information that improves the model's accuracy without introducing randomness.
- Core assumption: The features derived from LLM explanations are informative and can be effectively integrated into the simpler model without compromising its stability.
- Evidence anchors:
  - [abstract] "We show that it can be improved by inserting features derived from CamemBERT's explanations."
  - [section] "We show that it is possible to improve the accuracy of the linguistic model (which is still lower than that of the transformer models) while retaining deterministic results and thus invariant explanations for a given prediction."
  - [corpus] Weak evidence - the corpus neighbors do not directly address the enrichment of simpler models with features derived from LLM explanations.
- Break condition: If the features derived from LLM explanations are not informative or cannot be effectively integrated into the simpler model, or if the enrichment process introduces randomness.

## Foundational Learning

- Concept: Randomness in model training
  - Why needed here: Understanding how random elements in the training process affect the learned representations and explanations is crucial for assessing the sensitivity of LLM explanations to randomness.
  - Quick check question: How do random elements like weight initialization, data shuffling, and dropout affect the learned representations in a model?

- Concept: Explainability and faithfulness of explanations
  - Why needed here: Evaluating the sensitivity of LLM explanations to randomness requires understanding the criteria for explainability, such as faithfulness and plausibility, and how they are affected by randomness.
  - Quick check question: What are the key criteria for evaluating the explainability of model explanations, and how can randomness impact these criteria?

- Concept: Feature-based models and their advantages
  - Why needed here: Comparing the sensitivity of LLM explanations to randomness with that of simpler, feature-based models provides insights into the trade-offs between accuracy and explainability.
  - Quick check question: What are the advantages and disadvantages of using feature-based models compared to LLMs, particularly in terms of explainability and sensitivity to randomness?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (CamemBERT/LING-LR) -> Explanation generation (LRP/LAM) -> Evaluation and analysis
- Critical path: Data preprocessing → Model training → Explanation generation → Evaluation and analysis
- Design tradeoffs:
  - Accuracy vs. explainability: LLMs offer higher accuracy but are more sensitive to randomness, while simpler models provide stable explanations but may have lower accuracy.
  - Computational cost: LLMs require more computational resources for training and inference compared to simpler models.
- Failure signatures:
  - High variability in explanations across models with similar accuracy
  - Inability to characterize the statistical distribution of explanations
  - Lack of improvement in accuracy when enriching simpler models with features derived from LLM explanations
- First 3 experiments:
  1. Train multiple LLMs with different random seeds and compare their explanations for the same inputs.
  2. Generate explanations for the same inputs using different explanation methods (e.g., LRP and LAM) and compare their faithfulness and plausibility.
  3. Enrich a simpler, feature-based model with features derived from LLM explanations and evaluate the improvement in accuracy and the stability of explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the sensitivity of explanations to randomness affect their plausibility, particularly when compared to human annotations of journalistic texts?
- Basis in paper: [explicit] The authors suggest investigating the impact of randomness sensitivity on plausibility and propose setting up a human annotation experiment to explore whether variability in explanations reflects variability in human interpretations.
- Why unresolved: The paper identifies the need for such an investigation but does not conduct it, leaving the relationship between randomness sensitivity and plausibility unclear.
- What evidence would resolve it: Conducting human annotation experiments where annotators evaluate the plausibility of explanations generated from models with different random seeds, comparing these to human explanations of the same texts.

### Open Question 2
- Question: Does the mismatch between complex models like CamemBERT and simple explanation formats (e.g., token-level attention) increase the sensitivity of explanations to randomness?
- Basis in paper: [inferred] The authors hypothesize that methods providing simple explanations may not be well-suited to the complexity of large language models, potentially increasing randomness sensitivity.
- Why unresolved: The paper raises this hypothesis but does not test it by comparing models of varying complexity or by adapting explanation methods to better match model complexity.
- What evidence would resolve it: Testing simpler models for randomness sensitivity and developing or adapting explanation methods to better reflect the complexity of large language models, then comparing the sensitivity of explanations across these approaches.

### Open Question 3
- Question: Can reducing the dependence of large language models on random elements during training simplify the problem of explaining their decisions?
- Basis in paper: [explicit] The authors suggest that reducing randomness in training could potentially simplify the problem of explainability.
- Why unresolved: The paper does not explore methods to reduce randomness in training or assess the impact on explainability, leaving this as a potential research direction.
- What evidence would resolve it: Developing and testing training methods that minimize randomness, then evaluating whether these methods lead to more stable and interpretable explanations.

## Limitations

- The study's findings are limited by the narrow scope of evaluation across only two French news corpora and a single explanation method (LRP).
- The analysis focuses on binary classification of news vs opinion articles, which may not generalize to more complex or nuanced classification tasks.
- Computational constraints prevented exhaustive evaluation across all possible random seeds, potentially missing edge cases where explanation sensitivity is more pronounced.

## Confidence

**High Confidence**: The observation that equivalent models (similar accuracy) produce variable explanations is well-supported by the statistical analysis and correlation metrics presented.

**Medium Confidence**: The claim that characterizing the statistical distribution of explanations is necessary for LLM explainability is theoretically compelling but would benefit from broader empirical validation across different tasks, languages, and explanation methods.

**Low Confidence**: The assertion that enriching simpler models with features from LLM explanations consistently improves accuracy while maintaining deterministic results requires more extensive validation.

## Next Checks

1. **Cross-task validation**: Replicate the explanation sensitivity analysis on a different classification task (e.g., sentiment analysis or topic classification) to assess whether the observed phenomenon generalizes beyond news/opinion classification.

2. **Methodological robustness**: Compare LRP explanation sensitivity with other explanation methods (e.g., SHAP, Integrated Gradients, attention-based methods) to determine if the sensitivity to randomness is method-specific or inherent to the model architecture.

3. **Feature enrichment scalability**: Test the enrichment approach on a larger, more diverse feature set and evaluate whether the deterministic accuracy improvements observed in the current study hold when scaling to more complex feature spaces or different types of simpler models.