---
ver: rpa2
title: How Does Quantization Affect Multilingual LLMs?
arxiv_id: '2407.03211'
source_url: https://arxiv.org/abs/2407.03211
tags:
- quantization
- languages
- language
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study is the first to systematically evaluate how quantization
  impacts multilingual large language models across languages and scales. Using automatic
  benchmarks, LLM-as-a-Judge, and human evaluation on realistic prompts, the authors
  find that quantization harms multilingual performance far more than automatic metrics
  suggest: a 1.7% average drop in Japanese accuracy corresponds to a 16.0% drop in
  human evaluations.'
---

# How Does Quantization Affect Multilingual LLMs?

## Quick Facts
- **arXiv ID**: 2407.03211
- **Source URL**: https://arxiv.org/abs/2407.03211
- **Reference count**: 30
- **Primary result**: Quantization degrades multilingual LLM performance more severely than automatic metrics suggest, with non-Latin script languages and mathematical reasoning tasks disproportionately affected.

## Executive Summary
This study systematically evaluates how quantization impacts multilingual large language models across languages and scales. Using automatic benchmarks, LLM-as-a-Judge, and human evaluation on realistic prompts, the authors find that quantization harms multilingual performance far more than automatic metrics suggest: a 1.7% average drop in Japanese accuracy corresponds to a 16.0% drop in human evaluations. Non-Latin script languages are disproportionately affected, and challenging tasks like mathematical reasoning degrade fastest. While quantization occasionally provides performance boosts, these findings underscore the need to prioritize multilingual performance in efficient model design.

## Method Summary
The study evaluates Command R/R+ (103B, 35B) and Aya 23 (35B, 8B) models across 20+ languages using post-training quantization (PTQ) methods including W8, W8A8, W4-g, W8A8-sq, and W4. Evaluation combines automatic benchmarks (mMMLU, MGSM, FLORES-200, Language Confusion), LLM-as-a-Judge, and human evaluation on 300+ prompts per language. The research measures relative performance (%Δ vs. FP16 baseline) across language groups and task types, with particular focus on underrepresented languages and challenging reasoning tasks.

## Key Results
- Quantization causes a 16.0% drop in human evaluations compared to only 1.7% average drop in automatic metrics for Japanese accuracy
- Non-Latin script languages show the worst degradation under quantization
- Mathematical reasoning tasks degrade fastest, with 35B W4-g model showing -13.1% relative performance on MGSM
- Mitigation strategies like SmoothQuant and group-wise scaling help but introduce task-specific trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization degrades multilingual LLM performance more severely than monolingual due to asymmetric data distribution effects
- Mechanism: Lower-precision quantization reduces representational capacity unevenly across parameter ranges. Underrepresented languages have sparser gradients and lower-frequency token embeddings, so compression disproportionately erases fine-grained distinctions critical for those languages
- Core assumption: The training data distribution for underrepresented languages is sparse enough that quantization noise causes higher relative degradation than in high-resource languages
- Evidence anchors:
  - [abstract]: "languages are disparately affected by quantization, with non-Latin script languages impacted worst"
  - [section 2]: "many model design choices implicitly overfit to a handful of resource rich languages"
  - [corpus]: Weak evidence—no direct quantization comparison across resource tiers
- Break condition: If training data were perfectly balanced across languages, this mechanism would weaken significantly

### Mechanism 2
- Claim: Quantization particularly harms tasks requiring fine-grained semantic reasoning, such as mathematical reasoning
- Mechanism: Low-precision weights introduce numerical instability in operations requiring precise numerical computation. Mathematical reasoning tasks demand high fidelity in numerical token representations, which quantization disrupts more than general language tasks
- Core assumption: The quantization-induced numerical noise disproportionately impacts token embeddings and attention weights involved in numeric reasoning
- Evidence anchors:
  - [abstract]: "challenging tasks like mathematical reasoning degrade fastest"
  - [section 4.2]: "Mathematical reasoning (MGSM) is strikingly affected by quantization. Relative performance of the 35B W4-g model is a dismal −13.1%"
  - [corpus]: No direct evidence—break here
- Break condition: If quantization preserves numerical precision (e.g., with mixed-precision strategies), this mechanism would break

### Mechanism 3
- Claim: Quantization techniques like SmoothQuant and group-wise scaling mitigate degradation but can introduce task-specific trade-offs
- Mechanism: SmoothQuant redistributes quantization error across activations, reducing peak error in sensitive dimensions. Group-wise scaling adapts quantization granularity to local parameter variance, improving preservation of underrepresented features. However, both can alter model behavior in ways that help some tasks while hurting others
- Core assumption: The quantization strategy's mitigation effect depends on the task's sensitivity to activation magnitude and variance
- Evidence anchors:
  - [section 4.5]: "Group-Wise scaling greatly improves over column-wise W4, recovering over 6 percentage points lost on MGSM for Ltn/IE languages"
  - [section 4.5]: "SmoothQuant has a similar effect on average and for mMMLU, though to a lesser degree"
  - [corpus]: Weak evidence—no quantitative comparison of mitigation strategies across tasks
- Break condition: If all tasks were equally sensitive to quantization error distribution, this mechanism would not hold

## Foundational Learning

- **Concept**: Low-bit quantization and its impact on model weights and activations
  - Why needed here: To understand how reducing precision from FP16 to 8-bit or 4-bit alters the model's representational capacity and affects downstream performance
  - Quick check question: What is the difference between weight-only and weight-and-activation quantization in terms of memory and compute savings?

- **Concept**: Data distribution imbalance in multilingual models
  - Why needed here: To recognize why underrepresented languages suffer more from quantization—due to sparser gradients and fewer training examples
  - Quick check question: How might the amount of training data for a language correlate with its sensitivity to quantization?

- **Concept**: Task complexity and quantization sensitivity
  - Why needed here: To grasp why certain tasks (e.g., mathematical reasoning) degrade faster under quantization than others (e.g., translation)
  - Quick check question: Why might tasks requiring precise numerical reasoning be more vulnerable to quantization noise than general language tasks?

## Architecture Onboarding

- **Component map**: Prompts in multiple languages → Model (quantized or FP16) → Output → Evaluation metric (automatic/human) → Aggregate results
- **Critical path**: Prompt → Model (quantized or FP16) → Output → Evaluation metric → Aggregate results
- **Design tradeoffs**:
  - Precision vs. efficiency: Lower bits save memory and speed but hurt accuracy more, especially for underrepresented languages
  - Granularity: Group-wise scaling vs. column-wise scaling affects how well local parameter variance is preserved
  - Evaluation method: Automatic metrics may underestimate real-world degradation; human evaluation is more accurate but costly
- **Failure signatures**:
  - Automatic metrics show minor degradation but human evaluation shows large drops
  - Non-Latin script languages degrade faster than Latin-script languages
  - Mathematical reasoning tasks show disproportionate degradation
- **First 3 experiments**:
  1. Run W8A8 vs. FP16 on a balanced multilingual dataset to isolate quantization's effect on language-agnostic tasks
  2. Compare group-wise vs. column-wise scaling on MGSM to quantify numerical reasoning sensitivity
  3. Apply SmoothQuant and measure cross-lingual language confusion performance to detect task-specific side effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does quantization impact performance on languages that are not present in the training data at all?
- Basis in paper: [inferred] The authors note that performance deterioration is likely even larger for languages not in the pre-training data, but cannot evaluate this due to poor availability of benchmark data and human annotators
- Why unresolved: The study focused on languages supported by the evaluated models (20+ languages), making it impossible to test completely unseen languages
- What evidence would resolve it: Direct evaluation of quantized models on languages with zero training data exposure, using either machine-translated benchmarks or human annotators fluent in these languages

### Open Question 2
- Question: Why does quantization occasionally improve performance in some cases (e.g., W8A8 on the 35B model)?
- Basis in paper: [explicit] The authors note that quantization occasionally brings performance benefits, similar to findings on sparsity, but do not investigate the underlying causes
- Why unresolved: The paper observes this phenomenon but does not provide theoretical explanation or investigate which model architectures or data characteristics might make this more likely
- What evidence would resolve it: Systematic experiments varying model architecture, data distribution, and quantization parameters to identify conditions that produce performance gains rather than degradation

### Open Question 3
- Question: How do different quantization strategies (SmoothQuant vs. Group-Wise scaling) differentially affect the ability to maintain language consistency during generation?
- Basis in paper: [explicit] The authors observe that mitigation strategies have different effects on cross-lingual language confusion tasks, with SmoothQuant recovering all lost performance from naive W8A8 while Group-Wise scaling is actively damaging
- Why unresolved: The study identifies these differential effects but does not investigate the mechanisms by which lower-precision might affect language maintenance during decoding
- What evidence would resolve it: Detailed analysis of attention patterns and token distribution during generation across different quantization strategies, specifically measuring language switching behavior during text production

## Limitations
- Exact training data composition and size for underrepresented languages remains unknown, preventing definitive causal attribution between data scarcity and quantization sensitivity
- LLM-as-a-Judge evaluation relies on prompts and completions that are not fully specified, introducing potential variability in the evaluation process
- Human evaluation covers only 5 primary languages with 300 prompts per language, limiting generalizability to the full 23-language spectrum

## Confidence

- **High confidence**: The finding that quantization disproportionately harms non-Latin script languages and mathematical reasoning tasks
- **Medium confidence**: The mechanism that quantization-induced numerical noise particularly affects tasks requiring precise numerical computation
- **Medium confidence**: The effectiveness of quantization mitigation strategies (SmoothQuant, group-wise scaling) in recovering performance

## Next Checks
1. **Quantization-aware training comparison**: Replicate the study using quantization-aware training (QAT) instead of post-training quantization (PTQ) to determine if training-time quantization strategies can better preserve multilingual performance, particularly for underrepresented languages

2. **Detailed ablation of quantization error**: Instrument quantized models to measure attention weight variance and token embedding distance during mathematical reasoning tasks, establishing direct correlation between quantization error magnitude and task-specific degradation

3. **Cross-linguistic transferability analysis**: Design a controlled experiment varying training data quantity for a single language while keeping model architecture constant, measuring how quantization sensitivity changes with data volume to isolate the data scarcity mechanism