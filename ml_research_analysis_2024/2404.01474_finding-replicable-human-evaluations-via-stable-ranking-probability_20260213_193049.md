---
ver: rpa2
title: Finding Replicable Human Evaluations via Stable Ranking Probability
arxiv_id: '2404.01474'
source_url: https://arxiv.org/abs/2404.01474
tags:
- raters
- system
- studies
- evaluation
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a meta-evaluation framework for assessing the
  stability of NLG evaluation methodologies, focusing on how rater/item configurations
  affect the replicability of system rankings. Using MQM-based MT evaluation as a
  case study, the authors collected and released a dataset of ~140,000 segment ratings
  from professional translators.
---

# Finding Replicable Human Evaluations via Stable Ranking Probability

## Quick Facts
- arXiv ID: 2404.01474
- Source URL: https://arxiv.org/abs/2404.01474
- Reference count: 7
- Key outcome: Meta-evaluation framework for assessing NLG evaluation stability through SRP metric; recommends pSxS grouping, balanced workload, Z-score normalization, and single ratings per item

## Executive Summary
This paper proposes a meta-evaluation framework for assessing the stability and replicability of NLG evaluation methodologies, focusing on how rater/item configurations affect the replicability of system rankings. Using MQM-based MT evaluation as a case study, the authors collected a dataset of ~140,000 segment ratings from professional translators and introduce "Stable Ranking Probability" (SRP) as a metric measuring the proportion of study pairs where significant system differences are preserved across replications. Through simulation experiments, they evaluate how different configurations affect stability and provide specific recommendations for maximizing replicability in human evaluations.

## Method Summary
The authors propose a meta-evaluation framework that uses simulation experiments to assess how rater/item configurations affect the replicability of system rankings. They define "Stable Ranking Probability" (SRP) as the proportion of study pairs where significant system differences are preserved across replications. Using two MQM datasets for English-German and English-Chinese MT (totaling ~140,000 segment ratings from professional translators), they conduct simulations varying item grouping methods (pSxS, system-balanced, no grouping), workload distribution (fully balanced, entropy-balanced), normalization techniques (unnormalized, mean-normalized, error-normalized, Z-score-normalized), sample size, and ratings per item. The simulation engine tests different configurations to determine their impact on SRP, providing evidence-based recommendations for stable evaluation methodologies.

## Key Results
- Pseudo-side-by-side (pSxS) grouping provides the highest stability by assigning all system outputs on the same document to the same raters
- Fully balanced workload distribution is preferred when raters have similar behavior, though noise can affect this
- Z-score normalization provides the most stability, especially in imbalanced or ungrouped settings
- Single ratings per item are recommended to maximize distinct items within budget constraints
- Document count effects vary by dataset, so no universal recommendation is made

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-side-by-side (pSxS) grouping maximizes stability by ensuring that all system outputs from the same document are evaluated by the same rater.
- Mechanism: By assigning all system outputs for a given document to the same rater, pSxS controls for document-level noise and rater-specific preferences, reducing variability in system rankings.
- Core assumption: Raters have consistent preferences and behaviors when evaluating outputs from the same document.
- Evidence anchors:
  - [abstract] "we strongly recommend use of the pSxS methodology."
  - [section 7.1] "grouping items using the pSxS methodology... provides a massive stability boost over no grouping at all document counts, approximately doubling stability in the English-German case."
  - [corpus] Weak corpus support: neighbor papers discuss evaluation stability but do not directly support pSxS grouping.
- Break condition: If raters' preferences vary significantly across documents, pSxS grouping could amplify biases rather than reduce them.

### Mechanism 2
- Claim: Fully balanced workload distribution improves stability by preventing results from being skewed toward any particular rater.
- Mechanism: Equal distribution of items among raters ensures that no single rater's behavior disproportionately influences the final system ranking, leading to more consistent results across replications.
- Core assumption: Raters have similar levels of expertise and reliability.
- Evidence anchors:
  - [section 3] "We recommend that all raters be given an equal share of the total rating workload."
  - [section 7.2] "English-German results favoring fully balanced studies... we believe that low document-level ranking agreement is a sign of excessive noise, and when it is mitigated, fully balanced studies yield higher stability."
  - [corpus] Limited corpus support: neighbor papers discuss workload but not specifically balanced distribution.
- Break condition: If some raters are significantly more reliable or have higher expertise, equal distribution may dilute the impact of better raters.

### Mechanism 3
- Claim: Z-score normalization provides the most stability, especially in imbalanced or ungrouped settings, by standardizing rater scores.
- Mechanism: Normalizing scores to have mean zero and unit variance across raters removes the influence of individual rater leniency or harshness, leading to more comparable scores.
- Core assumption: Raters' score distributions are roughly normal and can be standardized.
- Evidence anchors:
  - [section 3] "We weakly recommend applying rater-wise Z-score normalization."
  - [section 7.3] "Z-score-normalized studies exhibit higher stability than others, but the magnitude of the difference is not always large."
  - [corpus] No direct corpus evidence for Z-score normalization in this context.
- Break condition: If rater score distributions are not normal or have extreme outliers, Z-score normalization may not effectively standardize scores.

## Foundational Learning

- Concept: Statistical significance testing
  - Why needed here: To determine if differences between system rankings are meaningful and not due to chance.
  - Quick check question: What p-value threshold is used to determine statistical significance in this study?

- Concept: Random permutation tests
  - Why needed here: To assess the stability of system rankings without assuming a specific distribution of the data.
  - Quick check question: How many permutations are used in the random permutation test for significance?

- Concept: Kendall's Tau correlation
  - Why needed here: To measure the agreement between rater pairs on system rankings.
  - Quick check question: What does a high Kendall's Tau value indicate about the agreement between two raters?

## Architecture Onboarding

- Component map: Dataset collection module -> Simulation engine -> Stability metric calculator (SRP) -> Recommendation engine
- Critical path: Dataset collection → Simulation setup (configuring features like item grouping, workload distribution, normalization) → Running simulations → Calculating SRP → Analyzing results to form recommendations
- Design tradeoffs: Balancing the number of items rated against the number of ratings per item; choosing between simplicity (pSxS) and potential gains from more complex configurations
- Failure signatures: High variance in SRP across simulations may indicate issues with dataset quality or simulation parameters; low overall SRP suggests the methodology is not stable
- First 3 experiments:
  1. Test the impact of pSxS grouping on SRP by comparing it to no grouping and system-balanced grouping
  2. Evaluate the effect of fully balanced vs. entropy-balanced workload distribution on SRP
  3. Assess the stability gains from applying Z-score normalization in various configurations

## Open Questions the Paper Calls Out

- How does the "Stable Ranking Probability" metric perform when comparing systems with very small quality differences versus large quality differences?
- Would the recommendation of single ratings per item hold in evaluation frameworks other than MQM for MT, such as direct assessment or Likert-style scoring?
- How does the stability of system rankings change when using larger pools of raters with less expertise than the professional translators used in this study?

## Limitations

- The findings are based on professional translator behavior and may not extend to crowdworkers or domain experts in other fields
- The simulation assumes rater behavior is stable across replications, which may not hold due to learning effects or fatigue
- Generalizability to non-MT evaluation tasks (summarization, dialogue) remains uncertain

## Confidence

- **High**: The superiority of pSxS grouping methodology
- **Medium**: The recommendation for fully balanced workload distribution
- **Medium**: The benefit of Z-score normalization

## Next Checks

1. Conduct a real-world replication study using the pSxS methodology with new MT systems to verify if the predicted stability improvements materialize in practice
2. Test the proposed configurations (pSxS grouping, balanced distribution, Z-score normalization) on non-MT evaluation tasks to assess generalizability
3. Perform a sensitivity analysis varying rater quality differences to determine when balanced vs. entropy-balanced distribution becomes preferable