---
ver: rpa2
title: 'Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges'
arxiv_id: '2406.12624'
source_url: https://arxiv.org/abs/2406.12624
tags:
- judge
- human
- alignment
- arxiv
- judges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of large language models
  (LLMs) as evaluators, focusing on their alignment with human judgments. Using the
  TriviaQA benchmark, the research evaluates 13 judge models across 9 exam-taker models,
  analyzing their scoring accuracy and consistency.
---

# Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges

## Quick Facts
- arXiv ID: 2406.12624
- Source URL: https://arxiv.org/abs/2406.12624
- Authors: Aman Singh Thakur; Kartik Choudhary; Venkat Srinik Ramayapally; Sankaran Vaidyanathan; Dieuwke Hupkes
- Reference count: 40
- Primary result: Even top-performing judge models differ by up to 5 points from human scores despite high alignment metrics

## Executive Summary
This study investigates the reliability of large language models as evaluators, focusing on their alignment with human judgments. Using the TriviaQA benchmark, the research evaluates 13 judge models across 9 exam-taker models, analyzing their scoring accuracy and consistency. While larger models like GPT-4 Turbo, Llama-3, and Llama-3.1 show better alignment with humans, even these top performers differ by up to 5 points from human-assigned scores. Smaller models and lexical metrics like "contains" provide reasonable rankings despite lower alignment scores. The study reveals vulnerabilities in judge models, including sensitivity to prompt complexity, length, and a tendency toward leniency. The findings emphasize the need for robust alignment metrics beyond simple percent agreement and highlight the importance of caution when deploying LLM judges in complex evaluation scenarios.

## Method Summary
The study uses 400 questions from the TriviaQA validation set, with exam-taker models (Llama-2, Mistral, GPT-4 Turbo) generating responses. Judge models (Llama-2, Llama-3, Llama-3.1, Mistral, Gemma, JudgeLM, GPT-4) evaluate each response as "correct" or "incorrect" against reference answers. Human annotations serve as ground truth. The research calculates alignment using Scott's Pi and percent agreement, analyzes error patterns through precision, recall, and false positive/negative rates, and assesses ranking consistency via Spearman's rank correlation. Baseline methods include exact match and contains match lexical approaches.

## Key Results
- Only the largest judge models (GPT-4 Turbo, Llama-3 70B, Llama-3.1 70B) achieve reasonable alignment with humans (Scott's π in high 80s)
- Scott's π distinguishes judges better than percent agreement, revealing systematic biases in simpler metrics
- Judge models show leniency bias, tending to mark ambiguous responses as correct (P+ significantly above 0.5)
- Even well-aligned judges can differ by up to 5 points from human scores on average
- Smaller judge models provide reasonable rankings despite lower absolute alignment scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-as-a-judge performance improves with model size due to increased recall and reduced false negatives
- Mechanism: Larger models have better semantic understanding and context tracking, allowing them to correctly identify more valid answers and fewer incorrect ones
- Core assumption: Model size correlates with ability to understand nuanced semantic equivalence
- Evidence anchors:
  - [abstract] "only the best (and largest) models achieve reasonable alignment with humans"
  - [section 4.1] "Scott's π values are in the high 80s for Llama-3 70B, Llama-3.1 70B and GPT-4 Turbo"
- Break condition: When semantic equivalence becomes too subtle for even large models to capture reliably

### Mechanism 2
- Claim: Scott's π is more discriminative than percent agreement for evaluating judge models
- Mechanism: Scott's π accounts for chance agreement by considering the marginal distributions of ratings, making it sensitive to systematic biases
- Core assumption: Chance agreement is significant when evaluating categorical responses
- Evidence anchors:
  - [abstract] "Scott'sπ distinguishes judges better than percent alignment"
  - [section 4.1] "Scott's π appears to be better able of discriminating various judge models"
- Break condition: When both raters use identical rating distributions, making chance agreement equal to observed agreement

### Mechanism 3
- Claim: Judge models exhibit leniency bias, tending to mark ambiguous responses as correct
- Mechanism: When judges lack perfect alignment with human evaluation criteria, they default to marking responses as correct to avoid penalizing potentially valid answers
- Core assumption: Judges prioritize avoiding false negatives over false positives when uncertain
- Evidence anchors:
  - [abstract] "sensitivity to prompt complexity and a bias toward leniency"
  - [section 5.4] "P+ for most models is significantly higher than 0.5, indicating a tendency of the judge models to evaluate responses as 'correct'"
- Break condition: When explicit evaluation criteria are provided that leave no ambiguity

## Foundational Learning

- Concept: Inter-rater reliability metrics
  - Why needed here: To quantify how well judge models align with human evaluators
  - Quick check question: What's the difference between percent agreement and Scott's π when two raters always agree on 80% of items but one rater marks 90% of items as correct?

- Concept: Precision and recall in evaluation context
  - Why needed here: To understand the types of errors judge models make (false positives vs false negatives)
  - Quick check question: If a judge model has high precision but low recall, what type of error is it making more frequently?

- Concept: Spearman rank correlation
  - Why needed here: To assess whether judges can rank models correctly even if absolute scores differ
  - Quick check question: What does a Spearman correlation of 0.95 between judge rankings and human rankings indicate about their agreement?

## Architecture Onboarding

- Component map: Judge models (LLM-as-a-judge) -> Exam-taker models -> Evaluation metrics -> Baseline methods
- Critical path: 1) Generate exam-taker model responses to TriviaQA questions 2) Have judge models evaluate each response against reference answers 3) Compare judge evaluations to human annotations 4) Calculate alignment metrics and error analysis
- Design tradeoffs:
  - Larger judge models provide better alignment but at higher computational cost
  - Percent agreement is simpler but less discriminative than Scott's π
  - Focusing on ranking vs absolute scoring affects which models are suitable
- Failure signatures:
  - High percent agreement but low Scott's π indicates systematic bias
  - Judge models marking "Yes" or "Sure" as correct responses
  - Inconsistent judgments when reference order is shuffled
- First 3 experiments:
  1. Run a small subset (50 questions) through one judge model vs human evaluation to verify pipeline
  2. Test prompt sensitivity by running same evaluations with different instruction prompt lengths
  3. Compare judge model performance on base vs instruction-tuned exam-taker pairs to understand knowledge unlearning effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-as-a-judge performance patterns generalize to more complex evaluation scenarios beyond simple knowledge benchmarks?
- Basis in paper: [inferred] The paper focuses on a "clean scenario" with high human agreement and suggests caution when applying judge models to more complex setups, but does not empirically test this claim
- Why unresolved: The study deliberately chose a simple task to isolate judge model performance from task ambiguity, leaving open whether results would hold in scenarios with lower human agreement, more subjective criteria, or multi-step reasoning tasks
- What evidence would resolve it: Comparative studies testing the same judge models across tasks with varying levels of human agreement, complexity, and subjectivity would reveal whether alignment metrics and performance patterns observed in simple benchmarks transfer to complex evaluation scenarios

### Open Question 2
- Question: What specific aspects of prompt design most significantly impact judge model alignment with human evaluators?
- Basis in paper: [explicit] The paper found that judge models show varying sensitivity to prompt length and specificity, with some models losing alignment when instructions become more complex, but did not systematically isolate which prompt elements matter most
- Why unresolved: While the paper tested different prompt versions, it did not conduct controlled experiments varying individual prompt components (e.g., evaluation criteria specificity, example inclusion, instruction complexity) to determine their relative impact on judge performance
- What evidence would resolve it: A factorial design experiment systematically varying individual prompt elements while keeping others constant would identify which specific prompt characteristics most strongly influence judge model alignment and consistency

### Open Question 3
- Question: Can we develop more robust alignment metrics that better predict judge model reliability than Scott's Pi or percent agreement?
- Basis in paper: [explicit] The authors found that even judges with high alignment scores (Scott's Pi > 80) can differ by up to 5 points from human scores, and that percent agreement poorly discriminates between judges, suggesting current metrics may not fully capture judge reliability
- Why unresolved: The study used existing alignment metrics but did not explore alternative formulations or additional metrics that might better capture the practical utility of judge models, particularly their ability to rank models consistently rather than assign accurate absolute scores
- What evidence would resolve it: Developing and validating new metrics that incorporate both alignment and consistency measures, then testing their predictive power for judge model performance across diverse evaluation tasks would identify whether better metrics exist

## Limitations
- Reliance on a single benchmark (TriviaQA) may limit generalizability to other domains
- Human annotations serve as ground truth but may contain subjectivity and inconsistency
- Study doesn't fully explore impact of judge model prompt engineering on evaluation outcomes
- Potential circularity in using LLM-based judges to evaluate other LLMs

## Confidence
- High confidence: Larger models (GPT-4 Turbo, Llama-3, Llama-3.1) show better alignment with human judgments
- Medium confidence: Judge models exhibit leniency bias toward ambiguous responses
- Low confidence: Scott's π is universally superior to percent agreement for discriminating judge models

## Next Checks
1. **Cross-domain validation:** Test the same judge models on non-TriviaQA benchmarks (e.g., MMLU, GSM8K) to assess whether model size correlation with human alignment holds across different knowledge domains and question formats

2. **Prompt engineering analysis:** Systematically vary judge model prompts (instruction length, specificity, evaluation criteria) to quantify the impact on alignment metrics and identify optimal prompt configurations for different judge model sizes

3. **Human-in-the-loop calibration:** Implement a calibration protocol where judge models are fine-tuned or prompted based on feedback from human evaluation disagreements to measure potential improvements in alignment scores and reduction in systematic biases