---
ver: rpa2
title: 'First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model
  Reasoning'
arxiv_id: '2406.16078'
source_url: https://arxiv.org/abs/2406.16078
tags:
- than
- more
- apples
- grapes
- bananas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) reason
  through multi-step problems by analyzing their use of heuristics and rational strategies.
  Through controlled experiments with arithmetic reasoning tasks, the authors find
  that LLMs rely more heavily on shallow heuristics (such as lexical overlap with
  the question) in the early stages of reasoning when the goal is distant, but gradually
  shift to more rational, goal-oriented strategies as they approach the answer.
---

# First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning

## Quick Facts
- arXiv ID: 2406.16078
- Source URL: https://arxiv.org/abs/2406.16078
- Reference count: 40
- Key outcome: LLMs shift from heuristic to rational reasoning strategies as they approach answers in multi-step problems

## Executive Summary
This paper investigates how large language models (LLMs) reason through multi-step problems by analyzing their use of heuristics and rational strategies. Through controlled experiments with arithmetic reasoning tasks, the authors find that LLMs rely more heavily on shallow heuristics (such as lexical overlap with the question) in the early stages of reasoning when the goal is distant, but gradually shift to more rational, goal-oriented strategies as they approach the answer. This dynamic transition suggests LLMs have limited capacity to track future reasoning paths and must combine heuristic shortcuts with rational planning to solve complex problems.

## Method Summary
The authors conducted controlled experiments using the GSM8K dataset and artificial datasets with 4-step and 5-step arithmetic reasoning tasks. They implemented chain-of-thought prompting with 4-shot examples to find minimal solutions, then measured the frequency of selecting distractors that matched three heuristic types (lexical overlap, positional, negation) during step-by-step reasoning. The experiments analyzed how selection patterns varied based on distance to answer and tested whether models could be influenced to use more heuristic-biased reasoning through few-shot examples.

## Key Results
- LLMs rely more heavily on shallow heuristics in earlier reasoning stages when more steps remain to reach the goal
- Reliance on heuristics decreases as models progress closer to the final answer
- Models exhibit specific heuristic biases including lexical overlap, positional bias, and negative avoidance in reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models dynamically shift from heuristic-based to rational reasoning as they approach the answer in multi-step tasks.
- Mechanism: At the start of reasoning, when the goal is distant (high d), models rely on superficial heuristics like lexical overlap with the question or positional biases. As reasoning progresses and the distance to the goal decreases (d approaches 0), reliance on heuristics decreases and rational, goal-directed strategies increase.
- Core assumption: Models have limited capacity to track or plan future reasoning steps, so they must combine heuristics and rational strategies dynamically.
- Evidence anchors:
  - [abstract] "LLMs rely more heavily on shallow heuristics...in the earlier stages of reasoning, where more reasoning steps remain to reach a goal. Conversely, their reliance on heuristics decreases as LMs progress closer to the final answer"
  - [section] "we hypothesized that the more distant the current reasoning step is from the answer (higher d), the more heavily models rely on heuristics"
- Break condition: If models could perfectly plan the full solution path in advance, they would not need to switch between heuristics and rational strategies.

### Mechanism 2
- Claim: Models exhibit specific heuristic biases (lexical overlap, positional, negative avoidance) in reasoning tasks.
- Mechanism: Models select premises based on superficial cues rather than semantic relevance, such as choosing premises with person names matching the question (lexical overlap), selecting first or last items (position), or avoiding negation words (negative avoidance).
- Core assumption: Neural models are prone to shortcut learning and superficial pattern matching, especially under chain-of-thought prompting.
- Evidence anchors:
  - [section] "LLMs rely on shallow heuristics more frequently in the earlier phase of multi-step reasoning...LLMs tend to rely on superficial, shallow similarity of texts when considering their associations"
- Break condition: If task design eliminates superficial cues or forces semantic matching, models must rely on rational reasoning.

### Mechanism 3
- Claim: Heuristic exploitation is task- and step-dependent; models are more likely to use heuristics when fewer future steps are predictable.
- Mechanism: When the number of remaining reasoning steps is large, models cannot reliably predict the necessary future information, so they default to heuristic shortcuts. As they near the goal, the future path becomes clearer, enabling rational selection.
- Core assumption: Limited lookahead capacity forces models to use heuristics when the solution path is uncertain.
- Evidence anchors:
  - [section] "the more distant the current step is from the answer (larger d), the more frequently the distractor is selected (larger r), which is typically above the chance rate"
- Break condition: If models had unlimited or very large lookahead, they could plan the entire path rationally without heuristics.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: The paper studies how LLMs solve multi-step reasoning tasks using chain-of-thought instruction, which is central to understanding their reasoning strategy.
  - Quick check question: What is chain-of-thought prompting and how does it differ from direct answer generation?

- Concept: Heuristic vs rational reasoning
  - Why needed here: The paper contrasts heuristic shortcuts (superficial cues) with rational, goal-directed strategies; understanding this distinction is key to interpreting results.
  - Quick check question: Give an example of a heuristic reasoning shortcut and a rational reasoning strategy in a math problem.

- Concept: Multi-hop reasoning
  - Why needed here: The experiments involve tasks requiring multiple reasoning steps (e.g., arithmetic word problems), so understanding how information must be combined across steps is essential.
  - Quick check question: What makes a reasoning task "multi-hop" and why is it challenging for LLMs?

## Architecture Onboarding

- Component map: LLM inference pipeline → Prompting interface → Step-by-step reasoning loop → Fact state tracker → Heuristic detection module → Output selector
- Critical path: Prompt → Step 1 reasoning → Fact state update → Step 2 reasoning → ... → Final answer
- Design tradeoffs: Heuristic detection vs rational planning accuracy; prompt complexity vs model capability; controlled distractor design vs natural dataset realism
- Failure signatures: Over-reliance on lexical overlap leading to wrong premises; failure to backtrack when heuristic path fails; inconsistent performance across different heuristic types
- First 3 experiments:
  1. Confirm baseline heuristic bias: Run model on controlled datasets with added distractors for each heuristic and measure selection frequency.
  2. Measure heuristic shift: Vary the distance to answer (d) and measure distractor selection ratio at each step.
  3. Test heuristic persistence: Replace few-shot examples with heuristic-biased examples and observe if model behavior changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different reasoning strategies (heuristic vs. rational) interact with specific task complexities in language models?
- Basis in paper: [explicit] The paper investigates how LLMs rely on heuristics in early reasoning stages and shift to rational strategies as they approach the answer.
- Why unresolved: The study focuses on controlled artificial data and does not explore how varying task complexities (e.g., number of steps, type of reasoning required) might affect the balance between heuristic and rational strategies.
- What evidence would resolve it: Experiments testing LLM performance on tasks with varying complexity levels, comparing the frequency and effectiveness of heuristic vs. rational strategies across different problem types.

### Open Question 2
- Question: What are the underlying mechanisms that cause LLMs to switch from heuristic to rational reasoning strategies?
- Basis in paper: [inferred] The paper observes that LLMs dynamically switch strategies but does not explain the cognitive or computational mechanisms driving this transition.
- Why unresolved: The study analyzes model outputs but does not investigate internal model states or training data patterns that might explain the observed behavior.
- What evidence would resolve it: Analysis of model activations, attention patterns, or training data statistics that correlate with the observed strategy shifts.

### Open Question 3
- Question: How generalizable are the findings across different LLM architectures and reasoning tasks?
- Basis in paper: [explicit] The paper tests four LLM variants on arithmetic reasoning tasks but acknowledges limitations in model and task coverage.
- Why unresolved: The study's controlled experiments may not reflect real-world reasoning scenarios or capture differences between model architectures.
- What evidence would resolve it: Testing the same hypotheses across diverse LLM architectures, reasoning task types, and real-world problem domains to assess consistency of findings.

## Limitations
- The study focuses exclusively on arithmetic reasoning tasks with highly controlled, artificial distractors, limiting generalizability to other reasoning domains
- The paper's claim about "limited capacity to track future reasoning paths" is asserted rather than directly tested through internal state analysis
- The paper relies heavily on behavioral observations without examining internal model states or attention patterns that could reveal why models make heuristic choices

## Confidence
- High Confidence: The observation that LLMs show differential reliance on heuristics across reasoning steps is well-supported by the experimental data
- Medium Confidence: The interpretation that this pattern reflects a "dynamic transition from heuristic to rational reasoning" is plausible but requires additional assumptions
- Low Confidence: The broader claims about cognitive plausibility and implications for LLM reasoning limitations extend beyond what the experimental evidence directly supports

## Next Checks
1. **Cross-domain replication**: Test whether the heuristic-rational transition pattern appears in non-arithmetic reasoning tasks (e.g., logical puzzles, commonsense reasoning, or multi-hop QA) to assess generalizability beyond the controlled arithmetic setting.

2. **Ablation of lookahead capacity**: Design experiments that explicitly manipulate the model's ability to plan ahead (e.g., through different prompt structures or intermediate subgoal instructions) to test whether limited lookahead is the actual mechanism driving heuristic use.

3. **Internal state analysis**: Examine attention weights, activation patterns, or intermediate reasoning states to identify what features models actually attend to when selecting premises, rather than relying solely on behavioral proxy measures of heuristic use.