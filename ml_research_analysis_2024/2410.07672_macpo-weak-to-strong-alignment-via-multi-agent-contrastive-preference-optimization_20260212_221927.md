---
ver: rpa2
title: 'MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization'
arxiv_id: '2410.07672'
source_url: https://arxiv.org/abs/2410.07672
tags:
- weak
- alignment
- teachers
- strong
- macpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning strong large language
  models (LLMs) using weak supervision, a scenario where human supervisors are outperformed
  by the models they are trying to align. Existing alignment methods, designed for
  strong-to-weak or self-alignment settings, struggle in this weak-to-strong alignment
  setting due to noise and potential collapse.
---

# MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization

## Quick Facts
- arXiv ID: 2410.07672
- Source URL: https://arxiv.org/abs/2410.07672
- Reference count: 40
- Primary result: MACPO achieves state-of-the-art weak-to-strong alignment on helpfulness and harmlessness tasks without collapse

## Executive Summary
This paper addresses the challenge of aligning strong large language models (LLMs) using weak supervision, where human supervisors are outperformed by the models they are trying to align. Existing alignment methods struggle in this weak-to-strong setting due to noise and potential collapse. The authors propose MACPO (Multi-Agent Contrastive Preference Optimization), a framework that facilitates mutual learning between weak teachers and strong students through two key strategies: mutual positive behavior augmentation and hard negative behavior construction. Experiments on HH-RLHF and PKU-SafeRLHF datasets demonstrate that MACPO significantly outperforms state-of-the-art baselines in both automatic and human evaluations.

## Method Summary
MACPO is a multi-agent contrastive preference optimization framework that aligns strong LLMs using weak supervision. It initializes weak teachers on ground truth data and a strong student on weak teacher outputs, then iteratively trains all agents using a combination of direct preference optimization (DPO) and supervised fine-tuning (SFT). The method filters weak teacher outputs using strong student perplexity to identify high-quality positive behaviors, while negative agents fine-tuned on negative behavioral data generate hard negatives. This process repeats for multiple rounds, with both weak teachers and strong students learning from each other's positive behaviors while being penalized for generating familiar negative behaviors.

## Key Results
- MACPO outperforms state-of-the-art baselines in both automatic third-party reward model scores and GPT-4 pairwise evaluations on HH-Helpful and PKU-SafeRLHF datasets
- Increasing the number of weak teachers improves MACPO's performance through more iterative optimization rounds
- MACPO achieves better alignment performance than self-alignment methods without the collapse typically seen in iterative training

## Why This Works (Mechanism)

### Mechanism 1: Mutual Positive Behavior Augmentation
- Claim: Weak teachers and strong students improve each other's alignment performance by iteratively exchanging high-quality positive behaviors.
- Mechanism: Weak teachers generate behavior, which the strong student filters based on perplexity to identify high-quality positive examples. The strong student then generates its own positive behavior, which the weak teachers directly adopt. This mutual exchange provides both agents with unfamiliar positive behaviors to reinforce.
- Core assumption: Different agents possess complementary knowledge, making each other's positive behaviors unfamiliar and thus valuable for learning.
- Evidence anchors: [abstract], [section], [corpus] Weak-to-Strong Generalization studies

### Mechanism 2: Hard Negative Behavior Construction
- Claim: Agents generate familiar negative behaviors by fine-tuning on negative behavioral data, creating effective hard negatives for contrastive learning.
- Mechanism: Both weak teachers and strong students are fine-tuned on negative behavioral data, then prompted to generate their own negative behaviors. These self-generated negative behaviors are considered familiar and thus serve as effective hard negatives for contrastive preference optimization.
- Core assumption: Agents have different knowledge bases, so self-generated negative behaviors will be more familiar and harder to distinguish from positive behaviors.
- Evidence anchors: [abstract], [section], [corpus] Direct Preference Optimization literature

### Mechanism 3: Iterative Optimization Without Collapse
- Claim: MACPO achieves better alignment performance through multiple optimization rounds without the collapse seen in self-alignment methods.
- Mechanism: By combining unfamiliar positive behaviors from other agents with familiar negative behaviors from self-generated data, MACPO creates a balanced training signal that avoids the collapse seen when models are trained only on self-generated data.
- Core assumption: The combination of unfamiliar positives and familiar negatives provides a stable learning signal that prevents collapse.
- Evidence anchors: [abstract], [section], [corpus] Self-alignment methods like SPIN and Self-rewarding

## Foundational Learning

- **Concept: Contrastive Preference Optimization**
  - Why needed here: MACPO builds on DPO to align models using pairwise comparisons of positive and negative behaviors
  - Quick check question: How does contrastive learning differ from standard supervised learning in preference optimization?

- **Concept: Perplexity-based filtering**
  - Why needed here: Used to identify high-quality positive behaviors from weak teachers by filtering out low-quality or noisy responses
  - Quick check question: Why would a strong student's perplexity be a good indicator of weak teacher label quality?

- **Concept: Multi-agent collaboration**
  - Why needed here: MACPO requires coordination between multiple agents (weak teachers and strong students) to achieve mutual learning
  - Quick check question: What advantages does multi-agent learning offer over single-agent approaches in weak-to-strong alignment?

## Architecture Onboarding

- **Component map**: Weak teachers (K agents) -> Perplexity filtering -> Strong student; Negative agents (fine-tuned on negative data) -> Hard negatives -> DPO optimizer -> Updated weak teachers and strong student

- **Critical path**: 
  1. Initialize weak teachers on ground truth data
  2. Initialize strong student on weak teacher outputs
  3. For each iteration:
     - Generate positive behaviors from all agents
     - Filter weak teacher outputs using strong student perplexity (top-k as positive)
     - Generate negative behaviors from fine-tuned negative agents
     - Apply contrastive preference optimization to update all positive agents
  4. Repeat until convergence or maximum iterations

- **Design tradeoffs**:
  - Number of weak teachers vs. computational cost
  - Perplexity filtering threshold vs. noise tolerance
  - Number of iterations vs. risk of overfitting or collapse
  - Balance between mutual learning and self-improvement

- **Failure signatures**:
  - Alignment performance degrades after multiple iterations (collapse)
  - Strong student perplexity increases over time (learning divergence)
  - Weak teachers' performance plateaus or decreases
  - Negative agents generate behaviors that are too similar to positives

- **First 3 experiments**:
  1. Baseline comparison: Run MACPO vs. Naive SFT and Confident Loss on HH-Helpful dataset
  2. Ablation study: Test MACPO without mutual positive behavior augmentation (-MP variant)
  3. Scale test: Compare MACPO performance with 1, 2, and 3 weak teachers on PKU-SafeRLHF dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MACPO perform when aligning strong LLMs on tasks beyond helpfulness and harmlessness, such as mathematical reasoning or code generation?
- Basis in paper: [inferred] The authors acknowledge the need to expand MACPO's evaluation to other tasks like mathematical reasoning, code programming, and question answering.
- Why unresolved: The paper only evaluates MACPO on helpfulness and harmlessness alignment datasets (HH-RLHF and PKU-SafeRLHF). Its performance on other alignment tasks remains untested.
- What evidence would resolve it: Experiments applying MACPO to datasets focused on mathematical reasoning, code generation, or other alignment tasks, with comparative results against existing methods.

### Open Question 2
- Question: What are the impacts of using more diverse or adversarial methods to generate negative behavior data for MACPO?
- Basis in paper: [inferred] The authors suggest exploring more jailbreaking attack methods like adversarial prompting and adversarial decoding to induce diverse negative behavior, beyond fine-tuning on negative behavioral data.
- Why unresolved: The current implementation only uses fine-tuning on negative behavioral data to generate negative behavior. The effects of alternative methods for generating negative behavior are unexplored.
- What evidence would resolve it: Experiments comparing MACPO's performance when using different methods (e.g., adversarial prompting, adversarial decoding) to generate negative behavior data, with analysis of the resulting alignment performance.

### Open Question 3
- Question: How does the number of weak teachers in MACPO affect the diversity and quality of positive behavior, and what is the optimal number for different alignment tasks?
- Basis in paper: [explicit] The paper shows that increasing the number of weak teachers improves MACPO's performance, but it does not explore the optimal number of teachers or the relationship between teacher diversity and alignment quality.
- Why unresolved: While the paper demonstrates that more weak teachers lead to better performance, it does not investigate the point of diminishing returns or the impact of teacher diversity on the quality of generated positive behavior.
- What evidence would resolve it: Experiments varying the number of weak teachers across different alignment tasks, measuring the diversity of generated positive behavior and the corresponding alignment performance, to identify optimal configurations.

## Limitations
- The method's performance on alignment tasks beyond helpfulness and harmlessness remains untested
- The impact of using diverse or adversarial methods to generate negative behavior data is unexplored
- The optimal number of weak teachers for different alignment tasks is unknown

## Confidence

- **Mechanism effectiveness**: Medium - The theoretical framework is sound but potential failure modes like model collapse over many iterations aren't fully addressed
- **Experimental results**: Medium - Results are encouraging but limited to two specific datasets and tasks
- **Generalization to other domains**: Low - Performance on mathematical reasoning, code generation, or other alignment tasks is untested
- **Scalability to larger models**: Low - Experiments only use models up to 70B parameters

## Next Checks

1. **Long-term stability test**: Run MACPO for 10+ optimization rounds on the HH-Helpful dataset to verify that the method doesn't collapse or degrade over time, measuring reward scores and perplexity at each iteration.

2. **Noise robustness evaluation**: Introduce varying levels of noise (10-50%) into weak teacher supervision data and measure how MACPO's performance degrades compared to baseline methods across both HH-Helpful and PKU-SafeRLHF datasets.

3. **Cross-domain generalization**: Apply MACPO to a different alignment task (e.g., instruction following or bias mitigation) using models of different scales (3b, 13b, 70b) to test whether the mutual learning benefits transfer beyond the tested helpfulness/harmlessness scenarios.