---
ver: rpa2
title: Dialectical Behavior Therapy Approach to LLM Prompting
arxiv_id: '2410.07768'
source_url: https://arxiv.org/abs/2410.07768
tags:
- prompting
- reasoning
- prompts
- datasets
- solve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using DBT techniques to improve LLM reasoning.
  The DBT-inspired prompts guide models to apply Wise Mind, Observation, Description,
  and Effectiveness skills when solving tasks.
---

# Dialectical Behavior Therapy Approach to LLM Prompting

## Quick Facts
- arXiv ID: 2410.07768
- Source URL: https://arxiv.org/abs/2410.07768
- Authors: Oxana Vitman; Nika Amaglobeli; Paul Plachinda
- Reference count: 7
- Primary result: DBT-inspired prompts improve reasoning accuracy for smaller LLMs (up to 7% gain on StrategyQA)

## Executive Summary
This paper introduces a novel approach to improving large language model reasoning by applying Dialectical Behavior Therapy (DBT) techniques to prompt engineering. The researchers developed prompts that incorporate DBT skills—Wise Mind, Observation, Description, and Effectiveness—to guide models through more systematic problem-solving. Their experiments across multiple reasoning datasets and model sizes demonstrate that DBT prompting significantly enhances performance for smaller models (8B-14B parameters) while showing minimal impact on larger models like GPT-3.5-turbo.

The work represents an innovative intersection between psychological therapy frameworks and AI system design. By structuring prompts around DBT's evidence-based techniques for emotional regulation and cognitive processing, the researchers created a prompting strategy that helps models engage more deliberately with complex reasoning tasks. The approach is particularly valuable for applications requiring compact models where computational resources are limited.

## Method Summary
The researchers developed DBT-inspired prompts incorporating four key skills: Wise Mind (balancing emotional and rational thinking), Observation (noticing problem details), Description (articulating observations), and Effectiveness (implementing solutions). They tested these prompts across four reasoning datasets—SVAMP (arithmetic), AQUA (algebraic word problems), GSM8K (math word problems), and StrategyQA (commonsense reasoning)—using llama3 (8B parameters), phi3:medium (14B parameters), and GPT-3.5-turbo Instruct models. The DBT prompts were compared against baseline methods including zero-shot prompting, chain-of-thought, and plan-and-solve approaches. The team measured accuracy improvements and analyzed performance differences across model sizes.

## Key Results
- DBT prompting achieved up to 7% accuracy gains on StrategyQA compared to baseline methods
- phi3:medium showed a 16.2% improvement on StrategyQA with DBT prompting
- Smaller models (8B-14B parameters) showed consistent improvements across all datasets
- Larger models like GPT-3.5-turbo showed minimal improvement or decline with DBT prompts

## Why This Works (Mechanism)
The DBT framework provides a structured approach to cognitive processing that helps models engage more systematically with reasoning tasks. By explicitly prompting for Wise Mind (balancing intuitive and analytical thinking), Observation (gathering relevant information), Description (organizing and articulating problem elements), and Effectiveness (implementing solutions), the prompts guide models through a more comprehensive problem-solving process. This structured approach appears to compensate for limitations in smaller models' reasoning capabilities by providing clearer cognitive scaffolding.

## Foundational Learning
- DBT skills framework: Why needed - Provides psychological basis for structured reasoning; Quick check - Verify understanding of Wise Mind, Observation, Description, and Effectiveness concepts
- Prompt engineering fundamentals: Why needed - Essential for implementing DBT techniques effectively; Quick check - Confirm ability to structure prompts with clear instructions and examples
- Reasoning task categorization: Why needed - Different reasoning types may benefit differently from DBT approaches; Quick check - Classify datasets by reasoning type (arithmetic, algebraic, commonsense)

## Architecture Onboarding

**Component Map:**
DBT Skills -> Prompt Template -> Model Input -> Reasoning Process -> Output Generation

**Critical Path:**
DBT Skill Selection → Prompt Construction → Model Processing → Answer Generation → Accuracy Evaluation

**Design Tradeoffs:**
- DBT prompting vs. demonstration-based approaches: DBT offers reasoning enhancement without requiring task-specific examples
- Model size considerations: Smaller models benefit more, suggesting DBT compensates for reasoning limitations
- Prompt complexity vs. execution time: DBT prompts add structure but may increase processing requirements

**Failure Signatures:**
- Large models showing minimal or negative improvement
- Inconsistent formatting of DBT prompts across different reasoning task types
- Over-reliance on DBT structure potentially limiting creative problem-solving approaches

**First 3 Experiments to Run:**
1. Apply DBT prompting to additional 8B-14B parameter models (Mistral, Gemma) to verify generalization
2. Test DBT prompting effectiveness on logical reasoning tasks beyond the current mathematical and commonsense domains
3. Conduct ablation studies removing individual DBT skills to identify which components drive performance improvements

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Inconsistent effectiveness across model sizes, with minimal gains for larger models like GPT-3.5-turbo
- Exact demonstration examples used for CoT + demo and DBT + demo variants not fully specified, affecting reproducibility
- Limited testing on diverse reasoning task types beyond mathematical and commonsense domains

## Confidence

**High confidence:**
- DBT prompting improves reasoning accuracy for smaller models (8B-14B parameters) on multiple datasets

**Medium confidence:**
- DBT prompting is particularly effective for compact LLMs and offers a promising approach to enhance reasoning without demonstrations
- Larger models show minimal improvement or decline with DBT prompting

## Next Checks

1. Test DBT prompting on additional smaller model architectures (Mistral, Gemma) to verify whether the effectiveness generalizes beyond llama3 and phi3:medium
2. Implement and test the DBT prompting approach on in-context learning tasks to determine if the benefits extend to few-shot scenarios
3. Conduct ablation studies to identify which DBT skills (Wise Mind, Observation, Description, Effectiveness) contribute most to performance improvements on different reasoning task types