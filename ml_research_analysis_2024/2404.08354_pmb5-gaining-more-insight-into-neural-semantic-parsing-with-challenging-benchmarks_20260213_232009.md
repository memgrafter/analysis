---
ver: rpa2
title: 'PMB5: Gaining More Insight into Neural Semantic Parsing with Challenging Benchmarks'
arxiv_id: '2404.08354'
source_url: https://arxiv.org/abs/2404.08354
tags:
- test
- semantic
- computational
- linguistics
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies that the current PMB test set is too easy
  and performance scores are inflated due to data leakage from training to test. To
  address this, the authors propose a systematic split method to create a more reliable
  standard test set and introduce two challenge sets: one with longer texts and one
  with compositional generalization via CCG recombination.'
---

# PMB5: Gaining More Insight into Neural Semantic Parsing with Challenging Benchmarks

## Quick Facts
- arXiv ID: 2404.08354
- Source URL: https://arxiv.org/abs/2404.08354
- Authors: Xiao Zhang; Chunliu Wang; Rik van Noord; Johan Bos
- Reference count: 0
- Key outcome: Current PMB test set is too easy; systematic split and challenge sets reveal significant performance drops for neural models.

## Executive Summary
The paper identifies that the current PMB test set is too easy and performance scores are inflated due to data leakage from training to test. To address this, the authors propose a systematic split method to create a more reliable standard test set and introduce two challenge sets: one with longer texts and one with compositional generalization via CCG recombination. Experiments with five neural models show significant performance drops on challenge sets, revealing limitations of current neural models. For example, the best parser (byT5) drops from 91.4 F1 on the standard test to 5.5 F1 on the long text set, and the best generator drops from 71.9 BLEU to 14.1 BLEU. The results demonstrate that semantic parsing and generation tasks are far from being solved and require more challenging benchmarks for accurate evaluation.

## Method Summary
The authors implement a two-round sorting approach for systematic data splitting to reduce overlap between training and test sets. Documents are sorted by length, then grouped and reordered by edit distance, allocating 80% to training and 20% to dev/test. Two challenge sets are created: one with longer texts selected for manual annotation, and one with compositional generalization via CCG derivation tree recombination. Models are fine-tuned on the new splits and evaluated on standard and challenge sets using SMATCH F1 for parsing and BLEU/METEOR/COMET for generation.

## Key Results
- byT5 parser drops from 91.4 F1 on standard test to 5.5 F1 on long text challenge set
- Best generator drops from 71.9 BLEU to 14.1 BLEU on compositional extension challenge set
- Systematic split reduces word overlap between train and test sets compared to random splits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The systematic split reduces data leakage by lowering word overlap between training and test sets.
- Mechanism: Documents are sorted by length, then grouped and reordered by edit distance, allocating 80% to training and 20% to dev/test, ensuring dissimilar sentences end up in different sets.
- Core assumption: Edit distance reordering preserves semantic diversity while minimizing lexical overlap.
- Evidence anchors:
  - [section] "The second sorting is particularly designed to create a certain degree of separation between the datasets, aiming at decreasing the word overlap."
  - [corpus] No explicit evidence of edit distance effectiveness provided; the claim is inferred from methodology description.
- Break condition: If edit distance fails to capture semantic similarity, high-frequency function words may still leak across splits, undermining the split's reliability.

### Mechanism 2
- Claim: CCG tree recombination exposes compositional generalization limits by generating novel but syntactically valid sentence structures.
- Mechanism: Substitution replaces subtrees with equivalent CCG categories; extension adds new subtrees, both operations preserve syntactic structure while altering semantics, forcing models to generalize beyond memorization.
- Core assumption: Models trained on original PMB trees will fail to parse or generate correctly when semantic components appear in unfamiliar syntactic contexts.
- Evidence anchors:
  - [abstract] "The compositional set consists of texts formed by recombining the Combinatorial Categorical Grammar (CCG, Steedman, 1996) derivation tree... This kind of tree recombination technique has been empirically validated for semantic data augmentation by Juvekar et al. (2023)."
  - [section] "We evaluate five neural models for semantic parsing and meaning-to-text generation. Our results show that model performance declines (in some cases dramatically) on the challenge sets."
- Break condition: If the model has seen semantically similar compositions during training, performance may not degrade as expected, limiting the test's diagnostic power.

### Mechanism 3
- Claim: Byte-level tokenization (byT5) improves handling of unseen words and reduces vocabulary size, enhancing robustness on challenge sets.
- Mechanism: Unlike BPE, byte-level tokenization maps each byte to a token, enabling representation of any unseen character sequence without expanding the vocabulary.
- Core assumption: Semantic parsing tasks in PMB often involve domain-specific or rare words not well covered by standard tokenization schemes.
- Evidence anchors:
  - [abstract] "The standout performance of byT5 and DRS-MLM can be attributed to byte-level tokenization and specific pre-training, respectively."
  - [section] "byT5â€™s byte-level tokenization, which can be seen as character-level within our four target languages, results in a smaller dictionary and has the ability to handle unseen words."
- Break condition: If most tokens are predictable via subword patterns, the marginal benefit of byte-level tokenization may be negligible, especially for languages with smaller character sets.

## Foundational Learning

- Concept: Edit distance as a measure of sentence similarity.
  - Why needed here: Used to reorder documents to minimize lexical overlap between splits.
  - Quick check question: What is the computational complexity of pairwise edit distance for N sentences of average length L?

- Concept: CCG derivation trees and compositional semantics.
  - Why needed here: Trees are recombined to create challenge sets that test compositional generalization.
  - Quick check question: How does the backward slash (\) differ from the forward slash (/) in CCG category notation?

- Concept: Masked language model pseudo-log-likelihood (PLL) scoring.
  - Why needed here: Filters semantically abnormal sentences generated by CCG recombination.
  - Quick check question: Why might PLL scores be unreliable for very long sentences in PMB?

## Architecture Onboarding

- Component map: Data preprocessing -> CCG recombination engine -> PLL filtering -> Model training -> Evaluation
- Critical path: 1. Generate CCG recombination candidates. 2. Filter with PLL scoring. 3. Fine-tune models on systematic split. 4. Evaluate on standard and challenge sets.
- Design tradeoffs:
  - Sorting by edit distance vs. random split: Reduces overlap but may introduce bias if edit distance correlates with difficulty.
  - Byte-level vs. subword tokenization: Broader coverage vs. efficiency.
  - Manual vs. automatic long text annotation: Accuracy vs. scalability.
- Failure signatures:
  - High ill-formed rate in parsing outputs suggests tokenization or sequence length issues.
  - Low performance on compositional sets indicates memorization rather than generalization.
  - High overlap between train and test splits indicates split bias.
- First 3 experiments:
  1. Run edit distance-based split on a small PMB sample; compare word overlap with random split.
  2. Generate 100 CCG recombination sentences; compute PLL scores; manually inspect top/bottom 10.
  3. Fine-tune byT5 on systematic split; evaluate on both standard and long-text sets; record sequence lengths.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond those addressed by the proposed challenge sets and systematic splitting methodology.

## Limitations
- Edit distance sorting effectiveness not empirically validated for PMB dataset
- Byte-level tokenization may amplify sequence lengths, hurting performance on long texts
- Performance drops could reflect domain adaptation issues rather than fundamental model limitations

## Confidence
- Mechanism 1: Medium - Edit distance effectiveness inferred but not validated
- Mechanism 2: Medium - CCG recombination method validated but filtering process unclear
- Mechanism 3: Medium - Byte-level tokenization advantage shown but limited to specific models

## Next Checks
1. Replicate the edit distance-based split on a small PMB sample and measure word overlap reduction compared to random split using exact metrics.
2. Generate 100 CCG recombination sentences, compute PLL scores, and manually verify the top/bottom 10 to assess filtering effectiveness.
3. Fine-tune byT5 on the systematic split and evaluate on both standard and long-text sets, recording sequence lengths and ill-formed output rates to identify tokenization impacts.