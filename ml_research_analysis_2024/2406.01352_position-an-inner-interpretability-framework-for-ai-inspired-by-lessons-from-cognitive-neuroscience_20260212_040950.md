---
ver: rpa2
title: 'Position: An Inner Interpretability Framework for AI Inspired by Lessons from
  Cognitive Neuroscience'
arxiv_id: '2406.01352'
source_url: https://arxiv.org/abs/2406.01352
tags:
- inner
- interpretability
- cognitive
- arxiv
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper draws parallels between challenges in AI inner
  interpretability and longstanding issues in Cognitive Neuroscience, proposing a
  conceptual framework to guide mechanistic explanations of AI systems. The authors
  argue that inner interpretability research often provides incomplete mechanisms,
  lacks motivation for abstraction levels, and debates top-down versus bottom-up approaches
  without clear guidance.
---

# Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience

## Quick Facts
- arXiv ID: 2406.01352
- Source URL: https://arxiv.org/abs/2406.01352
- Reference count: 40
- Primary result: Proposes a multilevel framework inspired by Cognitive Neuroscience to improve mechanistic explanations in AI interpretability research

## Executive Summary
This position paper addresses fundamental challenges in AI inner interpretability research by drawing parallels with longstanding issues in Cognitive Neuroscience. The authors argue that current interpretability work often provides incomplete mechanistic explanations, arbitrarily chooses abstraction levels, and lacks clear guidance on whether to use top-down or bottom-up approaches. They propose adapting Cognitive Neuroscience's multilevel explanation framework (computational, algorithmic, primitive operations, and implementation levels) to AI interpretability research, illustrated through the example of factual recall in language models. The framework aims to provide clearer guidance for mechanistic explanations, help situate previous studies, identify research gaps, and address criticisms about the field's lack of clarity and practical applicability.

## Method Summary
The paper adapts a multilevel explanation framework from Cognitive Neuroscience to AI interpretability research, consisting of four levels: computational (defining the problem and constraints), algorithmic (step-by-step procedures), primitive operations (theoretical building blocks), and implementation (model decomposition). The authors illustrate this framework using factual recall in language models and propose methodological strategies including severe testing of hypotheses and examining invariances across conditions. The approach emphasizes formal specification of computational problems, mutual constraints between explanation levels, and rigorous hypothesis testing to avoid confirmation bias and false understanding.

## Key Results
- Current AI interpretability research suffers from incomplete mechanisms, arbitrary abstraction choices, and unclear methodological guidance
- The proposed multilevel framework can help situate previous studies and identify research gaps in the field
- Severe testing methodology from Cognitive Neuroscience can improve hypothesis evaluation in AI interpretability
- The framework provides concrete examples for understanding factual recall in language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework improves mechanistic explanations by forcing explicit definition of computational problems and their constraints
- Mechanism: By requiring researchers to formally specify the input-output mapping and environmental constraints at the computational level, the framework prevents the common error of attributing observed behaviors to incorrect underlying capacities
- Core assumption: A formally defined computational problem that is both theoretically possible and consistent with the model architecture will constrain the search for valid mechanistic explanations
- Evidence anchors:
  - [abstract] "Computational-level descriptions (see Fig. 1, top) can be tested both formally and empirically"
  - [section] "In our example, a context where the model is expected to deliver truthful information (e.g., a chatbot interacting with users) encourages the emergence of factual recall"
  - [corpus] Weak evidence - no direct corpus matches for this specific mechanism
- Break condition: If computational descriptions cannot be validated formally or empirically, or if multiple incompatible mechanisms produce identical computational solutions

### Mechanism 2
- Claim: The framework enables better level-of-abstraction choices by creating mutual constraints between levels
- Mechanism: The multilevel framework creates feedback loops where choices at one level (e.g., primitive operations) constrain what can be implemented at other levels (e.g., implementation), preventing arbitrary abstraction choices
- Core assumption: There exist invariant mappings between cognitive primitives and neural implementation levels that can guide abstraction choices
- Evidence anchors:
  - [abstract] "Multilevel explanations include comprehensive functional characterizations of the behavior (what is the system doing and why), algorithmic descriptions of how the function is computed"
  - [section] "More recent work has pointed out that mutual constraints between levels provide an avenue to construct more complete mechanistic explanations in practice"
  - [corpus] Weak evidence - no direct corpus matches for this specific mechanism
- Break condition: If no consistent mapping exists between levels or if multiple abstraction choices produce equally valid explanations

### Mechanism 3
- Claim: The framework reduces misleading claims by requiring severe testing of mechanistic hypotheses
- Mechanism: By deriving falsifiable empirical predictions from mechanistic hypotheses and requiring methods that can detect absence if mechanism is incorrect, the framework prevents confirmation bias and false understanding
- Core assumption: Severe tests can be designed that have high probability of falsifying incorrect mechanistic proposals
- Evidence anchors:
  - [abstract] "Hypotheses can be evaluated using severe tests (Mayo, 2018), which have been recently introduced to the field of Cognitive Neuroscience"
  - [section] "Severe tests require work examining the adequacy of the methods themselves, for instance, by exploring whether they can falsify mechanistic hypotheses about a system whose design principles are known"
  - [corpus] Weak evidence - no direct corpus matches for this specific mechanism
- Break condition: If severe tests cannot be designed or if methods consistently fail to detect known absences

## Foundational Learning

- Concept: Computational problems and formal specification
  - Why needed here: Understanding how to formally specify input-output mappings and constraints is essential for correctly defining the computational level in the framework
  - Quick check question: Can you formally define the computational problem for a simple task like sorting numbers, specifying input domain, output domain, and environmental constraints?

- Concept: Mutual constraints between levels of explanation
  - Why needed here: Understanding how constraints flow between computational, algorithmic, primitive, and implementation levels is critical for applying the framework correctly
  - Quick check question: If you choose key-value memory pairs as primitives, what constraints does this place on possible implementation choices?

- Concept: Severe testing and falsification
  - Why needed here: Understanding how to design tests that can actually falsify mechanistic hypotheses is essential for avoiding confirmation bias
  - Quick check question: What would a severe test look like for the hypothesis that attention heads implement key-value memory systems?

## Architecture Onboarding

- Component map:
  - Computational level: Problem definition, input/output specification, environmental constraints
  - Algorithmic level: Step-by-step procedures, causal models, human-understandable operations
  - Primitive level: Theoretical primitives, empirically validated building blocks, post-hoc emerging operations
  - Implementation level: Model decomposition, causal components, abstraction choices

- Critical path: 1) Define computational problem formally, 2) Specify algorithmic procedure, 3) Identify primitive operations, 4) Map to implementation, 5) Design severe tests, 6) Iterate based on results

- Design tradeoffs: Formal computational specification vs. empirical flexibility, abstraction level choices vs. explanatory power, severe testing rigor vs. practical feasibility

- Failure signatures: Inconsistent explanations across levels, inability to formalize computational problem, failure to derive falsifiable predictions, explanations that don't scale to larger models

- First 3 experiments:
  1. Take a simple model behavior (e.g., next token prediction) and formally specify its computational problem, then verify it's consistent with transformer architecture capabilities
  2. Map a known primitive operation (e.g., key-value memory) to implementation components and verify through ablation studies
  3. Design a severe test for a mechanistic hypothesis about factual recall that would detect absence if the mechanism doesn't exist

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental primitive operations and representations that emerge consistently across different transformer architectures and training regimes?
- Basis in paper: [explicit] The paper discusses primitives like key-value memory pairs emerging across domains, models, tasks, and sub-modules, and asks whether certain post-hoc primitives emerge consistently in distinct models.
- Why unresolved: While the paper provides key-value memory pairs as an example, it acknowledges this needs more systematic investigation across diverse architectures and training conditions to establish true primitives.
- What evidence would resolve it: Comprehensive empirical studies mapping operations across diverse transformer families, establishing consistent patterns that transcend architectural variations.

### Open Question 2
- Question: How can we develop rigorous hypothesis-testing procedures that incorporate severe testing principles to evaluate mechanistic proposals in AI interpretability?
- Basis in paper: [explicit] Section 4.2 discusses the need for severe tests and asks "if the hypothesized mechanism is absent, how likely is the method to reveal its absence?"
- Why unresolved: While the paper introduces the concept of severe testing from Cognitive Neuroscience, it doesn't provide concrete methodologies for implementing these principles in AI interpretability research.
- What evidence would resolve it: Development and validation of specific testing frameworks that can reliably falsify incorrect mechanistic hypotheses about AI systems with known ground truth.

### Open Question 3
- Question: What are the appropriate levels of abstraction for implementing primitive operations in transformer architectures, and how do these choices affect the scalability of mechanistic explanations?
- Basis in paper: [explicit] Section 3.2 discusses the problem of choosing abstraction levels, noting that current work often chooses levels arbitrarily and that finding explanations at microscopic levels may not be computationally feasible in large models.
- Why unresolved: The paper identifies this as a critical issue but doesn't provide definitive guidance on how to determine optimal abstraction levels for different types of analyses.
- What evidence would resolve it: Systematic studies comparing mechanistic explanations at different abstraction levels across model scales, establishing criteria for selecting appropriate levels based on specific research goals and model characteristics.

## Limitations
- The framework's effectiveness depends heavily on the ability to formally specify computational problems for complex AI behaviors, which may be challenging for many capacities currently studied in interpretability research
- The severe testing methodology, while promising, requires significant methodological development to apply to complex AI systems where design principles may not be fully known
- Mutual constraints between levels, while theoretically appealing, remain difficult to achieve in practice, particularly when translating between algorithmic descriptions and implementation details

## Confidence
- **High confidence**: The identification of current interpretability research limitations (incomplete mechanisms, arbitrary abstraction choices, top-down vs bottom-up debates) is well-supported by examples from the field and aligns with documented criticisms of interpretability research
- **Medium confidence**: The adaptation of Cognitive Neuroscience's multilevel framework to AI interpretability is conceptually sound and has theoretical justification, but practical application to complex AI systems requires further validation
- **Low confidence**: The claim that this framework will resolve ongoing debates about the field's lack of clarity and practical applicability is speculative, as the paper provides conceptual arguments rather than empirical validation of the framework's impact on these issues

## Next Checks
1. Apply the computational level specification to a concrete AI capacity (e.g., sentiment analysis) and evaluate whether formal problem definition constrains the search space for mechanistic explanations in a measurable way
2. Test the mutual constraints mechanism by selecting a primitive operation (e.g., key-value memory) and mapping it through all four levels, measuring the consistency and predictive power of the resulting explanation
3. Design and execute a severe test for a mechanistic hypothesis about factual recall in language models, comparing the framework's predictions against alternative explanations to assess its discriminative power