---
ver: rpa2
title: 'FADE: Towards Fairness-aware Generation for Domain Generalization via Classifier-Guided
  Score-based Diffusion Models'
arxiv_id: '2406.09495'
source_url: https://arxiv.org/abs/2406.09495
tags:
- data
- fade
- fairness
- sensitive
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness-aware domain generalization (FairDG),
  a critical challenge for deploying trustworthy AI systems under distribution shifts.
  The proposed FADE framework generates unbiased and domain-invariant data by combining
  pre-trained score-based diffusion models (SDM) with classifier guidance.
---

# FADE: Towards Fairness-aware Generation for Domain Generalization via Classifier-Guided Score-based Diffusion Models

## Quick Facts
- arXiv ID: 2406.09495
- Source URL: https://arxiv.org/abs/2406.09495
- Authors: Yujie Lin; Dong Li; Minglai Shao; Guihong Wan; Chen Zhao
- Reference count: 23
- Primary result: FADE achieves superior accuracy-fairness trade-offs on three real-world datasets, with 84.55% accuracy and fairness metrics (ΔDP, ΔEO, ΔEOp) of 0.089, 0.121, and 0.192 respectively on the Adult dataset.

## Executive Summary
FADE addresses fairness-aware domain generalization (FairDG) by generating unbiased and domain-invariant data through classifier-guided score-based diffusion models. The framework pre-trains a diffusion model and two classifiers across multiple source domains, then guides the generation process to remove sensitive information while preserving class-relevant features. Extensive experiments on Adult, Bank, and NYSF datasets demonstrate that FADE outperforms existing methods in both accuracy and fairness metrics, achieving robust generalization under distribution shifts.

## Method Summary
FADE first pre-trains a score-based diffusion model (SDM) and two classifiers (label and sensitive) across source domains using a multi-task learning approach with support/query splits. During generation, the pre-trained classifiers guide the reverse diffusion process by maximizing entropy of sensitive attribute predictions (to remove bias) and conditioning on specified labels (to maintain balanced class distributions). The generated fair data is then used to train a downstream classifier that achieves high accuracy while maintaining fairness across unseen target domains.

## Key Results
- Achieves accuracy-fairness trade-offs with 84.55% accuracy and fairness metrics (ΔDP, ΔEO, ΔEOp) of 0.089, 0.121, and 0.192 on Adult dataset
- Outperforms all baseline methods in both accuracy and fairness metrics across three real-world datasets
- Maintains robust generalization under distribution shifts while eliminating sensitive information from generated data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training across multiple source domains with support/query split creates domain-invariant representations that generalize to unseen target domains.
- Mechanism: By constructing K tasks where each task has a support set (S-1 domains) and query set (1 domain), the model learns parameters Φ that are optimized for cross-domain generalization. The weighted sum of support and query losses ensures the model performs well on both seen and unseen domain combinations.
- Core assumption: The source domains are sufficiently diverse to represent the space of possible domain shifts.
- Evidence anchors: Pre-training equips components with generalization capabilities across different domains.
- Break condition: If source domains are too similar or limited in diversity, learned Φ will not generalize to truly unseen domain distributions.

### Mechanism 2
- Claim: Classifier-guided diffusion sampling can effectively remove sensitive information while preserving class-relevant features through entropy maximization of the sensitive classifier output.
- Mechanism: During reverse diffusion, the gradient update includes a term that maximizes entropy of the sensitive attribute prediction, forcing generated samples to have uniform sensitive attribute distributions while label guidance maintains class information.
- Core assumption: The sensitive classifier can accurately estimate p(z|x) and entropy maximization corresponds to removing sensitive information.
- Evidence anchors: Adding gradient of logarithm of entropy term in reverse diffusion process maximizes entropy of sensitive attribute predictions.
- Break condition: If sensitive classifier is poorly trained or sensitive information is too correlated with class features, entropy maximization may remove useful information or fail to remove bias.

### Mechanism 3
- Claim: Generating balanced class distributions through label guidance avoids minority class underrepresentation in downstream classifier training.
- Mechanism: By specifying labels during generation and using label classifier guidance, the framework can generate equal numbers of samples for each class, creating a balanced dataset that prevents downstream classifier bias toward majority classes.
- Core assumption: The label classifier is well-calibrated and generating samples with specified labels doesn't introduce artifacts.
- Evidence anchors: Specifying labels for generating samples creates balanced datasets to avoid insufficient feature information for minority classes.
- Break condition: If label classifier has systematic biases or generation process introduces artifacts when conditioning on specific labels.

## Foundational Learning

- **Concept: Diffusion models and score matching**
  - Why needed here: FADE uses score-based diffusion models as generative backbone, requiring understanding of iterative noise addition and removal
  - Quick check question: What is the key difference between score-based diffusion models and denoising diffusion probabilistic models (DDPMs)?

- **Concept: Algorithmic fairness metrics (DP, EO, EOP)**
  - Why needed here: Paper evaluates fairness using difference of demographic parity, equalized odds, and equalized opportunity
  - Quick check question: How does equalized odds differ from equalized opportunity in fairness evaluation?

- **Concept: Domain generalization and covariate shift**
  - Why needed here: FADE specifically addresses fairness-aware domain generalization under covariate shift where only marginal feature distribution changes across domains
  - Quick check question: What distinguishes covariate shift from other types of distribution shifts in domain generalization?

## Architecture Onboarding

- **Component map**: Score network (sθs) + Label classifier (ϕθy) + Sensitive classifier (ψθz) -> Classifier-guided diffusion sampling -> Generated fair data -> Downstream classifier

- **Critical path**: 1) Pre-train all three components across domains using support/query tasks 2) Use pre-trained components to guide diffusion sampling with fair control 3) Generate balanced, unbiased dataset 4) Train final classifier on generated data 5) Evaluate on unseen target domain

- **Design tradeoffs**: Strong fair control (high λz) vs. maintaining data quality and accuracy; Pre-training comprehensiveness vs. computational cost; Generated data diversity vs. domain alignment with target

- **Failure signatures**: Poor fairness metrics indicate insufficient debiasing; Low accuracy on target domain indicates poor generalization; Generated data with unrealistic artifacts indicates guidance strength issues; Downstream classifier performance degrading over time indicates distribution shift problems

- **First 3 experiments**:
  1. Ablation test: Run FADE with λz=0 (no fair control) and compare fairness metrics to full FADE
  2. Domain sensitivity: Train FADE on Adult dataset but test on Bank dataset to verify cross-dataset generalization
  3. Guidance strength sweep: Vary λz ∈ {0.1, 1, 10, 50, 100} and plot accuracy-fairness tradeoff curves to find optimal balance

## Open Questions the Paper Calls Out

- **Open Question 1**: How does FADE's performance scale with the number of domains in the source dataset, and what is the theoretical limit of its domain generalization capability?
  - Basis in paper: Paper tests on datasets with fixed number of domains (5 domains for Adult and Bank, 5 domains for NYSF) but doesn't systematically explore performance scaling
  - Why unresolved: Only tests on datasets with a fixed number of domains
  - What evidence would resolve it: Systematic experiments varying number of domains while measuring accuracy and fairness metrics

- **Open Question 2**: What is the impact of different diffusion model architectures (e.g., DDPM vs. score-based models) on FADE's fairness and accuracy performance?
  - Basis in paper: Paper uses score-based diffusion models but doesn't compare against other diffusion model architectures
  - Why unresolved: Focuses exclusively on score-based diffusion models without exploring alternative architectures
  - What evidence would resolve it: Direct comparison experiments using different diffusion model architectures while keeping classifier guidance framework constant

- **Open Question 3**: How does FADE's performance change when distribution shifts involve factors beyond covariate shifts, such as concept drift or label shift?
  - Basis in paper: Paper explicitly narrows scope to covariate shift while acknowledging other types of distribution shifts exist
  - Why unresolved: Deliberately limits scope to covariate shifts and doesn't investigate robustness to other types of distribution shifts
  - What evidence would resolve it: Experiments introducing various types of distribution shifts and measuring FADE's performance under each

## Limitations
- Performance scaling with increasing number of source domains remains unexplored
- Effectiveness of entropy-based debiasing mechanism lacks extensive empirical validation
- Framework specifically designed for covariate shift, limiting applicability to other distribution shift types

## Confidence
- **High confidence**: Overall framework design and experimental results showing FADE's superiority over baselines
- **Medium confidence**: Mechanism of classifier-guided diffusion for fair data generation, as theoretical justification is sound but lacks extensive validation
- **Low confidence**: Claim that entropy maximization of sensitive classifier outputs effectively removes bias, as this specific approach is novel and under-supported

## Next Checks
1. **Ablation Study**: Remove the entropy maximization term (λz=0) and measure if fairness metrics degrade proportionally to the increase in λz, validating the debiasing mechanism
2. **Classifier Robustness**: Test the sensitive classifier's performance on out-of-distribution data to ensure it can reliably guide the diffusion process across domains
3. **Generation Quality**: Conduct human evaluation of generated samples to verify they maintain realistic data characteristics while achieving the targeted fairness improvements