---
ver: rpa2
title: 'FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian
  Deep Learning'
arxiv_id: '2407.13711'
source_url: https://arxiv.org/abs/2407.13711
tags:
- laplace
- prior
- function
- neural
- aplace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FSP-Laplace, a method for performing Bayesian
  inference with neural networks by placing interpretable Gaussian process priors
  directly in function space rather than weight space. This addresses the difficulty
  of specifying meaningful priors over network weights, which are not interpretable.
---

# FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning

## Quick Facts
- arXiv ID: 2407.13711
- Source URL: https://arxiv.org/abs/2407.13711
- Reference count: 0
- Primary result: FSP-Laplace improves Bayesian deep learning performance on tasks with prior knowledge by placing interpretable GP priors directly in function space

## Executive Summary
This paper introduces FSP-Laplace, a method for Bayesian inference with neural networks that places interpretable Gaussian process priors directly in function space rather than weight space. By formulating training as finding the weak mode of the posterior measure under a GP prior restricted to functions representable by the network, then applying a linearized Laplace approximation, FSP-Laplace addresses the difficulty of specifying meaningful priors over network weights. The method is particularly effective when explicit prior knowledge is available and shows competitive performance on standard benchmarks including image classification, out-of-distribution detection, and Bayesian optimization.

## Method Summary
FSP-Laplace performs Bayesian inference by training neural networks with a novel objective that combines negative log-likelihood with RKHS regularization from a GP prior. The method uses context points sampled from the dataset to estimate the RKHS norm, then applies a linearized Laplace approximation around the MAP estimate to compute an approximate posterior covariance. Key technical innovations include an efficient, matrix-free algorithm using Lanczos iteration to compute the approximate posterior covariance without forming large matrices, and a heuristic to prevent exploding predictive variance. The method particularly excels on lower-dimensional problems where explicit prior knowledge is available.

## Key Results
- FSP-Laplace improves performance on tasks with prior knowledge (e.g., Mauna Loa CO2 prediction, ocean current modeling)
- Remains competitive on standard benchmarks including image classification and out-of-distribution detection
- Particularly excels on lower-dimensional problems where explicit prior knowledge is available

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Function-space priors improve prior specification by placing beliefs directly on the output function rather than uninterpretable network weights
- Mechanism: GP priors over function space allow encoding of structured inductive biases (smoothness, periodicity, length-scale) that map directly to desired function properties
- Core assumption: The space of functions representable by the neural network intersects nontrivially with the RKHS induced by the GP prior
- Evidence anchors:
  - [abstract] "we directly place a prior on function space... one can express structured and interpretable inductive biases, such as regularity or periodicity, directly in function space"
  - [section] "The FSP-L APLACE objective as an unnormalized log-density... the resulting expression is the Lebesgue density of the HΣ-orthogonal projection of the GP prior onto the finite-dimensional subspace spanned by the 'feature functions' learned by the neural network"

### Mechanism 2
- Claim: Linearization around the MAP estimate enables scalable Laplace approximation under function-space priors
- Mechanism: After training, the network is linearized at the MAP weights, converting the intractable function-space prior into a tractable Gaussian prior over weights
- Core assumption: The linearized model adequately approximates the true network behavior in the vicinity of the MAP estimate
- Evidence anchors:
  - [abstract] "After model linearization, the training objective induces a negative log-posterior density to which we apply a Laplace approximation"
  - [section] "After training the neural network using the objective function RFSP(w)..., we intuitively want to use the same objective function to compute an approximate posterior... we work around this issue by means of local linearization"

### Mechanism 3
- Claim: Matrix-free Lanczos iteration with low-rank kernel approximation enables scaling to large models and datasets
- Mechanism: The RKHS norm is estimated using a finite set of context points, and the kernel matrix is approximated via Lanczos iteration to avoid materializing large matrices
- Core assumption: The kernel Gram matrix exhibits rapid spectral decay, making low-rank approximation accurate
- Evidence anchors:
  - [abstract] "leveraging highly scalable methods from matrix-free linear algebra"
  - [section] "we devise an efficient routine for computing (a square root of) the approximate posterior covariance matrixΛ†... This allows the method to scale to large models"

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: The GP prior is defined over an RKHS, and the regularization term in the objective is the RKHS norm
  - Quick check question: What is the relationship between a GP prior and its associated RKHS? (Answer: The GP defines a Gaussian measure over the RKHS, and the RKHS norm appears in the prior density)

- Concept: Laplace Approximation
  - Why needed here: The method applies a Laplace approximation to the linearized model to obtain a Gaussian posterior over weights
  - Quick check question: What is the key approximation made in the linearized Laplace method? (Answer: The network is linearized around the MAP estimate, replacing the true Hessian with the generalized Gauss-Newton matrix)

- Concept: Matrix-free Linear Algebra
  - Why needed here: The method avoids forming large matrices by using matrix-vector products and Lanczos iteration
  - Quick check question: Why is Lanczos iteration particularly suitable for kernel matrices? (Answer: Kernel matrices often have rapidly decaying spectra, making low-rank approximations accurate)

## Architecture Onboarding

- Component map: Training objective (RFSP) -> Linearization at MAP -> Matrix-free Lanczos for covariance -> Posterior sampling

- Critical path:
  1. Sample context points C
  2. Compute RKHS norm estimate using C
  3. Optimize RFSP to find MAP weights w*
  4. Linearize network at w*
  5. Compute approximate posterior covariance via Lanczos
  6. Sample from posterior for predictions

- Design tradeoffs:
  - Context points: More points → better RKHS norm estimate but higher computational cost
  - Lanczos iterations: More iterations → better kernel approximation but higher cost
  - Rank r: Higher rank → more accurate covariance but more memory/computation

- Failure signatures:
  - Exploding predictive variance: Likely due to numerical issues distinguishing zero from small eigenvalues
  - Poor performance on high-dimensional data: GP priors may not capture complex patterns
  - Slow training: Inefficient RKHS norm estimation or poor optimization landscape

- First 3 experiments:
  1. 1D regression with RBF kernel: Verify smooth interpolation and uncertainty quantification
  2. Periodic function with periodic kernel: Verify correct periodicity without additional features
  3. Synthetic classification with Matern kernel: Verify rough decision boundary and calibrated uncertainty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical justification for the heuristic that zeroes out small eigenvalues in the approximate posterior covariance matrix to prevent exploding predictive variance?
- Basis in paper: [explicit] The authors use a heuristic based on the observation that the trace of the posterior covariance is upper-bounded by the trace of the prior covariance, and impose that this condition holds by successively zeroing-out the smallest eigenvalues
- Why unresolved: The paper states this is a heuristic without providing theoretical justification or analysis of its impact on the resulting posterior approximation
- What evidence would resolve it: Theoretical analysis proving convergence properties of this heuristic, or empirical comparison showing its impact on predictive performance and uncertainty calibration

### Open Question 2
- Question: How does the performance of FSP-Laplace scale with increasing input dimensionality, and what are the limitations of using GP priors in high-dimensional spaces?
- Basis in paper: [explicit] The authors state "In high dimensional spaces, where explicit prior knowledge is difficult to state, Gaussian process priors are naturally at a disadvantage" and acknowledge uncertainty about finding good function space priors in such spaces
- Why unresolved: The experiments show competitive performance on image classification but the authors don't provide theoretical analysis or systematic study of how performance degrades with increasing dimensionality
- What evidence would resolve it: Systematic experiments varying input dimensionality across multiple datasets, or theoretical analysis of the expressivity of GP priors in high-dimensional spaces

### Open Question 3
- Question: What is the formal relationship between the weak mode of the posterior measure and the minimizer of the constrained optimization problem in function space?
- Basis in paper: [explicit] The authors conjecture that the limit f* of the sequence {f*_n} is a minimizer of Equation (3.2), but note "we cannot give probabilistic meaning to optimization problems with hard constraints" and leave "a full proof for future work"
- Why unresolved: The paper establishes that weak modes minimize a regularized objective but does not formally prove the connection to the constrained optimization problem
- What evidence would resolve it: A rigorous proof establishing the equivalence between weak modes and minimizers of the constrained optimization problem, or a counterexample showing they are not equivalent

## Limitations
- Method effectiveness depends heavily on choice of GP kernel and network's ability to represent functions in kernel's RKHS
- GP priors may be too restrictive or fail to capture relevant structure for highly complex, high-dimensional data
- Linearization assumption may break down for highly nonlinear networks or multimodal posteriors
- Heuristic for preventing exploding predictive variance lacks theoretical justification and may require problem-specific tuning

## Confidence

- Mechanism 1 (Function-space priors): High confidence - The theoretical foundation connecting GP priors to RKHS regularization is well-established
- Mechanism 2 (Linearization + Laplace): Medium confidence - While linearization is standard, its validity for complex posteriors needs empirical verification
- Mechanism 3 (Matrix-free Lanczos): High confidence - The algorithmic approach is sound, though performance depends on kernel spectral properties

## Next Checks

1. **Ablation on context points**: Systematically vary the number of context points used for RKHS estimation and measure impact on both computational efficiency and predictive performance across multiple datasets

2. **Kernel sensitivity analysis**: Test FSP-Laplace with intentionally mismatched kernels (e.g., RBF prior on periodic functions) to quantify sensitivity to prior specification

3. **High-dimensional stress test**: Evaluate on high-dimensional synthetic functions where the true function space is known but does not align with the GP RKHS to identify breaking points