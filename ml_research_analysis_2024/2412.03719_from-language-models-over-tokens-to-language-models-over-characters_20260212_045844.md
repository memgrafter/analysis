---
ver: rpa2
title: From Language Models over Tokens to Language Models over Characters
arxiv_id: '2412.03719'
source_url: https://arxiv.org/abs/2412.03719
tags:
- language
- token
- probability
- string
- bundle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of using character-level prompts
  with token-level language models, where the tokenization process introduces unintuitive
  behaviors. The authors present algorithms to convert token-level language models
  into character-level ones, offering both exact and approximate methods.
---

# From Language Models over Tokens to Language Models over Characters

## Quick Facts
- arXiv ID: 2412.03719
- Source URL: https://arxiv.org/abs/2412.03719
- Reference count: 40
- Key outcome: Converts token-level language models to character-level ones using "coverings" to enumerate high-probability tokenizations, improving compression rates and resolving prompt boundary issues

## Executive Summary
This paper addresses the fundamental challenge of using character-level prompts with token-level language models, where tokenization introduces unintuitive behaviors and inefficiencies. The authors present a novel approach that converts token-level language models into character-level models by computing high-probability tokenizations of character strings using the concept of a "covering." Their method provides both exact and approximate algorithms for computing these coverings, enabling language models to operate more naturally at the character level while maintaining computational efficiency. The approach resolves the "prompt boundary problem" where token-level models struggle to handle arbitrary character prefixes and demonstrates significant improvements in compression rates (bits/byte) across multiple model architectures.

## Method Summary
The authors develop algorithms that convert token-level language models into character-level models by computing "coverings" - sets of high-probability tokenizations that decode to strings with a given character prefix. The exact algorithm enumerates all tokenizations that decode to a given character string, while the approximate algorithm efficiently selects a subset of tokenizations that closely approximates the full character-level distribution. The covering concept allows the method to maintain computational tractability while capturing the essential probability mass needed for accurate character-level predictions. The approach works by leveraging the probability distribution over token sequences and mapping these to their corresponding character representations, enabling models to reason directly about character-level inputs and outputs rather than being constrained by tokenization boundaries.

## Key Results
- Demonstrated improved compression rates (bits/byte) compared to canonical tokenization across four language models
- Successfully resolved the prompt boundary problem, enabling natural handling of character-level inputs
- Showed that approximate covering algorithms achieve high fidelity to exact character-level distributions with limited computational budget
- Validated the approach across multiple model architectures including Llama-3.2-1B, Meta-Llama-3.1-8B, DeepSeek-R1-Distill-Llama-8B, and phi-4

## Why This Works (Mechanism)
The method works by bridging the gap between token-level and character-level representations through systematic enumeration of high-probability tokenizations. Token-level language models assign probabilities to sequences of tokens, but these tokens don't align naturally with character sequences, creating inefficiencies and unnatural behavior when handling character-level prompts. By computing coverings - sets of tokenizations that decode to specific character strings - the method effectively converts the token-level probability distribution into a character-level one. The covering approach ensures that the most probable ways of tokenizing a character string are captured, allowing the model to reason about character sequences while still leveraging the learned token-level representations. This enables the model to handle arbitrary character prefixes and generate character-level outputs more naturally.

## Foundational Learning

**Tokenization and Subword Models**
*Why needed:* Understanding how token-level models split text into subword units is crucial for grasping the limitations this paper addresses.
*Quick check:* Can you explain why "character" might be tokenized as "char" and "##acter" in a subword model?

**Probability Distributions over Sequences**
*Why needed:* The method relies on computing and manipulating probability distributions over token sequences.
*Quick check:* How would you compute the probability of a character string given a token-level model?

**Dynamic Programming**
*Why needed:* The exact covering algorithm uses dynamic programming to enumerate all valid tokenizations efficiently.
*Quick check:* What is the time complexity of enumerating all tokenizations for a string of length n?

**Beam Search and Approximation**
*Why needed:* The approximate covering algorithm uses beam search to maintain computational tractability.
*Quick check:* How does beam width affect the trade-off between accuracy and computational cost in this context?

## Architecture Onboarding

**Component Map**
Tokenization layer -> Covering computation module -> Character-level probability distribution -> Language model output layer

**Critical Path**
Character input → Tokenization validation → Covering enumeration (exact/approximate) → Probability aggregation → Character-level output generation

**Design Tradeoffs**
The paper trades off completeness for computational efficiency by using approximate coverings rather than enumerating all possible tokenizations. This enables practical application but may miss low-probability tokenizations that could be important for certain tasks or languages.

**Failure Signatures**
The method may struggle with highly ambiguous tokenizations where multiple valid token sequences decode to the same character string, potentially missing important probability mass. Languages with complex morphology or non-standard tokenization patterns may also present challenges.

**First Experiments**
1. Implement the exact covering algorithm and verify it correctly enumerates all tokenizations for simple test cases
2. Compare the approximate covering output with the exact covering for a fixed set of character strings
3. Measure compression rates (bits/byte) on a validation dataset using both canonical tokenization and the covering-based approach

## Open Questions the Paper Calls Out

None

## Limitations

- The approximate algorithms may miss low-probability but potentially important tokenizations, especially for longer prompts or morphologically complex languages
- Experimental validation focuses primarily on compression metrics rather than downstream task performance
- Claims about enabling "efficient character-level conditional generation" lack supporting evidence of generation quality or diversity

## Confidence

- High: Algorithmic framework and ability to approximate character-level distributions
- Medium: Practical benefits in compression and prompt handling
- Low: Claims about enabling efficient character-level conditional generation without supporting evidence

## Next Checks

1. Evaluate the method's performance across diverse languages, particularly those with agglutinative morphology or non-Latin scripts, to assess coverage completeness and approximation quality.

2. Conduct downstream task evaluations (e.g., text completion, translation, summarization) to verify that character-level conversion improves task performance, not just compression metrics.

3. Compare the computational efficiency trade-offs between exact and approximate methods across varying prompt lengths and model sizes to establish practical scalability limits.