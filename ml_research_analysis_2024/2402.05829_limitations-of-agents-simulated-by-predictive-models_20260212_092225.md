---
ver: rpa2
title: Limitations of Agents Simulated by Predictive Models
arxiv_id: '2402.05829'
source_url: https://arxiv.org/abs/2402.05829
tags:
- agents
- actions
- predictive
- simulated
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes structural limitations of agents simulated
  by predictive models, specifically auto-suggestive delusions and predictor-policy
  incoherence. Auto-suggestive delusions occur when models incorrectly infer hidden
  states from their own generated actions, while predictor-policy incoherence arises
  when models act as if future actions will be suboptimal.
---

# Limitations of Agents Simulated by Predictive Models

## Quick Facts
- arXiv ID: 2402.05829
- Source URL: https://arxiv.org/abs/2402.05829
- Reference count: 37
- Agents simulated by predictive models suffer from auto-suggestive delusions and predictor-policy incoherence, both of which can be resolved by re-training on model outputs

## Executive Summary
This paper analyzes structural limitations of agents simulated by predictive models, specifically auto-suggestive delusions and predictor-policy incoherence. Auto-suggestive delusions occur when models incorrectly infer hidden states from their own generated actions, while predictor-policy incoherence arises when models act as if future actions will be suboptimal. The authors show both issues can be resolved by re-training models on their own outputs, converging to optimal policies. Experiments with Decision Transformers on simple games confirm that fine-tuning reduces both problems and improves agent performance.

## Method Summary
The authors analyze two structural limitations in predictive models: auto-suggestive delusions (incorrect hidden state inference from generated actions) and predictor-policy incoherence (acting as if future actions will be suboptimal). They propose resolving both issues by re-training models on their own outputs. This approach is validated experimentally using Decision Transformers on simple game environments, demonstrating that fine-tuning on generated trajectories reduces both problems and leads to convergence toward optimal policies.

## Key Results
- Auto-suggestive delusions occur when predictive models incorrectly infer hidden states from their own generated actions
- Predictor-policy incoherence arises when models act as if future actions will be suboptimal
- Re-training models on their own outputs resolves both issues and converges to optimal policies

## Why This Works (Mechanism)
The paper demonstrates that structural limitations in predictive models arise from the mismatch between training data (expert demonstrations) and model-generated trajectories. When models generate their own actions and then use these as inputs, they can fall into systematic reasoning errors. The proposed re-training approach works by exposing the model to its own generated behavior, allowing it to learn the actual consequences of its policy rather than relying on idealized training data. This creates a feedback loop where the model gradually corrects its misconceptions about hidden states and future action optimality.

## Foundational Learning

**Predictive Model Agents**: Agents that use predictive models to simulate future trajectories and select actions. Why needed: Forms the basis for understanding how agents can be simulated rather than explicitly trained. Quick check: Verify understanding of the difference between model-based RL and predictive model agents.

**Hidden State Inference**: The process by which agents infer unobservable aspects of the environment state. Why needed: Central to understanding auto-suggestive delusions. Quick check: Can you explain how hidden state inference differs from observation-based state estimation?

**Policy Coherence**: The consistency between an agent's current actions and its beliefs about future actions. Why needed: Key to understanding predictor-policy incoherence. Quick check: Describe what it means for a policy to be incoherent in the context of sequential decision making.

## Architecture Onboarding

**Component Map**: Decision Transformer (input processing) -> Trajectory Generation (action simulation) -> Reward Prediction (value estimation) -> Policy Selection (action choice)

**Critical Path**: Model training on expert demonstrations -> Trajectory generation with model → Hidden state inference and policy selection → Performance evaluation → Re-training on generated trajectories → Convergence verification

**Design Tradeoffs**: The paper focuses on Decision Transformers but notes that the limitations may apply to other predictive model architectures. The re-training approach trades off computational efficiency for improved policy quality.

**Failure Signatures**: Auto-suggestive delusions manifest as systematic errors in hidden state inference when models generate their own actions. Predictor-policy incoherence appears as suboptimal action selection based on incorrect beliefs about future action quality.

**First Experiments**: 1) Train Decision Transformer on expert demonstrations for a simple game. 2) Generate trajectories using the trained model and measure auto-suggestive delusions. 3) Apply re-training on generated trajectories and evaluate convergence to optimal policy.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses primarily on simple game environments and Decision Transformers
- Limited exploration of more complex, real-world scenarios
- Does not extensively investigate potential new failure modes from re-training

## Confidence
- Predictive model limitations: High
- Re-training resolution effectiveness: Medium
- Scalability to complex environments: Low
- Architecture independence: Low

## Next Checks
1. Test the re-training approach on more complex environments with larger state and action spaces, including continuous control tasks, to verify scalability and convergence properties.

2. Evaluate alternative predictive model architectures beyond Decision Transformers to determine if the identified limitations and solutions are architecture-independent.

3. Investigate the behavior of models when re-training on their own outputs in non-stationary environments where reward structures or transition dynamics may change over time.