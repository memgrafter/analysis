---
ver: rpa2
title: Exploration of Masked and Causal Language Modelling for Text Generation
arxiv_id: '2405.12630'
source_url: https://arxiv.org/abs/2405.12630
tags:
- text
- texts
- generation
- generated
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares masked language modeling (MLM) and causal language
  modeling (CLM) for text generation. MLM, traditionally used for language understanding,
  can generate tokens in any order and location, unlike CLM which generates text sequentially
  from left to right.
---

# Exploration of Masked and Causal Language Modelling for Text Generation

## Quick Facts
- arXiv ID: 2405.12630
- Source URL: https://arxiv.org/abs/2405.12630
- Authors: Nicolo Micheletti; Samuel Belkadi; Lifeng Han; Goran Nenadic
- Reference count: 35
- Key outcome: Masked Language Modeling (MLM) consistently outperforms Causal Language Modeling (CLM) for text generation across three datasets

## Executive Summary
This study compares masked language modeling (MLM) and causal language modeling (CLM) for text generation tasks. MLM, traditionally used for language understanding, can generate tokens in any order and location, unlike CLM which generates text sequentially from left to right. The authors pre-trained models on three datasets: medical discharge summaries, movie plot synopses, and authorship verification data. They evaluated generations using quantitative metrics (BLEU, ROUGE, METEOR, BERTScore) and human assessment for coherence and grammar. Additionally, they assessed usefulness in downstream tasks (named entity recognition, text classification, authorship verification). Results show MLM consistently outperforms CLM across all datasets with higher quantitative scores and better coherence. The study finds no strong correlation between generation quality and downstream task performance.

## Method Summary
The study pre-trained and fine-tuned several language models (BERT, RoBERTa, BiomedNLP-PubMedBERT, T5, SciFive, BART) using both MLM and CLM approaches with different masking strategies (random, stopwords, punctuation, NER) on three datasets. Generated synthetic datasets were created by applying trained models with different masking approaches, producing approximately 130-200 samples per model. The generated texts were evaluated using quantitative metrics (BLEU, ROUGE, METEOR, BERTScore), human evaluation for coherence and grammar, and tested on three downstream tasks (NER, text classification, authorship verification).

## Key Results
- MLM consistently outperformed CLM in text generation across all datasets with higher quantitative scores and better coherence
- No strong correlation was found between the quality of generated text and performance on downstream tasks
- Different masking strategies showed varying effectiveness, with punctuation masking achieving highest performance for CLM models while showing very low performance for MLM models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLM outperforms CLM for text generation because bidirectional context allows more flexible token selection without strict sequential constraints.
- Mechanism: MLM can generate tokens in any order and location by conditioning on both left and right context, whereas CLM must generate strictly left-to-right, limiting its ability to leverage full contextual information.
- Core assumption: Bidirectional context provides richer information for generating coherent text compared to unidirectional context.
- Evidence anchors:
  - [abstract]: "MLM, primarily used for language understanding tasks, can generate tokens anywhere in the text and in any order."
  - [section]: "MLM learn to generate tokens by looking at a bidirectional context... [MLM] can generate tokens anywhere in the text and in any order."
  - [corpus]: Weak - corpus neighbors discuss MLM vs CLM but don't provide direct evidence for bidirectional advantage in generation.
- Break condition: If task requires strict sequential generation or if computational overhead of MLM generation becomes prohibitive.

### Mechanism 2
- Claim: MLM generation quality is not strongly correlated with downstream task performance because the nature of synthetic data generation differs from task-specific fine-tuning.
- Mechanism: While MLM generates higher quality text per quantitative metrics, downstream models may be robust to noise or benefit from the variability in synthetic data, decoupling generation quality from task performance.
- Core assumption: Downstream models can learn effectively even from imperfect synthetic data, or the synthetic data variability enhances robustness.
- Evidence anchors:
  - [abstract]: "The study also finds no strong correlation between the quality of the generated text and the performance of the models on the downstream tasks."
  - [section]: "Remarkably, even when the masking ratio reaches 100%, the F1 score, though sharply decreased, remains effective enough to train the model..."
  - [corpus]: Weak - corpus neighbors discuss MLM vs CLM but don't directly address downstream task performance correlation.
- Break condition: If downstream task requires highly accurate data or if synthetic data quality falls below a critical threshold.

### Mechanism 3
- Claim: Random masking with lower ratios produces better generations because more context preserves coherence and reduces hallucinations.
- Mechanism: By masking fewer tokens (lower masking ratio), MLM retains more context for generating each token, leading to more coherent and contextually appropriate outputs.
- Core assumption: Preserving more context during generation improves output quality by reducing the model's need to hallucinate or make unsupported inferences.
- Evidence anchors:
  - [section]: "As expected, random masking with lower masking ratios (preserving more context) resulted in higher-quality generations."
  - [section]: "Quantitative metrics showed that texts generated by MLM models were more aligned with the original reference texts."
  - [corpus]: Weak - corpus neighbors don't provide direct evidence about masking ratios and generation quality.
- Break condition: If computational constraints require higher masking ratios or if downstream tasks benefit from more diverse, lower-quality data.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Understanding MLM is crucial because the paper compares MLM and CLM for text generation, showing MLM's advantages.
  - Quick check question: What is the key difference between how MLM and CLM handle context during token prediction?

- Concept: Causal Language Modeling (CLM)
  - Why needed here: CLM is the traditional approach for text generation, and understanding its limitations helps explain why MLM might be superior.
  - Quick check question: How does the unidirectional nature of CLM constrain its ability to generate coherent text compared to MLM?

- Concept: Text Generation Evaluation Metrics (BLEU, ROUGE, METEOR, BERTScore)
  - Why needed here: These metrics are used to quantitatively assess the quality of generated text, which is central to the paper's findings.
  - Quick check question: What aspect of text quality does each metric (BLEU, ROUGE, METEOR, BERTScore) primarily measure?

## Architecture Onboarding

- Component map:
  - Pre-training Phase: MLM and CLM models (BERT, RoBERTa, T5, BART) trained on datasets (MIMIC-III, MPST, PAN)
  - Generation Phase: Token masking strategies (random, stopwords, punctuation, NER) applied to corrupt text
  - Evaluation Phase: Quantitative metrics (BLEU, ROUGE, METEOR, BERTScore) and human evaluation for coherence and grammar
  - Downstream Application: NER, Text Classification, and Authorship Verification tasks using generated synthetic data

- Critical path:
  1. Pre-train models on domain-specific datasets
  2. Apply masking strategies to corrupt original text
  3. Generate synthetic text using MLM or CLM models
  4. Evaluate generated text with quantitative and qualitative metrics
  5. Use generated text for downstream tasks and evaluate performance

- Design tradeoffs:
  - MLM vs CLM: MLM offers better generation quality but may be computationally more expensive due to iterative masking/unmasking process
  - Masking ratio: Lower ratios preserve more context but may reduce diversity in generated data
  - Domain-specific vs general models: Domain-specific models may not always outperform general models in generation tasks

- Failure signatures:
  - Poor generation quality: High BLEU/ROUGE scores not correlating with human evaluation scores
  - Downstream task failure: Models trained on synthetic data perform significantly worse than those trained on real data
  - Computational issues: MLM generation taking prohibitively long due to iterative process

- First 3 experiments:
  1. Compare MLM and CLM generation quality using BLEU, ROUGE, METEOR, and BERTScore on a small sample from each dataset
  2. Test different masking strategies (random, stopwords, punctuation, NER) with varying ratios to find optimal approach for each model-dataset combination
  3. Evaluate the usefulness of generated text in one downstream task (e.g., NER) to establish baseline performance before expanding to other tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MLM consistently outperform CLM across diverse text generation tasks beyond those studied (medical discharge summaries, movie plot synopses, and authorship verification)?
- Basis in paper: [explicit] The authors explicitly state that MLM "consistently outperforms CLM in text generation across all datasets" but only tested three specific domains
- Why unresolved: The study only examined three specific datasets; broader generalization requires testing on more varied text generation tasks
- What evidence would resolve it: Systematic evaluation of MLM vs CLM on a wider range of text generation tasks including creative writing, dialogue generation, and summarization across multiple domains

### Open Question 2
- Question: What is the relationship between the quality of MLM-generated text and its usefulness in downstream tasks for different types of NLP tasks?
- Basis in paper: [explicit] The authors found "no strong correlation between the quality of the generated text and the performance of the models in the downstream tasks"
- Why unresolved: The study only tested three downstream tasks (NER, text classification, authorship verification); the relationship may vary for other task types
- What evidence would resolve it: Systematic testing of MLM-generated text across a broader range of downstream tasks including sentiment analysis, question answering, and machine translation to identify patterns in task-specific utility

### Open Question 3
- Question: Does the masking strategy (random, stopwords, punctuation, NER) affect the quality and usefulness of MLM-generated text differently across domains?
- Basis in paper: [explicit] The authors tested different masking approaches but found that punctuation alone achieved the highest performance for CLM models while showing very low performance for MLM models
- Why unresolved: The study only examined a limited set of masking strategies and did not systematically explore the interaction between masking approaches and domain characteristics
- What evidence would resolve it: Comprehensive evaluation of additional masking strategies (such as syntactic categories, semantic roles, or entity types) across multiple domains to identify optimal masking approaches for different text generation contexts

## Limitations
- The study only tested three specific datasets, limiting generalizability to other text generation domains
- No strong correlation was found between generation quality and downstream task performance, suggesting synthetic data utility may be task-specific
- Computational overhead of MLM generation was noted but not thoroughly analyzed for practical scalability

## Confidence

**High Confidence:** The finding that MLM outperforms CLM across quantitative metrics (BLEU, ROUGE, METEOR, BERTScore) and human evaluation is well-supported by the study's methodology and results. The consistent pattern across three diverse datasets strengthens this conclusion.

**Medium Confidence:** The claim that MLM can generate tokens in any order and location is technically accurate but may overstate practical capabilities. The iterative masking/unmasking process, while theoretically flexible, may have practical constraints not fully explored in the study.

**Medium Confidence:** The observation of no strong correlation between generation quality and downstream task performance is notable but requires further investigation. The study doesn't explore potential reasons for this decoupling or whether certain downstream tasks might be more sensitive to synthetic data quality than others.

## Next Checks

1. **Correlation Analysis Refinement:** Conduct a more granular analysis of the relationship between generation quality metrics and downstream task performance, potentially using regression analysis to identify any non-linear relationships or threshold effects.

2. **Human Evaluation Validation:** Implement a second round of human evaluation with different raters and additional evaluation criteria (e.g., factual consistency, relevance) to verify the coherence and grammar assessments and reduce potential bias.

3. **Computational Efficiency Study:** Measure and compare the computational costs (time, resources) of MLM vs CLM generation across different dataset sizes and model architectures to better understand practical scalability limitations.