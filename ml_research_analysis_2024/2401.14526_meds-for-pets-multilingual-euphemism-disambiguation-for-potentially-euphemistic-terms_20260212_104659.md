---
ver: rpa2
title: 'MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially Euphemistic
  Terms'
arxiv_id: '2401.14526'
source_url: https://arxiv.org/abs/2401.14526
tags:
- examples
- language
- data
- multilingual
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates cross-lingual euphemism disambiguation
  using a multilingual transformer model. The authors fine-tune XLM-RoBERTa on euphemism
  data from Chinese, English, Spanish, and Yoruba, and test it on all languages.
---

# MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially Euphemistic Terms

## Quick Facts
- arXiv ID: 2401.14526
- Source URL: https://arxiv.org/abs/2401.14526
- Authors: Patrick Lee; Alain Chirino Trujillo; Diana Cuevas Plancarte; Olumide Ebenezer Ojo; Xinyi Liu; Iyanuoluwa Shode; Yuan Zhao; Jing Peng; Anna Feldman
- Reference count: 6
- Key outcome: Multilingual training significantly improves euphemism disambiguation performance over monolingual training for Chinese, English, and Spanish, with evidence of zero-shot cross-lingual transfer and preliminary findings suggesting semantic categories influence transfer effectiveness.

## Executive Summary
This study investigates cross-lingual euphemism disambiguation using a multilingual transformer model. The authors fine-tune XLM-RoBERTa on euphemism data from Chinese, English, Spanish, and Yoruba, and test it on all languages. They find cases of zero-shot learning across languages and that multilingual training significantly improves performance over monolingual training for Chinese, English, and Spanish. They also explore whether cross-lingual transfer is related to euphemistic semantic categories, finding some evidence that out-of-language examples from the same category can outperform in-language examples from different categories.

## Method Summary
The study uses XLM-RoBERTa-base to fine-tune on euphemism datasets in four languages (Mandarin Chinese, American English, Spanish, and Yoruba). Each language has 1800 training examples (80-10-10 train-val-test split). The model is trained in both monolingual and multilingual settings with batch size 16, learning rate 1e-5, max epochs 30, and early stopping patience 5. Macro-F1 scores are used for evaluation, with statistical significance assessed via paired t-tests. Zero-shot cross-lingual transfer is tested by training on one language and evaluating on another. A follow-up analysis examines whether semantic categories (death, bodily functions, etc.) influence cross-lingual transfer by comparing same-category out-of-language (SC-OOL) versus same-language other-category (SL-OOC) training strategies.

## Key Results
- Multilingual training significantly improves euphemism disambiguation performance over monolingual training for Chinese, English, and Spanish (p<0.05)
- Zero-shot cross-lingual transfer occurs, with English-on-Chinese scoring 0.607 and Spanish-on-English scoring 0.639
- Preliminary evidence suggests SC-OOL examples outperform SL-OOC in 7 out of 16 language-category scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual training improves euphemism disambiguation performance over monolingual training for certain languages.
- Mechanism: When a model is trained on euphemism data from multiple languages, it learns cross-lingual patterns and transferable knowledge about euphemisms, which enhances its ability to disambiguate euphemisms in each language.
- Core assumption: Euphemisms share some universal semantic and syntactic properties across languages that can be learned and transferred by multilingual models.
- Evidence anchors:
  - [abstract] "We also show cases where multilingual models perform better on the task compared to monolingual models by a statistically significant margin, indicating that multilingual data presents additional opportunities for models to learn about cross-lingual, computational properties of euphemisms."
  - [section 4.2] "Lastly, we observed that the performances of the multilingual models were generally higher than those of the monolingual models. The boldfaced values in each column indicate the best setting for that test language, which was always multilingual."
- Break condition: If euphemisms are highly culture-specific with little to no overlap in semantic categories or linguistic structures across languages, multilingual training would not provide benefits and might even introduce noise.

### Mechanism 2
- Claim: Zero-shot cross-lingual transfer occurs for euphemism disambiguation, where a model trained on one language can perform the task on another language without seeing any examples from that language.
- Mechanism: The multilingual transformer model learns general representations of euphemistic language that can be applied to new languages, even if those languages were not present in the pre-training data (like Yoruba).
- Evidence anchors:
  - [abstract] "In line with current trends, we demonstrate that zero-shot learning across languages takes place."
  - [section 4.2] "Secondly, we observed an extent of zero-shot, cross-lingual learning taking place with the monolingual models (white cells). For instance, the English-on-Chinese score was 0.607, and Spanish-on-English was 0.639."
- Break condition: If the linguistic structures and cultural contexts of euphemisms are too dissimilar across languages, the model would not be able to transfer knowledge effectively, resulting in poor zero-shot performance.

### Mechanism 3
- Claim: The cross-lingual transfer of euphemism disambiguation knowledge is related to euphemistic semantic categories (e.g., death, bodily functions).
- Mechanism: Euphemisms within the same semantic category across different languages share common linguistic and conceptual patterns that the model can learn and transfer, leading to better performance when training and testing on the same category across languages.
- Evidence anchors:
  - [abstract] "In a follow-up analysis, we focus on universal euphemistic 'categories' such as death and bodily functions among others. We test to see whether cross-lingual data of the same domain is more important than within-language data of other domains to further understand the nature of the cross-lingual transfer."
  - [section 5] "While this is interesting, since we would expect that training on same-language examples should generally perform better, there are no obvious patterns with either language or category (except perhaps that Spanish did not generally benefit from SC-OOL examples). Despite this, the results suggest the overall possibility that examples which contribute cross-lingual understanding are related by semantic category."
- Break condition: If euphemistic categories are not universal or do not share common patterns across languages, training on the same category across languages would not provide benefits over training on different categories within the same language.

## Foundational Learning

- Concept: Cross-lingual transfer
  - Why needed here: The study investigates whether a model trained on euphemism data from one language can perform well on euphemism disambiguation in another language, which is a form of cross-lingual transfer.
  - Quick check question: What is the difference between zero-shot and few-shot cross-lingual transfer?

- Concept: Multilingual transformer models
  - Why needed here: The study uses XLM-RoBERTa, a multilingual transformer model, to investigate cross-lingual euphemism disambiguation. Understanding how these models work is crucial for interpreting the results.
  - Quick check question: How does a multilingual transformer model differ from a monolingual one in terms of architecture and training?

- Concept: Euphemistic semantic categories
  - Why needed here: The study explores whether cross-lingual transfer is related to euphemistic semantic categories, such as death and bodily functions. Understanding these categories is essential for interpreting the follow-up analysis.
  - Quick check question: What are some common euphemistic semantic categories that exist across different languages and cultures?

## Architecture Onboarding

- Component map: Datasets (Chinese, English, Spanish, Yoruba) -> Preprocessing (boundary tokens for Yoruba) -> XLM-RoBERTa-base -> Fine-tuning pipeline -> Evaluation (Macro-F1) -> Analysis (cross-lingual, semantic categories)
- Critical path: Fine-tuning multilingual model on combined euphemism data from multiple languages, then evaluating cross-lingual performance and analyzing semantic category effects
- Design tradeoffs: Using XLM-RoBERTa-base balances performance with computational efficiency, but may limit effectiveness for languages not in pre-training (like Yoruba). A larger model could improve performance but at higher computational cost.
- Failure signatures: Poor performance on Yoruba indicates limitations of pre-training coverage; inconsistent cross-lingual transfer suggests euphemistic patterns may be too language-specific; lack of semantic category patterns would suggest no universal euphemistic structure.
- First 3 experiments:
  1. Train monolingual models on each language separately and evaluate on in-language test sets
  2. Train multilingual model on all four languages and evaluate on each language's test set
  3. Train monolingual model on one language and evaluate on another language's test set (zero-shot transfer)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific language combinations and euphemistic categories interact to influence cross-lingual transfer?
- Basis in paper: [explicit] The paper found preliminary evidence that cross-lingual transfer may be related to euphemistic semantic categories, with SC-OOL examples performing better than SL-OOC in 7 out of 16 language-category scenarios.
- Why unresolved: The study only explored a limited set of language combinations and categories, and no clear patterns emerged. More testing with specific language pairs and categories is needed.
- What evidence would resolve it: Conducting experiments with a wider range of language pairs (including low-resource languages) and all possible euphemistic categories, while controlling for other variables, would clarify the interaction between language combinations and categories in cross-lingual transfer.

### Open Question 2
- Question: What is the optimal data size and distribution for multilingual euphemism disambiguation models?
- Basis in paper: [inferred] The paper observed that cross-lingual scores were slightly higher when using 1500 examples compared to 1800, suggesting a relationship between data size and cross-lingual performance.
- Why unresolved: The study only tested two data sizes (1500 and 1800 examples) and did not systematically explore the impact of data distribution across languages and categories.
- What evidence would resolve it: Conducting experiments with various data sizes and distributions, while keeping other parameters constant, would reveal the optimal data size and distribution for multilingual euphemism disambiguation models.

### Open Question 3
- Question: How does the pre-training scheme of multilingual models affect their performance on euphemism disambiguation tasks?
- Basis in paper: [explicit] The paper used XLM-R, which was pre-trained on multiple languages but not Yoruba, and observed poorer performance on Yoruba compared to other languages.
- Why unresolved: The study did not investigate the impact of XLM-R's pre-training scheme on euphemism disambiguation performance, nor did it compare it with other pre-trained models.
- What evidence would resolve it: Comparing the performance of XLM-R and other multilingual models (e.g., mBERT, mT5) with different pre-training schemes on euphemism disambiguation tasks would clarify the impact of pre-training on model performance.

## Limitations

- Yoruba performance limitations: XLM-RoBERTa was not pre-trained on Yoruba, resulting in poorer cross-lingual transfer performance for this language compared to others
- Limited semantic category analysis: The follow-up analysis had small sample sizes per category and lacked statistical significance testing, making it difficult to draw definitive conclusions
- No explicit error analysis: The study does not provide detailed error analysis to understand when and why cross-lingual transfer succeeds or fails

## Confidence

- Multilingual training improves performance: Medium confidence - statistically significant improvements shown for three languages, but effect sizes vary
- Zero-shot cross-lingual transfer occurs: Medium confidence - evidence demonstrated but performance varies significantly across language pairs
- Cross-lingual transfer relates to semantic categories: Low confidence - preliminary patterns observed but lack statistical significance and have small sample sizes

## Next Checks

1. **Error analysis validation**: Conduct a detailed error analysis on the Yoruba test set to understand whether poor performance stems from lack of pre-training data, cultural differences in euphemism usage, or model architecture limitations. Compare error patterns between Yoruba and the other languages to identify systematic issues.

2. **Category-specific statistical testing**: Perform statistical significance testing on the semantic category analysis results using appropriate tests (e.g., McNemar's test for paired classification results) to determine whether observed differences between SC-OOL and SL-OOC training strategies are statistically significant or could occur by chance.

3. **Cross-lingual embedding analysis**: Visualize and analyze the multilingual embeddings learned by XLM-RoBERTa to determine whether euphemistic terms from different languages cluster together by semantic category rather than by language. This would provide evidence for whether the model is learning cross-lingual semantic patterns or simply memorizing language-specific features.