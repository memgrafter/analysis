---
ver: rpa2
title: 'Towards Ideal Temporal Graph Neural Networks: Evaluations and Conclusions
  after 10,000 GPU Hours'
arxiv_id: '2412.20256'
source_url: https://arxiv.org/abs/2412.20256
tags:
- node
- memory
- neighbor
- temporal
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of Temporal Graph
  Neural Networks (TGNNs) using over 10,000 GPU hours. The authors identify critical
  issues in existing TGNN evaluations, including inadequate exploration of the design
  space and unoptimized implementations that hinder fair comparisons.
---

# Towards Ideal Temporal Graph Neural Networks: Evaluations and Conclusions after 10,000 GPU Hours

## Quick Facts
- arXiv ID: 2412.20256
- Source URL: https://arxiv.org/abs/2412.20256
- Reference count: 40
- This paper presents a comprehensive evaluation of Temporal Graph Neural Networks (TGNNs) using over 10,000 GPU hours

## Executive Summary
This paper addresses critical evaluation gaps in Temporal Graph Neural Networks (TGNNs) by presenting a comprehensive analysis of 10,000 GPU hours across diverse architectures and datasets. The authors identify that existing evaluations often fail to systematically explore the design space or optimize implementations, leading to unreliable comparisons. Through a modular evaluation framework with optimized code, they analyze four key TGNN components: neighbor sampling, node memory, neighbor aggregation, and prediction modules.

The study reveals several counterintuitive findings: most recent neighbor sampling outperforms uniform sampling; attention-based aggregators are superior to MLP-Mixer aggregators; static node memory can be as effective as RNN-based memory; and deeper neighbor sampling provides minimal gains when combined with node memory. Most importantly, the research shows that optimal TGNN performance can be achieved with small neighbor sampling budgets (e.g., 10 neighbors for 1-layer models) when combined with node memory, demonstrating superior cost-effectiveness compared to pure sampling approaches.

## Method Summary
The authors develop a modular evaluation framework to systematically analyze TGNN architectures. They evaluate seven real-world datasets (Reddit, Wikipedia, Amazon, MOOC, SuperUser, LastFM, Flickr) and two synthetic datasets with controlled repetition patterns. The framework tests various combinations of neighbor sampling strategies (uniform vs most recent), node memory types (RNN vs static embedding tables), neighbor aggregators (attention vs MLP-Mixer), and prediction modules. Each experiment is run multiple times with different random seeds, and performance is measured using ROC-AUC and PR-AUC metrics. The evaluation covers both transductive and inductive settings, with inductive testing on the Reddit dataset.

## Key Results
- Most recent neighbor sampling outperforms uniform sampling across diverse TGNN architectures and datasets
- Attention-based neighbor aggregators are superior to MLP-Mixer aggregators
- Static node memory is an effective alternative to RNN-based memory, with effectiveness depending on dataset repetition patterns
- Deeper/wider neighbor sampling provides minimal performance gains when node memory is used
- TGNNs achieve optimal performance with a small neighbor sampling budget (e.g., 10 neighbors for 1-layer models) when combined with node memory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Most recent neighbor sampling outperforms uniform neighbor sampling across diverse TGNN architectures and datasets.
- **Mechanism**: The most recent neighbor sampler prioritizes temporally relevant interactions, capturing short-term repetition patterns that are more predictive of future links in dynamic graphs.
- **Core assumption**: Temporal locality is a dominant factor in link prediction accuracy, and recent interactions are more likely to reflect current network states.
- **Evidence anchors**:
  - [abstract]: "most recent neighbor sampling and attention aggregator outperform uniform neighbor sampling and MLP-Mixer aggregator"
  - [section 4.1]: "Our results indicate that TGNN models do not need many supporting neighbors or deep model architecture to perform well. Specifically, while some models improve as more neighbors are sampled, models with the highest-performing combinations reach their peaks quickly with less than 10 neighbors in 1-layer models"
  - [corpus]: Weak - no direct evidence in related papers
- **Break condition**: In datasets with strong long-term repetition patterns, uniform sampling may capture periodic interactions that recent sampling misses.

### Mechanism 2
- **Claim**: Static node memory with embedding tables can outperform RNN-based dynamic memory, depending on dataset repetition patterns.
- **Mechanism**: Embedding table-based memory effectively captures long-term repetition patterns through learned static representations, while RNN memory better handles short-term temporal dependencies through sequential updates.
- **Core assumption**: The effectiveness of node memory depends on the temporal structure of the underlying graph data.
- **Evidence anchors**:
  - [abstract]: "Assessing static node memory as an effective node memory alternative, and showing that the choice between static or dynamic node memory should be based on the repetition patterns in the dataset"
  - [section 4.2]: "We observe an alignment between the repetitions and a preference towards RNN or emb node memory: datasets with long-term repetition patterns have a strong preference towards emb node memory, while those with short-term repetition patterns tend to favor RNN"
  - [corpus]: Weak - no direct evidence in related papers
- **Break condition**: When dataset repetition patterns are mixed or when inductive settings require capturing new node information during test time.

### Mechanism 3
- **Claim**: Combining node memory with deep neighbor sampling architectures provides minimal performance gains due to overlapping receptive field effects.
- **Mechanism**: Node memory already captures temporal neighborhood information, making deep sampling architectures redundant as they attempt to capture similar information through expanded receptive fields.
- **Core assumption**: Node memory and deep sampling architectures target the same underlying information - temporal neighborhood context.
- **Evidence anchors**:
  - [abstract]: "deeper/wider neighbor sampling provides minimal performance gains when node memory is used"
  - [section 4.1]: "Combining node memory with a deeper neighbor sampling strategy proves ineffective due to their overlapping effects"
  - [corpus]: Weak - no direct evidence in related papers
- **Break condition**: When node memory implementations are particularly poor at capturing temporal context, deep sampling may provide compensatory benefits.

## Foundational Learning

- **Concept**: Temporal graph representation learning
  - Why needed here: Understanding how dynamic graphs differ from static graphs and why specialized architectures are needed
  - Quick check question: What are the key challenges in representing time-evolving graph structures compared to static graphs?

- **Concept**: Message passing in graph neural networks
  - Why needed here: TGNNs build on GNN message-passing frameworks, but add temporal awareness to the aggregation process
  - Quick check question: How does message passing differ between static GNNs and temporal GNNs?

- **Concept**: Sampling strategies in large-scale graph learning
  - Why needed here: Neighbor sampling is crucial for scalability in both static and temporal graphs, with different strategies having different tradeoffs
  - Quick check question: What are the computational implications of uniform vs. most recent neighbor sampling in temporal graphs?

## Architecture Onboarding

- **Component map**: Update → Sample → Aggregate → Predict
- **Critical path**: The forward pass through neighbor sampling, node memory update, neighbor aggregation, and prediction represents the critical computational path for inference.
- **Design tradeoffs**:
  - Memory vs. Sampling: Node memory can reduce sampling requirements but may miss important temporal context
  - Depth vs. Width: Deeper architectures increase receptive field but may introduce generalization gaps
  - Static vs. Dynamic Memory: Static memory is more efficient but less adaptive to temporal changes
- **Failure signatures**:
  - Performance plateaus with increasing sampling budget indicate overlapping effects
  - Inconsistent performance across datasets suggests module-dataset mismatches
  - High training time with minimal accuracy gains suggests inefficient module combinations
- **First 3 experiments**:
  1. Compare most recent vs uniform sampling on a small dataset with both RNN and embedding memory
  2. Test different sampling budgets (1, 5, 10, 20 neighbors) with attention aggregator and MR sampling
  3. Evaluate static vs RNN memory on datasets with known short-term vs long-term repetition patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between dataset repetition patterns (short-term vs long-term) and node memory effectiveness across different temporal graph domains?
- Basis in paper: [explicit] The paper explicitly investigates this through synthetic datasets and empirical analysis of seven real-world datasets, showing that short-term repetitive datasets favor RNN-based memory while long-term repetitive datasets favor static embedding-based memory.
- Why unresolved: While the paper demonstrates this relationship exists and provides theoretical justification, the exact mathematical characterization of how different repetition patterns affect memory effectiveness remains unclear.
- What evidence would resolve it: Systematic analysis of additional temporal graph datasets with varying repetition patterns, coupled with quantitative metrics linking specific repetition characteristics to memory performance.

### Open Question 2
- Question: How do different negative sampling strategies (1:1, 1:9, 1:49 ratios) affect the relative performance of node memory types across datasets with different repetition patterns?
- Basis in paper: [inferred] The paper mentions using 1:1, 1:9, and 1:49 positive:negative ratios but doesn't systematically explore how these ratios interact with node memory effectiveness across different repetition patterns.
- Why unresolved: The paper focuses on memory effectiveness but doesn't isolate the interaction between negative sampling strategies and memory types across different dataset characteristics.
- What evidence would resolve it: Controlled experiments varying negative sampling ratios while holding other factors constant, across datasets with known short-term and long-term repetition patterns.

### Open Question 3
- Question: What is the optimal combination of node memory type and sampling strategy for maximizing performance on mixed-repetition pattern datasets like SuperUser?
- Basis in paper: [explicit] The paper identifies SuperUser as having mixed repetition patterns where both RNN and embedding-based memory show similar performance, but doesn't provide a clear recommendation for optimal combinations.
- Why unresolved: While the paper identifies the dataset's mixed nature, it doesn't systematically explore combinations of memory types with different sampling strategies to find optimal configurations.
- What evidence would resolve it: Systematic experimentation with all memory-sampling combinations on mixed-pattern datasets, including analysis of how different configurations perform across different time windows.

## Limitations
- The study focuses on transductive link prediction settings, which may not generalize to inductive scenarios
- Relatively limited number of datasets tested (6 total) for node memory effectiveness conclusions
- The evaluation of relatively shallow architectures (1-3 layers) may not capture benefits of deeper networks

## Confidence
- Most recent sampling superiority: High confidence
- Attention aggregator superiority: High confidence
- Node memory effectiveness: Medium confidence
- Deeper sampling minimal gains: Medium confidence

## Next Checks
1. **Dataset Diversity Validation**: Test the proposed design principles (most recent sampling + attention aggregator + node memory) on additional temporal graph datasets with varying temporal characteristics, particularly those with strong periodic patterns or longer time horizons.

2. **Inductive Setting Validation**: Evaluate the framework's effectiveness in inductive link prediction scenarios where test nodes are not seen during training, assessing whether the sampling and memory strategies transfer to new nodes.

3. **Memory Mechanism Comparison**: Systematically compare different node memory implementations (attention-based, gated, hierarchical) against the basic RNN and embedding table approaches to determine if the observed effectiveness is specific to the implementation or a more general principle.