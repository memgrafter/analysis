---
ver: rpa2
title: 'Black Boxes and Looking Glasses: Multilevel Symmetries, Reflection Planes,
  and Convex Optimization in Deep Networks'
arxiv_id: '2410.04279'
source_url: https://arxiv.org/abs/2410.04279
tags:
- test
- networks
- training
- lasso
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a novel geometric interpretation of deep neural\
  \ networks by establishing an equivalence between training deep networks with absolute\
  \ value activation and solving convex Lasso problems with specific geometric features.\
  \ The key insight is that deep networks inherently favor symmetric structures in\
  \ their fitted functions, with greater depth enabling multilevel symmetries\u2014\
  symmetries within symmetries."
---

# Black Boxes and Looking Glasses: Multilevel Symmetries, Reflection Planes, and Convex Optimization in Deep Networks

## Quick Facts
- arXiv ID: 2410.04279
- Source URL: https://arxiv.org/abs/2410.04279
- Authors: Emi Zeger; Mert Pilanci
- Reference count: 40
- This paper provides a novel geometric interpretation of deep neural networks by establishing an equivalence between training deep networks with absolute value activation and solving convex Lasso problems with specific geometric features.

## Executive Summary
This paper presents a groundbreaking geometric interpretation of deep neural networks by proving that training narrow deep networks with absolute value activation functions is equivalent to solving convex Lasso optimization problems with reflection plane features. The key insight reveals that deep networks inherently favor symmetric structures in their learned functions, with deeper networks enabling increasingly complex multilevel symmetries - symmetries within symmetries. The authors establish that while shallow networks learn simple reflection features about training points, deeper networks learn both first and second-order reflections, providing a principled explanation for why depth matters in neural network architectures.

## Method Summary
The authors establish a mathematical equivalence between training narrow deep neural networks with absolute value activation functions and solving convex Lasso problems. They prove that for a network with L layers and constant width, the optimization problem can be transformed into finding sparse combinations of geometric features representing distances to reflection planes. These reflection planes are orthogonal to optimal weight vectors and pass through training data points. The proof leverages techniques from geometric algebra and convex optimization, showing that deeper networks learn increasingly complex reflection features - 3-layer networks learn first-order reflections while 4-layer networks learn both first and second-order reflections. The theoretical results are validated through numerical experiments on both simulated data and real-world text embeddings from large language models.

## Key Results
- Deep networks with absolute value activation are mathematically equivalent to convex Lasso problems with reflection plane features
- Shallow networks (L=2) learn distance features between training points, while deeper networks learn increasingly complex reflection features
- Numerical experiments on IMDB sentiment and GLUE tasks demonstrate multi-level symmetries consistent with theoretical predictions
- The framework provides global optimization guarantees and interpretable geometric features for deep learning

## Why This Works (Mechanism)
The equivalence works because the absolute value activation function creates piecewise linear decision boundaries that can be expressed as convex combinations of reflection features. Each layer in the network adds another level of reflection symmetry, with deeper networks learning higher-order reflections that capture increasingly complex geometric relationships in the data. The narrow width constraint ensures that the network's representational capacity matches the finite set of reflection features in the equivalent convex problem, enabling the mathematical equivalence.

## Foundational Learning
- **Convex Optimization**: Needed to understand the Lasso problem equivalence and global optimization guarantees; quick check: verify Karush-Kuhn-Tucker conditions for simple convex problems
- **Geometric Algebra**: Required for understanding reflection planes and their algebraic properties; quick check: demonstrate reflection of a vector across a plane using geometric algebra
- **Neural Network Training Dynamics**: Essential for interpreting how different activation functions affect learned representations; quick check: compare training trajectories of ReLU vs absolute value networks
- **Symmetry Groups**: Important for understanding multilevel symmetries and their mathematical structure; quick check: identify symmetry groups in simple geometric patterns
- **Lasso Regularization**: Critical for understanding the sparsity-inducing properties in the equivalent convex formulation; quick check: implement Lasso regression and observe feature selection
- **Reflection Transformations**: Fundamental to the geometric interpretation of network features; quick check: verify that reflecting twice across the same plane returns the original point

## Architecture Onboarding
- **Component Map**: Input -> Absolute Value Layers -> Output; mathematically equivalent to Convex Lasso Problem with Reflection Features
- **Critical Path**: Training data points define reflection planes -> Network layers learn sparse combinations of these reflections -> Deeper layers create multilevel symmetries
- **Design Tradeoffs**: Narrow width enables mathematical equivalence but limits representational capacity; absolute value activation enables convex optimization but may restrict expressivity compared to smooth activations
- **Failure Signatures**: Networks may fail to learn when reflection features are insufficient to capture data structure; shallow networks may underfit complex symmetries; the narrow width constraint may limit performance on high-dimensional tasks
- **First Experiments**: 1) Train networks of varying depths on synthetic data with known symmetries and visualize learned reflection planes 2) Compare performance of networks with different activation functions on standard benchmarks 3) Remove reflection features from trained networks and measure impact on performance

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical results rely on narrow width constraints and absolute value activation, which may not generalize to practical deep learning architectures
- The geometric interpretation may not capture all aspects of deep network behavior, particularly for complex real-world datasets
- The framework's applicability to varying-width networks or other activation functions like ReLU and GELU remains unproven

## Confidence
- High confidence in the mathematical equivalence between narrow deep networks and convex Lasso problems under stated assumptions
- Medium confidence in the geometric interpretation of reflection planes and symmetries, pending broader empirical validation
- Low confidence in the practical implications for general deep learning applications beyond the studied cases

## Next Checks
1. Test the theoretical predictions on networks with varying widths and different activation functions to assess the robustness of the reflection plane interpretation
2. Conduct ablation studies removing reflection features from trained networks to quantify their contribution to performance on real tasks
3. Apply the geometric framework to analyze transfer learning scenarios where networks are fine-tuned from pre-trained weights to validate if reflection symmetries persist or evolve