---
ver: rpa2
title: 'Fragile Giants: Understanding the Susceptibility of Models to Subpopulation
  Attacks'
arxiv_id: '2410.08872'
source_url: https://arxiv.org/abs/2410.08872
tags:
- poisoning
- subpopulation
- attacks
- subpopulations
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how model complexity influences vulnerability
  to subpopulation poisoning attacks, where adversaries manipulate training data to
  degrade model performance on specific subgroups. The authors propose a theoretical
  framework showing that overparameterized models are more susceptible to these attacks
  due to their increased capacity to memorize rare subpopulations.
---

# Fragile Giants: Understanding the Susceptibility of Models to Subpopulation Attacks

## Quick Facts
- arXiv ID: 2410.08872
- Source URL: https://arxiv.org/abs/2410.08872
- Reference count: 40
- Models with higher capacity are more vulnerable to subpopulation poisoning attacks due to increased memorization capabilities

## Executive Summary
This paper investigates how model complexity influences vulnerability to subpopulation poisoning attacks, where adversaries manipulate training data to degrade model performance on specific subgroups. The authors propose a theoretical framework showing that overparameterized models are more susceptible to these attacks due to their increased capacity to memorize rare subpopulations. Through extensive experiments on three real-world datasets (Adult, CivilComments, and CelebA) using various model architectures, they demonstrate that larger, more complex models are significantly more vulnerable to subpopulation poisoning. They find that while small subgroups are difficult to poison, medium-sized subgroups show disproportionate susceptibility to larger models. The results highlight the need for developing defenses that specifically address subpopulation vulnerabilities, particularly as models become more complex in pursuit of better performance.

## Method Summary
The authors investigate subpopulation poisoning attacks by first defining semantically meaningful subpopulations in three real-world datasets using annotations. They then apply label flipping poisoning attacks with varying strengths (poisoning ratios) to target subpopulations. Models of varying complexity (MLPs for tabular data, BERT variants for text, and ResNet variants for images) are trained on both clean and poisoned data. The primary metric used is "target damage," which measures the difference in accuracy on the target subpopulation between clean and poisoned models. The experiments systematically vary model complexity, attack strength, and subpopulation size to identify patterns in vulnerability.

## Key Results
- Larger, more complex models show significantly higher vulnerability to subpopulation poisoning attacks
- Medium-sized subgroups are disproportionately susceptible to poisoning compared to very small or large subgroups
- Attack effectiveness varies across datasets based on how explicit subgroup-defining features are in the training data
- Small subgroup attacks often go undetected by complex models while maintaining overall accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overparameterized models are more susceptible to subpopulation poisoning because their increased capacity enables memorization of rare subpopulations, making local decision boundaries more flexible to adversarial manipulation.
- Mechanism: As model capacity increases, the model's ability to memorize individual training samples improves. This leads to stronger local dependence on subpopulations, where predictions for a point are closely tied to the behavior of its subpopulation. An adversary can exploit this by flipping labels of a subset of samples in a target subpopulation, causing the model to misclassify the entire subpopulation.
- Core assumption: The relationship between model capacity and memorization of rare subpopulations is linear and predictable.
- Evidence anchors:
  - [abstract] "overparameterized models are more susceptible to these attacks due to their increased capacity to memorize rare subpopulations."
  - [section 4.1] "larger models are more prone to local dependence on subpopulations, which in turn suggests that these are more vulnerable to subpopulation poisoning attacks."
  - [corpus] Weak evidence. Related papers focus on data poisoning in preference learning and graph neural networks, but don't directly address the memorization mechanism.
- Break condition: If the model has regularization techniques that explicitly prevent memorization of rare samples, or if the subpopulation is too small for the attack to be effective.

### Mechanism 2
- Claim: Attacks on smaller, human-interpretable subgroups often go undetected by complex models due to their tendency to overfit on the majority of the data while neglecting rare subgroups.
- Mechanism: Complex models prioritize fitting the majority of the data to minimize overall loss. When a small subgroup is poisoned, the model may not adjust its decision boundaries significantly because the impact on overall accuracy is minimal. This allows the attack to remain stealthy.
- Core assumption: Models optimize for overall accuracy rather than subgroup-specific accuracy.
- Evidence anchors:
  - [abstract] "Moreover, we find that attacks on smaller, human-interpretable subgroups often go undetected by these models."
  - [section 5.1] "There appears to be a threshold for subgroup size below which the poisoning attack does not achieve significant target damage."
  - [corpus] No direct evidence in corpus. Related work focuses on backdoor attacks and model watermarks.
- Break condition: If the model is evaluated on subgroup-specific metrics or if the poisoning attack is large enough to significantly impact overall accuracy.

### Mechanism 3
- Claim: The susceptibility of subpopulations to poisoning attacks varies across datasets based on how explicit the subgroup-defining features are in the training data.
- Mechanism: When subgroup features are explicit in the data (e.g., tabular features), the model can easily identify and learn the subgroup boundaries. This makes the model more susceptible to targeted attacks. When features are implicit (e.g., inferred from images), the model may not effectively disentangle subgroup-specific features, leading to lower susceptibility.
- Core assumption: The model's ability to identify subgroups is directly related to the explicitness of subgroup features in the data.
- Evidence anchors:
  - [section 5] "For instance, for a subgroup that constitutes 3% of the training set in the Adult dataset, the largest model can experience up to 80% target damage, while in CivilComments and CelebA, the corresponding figures are around 35% and 25%, respectively."
  - [corpus] No direct evidence in corpus. Related work focuses on model poisoning in LLMs and graph reduction techniques.
- Break condition: If the model has explicit mechanisms for subgroup identification or if the subgroup features are engineered to be more explicit.

## Foundational Learning

- Concept: Long-tailed distributions and their impact on machine learning models
  - Why needed here: The paper's theoretical framework relies on modeling data as a mixture of subpopulations with varying frequencies, which is characteristic of long-tailed distributions.
  - Quick check question: In a long-tailed distribution, which part of the data is most likely to be memorized by overparameterized models?
- Concept: Data poisoning attacks and their categorization (availability vs. targeted)
  - Why needed here: The paper focuses on subpopulation poisoning, which is a targeted attack that aims to degrade performance on specific subgroups while maintaining overall accuracy.
  - Quick check question: How does a subpopulation poisoning attack differ from a traditional targeted poisoning attack?
- Concept: Model capacity and its relationship to memorization and generalization
  - Why needed here: The paper's core hypothesis is that increased model capacity leads to increased susceptibility to subpopulation poisoning due to enhanced memorization capabilities.
  - Quick check question: What is the trade-off between model capacity and vulnerability to subpopulation poisoning attacks?

## Architecture Onboarding

- Component map: Data preprocessing and subpopulation definition -> Model training with and without poisoning -> Evaluation of target damage across subpopulations -> Analysis of the relationship between model complexity and attack susceptibility
- Critical path:
  1. Define subpopulations based on dataset annotations
  2. Apply poisoning attacks to target subpopulations
  3. Train models of varying complexity on poisoned and clean data
  4. Evaluate target damage and overall accuracy
  5. Analyze results to identify trends in model vulnerability
- Design tradeoffs:
  - Model complexity vs. vulnerability to subpopulation poisoning
  - Attack strength vs. stealthiness
  - Subpopulation size vs. susceptibility to poisoning
- Failure signatures:
  - Unexpectedly high target damage on large subpopulations
  - Low correlation between model complexity and vulnerability
  - Inconsistent results across different datasets
- First 3 experiments:
  1. Reproduce the Gaussian experiment to visualize decision boundary shifts for different model complexities
  2. Apply poisoning attacks to the Adult dataset with varying attack strengths and evaluate target damage
  3. Compare the susceptibility of different model architectures (e.g., ResNet, BERT) to subpopulation poisoning on the CelebA and CivilComments datasets

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the findings and discussion, several important questions remain:

1. How do subpopulation poisoning attacks differ in effectiveness across model architectures beyond the ones tested (MLP, BERT, ResNet)?
2. What is the relationship between the granularity of subpopulation annotations and susceptibility to poisoning attacks?
3. How do defenses specifically designed for subpopulation poisoning attacks compare to general poisoning defenses?
4. How does the order of data presentation during training affect susceptibility to subpopulation poisoning?
5. What is the relationship between subpopulation poisoning vulnerability and other robustness metrics like adversarial robustness or fairness?

## Limitations

- The study is limited to specific model architectures (MLPs, BERT, ResNet) and may not generalize to other model types
- The relationship between feature explicitness and attack effectiveness is observed but not formally quantified
- The study focuses on label flipping attacks and doesn't explore other poisoning strategies

## Confidence

- High Confidence: The experimental results demonstrating increased vulnerability of larger models to subpopulation poisoning across all three datasets
- Medium Confidence: The theoretical framework linking model capacity to memorization of rare subpopulations and subsequent vulnerability to poisoning attacks
- Low Confidence: The generalizability of the findings to other model architectures, datasets, and attack types beyond label flipping

## Next Checks

1. Evaluate the susceptibility of other model architectures (e.g., Vision Transformers, Large Language Models) to subpopulation poisoning attacks on the same datasets to assess the generalizability of the findings
2. Investigate the effect of different regularization techniques (e.g., dropout, weight decay, data augmentation) on the vulnerability of overparameterized models to subpopulation poisoning attacks
3. Develop a metric to quantify the explicitness of subgroup features in a dataset and validate its predictive power for attack susceptibility across multiple datasets and model architectures