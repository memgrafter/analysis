---
ver: rpa2
title: 'Multi-Reranker: Maximizing performance of retrieval-augmented generation in
  the FinanceRAG challenge'
arxiv_id: '2411.16732'
source_url: https://arxiv.org/abs/2411.16732
tags:
- retrieval
- financial
- corpora
- performance
- reranker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed a high-performance finance-specific Retrieval-Augmented
  Generation (RAG) system for the FinanceRAG competition, achieving 2nd place. Their
  system optimized query expansion and corpus refinement in the pre-retrieval phase,
  employed multiple reranker models for enhanced retrieval accuracy, and introduced
  an efficient method for managing long context sizes during generation.
---

# Multi-Reranker: Maximizing performance of retrieval-augmented generation in the FinanceRAG challenge

## Quick Facts
- arXiv ID: 2411.16732
- Source URL: https://arxiv.org/abs/2411.16732
- Reference count: 14
- Primary result: Achieved 2nd place in FinanceRAG competition with NDCG@10 score of 0.63996

## Executive Summary
The Multi-Reranker system optimizes finance-specific Retrieval-Augmented Generation (RAG) through a comprehensive pipeline addressing the unique challenges of financial document retrieval and generation. The system employs query expansion techniques to handle financial abbreviations and complex reasoning, multiple reranker models to enhance retrieval accuracy, and context size management to maintain LLM performance. By combining these approaches, the system achieved 2nd place in the FinanceRAG competition, demonstrating the effectiveness of their multi-stage optimization strategy for financial question answering tasks.

## Method Summary
The Multi-Reranker system implements a three-phase approach to finance-specific RAG: pre-retrieval optimization using query expansion and corpus refinement, multi-stage reranking with lightweight and precise models, and context-aware generation with token limits. Query expansion processes raw financial queries through paraphrasing, keyword extraction, and hypothetical document generation using GPT-4o-mini, then combines these with the original query. The system employs a two-stage reranking pipeline: first extracting top 200 candidates with jina-reranker-v2-base-multilingual, then applying gte-multilingual-reranker-base and bge-reranker-v2-m3 for final selection of top 10 corpora. For generation, the system limits context to 32k tokens, splitting inputs exceeding this threshold and processing separately before fusing results.

## Key Results
- Achieved 2nd place in ACM-ICAIF '24 FinanceRAG competition
- Obtained NDCG@10 score of 0.63996 on public leaderboard
- Demonstrated effectiveness of multi-reranker approach on seven financial datasets (FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, MultiHiertt)

## Why This Works (Mechanism)

### Mechanism 1
Query expansion techniques improve retrieval effectiveness by resolving financial abbreviations and decomposing complex queries into simpler components. The system uses OpenAI/GPT-4o-mini to process raw queries through paraphrasing, keyword extraction, and hypothetical document generation, then combines these enhanced versions with the original query to create more comprehensive search terms. This works because financial queries often contain domain-specific jargon and require multi-step reasoning that simple keyword matching cannot capture. The core assumption is that language models can effectively interpret financial terminology and decompose complex reasoning tasks. Break condition occurs when financial terminology is too domain-specific for general language models to interpret correctly.

### Mechanism 2
Multiple reranker models in sequence significantly improve retrieval accuracy compared to single reranking. The system first uses a lightweight reranker (jina-reranker-v2-base-multilingual) to extract top 200 candidates, then applies more precise rerankers (gte-multilingual-reranker-base and bge-reranker-v2-m3) for final selection of top 10 corpora. This works because different reranker models capture different aspects of relevance, and sequential application allows balancing speed and precision. The core assumption is that model diversity in reranking leads to better overall relevance assessment. Break condition occurs when corpus size exceeds model capacity or when financial documents contain too much numerical data for text-based rerankers.

### Mechanism 3
Context splitting with fusion prevents LLM performance degradation while maintaining input comprehensiveness. When input exceeds 32k tokens, corpora are split into two halves (top 1-10 and 11-20), processed separately through LLM, then fused to create final answer. This works because LLM performance degrades significantly beyond 32k tokens, but splitting maintains answer quality by keeping each input within optimal range. The core assumption is that splitting documents preserves answer quality while preventing performance degradation. Break condition occurs when the answer requires cross-referencing information across the split boundary.

## Foundational Learning

- **Financial domain knowledge and terminology**: Understanding financial-specific abbreviations, jargon, and multi-step reasoning inherent in financial queries is essential for effective query processing and answer generation. Quick check: Can you explain what a "10-K report" is and why it's important in financial analysis?

- **Retrieval-Augmented Generation (RAG) architecture**: The system is built on RAG principles combining information retrieval with language model generation for financial question answering. Quick check: What are the three main stages of a RAG system and how do they interact?

- **Context window management and LLM limitations**: The system must handle long financial documents while respecting LLM context limitations to maintain performance. Quick check: Why does splitting long contexts into smaller chunks and processing them separately help maintain LLM performance?

## Architecture Onboarding

- **Component map**: Query preprocessing (expansion techniques) → Corpus preprocessing (summarization, table extraction) → First reranking stage (200 candidates) → Second reranking stage (10 final candidates) → Context management (splitting/fusion if >32k tokens) → Generation stage (LLM with engineered prompts) → Post-processing (answer formatting)

- **Critical path**: Query → Query expansion → Rerank 1 (200) → Rerank 2 (10) → Context check → Split/Fusion (if needed) → LLM generation → Answer output

- **Design tradeoffs**: Query expansion increases retrieval recall but adds processing time; multiple rerankers improve precision but increase computational cost; context splitting prevents performance degradation but risks losing cross-document relationships; using multiple rerankers per dataset optimizes for specific dataset characteristics but complicates maintenance

- **Failure signatures**: Low NDCG@10 scores indicate query expansion or reranking problems; degradation in answer quality suggests context splitting issues; long processing times point to inefficient reranking or expansion; numerical errors indicate problems with table extraction or context fusion

- **First 3 experiments**: Test baseline NDCG@10 with original queries vs. expanded queries on a small dataset subset; compare single reranker vs. multi-reranker pipeline on retrieval accuracy; validate context splitting approach by comparing answers from full context vs. split contexts on queries requiring cross-document reasoning

## Open Questions the Paper Calls Out

### Open Question 1
The paper explicitly suggests that parameter-efficient fine-tuning (PEFT) of reranker models could improve retrieval performance on finance-specific datasets. This remains unresolved because competition rules prohibited training models using the source data. Evidence that would resolve this includes empirical results comparing baseline reranker models against PEFT-tuned versions on the same finance datasets, showing improvements in NDCG@10 scores.

### Open Question 2
The optimal context size threshold for maintaining LLM performance in finance-specific RAG tasks is unclear. The authors used 32k tokens based on manual analysis but acknowledge this may not be optimal for all finance queries, while referencing other research suggesting 64k as a threshold. This requires systematic evaluation of LLM performance across various context sizes using standardized finance queries and metrics.

### Open Question 3
The impact of using pre-summarized corpora on numerical response accuracy in finance RAG systems remains unexplored. The authors found pre-summarized corpora led to significant performance drops for queries requiring specific numerical responses, but didn't detail the extent of degradation or identify which numerical query types are most affected. Comparative analysis showing performance differences between full-text and summarized corpora across various numerical query types would resolve this.

## Limitations

- Context splitting and fusion mechanism lacks detailed specifications for reconciling contradictory or complementary information across split corpora
- Exact prompt engineering methodology for ensuring financial experts receive specific numerical values is not specified
- Table extraction process for the MultiHiertt dataset is mentioned but not detailed in terms of accuracy or methodology

## Confidence

- **High Confidence**: The two-stage reranking approach using multiple models (jina-reranker-v2-base-multilingual → gte-multilingual-reranker-base/bge-reranker-v2-m3) is well-supported by the literature and achieves demonstrable results
- **Medium Confidence**: Query expansion techniques show promise but the specific implementation details and their relative contributions are not fully specified
- **Medium Confidence**: The 32k token context limit and splitting strategy is empirically justified but lacks detailed validation for cross-document reasoning scenarios

## Next Checks

1. Conduct ablation testing comparing single vs. multi-reranker pipelines on retrieval accuracy across all seven financial datasets to isolate the contribution of each reranking stage

2. Implement controlled experiments testing context splitting effectiveness by creating queries requiring information spanning split boundaries, measuring answer quality degradation

3. Validate query expansion impact by comparing NDCG@10 scores using original queries vs. expanded queries on a subset of the MultiHiertt dataset, isolating the contribution of each expansion technique