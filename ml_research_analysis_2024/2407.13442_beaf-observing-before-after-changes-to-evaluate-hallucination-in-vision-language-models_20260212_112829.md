---
ver: rpa2
title: 'BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language
  Models'
arxiv_id: '2407.13442'
source_url: https://arxiv.org/abs/2407.13442
tags:
- object
- image
- vlms
- hallucination
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEAF, a benchmark for evaluating hallucination
  in vision-language models (VLMs) by manipulating visual scenes and tracking changes
  in model responses. Unlike prior work focusing only on text-based questions, BEAF
  removes objects from images and assesses whether models correctly perceive these
  changes.
---

# BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models

## Quick Facts
- **arXiv ID**: 2407.13442
- **Source URL**: https://arxiv.org/abs/2407.13442
- **Reference count**: 40
- **Primary result**: Introduces BEAF benchmark showing VLMs often hallucinate, with LLaVA-v1.5-13B achieving only 24.3% True Understanding despite high traditional accuracy

## Executive Summary
This paper introduces BEAF, a novel benchmark for evaluating hallucination in vision-language models (VLMs) by manipulating visual scenes and tracking changes in model responses. Unlike prior work focusing only on text-based questions, BEAF removes objects from images and assesses whether models correctly perceive these changes. The authors propose four new metrics—True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID)—to measure different aspects of hallucination. Experiments on 500 MS-COCO images with 26K image-question pairs reveal that models often fail to recognize removed objects and exhibit stubbornness by repeating incorrect answers, highlighting the importance of change-aware evaluation.

## Method Summary
The BEAF benchmark manipulates MS-COCO images by removing objects using SAM for mask extraction and LaMa for inpainting, creating before/after image pairs. For each pair, questions about object presence are generated and answered by VLMs. The four proposed metrics measure different hallucination behaviors: TU for correct before/after consistency, IG for persistent ignorance, SB for stubbornness (repeating wrong answers), and ID for indecision about unmanipulated objects. The benchmark also visualizes object relationships across vision and text axes to expose inter-object dependencies that may cause hallucination.

## Key Results
- LLaVA-v1.5-13B achieves only 24.3% TU despite high traditional accuracy, demonstrating the importance of change-aware evaluation
- Models frequently exhibit stubbornness (SB), repeating incorrect answers even after scene changes
- Visualization reveals complex object relationships where altering one object affects answers about others
- VLMs struggle particularly with recognizing removed objects like traffic lights, vases, and remotes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manipulating images and tracking before/after answer changes isolates hallucination more effectively than text-only evaluation
- Mechanism: By removing objects from images and re-asking the same questions, the benchmark can detect whether a model's answer changes appropriately, revealing whether it is grounding its response in the image or relying on internal knowledge
- Core assumption: If a model truly understands the scene, its answers must reflect visual changes; otherwise, repeated answers indicate hallucination
- Evidence anchors:
  - [abstract] "Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes."
  - [section] "Our key idea is to manipulate a visual scene by removing and inpainting objects within an image. This allows us to focus on changes and to check whether the VLMs are correctly aware of it."

### Mechanism 2
- Claim: The four proposed metrics (TU, IG, SB, ID) capture different hallucination failure modes that traditional accuracy masks
- Mechanism: TU measures correct before/after consistency; IG captures persistent ignorance; SB quantifies stubbornness (repeated wrong answers); ID flags indecision about unmanipulated objects. Together, they reveal nuanced failure patterns
- Core assumption: Each hallucination type is distinguishable by observing answer patterns across before/after manipulations
- Evidence anchors:
  - [abstract] "We propose four new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID)—to measure different aspects of hallucination."
  - [section] "SB is related to the phenomenon of giving the same answer instead of correctly recognizing the image. ID measures cases where the answer changes even though the target object does not change or does not exist in the image."

### Mechanism 3
- Claim: Visualizing object relationships across vision and text axes exposes inter-object dependencies that cause hallucination
- Mechanism: By aligning removed objects (vision axis) with queried objects (text axis), the benchmark reveals whether changes in one object's presence influence answers about another, uncovering hidden scene dependencies
- Core assumption: Changes in one object's presence can alter model perception of unrelated objects if the model lacks true scene grounding
- Evidence anchors:
  - [section] "We also visualize image-wise object relationship by virtue of our two-axis view: vision and text."
  - [section] "Altering the scene affects the textual correctness...This suggests a correlation between the couch and the book, indicating that complex scenes can lead to model hallucinations even in the absence of the book."

## Foundational Learning

- **Concept: Zero-shot generalization in VLMs**
  - Why needed here: VLMs achieve high performance without fine-tuning; understanding this capability is key to interpreting why they may still hallucinate
  - Quick check question: What enables VLMs to perform well on unseen tasks without task-specific training?

- **Concept: Hallucination in multimodal models**
  - Why needed here: The paper focuses on detecting when VLMs generate content not grounded in input images; foundational knowledge of hallucination types is essential
  - Quick check question: How does hallucination in VLMs differ from that in text-only LLMs?

- **Concept: Image manipulation for evaluation**
  - Why needed here: The benchmark relies on removing objects and inpainting; understanding manipulation quality and its impact on model perception is critical
  - Quick check question: Why might imperfect object removal lead to false negatives in hallucination detection?

## Architecture Onboarding

- **Component map**: Visual encoder (CLIP-based) -> Large language model (Vicuna, LLaVA) -> Alignment module (image-to-token translation) -> Prompt template generator -> Manipulation pipeline (SAM + inpainting model) -> Metric computation engine (TU, IG, SB, ID)

- **Critical path**: 1) Load image and extract object masks via SAM 2) Remove target object using inpainting (LaMa) 3) Generate before/after question pairs 4) Run VLM on original and manipulated images 5) Compute change-aware metrics 6) Visualize object relationships

- **Design tradeoffs**: Quality vs. automation in image manipulation; Granularity of metrics vs. interpretability; Dataset size vs. coverage of object co-occurrences

- **Failure signatures**: High SB + low TU → stubborn hallucination; High IG → consistent ignorance of removed objects; High ID → random guessing on unrelated objects

- **First 3 experiments**: 1) Run LLaVA-v1.5-13B on original BEAF images and record accuracy 2) Apply object removal and re-run same model; compute TU, IG, SB, ID 3) Visualize object relationship matrix for a subset of images to identify dependency patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different object categories influence hallucination rates in VLMs when removed from scenes?
- Basis in paper: [explicit] The paper visualizes object-wise error rates and notes that VLMs frequently fail to recognize certain objects like traffic lights, vases, and remotes
- Why unresolved: While the paper identifies which objects are challenging, it does not explore the underlying reasons for these difficulties or whether specific object characteristics (e.g., complexity, training data frequency) contribute to higher hallucination rates
- What evidence would resolve it: Comparative analysis of hallucination rates across diverse object categories, examining factors such as object complexity, background interactions, and training data representation

### Open Question 2
- Question: To what extent do scene complexity and object interactions affect VLM performance in detecting removed objects?
- Basis in paper: [inferred] The paper discusses image-wise object relationships and notes that altering scenes affects textual correctness, suggesting that complex scenes may lead to model hallucinations
- Why unresolved: The paper provides qualitative insights into scene complexity but does not quantitatively measure its impact on VLM performance or identify specific interaction patterns that exacerbate hallucinations
- What evidence would resolve it: Quantitative studies correlating scene complexity metrics with hallucination rates, along with controlled experiments manipulating object interactions

### Open Question 3
- Question: Can incorporating location-aware strategies improve VLM accuracy in hallucination detection?
- Basis in paper: [explicit] The paper mentions that Shikra, which is trained using a location-aware strategy, performs better in hallucination evaluation, implying that location knowledge aids in judging object existence
- Why unresolved: While the paper highlights the benefits of location-aware training, it does not explore whether other models could achieve similar improvements or the specific mechanisms by which location awareness enhances performance
- What evidence would resolve it: Comparative studies of VLMs with and without location-aware training across various benchmarks, along with ablation studies isolating the impact of location information

## Limitations
- Reliance on manual filtering and human-guided refinement introduces potential subjectivity and scalability concerns
- Quality of object removal via inpainting is critical - any residual artifacts or shadows could cause models to detect removed objects, breaking the before/after comparison mechanism
- Benchmark's focus on object presence/absence questions may not fully capture hallucination in more complex reasoning tasks

## Confidence

- **High Confidence**: The core mechanism of using before/after image manipulation to detect hallucination is well-founded and the mathematical formulation of the four metrics (TU, IG, SB, ID) is rigorous and reproducible
- **Medium Confidence**: The claim that LLaVA-v1.5-13B achieves only 24.3% TU despite high traditional accuracy is supported by experimental results, though the generalizability across different VLM architectures needs further validation
- **Low Confidence**: The interpretation of object relationship visualizations as evidence of inter-object dependencies causing hallucination is suggestive but requires more rigorous statistical validation to rule out coincidental patterns

## Next Checks

1. **Artifact Detection Analysis**: Systematically evaluate the manipulated images for residual artifacts (shadows, outlines, blurriness) and measure their correlation with hallucination detection rates across different models
2. **Cross-Architecture Replication**: Test the BEAF benchmark on a broader range of VLM architectures beyond the four models evaluated, particularly including models with different visual encoders and alignment strategies
3. **Statistical Dependency Validation**: Apply formal statistical tests to the object relationship visualizations to determine whether observed dependencies are statistically significant or could arise by chance in the dataset