---
ver: rpa2
title: Theoretical and Empirical Insights into the Origins of Degree Bias in Graph
  Neural Networks
arxiv_id: '2404.03139'
source_url: https://arxiv.org/abs/2404.03139
tags:
- nodes
- training
- degree
- low-degree
- high-degree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper theoretically analyzes the origins of degree bias in
  graph neural networks (GNNs), where high-degree nodes tend to perform better than
  low-degree nodes on node classification tasks. The authors prove that high-degree
  test nodes have a lower probability of misclassification, regardless of how GNNs
  are trained.
---

# Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks

## Quick Facts
- arXiv ID: 2404.03139
- Source URL: https://arxiv.org/abs/2404.03139
- Authors: Arjun Subramonian; Jian Kang; Yizhou Sun
- Reference count: 40
- Key outcome: Proves high-degree nodes have lower misclassification probability in GNNs regardless of training approach

## Executive Summary
This paper provides a theoretical analysis of degree bias in graph neural networks, where high-degree nodes consistently outperform low-degree nodes on node classification tasks. The authors prove that high-degree test nodes have a lower probability of misclassification regardless of training methodology. They demonstrate that degree bias emerges from factors intrinsically linked to node degree, particularly homophily and neighbor diversity. The theoretical framework is validated across 8 real-world datasets, providing empirical support for the mathematical proofs.

## Method Summary
The authors employ a theoretical approach to analyze degree bias in graph neural networks. They begin with a linearized message-passing GNN framework to prove that high-degree nodes have lower misclassification probabilities. The analysis considers factors such as homophily (tendency of nodes to connect to similar nodes) and neighbor diversity. Theoretical bounds are established for misclassification probabilities based on node degree. The authors then validate their theoretical findings through experiments on 8 real-world graph datasets, comparing different GNN architectures and training approaches.

## Key Results
- High-degree test nodes have provably lower misclassification probability than low-degree nodes, independent of training methodology
- Degree bias arises from factors intrinsically associated with node degree, including homophily and diversity of neighbors
- During training, some GNNs adjust loss more slowly on low-degree nodes compared to high-degree nodes
- With sufficient training epochs, message-passing GNNs can achieve maximum possible training accuracy despite degree bias
- Theoretical predictions validated on 8 diverse real-world graph datasets

## Why This Works (Mechanism)
The paper establishes that degree bias in GNNs stems from fundamental structural properties of graphs rather than training artifacts. High-degree nodes benefit from having more neighbors, which provides more information for classification when the graph exhibits homophily. The theoretical framework shows that the probability of misclassification decreases with node degree because high-degree nodes have more opportunities to receive correct signals from their neighbors. This effect persists regardless of how the GNN is trained, indicating that degree bias is an inherent property of the graph structure when processed by message-passing networks.

## Foundational Learning
- **Graph Neural Networks**: Why needed - Understanding the core architecture being analyzed. Quick check - Message-passing mechanism aggregates neighbor information to update node representations.
- **Node Degree**: Why needed - Central concept being studied. Quick check - Number of edges connected to a node in the graph.
- **Homophily**: Why needed - Key factor contributing to degree bias. Quick check - Tendency of nodes to connect to similar nodes based on attributes or features.
- **Linearized Message-Passing**: Why needed - Theoretical framework assumption. Quick check - Simplified GNN model where message passing operations are linearized for analysis.
- **Misclassification Probability**: Why needed - Primary metric being analyzed. Quick check - Probability that a node's predicted label differs from its true label.

## Architecture Onboarding

Component Map:
GNN Architecture -> Message Passing -> Node Representation Update -> Classification

Critical Path:
Node features → Message aggregation → Non-linear transformation → Classification head

Design Tradeoffs:
- Linearized analysis vs. non-linear activation functions
- Transductive vs. inductive learning settings
- Homogeneous vs. heterogeneous network structures

Failure Signatures:
- Persistent performance gap between high and low-degree nodes
- Slower convergence on low-degree nodes during training
- Over-reliance on structural information over node features

Three First Experiments:
1. Test theoretical predictions on synthetic graphs with controlled homophily levels
2. Compare degree bias across different GNN architectures (GCN, GAT, GraphSAGE)
3. Analyze the effect of feature normalization on degree bias magnitude

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes linearized message-passing GNNs, not accounting for non-linear activation functions
- Empirical validation limited to 8 datasets, potentially missing edge cases
- Analysis focuses on transductive learning, leaving inductive learning behavior unexplored
- Study limited to homogeneous and undirected networks, not addressing heterogeneous or directed networks

## Confidence
High confidence in theoretical framework and mathematical proofs
Medium confidence in empirical validation scope
Medium confidence in practical implications

## Next Checks
1. Test the theoretical predictions across a broader range of graph types, including scale-free networks, temporal graphs, and graphs with varying homophily levels
2. Evaluate the proposed roadmap for alleviating degree bias on more diverse GNN architectures beyond standard message-passing models
3. Investigate the interaction between degree bias and other structural biases (e.g., community structure, node attributes) in real-world applications