---
ver: rpa2
title: 'CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network
  for Crisis Event Classification'
arxiv_id: '2401.06194'
source_url: https://arxiv.org/abs/2401.06194
tags:
- text
- multimodal
- crisiskan
- knowledge
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CrisisKAN, a knowledge-infused and explainable
  multimodal attention network for crisis event classification. The proposed model
  addresses the semantic gap between image and text modalities by integrating external
  knowledge from Wikipedia using a proposed wiki extraction algorithm and employing
  a guided cross-attention module for effective fusion.
---

# CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification

## Quick Facts
- arXiv ID: 2401.06194
- Source URL: https://arxiv.org/abs/2401.06194
- Authors: Shubham Gupta; Nandini Saini; Suman Kundu; Debasis Das
- Reference count: 40
- Key outcome: CrisisKAN achieves an overall Multi-task Model Strength (MTMS) score of 87.1% on crisis event classification

## Executive Summary
This paper introduces CrisisKAN, a knowledge-infused and explainable multimodal attention network designed for crisis event classification. The model addresses the semantic gap between image and text modalities by integrating external knowledge from Wikipedia through a proposed wiki extraction algorithm and employing a guided cross-attention module for effective fusion. To ensure reliability and interpretability, the model incorporates Gradient-weighted Class Activation Mapping (Grad-CAM) for generating visual explanations. Extensive experiments on the CrisisMMD dataset demonstrate superior performance compared to existing state-of-the-art solutions across various crisis-specific tasks and settings.

## Method Summary
CrisisKAN combines multimodal inputs (images and text) with external knowledge extracted from Wikipedia to classify crisis events. The model uses a wiki extraction algorithm to gather relevant knowledge embeddings, which are then integrated with visual and textual features through a guided cross-attention mechanism. Grad-CAM is employed to generate visual explanations, enhancing interpretability. The model is evaluated on the CrisisMMD dataset, showing significant improvements in classification accuracy, macro F1-score, and weighted F1-score across multiple crisis-specific tasks.

## Key Results
- Achieves an overall Multi-task Model Strength (MTMS) score of 87.1%
- Demonstrates superior performance compared to state-of-the-art baselines
- Shows significant improvements in classification accuracy, macro F1-score, and weighted F1-score across various crisis-specific tasks

## Why This Works (Mechanism)
CrisisKAN's effectiveness stems from its integration of external knowledge to bridge the semantic gap between image and text modalities. The wiki extraction algorithm provides contextual information that enhances feature representations, while the guided cross-attention mechanism ensures effective fusion of multimodal inputs. The incorporation of Grad-CAM not only aids in interpretability but also aligns the model's focus with human-understandable visual cues during crisis events.

## Foundational Learning
- **Multimodal Fusion**: Combining information from different modalities (text, images) is crucial for comprehensive understanding of crisis events. Quick check: Verify that the fusion mechanism preserves complementary information from both modalities.
- **Attention Mechanisms**: Guided cross-attention helps the model focus on relevant features across modalities. Quick check: Ensure attention weights highlight semantically meaningful regions and words.
- **Knowledge Infusion**: External knowledge from Wikipedia provides contextual depth to feature representations. Quick check: Validate that extracted knowledge is relevant and enhances classification performance.
- **Explainability with Grad-CAM**: Visual explanations improve model interpretability and trust. Quick check: Confirm that Grad-CAM highlights regions consistent with human reasoning for crisis classification.
- **Crisis-Specific Classification**: Tailoring models to crisis event characteristics improves performance on real-world applications. Quick check: Evaluate model performance on diverse crisis scenarios within the dataset.

## Architecture Onboarding

Component Map:
Text Encoder -> Wiki Extraction -> Knowledge Embeddings -> Cross-Attention -> Fusion Layer -> Classifier

Critical Path:
Text features → Wiki extraction → Knowledge embeddings → Cross-attention with image features → Classifier output

Design Tradeoffs:
- External knowledge integration adds computational overhead but improves semantic understanding
- Grad-CAM explanations increase interpretability but require additional processing
- Multimodal fusion enhances performance but introduces complexity in alignment and synchronization

Failure Signatures:
- Poor performance on rare crisis categories may indicate insufficient knowledge extraction
- Misalignment between visual and textual attention maps could suggest cross-attention issues
- Grad-CAM explanations that highlight irrelevant regions may indicate knowledge integration problems

First Experiments:
1. Test knowledge extraction quality by measuring relevance of Wikipedia content to crisis categories
2. Evaluate cross-attention effectiveness by comparing with simple concatenation baselines
3. Validate Grad-CAM explanations through human evaluation of highlighted regions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on CrisisMMD dataset may limit generalizability to other crisis-related datasets
- Knowledge extraction process relies on predefined keywords which may miss relevant contextual information
- Computational complexity of multiple attention mechanisms may hinder real-time deployment in resource-constrained environments

## Confidence

**High confidence**: The model's superior performance metrics compared to baselines on the CrisisMMD dataset are well-supported by experimental results

**Medium confidence**: The effectiveness of the proposed knowledge extraction algorithm and cross-attention mechanism, as these components' individual contributions are not isolated through ablation studies

**Medium confidence**: The interpretability claims based on Grad-CAM visualizations, as user studies or expert evaluations of explanation quality are not reported

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of the knowledge infusion module and cross-attention mechanism to overall performance

2. Test the model's performance on additional crisis datasets or real-world crisis data to evaluate generalizability

3. Perform computational efficiency analysis to determine model latency and resource requirements for real-time deployment scenarios