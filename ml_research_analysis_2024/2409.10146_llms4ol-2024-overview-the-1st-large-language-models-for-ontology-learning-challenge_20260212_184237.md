---
ver: rpa2
title: 'LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning
  Challenge'
arxiv_id: '2409.10146'
source_url: https://arxiv.org/abs/2409.10146
tags:
- task
- challenge
- tasks
- llms4ol
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LLMs4OL 2024 challenge explored using large language models
  for ontology learning tasks including term typing, taxonomy discovery, and relation
  extraction across six domains. Fourteen teams participated using various approaches
  including fine-tuning, prompt-tuning, and retrieval-augmented generation.
---

# LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge

## Quick Facts
- arXiv ID: 2409.10146
- Source URL: https://arxiv.org/abs/2409.10146
- Reference count: 35
- Key outcome: The challenge explored using LLMs for ontology learning tasks across six domains, with strong performance in term typing but ongoing challenges in non-taxonomic relation extraction.

## Executive Summary
The LLMs4OL 2024 challenge explored the application of large language models to ontology learning tasks including term typing, taxonomy discovery, and relation extraction across six domains. Fourteen teams participated using various approaches including fine-tuning, prompt-tuning, and retrieval-augmented generation. The challenge included both few-shot and zero-shot evaluation phases, revealing that while LLMs show strong promise for ontology learning tasks, particularly in term typing, non-taxonomic relation extraction remains challenging and requires further research. Domain-specific fine-tuning and prompt engineering emerged as key strategies for improving performance.

## Method Summary
The challenge involved three main ontology learning tasks: term typing (Task A), taxonomy discovery (Task B), and relation extraction (Task C) across six domains including WordNet, GeoNames, UMLS, GO, DBO, and FoodOn. Teams could use various approaches including fine-tuning, prompt-tuning, and retrieval-augmented generation with no restrictions on prompting methods. Evaluation was conducted through two phases: few-shot testing using partial ontologies for training and testing, and zero-shot testing using completely unseen ontologies. Performance was measured using precision, recall, and F1-score metrics.

## Key Results
- Strong performance in term typing tasks, particularly with domain-specific fine-tuning and prompt engineering
- Retrieval-augmented generation systems showed promise for improving accuracy in ontology learning
- Significant performance drops observed when transitioning from few-shot to zero-shot testing scenarios
- Non-taxonomic relation extraction remains a challenging task requiring further research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining LLMs with retrieval mechanisms improves ontology learning accuracy
- Mechanism: Retrieval-augmented generation (RAG) systems use a retriever model to fetch relevant contextual information from external knowledge sources, then pass this context to an LLM for more informed predictions
- Core assumption: The retriever can identify and provide relevant supporting information that the LLM needs to make accurate ontology learning predictions
- Evidence anchors:
  - [section]: "These approaches were used to analyze OL tasks across domains like lexicosemantics, geographical locations, biomedical concepts, and more"
  - [section]: "They incorporated Mistral-7B as LLM and Dense Passage Retrieval (DPR) model as the retriever model in the RAG framework"
- Break condition: If the retriever fails to find relevant context or if the LLM cannot effectively integrate the retrieved information into its predictions

### Mechanism 2
- Claim: Domain-specific fine-tuning significantly improves LLM performance on ontology learning tasks
- Mechanism: Fine-tuning LLMs on domain-specific data allows the models to learn specialized terminology, relationships, and patterns unique to each domain
- Core assumption: Domain-specific data contains patterns and relationships that general-purpose LLMs haven't learned during pretraining
- Evidence anchors:
  - [section]: "The proposed approach performs well on several subtasks, showcasing that incorporating domain-specific information and providing a list of classification types enhances inference performance"
  - [section]: "Fine-tuning of such model can achieve superior performance"
- Break condition: If the domain-specific data is too limited or if the fine-tuning process overfits to the training data without improving generalization

### Mechanism 3
- Claim: Different LLM architectures have complementary strengths for ontology learning
- Mechanism: Various LLM families (GPT, LLaMA, BLOOM, Mistral) each excel in different aspects - GPT models in context comprehension, LLaMA in scalability, BLOOM in multilingual capabilities, and Mistral in performance efficiency
- Core assumption: Different architectural approaches capture different aspects of language understanding and generation, making them suitable for different ontology learning subtasks
- Evidence anchors:
  - [section]: "With models like LLaMA-2