---
ver: rpa2
title: Stochastic Resetting Mitigates Latent Gradient Bias of SGD from Label Noise
arxiv_id: '2406.00396'
source_url: https://arxiv.org/abs/2406.00396
tags:
- resetting
- stochastic
- reset
- noise
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Stochastic resetting mitigates overfitting to label noise in DNN
  training by periodically restarting from optimal checkpoints. The method exploits
  a latent gradient bias induced by noisy labels, which gradually shifts the SGD drift
  toward suboptimal parameters.
---

# Stochastic Resetting Mitigates Latent Gradient Bias of SGD from Label Noise

## Quick Facts
- arXiv ID: 2406.00396
- Source URL: https://arxiv.org/abs/2406.00396
- Reference count: 40
- Primary result: Stochastic resetting improves test accuracy by 3-4% under high noise rates by periodically removing latent gradient bias from noisy labels.

## Executive Summary
This paper introduces stochastic resetting as a method to mitigate overfitting to label noise in deep neural network training. By periodically restarting from optimal checkpoints, the method exploits a latent gradient bias that accumulates when training with noisy labels. The authors provide both theoretical analysis, mapping SGD dynamics to overdamped Langevin dynamics, and extensive empirical validation across CIFAR-10/100, real-world noisy datasets, and vision transformers. The method shows consistent improvements in test accuracy, particularly under high noise rates and small-batch training conditions, with minimal computational overhead and easy integration with existing robust training techniques.

## Method Summary
The core algorithm involves standard SGD training with periodic resetting to checkpoints that correspond to parameters with minimum validation loss. During training, the model monitors validation loss and updates a checkpoint whenever validation loss improves. With a predefined reset probability r, the training parameters are reset to this checkpoint, effectively removing accumulated gradient bias from noisy labels. The method can be applied as full resetting (entire network) or partial resetting (specific layers). The optimal reset probability depends on factors like batch size and noise rate, with smaller batches and higher noise rates benefiting more from frequent resetting. The approach is compatible with existing robust training techniques and requires only minimal modification to standard training loops.

## Key Results
- Test accuracy improvements of 3-4% under high noise rates (40% symmetric label noise) on CIFAR-10/100
- Consistent performance gains across multiple architectures including CNNs and vision transformers
- Relative improvement in test accuracy (RDTAcc.) up to 0.7% on real-world noisy datasets
- Optimal reset probability exists and depends on batch size and noise rate, with benefits diminishing for large batch sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic resetting mitigates overfitting by periodically removing the latent gradient bias introduced by noisy labels.
- Mechanism: During SGD training with noisy labels, the gradient drift accumulates a bias toward incorrectly labeled samples. Resetting to a checkpoint (typically the parameters with lowest validation loss) removes this accumulated bias and restarts training closer to a cleaner gradient direction.
- Core assumption: The optimal checkpoint is one where the model has not yet strongly memorized noisy labels but still captures generalizable patterns.
- Evidence anchors:
  - [abstract] "By deconstructing the dynamics of stochastic gradient descent (SGD), we identify the behavior of a latent gradient bias induced by noisy labels, which harms generalization."
  - [section] "We reveal a latent gradient bias in the SGD dynamics induced by noisy labels, which drives the memorization effect in DNNs."
- Break condition: If checkpoints are taken too late (after memorization is complete) or too early (before general patterns are learned), the resetting effect diminishes or vanishes.

### Mechanism 2
- Claim: Resetting is most beneficial when the ratio of stochastic noise to gradient drift toward the optimum is high.
- Mechanism: When batch size is small and noise rate is high, the diffusion component of SGD dynamics dominates the drift component. Resetting periodically prevents the model from drifting too far into suboptimal regions by exploiting the stochasticity to find better basins.
- Core assumption: The Péclet number (Pe = Lv / 2D) being less than or equal to 1 indicates favorable conditions for resetting.
- Evidence anchors:
  - [abstract] "we identify that resetting is most beneficial when stochasticity is large and drift toward the optimum is weak—conditions prevalent in small-batch training with high noise rates."
  - [section] "we hypothesize this mechanism is also applicable to the noisy label problem in DNNs."
- Break condition: When batch size is large or noise rate is low, the drift component dominates, making resetting less effective or even counterproductive.

### Mechanism 3
- Claim: Partial resetting (resetting only the later layers) can be more effective than full resetting for certain architectures.
- Mechanism: Later layers (closer to output) tend to memorize noisy labels more than earlier layers. Resetting only these layers removes the memorization bias while preserving the general feature extraction capabilities of earlier layers.
- Core assumption: Different layers exhibit varied learning behaviors, with later layers specializing in task-specific patterns that are more prone to memorization.
- Evidence anchors:
  - [section] "we introduce partial resetting, which involves resetting only one section of the network layers rather than the entire network"
  - [section] "partial resetting of the latter section can further enhance generalization performance compared to full resetting"
- Break condition: If the architecture distributes memorization evenly across layers or if the loss function already strongly regularizes later layers, partial resetting may not provide additional benefit.

## Foundational Learning

- Concept: Stochastic Gradient Descent dynamics and its mapping to Langevin dynamics
  - Why needed here: The paper's theoretical framework relies on treating SGD as an overdamped Langevin process to analyze how resetting affects the search for optimal parameters.
  - Quick check question: What are the two components of SGD dynamics when mapped to Langevin dynamics, and how do they correspond to the underlying optimization process?

- Concept: First Passage Time (FPT) and Mean First Passage Time (MFPT) in random search processes
  - Why needed here: The efficiency of resetting is quantified using MFPT, which measures the average time to reach a target in a search space.
  - Quick check question: How does the introduction of stochastic resetting change the MFPT compared to a pure diffusive search without resetting?

- Concept: Label noise effects on deep learning generalization
  - Why needed here: Understanding how noisy labels cause models to initially learn general patterns and then memorize corrupted data is essential to grasp why resetting helps.
  - Quick check question: What is the typical learning curve pattern observed when training on noisy labels, and at what point does overfitting to noise typically begin?

## Architecture Onboarding

- Component map: Standard SGD training loop -> Validation monitoring -> Checkpoint update -> Stochastic reset (probability r) -> Continue training
- Critical path: Train → Monitor validation loss → Update checkpoint if validation loss improves → With probability r, reset parameters to latest checkpoint → Continue training.
- Design tradeoffs: Full resetting vs. partial resetting (layer-specific); higher reset probability provides stronger regularization but may slow convergence; checkpoint selection timing affects performance.
- Failure signatures: No improvement over baseline indicates either reset probability too low (insufficient regularization) or too high (excessive disruption); degradation suggests poor checkpoint selection or inappropriate reset frequency.
- First 3 experiments:
  1. Baseline: Train on clean ciFAIR-10 with VCNN architecture, no resetting, measure test accuracy.
  2. Simple resetting: Add resetting with r=0.01 on ciFAIR-10 with 40% symmetric label noise, use checkpoint at minimum validation loss.
  3. Batch size sensitivity: Repeat experiment 2 with varying batch sizes (8, 16, 32) to observe how stochasticity affects resetting benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the orthogonality between gradient components persist in deeper architectures or alternative normalization schemes?
- Basis in paper: [explicit] The paper observes orthogonality between gradients from correct and wrong labels in experiments with batch normalization, particularly in deeper layers.
- Why unresolved: The study only tests batch normalization; other normalization methods like layer normalization or group normalization were not explored.
- What evidence would resolve it: Experiments on deeper networks (e.g., ResNet-50+) and architectures with different normalization layers would clarify if orthogonality is a general property or batch normalization-specific.

### Open Question 2
- Question: Can the optimal reset probability be predicted analytically based on dataset properties?
- Basis in paper: [inferred] The paper identifies that reset probability should depend on batch size and noise rate, but determining optimal values requires empirical tuning.
- Why unresolved: While theoretical relationships between reset probability and factors like stochasticity are proposed, no analytical formula for the optimal reset probability is provided.
- What evidence would resolve it: A theoretical framework linking reset probability to measurable properties like noise rate, batch size, and loss landscape sharpness would enable prediction without trial-and-error.

### Open Question 3
- Question: How does stochastic resetting interact with double descent phenomena?
- Basis in paper: [explicit] The paper acknowledges that the method may be less effective when double descent occurs, as the validation loss may not have a clear minimum.
- Why unresolved: Experiments focus on standard training scenarios; double descent cases are not systematically tested.
- What evidence would resolve it: Controlled experiments on datasets or architectures known to exhibit double descent, comparing resetting performance across different noise regimes, would clarify effectiveness limits.

## Limitations

- Theoretical framework relies on approximations that may break down for deep networks with complex loss landscapes
- Effectiveness depends on the presence of a clear minimum in validation loss, potentially limiting utility in double descent scenarios
- Optimal reset probability requires empirical tuning and may vary significantly across different architectures and datasets

## Confidence

- Theoretical mapping to Langevin dynamics: Medium - sound derivation but applicability to non-convex DNN optimization needs further validation
- Empirical effectiveness across datasets: High - consistent improvements observed across multiple architectures and noise regimes
- Partial resetting approach: Medium - shows promise but lacks extensive ablation studies on different layer configurations

## Next Checks

1. Test resetting effectiveness on larger batch sizes (64, 128) to verify the predicted degradation in performance as drift dominates diffusion
2. Evaluate the method on non-image datasets with structured label noise to assess generalizability beyond CIFAR-style benchmarks
3. Conduct ablation studies on checkpoint selection timing, specifically comparing resetting to checkpoints at different points along the validation loss curve to quantify the optimal checkpoint selection window