---
ver: rpa2
title: 'Project MOSLA: Recording Every Moment of Second Language Acquisition'
arxiv_id: '2403.17314'
source_url: https://arxiv.org/abs/2403.17314
tags:
- language
- data
- learning
- speech
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Project MOSLA presents a longitudinal, multimodal dataset of second
  language acquisition, featuring over 250 hours of recorded lessons across Arabic,
  Spanish, and Chinese. The dataset is semi-automatically annotated with speaker/language
  IDs and transcripts using fine-tuned speech models, achieving speaker identification
  accuracy of 90-95% and language identification accuracy of 95-92%.
---

# Project MOSLA: Recording Every Moment of Second Language Acquisition

## Quick Facts
- arXiv ID: 2403.17314
- Source URL: https://arxiv.org/abs/2403.17314
- Authors: Masato Hagiwara; Joshua Tanner
- Reference count: 0
- Primary result: Longitudinal multimodal dataset of second language acquisition with over 250 hours of recorded lessons across three languages.

## Executive Summary
Project MOSLA presents a novel longitudinal, multimodal dataset capturing second language acquisition through over 250 hours of recorded lessons in Arabic, Spanish, and Chinese. The dataset includes video, audio, screen share, and transcripts, annotated with speaker and language identification using fine-tuned speech models. The study demonstrates increasing target language usage and lexical diversity over time, with Guiraud's index showing correlation of 0.30-0.55 with lesson progression. The dataset enables novel analytics including unsupervised screen focus detection using the Matchmap method.

## Method Summary
The MOSLA dataset was created by recording Zoom lessons over two years, then semi-automatically annotating them using fine-tuned Whisper models for speaker and language identification, and transcription. The pipeline included Pyannote for diarization, with human annotation of 5-minute samples to train the models. Speaker identification accuracy reached 90-95% and language identification accuracy reached 95-92% after fine-tuning. The dataset enables both linguistic analysis (lexical diversity metrics) and multimodal analytics (screen focus detection using contrastive learning).

## Key Results
- Speaker identification accuracy of 90-95% and language identification accuracy of 95-92% achieved through fine-tuning.
- Guiraud's lexical diversity index correlates 0.30-0.55 with lesson progression, showing increasing vocabulary sophistication.
- Unsupervised screen focus detection using Matchmap method demonstrates potential for analyzing learning behaviors from raw audio-video data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Longitudinal, multimodal data enables better SLA modeling than unimodal snapshots.
- Mechanism: Capturing video, audio, screen share, and transcripts over two years allows tracking both linguistic and non-linguistic learning cues (e.g., gestures, material use) and their evolution.
- Core assumption: Non-linguistic cues and material interactions contribute meaningfully to SLA and can be extracted from multimodal data.
- Evidence anchors:
  - [abstract] states dataset covers "verbal and non-verbal communication, the use of teaching materials, student-teacher interactions."
  - [section] 1 emphasizes "multimodal, longitudinal interaction is a crucial factor in SLA."
  - [corpus] shows related work on multimodal learning analytics, but no direct validation of non-linguistic cue utility here.
- Break condition: If linguistic markers alone predict outcomes, the extra multimodal cost may not pay off.

### Mechanism 2
- Claim: Fine-tuning Whisper on small annotated corpora improves ASR and downstream classification tasks.
- Mechanism: Domain-specific fine-tuning adapts pre-trained speech models to learner speech characteristics (e.g., non-native accents, code-switching).
- Core assumption: Pre-trained Whisper generalizes well enough that modest labeled data suffices for significant gains.
- Evidence anchors:
  - [section] 4.2.3 shows CER drops from 60%→25% (Arabic), 33%→28% (Spanish), 32%→17% (Chinese) after fine-tuning.
  - [section] 4.2.2 reports speaker ID accuracy improves from 46-76%→90-95% and language ID from 46-76%→89-95% post-fine-tuning.
  - [corpus] related papers use Whisper fine-tuning for SLA but no explicit claim about small-data effectiveness.
- Break condition: If learner speech is too divergent or noisy, fine-tuning may not converge or may overfit.

### Mechanism 3
- Claim: Matchmap-based contrastive learning can locate learner focus on screen from unannotated audio-video alone.
- Mechanism: Audio-visual co-occurrence patterns encode spatial-temporal attention; neural encoders learn to align them without labels.
- Core assumption: Teachers/students consistently look at/engage with relevant screen content while speaking about it.
- Evidence anchors:
  - [section] 5.2 shows Matchmap highlights speaker and content regions in sample visualizations.
  - [section] claims "potential for automatically detecting the areas of focus on the screen."
  - [corpus] contains no direct validation of Matchmap performance on this task; only shows conceptual alignment.
- Break condition: If learners multitask or audio doesn't reliably correlate with gaze, the method fails.

## Foundational Learning

- Concept: Speaker diarization and VAD differences
  - Why needed here: To segment lesson audio into speaker turns for downstream classification and ASR.
  - Quick check question: What metric distinguishes a diarization system from a VAD-only system in this pipeline?

- Concept: Lexical diversity metrics (TTR vs. Guiraud's index)
  - Why needed here: To track vocabulary growth over time while controlling for utterance length effects.
  - Quick check question: Why does Guiraud's index outperform raw TTR for this longitudinal analysis?

- Concept: Contrastive learning and Matchmap
  - Why needed here: To learn cross-modal alignment without manual labels for screen focus detection.
  - Quick check question: How does the loss function in Equation 6 encourage audio-image alignment?

## Architecture Onboarding

- Component map: Raw Zoom recordings -> Pyannote diarization -> Fine-tuned Whisper (speaker/language ID + ASR) -> Human annotation pipeline -> Downstream analysis
- Critical path: The fine-tuning pipeline (Whisper + Pyannote) is critical as it enables all downstream analyses including speaker/language tracking and lexical diversity metrics.
- Design tradeoffs: Semi-automatic annotation balances scalability with accuracy; human annotation of only 5-minute samples keeps costs manageable while enabling effective fine-tuning.
- Failure signatures: High ASR error rates (>30%) would undermine lexical diversity analysis; poor diarization would corrupt speaker/language tracking; inconsistent multimodal alignment would invalidate screen focus detection.
- First experiments:
  1. Validate fine-tuned Whisper speaker/language identification on a held-out 5-minute sample.
  2. Test ASR performance on learner speech with varying accents/proficiency levels.
  3. Implement basic screen focus detection on a single lesson to verify multimodal alignment.

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset covers only 3 learners studying 3 languages, limiting generalizability across learner populations and proficiency levels.
- Matchmap-based screen focus detection lacks quantitative validation metrics, providing only qualitative visualizations.
- ASR character error rates remain relatively high (25-28% after fine-tuning), with no reported downstream task performance analysis.

## Confidence

- **High Confidence**: Longitudinal trends in target language usage and lexical diversity (Guiraud's index correlation 0.30-0.55).
- **Medium Confidence**: Fine-tuning Whisper improves speaker/language identification (90-95% accuracy) and ASR performance.
- **Low Confidence**: Matchmap-based screen focus detection utility due to lack of quantitative validation.

## Next Checks

1. Cross-validate the machine annotation pipeline on a held-out learner or language not included in the training data to assess generalization.
2. Implement quantitative evaluation of the Matchmap method using a manually annotated subset where gaze/screen focus is labeled, reporting standard detection metrics.
3. Conduct error analysis on ASR output and measure how transcription errors affect lexical diversity metrics and other downstream linguistic analyses.