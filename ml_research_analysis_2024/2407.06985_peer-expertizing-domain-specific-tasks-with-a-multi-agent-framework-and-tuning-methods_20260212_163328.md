---
ver: rpa2
title: 'PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning
  Methods'
arxiv_id: '2407.06985'
source_url: https://arxiv.org/abs/2407.06985
tags:
- peer
- answer
- agent
- question
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PEER, a multi-agent framework designed to\
  \ address the tri-lemma of performance, cost, and data privacy in domain-specific\
  \ applications like financial question answering. The framework divides tasks into\
  \ four roles\u2014Plan, Execute, Express, and Review\u2014each handled by specialized\
  \ agents that collaborate through a cyclic workflow."
---

# PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods

## Quick Facts
- arXiv ID: 2407.06985
- Source URL: https://arxiv.org/abs/2407.06985
- Authors: Yiying Wang; Xiaojing Li; Binzhu Wang; Yueyang Zhou; Yingru Lin; Han Ji; Hong Chen; Jinshi Zhang; Fei Yu; Zewei Zhao; Song Jin; Renji Gong; Wanqing Xu
- Reference count: 25
- One-line primary result: PEER achieves 95.0% of GPT-4's performance on financial QA while reducing cost and maintaining data privacy

## Executive Summary
This paper introduces PEER, a multi-agent framework designed to address the tri-lemma of performance, cost, and data privacy in domain-specific applications like financial question answering. The framework divides tasks into four roles—Plan, Execute, Express, and Review—each handled by specialized agents that collaborate through a cyclic workflow. The authors also propose iterative tuning methods, including supervised fine-tuning, rejection sampling, and direct preference optimization, enhanced with online feedback for efficient model adaptation. Experiments show PEER outperforms baselines like BabyAGI and achieves 95.0% of GPT-4's performance on financial QA tasks while significantly reducing cost and maintaining data privacy. The approach demonstrates effective balance between model flexibility, controllability, and industrial applicability.

## Method Summary
PEER implements a multi-agent framework with four specialized roles: Plan agent decomposes questions into sub-questions, Execute agent retrieves information, Express agent synthesizes answers, and Review agent evaluates output and provides feedback for iterative refinement. The framework uses iterative tuning methods including supervised fine-tuning on curated datasets, rejection sampling for quality filtering, direct preference optimization for preference-based tuning, and online feedback loops using reward models (GPT-4o). The cyclic workflow enables continuous improvement through feedback-based iterations, while maintaining data privacy by avoiding external API calls for sensitive information.

## Key Results
- Achieves 95.0% of GPT-4's performance on financial QA tasks
- Significantly reduces operational costs compared to GPT-4 API usage
- Maintains data privacy by keeping sensitive information within controlled infrastructure
- Outperforms baseline frameworks like BabyAGI on domain-specific benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEER's cyclic workflow improves answer quality through iterative refinement.
- Mechanism: The "Review" agent evaluates the "Express" agent's output and provides feedback, triggering another iteration if criteria aren't met. This feedback loop allows continuous improvement toward optimal solutions.
- Core assumption: The "Review" agent can accurately assess whether answers meet predefined quality criteria and provide actionable feedback.
- Evidence anchors:
  - [abstract] "The 'Review' agent evaluates whether the 'Express' agent's answer meets pre-established criteria. If satisfied, the final answer is delivered; if not, it provides modification suggestions and initiates another PEER iteration, enhancing answer quality through feedback."
  - [section] "This ensures that the answers continuously improve towards the optimal solution. If an answer does not meet user requirements, the 'Review' agent suggests modifications for the 'Plan,' 'Execute,' or 'Express' agents."
  - [corpus] No direct evidence in corpus; this mechanism appears to be a novel contribution of the paper.
- Break condition: If the "Review" agent cannot accurately assess quality or provide useful feedback, the iterative improvement cycle breaks down.

### Mechanism 2
- Claim: Iterative training with online AI feedback improves model performance over time.
- Mechanism: The model generates candidate responses, uses a reward model (GPT-4o) to select the best response, and compares it with ground truth. If the model-generated response exceeds the quality threshold, it replaces the original training sample. This creates a self-refining training paradigm.
- Core assumption: The reward model (GPT-4o) can reliably evaluate response quality and the model can learn from this feedback.
- Evidence anchors:
  - [abstract] "We developed industrial practices leveraging online data and user feedback for efficient model tuning."
  - [section] "Algorithm 1 outlines our iterative training process, starting with the initial dataset Doff. The agent processes each batch iteratively, involving model evaluation, data generation, and refinement."
  - [corpus] No direct evidence in corpus; this mechanism appears to be a novel contribution of the paper.
- Break condition: If the reward model provides inconsistent or incorrect feedback, or if the model fails to learn from the feedback, the iterative improvement stops working.

### Mechanism 3
- Claim: Role specialization in PEER improves task performance compared to monolithic approaches.
- Mechanism: PEER divides tasks into four specialized roles (Plan, Execute, Express, Review), each handled by a dedicated agent. This specialization allows each agent to focus on its specific task, improving overall performance.
- Core assumption: Task decomposition into specialized roles leads to better performance than general-purpose agents handling all tasks.
- Evidence anchors:
  - [abstract] "The framework divides tasks into four roles—Plan, Execute, Express, and Review—each handled by specialized agents that collaborate through a cyclic workflow."
  - [section] "Each agent specializes in a single task, working together to accomplish the overall objective."
  - [corpus] No direct evidence in corpus; this mechanism appears to be a novel contribution of the paper.
- Break condition: If the overhead of coordination between specialized agents exceeds the benefits of specialization, or if the tasks don't benefit from decomposition.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is used as the initial training method to adapt base models to domain-specific tasks using curated datasets.
  - Quick check question: What is the primary loss function used in SFT for classification tasks?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is used as an alternative to RLHF for preference-based model tuning, avoiding the need for a separate reward model.
  - Quick check question: How does DPO differ from traditional RLHF in terms of model architecture requirements?

- Concept: Rejection Sampling
  - Why needed here: Rejection sampling is used to filter and improve the quality of training data by automatically removing low-quality samples before human annotation.
  - Quick check question: What is the main advantage of using rejection sampling in dataset curation?

## Architecture Onboarding

- Component map:
  - Plan Agent → Execute Agent → Express Agent → Review Agent → (feedback loop)
  - Reward Model: GPT-4o for online feedback evaluation
  - Data Pipeline: Handles offline and online datasets

- Critical path:
  1. User query → Plan Agent
  2. Plan Agent outputs → Execute Agent
  3. Execute Agent outputs → Express Agent
  4. Express Agent output → Review Agent
  5. Review Agent evaluates and either outputs final answer or triggers iteration

- Design tradeoffs:
  - Specialization vs. coordination overhead
  - Model size vs. performance (14B parameter model vs. GPT-4)
  - Offline data quality vs. online feedback volume
  - Iteration depth vs. computational cost

- Failure signatures:
  - Plan Agent produces vague or irrelevant sub-questions
  - Execute Agent retrieves incorrect or insufficient information
  - Express Agent fails to synthesize coherent answers
  - Review Agent provides poor or inconsistent feedback
  - Reward model gives unreliable quality assessments

- First 3 experiments:
  1. Run PEER with a simple question (e.g., "What is the capital of France?") to verify basic workflow functionality
  2. Test each agent individually with known inputs to verify correct behavior
  3. Run a full iteration with a complex question to test the cyclic workflow and feedback mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PEER's performance scale when applied to domains beyond financial question answering, such as healthcare or legal domains?
- Basis in paper: [inferred] The paper demonstrates PEER's effectiveness in financial QA but does not explore its applicability to other domains.
- Why unresolved: The study focuses solely on financial QA, leaving the generalizability of PEER to other domains untested.
- What evidence would resolve it: Experiments applying PEER to other domain-specific tasks with comparable performance metrics.

### Open Question 2
- Question: What are the long-term effects of PEER's iterative tuning approach on model performance and data privacy?
- Basis in paper: [explicit] The paper mentions iterative tuning methods but does not discuss long-term impacts.
- Why unresolved: The study does not address how continuous iterative tuning affects model performance or data privacy over extended periods.
- What evidence would resolve it: Longitudinal studies tracking model performance and data privacy metrics over time.

### Open Question 3
- Question: How does the cost-effectiveness of PEER compare to other multi-agent frameworks in real-world industrial applications?
- Basis in paper: [explicit] The paper claims cost-effectiveness but lacks detailed comparative analysis with other frameworks.
- Why unresolved: The study provides limited data on cost comparisons with other multi-agent systems.
- What evidence would resolve it: Comprehensive cost analysis comparing PEER to other frameworks in various industrial settings.

## Limitations
- Performance claims rely on proprietary financial QA dataset that limits reproducibility
- Evaluation methodology using LLM-as-a-Judge may introduce subjectivity and may not align with human judgment standards
- Detailed cost analysis is insufficient to verify claimed cost reductions

## Confidence

**High Confidence** (Strong empirical support, well-established mechanisms):
- The cyclic workflow architecture of PEER (Plan→Execute→Express→Review) is clearly defined and implementable
- The four specialized agent roles are logically coherent and technically feasible
- The iterative training methods (SFT, rejection sampling, DPO) are well-established techniques

**Medium Confidence** (Reasonable support but with notable gaps):
- Performance claims relative to GPT-4 (95.0%) - dependent on proprietary dataset and LLM-as-a-Judge evaluation
- Cost reduction claims - lack detailed cost analysis
- Industrial applicability claims - based on case study rather than broad deployment evidence

**Low Confidence** (Significant uncertainties or missing evidence):
- Claims about balancing the "tri-lemma" of performance, cost, and privacy - insufficient quantitative analysis
- Generalization to non-financial domains - no cross-domain validation presented
- Long-term stability of iterative feedback mechanisms - no temporal analysis of model drift

## Next Checks
1. **Reproduce on Public Dataset**: Implement PEER on a publicly available financial QA dataset (e.g., FiQA or Financial PhraseBank) to verify if similar performance gains can be achieved without the proprietary dataset advantage.

2. **Cost Analysis Validation**: Conduct a detailed cost analysis comparing PEER's actual computational overhead (including all agent interactions and iterations) against direct GPT-4 API usage for equivalent tasks, accounting for both monetary cost and computational resources.

3. **Cross-Domain Generalization Test**: Apply PEER to a different domain (e.g., medical QA or legal document analysis) using the same framework architecture to assess whether the performance benefits extend beyond the financial domain where it was developed.