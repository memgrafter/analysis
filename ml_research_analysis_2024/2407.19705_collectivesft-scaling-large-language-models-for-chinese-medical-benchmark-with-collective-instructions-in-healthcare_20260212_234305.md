---
ver: rpa2
title: 'CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark
  with Collective Instructions in Healthcare'
arxiv_id: '2407.19705'
source_url: https://arxiv.org/abs/2407.19705
tags:
- medical
- dataset
- https
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of improving large language model
  (LLM) performance in the Chinese medical domain, specifically focusing on the Comprehensive
  Medical Benchmark (CMB). The core method involves fine-tuning a smaller base model,
  InternLM2.5-7B, using a diverse and well-distributed dataset of medical instructions
  and dialogues, referred to as "CollectiveSFT." This dataset integrates multiple
  sources and formats, including real-world dialogues, consultation records, and question-answering
  pairs, all standardized into a unified format.
---

# CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare

## Quick Facts
- **arXiv ID:** 2407.19705
- **Source URL:** https://arxiv.org/abs/2407.19705
- **Reference count:** 31
- **Primary result:** Fine-tuned InternLM2.5-7B achieved 77.05 on CMB, outperforming larger models like HuatuoGPTII-34B and Qwen-72B-Chat

## Executive Summary
This study addresses the challenge of improving large language model performance in the Chinese medical domain through fine-tuning a smaller base model (InternLM2.5-7B) using a diverse, well-distributed dataset called "CollectiveSFT." The dataset integrates multiple sources including real-world dialogues, consultation records, and question-answering pairs, all standardized into a unified Alpaca format. The core finding demonstrates that the fine-tuned model achieved 77.05 on the Comprehensive Medical Benchmark, outperforming significantly larger models, suggesting that carefully curated datasets can enable smaller models to reach high performance levels.

## Method Summary
The approach involves fine-tuning InternLM2.5-7B using the CollectiveSFT dataset, which aggregates diverse medical instruction formats (MCQA, QA, Dialogue, NER) and normalizes them into a unified Alpaca-style format. The method emphasizes dataset quality and diversity over model size, using supervised fine-tuning with hyperparameter optimization (cut-off length, epoch count, learning rate). The standardized format ensures consistent data processing for efficient training, while the diverse sources aim to enhance generalization across medical tasks. The primary evaluation metric is performance on the Comprehensive Medical Benchmark (CMB).

## Key Results
- Fine-tuned InternLM2.5-7B achieved 77.05 on CMB benchmark
- Outperformed larger models including HuatuoGPTII-34B and Qwen-72B-Chat
- Demonstrated that diverse instruction formats improve model generalization
- Showed that dataset quality and diversity can compensate for fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse instruction formats improve generalization by forcing the model to adapt to varied input patterns.
- Mechanism: The fine-tuning dataset aggregates multiple formats (MCQA, QA, Dialogue, NER) and normalizes them into a unified Alpaca-style format, allowing the model to learn flexible representations.
- Core assumption: The model can effectively generalize from multiple instruction types to perform well on the CMB benchmark.
- Evidence anchors: [abstract] "By integrating a wide range of instructional content, our approach addresses potential issues such as data quality inconsistencies." [section] "We construct instructions based on the data types of the collected datasets, ensuring that each type is processed into a unified format."
- Break condition: If the model shows high variance in performance depending on instruction type.

### Mechanism 2
- Claim: Dataset quality and diversity are more important than model size for achieving competitive performance.
- Mechanism: The fine-tuned InternLM2.5-7B base model outperforms much larger models like HuatuoGPTII-34B and Qwen-72B-Chat on the CMB benchmark.
- Core assumption: The CMB benchmark is sufficiently representative and challenging that high performance reflects strong generalization across medical tasks.
- Evidence anchors: [abstract] "This study suggests that even smaller models may reach high performance levels with carefully curated and varied datasets." [section] "Our results imply that a broader spectrum of training data may enhance a model's ability to generalize."
- Break condition: If the model performs well on CMB but poorly on other medical tasks or benchmarks.

### Mechanism 3
- Claim: Standardized data formatting enables consistent fine-tuning and better model learning.
- Mechanism: By converting all collected datasets into the Alpaca format, the study ensures consistent data processing, which enhances training efficiency and model understanding.
- Core assumption: The Alpaca format is sufficiently expressive to capture nuances of different medical data types while maintaining consistency.
- Evidence anchors: [section] "To tackle these issues, we decided to standardize all datasets into the Alpaca format... By adopting a standardized format, we ensure consistent data processing."
- Break condition: If alternative formats yield better performance, indicating standardization may have introduced unnecessary constraints.

## Foundational Learning

- **Concept:** Dataset diversity and its impact on model generalization
  - Why needed here: The study hinges on the idea that diverse training data improves model performance across different medical tasks.
  - Quick check question: Why might a model trained only on question-answer pairs perform worse than one trained on a mix of Q&A, dialogues, and NER tasks?

- **Concept:** Fine-tuning vs. pretraining
  - Why needed here: The study uses supervised fine-tuning (SFT) on a base model rather than full pretraining, so understanding SFT's scope and limitations is crucial.
  - Quick check question: What is the main difference between fine-tuning and pretraining in terms of data requirements and expected outcomes?

- **Concept:** Benchmark evaluation and its limitations
  - Why needed here: The study's results are benchmark-based; understanding what CMB measures and its limitations is important for interpreting findings.
  - Quick check question: What are potential risks of using a single benchmark (like CMB) to claim broad model capability?

## Architecture Onboarding

- **Component map:** Data collection pipeline -> Dataset formatting module -> Fine-tuning engine -> Evaluation harness -> Open-source release
- **Critical path:** 1. Aggregate raw medical datasets (Chinese/English). 2. Standardize into Alpaca format. 3. Fine-tune InternLM2.5-7B with optimized hyperparameters. 4. Evaluate on CMB. 5. Release model and dataset.
- **Design tradeoffs:** Smaller model (7B) chosen for efficiency vs. larger models with potentially higher raw capacity; diverse but potentially noisier data vs. cleaner but narrower datasets; standardized format for consistency vs. potential loss of domain-specific nuance.
- **Failure signatures:** Poor generalization across medical tasks despite high CMB score; high variance in performance depending on instruction type; hallucination or incorrect responses in interactive use cases.
- **First 3 experiments:** 1. Fine-tune on only one data type (e.g., MCQA) and compare CMB score to multi-format model. 2. Train a larger model (e.g., 34B) with same dataset to test if size still matters. 3. Evaluate model on a different medical benchmark (e.g., MedQA) to check for overfitting to CMB.

## Open Questions the Paper Calls Out

The paper explicitly identifies several unresolved questions: how CollectiveSFT-7B compares to larger models on conversational medical tasks requiring patient interaction or complex diagnostic discussions; what specific strategies can mitigate hallucination in smaller fine-tuned models while maintaining specialized task performance; and how the diversity and distribution of the training dataset influences the model's ability to generalize across different medical specialties or languages.

## Limitations
- Heavy reliance on single benchmark (CMB) for evaluation raises overfitting concerns
- Lack of ablation studies on individual data sources prevents quantifying component contributions
- No human evaluation or qualitative analysis limits understanding of practical utility and safety

## Confidence
- **High Confidence:** Diverse instruction formats improve generalization is well-supported by methodology and results
- **Medium Confidence:** Dataset quality and diversity can compensate for model size is supported but needs validation on additional benchmarks
- **Low Confidence:** Standardized data formatting enhances training efficiency is plausible but lacks direct experimental evidence

## Next Checks
1. Test the fine-tuned model on additional medical benchmarks (e.g., MedQA, USMLE-style questions) to assess generalization beyond CMB and identify potential overfitting
2. Conduct ablation studies by training separate models on individual data sources to quantify each component's contribution to final performance
3. Perform qualitative assessments by medical professionals to evaluate model responses for accuracy, safety, and practical utility in real-world medical scenarios