---
ver: rpa2
title: Investigating Regularization of Self-Play Language Models
arxiv_id: '2404.04291'
source_url: https://arxiv.org/abs/2404.04291
tags:
- spin
- base
- arxiv
- preprint
- previous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates regularization methods for self-play fine-tuning
  (SPIN), an alignment technique that iteratively refines language models without
  human preference data. The authors propose two key directions: adding a KL-regularization
  term to keep the model close to the base policy, and using a mixture of past policies
  (fictitious play) for generating training data.'
---

# Investigating Regularization of Self-Play Language Models

## Quick Facts
- arXiv ID: 2404.04291
- Source URL: https://arxiv.org/abs/2404.04291
- Reference count: 16
- Self-play fine-tuning with regularization improves alignment performance on Humanities and STEM tasks

## Executive Summary
This paper investigates regularization methods for self-play fine-tuning (SPIN), an alignment technique that iteratively refines language models without human preference data. The authors propose two key directions: adding a KL-regularization term to keep the model close to the base policy, and using a mixture of past policies (fictitious play) for generating training data. They introduce α-SPIN, which combines these ideas, where α controls the trade-off between following the previous policy and the base model. Empirically, α-SPIN with h=2 (mixing two past policies) outperforms standard SPIN, especially on Humanities and STEM tasks. KL regularization (α=0.95) further improves performance. Fictitious play also shows promise, particularly in Writing, Roleplay, and STEM domains. However, attempts to use GFlowNets for sampling from the geometric mixture were less successful, indicating hyperparameter tuning is needed. The study demonstrates that careful regularization in SPIN can improve alignment quality while maintaining stability.

## Method Summary
The authors investigate regularization in self-play fine-tuning by introducing KL-regularization to keep models close to their base policy and fictitious play to use mixtures of past policies. They propose α-SPIN, which combines these approaches with a hyperparameter α controlling the trade-off between following the previous policy and the base model. The method uses a history length parameter h to determine how many past policies to mix. They also explore using GFlowNet-finetuning to sample from the geometric mixture of policies. Experiments compare different configurations on various benchmarks, measuring performance across Humanities, STEM, Writing, and Roleplay domains.

## Key Results
- α-SPIN with h=2 outperforms standard SPIN on Humanities and STEM tasks
- KL regularization (α=0.95) further improves performance beyond basic α-SPIN
- Fictitious play shows particular promise in Writing, Roleplay, and STEM domains
- GFlowNet approach for geometric mixture sampling requires additional tuning

## Why This Works (Mechanism)
The paper demonstrates that regularization in self-play fine-tuning helps maintain stability while improving alignment quality. KL regularization prevents the model from drifting too far from the base policy, avoiding catastrophic forgetting and ensuring the model remains grounded. Fictitious play leverages the wisdom of multiple past policies, creating a richer training signal that captures diverse behaviors learned throughout training. The α parameter allows fine-tuning the balance between exploration (following the previous policy) and exploitation (staying close to the base model), with α=0.95 providing an optimal balance for the tested tasks.

## Foundational Learning

### KL Divergence
- **Why needed**: Measures how one probability distribution diverges from another, essential for regularization
- **Quick check**: Compute KL divergence between two Gaussian distributions with different means

### Self-Play Fine-Tuning
- **Why needed**: Iterative training method where model generates its own training data
- **Quick check**: Implement basic self-play loop on a simple text generation task

### Fictitious Play
- **Why needed**: Game theory concept for using mixtures of past strategies in iterative learning
- **Quick check**: Implement fictitious play on a two-player matrix game

### GFlowNets
- **Why needed**: Deep generative models for sampling from complex distributions
- **Quick check**: Train a simple GFlowNet on a synthetic distribution

## Architecture Onboarding

### Component Map
Base Model -> α-SPIN (KL Regularization + Fictitious Play) -> Improved Model

### Critical Path
Data Generation → Policy Update → Regularization → Evaluation

### Design Tradeoffs
- **α parameter**: Higher values maintain base model properties but may limit improvement; lower values allow more exploration but risk instability
- **History length h**: Longer histories provide more diverse training data but increase computational cost
- **KL weight**: Stronger regularization ensures stability but may slow learning

### Failure Signatures
- Performance degradation when α < 0.8 (model stays too close to base)
- Instability when h is too large relative to α
- Poor sampling quality when GFlowNet hyperparameters are not properly tuned

### First Experiments
1. Compare standard SPIN vs α-SPIN with h=1 on a simple text generation task
2. Sweep α values from 0.5 to 0.99 to find optimal balance
3. Test fictitious play vs single-policy training on a benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do different values of the KL regularization parameter α in α-SPIN affect the trade-off between staying close to the base model and improving performance on downstream tasks?
- **Basis in paper**: The paper states "We considered different values of α for the experiments and observed that for α < 0.8, the learned iterates stay too close to the base model, and their performances on the benchmarks do not significantly increase at each iteration. Therefore, in our experiments, we report the results for α = 0.95 only."
- **Why unresolved**: The paper only reports results for α = 0.95, leaving the full spectrum of α values unexplored.
- **What evidence would resolve it**: Empirical results comparing the performance of α-SPIN with different α values (e.g., 0.5, 0.7, 0.8, 0.9) on the same downstream tasks would clarify the optimal trade-off.

### Open Question 2
- **Question**: How does the choice of history length h in α-SPIN impact the stability and performance of the learning process, especially in comparison to the fictitious play approach (h = ∞)?
- **Basis in paper**: The paper investigates the effect of history length h in α-SPIN, comparing h = 2 (mixing two past policies) to h = 1 (using only the previous policy) and h = ∞ (fictitious play).
- **Why unresolved**: While the paper shows that h = 2 performs better than h = 1, the comparison to h = ∞ is not conclusive, and the impact of different h values on stability is not fully explored.
- **What evidence would resolve it**: A comprehensive study comparing the performance and stability of α-SPIN with various h values (e.g., 1, 2, 3, 5, 10, ∞) on the same tasks would clarify the optimal history length.

### Open Question 3
- **Question**: Can the GFlowNet-finetuning approach be improved to effectively sample from the geometric mixture πref, and what are the key challenges and potential solutions?
- **Basis in paper**: The paper investigates using GFlowNet-finetuning to sample from the geometric mixture but reports that "more effort should be spent on the specifics of GFlowNet-finetuning in order to get accurate approximate samplers of the geometric mixture."
- **Why unresolved**: The paper acknowledges the potential of GFlowNet-finetuning but does not provide a definitive solution to the challenges encountered.
- **What evidence would resolve it**: Successful implementation and evaluation of GFlowNet-finetuning for sampling from the geometric mixture, along with a detailed analysis of the challenges and proposed solutions, would resolve this question.

## Limitations

- Evaluation limited to specific domains (Humanities, STEM, Writing, Roleplay) without analysis of broader task categories
- GFlowNet approach for geometric mixture sampling was not fully successful and requires additional tuning
- No analysis of long-term stability or catastrophic forgetting in iterative self-play approaches
- Results may be sensitive to implementation details and hyperparameters

## Confidence

- **High confidence**: The observation that α-SPIN with h=2 outperforms standard SPIN on the tested benchmarks
- **Medium confidence**: The general effectiveness of KL regularization and fictitious play, given the limited scope of tasks
- **Medium confidence**: The conclusion that regularization improves alignment quality, though the specific optimal configurations may be sensitive to implementation details

## Next Checks

1. Evaluate the approach across a broader and more diverse set of tasks, including non-text generation domains and long-context scenarios
2. Conduct ablation studies to isolate the contributions of KL regularization versus fictitious play versus their combination
3. Test the stability of the approach over extended training horizons and across multiple model sizes to assess scalability limits