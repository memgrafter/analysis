---
ver: rpa2
title: A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned
  Label Only
arxiv_id: '2404.12704'
source_url: https://arxiv.org/abs/2404.12704
tags:
- backdoor
- attack
- graph
- training
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CBAG, a clean-graph backdoor attack against
  GCNs that achieves backdoor injection by poisoning only the training labels without
  modifying graph structure or features. The attack selects robust feature combinations
  strongly associated with the target label as triggers and uses a scoring function
  to choose nodes for label poisoning.
---

# A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only

## Quick Facts
- arXiv ID: 2404.12704
- Source URL: https://arxiv.org/abs/2404.12704
- Authors: Jiazhu Dai; Haoyu Sun
- Reference count: 25
- Key outcome: Achieves 99% attack success rate with poisoning rates below 5% by poisoning only training labels

## Executive Summary
This paper proposes CBAG, a clean-graph backdoor attack that compromises Graph Convolutional Networks (GCNs) solely through label poisoning without modifying graph structure or node features. The attack identifies robust feature combinations strongly associated with target labels and strategically poisons training samples to create a backdoor trigger. By exploiting feature-label associations learned during GCN training, the attack achieves high effectiveness while maintaining model functionality on benign samples.

## Method Summary
CBAG works by first calculating feature importance scores to identify trigger patterns - feature combinations that strongly distinguish target label nodes from others. A scoring function then ranks nodes with high trigger feature values but incorrect labels, selecting top candidates for label poisoning. The poisoned samples are added to training data, causing the GCN to learn associations between the trigger features and target label. During inference, activating these features causes misclassification to the target class.

## Key Results
- Achieves 99% attack success rate on Cora, CiteSeer, Cora ML, and PubMed datasets
- Maintains clean accuracy drop below 5% across all datasets
- Effective with poisoning rates below 5% of training data
- Outperforms baseline CBAG-R using random feature combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GCNs can be backdoored solely by manipulating training labels without modifying graph structure or features.
- Mechanism: By poisoning the labels of nodes that have feature combinations strongly associated with the target label, the model learns to associate these feature patterns with the target class during training. During inference, activating these features triggers misclassification to the target label.
- Core assumption: The model learns feature-label associations during training that can be exploited by relabeling nodes with similar feature patterns.
- Evidence anchors:
  - [abstract]: "CBAG achieves backdoor injection by poisoning only the training labels without modifying graph structure or features"
  - [section 4.2]: "Some combinations of features can be regarded as the trigger of backdoor attack"
- Break condition: If the model architecture or training process prevents learning feature-label associations, or if feature distributions are too uniform to find strong associations.

### Mechanism 2
- Claim: Selecting robust feature combinations that are strongly associated with the target label improves attack effectiveness.
- Mechanism: The attack calculates feature importance by comparing average feature values of target label nodes against all nodes, selecting combinations with highest differences as triggers. This ensures the trigger features are both distinctive and representative of the target class.
- Core assumption: Features with high importance scores (difference between target and average node features) will create strong feature-label associations when used as triggers.
- Evidence anchors:
  - [section 4.2]: "we subtract the average features value of all nodes from the average features value of the nodes with the target label, and the difference size reflects the robustness of the features"
  - [section 5.2.3]: "CBAG only needs to choose a small number of features combination as the trigger pattern to achieve good attack performance"
- Break condition: If feature distributions are highly overlapping between classes or if the feature space lacks discriminative dimensions.

### Mechanism 3
- Claim: Scoring function based on sum of trigger feature values effectively selects poisoning candidates.
- Mechanism: Nodes with high values in trigger feature dimensions but incorrect labels are scored and selected for label poisoning, ensuring poisoned samples reinforce the trigger-target association during training.
- Core assumption: Nodes with naturally high trigger feature values but wrong labels are the most effective poisoning candidates because they already exhibit trigger characteristics.
- Evidence anchors:
  - [section 4.3]: "we sum up the values of each non-target label node in each of these feature dimensions as their scores"
  - [section 5.2.1]: "The experimental results showed that CBAG achieved excellent performance" with 99.83% ASR on Cora dataset
- Break condition: If trigger feature values are uniformly distributed or if no nodes have high trigger values with incorrect labels.

## Foundational Learning

- Concept: Graph Convolutional Networks and message passing
  - Why needed here: Understanding how GCNs aggregate neighbor information and learn representations is crucial for understanding why label poisoning can be effective
  - Quick check question: How does the GCN aggregation rule update node representations using neighbor features?

- Concept: Backdoor attack principles in machine learning
  - Why needed here: The paper builds on standard backdoor attack concepts but applies them to graph data without trigger modification
  - Quick check question: What distinguishes a backdoor attack from other poisoning attacks in terms of attack objectives?

- Concept: Feature importance and association in classification
  - Why needed here: The attack relies on finding feature combinations strongly associated with target labels to create effective triggers
  - Quick check question: How can feature importance be quantified to identify discriminative feature combinations?

## Architecture Onboarding

- Component map: Feature importance calculator -> Scoring function -> Label poisoning module -> GCN training pipeline -> Trigger activation module
- Critical path: Feature importance calculation → Scoring function → Label poisoning → GCN training → Backdoor activation testing
- Design tradeoffs:
  - Trigger size vs. stealth: Smaller triggers are more stealthy but may reduce effectiveness
  - Poisoning rate vs. detection risk: Higher poisoning rates increase effectiveness but also detection risk
  - Feature selection method vs. attack success: More sophisticated feature selection improves effectiveness
- Failure signatures:
  - Low attack success rate despite poisoning
  - Significant clean accuracy degradation
  - No correlation between trigger activation and misclassification
- First 3 experiments:
  1. Test feature importance calculation on Cora dataset with varying λ values
  2. Evaluate scoring function effectiveness by comparing ASR with random vs. score-based poisoning
  3. Measure CAD impact at different poisoning rates (1%, 3%, 5%) on all four datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CBAG's backdoor trigger performance compare to traditional backdoor attacks that modify graph structure or features?
- Basis in paper: [explicit] The paper compares CBAG to baseline CBAG-R which uses random feature combinations, but doesn't directly compare to structure-modifying attacks like Zhang et al. 2021 or Xu et al. 2021
- Why unresolved: The paper focuses on demonstrating CBAG's effectiveness compared to its own variants, not to traditional backdoor attacks
- What evidence would resolve it: Direct comparison of CBAG's attack success rate and stealthiness against structure-modifying backdoor attacks on the same datasets

### Open Question 2
- Question: What is the minimum number of poisoned labels needed to achieve a successful backdoor attack?
- Basis in paper: [inferred] The paper shows CBAG works with poisoning rates below 5%, but doesn't identify the theoretical minimum
- Why unresolved: The paper tests multiple poisoning rates but doesn't perform analysis to determine the smallest effective poisoning rate
- What evidence would resolve it: Systematic testing of CBAG with increasingly smaller poisoning rates to identify the threshold where attack success rate drops below acceptable levels

### Open Question 3
- Question: How does CBAG perform on graph neural networks other than GCNs?
- Basis in paper: [explicit] The paper states "we highly focus on GCNs" and evaluates only GCN models
- Why unresolved: The paper doesn't test CBAG on other GNN architectures like GraphSAGE, GAT, or GIN
- What evidence would resolve it: Replication of CBAG experiments on multiple GNN architectures to test cross-model effectiveness

## Limitations
- The attack is specifically designed for node classification tasks in GCNs and hasn't been tested on other graph learning tasks or GNN architectures
- The exact implementation details of feature importance calculation and scoring function are not fully specified, affecting reproducibility
- Claims about stealthiness and resistance to detection lack empirical validation against actual defense systems

## Confidence
- High confidence: The core mechanism that GCNs can be backdoored through label poisoning alone is well-supported by experimental results
- Medium confidence: The effectiveness of the scoring function and feature importance calculation is supported by ablation studies, but parameter choices are not fully explored
- Low confidence: Claims about the attack's stealthiness and resistance to detection mechanisms lack empirical validation against existing defenses

## Next Checks
1. Parameter sensitivity analysis: Systematically vary λ values and trigger dimensions to quantify their impact on ASR and CAD across all four datasets
2. Defense evasion testing: Evaluate CBAG against state-of-the-art GCN defenses to determine whether the clean-label nature provides genuine stealth advantages
3. Cross-dataset generalization: Apply CBAG to additional graph datasets beyond the four used in experiments to assess generalizability