---
ver: rpa2
title: 'GenEx: Generating an Explorable World'
arxiv_id: '2412.09624'
source_url: https://arxiv.org/abs/2412.09624
tags:
- world
- exploration
- image
- genex
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces GenEx, a system for generating and exploring\
  \ imaginative 3D environments from a single image. GenEx combines a world initialization\
  \ model that creates a 360\xB0 panoramic view from an input image, and a world transition\
  \ model that generates coherent video sequences as the agent moves."
---

# GenEx: Generating an Explorable World

## Quick Facts
- arXiv ID: 2412.09624
- Source URL: https://arxiv.org/abs/2412.09624
- Authors: Taiming Lu; Tianmin Shu; Junfei Xiao; Luoxin Ye; Jiahao Wang; Cheng Peng; Chen Wei; Daniel Khashabi; Rama Chellappa; Alan Yuille; Jieneng Chen
- Reference count: 9
- Primary result: GenEx generates explorable 3D environments from single images with FVD 69.5 and enables imagination-augmented decision-making

## Executive Summary
GenEx is a system that generates and explores imaginative 3D environments from a single input image. It combines a world initialization model that creates a 360Â° panoramic view from the image, and a world transition model that generates coherent video sequences as the agent moves through the environment. The approach leverages physics-based data from game engines and employs spherical-consistent learning to maintain visual coherence. Agents can explore using interactive control, GPT-assisted navigation, or goal-driven planning, with applications in 3D mapping, object view synthesis, and embodied AI.

## Method Summary
GenEx consists of two main components: a world initialization model and a world transition model. The initialization model extends text-to-panorama models to condition on both input images and text descriptions, generating coherent 360Â° panoramas. The transition model uses spherical-consistent learning to generate panoramic video sequences conditioned on previous panoramas and agent actions. The system is trained on physics-based data from Unreal Engine and Unity, capturing exploration trajectories and corresponding panoramic cubemap sequences. Agents can navigate the generated world using various control methods, with decision-making enhanced through imagination-augmented policies that incorporate both real and imagined observations.

## Key Results
- Video generation quality achieves FVD score of 69.5
- Loop consistency remains high over 20-meter exploration trajectories
- Imagination-augmented policies improve decision accuracy compared to single-view inputs
- System supports both single and multi-agent scenarios for informed decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spherical-consistent learning (SCL) maintains visual coherence during panoramic video generation.
- Mechanism: By training the video diffusion model on equirectangular panoramas with spherical consistency constraints, the model learns to generate seamless transitions across the 360Â° boundary, preventing edge discontinuities.
- Core assumption: The spherical representation preserves global context better than planar projections, and the SCL technique effectively enforces continuity across the panorama's seam.
- Evidence anchors:
  - [abstract]: "To address field-of-view constraints, we utilize panoramic representations and train our video diffusion models with spherical-consistent learning techniques."
  - [section]: "To address this, we adopt spherical-consistency learning, or SCL, detailed in (Lu et al., 2024), which promotes smooth and continuous imagery across all viewing directions on the sphere."
  - [corpus]: Weak evidence - no direct mention of SCL in corpus papers, but related works mention panoramic representations and 360Â° generation.
- Break condition: If the model is trained without SCL, edge discontinuities appear at the panorama boundaries, degrading visual quality.

### Mechanism 2
- Claim: The world transition model generates coherent panoramic video sequences based on action-driven rotation and movement.
- Mechanism: The transition model takes the latest explored panorama, applies spherical rotation based on the action (angle and distance), and generates a new panoramic video sequence that maintains coherence with the previous view while incorporating stochastic variations.
- Core assumption: The action space is sufficiently rich to cover realistic movements, and the video generation model can learn to produce coherent transitions from the physics engine training data.
- Evidence anchors:
  - [abstract]: "a world transition model that generates coherent video sequences as the agent moves"
  - [section]: "We model this transition as an action-driven panoramic video generation process, transforming the previously observed panorama into a new, forward-looking view as the agent progresses."
  - [corpus]: Moderate evidence - GenEx shares similarities with "AirScape: An Aerial Generative World Model" which also uses action-driven generation for aerial agents.
- Break condition: If the action space is too limited or the video generation model cannot learn coherent transitions, the generated videos will appear disjointed or unrealistic.

### Mechanism 3
- Claim: Imagination-augmented policy improves decision-making by allowing agents to explore and gather information about unseen parts of the environment before taking actions.
- Mechanism: The agent uses GenEx to generate imagined observations of the environment by exploring in the generative world, then selects actions based on both real and imagined observations to maximize the policy.
- Core assumption: The generative world accurately represents the physical environment's structure, and the agent can effectively use imagined observations to make better decisions.
- Evidence anchors:
  - [abstract]: "These agents utilize predictive expectations regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices."
  - [section]: "By navigating through the physical space, the agent gathers additional information about its environment... To streamline this process, we use imagination as a pathway for the agent to simulate outcomes without physically traversing."
  - [corpus]: Strong evidence - "Generative World Explorer" is the same paper (185532) with FMR 0.56, showing this is the core contribution.
- Break condition: If the generative world diverges significantly from the physical environment, imagined observations will mislead the agent and degrade decision quality.

## Foundational Learning

- Concept: Panoramic representations (equirectangular, cubemap, spherical)
  - Why needed here: GenEx needs to represent 360Â° environments to enable complete exploration and maintain coherence during transitions
  - Quick check question: What are the three main representations of 360Â° panoramic worlds mentioned in the paper, and how do they transform between each other?

- Concept: Video diffusion models and conditional generation
  - Why needed here: The world transition model uses video diffusion to generate coherent panoramic sequences based on actions and previous observations
  - Quick check question: How does the video diffusion model in GenEx differ from standard video generation models in terms of conditioning inputs?

- Concept: Embodied AI and decision-making under partial observability
  - Why needed here: The imagination-augmented policy addresses the challenge of making decisions with limited information by using generative exploration to gather more information
  - Quick check question: What is the key difference between the common policy (arg max ðœ‹ðœƒ(ð´|ð‘œ,ð‘”)) and the imagination-augmented policy in terms of observation inputs?

## Architecture Onboarding

- Component map:
  - Single image input -> World initialization model (image-to-panorama) -> World transition model (panorama + action -> panoramic video) -> GPT pilot/policy -> Embodied decision-making -> Loop back to action selection

- Critical path:
  1. Single image input
  2. World initialization (panorama generation)
  3. Action selection (human or GPT)
  4. World transition (panoramic video generation)
  5. Embodied decision-making (using imagination-augmented policy)
  6. Loop back to action selection

- Design tradeoffs:
  - Single image vs. multiple images for initialization: Simpler input but may limit initial context
  - Physics engine data vs. real-world data: Easier to collect but may have domain gap
  - Spherical vs. other representations: Better global context but more complex transformations
  - GPT vs. learned policy for action selection: More flexible but potentially less efficient

- Failure signatures:
  - Edge discontinuities in generated panoramas: Indicates SCL not working properly
  - Inconsistent transitions between frames: Suggests world transition model not learning coherence
  - Poor decision quality despite imagination: Means generative world doesn't match physical environment
  - Slow generation speed: Could indicate model complexity or inefficient implementation

- First 3 experiments:
  1. Test world initialization: Input single image, verify 360Â° panorama quality and consistency with input
  2. Test world transition: Generate short panoramic videos with simple actions, check for seamless transitions
  3. Test imagination-augmented policy: Compare decision accuracy with and without imagined observations on simple navigation tasks

## Open Questions the Paper Calls Out
The paper acknowledges several limitations and calls out future research directions:

1. Bridging imaginative and real-world environments remains challenging, particularly regarding sim-to-real adaptation, real sensor integration, and dynamic conditions.
2. Computational requirements and efficiency for real-time applications are not discussed, which would be crucial for practical deployment.
3. Long-term consistency beyond 20-meter explorations needs investigation, as current results only show short-term coherence.

## Limitations
- Spherical-consistent learning details require external references (Lu et al., 2024) for complete understanding
- Evaluation focuses on video generation metrics rather than comprehensive embodied decision-making comparisons
- No computational analysis provided for real-time application feasibility

## Confidence
- High confidence: World initialization and transition pipeline architecture, spherical representation usage, multi-agent scenario feasibility
- Medium confidence: SCL effectiveness based on claimed results without detailed ablations, imagination-augmented policy improvements shown through specific examples
- Low confidence: Long-term trajectory coherence without explicit degradation analysis, generalization to real-world environments beyond synthetic data

## Next Checks
1. Verify SCL implementation by testing panorama generation with and without spherical consistency constraints to measure edge discontinuity reduction
2. Conduct ablation study comparing imagination-augmented policy against baseline approaches across varying levels of environmental complexity
3. Test system robustness by evaluating generation quality on diverse input image types (indoor vs outdoor, structured vs natural scenes) to identify failure patterns