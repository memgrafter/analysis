---
ver: rpa2
title: Scaling NVIDIA's Multi-speaker Multi-lingual TTS Systems with Zero-Shot TTS
  to Indic Languages
arxiv_id: '2401.13851'
source_url: https://arxiv.org/abs/2401.13851
tags:
- challenge
- dataset
- zero-shot
- speaker
- rad-mmm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NVIDIA's MMITS-VC 2024 Challenge systems used RAD-MMM for few-shot
  TTS in Tracks 1-2, training additionally on 5 minutes of target speaker data to
  disentangle speaker, accent, and language attributes. For Track 3's zero-shot TTS,
  they employed P-Flow with speech prompting and extended training data to include
  LibriTTS and the challenge dataset.
---

# Scaling NVIDIA's Multi-speaker Multi-lingual TTS Systems with Zero-Shot TTS to Indic Languages

## Quick Facts
- arXiv ID: 2401.13851
- Source URL: https://arxiv.org/abs/2401.13851
- Reference count: 0
- NVIDIA's systems achieved first place in Track 3 (zero-shot TTS) with MOS 4.4 and SMOS 3.62

## Executive Summary
NVIDIA's MMITS-VC 2024 Challenge systems demonstrated state-of-the-art performance in multi-speaker, multi-lingual TTS for Indic languages. Their approach used RAD-MMM for few-shot TTS in Tracks 1-2, training on 5 minutes of target speaker data, and P-Flow for zero-shot TTS in Track 3, using only 3 seconds of reference speech. All tracks employed HiFi-GAN vocoders. The systems achieved competitive performance on few-shot tracks and first place on the zero-shot track, demonstrating effective speaker, accent, and language disentanglement capabilities.

## Method Summary
The systems employed RAD-MMM with deterministic attribute predictors for Tracks 1-2, enabling cross-lingual synthesis by disentangling speaker, accent, and language attributes. For Track 3, P-Flow with speech prompting was used for zero-shot adaptation to unseen speakers. Both approaches were trained on the challenge dataset with additional external data (LibriTTS and VCTK) to improve generalization. HiFi-GAN neural vocoders converted acoustic features to waveforms for all tracks.

## Key Results
- RAD-MMM achieved competitive performance on Tracks 1-2 (few-shot TTS with 5 minutes target speaker data)
- P-Flow achieved first place in Track 3 (zero-shot TTS) with MOS 4.4 and SMOS 3.62
- Both systems successfully disentangled speaker, accent, and language attributes for cross-lingual synthesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAD-MMM disentangles speaker, accent, and language attributes to enable cross-lingual synthesis without bilingual data
- Mechanism: Deterministic attribute predictors separately predict F0 and energy from text, accent, and speaker inputs, allowing the model to control each attribute independently during synthesis
- Core assumption: Language information can be captured implicitly in the phoneme sequence while accent explains fine-grained pronunciation differences between languages
- Evidence anchors:
  - [abstract] "RAD-MMM [1] to disentangle attributes such as speaker, accent and language, such that the model can synthesize speech for the desired speaker, and the desired language and accent"
  - [section] "Following RAD-MMM, we use deterministic attribute predictors to separately predict fine-grained features like fundamental frequency (F0) and energy given text, accent and speaker"
- Break condition: If accent predictors cannot capture cross-lingual pronunciation differences, the model will fail to produce authentic accents for languages not seen during training

### Mechanism 2
- Claim: P-Flow achieves zero-shot TTS through speech prompting with minimal reference data
- Mechanism: The model adapts to unseen speakers using only 3 seconds of reference speech by conditioning on speech features rather than requiring speaker identity embeddings or extensive adaptation data
- Core assumption: 3 seconds of speech contains sufficient speaker-specific information for the model to capture speaker identity and reproduce similar speech characteristics
- Evidence anchors:
  - [abstract] "P-Flow [2] to perform zero-shot TTS by training on the challenge dataset as well as external datasets" and "P-Flow performs zero-shot TTS using only 3 seconds of reference data for the target speaker"
  - [section] "We train this modified version of P-Flow on both LibriTTS and the challenge datasets" and "During inference, we use Euler's method for sampling with... 3 seconds of speech from the given target speaker for zero-shot TTS"
- Break condition: If reference audio quality is poor or contains significant noise, the speech prompt may not contain enough clean speaker information for adaptation

### Mechanism 3
- Claim: External dataset augmentation improves model generalization across speakers and languages
- Mechanism: Training on LibriTTS and VCTK datasets alongside the challenge data exposes the model to diverse speakers and accents, improving its ability to generalize to unseen speakers and languages
- Core assumption: Additional English-only datasets with many speakers can improve multilingual performance by providing better speaker representation learning
- Evidence anchors:
  - [abstract] "In Track 3, we utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as well as external datasets"
  - [section] "We train this modified version of P-Flow on both LibriTTS and the challenge datasets" and "In Track 2, we train RAD-MMM on the challenge dataset, LibriTTS and VCTK"
- Break condition: If the external datasets have different acoustic characteristics than the target languages, the model may overfit to the external data characteristics rather than learning generalizable speaker representations

## Foundational Learning

- Concept: Disentangled representation learning
  - Why needed here: The challenge requires synthesizing speech across multiple languages and speakers, necessitating separate control over linguistic and speaker attributes
  - Quick check question: How does the model separate language-specific pronunciation patterns from speaker identity?

- Concept: Speech prompting and adaptation
  - Why needed here: Track 3 requires zero-shot synthesis for unseen speakers, making efficient speaker adaptation crucial for maintaining voice similarity
  - Quick check question: What minimal reference data is sufficient for the model to capture and reproduce speaker characteristics?

- Concept: Multi-speaker, multi-lingual data preprocessing
  - Why needed here: The diverse Indic language dataset requires careful preprocessing to handle different phonetic systems, scripts, and acoustic characteristics
- Quick check question: How are language-specific phonemes normalized across the different Indic languages?

## Architecture Onboarding

- Component map: Text → Phonetic representation → Attribute predictors (F0, energy) → Acoustic model → Vocoder → Waveform
- Critical path: Text → Phonetic representation → Attribute predictors (F0, energy) → Acoustic model → Vocoder → Waveform
- Design tradeoffs: Few-shot vs zero-shot approaches balance between adaptation quality and data requirements; external dataset inclusion improves generalization but increases training complexity
- Failure signatures: Speaker identity mismatch in synthesized speech, accent authenticity issues for cross-lingual synthesis, pronunciation errors for specific phonemes
- First 3 experiments:
  1. Train RAD-MMM on challenge dataset only, evaluate mono-lingual synthesis quality
  2. Add LibriTTS/VCTK data to RAD-MMM, evaluate cross-lingual synthesis improvement
  3. Implement P-Flow speech prompting, test zero-shot synthesis with varying reference audio lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAD-MMM change when trained on truly bilingual datasets rather than disentangling speaker, accent, and language from monolingual data?
- Basis in paper: [inferred] The paper states that RAD-MMM can synthesize speech without relying on bilingual data, but does not compare its performance to bilingual training approaches
- Why unresolved: The paper only demonstrates RAD-MMM's capability with monolingual data and does not benchmark against bilingual alternatives
- What evidence would resolve it: Comparative MOS and SMOS results showing performance differences between RAD-MMM trained on monolingual vs. bilingual datasets for the same languages and speakers

### Open Question 2
- Question: What is the minimum amount of reference speech required for P-Flow to maintain its high performance in zero-shot TTS scenarios?
- Basis in paper: [explicit] The paper mentions P-Flow performs zero-shot TTS using only 3 seconds of reference data, but does not explore whether this is the optimal or minimum duration
- Why unresolved: The paper only reports results using 3 seconds of reference data without investigating the impact of shorter or longer reference durations
- What evidence would resolve it: MOS and SMOS scores for P-Flow using reference durations ranging from 1 second to 10 seconds to determine the threshold where performance plateaus

### Open Question 3
- Question: How does the inclusion of additional Indic language datasets beyond the challenge dataset affect the zero-shot TTS performance of P-Flow for languages not seen during training?
- Basis in paper: [inferred] The paper extends P-Flow to seven additional Indic languages but does not test its generalization to other Indic languages beyond those included in the challenge
- Why unresolved: The evaluation only covers the seven languages in the challenge dataset without testing transfer to other Indic languages
- What evidence would resolve it: MOS and SMOS results for P-Flow synthesizing speech in Indic languages that were not part of the training or challenge datasets

## Limitations

- Lack of detailed hyperparameter specifications and training configurations makes exact reproduction challenging
- Evaluation only covers specific Indic language dataset, limiting generalizability claims to other language families
- Reliance on English-only external datasets for improving multilingual performance may not generalize to non-Indo-European languages

## Confidence

**High Confidence**: Experimental methodology and evaluation metrics (MOS, SMOS) are clearly specified and follow standard practices in TTS evaluation.

**Medium Confidence**: Effectiveness of attribute disentanglement in RAD-MMM is demonstrated empirically but lacks theoretical grounding for why deterministic predictors successfully separate speaker, accent, and language information.

**Low Confidence**: Claims about generalization to unseen speakers and languages beyond the specific Indic languages tested are not empirically validated.

## Next Checks

1. **Ablation Study on External Datasets**: Train RAD-MMM and P-Flow with and without LibriTTS/VCTK data to quantify the actual contribution of external datasets to cross-lingual performance.

2. **Speaker Disentanglement Analysis**: Conduct controlled experiments varying speaker identity while keeping language and accent constant, then measuring objective metrics to verify that attribute predictors successfully isolate speaker-specific characteristics.

3. **Cross-Lingual Generalization Test**: Evaluate the trained models on a held-out language family (e.g., Slavic or East Asian languages) to assess whether attribute disentanglement generalizes beyond Indic languages.