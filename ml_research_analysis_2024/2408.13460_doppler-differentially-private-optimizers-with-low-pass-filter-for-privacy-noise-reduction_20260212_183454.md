---
ver: rpa2
title: 'DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy
  Noise Reduction'
arxiv_id: '2408.13460'
source_url: https://arxiv.org/abs/2408.13460
tags:
- filter
- noise
- gradient
- signal
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of privacy noise degradation in
  differentially private (DP) deep learning training, which severely impacts model
  performance especially for large foundation models. The core method idea is DOPPLER,
  a low-pass filter approach that reduces DP noise in the frequency domain while preserving
  gradient signals.
---

# DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction

## Quick Facts
- arXiv ID: 2408.13460
- Source URL: https://arxiv.org/abs/2408.13460
- Reference count: 40
- Primary result: DP optimizers with low-pass filter outperform counterparts by 3%-10% in test accuracy across various models and datasets

## Executive Summary
This paper addresses the critical challenge of performance degradation in differentially private deep learning training, particularly for large foundation models where DP noise injection severely impacts model quality. The proposed DOPPLER method introduces a low-pass filter approach that operates in the frequency domain to separate gradient signals from DP noise, effectively reducing noise while preserving essential gradient information. The approach demonstrates significant improvements in test accuracy (3%-10%) across multiple datasets and model architectures while maintaining theoretical privacy guarantees.

## Method Summary
DOPPLER is a frequency-domain approach that applies low-pass filtering to privatized gradients to reduce DP noise while preserving gradient signals. The method processes gradients by amplifying low-frequency components (representing the true gradient signals) and suppressing high-frequency components (representing DP noise). It can be combined with existing DP optimization techniques like DPSGD, DPAdam, and DPGalore without additional privacy cost, as the filtering operation is post-processing on DP outputs. The approach is particularly effective when DP noise is large, such as during pretraining stages or for large models.

## Key Results
- LP-DPSGD, LP-DPAdam, and LP-DPGalore consistently outperform their non-filtered counterparts by 3%-10% in test accuracy
- DOPPLER maintains theoretical privacy guarantees while improving practical performance
- The method effectively closes the gap between DP and non-DP training performance
- Particularly effective for large models and pretraining stages where DP noise is most problematic

## Why This Works (Mechanism)

### Mechanism 1
Low-pass filtering separates gradient signals from DP noise by exploiting their different frequency characteristics. The algorithm processes privatized gradients by amplifying low-frequency components (gradients) and suppressing high-frequency components (DP noise) in the frequency domain. Core assumption: gradients over training steps form a low-frequency signal while DP noise is a white noise signal.

### Mechanism 2
DOPPLER improves SNR in frequency domain, orthogonal to existing time-domain noise reduction methods. By filtering out high-frequency noise while preserving low-frequency gradient signals, the method increases signal-to-noise ratio. Core assumption: the signal-to-noise ratio in frequency domain is the key metric for DP optimization quality.

### Mechanism 3
The low-pass filter can be combined with existing DP optimization techniques without additional privacy cost. DOPPLER acts as post-processing on privatized gradients, which is immune to privacy guarantees. Core assumption: post-processing of DP outputs does not affect the privacy guarantee.

## Foundational Learning

- Concept: Power Spectral Density (PSD)
  - Why needed here: Used to characterize how gradient and noise power distribute across frequencies
  - Quick check question: How would you compute PSD for a discrete time series of gradients?

- Concept: Differential Privacy (DP) Gaussian Mechanism
  - Why needed here: Core mechanism for understanding how DP noise is injected and affects optimization
  - Quick check question: What is the relationship between privacy budget and noise variance in DP?

- Concept: Signal Processing Filters
  - Why needed here: Understanding how filters work in both time and frequency domains
  - Quick check question: What is the difference between FIR and IIR filters in terms of frequency response?

## Architecture Onboarding

- Component map: Gradient computation → Clipping operation → Noise injection → Low-pass filter → Bias correction → Parameter update

- Critical path: Gradient computation → Noise injection → Low-pass filtering → Parameter update

- Design tradeoffs:
  - Filter order vs memory consumption
  - Frequency response shape vs computational complexity
  - Privacy budget vs model performance
  - Filter coefficients tuning vs automatic adaptation

- Failure signatures:
  - Slow convergence due to over-smoothing
  - Noisy training signals if filter is too weak
  - Memory overflow for high-order filters
  - Privacy budget exhaustion

- First 3 experiments:
  1. Compare DPSGD vs LP-DPSGD on MNIST with small model to verify basic functionality
  2. Test different filter coefficients on CIFAR-10 to find optimal configuration
  3. Measure frequency domain separation between gradients and noise on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DOPPLER scale with different filter orders (na, nb) and what is the optimal filter order for various model sizes and datasets? The paper only tests filters up to 3rd order and does not provide a comprehensive analysis of how filter order affects performance across different scenarios.

### Open Question 2
What is the theoretical foundation for the optimal filter coefficients in DOPPLER, and how can they be adaptively determined during training? The paper provides a heuristic method for estimating filter coefficients but does not establish a rigorous theoretical framework for determining optimal coefficients.

### Open Question 3
How does DOPPLER perform in large-scale foundation model pretraining scenarios with billions of parameters and millions of training steps? The experiments are limited to smaller models, leaving the performance on true foundation models unexplored.

## Limitations
- The frequency-domain separation mechanism relies on the assumption that gradients form smooth low-frequency signals while DP noise is white noise, which may not hold for all architectures
- Filter coefficient selection is described as empirical, with specific values provided only for certain configurations, potentially limiting reproducibility
- The theoretical SNR improvement mechanism assumes idealized conditions that may not translate directly to practical implementations

## Confidence
- High Confidence: The post-processing claim that DOPPLER maintains DP privacy guarantees
- Medium Confidence: The reported 3%-10% accuracy improvements across multiple datasets and models
- Low Confidence: The theoretical SNR improvement mechanism based on frequency domain analysis

## Next Checks
1. Measure and compare the power spectral density of gradients versus DP noise on held-out validation data to empirically verify the low-pass filtering assumption
2. Evaluate DOPPLER's performance across a wider range of filter orders and coefficient values to understand sensitivity and establish robust hyperparameter selection guidelines
3. Test DOPPLER on architectures beyond those reported (CNNs, ResNets, transformers) to assess generalizability, particularly for architectures with potentially high-frequency gradient components