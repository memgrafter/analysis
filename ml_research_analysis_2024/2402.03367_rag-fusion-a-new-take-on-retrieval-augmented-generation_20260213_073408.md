---
ver: rpa2
title: 'RAG-Fusion: a New Take on Retrieval-Augmented Generation'
arxiv_id: '2402.03367'
source_url: https://arxiv.org/abs/2402.03367
tags:
- rag-fusion
- infineon
- query
- queries
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RAG-Fusion, a novel retrieval-augmented generation
  approach that combines RAG with reciprocal rank fusion (RRF) to improve chatbot
  responses. The method generates multiple queries from the original query, retrieves
  documents, re-ranks them using RRF scores, and fuses the results before generating
  a final answer.
---

# RAG-Fusion: a New Take on Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2402.03367
- Source URL: https://arxiv.org/abs/2402.03367
- Authors: Zackary Rackauckas
- Reference count: 16
- Primary result: RAG-Fusion combines RAG with reciprocal rank fusion to improve chatbot responses through multiple query generation and re-ranking

## Executive Summary
RAG-Fusion introduces a novel retrieval-augmented generation approach that enhances traditional RAG by generating multiple contextual queries and using reciprocal rank fusion (RRF) to re-rank retrieved documents. The method was evaluated on an Infineon chatbot for technical product questions, sales queries, and customer inquiries, demonstrating improved accuracy and comprehensiveness compared to traditional RAG. While showing promise for generating more thorough answers, the approach faces challenges with longer response times (1.77x slower) and occasional relevance issues when generated queries are not sufficiently aligned with the original intent.

## Method Summary
RAG-Fusion operates by first generating multiple queries from the original user query using a large language model, exploring different perspectives of the same topic. These queries are used to retrieve documents from a vector database, with each query producing its own ranked list of relevant documents. The system then applies reciprocal rank fusion (RRF) to combine and re-rank documents across all query result sets, giving priority to documents that appear highly ranked in multiple queries. Finally, an LLM generates the final answer using the re-ranked documents and queries. This multi-query approach aims to capture more comprehensive context while reducing hallucinations compared to traditional RAG.

## Key Results
- RAG-Fusion provided more accurate and comprehensive answers compared to traditional RAG on technical product questions
- Manual evaluation found RAG-Fusion excelled in accuracy and comprehensiveness but sometimes lacked relevance
- Response times were 1.77x slower than traditional RAG on average due to multiple query processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple query generation contextualizes the original query from various perspectives, leading to more comprehensive answers.
- Mechanism: The system generates several new search queries from the original user query using a large language model. These queries explore different angles of the same topic, and the resulting document sets from each query are combined and re-ranked using reciprocal rank fusion (RRF).
- Core assumption: The generated queries will retrieve documents that are relevant to the original intent, even if they are not highly relevant to the original query alone.
- Break condition: If generated queries are off-topic or not sufficiently relevant to the original query, the final answer may include irrelevant information or stray from the original intent.

### Mechanism 2
- Claim: Reciprocal rank fusion (RRF) improves document ranking by combining evidence across multiple query result sets.
- Mechanism: RRF assigns a reciprocal rank score to each document based on its position in the ranked lists from different queries. The scores are accumulated and documents are re-ranked by total score, giving higher priority to documents that appear highly ranked in multiple queries.
- Core assumption: Documents that are relevant across multiple queries are more likely to be relevant to the original user query.
- Break condition: If the k parameter in the RRF formula is not well-tuned, the re-ranking may not effectively combine evidence across queries.

### Mechanism 3
- Claim: Combining RAG with RRF addresses limitations of traditional RAG by providing more comprehensive coverage and reducing hallucinations.
- Mechanism: Traditional RAG retrieves documents based on vector distance to a single query, while RAG-Fusion retrieves documents for multiple query variations and uses RRF to prioritize the most relevant ones. This multi-query approach captures more context and reduces the chance that the LLM generates unsupported information.
- Core assumption: The additional context from multiple query variations will reduce the LLM's reliance on its own knowledge base.
- Break condition: If the generated queries are not sufficiently relevant to the original query, the additional context may not improve accuracy and could introduce irrelevant information.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their role in text generation
  - Why needed here: The system uses an LLM to generate multiple queries from the original query and to generate the final answer from the retrieved documents.
  - Quick check question: What is the difference between using an LLM for query generation versus using it for final answer generation?

- Concept: Vector embeddings and similarity search
  - Why needed here: The system uses vector embeddings to represent text and performs similarity search to retrieve documents based on their relevance to the queries.
  - Quick check question: How does vector distance relate to document relevance in the context of RAG?

- Concept: Reciprocal Rank Fusion (RRF) algorithm
  - Why needed here: RRF is used to re-rank documents retrieved from multiple queries by combining their individual ranks into a single score.
  - Quick check question: How does the k parameter in the RRF formula affect the weight given to existing ranks?

## Architecture Onboarding

- Component map: User query -> Query Generator -> Document Retriever (multiple times) -> RRF Engine -> Answer Generator -> Final answer
- Critical path: User query → Query Generator → Document Retriever (multiple times) → RRF Engine → Answer Generator → Final answer
- Design tradeoffs:
  - Query generation increases comprehensiveness but adds latency
  - RRF improves ranking but requires careful tuning of the k parameter
  - Multiple queries increase coverage but may introduce irrelevant documents
  - Using a larger document store improves accuracy but increases retrieval time
- Failure signatures:
  - Slow response times (expected to be 1.77x slower than RAG)
  - Answers that include irrelevant information from off-topic queries
  - Failure to provide negative answers when information is not in the database
  - Occasional need for prompt engineering to generate relevant queries
- First 3 experiments:
  1. Compare answer accuracy and comprehensiveness between RAG and RAG-Fusion on a set of technical product questions
  2. Measure response time for RAG-Fusion with different numbers of generated queries (e.g., 3, 5, 7) to find the optimal tradeoff
  3. Test RAG-Fusion with different k values in the RRF formula to optimize document re-ranking

## Open Questions the Paper Calls Out
The paper identifies several future research directions, including: adapting RAG-Fusion for multimodal data (PDFs with images and tables), optimizing for real-time performance through local LLM hosting, and developing methods to automatically determine the optimal number of generated queries.

## Limitations
- Manual evaluation of answer quality rather than automated metrics limits reproducibility and scalability
- Specific LLM model, parameters, and RRF configuration details are not provided, making exact replication difficult
- Study only tested on Infineon's proprietary dataset without external validation

## Confidence
- **High confidence**: The mechanism of generating multiple queries to capture different perspectives and using RRF for document re-ranking is technically sound and theoretically justified
- **Medium confidence**: The claim that RAG-Fusion provides more accurate and comprehensive answers is supported by manual evaluation but lacks quantitative metrics or external validation
- **Low confidence**: The assertion that RAG-Fusion significantly reduces hallucinations compared to traditional RAG is not empirically demonstrated in the paper

## Next Checks
1. Conduct automated evaluation using established RAG metrics (e.g., ROUGE, BLEU, BERTScore) on a public benchmark dataset to verify accuracy improvements
2. Perform ablation studies to determine the optimal number of generated queries and RRF parameters for balancing accuracy and latency
3. Test RAG-Fusion on diverse datasets from multiple domains to assess generalizability beyond technical product documentation