---
ver: rpa2
title: Towards flexible perception with visual memory
arxiv_id: '2408.08172'
source_url: https://arxiv.org/abs/2408.08172
tags:
- memory
- visual
- dinov2
- neighbors
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a flexible visual memory system that replaces
  traditional static neural network training with a nearest-neighbor retrieval approach
  using pre-trained embeddings. This system allows for easy addition and removal of
  data, interpretability of decisions, and scalability to billion-scale datasets.
---

# Towards flexible perception with visual memory

## Quick Facts
- arXiv ID: 2408.08172
- Source URL: https://arxiv.org/abs/2408.08172
- Authors: Robert Geirjos, Priyank Jaini, Austin Stone, Sourabh Medapati, Xi Yi, George Toderici, Abhijit Ogale, Jonathon Shlens
- Reference count: 40
- One-line primary result: Proposes a visual memory system achieving 88.5% ImageNet top-1 accuracy using billion-scale nearest-neighbor retrieval with RankVoting aggregation

## Executive Summary
This paper introduces a flexible visual memory system that replaces traditional neural network training with nearest-neighbor retrieval using pre-trained embeddings. The approach decomposes image classification into similarity search and knowledge database lookup, enabling capabilities like easy data addition/removal, interpretability, and scalability to billion-scale datasets. The authors demonstrate that their RankVoting aggregation strategy achieves stable performance across different k values while outperforming traditional softmax and distance-weighted methods.

The system shows strong performance across multiple models and datasets, with the key insight that smaller models with larger memories can match larger models with smaller memories. By leveraging efficient similarity search algorithms and billion-scale visual memory, the approach enables lifelong learning, data pruning, and machine unlearning capabilities that are difficult to achieve with conventional training methods.

## Method Summary
The visual memory system consists of two modular components: a frozen pre-trained feature extractor that maps images to embeddings, and a dynamic database of feature-label pairs that can be freely added to, removed from, or reweighted. For classification, the system extracts features from a query image, retrieves k nearest neighbors from the database using cosine similarity, and aggregates neighbor labels using a voting strategy. The authors propose RankVoting, which assigns weights based on neighbor rank using a power-law function (1/(α + rank)), as an improvement over traditional softmax and distance-weighted aggregation methods. The approach is evaluated on ImageNet-1K with both million-scale and billion-scale visual memories, demonstrating that performance scales predictably with memory size following logarithmic scaling laws.

## Key Results
- Achieves 88.5% top-1 ImageNet accuracy when combining RankVoting with vision-language model re-ranking
- Billion-scale visual memory (100M+ images) enables smaller models to match larger models' performance
- RankVoting aggregation provides stable performance across different k values while outperforming softmax and distance-weighted methods
- System enables interpretable decisions through attribution analysis and supports machine unlearning by simply removing samples

## Why This Works (Mechanism)

### Mechanism 1
Decomposing classification into similarity search + nearest neighbor retrieval enables flexible knowledge editing by replacing monolithic network training with modular components. The frozen feature extractor and dynamic database allow for easy addition, removal, and reweighting of knowledge without retraining.

### Mechanism 2
Rank-based voting aggregation outperforms distance-weighted and softmax methods because it avoids overconfidence in distant neighbors. The power-law weighting scheme (1/(α + rank)) provides stable performance across different k values by preventing distant neighbors from dominating the vote.

### Mechanism 3
Billion-scale visual memory improves performance through both better features and more data, following predictable logarithmic scaling laws. As memory size increases from millions to billions of examples, classification accuracy improves consistently, allowing smaller models with larger memories to match larger models with smaller memories.

## Foundational Learning

- **Feature embedding spaces**: Understanding how pre-trained models map images to semantic spaces where similarity corresponds to classification relevance. Why needed: The entire system relies on this mapping being meaningful. Quick check: If two images of the same class are far apart in the embedding space, will kNN classification work effectively?

- **Nearest neighbor search algorithms**: Knowledge of approximate vs exact search, indexing strategies, and computational tradeoffs. Why needed: Efficient retrieval at scale requires understanding these tradeoffs. Quick check: What is the latency difference between exact nearest neighbor search and approximate methods like ScaNN for a 1M image database?

- **Voting aggregation strategies**: Understanding different aggregation methods and their bias-variance tradeoffs. Why needed: Different methods have different sensitivities to hyperparameters and neighbor reliability. Quick check: Why does softmax voting with τ=0.07 perform worse than rank voting for large k values?

## Architecture Onboarding

- **Component map**: Feature extractor (frozen pre-trained model) -> Feature database (vector store) -> Retrieval engine (ScaNN/matrix multiplication) -> Aggregation module (voting strategy) -> Optional re-ranker (vision-language model)

- **Critical path**: 1) Extract features from training images using pre-trained encoder, 2) Store features + labels in database with indexing, 3) For query image: extract features, retrieve k nearest neighbors, 4) Aggregate neighbor labels using chosen voting strategy, 5) (Optional) Re-rank top predictions using vision-language model

- **Design tradeoffs**: Model size vs memory size (smaller models can match larger models with proportionally larger memories), exact vs approximate search (guaranteed recall vs scalability), aggregation sensitivity (simple methods are hyperparameter-free but less accurate vs complex methods are more accurate but sensitive to temperature settings)

- **Failure signatures**: Poor performance despite large memory (check embedding space quality and neighbor reliability), unstable results across different k values (aggregation method may be overconfident in distant neighbors), high latency at scale (consider approximate search methods or distributed retrieval)

- **First 3 experiments**: 1) Reproduce kNN baseline: Extract DinoV2 features, implement softmax voting with τ=0.07, measure accuracy vs k, 2) Test RankVoting: Implement power-law aggregation, compare accuracy stability across k values against softmax baseline, 3) Scale memory experiment: Start with 10K samples, measure accuracy, double memory size repeatedly to observe scaling trend

## Open Questions the Paper Calls Out

- **Embedding model updates**: How does performance scale when the embedding model is updated over time, and what are optimal strategies for updating embeddings while maintaining memory benefits? The paper focuses on fixed pre-trained models and doesn't explore embedding updates.

- **Robustness to adversarial attacks**: Can the memory pruning approach identify and remove samples that are harmful to model robustness against adversarial attacks or distribution shifts? The paper demonstrates pruning for standard accuracy but not robustness.

- **Compositionality and scaling**: What is the relationship between memory size and the ability to capture compositional visual concepts, and how does this compare across different embedding models like DinoV2 versus CLIP? The paper shows performance scales with memory but doesn't systematically quantify compositionality effects.

## Limitations

- The system's effectiveness depends critically on the quality of pre-trained embedding spaces, which may not preserve task-relevant similarity across all domains
- Billion-scale implementation faces practical challenges in storage, indexing, and distributed retrieval that aren't fully addressed
- Performance on ImageNet doesn't guarantee robustness to significant domain shifts or out-of-distribution data

## Confidence

**High Confidence**: RankVoting aggregation outperforms traditional methods in stability; the decomposition approach enables interpretable decisions and flexible knowledge editing; scaling laws show predictable improvement with memory size

**Medium Confidence**: Billion-scale visual memory is practical and provides significant advantages; the system enables lifelong learning, data pruning, and machine unlearning as claimed; vision-language re-ranking consistently improves performance

**Low Confidence**: The approach will generalize effectively to completely different domains beyond ImageNet-style classification; the scaling relationship holds indefinitely as memory approaches billions; approximate search maintains accuracy while providing claimed performance benefits

## Next Checks

1. **Embedding Space Robustness Test**: Evaluate the visual memory system using different pre-trained feature extractors (MAE, BEiT, OpenCLIP) to verify strong performance isn't specific to DinoV2 or CLIP, and quantify sensitivity to embedding quality

2. **Billion-Scale Prototype Implementation**: Build a prototype with 100M+ images to identify practical bottlenecks in storage, indexing, and retrieval latency, and measure the actual performance gap between exact and approximate search methods

3. **Domain Adaptation Evaluation**: Test the system on multiple datasets with varying domain shifts (CIFAR-10, Food-101, EuroSAT) to assess cross-domain generalization and measure performance degradation as a function of domain shift magnitude