---
ver: rpa2
title: 'FastFiD: Improve Inference Efficiency of Open Domain Question Answering via
  Sentence Selection'
arxiv_id: '2408.06333'
source_url: https://arxiv.org/abs/2408.06333
tags:
- sentences
- fastfid
- passages
- training
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FastFiD, a method to improve the inference
  efficiency of the Fusion-in-Decoder (FiD) framework for open-domain question answering
  (ODQA). FastFiD addresses the time-consuming nature of FiD's decoding process, which
  processes all retrieved passages, by implementing a two-stage training approach.
---

# FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection

## Quick Facts
- **arXiv ID:** 2408.06333
- **Source URL:** https://arxiv.org/abs/2408.06333
- **Authors:** Yufei Huang; Xu Han; Maosong Sun
- **Reference count:** 30
- **Primary result:** FastFiD achieves 2.3X-5.7X speedup in inference while maintaining QA performance through sentence selection

## Executive Summary
This paper introduces FastFiD, a method to improve the inference efficiency of the Fusion-in-Decoder (FiD) framework for open-domain question answering (ODQA). FastFiD addresses the time-consuming nature of FiD's decoding process, which processes all retrieved passages, by implementing a two-stage training approach. The first stage employs multi-task learning to simultaneously train sentence selection and answer generation, while the second stage fine-tunes the model to generate answers using only the selected sentences. Experiments on three datasets (Natural Questions, TriviaQA, and ASQA) demonstrate that FastFiD can achieve comparable or even superior QA performance while significantly reducing inference time by 2.3X-5.7X and context length by up to 38X compared to the original FiD model.

## Method Summary
FastFiD improves FiD's inference efficiency through a two-stage training approach with sentence selection. In Stage 1, the model learns both sentence selection and answer generation simultaneously using multi-task learning, where a classification head predicts start/end positions of key sentences and a decoder generates answers. The losses are balanced with a hyperparameter λ. In Stage 2, the model is fine-tuned to generate answers using only the selected sentences, addressing the context length discrepancy between training and inference. During inference, sentences are selected based on cross-attention scores, and answers are generated using only these selected sentences.

## Key Results
- FastFiD achieves 2.3X-5.7X speedup in inference time compared to FiD
- Context length is reduced by up to 38X while maintaining comparable QA performance
- Sentence selection based on cross-attention scores identifies sentences that contribute more to final answers
- FastFiD outperforms HybridFiD (which uses all passages) on Natural Questions when processing 400 passages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-task training with joint sentence selection and answer generation does not create interference, enabling both tasks to improve simultaneously.
- **Mechanism:** By combining a sentence selection loss (LS) and an answer generation loss (LG) in the first training stage, the model learns to identify key sentences while generating answers. The losses are balanced with a hyperparameter λ, allowing the model to handle both tasks without conflict.
- **Core assumption:** The encoder-decoder architecture can support parallel learning of two related but distinct objectives without one dominating the other.
- **Evidence anchors:**
  - [abstract] "Experiments on three commonly used datasets... demonstrate that our method can enhance the inference speed... while simultaneously maintaining the model’s performance."
  - [section] "We carry out experiments to ascertain that the multi-task training... does not conflict with one another during the model’s learning process."
  - [corpus] Found 25 related papers; average neighbor FMR=0.525, average citations=0.0. Top related titles include work on retrieval-augmented generation and passage reranking, suggesting active research in related areas but no direct citations to this specific multi-task approach.
- **Break condition:** If the balance parameter λ is set too high for sentence selection, the answer generation quality may degrade. Conversely, too low a value for sentence selection may prevent effective identification of key sentences.

### Mechanism 2
- **Claim:** Sentence selection based on cross-attention scores identifies sentences that contribute more to the final answer, justifying their use in the decoding process.
- **Mechanism:** During multi-task training, the decoder's cross-attention scores reveal which tokens (from selected sentences) have higher average attention compared to non-selected tokens. This indicates that selected sentences contain more relevant information for answer generation.
- **Core assumption:** Higher cross-attention scores correlate with greater relevance to the answer, making these sentences valuable for decoding.
- **Evidence anchors:**
  - [abstract] "An in-depth analysis of the model’s attention reveals that the selected sentences indeed hold a substantial contribution towards the final answer."
  - [section] "An in-depth analysis of the decoder’s cross-attention reveals that tokens from the chosen sentences yield a higher average attention score compared to those unchosen."
  - [corpus] No direct corpus evidence for this specific attention-based selection mechanism; the related papers focus on passage reranking and retrieval rather than sentence-level attention analysis.
- **Break condition:** If the attention mechanism is misled by common but irrelevant phrases, it might select non-informative sentences, reducing answer quality.

### Mechanism 3
- **Claim:** Two-stage training reduces the context length discrepancy between training and inference, improving performance when decoding with selected sentences.
- **Mechanism:** The first stage trains the model to perform both sentence selection and answer generation with full context. The second stage fine-tunes the model to generate answers using only the selected sentences, adapting it to the reduced context length.
- **Core assumption:** The gap between training (full context) and inference (selected context) causes performance degradation, which can be mitigated by additional fine-tuning.
- **Evidence anchors:**
  - [abstract] "Guided by this insight, we execute a secondary training phase, obliging the model to solely anchor to the selected encoder outputs when making the final prediction."
  - [section] "We introduce a second stage of continuous training aimed at minimizing this discrepancy linked with context length."
  - [corpus] No direct corpus evidence for this two-stage approach; related work focuses on efficiency improvements but not specifically on addressing training-inference context length mismatch.
- **Break condition:** If the selected sentences do not contain sufficient information to answer the question, the second-stage training cannot compensate, leading to degraded performance.

## Foundational Learning

- **Concept:** Multi-task learning with shared encoder
  - Why needed here: Enables the model to learn both sentence selection and answer generation simultaneously without requiring separate models.
  - Quick check question: What happens if the sentence selection loss dominates the answer generation loss during training?

- **Concept:** Cross-attention in encoder-decoder models
  - Why needed here: Used to identify which sentences contribute more to answer generation, guiding the selection process.
  - Quick check question: How do you calculate the average cross-attention score for tokens in selected vs. non-selected sentences?

- **Concept:** Context length and computational efficiency
  - Why needed here: Reducing context length by selecting key sentences significantly speeds up inference without losing relevant information.
  - Quick check question: Why does selecting 200 sentences instead of processing all 100 passages reduce context length by up to 38X?

## Architecture Onboarding

- **Component map:**
  Retriever (DPR) -> Encoder (T5) -> Sentence Selection Head -> Decoder (T5) -> Answer Generation

- **Critical path:**
  1. Retrieve passages using DPR
  2. Encode each passage with T5 encoder
  3. Apply sentence selection head to identify key sentences
  4. Concatenate selected sentence embeddings
  5. Decode answer using selected context

- **Design tradeoffs:**
  - Selecting more sentences increases answer quality but reduces speed gains
  - Balancing λ affects both sentence selection accuracy and answer generation performance
  - Using sentence selection vs. passage reranking affects information density and context length

- **Failure signatures:**
  - Degraded EM score indicates insufficient information in selected sentences
  - Slow inference suggests too many sentences selected or inefficient implementation
  - High variance in cross-attention scores may indicate unstable sentence selection

- **First 3 experiments:**
  1. Vary λ in the multi-task loss to find optimal balance between sentence selection and answer generation
  2. Test different numbers of selected sentences to maximize speed-quality tradeoff
  3. Compare sentence selection vs. passage reranking under similar context lengths

## Open Questions the Paper Calls Out
No explicit open questions are identified in the paper.

## Limitations
- **Architecture Scalability:** The method is only validated on T5-base and T5-large models, with unclear effectiveness for larger models or different architectures.
- **Dataset Generalization:** Experiments are limited to three datasets (Natural Questions, TriviaQA, ASQA) from similar domains, with untested performance on diverse question types.
- **Implementation Complexity:** The two-stage training procedure adds complexity without addressing potential challenges in distributed training or maintenance overhead.

## Confidence
- **High Confidence (8/10):** The core claim that sentence selection can reduce inference time by 2.3X-5.7X while maintaining QA performance is well-supported by experimental results across three datasets.
- **Medium Confidence (6/10):** The assertion that selected sentences contribute more to answers based on cross-attention analysis is plausible but relies on indirect evidence.
- **Low Confidence (4/10):** The claim that multi-task training does not create interference between objectives lacks rigorous validation through ablation studies.

## Next Checks
1. **Attention-Correlation Study:** Measure the actual correlation between cross-attention scores and answer relevance by comparing model attention-based selections against human-rated sentence relevance annotations.

2. **Context-Length Ablation:** Systematically vary the number of selected sentences and measure both inference speed and answer quality to identify the optimal balance point and test the claimed 38X context reduction.

3. **Multi-Task Interference Analysis:** Implement an ablation study comparing standard FiD, FastFiD with separate training stages, and FastFiD with simultaneous training to quantify any interference effects and measure training stability.