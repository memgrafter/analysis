---
ver: rpa2
title: Inference to the Best Explanation in Large Language Models
arxiv_id: '2402.10767'
source_url: https://arxiv.org/abs/2402.10767
tags:
- explanation
- explanations
- language
- figure
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes IBE-Eval, a framework inspired by philosophical
  accounts on Inference to the Best Explanation (IBE) to evaluate and interpret explanations
  generated by Large Language Models (LLMs). IBE-Eval estimates the plausibility of
  natural language explanations through a combination of explicit logical and linguistic
  features including: consistency, parsimony, coherence, and uncertainty.'
---

# Inference to the Best Explanation in Large Language Models

## Quick Facts
- arXiv ID: 2402.10767
- Source URL: https://arxiv.org/abs/2402.10767
- Authors: Dhairya Dalal; Marco Valentino; André Freitas; Paul Buitelaar
- Reference count: 40
- Primary result: IBE-Eval identifies the best explanation with up to 77% accuracy, improving upon GPT 3.5-as-a-Judge baseline by ≈17%

## Executive Summary
This paper introduces IBE-Eval, a framework for evaluating and interpreting explanations generated by Large Language Models (LLMs) using philosophical principles of Inference to the Best Explanation (IBE). The framework estimates explanation plausibility through four criteria: consistency, parsimony, coherence, and uncertainty, using external tools like Prolog solvers and NLI models. Experiments on causal question answering tasks show IBE-Eval can successfully identify the best explanation with up to 77% accuracy, significantly outperforming random selection and a GPT 3.5-as-a-Judge baseline. The study reveals that LLM-generated explanations generally conform to IBE criteria, with linguistic uncertainty being the strongest predictor of explanation quality.

## Method Summary
IBE-Eval generates explanations for each hypothesis using modified Chain-of-Thought prompts, then extracts logical and linguistic features through external tools (Prolog solvers for consistency, NLI models for coherence, uncertainty classifiers). These features are combined via linear regression to score explanation plausibility, with the highest-scoring explanation selected. The framework is evaluated on COPA and E-CARE datasets, comparing accuracy against random selection and GPT-3.5-as-a-Judge baselines.

## Key Results
- IBE-Eval achieves up to 77% accuracy in identifying the best explanation, ≈27% above random selection
- Performance improves upon GPT 3.5-as-a-Judge baseline by ≈17%
- Linguistic uncertainty is the strongest predictor of explanation quality across all evaluated LLMs
- Parsimony and coherence are better predictors than logical consistency for correct hypotheses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IBE-Eval can identify the best explanation by scoring linguistic and logical features of LLM-generated explanations.
- Mechanism: Computes plausibility scores based on four criteria—consistency, parsimony, coherence, and uncertainty—using external tools like Prolog solvers and NLI models, then selects the highest-scoring explanation.
- Core assumption: These explicit linguistic and logical features are measurable and meaningfully differentiate between plausible and implausible explanations.
- Evidence anchors:
  - [abstract] "IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty."
  - [section 4] "IBE-Eval computes an explanation plausibility score derived from the linear combination of the computed selection criteria."
- Break condition: If LLM generates tautological or self-evident explanations, metrics may become uninformative, reducing IBE-Eval's effectiveness.

### Mechanism 2
- Claim: LLMs tend to conform to IBE expectations, with linguistic uncertainty being the strongest predictor of explanation quality.
- Mechanism: Regression analysis shows higher linguistic uncertainty (hedging language) correlates with lower accuracy, indicating LLMs use more ambiguous language when explaining weaker hypotheses.
- Core assumption: Hedging language is a reliable proxy for explanation plausibility.
- Evidence anchors:
  - [abstract] "Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment."
  - [section 7] "The results reveal that linguistic uncertainty is the strongest predictor of explanation quality and is a statistically significant feature for all LLMs."
- Break condition: If LLMs learn to mask uncertainty with confident but incorrect language, uncertainty metric may fail to predict plausibility.

### Mechanism 3
- Claim: Parsimony and coherence are better predictors of explanation quality than logical consistency.
- Mechanism: LLMs often generate logically consistent explanations for incorrect answers (over-rationalization), but explanations for correct answers tend to have lower proof depth, less concept drift, and higher coherence scores.
- Core assumption: Simpler explanations with stronger step-wise entailment are more likely to be correct.
- Evidence anchors:
  - [section 7] "The results suggest that parsimony has a more consistent effect and is a better predictor of explanation quality... We observe negative correlations between proof depth, concept drift, and question-answering accuracy."
  - [section 7] "Both GPT and Llama 2 13B exhibit a higher relative difference between the correct and incorrect hypotheses in contrast to Llama 2 7B."
- Break condition: If LLM's reasoning process inherently requires complex explanations for correct answers, parsimony metrics may incorrectly penalize valid explanations.

## Foundational Learning

- Concept: Causal Question Answering (CQA)
  - Why needed here: IBE-Eval is evaluated on CQA tasks, requiring understanding of how to identify plausible causes or effects from event descriptions.
  - Quick check question: Given "The plant grew tall. What was the cause? A) It was watered. B) It was painted." which option is the more plausible cause and why?

- Concept: Inference to the Best Explanation (IBE)
  - Why needed here: IBE is the philosophical foundation for the evaluation framework, guiding selection of explanations based on criteria like parsimony and coherence.
  - Quick check question: If two explanations are equally consistent, but one is simpler and more coherent, which should be selected according to IBE principles?

- Concept: Neuro-symbolic Integration
  - Why needed here: Framework uses Prolog solvers for entailment verification, requiring understanding of how natural language is translated into formal logic for automated reasoning.
  - Quick check question: How would you convert the statement "IF it rains, THEN the ground gets wet" into a Prolog rule for entailment checking?

## Architecture Onboarding

- Component map: Explanation Generation -> Autoformalization -> Consistency Check -> Parsimony/Coherence/Uncertainty Scoring -> Linear Regression -> Best Explanation Selection

- Critical path: Explanation Generation → Autoformalization → Consistency Check → Parsimony/Coherence/Uncertainty Scoring → Linear Regression → Best Explanation Selection

- Design tradeoffs:
  - Using external solvers adds computational overhead but provides interpretable logical verification
  - Relying on aggregated metrics may hide internal weaknesses in longer explanations
  - Framework is limited to single natural language explanations and evaluated only on causal commonsense reasoning

- Failure signatures:
  - If LLM generates self-evident explanations (proof depth=1, concept drift=0), IBE-Eval may fail to differentiate
  - If NLI model misclassifies entailment strength, coherence scores will be unreliable
  - If uncertainty model fails to detect hedging, linguistic uncertainty will not predict plausibility

- First 3 experiments:
  1. Run IBE-Eval on a small COPA test set and verify it correctly identifies the best explanation more often than random selection
  2. Analyze a subset of explanations where IBE-Eval failed and check if they were self-evident or contained factual errors
  3. Compare IBE-Eval's performance against a GPT-3.5-as-a-Judge baseline on both COPA and E-CARE datasets

## Open Questions the Paper Calls Out

- Does IBE-Eval generalize beyond causal reasoning to other domains like numerical reasoning or symbolic tasks?
  - Basis: Paper notes IBE-Eval was evaluated only on causal commonsense reasoning and suggests future work could extend to more diverse settings
  - Why unresolved: Framework relies on explicit logical and linguistic features that may not transfer directly to non-causal domains
  - What evidence would resolve it: Applying IBE-Eval to GSM8K (numerical reasoning) or logical puzzles, measuring accuracy and correlation with human judgment

- How does IBE-Eval perform when explanations are not provided in entailment form with explicit If-Then steps?
  - Basis: Framework assumes explanations are structured with If-Then statements and assumptions, but paper acknowledges this may not always be the case
  - Why unresolved: Current pipeline depends on autoformalization and structured parsing; unstructured explanations would break this pipeline
  - What evidence would resolve it: Testing IBE-Eval on free-form explanations from open-ended QA tasks, evaluating whether criteria can be adapted

- To what extent does logical consistency actually predict explanation plausibility when LLMs are strong rationalizers?
  - Basis: Paper found LLMs can generate logically consistent explanations for both correct and incorrect answers, making consistency less effective as standalone predictor
  - Why unresolved: While paper notes this limitation, it doesn't quantify how often consistency misleads the model or how it interacts with other features
  - What evidence would resolve it: Ablation studies isolating consistency in IBE-Eval, or analyzing cases where consistent but incorrect explanations are selected

## Limitations
- Limited to single natural language explanations and evaluated only on causal commonsense reasoning tasks (COPA, E-CARE)
- Framework requires fine-tuning of external tools (Prolog solver, NLI model, uncertainty classifier) on target dataset
- Aggregated metrics may mask internal weaknesses in longer explanations
- Autoformalization step introduces potential errors in translating natural language to Prolog rules

## Confidence
- High Confidence: IBE-Eval framework can identify the best explanation with significantly above-random accuracy (≈27% improvement over random, ≈17% over GPT-3.5-as-a-Judge baseline) on COPA and E-CARE datasets
- Medium Confidence: LLM-generated explanations tend to conform to IBE criteria, with linguistic uncertainty being the strongest predictor of explanation quality across all evaluated models
- Medium Confidence: Parsimony and coherence are better predictors of explanation quality than logical consistency, particularly for correct hypotheses, though this may be model-dependent

## Next Checks
1. Evaluate IBE-Eval on additional reasoning tasks beyond causal commonsense (e.g., scientific reasoning, mathematical proofs) to assess framework robustness across domains
2. Systematically analyze explanations where IBE-Eval fails to identify the correct answer, focusing on cases involving self-evident explanations, factual errors, or complex reasoning chains
3. Conduct a more granular ablation study by evaluating IBE-Eval's performance with individual IBE criteria (consistency, parsimony, coherence, uncertainty) removed to quantify their relative contributions