---
ver: rpa2
title: Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language
  Models
arxiv_id: '2408.15796'
source_url: https://arxiv.org/abs/2408.15796
tags:
- entity
- named
- large
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Few-Shot Prompting with Large Language Models
  (LLMs) for Named Entity Recognition (NER), comparing their performance to traditional
  fully supervised methods. The approach uses minimal examples in prompts to guide
  LLMs like GPT-4 for entity recognition tasks, addressing the high cost and time
  required for extensive labeled datasets in traditional NER systems.
---

# Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models

## Quick Facts
- arXiv ID: 2408.15796
- Source URL: https://arxiv.org/abs/2408.15796
- Authors: HÃ©di Zeghidi; Ludovic Moncla
- Reference count: 4
- Primary result: GPT-4 achieved 0.67 F1-score in few-shot NER vs 0.93 for fine-tuned BERT

## Executive Summary
This study evaluates Few-Shot Prompting with Large Language Models (LLMs) for Named Entity Recognition (NER), comparing their performance to traditional fully supervised methods. The approach uses minimal examples in prompts to guide LLMs like GPT-4 for entity recognition tasks, addressing the high cost and time required for extensive labeled datasets in traditional NER systems. Experiments on the GeoEDdA dataset showed that while GPT-4 achieved an F1-score of 0.67 at the token level compared to 0.93 for fine-tuned BERT, it demonstrated strong adaptability to new entity types with limited data. Span-level detection revealed GPT-4o correctly identified 49% of entities versus 28% for GPT-3.5. Smaller local LLMs produced inconsistent results, often failing to understand task requirements or output formats. The study highlights Few-Shot Learning's potential to reduce dependency on large labeled datasets while identifying performance gaps that require further improvement.

## Method Summary
The study employs few-shot prompting with Large Language Models for NER tasks, using a single example from the training set in the prompt to guide entity recognition. The GeoEDdA dataset containing semantic annotations for named entities (Spatial, Person, Misc), nominal entities, spatial relations, and geographic coordinates is used for evaluation. The approach requires JSON output format with token and character positions, implemented through the LangChain framework with OpenAI API calls to GPT models (3.5, 4, 4o). Token-level and span-level performance are measured using precision, recall, and F1-score metrics, with micro average scores for token classification and exact boundary matching for span-level evaluation. The study compares LLM performance against a fine-tuned BERT baseline and tests various local LLMs through LM Studio.

## Key Results
- GPT-4 achieved F1-score of 0.67 at token level compared to 0.93 for fine-tuned BERT
- Span-level detection: GPT-4o correctly identified 49% of entities vs 28% for GPT-3.5
- Smaller local LLMs (Phi3, Gemma, Mistral, Qwen, Llama) produced inconsistent results, often failing to understand task requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting enables LLMs to perform NER with minimal labeled examples by leveraging in-context learning capabilities
- Mechanism: The model learns patterns from a single example in the prompt and applies them to new data without gradient updates
- Core assumption: LLMs retain sufficient task understanding from limited context to generalize to unseen examples
- Evidence anchors:
  - [abstract]: "Few-Shot Prompting or in-context learning enables models to recognize entities with minimal examples"
  - [section]: "A preliminary experiment reveals that LLMs struggle to accurately retrieve or calculate token or span positions from raw input text"
  - [corpus]: Weak - corpus contains related few-shot NER papers but doesn't directly validate this mechanism
- Break condition: Performance degrades when context window is insufficient or when entity patterns differ significantly from the single example provided

### Mechanism 2
- Claim: JSON output format with token/character positions improves NER task precision by providing explicit location information
- Mechanism: Structured output format constrains the model to produce positionally accurate results rather than just entity lists
- Core assumption: LLMs can accurately calculate and output positional information when explicitly required
- Evidence anchors:
  - [section]: "We aim to improve the process by identifying each entity's position in the text and outputting it in JSON format, including both token and character positions"
  - [section]: "Although the output format was correct, the generated position values were inaccurate, resembling hallucinations or random numbers"
  - [corpus]: Weak - corpus neighbors focus on few-shot methods but don't specifically address positional output formats
- Break condition: Model generates plausible-looking but incorrect position values, or fails to follow JSON schema consistently

### Mechanism 3
- Claim: GPT-4 demonstrates superior performance over GPT-3.5 due to enhanced few-shot learning capabilities
- Mechanism: Larger model size and improved architecture enable better pattern recognition from limited examples
- Core assumption: Model scale directly correlates with few-shot learning effectiveness
- Evidence anchors:
  - [section]: "Experiments have shown significant variation across the different classes" and "The experiment confirmed the performance improvement between GPT versions 4 and 3.5"
  - [section]: "Using GPT-3.5, 28% of the predicted spans only are correct (both boundaries and labels) compared to 49% with GPT-4o"
  - [corpus]: Strong - corpus includes papers specifically comparing LLM versions for NER tasks
- Break condition: Performance plateaus or degrades despite model size increases, or smaller models unexpectedly outperform larger ones

## Foundational Learning

- Concept: Token-level vs span-level NER evaluation
  - Why needed here: The paper evaluates both token classification and span detection, which have different success criteria and output formats
  - Quick check question: What's the difference between token-level precision (0.81 for GPT-3.5) and span-level correctness (28% for GPT-3.5)?

- Concept: Few-shot learning vs zero-shot learning vs fine-tuning
  - Why needed here: The study specifically uses few-shot prompting with one example, which is distinct from both zero-shot (no examples) and traditional fine-tuning (thousands of examples)
  - Quick check question: How does the F1-score of 0.67 for GPT-4 in few-shot mode compare to the 0.93 for fine-tuned BERT?

- Concept: JSON output schema design for NER tasks
  - Why needed here: The paper requires structured output with both token and character positions, which is non-standard for LLM outputs
  - Quick check question: Why did the authors include tokenization information in the input data instead of relying on the model to calculate positions?

## Architecture Onboarding

- Component map: GeoEDdA dataset loading -> tokenization -> prompt construction -> LLM inference (GPT-3.5/4/4o) -> JSON parsing -> metric calculation
- Critical path: Dataset -> Prompt engineering -> LLM inference -> JSON parsing -> Metric calculation
- Design tradeoffs: Single example vs multiple examples in few-shot prompting; token positions vs character positions; strict vs partial matching for evaluation
- Failure signatures: Incorrect JSON syntax, missing position attributes, entity boundary mismatches, model outputting unrelated content
- First 3 experiments:
  1. Test with a simple sentence and one entity type to verify JSON output format and position accuracy
  2. Compare GPT-3.5 vs GPT-4 on the same example to observe performance differences
  3. Run with the full test set but only one entity type to establish baseline token-level metrics before span-level analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of few-shot prompting in LLMs vary across different entity types and domains?
- Basis in paper: [explicit] The paper mentions that experiments have shown significant variation across different classes, but does not provide detailed analysis of how performance varies across entity types or domains.
- Why unresolved: The paper only briefly mentions variation across classes without providing a comprehensive analysis of performance differences between entity types or across different domains.
- What evidence would resolve it: A detailed analysis comparing LLM performance across various entity types (e.g., spatial, person, misc) and domains, with specific F1 scores for each category and domain.

### Open Question 2
- Question: What is the optimal number of examples to include in few-shot prompts for NER tasks, and how does this vary between different LLM models?
- Basis in paper: [explicit] The paper references a related study (Agarwal et al., 2024) that indicates the order of examples affects performance and that adding more examples than optimal can degrade performance, but does not investigate this specifically for NER tasks.
- Why unresolved: The paper uses only one example in its experiments but does not explore how varying the number of examples affects NER performance or whether there's an optimal number that differs between models.
- What evidence would resolve it: Systematic experiments varying the number of examples in prompts for different LLM models (GPT-3.5, GPT-4, GPT-4o) and measuring the impact on NER performance metrics.

### Open Question 3
- Question: Why do smaller local LLMs fail to understand task requirements or output formats for NER, and what architectural or training modifications could improve their performance?
- Basis in paper: [explicit] The paper states that smaller local LLMs like Phi3, Gemma, Mistral, Qwen, and Llama "provide the correct JSON output syntax but don't really understand the defined set of labels" or "completely don't understand the task."
- Why unresolved: The paper observes this failure but does not investigate the underlying reasons for these failures or propose potential solutions to improve smaller models' understanding of NER tasks.
- What evidence would resolve it: Comparative analysis of the architectural differences between successful and failing models, investigation of training data differences, and experiments testing modifications to improve smaller models' task understanding.

### Open Question 4
- Question: How can the accuracy of token and span position extraction be improved in LLM-based NER systems?
- Basis in paper: [explicit] The paper notes that "LLMs struggle to accurately retrieve or calculate token or span positions from raw input text" and that generated position values were "inaccurate, resembling hallucinations or random numbers."
- Why unresolved: While the paper proposes including tokenization information in the input as a partial solution, it does not explore other methods to improve position accuracy or investigate why LLMs struggle with this specific task.
- What evidence would resolve it: Comparative experiments testing various approaches to position extraction (e.g., post-processing corrections, specialized fine-tuning, different prompt formulations) and their impact on position accuracy metrics.

## Limitations

- Limited generalization to complex entity types: Performance on nested entities, overlapping spans, or ambiguous boundaries remains unknown
- Position calculation reliability: LLMs struggle to accurately calculate token and character positions, producing hallucinations
- Model dependency and cost: Superior GPT-4 performance requires significant computational cost and API dependency

## Confidence

**High Confidence**: The comparative performance differences between GPT-3.5 and GPT-4 (28% vs 49% span-level correctness) are well-supported by experimental results and align with expectations about model capability scaling. The token-level metrics showing GPT-4 at 0.67 F1-score versus BERT's 0.93 are also reliably measured.

**Medium Confidence**: The claim that few-shot prompting reduces dependency on large labeled datasets is supported by the experimental design but lacks comparison to intermediate supervision levels (e.g., 10 examples, 100 examples). The mechanism by which single examples enable pattern recognition is plausible but not thoroughly validated across diverse entity types.

**Low Confidence**: The reliability of JSON output with position information is questionable given the documented hallucinations in position values. The paper's approach of requiring token and character positions may be fundamentally incompatible with current LLM capabilities for precise numerical calculations.

## Next Checks

1. **Position Verification Benchmark**: Create a controlled experiment with simple, unambiguous text where correct token positions can be algorithmically verified. Test whether GPT-4 consistently produces accurate position values across multiple runs, or if position accuracy correlates with text complexity.

2. **Intermediate Supervision Analysis**: Replicate the experiments with 5, 10, and 50 training examples per entity type to determine whether few-shot performance represents a true inflection point or if additional examples would significantly improve results. This would clarify whether the single-example approach is optimal or merely a starting point.

3. **Cross-Dataset Generalization Test**: Evaluate the same GPT-4 few-shot approach on a more complex NER dataset (e.g., CoNLL-2003 or OntoNotes) with nested entities and overlapping spans. This would test whether the 0.67 F1-score on GeoEDdA represents a generalizable capability or dataset-specific performance.