---
ver: rpa2
title: Unifying Global and Near-Context Biasing in a Single Trie Pass
arxiv_id: '2409.13514'
source_url: https://arxiv.org/abs/2409.13514
tags:
- n-gram
- biasing
- keyword
- language
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving named entity (NE)
  recognition and out-of-vocabulary (OOV) word handling in end-to-end automatic speech
  recognition (ASR) systems. The proposed method combines keyword biasing and word-level
  n-gram language modeling (LM) within a single trie structure using the Aho-Corasick
  algorithm, enabling efficient shallow fusion during beam search decoding.
---

# Unifying Global and Near-Context Biasing in a Single Trie Pass

## Quick Facts
- arXiv ID: 2409.13514
- Source URL: https://arxiv.org/abs/2409.13514
- Reference count: 40
- Up to 32% relative improvement in named entity recognition accuracy and 12% relative reduction in word error rate

## Executive Summary
This paper presents a method for improving named entity recognition and out-of-vocabulary word handling in end-to-end ASR systems by combining keyword biasing with word-level n-gram language modeling in a single Aho-Corasick trie structure. The approach efficiently integrates both biasing methods during beam search decoding without modifying the ASR model architecture. Experiments across four languages and three datasets demonstrate significant improvements in entity recognition while maintaining real-time decoding performance.

## Method Summary
The method builds a unified Aho-Corasick trie containing both n-gram language model probabilities and keyword biasing information. Word-level n-gram LM weights are converted to subword-level arc costs using exponential transformation, then integrated with keyword costs that distinguish between in-vocabulary and out-of-vocabulary entities. During decoding, the Aho-Corasick algorithm efficiently matches partial hypotheses against the trie, adding appropriate costs to log probabilities. The approach uses shallow fusion integration with a Transformer-Transducer ASR model, enabling context adaptation with minimal computational overhead.

## Key Results
- Up to 32% relative improvement in named entity recognition accuracy
- Up to 12% relative reduction in word error rate
- Up to 21.6% relative improvement in general word error rate with maintained real-time decoding performance
- Improved recognition of out-of-vocabulary named entities across multiple languages and domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Aho-Corasick algorithm enables efficient on-the-fly integration of word-level n-gram LM with subword-level beam search.
- Mechanism: AC trie is built directly from n-gram LM probabilities, converted to exponential-based costs for subword arcs. During decoding, the AC automaton efficiently finds partial matches and backtracks on failures, avoiding duplicate prefix matches.
- Core assumption: Converting word-level n-gram LM probabilities to subword-level arc costs preserves the LM's discriminative power while maintaining compatibility with BPE-level ASR hypotheses.
- Evidence anchors:
  - [abstract] "We efficiently integrate this enriched biasing method into a transducer-based ASR system, enabling context adaptation with almost no computational overhead."
  - [section] "The AC algorithm has three data structures as a representation of a search set, or a transition diagram: a trie, an output table, and a failure function... This allows finding partial matches and also makes the arrays of the trie sparse, which helps improve the efficiency of the algorithm."
  - [corpus] Weak evidence - no direct mention of Aho-Corasick in corpus titles/abstracts.
- Break condition: If n-gram LM vocabulary significantly mismatches the ASR model's BPE vocabulary, the conversion to subword arcs may lose important word-level distinctions.

### Mechanism 2
- Claim: Combining keyword biasing with n-gram LM in a single trie improves overall WER while maintaining strong named entity recognition.
- Mechanism: The single trie contains both n-gram LM arcs (with probability-based costs) and keyword arcs (with fixed bias costs). When a hypothesis matches both an n-gram and a keyword, the system benefits from word-level statistics while still promoting specific entities.
- Core assumption: The n-gram LM's word-level statistics provide complementary information to keyword biasing, improving overall coverage without degrading entity-specific performance.
- Evidence anchors:
  - [abstract] "We demonstrate that the proposed combination of keyword biasing and n-gram LM improves entity recognition by up to 32% relative and reduces overall WER by up to a 12% relative."
  - [section] "We introduce a light n-gram LM converting words into subwords first and then adjusting the LM weights to correspond to the units after the split."
  - [corpus] Weak evidence - corpus papers mention trie-based biasing but not the specific combination approach.
- Break condition: If the keyword list contains many entities that are semantically similar to high-probability n-grams, the fixed bias costs may conflict with the LM's statistical preferences.

### Mechanism 3
- Claim: The exponential-based cost conversion preserves the relative importance of n-grams while avoiding log-domain bias amplification.
- Mechanism: Instead of using log probabilities directly (which would create uneven bias due to exponentiation during decoding), the method converts LM log probabilities back to probabilities using exp(LM w) and uses these as arc costs.
- Core assumption: The relative ordering of n-gram probabilities is preserved under the exponential transformation, ensuring that more probable n-grams receive proportionally higher support.
- Evidence anchors:
  - [section] "To control for the influence of word-level statistics integrated on the subword level... we convert LM log probabilities back to probabilities by taking an exponent."
  - [abstract] "We achieve up to 21.6% relative improvement in the general word error rate with no practical difference in the inverse real-time factor."
  - [corpus] No direct evidence in corpus - this appears to be a novel contribution.
- Break condition: If the LM contains very low-probability n-grams, the exponential conversion may still create excessive bias for these unlikely sequences.

## Foundational Learning

- Concept: Aho-Corasick string matching algorithm
  - Why needed here: Enables efficient trie-based pattern matching with failure transitions, allowing the system to find partial matches and backtrack without restarting the search.
  - Quick check question: What is the key difference between Aho-Corasick and standard trie search that makes it suitable for real-time ASR decoding?

- Concept: Shallow fusion in end-to-end ASR
  - Why needed here: Provides a way to integrate external LMs during decoding without modifying the trained ASR model, using log-linear interpolation of model scores.
  - Quick check question: In shallow fusion, what hyperparameter controls the influence of the external LM on the final recognition result?

- Concept: BPE (Byte-Pair Encoding) subword units
  - Why needed here: The ASR model outputs hypotheses at the BPE level, so the word-level n-gram LM must be converted to subword-level representations for integration.
  - Quick check question: Why must the word-level n-gram LM be converted to subword units before being used with a BPE-based ASR model?

## Architecture Onboarding

- Component map:
  Transformer-Transducer ASR model (Zipformer) -> SentencePiece tokenizer -> Aho-Corasick trie (n-gram LM + keywords) -> Icefall framework with AC algorithm -> Beam search decoder with shallow fusion

- Critical path:
  1. Build Aho-Corasick trie from n-gram LM (converted to BPE) + keywords
  2. During beam search, match hypotheses against AC trie
  3. Add appropriate costs to log probabilities based on matches
  4. Continue decoding with modified scores

- Design tradeoffs:
  - Using word-level LM provides better statistics but requires BPE conversion
  - Single trie simplifies implementation but may create conflicts between LM and keyword biases
  - Exponential cost conversion preserves relative probabilities but may amplify low-probability n-grams

- Failure signatures:
  - If RTFX increases significantly, the AC trie may be too large or inefficient
  - If WER degrades on general vocabulary, the keyword bias costs may be too aggressive
  - If NE recognition doesn't improve, the keyword list may not be well-matched to the audio content

- First 3 experiments:
  1. Compare baseline beam search vs. SF with n-gram LM only (experiment 2 in Tab. III)
  2. Add keyword biasing without LM to establish NE recognition ceiling (experiment 4 in Tab. III)
  3. Combine both approaches in single trie and measure WER/NE trade-off (experiment 5 in Tab. III)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance scale with increasing vocabulary size and entity list complexity across different languages?
- Basis in paper: [inferred] The paper evaluates performance across four languages and three datasets, but doesn't systematically explore scaling behavior with vocabulary size or entity list complexity.
- Why unresolved: The experiments use fixed datasets with varying but unspecified vocabulary sizes and entity list complexities. No controlled experiments were conducted to isolate the effects of these factors.
- What evidence would resolve it: Systematic experiments varying vocabulary size and entity list complexity while measuring WER, NE accuracy, and RTFX across multiple languages would provide definitive answers.

### Open Question 2
- Question: What is the impact of different subword segmentation strategies (beyond SentencePieces) on the integration of word-level n-gram LMs with transducer-based ASR systems?
- Basis in paper: [explicit] The paper mentions using SentencePieces for converting words to subwords, but doesn't explore alternative segmentation strategies or their impact.
- Why unresolved: Only one subword segmentation approach was evaluated, limiting understanding of how different strategies might affect the fusion of word-level LMs with subword-level ASR outputs.
- What evidence would resolve it: Comparative experiments using different subword segmentation approaches (e.g., BPE variants, Unigram, WordPiece) while measuring WER, NE accuracy, and RTFX would clarify this impact.

### Open Question 3
- Question: How does the proposed approach perform in streaming/online ASR scenarios with real-time entity list updates?
- Basis in paper: [inferred] While the paper mentions "on-the-fly decoding" and RTFX measurements, it doesn't specifically address streaming scenarios with dynamic entity list updates.
- Why unresolved: The experiments focus on batch decoding with static entity lists, not on the challenges of maintaining performance during continuous entity list modifications in streaming applications.
- What evidence would resolve it: Experiments measuring WER, NE accuracy, and latency in streaming scenarios with dynamic entity list updates would provide definitive answers about real-world applicability.

## Limitations
- Method effectiveness depends heavily on quality and coverage of n-gram language model and keyword lists
- Requires careful tuning of bias weights (αinLM and αoutLM) to balance WER improvement and entity recognition
- Exponential cost conversion may create disproportionate bias for low-probability n-grams in some domains

## Confidence

- **High Confidence**: The core mechanism of Aho-Corasick trie integration with n-gram LM weights and the overall improvement trends across datasets
- **Medium Confidence**: The specific exponential cost conversion formula and its superiority over log-based alternatives (based on experimental results but limited theoretical justification)
- **Medium Confidence**: The generalization across four languages, though the datasets vary significantly in domain and annotation quality

## Next Checks
1. Cross-domain robustness test: Apply the method to a dataset from a completely different domain (e.g., conversational speech) to evaluate generalization beyond the banking/insurance/healthcare and financial domains tested
2. Low-resource language validation: Test the approach on a low-resource language with limited training data to assess performance when n-gram LM quality may be lower
3. Bias weight sensitivity analysis: Systematically vary αinLM and αoutLM parameters to characterize the trade-off surface between WER improvement and NE recognition accuracy, providing guidance for optimal parameter selection