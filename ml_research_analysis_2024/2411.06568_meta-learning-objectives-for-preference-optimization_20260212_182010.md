---
ver: rpa2
title: Meta-Learning Objectives for Preference Optimization
arxiv_id: '2411.06568'
source_url: https://arxiv.org/abs/2411.06568
tags:
- algorithms
- which
- should
- preference
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for discovering new preference
  optimization algorithms using evolutionary strategies. The key innovation is Mirror
  Preference Optimization (MPO), which generalizes existing methods like DPO and ORPO
  by using Bregman divergences instead of KL divergence.
---

# Meta-Learning Objectives for Preference Optimization

## Quick Facts
- arXiv ID: 2411.06568
- Source URL: https://arxiv.org/abs/2411.06568
- Reference count: 40
- Authors propose a framework using evolutionary strategies to discover new preference optimization algorithms, achieving state-of-the-art results on both MuJoCo and LLM alignment tasks

## Executive Summary
This paper introduces Mirror Preference Optimization (MPO), a framework that generalizes existing preference optimization methods by using Bregman divergences instead of KL divergence. Through evolutionary strategies, the authors discover specialized algorithms for different qualities of preference data, addressing a critical gap where existing methods struggle with noisy and mixed-quality datasets. The approach demonstrates that insights from simpler reinforcement learning environments can transfer to complex LLM alignment tasks, leading to the development of TeMPO, a temporally-aware algorithm that significantly outperforms existing baselines.

## Method Summary
The authors develop a meta-learning framework that parameterizes preference optimization objectives as neural networks, allowing evolutionary strategies to discover optimal functions g, ψ, and ϕ⁻¹ simultaneously. They test this approach on MuJoCo environments (Three Legged Ant and Hopper) with preference datasets of varying quality (base, noisy with ε=0.1, and mixed). The discovered algorithms are then transferred to LLM alignment tasks by adapting the Alignment Handbook codebase for Gemma-7B and Mistral-7B models. The framework uses Mirror Descent theory with Bregman divergences to generalize existing methods like DPO and ORPO, while temporal weighting strategies are employed to balance supervised fine-tuning and preference optimization.

## Key Results
- Discovered MPO algorithms outperform existing baselines (DPO, ORPO, SimPO, CPO) on MuJoCo tasks with noisy and mixed-quality datasets
- TeMPO, a temporally-aware algorithm derived from MuJoCo insights, achieves state-of-the-art performance on LLM alignment tasks
- The evolutionary strategies approach successfully identifies specialized algorithms for different dataset qualities, demonstrating the value of meta-learning in objective discovery

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Evolutionary strategies can discover specialized PO algorithms by optimizing parameters of the mirror map, monotonic transformations, and classification functions simultaneously.
- **Mechanism**: The paper parameterizes g, ψ, and ϕ⁻¹ as neural networks with non-negative kernels and monotonicity constraints, then uses OpenAI-ES to estimate gradients of the expected cumulative reward with respect to these parameters. This allows exploration of a continuous space of PO objectives rather than testing discrete hand-crafted variants.
- **Core assumption**: The parameterized class of functions (with monotonicity constraints) contains algorithms that can outperform existing baselines across different dataset qualities.
- **Evidence anchors**:
  - [abstract]: "Through evolutionary strategies, we search this class to discover algorithms specialized to specific properties of preference datasets"
  - [section 3.1]: "we employ a neural network parametrization for g, ψ, and ϕ⁻¹, which we optimize using evolutionary strategies"
  - [corpus]: Weak - corpus lacks papers describing ES-based discovery of PO algorithms
- **Break condition**: If the parameterization is too restrictive or the ES optimization gets stuck in local optima, the discovered algorithms may not generalize beyond the training environment.

### Mechanism 2
- **Claim**: Bregman divergences generalize KL divergence regularization, enabling algorithms to handle noise and mixed-quality data better than existing methods.
- **Mechanism**: By replacing KL divergence with a Bregman divergence induced by a mirror map hϕ, the regularization geometry can be adapted to different data distributions. The mirror map controls how updates are scaled based on current parameter values, preventing overoptimization in noisy regions.
- **Core assumption**: Different Bregman divergences provide regularization geometries that are more suitable for noisy and mixed-quality datasets than the standard KL divergence.
- **Evidence anchors**:
  - [abstract]: "Mirror Preference Optimization (MPO), which generalizes existing methods like DPO and ORPO by using Bregman divergences instead of KL divergence"
  - [section 3]: "we have the following result...r(τ) = βϕ⁻¹(π⋆(τ)) − βϕ⁻¹(πref(τ)) + c(s₀)"
  - [corpus]: Weak - corpus lacks comparative studies of Bregman vs KL divergence in PO
- **Break condition**: If the theoretical justification for replacing KL with Bregman divergences doesn't hold in practice, or if the optimal mirror map varies too much between datasets.

### Mechanism 3
- **Claim**: Temporally-aware PO algorithms that gradually shift from SFT to preference optimization outperform static methods.
- **Mechanism**: The TeMPO algorithm uses a time-varying weight α(t) that starts with emphasis on the SFT term and gradually shifts to the PO term. This prevents early overoptimization on noisy data while still allowing sufficient preference optimization later in training.
- **Core assumption**: The optimal balance between SFT and PO changes during training, with more SFT needed early when the policy is uncertain and more PO needed later when the policy is more stable.
- **Evidence anchors**:
  - [abstract]: "based on the insights gained from our MuJoCo experiments, we design a PO algorithm that significantly outperform existing baselines in an LLM alignment task"
  - [section 4.3]: "TeMPO consists of single-phase algorithms that optimize the objective...where α: [0,1] → [0,1] is an increasing function of the percentage of training progress"
  - [corpus]: Weak - corpus lacks papers describing temporally-aware PO algorithms
- **Break condition**: If the optimal α(t) schedule is highly dataset-dependent, requiring manual tuning for each new dataset.

## Foundational Learning

- **Concept**: Mirror descent and Bregman divergences
  - Why needed here: The paper's MPO framework is built on mirror descent theory, using Bregman divergences to generalize KL regularization
  - Quick check question: What is the relationship between mirror maps, Bregman divergences, and the KL divergence?

- **Concept**: Evolutionary strategies for gradient estimation
  - Why needed here: ES is used to optimize the parameters of the PO algorithm itself, which is a non-differentiable optimization problem
  - Quick check question: How does OpenAI-ES estimate gradients without backpropagation?

- **Concept**: Bradley-Terry model for preference modeling
  - Why needed here: The preference datasets are assumed to follow the Bradley-Terry model, which relates reward differences to choice probabilities
  - Quick check question: What is the mathematical form of the Bradley-Terry model and how does it connect to the sigmoid function?

## Architecture Onboarding

- **Component map**: MuJoCo environment wrapper with custom tasks (TLA, Hopper) -> Preference dataset generator with different quality levels (base, noisy, mixed) -> PO algorithm implementations (DPO, ORPO, SimPO, CPO, etc.) -> MPO framework with mirror map parameterization -> Evolutionary strategies optimizer -> LLM fine-tuning pipeline (Alignment Handbook modification) -> Evaluation framework (AlpacaEval with GPT-4 judge)

- **Critical path**: 
  1. Generate MuJoCo preference datasets
  2. Train baseline PO algorithms on each dataset
  3. Discover MPO algorithms using ES
  4. Analyze discovered algorithms for insights
  5. Design TeMPO based on insights
  6. Transfer to LLM fine-tuning
  7. Evaluate against baselines

- **Design tradeoffs**:
  - MuJoCo vs real LLM environments: MuJoCo provides ground truth rewards but may not capture LLM complexities
  - ES computational cost vs gradient-based optimization: ES is more expensive but handles non-differentiable objectives
  - Single vs multi-stage training: TeMPO combines SFT and PO but may be harder to tune than separate stages

- **Failure signatures**:
  - Poor transfer from MuJoCo to LLM: Discovered algorithms overfit to simple environments
  - ES optimization divergence: Gradient estimates become unreliable with high variance
  - Dataset quality mismatch: Algorithms perform well on generated data but poorly on real human preferences

- **First 3 experiments**:
  1. Run all baseline PO algorithms on TLA base dataset to establish performance floor
  2. Discover MPO algorithms using ES on TLA noisy dataset (ε=0.1)
  3. Compare discovered algorithms' performance across all three dataset qualities to identify specialization patterns

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of TeMPO scale when applied to larger LLMs (e.g., GPT-4, Claude, or models with >100B parameters)?
  - Basis in paper: [inferred] The paper demonstrates TeMPO's effectiveness on Gemma-7B and Mistral-7B, but does not test it on larger frontier models where scaling effects might differ.
  - Why unresolved: The authors focused on smaller, more accessible models for evaluation, leaving open whether the insights from MuJoCo transfer to the most challenging LLM alignment scenarios.
  - What evidence would resolve it: Empirical results showing TeMPO's win-rates and training stability on LLMs with 100B+ parameters across diverse alignment datasets.

- **Open Question 2**: What is the computational overhead of using evolutionary strategies to discover PO objectives compared to traditional hyperparameter tuning?
  - Basis in paper: [explicit] The paper uses ES to discover new PO objectives but does not compare its computational cost to standard methods like grid search or Bayesian optimization.
  - Why unresolved: While ES successfully finds better objectives, the paper does not quantify whether this approach is practical for widespread adoption given the resource requirements.
  - What evidence would resolve it: A detailed computational cost analysis comparing ES-based discovery to traditional hyperparameter optimization across multiple tasks and model sizes.

- **Open Question 3**: How sensitive are the discovered MPO objectives to the specific characteristics of the preference dataset (e.g., distribution of trajectory lengths, quality variance)?
  - Basis in paper: [inferred] The paper tests on base, noisy, and mixed-quality datasets but does not systematically vary dataset characteristics to test robustness of the discovered algorithms.
  - Why unresolved: The current experiments use relatively uniform dataset structures, leaving open how the discovered objectives perform when dataset properties deviate significantly from the training distribution.
  - What evidence would resolve it: Experiments varying dataset characteristics (length distributions, quality variance, sampling strategies) to measure performance degradation of discovered objectives.

## Limitations
- The transfer from MuJoCo to LLM alignment, while promising, represents a significant extrapolation that may not fully capture the complexities of real-world preference data
- The evolutionary strategies approach for discovering new algorithms is computationally expensive and may not scale well to more complex environments or objective functions
- The assumption that parameterized functions with monotonicity constraints can discover superior algorithms may be overly restrictive, potentially missing out on useful objective forms outside this class

## Confidence
- **High confidence**: The mathematical framework of Mirror Preference Optimization (MPO) and its relationship to existing methods (DPO, ORPO) is well-established and theoretically sound
- **Medium confidence**: The empirical results showing superior performance of discovered algorithms on MuJoCo tasks are convincing, but the generalization to LLM alignment needs more validation
- **Medium confidence**: The insights about algorithm specialization for different dataset qualities are well-supported by the experimental results, though the underlying reasons could be explored further

## Next Checks
1. **Cross-environment validation**: Test the discovered MPO algorithms on additional MuJoCo environments beyond Ant and Hopper to verify generalization across different dynamics and reward structures

2. **Real-world preference data**: Evaluate TeMPO and other discovered algorithms on preference datasets collected from human annotators for LLM tasks, comparing performance against baselines on both in-distribution and out-of-distribution prompts

3. **Ablation studies on parameterization**: Systematically vary the neural network architecture and constraint sets for g, ψ, and ϕ⁻¹ to understand the sensitivity of the discovered algorithms to these design choices