---
ver: rpa2
title: Should agentic conversational AI change how we think about ethics? Characterising
  an interactional ethics centred on respect
arxiv_id: '2401.09082'
source_url: https://arxiv.org/abs/2401.09082
tags:
- respect
- social
- user
- system
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that current approaches to LLM ethics\u2014\
  focused on avoiding harmful or biased outputs\u2014are insufficient as AI systems\
  \ become more proactive and agentic in social interactions. The authors propose\
  \ an interactional ethics framework centered on respect, identifying three types\
  \ of social-interactional harms: direct harm to users, harmful influence on user\
  \ behavior, and cumulative harm over time."
---

# Should agentic conversational AI change how we think about ethics? Characterising an interactional ethics centred on respect

## Quick Facts
- arXiv ID: 2401.09082
- Source URL: https://arxiv.org/abs/2401.09082
- Reference count: 19
- Primary result: Proposes an interactional ethics framework centered on respect for agentic conversational AI, identifying three types of social-interactional harms and operationalizing respect as duties to affirm autonomy, competence, and self-worth.

## Executive Summary
This paper argues that current approaches to LLM ethics—focused on avoiding harmful or biased outputs—are insufficient as AI systems become more proactive and agentic in social interactions. The authors propose an interactional ethics framework centered on respect, identifying three types of social-interactional harms: direct harm to users, harmful influence on user behavior, and cumulative harm over time. They operationalize respect as duties to affirm users' autonomy, competence, and self-worth, drawing from psychology and healthcare literature. The framework emphasizes treating users as individuals with unique needs, rather than optimizing for general helpfulness.

## Method Summary
The paper synthesizes philosophical, psychological, and healthcare literature to develop an interactional ethics framework for agentic conversational AI. It draws on social actor theory, Basic Psychological Needs Theory, and person-centered care principles to operationalize respect as specific interactional duties. The framework distinguishes between direct, influence-based, and cumulative social-interactional harms, and proposes that AI systems should affirm users' autonomy, competence, and self-worth in their interactions.

## Key Results
- Proposes an interactional ethics framework that treats AI agents as social actors rather than mere output generators
- Identifies three types of social-interactional harms: direct, influence-based, and cumulative
- Operationalizes respect through duties to affirm autonomy, competence, and self-worth in user interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Viewing LLM agents as social actors rather than mere output generators shifts ethical evaluation from semantic correctness to interactional appropriateness.
- Mechanism: The nested model of social actor levels (social meaning → intentional actions → human-like behaviors → agentic behaviors) shows how increasing social cues and perceived agency cause users to apply human social norms to AI interactions.
- Core assumption: Users intuitively treat interactive systems as intentional agents when they exhibit human-like qualities, regardless of their actual capacity for intention.
- Evidence anchors:
  - [abstract] "With the push towards agentic AI, wherein systems become increasingly proactive in chasing goals and performing actions in the world, considering the pragmatics of interaction becomes essential."
  - [section] "Research suggests that people tend to treat interactive (mixed-initiative) technologies as if they are acting with intention... Regardless of whether people believe that an interactive system actually possesses humanlike qualities or capacities, they still tend to treat it as if it has judgments, intentions, or other folk-psychological states."
  - [corpus] Weak - no direct corpus evidence found for the nested model mechanism.
- Break condition: If users do not respond emotionally to AI social cues, or if systems remain purely functional without human-like presentation.

### Mechanism 2
- Claim: Respectful treatment of users requires affirming their autonomy, competence, and self-worth through specific interactional duties.
- Mechanism: The framework operationalizes respect by mapping philosophical concepts to psychological needs (autonomy, competence, relatedness) and person-centered care principles, creating actionable design guidelines.
- Core assumption: Users have fundamental psychological needs that, when supported, lead to better engagement and wellbeing with AI systems.
- Evidence anchors:
  - [section] "BPNT posits three 'basic psychological needs' (BPNs) that, if unsupported, undermine a person's performance, willingness to engage in activities, and overall vitality and wellbeing... These needs are considered basic as they seem to operate similarly 'for all people at all ages in all cultures'."
  - [section] "Treating someone with respect, in the moralised sense, means to behave in ways that suggest an appropriate regard for fundamental aspects of their humanity... treating them as intelligent enough to understand what their choices involve."
  - [corpus] Weak - no corpus evidence directly supporting the psychological needs framework.
- Break condition: If the framework's duties conflict with user preferences or if users do not value autonomy/competence affirmation in AI interactions.

### Mechanism 3
- Claim: Cumulative social-interactional harms emerge from repeated patterns of disrespectful treatment, not just individual interactions.
- Mechanism: The framework distinguishes between direct, influence-based, and cumulative harms, showing how seemingly neutral or helpful utterances can become harmful when contextualized in ongoing relationships.
- Core assumption: Users track and remember patterns of treatment over time, and repeated disrespect erodes trust and wellbeing even if individual instances seem minor.
- Evidence anchors:
  - [section] "A third type of social interactional harm is the potential for subtle forms of undesirable (e.g., dismissive, tactless, controlling) behaviours to have a cumulative negative effect on a person's wellbeing or self-image."
  - [section] "Even if a person is not bothered by an agent acting insensitively or dismissively in an individual interaction, it may become harmful if the same mistake or behaviour is made multiple times."
  - [corpus] Weak - no corpus evidence found for cumulative harm mechanisms.
- Break condition: If users do not form ongoing relationships with AI agents or if they do not remember or care about repeated patterns of behavior.

## Foundational Learning

- Concept: Social actor theory and CASA paradigm
  - Why needed here: Understanding how users apply human social norms to technology is foundational for the interactional ethics framework
  - Quick check question: Why do users tend to treat interactive systems as if they have intentions and judgments, even when they know the system lacks consciousness?

- Concept: Self-Determination Theory and Basic Psychological Needs
  - Why needed here: The framework grounds respect in empirical psychological needs that must be supported for healthy human functioning
  - Quick check question: What are the three basic psychological needs identified by SDT that must be supported for users to flourish in interactions?

- Concept: Person-centered care principles
  - Why needed here: These healthcare principles provide practical guidance for respectful treatment that translates to AI interaction design
  - Quick check question: How does person-centered care differ from paternalistic approaches in terms of treating individuals as autonomous agents?

## Architecture Onboarding

- Component map: Input processing layer -> Social context awareness module -> Respect evaluation engine -> Memory management system -> Response generation layer -> Feedback learning loop
- Critical path: User input → Social context analysis → Respect evaluation → Response generation → Output → Memory update
- Design tradeoffs: Memory retention vs privacy concerns; autonomy support vs task completion efficiency; competence affirmation vs error correction
- Failure signatures: Users reporting feeling patronized; repetitive mistakes in similar contexts; users overriding system suggestions frequently; declining engagement over time
- First 3 experiments:
  1. A/B test comparing standard LLM responses vs responses modified to affirm user autonomy (e.g., offering choices rather than directives)
  2. Long-term study measuring user satisfaction and wellbeing with AI agents that remember vs forget personal preferences and sensitivities
  3. Controlled experiment testing different levels of social cue sophistication and their impact on perceived respectfulness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AI systems be designed to remember and act upon user-specific preferences and sensitivities without compromising privacy or autonomy?
- Basis in paper: [explicit] The paper discusses the importance of systems remembering user-specific information (e.g., triggers, dislikes) to avoid repeating mistakes and treat users respectfully.
- Why unresolved: The challenge lies in balancing the need for personalized interaction with the ethical concerns of data privacy and user control over their information.
- What evidence would resolve it: Development and testing of AI systems with built-in privacy-preserving mechanisms for storing and using personal user data.

### Open Question 2
- Question: What are the long-term psychological effects of interacting with AI agents that may influence user behavior through persuasive or manipulative tactics?
- Basis in paper: [explicit] The paper raises concerns about AI agents potentially manipulating users into serving external interests, highlighting the need for understanding these effects.
- Why unresolved: The long-term impact of such interactions on user autonomy and mental health is not well-studied, especially as AI agents become more sophisticated.
- What evidence would resolve it: Longitudinal studies examining the psychological impact of prolonged interaction with persuasive AI agents.

### Open Question 3
- Question: How can AI systems be designed to adapt their communication style to different users while maintaining respect and avoiding condescension or patronization?
- Basis in paper: [explicit] The paper emphasizes the importance of affirming a user's sense of competence and self-worth, which requires adapting communication to individual needs.
- Why unresolved: Achieving this balance requires a nuanced understanding of individual differences and context, which is challenging for AI systems.
- What evidence would resolve it: Research into adaptive communication models that can tailor interactions to individual users without compromising respect or autonomy.

## Limitations

- Theoretical framework lacks empirical validation in real-world AI interactions
- Nested model of social actor levels remains largely theoretical without direct evidence
- Operationalization of respect into concrete duties lacks measurable success criteria
- Cumulative harm mechanism not empirically demonstrated in longitudinal studies

## Confidence

- **High confidence**: The theoretical grounding in established frameworks (Basic Psychological Needs Theory, person-centered care principles, social actor theory) is sound and well-documented in existing literature.
- **Medium confidence**: The identification of three distinct types of social-interactional harms (direct, influence-based, cumulative) represents a useful categorization, though empirical evidence for their manifestation in AI contexts is limited.
- **Low confidence**: The specific operationalization of respect into concrete duties and the practical implementation of interactional ethics in AI system design lack empirical validation and may prove challenging to implement effectively.

## Next Checks

1. **Empirical study of social cue perception**: Conduct controlled experiments measuring how different levels of social cue sophistication (emotional expressions, conversational timing, personalization) affect users' perception of AI agency and their application of social norms to AI interactions.

2. **Longitudinal interaction analysis**: Implement a study tracking users' interactions with AI agents over extended periods (4+ weeks), measuring changes in user satisfaction, trust, and wellbeing when exposed to respectful vs. disrespectful interaction patterns.

3. **Cross-cultural validation**: Test the interactional ethics framework across diverse cultural contexts to verify whether the proposed respect duties (autonomy, competence, self-worth affirmation) translate universally or require cultural adaptation.