---
ver: rpa2
title: Deep Generative Modeling for Identification of Noisy, Non-Stationary Dynamical
  Systems
arxiv_id: '2410.02079'
source_url: https://arxiv.org/abs/2410.02079
tags:
- time
- sindy
- data
- dynamic
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces dynamic SINDy, a machine learning method
  for identifying governing equations of noisy, non-stationary dynamical systems.
  The method combines variational autoencoders with sparse identification of nonlinear
  dynamics (SINDy) to model time-varying coefficients of sparse ordinary differential
  equations (ODEs), enabling uncertainty quantification and discovery of latent variables.
---

# Deep Generative Modeling for Identification of Noisy, Non-Stationary Dynamical Systems

## Quick Facts
- arXiv ID: 2410.02079
- Source URL: https://arxiv.org/abs/2410.02079
- Authors: Doris Voina; Steven Brunton; J. Nathan Kutz
- Reference count: 40
- Primary result: Dynamic SINDy combines VAEs with SINDy to identify time-varying coefficients in non-stationary dynamical systems

## Executive Summary
This paper introduces dynamic SINDy, a machine learning method that combines variational autoencoders (VAEs) with sparse identification of nonlinear dynamics (SINDy) to identify governing equations of noisy, non-stationary dynamical systems. The method models time-varying coefficients as latent variables through a VAE framework, enabling uncertainty quantification and discovery of latent variables. Dynamic SINDy was validated on synthetic data including nonlinear oscillators and the Lorenz system, demonstrating accurate recovery of time-varying coefficients and robust performance under noise. The method was also applied to C. elegans neuronal activity data, successfully uncovering a global nonlinear switching model that captures key dynamical features without requiring manual labeling.

## Method Summary
Dynamic SINDy integrates VAEs with SINDy to model time-varying coefficients of sparse ordinary differential equations (ODEs) as latent variables. The VAE encodes observed time series into a distribution over latent variables representing ODE coefficients, which the decoder then reconstructs by combining these coefficients with a library of candidate functions. This allows handling systems where parameters change over time by capturing non-stationarity through latent variable time series. The method employs a two-stage training approach: first identifying sparsity patterns, then recovering coefficients. A loss function combines reconstruction error, KL divergence, sparsity penalty, and total variation penalty to train the model.

## Key Results
- Dynamic SINDy accurately recovered time-varying coefficients in synthetic non-autonomous harmonic oscillators with different signal types (sigmoid, switch, sinusoid, Fourier series)
- The method successfully discovered latent variables in the Lotka-Volterra system by inferring hidden predator population dynamics from prey-only observations
- Applied to C. elegans neuronal activity data, dynamic SINDy uncovered a global nonlinear switching model capturing key dynamical features without manual labeling
- Outperformed switching linear dynamical systems and group-sparse regression in handling complex, time-dependent systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic SINDy can identify time-varying coefficients in non-stationary dynamical systems by treating them as latent variables modeled through a VAE.
- Mechanism: The VAE encodes the observed time series into a distribution over latent variables, which represent the ODE coefficients. The decoder then reconstructs the system dynamics by combining these latent coefficients with a library of candidate functions. This allows the method to handle systems where parameters change over time, capturing the non-stationarity through the latent variable time series.
- Core assumption: The time-varying component of the system is separable from the main variables of interest (e.g., ẋ = f(x, t) = sin(t)x, not ẋ = f(x, t) = sin(tx)).
- Evidence anchors:
  - [abstract] "Our method, dynamic SINDy, combines variational inference with SINDy (sparse identification of nonlinear dynamics) to model time-varying coefficients of sparse ODEs."
  - [section] "Dynamic SINDy combines the interpretability of differential equations with the power of deep learning. It uses a deep generative model to uncover sparse governing equations directly from data, employing a variational autoencoder (VAE) to generate time series for differential equation coefficients."
  - [corpus] Weak: No direct mention of time-varying coefficients as latent variables in corpus papers.
- Break condition: If the system dynamics are not separable into a time-varying component and main variables, or if the VAE architecture cannot adequately model the complexity of the time-varying coefficients.

### Mechanism 2
- Claim: Dynamic SINDy can discover latent variables in incomplete datasets by inferring the dynamics of missing variables.
- Mechanism: By applying dynamic SINDy to the observed variables, the method infers a time series for the missing variables. This inferred time series is then used to reconstruct a complete autonomous system. For example, in the Lotka-Volterra system, dynamic SINDy is applied to the observed prey population, inferring the hidden predator population and reconstructing the full 2D system.
- Core assumption: The observed variables are sufficient to infer the dynamics of the missing variables through the coupling in the governing equations.
- Evidence anchors:
  - [section] "Dynamic SINDy is particularly useful for discovering hidden (latent) variables from incomplete datasets. We demonstrate this using a toy model from ecology: the Lotka-Volterra equations...We apply dynamic SINDy to x, using a library with just three terms: x, x², x³. As expected, ẋ is expressed solely in terms of x, with the x² and x³ terms vanishing. We derive a time series for the coefficient ˜y, where ẋ = x˜y(t). This inferred ˜y correlates with the hidden y..."
  - [corpus] Weak: No direct mention of latent variable discovery in corpus papers.
- Break condition: If the coupling between observed and missing variables is too weak or if the system is too complex for the inferred time series to accurately represent the missing variables.

### Mechanism 3
- Claim: Dynamic SINDy provides uncertainty quantification for the inferred ODE coefficients through the probabilistic nature of the VAE.
- Mechanism: The VAE encodes the input data into a distribution in the latent space, allowing the decoder to generate multiple samples of the ODE coefficients by sampling from this distribution. The variance of these samples provides an estimate of the uncertainty in the coefficients.
- Core assumption: The VAE is properly trained and the latent space is well-regularized to approximate a standard distribution.
- Evidence anchors:
  - [abstract] "This framework allows for uncertainty quantification of ODE coefficients, expanding on previous methods for autonomous systems."
  - [section] "We use VAEs to quantify uncertainty by estimating the standard deviation of the coefficients over time. Therefore we generate multiple trajectories by sampling z from a standard normal distribution during testing."
  - [corpus] Weak: No direct mention of uncertainty quantification in corpus papers.
- Break condition: If the VAE is not properly trained, or if the latent space is not well-regularized, leading to unreliable uncertainty estimates.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs are used to model the time-varying coefficients of the ODEs as latent variables, allowing for uncertainty quantification and handling of non-stationary systems.
  - Quick check question: What is the key difference between a standard autoencoder and a VAE, and why is this difference important for dynamic SINDy?

- Concept: Sparse Identification of Nonlinear Dynamics (SINDy)
  - Why needed here: SINDy is used to identify the governing equations of the system from the data by promoting sparsity in the coefficients, leading to interpretable and parsimonious models.
  - Quick check question: How does SINDy enforce sparsity in the identified coefficients, and why is this important for model interpretability?

- Concept: Time Series Analysis and Dynamic VAEs
  - Why needed here: Dynamic VAEs are used to handle the temporal dependencies in the data, allowing the model to capture the time-varying nature of the coefficients and the system dynamics.
  - Quick check question: What are the key challenges in generating time series data, and how do dynamic VAEs address these challenges?

## Architecture Onboarding

- Component map:
  - Encoder: Maps the observed time series to a distribution over latent variables (ODE coefficients)
  - Decoder: Samples from the latent distribution to generate time series of ODE coefficients
  - SINDy Library: A pre-defined set of candidate functions (e.g., polynomials, trigonometric functions) that are combined with the latent coefficients to reconstruct the system dynamics
  - Loss Function: Combines the reconstruction error, KL divergence, sparsity penalty, and total variation penalty to train the model

- Critical path:
  1. Encode the observed time series into a latent distribution
  2. Sample from the latent distribution to generate ODE coefficients
  3. Combine the coefficients with the SINDy library to reconstruct the system dynamics
  4. Compute the loss and backpropagate to update the model parameters

- Design tradeoffs:
  - VAE architecture: Different VAE architectures (e.g., timeVAE, dynamic HyperSINDy) have different strengths and weaknesses in terms of computational efficiency, memory usage, and ability to handle long time series
  - Regularization: The balance between sparsity and total variation regularization affects the model's ability to capture the underlying dynamics while avoiding overfitting
  - Hyperparameters: The choice of hyperparameters (e.g., latent dimension, learning rate) can significantly impact the model's performance and requires careful tuning

- Failure signatures:
  - Poor reconstruction of the system dynamics: This could indicate that the VAE is not properly capturing the latent variables or that the SINDy library is not rich enough to represent the system
  - Overfitting or underfitting: This could be due to improper regularization or insufficient training data
  - Unreliable uncertainty estimates: This could indicate that the VAE is not well-trained or that the latent space is not well-regularized

- First 3 experiments:
  1. Test the model on a simple non-autonomous harmonic oscillator with known time-varying coefficients to verify that it can accurately identify the coefficients and reconstruct the dynamics
  2. Test the model on a system with a known latent variable (e.g., Lotka-Volterra) to verify that it can discover the hidden variable and reconstruct the complete system
  3. Test the model on a noisy version of the harmonic oscillator to verify that it can handle noise and provide reliable uncertainty estimates

## Open Questions the Paper Calls Out

- Question: How can the hyperparameter sensitivity of dynamic SINDy be systematically addressed to improve robustness across different datasets and noise levels?
  - Basis in paper: [explicit] The paper acknowledges that the DVAE architecture has many hyperparameters to tune, and results may not be robust to these settings, especially in noisy datasets.
  - Why unresolved: While the paper suggests future research should explore different DVAE architectures and hyperparameter tuning approaches, it does not provide specific methods for systematic hyperparameter optimization.
  - What evidence would resolve it: Development and testing of automated hyperparameter optimization techniques (e.g., Bayesian optimization, meta-learning) that consistently improve dynamic SINDy performance across diverse datasets.

- Question: Can dynamic SINDy be extended to handle non-stationary systems where the time-varying component and the main variables of interest are not separable?
  - Basis in paper: [explicit] The paper assumes that the time-varying component and the main variables of interest are separable (e.g., ẋ = f(x, t) = sin(t)x, but not ẋ = f(x, t) = sin(tx)).
  - Why unresolved: The paper does not explore or provide methods for handling non-separable non-stationary systems, which are more complex and common in real-world applications.
  - What evidence would resolve it: Demonstration of dynamic SINDy's ability to accurately identify governing equations for non-separable non-stationary systems through synthetic or real-world examples.

- Question: How can the uncertainty quantification in dynamic SINDy be improved to better estimate the standard deviation of ODE coefficients, particularly for complex time-varying functions like Fourier series?
  - Basis in paper: [explicit] The paper shows that while dynamic SINDy can quantify uncertainty by estimating the standard deviation of coefficients over time, the estimated standard deviation aligns well with the ground truth for switch signal coefficients but is less clear for Fourier series coefficients.
  - Why unresolved: The paper acknowledges the need for further work to improve standard deviation estimation, considering the VAE architecture and hyperparameters, but does not provide specific solutions.
  - What evidence would resolve it: Development and validation of improved uncertainty quantification methods within the dynamic SINDy framework that accurately estimate standard deviations for various types of time-varying coefficients, including Fourier series.

## Limitations

- The method has significant hyperparameter sensitivity, particularly regarding regularization terms and latent space dimensionality, which may limit robustness across different datasets
- Computational complexity of the VAE-based approach may limit scalability to high-dimensional systems, though this was not thoroughly explored
- While the method shows promise on synthetic and biological data, its generalizability to other real-world systems remains uncertain

## Confidence

- Mathematical framework and synthetic system validation: **Medium-High**
- Latent variable discovery: **Medium**
- Biological data applications: **Medium**

## Next Checks

1. **Ablation Study on Regularization**: Systematically vary λsp and λtv parameters across orders of magnitude on synthetic systems to quantify their impact on coefficient recovery accuracy and model interpretability.

2. **Cross-Dataset Transferability**: Test dynamic SINDy on a completely different biological dataset (e.g., neuronal recordings from a different organism or sensor network data) to assess generalizability beyond C. elegans.

3. **Scalability Assessment**: Evaluate performance on high-dimensional systems (n > 10) by generating synthetic datasets with increasing dimensionality and measuring computation time, memory usage, and coefficient recovery accuracy.