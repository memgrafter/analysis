---
ver: rpa2
title: 'Crafting Customisable Characters with LLMs: A Persona-Driven Role-Playing
  Agent Framework'
arxiv_id: '2406.17962'
source_url: https://arxiv.org/abs/2406.17962
tags:
- character
- your
- characters
- boulder
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Customisable Conversation Agent framework
  for creating role-playing agents with freely customisable characters. The framework
  constructs the SimsConv dataset of 68 customisable characters and 13,971 multi-turn
  dialogues across 1,360 scenes, using structured character creation with pre-defined
  aspects (career, aspiration, traits, skills) and detailed personal/social profiles.
---

# Crafting Customisable Characters with LLMs: A Persona-Driven Role-Playing Agent Framework

## Quick Facts
- arXiv ID: 2406.17962
- Source URL: https://arxiv.org/abs/2406.17962
- Reference count: 40
- Primary result: SimsChat achieves 6.18 overall score on SimsConv dataset and 0.91 role consistency on WikiRoleEval

## Executive Summary
This paper introduces a framework for creating customizable role-playing agents with freely customizable characters. The framework constructs the SimsConv dataset of 68 characters and 13,971 multi-turn dialogues across 1,360 scenes, using structured character creation with pre-defined aspects (career, aspiration, traits, skills) and detailed personal/social profiles. Based on this dataset, SimsChat is developed as a freely customizable role-playing agent. Experimental results show SimsChat achieves superior performance on both SimsConv and WikiRoleEval datasets, with automatic evaluation scores of 6.18 overall (vs. 4.95 baseline) on SimsConv and 0.91 role consistency on WikiRoleEval.

## Method Summary
The framework constructs the SimsConv dataset using GPT-4 to generate 68 customisable characters from pre-defined aspects (career, aspiration, traits, skills), then expands through personal and social profiles. Characters interact within 1,360 scenes with specified emotions and conversation topics. The dataset includes 13,971 multi-turn dialogues with both dialogue and internal thinking content. SimsChat is then developed by fine-tuning LLaMA-3-8B-Instruct on the SimsConv dataset using AdamW optimizer for 5 epochs with learning rate warmup to 3e-5. The approach focuses on character-specific training to prevent hallucination and maintain consistency.

## Key Results
- SimsChat achieves 6.18 overall automatic evaluation score on SimsConv dataset versus 4.95 baseline
- Role consistency score of 0.91 on WikiRoleEval benchmark
- Human evaluation confirms results with 6.08 overall score on SimsConv
- Superior performance across all evaluation dimensions including character consistency, knowledge accuracy, question rejection, memorization, personality, hallucination, and stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pre-defined aspects framework (career, aspiration, traits, skills) enables structured character diversity while maintaining coherence
- Mechanism: By constraining character generation with these four core dimensions, GPT-4 can produce diverse yet consistent character profiles that capture essential personality elements without becoming incoherent
- Core assumption: Pre-defined aspects provide sufficient scaffolding for LLMs to generate realistic, varied characters without requiring free-form creation
- Evidence anchors:
  - [abstract]: "Characters are initially customised using pre-defined elements (career, aspiration, traits, skills), then expanded through personal and social profiles"
  - [section]: "Our framework draws inspiration from the life simulation video game The Sims, which has proven effective in role-playing agent development"
  - [corpus]: "Found 25 related papers... CharacterBench: Benchmarking Character Customization of Large Language Models" - shows this approach is novel in incorporating pre-defined elements

### Mechanism 2
- Claim: Scene construction with specified emotions and conversation topics enables controlled interaction generation
- Mechanism: By generating scenes with pre-defined emotional states and conversation topics, the framework ensures that character interactions reflect intended experiences rather than random dialogue
- Core assumption: Emotions and conversation topics provide sufficient control over dialogue generation to simulate authentic life experiences
- Evidence anchors:
  - [abstract]: "Characters then interact within these scenes, with specified emotions and conversation topics enhancing control over interactions"
  - [section]: "To enhance control over character experiences, rather than allowing random dialogue generation, we specify both emotions and conversation topics"
  - [corpus]: "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent" - shows multi-character interaction is challenging without such controls

### Mechanism 3
- Claim: Training SimsChat on character-specific experiences prevents character hallucination and improves consistency
- Mechanism: By fine-tuning on data where each character only sees their own experiences, the model learns to maintain character boundaries and avoid mixing knowledge across personas
- Core assumption: Character-specific training data enables the model to learn distinct personality patterns and knowledge boundaries
- Evidence anchors:
  - [abstract]: "Experimental results on both SimsConv and WikiRoleEval datasets demonstrate SimsChat's superior performance in maintaining character consistency, knowledge accuracy, and appropriate question rejection"
  - [section]: "To prevent character hallucination, each character's training process utilises only their specific experiences from the dataset"
  - [corpus]: "CharacterBox: Evaluating the Role-Playing Capabilities of LLMs in Text-Based Virtual Worlds" - indicates character consistency is a key evaluation dimension

## Foundational Learning

- Concept: Multi-turn dialogue generation with internal thoughts
  - Why needed here: Enables realistic character interactions by capturing both spoken words and internal reflections, essential for immersive role-playing
  - Quick check question: Why does the framework require both dialogue and thinking content in each turn?

- Concept: Pre-defined aspect selection from controlled vocabularies
  - Why needed here: Ensures character diversity while preventing random or incoherent combinations that would break immersion
  - Quick check question: How do the 26 careers, 10 aspirations, 39 traits, and 41 skills contribute to character variety?

- Concept: Human verification in data generation pipeline
  - Why needed here: Mitigates potential biases in LLM-generated content and ensures alignment with predefined parameters
  - Quick check question: What percentage of generated profiles underwent manual review, and what was the acceptance rate?

## Architecture Onboarding

- Component map: Character Construction -> Scene Construction -> Dialogue Generation -> Fine-tuning -> Evaluation
- Critical path: Character Construction → Scene Construction → Dialogue Generation → Fine-tuning → Evaluation
- Design tradeoffs:
  - Pre-defined aspects vs. free-form creation (structure vs. flexibility)
  - GPT-4 generation vs. human writing (scale vs. authenticity)
  - Character-specific training vs. shared knowledge (consistency vs. generalization)
- Failure signatures:
  - Low memorisation scores indicate insufficient character-specific training
  - Poor personality scores suggest inadequate pre-defined aspect coverage
  - High hallucination scores mean characters are mixing knowledge inappropriately
- First 3 experiments:
  1. Train with simplified character profiles (remove personal/social aspects) to measure impact on performance
  2. Remove emotion and topic control to test importance of interaction constraints
  3. Train without scene construction to validate importance of realistic scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SimsChat compare to fine-tuned models with similar parameter counts when evaluated on characters outside the WikiRoleEval benchmark?
- Basis in paper: [inferred] The paper mentions that SimsChat outperforms existing models on WikiRoleEval but does not provide comparisons with other fine-tuned models on different benchmarks.
- Why unresolved: The evaluation is limited to WikiRoleEval, and there is no mention of testing on other character datasets or benchmarks.
- What evidence would resolve it: Additional experiments comparing SimsChat to other fine-tuned models on diverse character datasets or benchmarks.

### Open Question 2
- Question: What is the impact of increasing the number of pre-defined aspects (career, aspiration, traits, skills) on the diversity and quality of generated characters?
- Basis in paper: [explicit] The paper discusses the use of pre-defined aspects but does not explore the effects of expanding these categories.
- Why unresolved: The paper uses a fixed set of pre-defined aspects without testing variations in their number or types.
- What evidence would resolve it: Experiments varying the number and types of pre-defined aspects and measuring the resulting character diversity and quality.

### Open Question 3
- Question: How does the inclusion of emotion and topic control affect the model's ability to generalize to unseen characters or scenarios?
- Basis in paper: [explicit] The ablation study shows the importance of emotion and topic control, but its effect on generalization is not explicitly tested.
- Why unresolved: The paper focuses on in-domain performance rather than out-of-domain generalization.
- What evidence would resolve it: Testing the model on unseen characters or scenarios with and without emotion and topic control to measure generalization performance.

### Open Question 4
- Question: What is the long-term stability of character consistency when SimsChat is used for extended interactions over multiple sessions?
- Basis in paper: [inferred] The paper evaluates stability within single interactions but does not address multi-session or long-term interactions.
- Why unresolved: The evaluation is limited to single-session interactions without considering prolonged usage.
- What evidence would resolve it: Longitudinal studies tracking character consistency over multiple sessions or extended interaction periods.

## Limitations

- The framework's performance on characters with complex, conflicting traits remains untested
- The evaluation methodology primarily relies on automatic metrics that may not fully capture nuanced role-playing quality
- The framework's scalability to hundreds or thousands of characters without performance degradation is unproven

## Confidence

**High Confidence Claims:**
- The framework successfully constructs a diverse dataset of customisable characters with pre-defined aspects
- SimsChat achieves superior performance on both SimsConv and WikiRoleEval datasets compared to baseline models
- The character-specific training approach effectively prevents character hallucination and improves consistency

**Medium Confidence Claims:**
- The pre-defined aspects framework (career, aspiration, traits, skills) provides optimal balance between structure and diversity
- Scene construction with specified emotions and conversation topics significantly improves interaction quality
- The framework's design tradeoffs (structure vs. flexibility, scale vs. authenticity) represent the best approach for role-playing agents

**Low Confidence Claims:**
- The framework would scale effectively to hundreds or thousands of characters without significant performance degradation
- The current evaluation metrics fully capture all aspects of role-playing quality and character immersion
- The framework's performance would remain consistent across different LLM architectures and sizes

## Next Checks

1. **Edge Case Character Generation**: Test the framework's ability to generate and maintain consistency for characters with highly complex, conflicting traits (e.g., ambitious but lazy, extroverted but shy) to evaluate the robustness of the pre-defined aspects system.

2. **Long-term Interaction Stability**: Conduct extended multi-turn conversations (100+ turns) with the same character to assess whether the character maintains consistency and avoids knowledge mixing over prolonged interactions, which current evaluations may not capture.

3. **Cross-model Generalization**: Fine-tune the same SimsConv dataset on different LLM architectures (e.g., Mistral, Claude) to determine whether the framework's success is dependent on LLaMA-3 or represents a more general approach to customisable role-playing agents.