---
ver: rpa2
title: Training on the Test Task Confounds Evaluation and Emergence
arxiv_id: '2407.07890'
source_url: https://arxiv.org/abs/2407.07890
tags:
- test
- task
- training
- data
- mmlu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that training on the test task (using knowledge
  about evaluation tasks at training time) confounds both relative model evaluations
  and claims about emergent capabilities in large language models. The authors show
  that newer models trained after November 2023 outperform older models by 7-19 percentage
  points on MMLU and GSM8K benchmarks, but this difference disappears when all models
  are fine-tuned on the same task-relevant data before evaluation.
---

# Training on the Test Task Confounds Evaluation and Emergence

## Quick Facts
- arXiv ID: 2407.07890
- Source URL: https://arxiv.org/abs/2407.07890
- Authors: Ricardo Dominguez-Olmedo; Florian E. Dorner; Moritz Hardt
- Reference count: 40
- Primary result: Training on the test task is a more dominant factor in benchmark performance than data contamination, making emergence claims unreliable.

## Executive Summary
This paper reveals that training on the test task—using knowledge about evaluation tasks at training time—profoundly confounds both relative model evaluations and claims about emergent capabilities in large language models. The authors demonstrate that newer models trained after November 2023 systematically outperform older models on MMLU and GSM8K benchmarks, but this performance gap disappears when all models are fine-tuned on the same task-relevant data before evaluation. They propose adjusting for training on the test task by fine-tuning each model on sufficient task-specific data prior to evaluation, which equalizes performance across model families and restores cleaner log-linear scaling relationships. The study shows that emergence disappears as models train more on the test task, making capabilities predictable at smaller scales rather than genuinely emergent.

## Method Summary
The study evaluates 56 base language models ranging from 70M to 70B parameters from the HuggingFace Open LLM Leaderboard v1. Models are split into two groups based on training date: pre-November 2023 and post-November 2023. Both groups are evaluated on MMLU and GSM8K using LM Evaluation Harness (5-shot for both). All models are then fine-tuned on the same task-relevant datasets—MMLU auxiliary training set (100K examples) and combined MetaMathQA/Orca-Math (600K examples)—for 3 epochs. The performance difference between newer and older models is measured before and after fine-tuning to quantify the effect of training on the test task. The study also analyzes scaling relationships and emergence points across model families.

## Key Results
- Newer models (post-Nov 2023) outperform older models by 7-19 percentage points on MMLU and GSM8K benchmarks when evaluated directly.
- After fine-tuning all models on the same task-relevant data, performance differences between newer and older models disappear, equalizing across model families.
- Training on the test task is a more dominant factor in benchmark performance than data contamination, confounding claims about emergent capabilities.
- After adjusting for test task training, log-linear scaling between pretraining compute and benchmark performance is restored, making capabilities predictable at smaller scales.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on the test task can systematically elevate benchmark scores without increasing pretraining compute.
- Mechanism: Models acquire task-specific capabilities through implicit or explicit exposure to evaluation task formats during pretraining, so their performance improves beyond what pretraining compute scaling alone would predict.
- Core assumption: The evaluation task format (e.g., multiple-choice question answering) can be approximated by pretraining data, and models learn generalizable strategies from such exposure.
- Evidence anchors:
  - [abstract]: "We group strategies to utilize task knowledge at training time under the umbrella term of training on the test task."
  - [section]: "We observe that older models tend to benefit much more from fine-tuning on task-relevant data compared to newer models."
  - [corpus]: Weak. Related work discusses emergent abilities but not task-specific training effects.
- Break condition: If the pretraining data does not resemble the evaluation task format, or if evaluation tasks are highly novel and cannot be anticipated in pretraining data.

### Mechanism 2
- Claim: Adjusting for training on the test task restores log-linear scaling between pretraining compute and benchmark performance.
- Mechanism: Fine-tuning all models on the same, sufficient amount of task-relevant data before evaluation levels the playing field by normalizing differences in test task exposure, revealing the true scaling relationship.
- Core assumption: Sufficient task-relevant fine-tuning can compensate for differences in pretraining test task exposure, making performance differences due to pretraining compute alone.
- Evidence anchors:
  - [abstract]: "We propose to mitigate the effects of training on the test task on benchmark evaluations by doing more of it."
  - [section]: "After fine-tuning on the same task data, both newer and older models follow remarkably similar scaling trends."
  - [corpus]: Weak. Related work focuses on emergent abilities and evaluation metrics, not on fine-tuning as an adjustment mechanism.
- Break condition: If task-relevant fine-tuning data is unavailable or insufficient to saturate the test task training effect.

### Mechanism 3
- Claim: Training on the test task distorts model family comparisons and rankings by creating artificial performance advantages.
- Mechanism: Different model families have varying degrees of test task training in their pretraining data, leading to misleading comparisons of absolute performance and rankings.
- Core assumption: The extent of test task training in pretraining data varies systematically across model families.
- Evidence anchors:
  - [abstract]: "We show that training on the test task has profound implications for the study of emergent capabilities."
  - [section]: "After adjusting for test task training, none of the model families appears to be superior beyond their pretraining compute."
  - [corpus]: Weak. Related work discusses data contamination but not family-specific test task training differences.
- Break condition: If model families have similar test task training exposure or if other factors (e.g., architectural differences) dominate performance.

## Foundational Learning

- Concept: Causal inference and adjustment methods
  - Why needed here: To quantify the effect of training on the test task and adjust for it, we need to understand how to intervene on a variable (test task training) and measure its causal impact on benchmark performance.
  - Quick check question: If we fine-tune all models on the same task data, what should happen to the performance difference between newer and older models if test task training is the main confounder?
- Concept: Scaling laws and log-linear relationships
  - Why needed here: Understanding how model performance scales with pretraining compute is crucial for detecting deviations caused by test task training and for validating the adjustment method.
  - Quick check question: What is the expected relationship between pretraining compute and benchmark accuracy if test task training is not a factor?
- Concept: Data contamination vs. task format exposure
  - Why needed here: Distinguishing between memorization of test data (contamination) and learning task formats (training on the test task) is essential for correctly interpreting the results and designing appropriate adjustments.
  - Quick check question: How can we tell if a model's performance gain is due to memorizing test data versus learning the task format?

## Architecture Onboarding

- Component map: Model selection -> Pretraining compute estimation -> Fine-tuning on task-relevant data -> Benchmark evaluation -> Analysis of scaling and emergence
- Critical path: Select models → Estimate pretraining compute → Fine-tune on task-relevant data → Evaluate on benchmarks → Analyze results (compare pre- and post-adjustment)
- Design tradeoffs: Using more task-relevant data for fine-tuning improves adjustment but increases computational cost. Choosing between cloze and multiple-choice evaluations affects the detection of test task training effects.
- Failure signatures: If fine-tuning does not reduce the performance gap between newer and older models, it may indicate insufficient task-relevant data or that other factors dominate the performance difference.
- First 3 experiments:
  1. Fine-tune a small set of older models on task-relevant data and compare their performance to newer models before and after fine-tuning.
  2. Reformulate a benchmark task (e.g., ARC) as multiple-choice and evaluate models before and after fine-tuning to observe the effect of test task training.
  3. Vary the amount of task-relevant data used for fine-tuning and measure the impact on the performance gap between newer and older models to find the saturation point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which pretraining data quality leads to improved downstream benchmark performance, beyond the inclusion of test task data?
- Basis in paper: [explicit] The paper suggests that superior "quality" pretraining datasets may contain more test task data, implying that data quality and test task training are confounded.
- Why unresolved: The paper doesn't isolate the effect of genuine data quality improvements (e.g., better representation, diversity) from the effect of test task data inclusion. It's unclear how much of the performance gain attributed to data quality is actually due to test task training.
- What evidence would resolve it: Controlled experiments comparing models trained on datasets with identical test task data but varying in other quality metrics (e.g., diversity, representation) would isolate the effect of data quality. Additionally, analyzing the pretraining data distributions of high-performing models to quantify the proportion of test task data versus other high-quality data would be informative.

### Open Question 2
- Question: How does the effectiveness of fine-tuning as an adjustment method vary across different task types and benchmark characteristics?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of fine-tuning on task-relevant data for MMLU, GSM8K, ARC, and HellaSwag, but acknowledges that the method's effectiveness may depend on the availability and relevance of task data.
- Why unresolved: The paper doesn't systematically explore how the adjustment method performs for a wider range of task types (e.g., open-ended generation, code generation) or benchmark characteristics (e.g., size, difficulty, format). The generalizability of the method across diverse evaluation scenarios remains unclear.
- What evidence would resolve it: Extensive experiments applying the fine-tuning adjustment to a broad spectrum of benchmarks with varying characteristics would reveal the method's strengths and limitations. Analyzing the relationship between task type, benchmark characteristics, and the effectiveness of fine-tuning would provide insights into when and how to best apply the adjustment.

### Open Question 3
- Question: What are the long-term implications of training on the test task for the development and evaluation of large language models?
- Basis in paper: [inferred] The paper highlights the profound implications of training on the test task for model comparisons, claims of emergence, and the overall evaluation ecosystem. It suggests a need for a major reorientation of evaluation practices.
- Why unresolved: The paper doesn't explore the broader consequences of widespread test task training, such as its impact on model generalization, robustness, and the reliability of benchmark evaluations as indicators of real-world performance. The potential for a "race to the bottom" where models are optimized for benchmark performance rather than genuine capabilities is also not fully addressed.
- What evidence would resolve it: Longitudinal studies tracking the performance of models trained with and without extensive test task training on a variety of benchmarks and real-world tasks would reveal the long-term effects on model capabilities. Investigating the relationship between test task training and model robustness to adversarial examples or out-of-distribution data would also be valuable. Additionally, exploring alternative evaluation paradigms that are less susceptible to test task training could provide insights into more reliable assessment methods.

## Limitations

- Incomplete visibility into pretraining data composition for newer models makes it difficult to quantify exact test task exposure.
- The temporal split (pre- vs post-November 2023) assumes meaningful differences in test task training, but models may have been trained in parallel or release dates may not reflect pretraining completion.
- The adjustment method assumes sufficient task-relevant fine-tuning can fully compensate for test task training differences, but the saturation point remains unknown.

## Confidence

- High confidence: The observation that newer models outperform older models on MMLU and GSM8K by 7-19 percentage points when evaluated directly.
- Medium confidence: The claim that this performance gap is primarily due to training on the test task rather than pretraining compute scaling.
- Medium confidence: The assertion that emergence disappears as models train more on the test task.

## Next Checks

1. **Temporal Robustness Check**: Repeat the analysis using different temporal cutoffs (e.g., June 2023 vs. November 2023) to verify that the observed effects are robust to the chosen split point and not artifacts of the specific date selection.

2. **Fine-tuning Saturation Analysis**: Systematically vary the amount of task-relevant data and fine-tuning epochs to identify the point at which performance differences between newer and older models stabilize, establishing whether the adjustment truly saturates or if diminishing returns continue.

3. **Cross-benchmark Validation**: Apply the same methodology to additional benchmarks beyond MMLU and GSM8K (such as Big-Bench or HELM tasks) to determine whether the effects are specific to certain task formats or represent a general phenomenon across diverse evaluation settings.