---
ver: rpa2
title: 'Asterisk*: Keep it Simple'
arxiv_id: '2411.05691'
source_url: https://arxiv.org/abs/2411.05691
tags:
- asterisk
- embeddings
- performance
- classification
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Asterisk, a compact GPT-based model for generating
  text embeddings using a minimalist architecture with 2 layers, 2 attention heads,
  and 256 embedding dimensions. The model employs knowledge distillation from OpenAI's
  text-embedding-3-small model, achieving a cosine similarity of 0.65 with teacher
  embeddings during training.
---

# Asterisk*: Keep it Simple

## Quick Facts
- arXiv ID: 2411.05691
- Source URL: https://arxiv.org/abs/2411.05691
- Authors: Andrew Semenov
- Reference count: 6
- Key outcome: A compact GPT-based model for generating text embeddings with minimalist architecture achieves top rankings on MTEB Leaderboard while maintaining computational efficiency.

## Executive Summary
This paper presents Asterisk*, a compact GPT-based model for generating text embeddings using a minimalist architecture with 2 layers, 2 attention heads, and 256 embedding dimensions. The model employs knowledge distillation from OpenAI's text-embedding-3-small model, achieving a cosine similarity of 0.65 with teacher embeddings during training. While raw embeddings showed moderate performance in zero-shot classification (e.g., 46% on MassiveIntentClassification), adding a fully-connected network significantly improved results, achieving 94% on the same task. The model ranks 1st on MassiveIntentClassification and 2nd on AmazonReviewsClassification in the MTEB Leaderboard. The study demonstrates that simple architectures with task-specific components can outperform larger models while maintaining computational efficiency.

## Method Summary
The Asterisk* model utilizes a minimalist GPT architecture with only 2 layers, 2 attention heads, and 256 embedding dimensions. The model is trained using knowledge distillation from OpenAI's text-embedding-3-small model, with the distillation process optimizing for cosine similarity between the student and teacher embeddings. The training process involves fine-tuning the compact model to mimic the behavior of the larger teacher model, achieving a cosine similarity of 0.65 during training. The model is evaluated on the Massive Text Embedding Benchmark (MTEB) Leaderboard, demonstrating strong performance in both zero-shot classification and with added task-specific components.

## Key Results
- Asterisk* ranks 1st on MassiveIntentClassification and 2nd on AmazonReviewsClassification in the MTEB Leaderboard
- Raw embeddings achieved 46% on MassiveIntentClassification, while adding a fully-connected network improved results to 94%
- The model maintains high cosine similarity (0.65) with teacher embeddings during knowledge distillation training

## Why This Works (Mechanism)
The success of Asterisk* stems from the effective use of knowledge distillation to transfer the capabilities of a larger model to a compact architecture. By training a minimalist GPT model to mimic the behavior of OpenAI's text-embedding-3-small, the approach leverages the strengths of larger models while maintaining computational efficiency. The significant performance improvement when adding a fully-connected network suggests that the compact model captures essential semantic information, which can be further refined for specific tasks. This demonstrates that even simple architectures can achieve strong results when combined with appropriate task-specific components and effective knowledge transfer techniques.

## Foundational Learning
1. Knowledge Distillation
   - Why needed: To transfer the capabilities of a larger, more complex model to a compact architecture
   - Quick check: Evaluate the cosine similarity between student and teacher embeddings during training

2. Text Embeddings
   - Why needed: To represent textual information in a numerical format suitable for machine learning tasks
   - Quick check: Assess the model's performance on various text classification benchmarks

3. Zero-shot Learning
   - Why needed: To evaluate the model's ability to perform tasks without task-specific training
   - Quick check: Compare the model's performance on zero-shot classification tasks with and without additional task-specific components

4. MTEB Leaderboard
   - Why needed: To provide a standardized benchmark for evaluating text embedding models across diverse tasks
   - Quick check: Verify the model's rankings on the MTEB Leaderboard for various benchmarks

## Architecture Onboarding

Component Map:
GPT-based encoder -> Knowledge Distillation -> Text Embeddings -> Task-specific Components (optional) -> Downstream Tasks

Critical Path:
The critical path for achieving high performance with Asterisk* involves the knowledge distillation process from the teacher model, followed by the generation of text embeddings using the compact GPT architecture. The optional addition of task-specific components, such as a fully-connected network, can further enhance performance on specific tasks.

Design Tradeoffs:
- Compact architecture (2 layers, 2 attention heads, 256 embedding dimensions) vs. larger models: Reduced computational requirements but potential limitations in capturing complex semantic relationships
- Knowledge distillation from a larger model: Leverages the strengths of larger models but may introduce biases or limitations from the teacher model
- Addition of task-specific components: Improves performance on specific tasks but increases model complexity and computational requirements

Failure Signatures:
- Poor performance on tasks requiring deep understanding of complex semantic relationships
- Limited generalization across diverse tasks and domains
- Potential biases or limitations inherited from the teacher model during knowledge distillation

First Experiments:
1. Evaluate the model's performance on a diverse set of text classification tasks to assess its generalizability and robustness
2. Conduct an ablation study to quantify the individual contributions of the compact architecture, knowledge distillation, and task-specific components to overall performance
3. Investigate the long-term stability and effectiveness of the knowledge distillation process by evaluating the model's performance over extended periods and in different deployment scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- The knowledge distillation process from OpenAI's model is central to the approach, but the specifics of the methodology and potential biases are not fully detailed
- The significant performance improvement when adding a fully-connected network raises questions about the true capabilities of the compact model versus the added task-specific components
- The model's performance across diverse tasks beyond the reported benchmarks needs validation to assess its generalizability

## Confidence
- High confidence: The model's compact architecture with 2 layers, 2 attention heads, and 256 embedding dimensions is accurately described. The reported rankings on the MTEB Leaderboard for MassiveIntentClassification and AmazonReviewsClassification are factual statements based on the benchmark results.
- Medium confidence: The claim that the model achieves high cosine similarity (0.65) with teacher embeddings during training is based on the reported results but may be influenced by the specific training setup and evaluation methodology. The assertion that the model outperforms larger models in certain tasks is supported by the benchmark results but may not generalize across all scenarios.
- Low confidence: The generalizability of the model's performance across diverse tasks beyond the reported benchmarks is uncertain. The long-term stability and effectiveness of the knowledge distillation process from OpenAI's model in various real-world applications are not fully established.

## Next Checks
1. Conduct a comprehensive evaluation of the model's performance across a wider range of tasks and domains to assess its generalizability and robustness.
2. Perform an ablation study to quantify the individual contributions of the compact architecture, knowledge distillation, and the fully-connected network to the overall performance, and to understand the trade-offs involved.
3. Investigate the long-term stability and effectiveness of the knowledge distillation process by evaluating the model's performance over extended periods and in different deployment scenarios, and compare it with other state-of-the-art embedding models.