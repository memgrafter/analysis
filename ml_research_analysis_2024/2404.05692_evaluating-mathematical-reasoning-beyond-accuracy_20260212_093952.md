---
ver: rpa2
title: Evaluating Mathematical Reasoning Beyond Accuracy
arxiv_id: '2404.05692'
source_url: https://arxiv.org/abs/2404.05692
tags:
- reasoning
- steps
- reason
- step
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating the quality of mathematical
  reasoning in large language models beyond simple final-answer accuracy. The core
  method, REASON EVAL, introduces a step-by-step evaluation framework that assesses
  reasoning steps for validity (correctness) and redundancy (efficiency) using trained
  LLM evaluators.
---

# Evaluating Mathematical Reasoning Beyond Accuracy

## Quick Facts
- arXiv ID: 2404.05692
- Source URL: https://arxiv.org/abs/2404.05692
- Reference count: 23
- This paper introduces REASON EVAL, a step-by-step evaluation framework that assesses mathematical reasoning quality beyond final-answer accuracy by examining the validity and redundancy of individual reasoning steps.

## Executive Summary
This paper addresses the critical gap in evaluating mathematical reasoning in large language models by introducing REASON EVAL, a comprehensive framework that goes beyond simple final-answer accuracy. The method evaluates each reasoning step for validity (correctness) and redundancy (efficiency) using trained LLM evaluators, providing a more nuanced assessment of mathematical reasoning quality. Through extensive meta-evaluation, the paper demonstrates that REASON EVAL consistently outperforms baseline methods and can effectively select high-quality training data to improve model reasoning performance. The work shows that improved final-answer accuracy does not guarantee better reasoning quality, establishing a new standard for mathematical reasoning evaluation.

## Method Summary
REASON EVAL implements a step-by-step evaluation framework that classifies each reasoning step as valid/invalid and redundant/non-redundant using fine-tuned LLMs. The method uses PRM800K dataset for training, with a classification task involving three classes (positive, neutral, negative) for each step. The evaluation pipeline replaces the next-token prediction layer with a linear layer and softmax output, then aggregates individual step scores using min/max functions to produce final validity and redundancy scores. The approach is validated through meta-evaluation on MR-MATH and MR-GSM8K datasets, comparing performance against baseline methods like ROSCOE, GPT-3.5-turbo, GPT-4, and Math-shepherd-Mistral-7B.

## Key Results
- REASON EVAL outperforms baseline methods in detecting reasoning errors, with significantly higher AUC scores for identifying invalid steps
- An increase in final-answer accuracy does not necessarily improve reasoning quality for challenging mathematical problems
- Combining validity (below 0.5) and redundancy (above 0.15) thresholds enables selecting high-quality training data while reducing data requirements by 50%
- Larger base models show better performance, with LLaMA70B-PRM800K achieving 21.8% false positive rate compared to 40.6% for LLaMA13B-PRM800K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-level evaluation identifies reasoning errors missed by final-answer accuracy
- Mechanism: By evaluating each reasoning step individually, the system detects logical errors and inefficient steps hidden when only final answers are checked
- Core assumption: Correct final answers don't guarantee correct intermediate reasoning steps
- Evidence anchors: Abstract states increase in final-answer accuracy doesn't guarantee reasoning quality improvement; section argues for step-level evaluation alongside final results

### Mechanism 2
- Claim: Strong base models and high-quality training data lead to superior evaluation performance
- Mechanism: Evaluator performance depends on mathematical reasoning capabilities of base LLM and quality of fine-tuning data
- Core assumption: Mathematical reasoning ability in base models transfers to evaluation capability when fine-tuned
- Evidence anchors: Abstract shows REASON EVAL outperforms baselines with strong mathematical knowledge; section shows positive correlation between base model ability and AUC scores

### Mechanism 3
- Claim: REASON EVAL generalizes across different mathematical problem types
- Mechanism: The evaluator handles out-of-distribution problems without requiring specific training data for those types
- Core assumption: Evaluation methodology captures general reasoning principles applicable across mathematical domains
- Evidence anchors: Abstract highlights strong generalization capabilities; section shows REASON EVAL is more robust to question types compared to other methods

## Foundational Learning

- Concept: Step-by-step reasoning decomposition
  - Why needed here: REASON EVAL evaluates each reasoning step individually, so understanding how to properly decompose solutions into meaningful steps is crucial
  - Quick check question: Can you identify where one reasoning step ends and another begins in a complex mathematical solution?

- Concept: Validity vs. redundancy distinction
  - Why needed here: The evaluation system distinguishes between incorrect steps (invalid) and correct but unnecessary steps (redundant)
  - Quick check question: Given a correct but unnecessary step in a solution, can you classify it as redundant rather than invalid?

- Concept: Meta-evaluation methodology
  - Why needed here: The paper uses meta-evaluation to assess the quality of the evaluation method itself
  - Quick check question: How would you design a dataset to test whether an evaluator can detect reasoning errors versus just checking final answers?

## Architecture Onboarding

- Component map: Problem → Solution steps → Individual step classification → Aggregation (min/max) → Final validity/redundancy scores
- Critical path: Problem → Solution steps → Individual step classification → Aggregation (min/max) → Final validity/redundancy scores
- Design tradeoffs:
  - Step granularity vs. evaluation accuracy: Finer steps provide more detailed evaluation but may introduce noise
  - Model size vs. performance: Larger models generally perform better but increase computational cost
  - Training data quality vs. quantity: High-quality labeled data is more important than large amounts of noisy data
- Failure signatures:
  - Low AUC scores on meta-evaluation datasets
  - Inconsistent step-level classifications
  - Poor performance on out-of-distribution problem types
  - High false positive rates when evaluating model solutions
- First 3 experiments:
  1. Train REASON EVAL on PRM800K with different base models (Llama2-7B, Mistral-7B) and compare performance on MR-MATH-invalid
  2. Test generalization by evaluating solutions to reversed reasoning problems from MR-GSM8K
  3. Evaluate the false positive rate of different Abel and WizardMath models using the trained evaluator

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the noise in Math-shepherd's step-level labels be reduced while maintaining its 6x larger training dataset size?
- Basis in paper: The paper mentions that Math-shepherd has more noise in its step-level labels compared to PRM800K, which negatively impacts its precision in identifying errors
- Why unresolved: The paper identifies this as a limitation but doesn't provide solutions for reducing noise while preserving the dataset size advantage
- What evidence would resolve it: A follow-up study demonstrating a method to filter or clean Math-shepherd's labels while maintaining or improving its performance metrics

### Open Question 2
- Question: What is the relationship between model scale and reasoning quality improvements beyond what was tested in this study?
- Basis in paper: The paper shows that model scale significantly affects false positive rates, with LLaMA70B-PRM800K achieving 21.8% FPR compared to 40.6% for LLaMA13B-PRM800K
- Why unresolved: The study tested up to 34B parameters, but the relationship between scale and reasoning quality may continue beyond this range
- What evidence would resolve it: Testing models with 70B+ parameters and analyzing whether the correlation between scale and reasoning quality continues to follow the same pattern

### Open Question 3
- Question: Can the data synthesis method used in Math-shepherd be optimized to reduce noise while preserving its efficiency advantage?
- Basis in paper: The paper notes that Math-shepherd uses an unsupervised method for generating labels and mentions this approach is "promising" despite its noise issues
- Why unresolved: The paper identifies this as a future work direction but doesn't explore how to optimize the synthesis method
- What evidence would resolve it: A study demonstrating an improved data synthesis pipeline that maintains Math-shepherd's efficiency while achieving PRM800K-level label quality

## Limitations
- Performance heavily depends on quality of base LLM and training data, creating dependency chains where errors propagate through multiple layers
- Methodology assumes reasoning steps can be cleanly decomposed, which may not hold for all mathematical problems requiring holistic or non-linear reasoning
- Generalization claims are primarily supported by limited out-of-distribution testing, with false positive rate evaluation not fully capturing real-world usage scenarios

## Confidence
- High Confidence: Core finding that final-answer accuracy doesn't guarantee reasoning quality is well-supported; meta-evaluation methodology and baseline comparisons are rigorously demonstrated
- Medium Confidence: Generalization claims and data selection effectiveness have reasonable support but need broader testing across more diverse problem types and larger model scales
- Low Confidence: Assumption that step-level evaluation fully captures reasoning quality may be oversimplified; doesn't adequately address interdependent steps or alternative valid solution paths

## Next Checks
1. **Cross-domain generalization test**: Evaluate REASON EVAL on mathematical reasoning problems from entirely different domains (e.g., proof-based mathematics, applied engineering problems) to verify claimed strong generalization capabilities beyond GSM8K and MATH datasets

2. **Ablation study on training data quality**: Systematically vary the quality and quantity of training data for REASON EVAL to quantify impact on evaluation performance, particularly focusing on relationship between training data noise and false positive rates

3. **Human evaluation benchmark**: Conduct comprehensive human evaluation study comparing REASON EVAL's step-level classifications against expert annotators on diverse mathematical problems, measuring agreement rates and identifying systematic biases or blind spots in automated evaluation