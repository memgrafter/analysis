---
ver: rpa2
title: Counterfactual explainability and analysis of variance
arxiv_id: '2411.01625'
source_url: https://arxiv.org/abs/2411.01625
tags:
- explainability
- counterfactual
- education
- causal
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces counterfactual explainability, a causal approach
  to attributing the variability of an outcome variable to a set of explanatory factors.
  Unlike traditional methods such as Sobol's indices or Shapley values, which assume
  independent inputs and do not provide mechanistic insight, counterfactual explainability
  leverages a directed acyclic graph (DAG) to model causal relationships among variables
  and incorporates their basic potential outcomes.
---

# Counterfactual explainability and analysis of variance

## Quick Facts
- **arXiv ID**: 2411.01625
- **Source URL**: https://arxiv.org/abs/2411.01625
- **Reference count**: 40
- **Primary result**: Introduces counterfactual explainability as a causal approach to attributing outcome variance to explanatory factors using DAGs and potential outcomes

## Executive Summary
This paper introduces counterfactual explainability, a causal approach to attributing the variability of an outcome variable to a set of explanatory factors. Unlike traditional methods such as Sobol's indices or Shapley values, which assume independent inputs and do not provide mechanistic insight, counterfactual explainability leverages a directed acyclic graph (DAG) to model causal relationships among variables and incorporates their basic potential outcomes. The authors define a probability measure over the explanation algebra that quantifies how much of the outcome's variance is attributable to different combinations of factors.

The method is applied to the UCI Adult dataset to explain income inequality by gender, race, and education, revealing non-trivial patterns and demonstrating that the approach naturally handles interactions and heterogeneous effects. The paper bridges global sensitivity analysis and causal inference, offering a principled framework for mechanistic explanations in complex systems. Under a comonotonicity assumption on potential outcomes, the measure is identifiable and can be estimated using Monte Carlo sampling and conditional quantile regression.

## Method Summary
Counterfactual explainability extends variance-based variable importance methods by incorporating causal structure through a directed acyclic graph (DAG). The approach defines a probability measure on an explanation algebra that assigns weights to different subsets of explanatory variables based on how much of the outcome's variance they explain. Under the comonotonicity assumption on basic potential outcomes, the method enables point identification and can be estimated using Monte Carlo sampling with conditional quantile regression. The framework is applied to explain income inequality using the UCI Adult dataset, comparing results across different estimation methods.

## Key Results
- Introduces counterfactual explainability as a causal approach that bridges global sensitivity analysis and causal inference
- Shows that the explanation algebra unifies different variable importance measures through a unique probability measure
- Demonstrates that comonotonicity of basic potential outcomes enables point identification from observable data
- Applies the method to UCI Adult dataset, revealing non-trivial patterns in income inequality explanations
- Compares three estimation methods (quantile regression, additive noise, Gaussian noise) showing markedly different results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual explainability bridges global sensitivity analysis and causal inference by replacing variables with their "basic potential outcomes" in the DAG framework.
- Mechanism: In the causal Markov model, basic potential outcomes (W* for W) are independent noise terms. Replacing W with W* in Sobol's pick-freeze formula allows dependent explanatory variables to be handled while preserving causal interpretation.
- Core assumption: The causal relationship between variables can be modeled as a DAG, and the distribution is causal Markov with respect to this DAG.
- Evidence anchors:
  - [abstract]: "Counterfactual explainability extends methods for global sensitivity analysis... to dependent explanations by using a directed acyclic graphs to describe their causal relationship."
  - [section 3.2]: "Our approach is based on defining total explainability using (1) but with the variables replaced by their 'basic potential outcomes'."
  - [corpus]: Weak. Found related papers on counterfactual explainability but none specifically discuss this DAG-to-noise transformation mechanism.
- Break condition: If the causal DAG is misspecified or the causal Markov assumption fails, the replacement of variables with basic potential outcomes loses validity.

### Mechanism 2
- Claim: The explanation algebra unifies different variable importance measures (Sobol's indices, Shapley values, etc.) through a unique probability measure on Boolean clauses.
- Mechanism: By mapping each subset S to its corresponding clause in the explanation algebra and assigning probability mass proportional to interaction variance, all existing importance measures become equivalent representations of a single underlying measure.
- Core assumption: Explanatory variables are independent (for the standard global sensitivity analysis version) and the functional ANOVA decomposition exists.
- Evidence anchors:
  - [section 2.2]: "various notions of subset variable importance... can be unified using what we call the explanation algebra."
  - [Theorem 2]: "Let ξ 1 be any probability measure on E(W) such that... Then ξ 1 = ξ 2 = ξ 3 = ξ 4."
  - [corpus]: Weak. Related papers discuss Shapley values and counterfactual explainability separately but don't unify them through explanation algebra.
- Break condition: When explanatory variables are dependent, the explanation algebra still exists but the standard global sensitivity measures lose their equivalence properties.

### Mechanism 3
- Claim: Comonotonicity of basic potential outcomes enables point identification of counterfactual explainability from observable data.
- Mechanism: Under comonotonicity, the joint distribution of basic potential outcomes is determined by their marginal distributions. This allows sampling counterfactual outcomes using conditional quantile regression and Monte Carlo methods.
- Core assumption: Basic potential outcomes are comonotone (perfectly positively correlated).
- Evidence anchors:
  - [section 4.1]: "If the basic potential outcomes are comonotone, the counterfactual explainability is point identifiable."
  - [Definition 8]: "We say the basic potential outcomes are comonotone if there exist non-decreasing functions..."
  - [section 4.2]: "Under this assumption, we discuss some estimation methods based on Monte Carlo."
- Break condition: If the comonotonicity assumption fails, only partial identification bounds are available, which may be very wide.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG) causal models
  - Why needed here: The entire counterfactual explainability framework relies on representing causal relationships through DAGs, where variables are nodes and causal effects are directed edges.
  - Quick check question: In a DAG with W1 → W2 → Y, what are the basic potential outcomes for W2, and how do they relate to the observed W2?

- Concept: Functional ANOVA decomposition
  - Why needed here: This decomposition provides the mathematical foundation for variance-based variable importance measures that counterfactual explainability extends.
  - Quick check question: For independent variables W1, W2, what is the relationship between the total variance and the sum of interaction variances in the functional ANOVA?

- Concept: Potential outcomes framework
  - Why needed here: Counterfactual explainability fundamentally relies on comparing potential outcomes under different interventions, which requires understanding the notation Y(w) and basic potential outcomes W*.
  - Quick check question: If W1 and W2 are causally independent Rademacher variables, what is Y(W1, W2) - Y(W1', W2) when Y(w1, w2) = w1*w2?

## Architecture Onboarding

- Component map: Dataset (Y, W1...WK) -> DAG specification -> Conditional quantile regression -> Monte Carlo sampling -> Inclusion-exclusion principle -> Counterfactual explainability measure ξ(C)
- Critical path:
  1. Specify DAG and estimate conditional quantile functions for each node
  2. Sample basic potential outcomes using comonotonic coupling
  3. Compute total explainability using pick-freeze formula
  4. Derive explainability for all clauses via inclusion-exclusion
- Design tradeoffs:
  - Comonotonicity assumption vs. partial identification: Strong assumption enables point estimation but may not hold in practice
  - Conditional quantile regression vs. additive noise models: Quantile regression handles non-Gaussian distributions but is computationally heavier
  - Full DAG vs. ancestral margin: More detailed DAG provides finer explanations but requires more assumptions
- Failure signatures:
  - Wide partial identification bounds indicate comonotonicity assumption likely violated
  - Negative explainability estimates suggest estimation method issues (e.g., additive noise model inappropriate)
  - Inconsistent results across DAG ancestral margins suggest model misspecification
- First 3 experiments:
  1. Apply to synthetic data where ground truth is known (e.g., linear model with independent noise) to verify method recovers expected explainability
  2. Test comonotonicity assumption by examining joint distributions of basic potential outcomes in real data
  3. Compare results using different estimation methods (quantile regression vs. additive noise) on same dataset to assess sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can we establish tight upper bounds for counterfactual explainability when comonotonicity fails?
- Basis in paper: [inferred] The authors note that without comonotonicity, counterfactual explainability is only partially identified, and they reference Lei et al. (2025) for lower bounds but indicate that upper bounds are generally not tight.
- Why unresolved: The paper does not provide a general method for obtaining tight upper bounds, only noting the difficulty and suggesting that computational tools using multi-marginal optimal transport might be needed.
- What evidence would resolve it: A formal proof establishing conditions under which tight upper bounds exist, or a computational method that consistently produces such bounds across different DAG structures.

### Open Question 2
- Question: How does the choice of estimation method for conditional quantile functions affect the stability and interpretability of counterfactual explainability estimates?
- Basis in paper: [explicit] The authors compare three estimation methods (additive independent noise, heteroskedastic Gaussian noise, and conditional quantile regression) and show they produce markedly different results on the US Income dataset.
- Why unresolved: While the paper demonstrates sensitivity to estimation method, it does not provide guidance on how to choose the most appropriate method for a given dataset or causal structure.
- What evidence would resolve it: A systematic comparison across multiple datasets showing which estimation method performs best under different data characteristics (e.g., distribution shapes, sample sizes, variable types).

### Open Question 3
- Question: Can counterfactual explainability be meaningfully extended to handle non-real-valued explanatory variables (e.g., categorical or structured data)?
- Basis in paper: [explicit] The authors note that comonotonicity implicitly assumes real-valued variables and suggest that counterfactual explainability could work for non-Euclidean variables if their counterfactuals can be sampled from a generative model.
- Why unresolved: The paper does not explore how to define or estimate counterfactual explainability for categorical variables or more complex data structures like graphs or images.
- What evidence would resolve it: A formal framework for defining counterfactual explainability with non-real-valued variables, along with a demonstration on a real dataset containing mixed variable types.

## Limitations
- The framework's reliance on the comonotonicity assumption for point identification represents a significant limitation, as real-world potential outcomes may exhibit more complex dependence structures.
- The explanation algebra approach requires careful DAG specification that may be challenging in practice, especially for high-dimensional or complex systems.
- The method's performance with high-dimensional datasets and complex non-linear relationships remains an open question, with computational costs potentially limiting scalability.

## Confidence
- **High confidence**: The theoretical foundation establishing the uniqueness of the counterfactual explainability measure under the stated axioms (symmetry, additivity, ancestral consistency)
- **Medium confidence**: The practical utility demonstrated on the UCI Adult dataset, though results depend heavily on DAG specification
- **Medium confidence**: The claim that this bridges global sensitivity analysis and causal inference, as the practical differences from existing methods need further empirical validation

## Next Checks
1. Test the method on synthetic datasets where ground truth explainability is known to verify estimation accuracy
2. Examine the sensitivity of results to different DAG specifications for the same dataset
3. Compare partial identification bounds with point estimates under comonotonicity to assess the assumption's validity in real data