---
ver: rpa2
title: Presentations are not always linear! GNN meets LLM for Document-to-Presentation
  Transformation with Attribution
arxiv_id: '2405.13095'
source_url: https://arxiv.org/abs/2405.13095
tags:
- presentation
- document
- slide
- slides
- paragraphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GDP, a novel approach to automatically generating
  presentations from long documents. The key innovation is using a graph neural network
  (GNN) to capture non-linear relationships between document paragraphs and map them
  to presentation slides, addressing limitations of LLMs that struggle with long contexts
  and tend to produce linear summaries.
---

# Presentations are not always linear! GNN meets LLM for Document-to-Presentation Transformation with Attribution

## Quick Facts
- **arXiv ID**: 2405.13095
- **Source URL**: https://arxiv.org/abs/2405.13095
- **Reference count**: 40
- **Primary result**: Graph Neural Network + LLM approach outperforms GPT-based baselines on SciDuet dataset for automatic presentation generation from documents

## Executive Summary
This paper introduces GDP, a novel approach to automatically generating presentations from long documents. The key innovation is using a graph neural network (GNN) to capture non-linear relationships between document paragraphs and map them to presentation slides, addressing limitations of LLMs that struggle with long contexts and tend to produce linear summaries. GDP first trains a classifier to predict paragraph co-occurrence in slides, constructs a graph where nodes represent paragraphs and edges indicate likelihood of appearing together, then uses a 2-layer GCN with unsupervised loss to learn paragraph embeddings. Spectral clustering on these embeddings produces slide clusters, which are ordered by paragraph index and fed to an LLM for final slide generation with attribution.

## Method Summary
GDP transforms long documents into presentations by learning a graph structure that captures non-linear paragraph relationships. The method trains a RoBERTa classifier to predict which paragraphs co-occur in the same slide, then constructs a graph where paragraphs are nodes and edges represent co-occurrence likelihood. A 2-layer GCN learns paragraph embeddings from this graph, which are clustered using spectral clustering to form slide groups. These clusters are ordered by their minimum paragraph index to maintain rough document flow, then fed sequentially to GPT-3.5 for slide generation. The approach explicitly handles the non-linear nature of presentations where slides often combine non-adjacent document content, while maintaining attribution to source paragraphs.

## Key Results
- GDP achieves strong coverage (39.05%), low perplexity (56.01), and high G-Eval scores (7.78) compared to GPT-based baselines
- Human evaluation confirms superior performance in language quality, narrative coherence, and utility
- GDP produces more non-linear slide mappings (24.9% non-linearity) closer to human-generated presentations than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph Neural Networks (GNN) can capture non-linear relationships between document paragraphs by learning embeddings that preserve structural proximity.
- Mechanism: GDP constructs a graph where nodes are paragraphs and edges are created based on classifier probabilities of co-occurrence in the same slide. A 2-layer GCN then learns embeddings such that connected nodes have similar embeddings, preserving non-linear structure.
- Core assumption: The pairwise classifier accurately captures semantic similarity relevant to slide membership.
- Evidence anchors:
  - [abstract]: "We propose a novel graph based solution where we learn a graph from the input document and use a combination of graph neural network and LLM to generate a presentation with attribution of content for each slide."
  - [section]: "We use a 2-layered graph convolution encoder... to obtain representation of each node... Since our main goal is to cluster the paragraphs in such a way that each cluster can correspond to a slide, we use spectral clustering on these paragraph embeddings from GNN."
  - [corpus]: Weak - related papers focus on LLM-based approaches, no direct GNN-based document-to-presentation method found.
- Break condition: If the classifier produces noisy or random probabilities, the graph structure becomes uninformative and GNN embeddings lose semantic meaning.

### Mechanism 2
- Claim: Spectral clustering on GNN node embeddings produces balanced clusters that map well to presentation slides with non-linear content.
- Mechanism: After GNN training, spectral clustering is applied to node embeddings with K clusters (number of slides). This groups paragraphs that are structurally and semantically related, even if non-contiguous in the document.
- Core assumption: GNN embeddings preserve both local graph structure and semantic similarity, making them suitable for clustering.
- Evidence anchors:
  - [abstract]: "GDP achieves strong coverage... and high G-Eval scores... compared to GPT-based baselines. Human evaluation confirms superior performance... and utility."
  - [section]: "We have observed empirically that spectral clustering is able to produce more balanced clusters compared to other algorithms such as KMeans on these node embeddings."
  - [corpus]: Weak - no direct evidence of spectral clustering outperforming other methods on document clustering for presentation generation.
- Break condition: If K is poorly chosen or GNN fails to produce meaningful embeddings, clusters may be incoherent or too granular/fragmented.

### Mechanism 3
- Claim: Sequential LLM generation with paragraph clusters and previous slide titles maintains narrative coherence while incorporating non-linear content mapping.
- Mechanism: GDP orders clusters by minimum paragraph index to roughly follow document flow, then feeds each cluster's paragraphs plus previous slide titles to GPT-3.5 for slide generation, preserving flow while allowing non-linear content assembly.
- Core assumption: LLMs can maintain narrative coherence when given partial context from previous slides and non-linear paragraph clusters.
- Evidence anchors:
  - [abstract]: "GDP achieves strong coverage... and high G-Eval scores... compared to GPT-based baselines. Human evaluation confirms superior performance in language quality, narrative coherence, and utility."
  - [section]: "To generate a slide sk, we provide the texts present in the paragraphs (pi|pi ∈ C′k), along with the titles of the previous slides s1, · · · , sk−1... Experimentally, we found that providing information about the previous slides help GPT to maintain a good flow in the presentation."
  - [corpus]: Weak - no direct evidence that GPT-3.5 maintains coherence better with this specific prompting pattern versus alternatives.
- Break condition: If LLM context window is insufficient or if paragraph clusters are too large, coherence may degrade or important content may be omitted.

## Foundational Learning

- Concept: Graph Neural Networks and spectral clustering
  - Why needed here: GDP relies on GNN to learn paragraph embeddings from graph structure, then uses spectral clustering to form slide clusters. Understanding these methods is essential for implementing and debugging the pipeline.
  - Quick check question: What is the key difference between spectral clustering and KMeans, and why might spectral clustering be preferred for graph-structured data?

- Concept: Sentence embeddings and similarity metrics
  - Why needed here: The classifier uses sentence embeddings to determine paragraph-slide relationships, and coverage metrics use cosine similarity between embeddings. Proper understanding is needed for feature engineering and evaluation.
  - Quick check question: How does cosine similarity between sentence embeddings relate to semantic similarity, and what are its limitations?

- Concept: Large Language Model prompting and context management
  - Why needed here: GDP uses GPT-3.5 sequentially, feeding previous slide titles and current paragraph clusters. Understanding prompt engineering and context window limits is crucial for effective generation.
  - Quick check question: Why might providing previous slide titles help maintain narrative coherence in sequential LLM generation?

## Architecture Onboarding

- Component map: PDF extraction -> paragraph segmentation -> sentence embeddings -> classifier -> graph construction -> GNN -> spectral clustering -> GPT-3.5 sequential generation -> final slides
- Critical path:
  1. Extract paragraphs from PDF
  2. Compute sentence embeddings
  3. Run classifier on paragraph pairs
  4. Build graph and run GNN
  5. Cluster embeddings into K groups
  6. Order clusters by min paragraph index
  7. Generate slides sequentially with GPT-3.5

- Design tradeoffs:
  - Graph-based vs. pure LLM: GNN adds complexity but better handles non-linearity; pure LLM simpler but struggles with long contexts.
  - Spectral vs. KMeans clustering: Spectral better for graph structure but more computationally intensive.
  - Sequential vs. parallel generation: Sequential maintains flow but slower; parallel faster but may lose coherence.

- Failure signatures:
  - Low classifier accuracy -> noisy graph edges -> poor GNN embeddings
  - Imbalanced or fragmented clusters -> incoherent slides
  - High perplexity or low G-Eval -> poor language quality or narrative
  - Low coverage -> important content omitted

- First 3 experiments:
  1. Run classifier on sample paragraph pairs and inspect probability distribution to verify it captures semantic similarity.
  2. Build graph with varying edge thresholds (α) and visualize to ensure meaningful structure emerges.
  3. Apply spectral clustering with different K values on a small document and inspect cluster coherence manually.

## Open Questions the Paper Calls Out
None

## Limitations
- Classifier Quality Dependency: The entire pipeline critically depends on RoBERTa classifier accuracy with no quantitative performance metrics provided
- Graph Construction Sensitivity: No ablation studies on edge threshold α or justification for α = 0.5 choice
- Spectral Clustering vs. Alternatives: Claims spectral clustering superiority without empirical comparisons to other clustering methods

## Confidence

**High Confidence Claims**:
- GDP outperforms GPT-based baselines on SciDuet dataset for ROUGE-1, ROUGE-2, coverage, and G-Eval scores
- Human evaluation confirms GDP's superiority in language quality, narrative coherence, and utility
- GDP produces more non-linear slide mappings (24.9% non-linearity) compared to baselines

**Medium Confidence Claims**:
- The GNN+spectral clustering approach specifically enables better non-linear content mapping
- Providing previous slide titles to GPT-3.5 improves narrative coherence
- The graph-based approach is more effective than pure LLM approaches for long documents

**Low Confidence Claims**:
- Spectral clustering is definitively better than alternative clustering methods for this task
- The specific choice of α = 0.5 is optimal across different document types
- The 2-layer GCN architecture is optimal for learning paragraph embeddings

## Next Checks

1. **Classifier Performance Audit**: Run the RoBERTa classifier on held-out validation data and report precision, recall, and F1 scores for paragraph co-occurrence prediction. Compare these metrics against random baseline and assess correlation with downstream GDP performance.

2. **Graph Construction Sensitivity Analysis**: Perform ablation studies varying the edge threshold α from 0.3 to 0.7 in increments of 0.1, and also test different graph construction strategies (e.g., k-NN graphs vs. threshold-based). Measure how each variant affects coverage, perplexity, and human evaluation scores.

3. **Alternative Clustering Comparison**: Replace spectral clustering with KMeans, hierarchical clustering, and DBSCAN on the same GNN embeddings. Use silhouette scores, Calinski-Harabasz index, and downstream evaluation metrics to quantitatively compare cluster quality and presentation generation performance.