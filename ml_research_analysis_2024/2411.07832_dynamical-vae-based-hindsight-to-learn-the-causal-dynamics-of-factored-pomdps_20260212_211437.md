---
ver: rpa2
title: Dynamical-VAE-based Hindsight to Learn the Causal Dynamics of Factored-POMDPs
arxiv_id: '2411.07832'
source_url: https://arxiv.org/abs/2411.07832
tags:
- hidden
- state
- transition
- encoder
- hindsight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a DVAE-based hindsight encoder for learning
  the causal dynamics of factored POMDPs from offline trajectories. The key idea is
  to use an extended hindsight framework that combines past, current, and multi-step
  future information within a factored POMDP setting, instead of just relying on history
  of past observations and actions.
---

# Dynamical-VAE-based Hindsight to Learn the Causal Dynamics of Factored-POMDPs

## Quick Facts
- arXiv ID: 2411.07832
- Source URL: https://arxiv.org/abs/2411.07832
- Reference count: 14
- This paper introduces a DVAE-based hindsight encoder for learning the causal dynamics of factored POMDPs from offline trajectories

## Executive Summary
This paper presents a novel approach for learning the causal dynamics of factored partially observable Markov decision processes (POMDPs) using a dynamical variational autoencoder (DVAE) with hindsight. The key innovation is an encoder that conditions on past, current, and multi-step future observations and actions, rather than just historical data. This hindsight approach enables better disentanglement of hidden states from exogenous noise and more accurate identification of causal transition graphs through conditional mutual information analysis.

## Method Summary
The method employs a DVAE framework where a backward RNN captures future dependencies and an MLP models Markovian past dependencies. These components are combined to infer current hidden states from a factored POMDP setting. The approach uses stop-gradient techniques to prevent representational collapse and evaluates conditional mutual information (CMI) between state factors to discover the causal transition graph. The model is trained using variational lower bound objectives with masked and causal loss terms, and the transition graph is inferred by thresholding CMI values.

## Key Results
- DVAE-based hindsight encoder outperforms history-based and earlier hindsight-based methods in identifying hidden state factors
- Achieves near-perfect accuracy in inferring the transition graph and predicting states and rewards in the Modulo environment
- History-based encoders struggle to learn hidden states and perform accurate transitions and reward predictions
- The method successfully handles stochastic, discrete state-action, factored POMDP environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DVAE-based hindsight encoder outperforms history-based encoders because it incorporates both past and future information to disentangle hidden states from exogenous noise.
- Mechanism: The encoder conditions on 1-step past, current, and all future observations/actions, allowing it to infer hidden states by leveraging future observations that carry noise information.
- Core assumption: Future observations contain information about current hidden states contaminated with noise, which can be disentangled using past information.
- Evidence anchors:
  - [abstract]: "We demonstrate that incorporating future information is essential to accurately capture causal dynamics and enhance state representations."
  - [section]: "An encoder conditioned on ot, at, and oi t+1, as in Jarrett et al. (2023), would infer hj t by inverting the transition function of the parent of hj t, i.e., oi t+1. However, the inferred hj t would be contaminated with ϵi t."
- Break condition: If future observations do not contain additional information about current hidden states beyond what is available in the past, or if the noise is completely independent of future observations.

### Mechanism 2
- Claim: The proposed method learns causal transition graphs more effectively by using conditional mutual information to identify parent-child relationships between state factors.
- Mechanism: The method evaluates CMI between state factors across time steps, using masked and causal transition distributions to distinguish genuine causal dependencies from spurious correlations.
- Core assumption: The true causal transition graph can be recovered by thresholding CMI values between state factors.
- Evidence anchors:
  - [section]: "The causal dependency of each transition pair si t → sj t+1 or at → sj t+1 is estimated through conditional mutual information (CMI)."
  - [section]: "The existence of an edge in the transition graph, i.e., si t → sj t+1 or at → sj t+1, is determined by whether the corresponding CMI value CMIi,j exceeds a predefined threshold δ."
- Break condition: If the environment has complex non-linear relationships or hidden confounders that CMI cannot capture, or if the threshold δ is poorly chosen.

### Mechanism 3
- Claim: The stop-gradient technique prevents representational collapse by providing a stable target distribution for the hidden state encoder.
- Mechanism: The KL-divergence term uses a stop-gradient version of the hidden state encoder parameters as the target distribution, which helps avoid trivial constant representations.
- Core assumption: Using the same parameters for both the inference and generative models leads to degenerate solutions where the encoder outputs a constant value.
- Evidence anchors:
  - [section]: "The notation ϕ denotes the stop-gradient version of ϕ, which is detached from the computation graph and replaced by a copy of ϕ from the previous training step."
  - [section]: "Using a stop-gradient target in self-predictive representations is common in practice (Zhang et al., 2020; Ghugare et al., 2022), as this technique helps avoid representational collapse (Ni et al., 2024)."
- Break condition: If the learning rate is too high or the model architecture is too simple, leading to instability regardless of the stop-gradient technique.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: The DVAE framework provides the probabilistic modeling foundation for inferring hidden states from partial observations
  - Quick check question: What is the difference between the generative model and inference model in a VAE?

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The problem setting assumes underlying MDP dynamics that are not directly observable
  - Quick check question: How does a POMDP differ from an MDP in terms of state observability?

- Concept: Causal discovery through conditional mutual information
  - Why needed here: The method uses CMI to identify causal dependencies between state factors in the transition graph
  - Quick check question: What does it mean for two variables to be conditionally independent given a third variable?

## Architecture Onboarding

- Component map:
  - Hidden encoder: Backward RNN + MLP + combiner function
  - Transition model: Masked MLPs for observed and hidden state transitions
  - Reward predictor: Separate network conditioned on encoded hidden states
  - CMI evaluator: Computes conditional mutual information between state factors

- Critical path:
  1. Encode hidden states using past, current, and future observations
  2. Predict next state observations and hidden states
  3. Compute losses (NLL for observations, KL for hidden states, reward prediction)
  4. Update parameters using gradient descent
  5. Periodically evaluate CMI to update transition graph

- Design tradeoffs:
  - Forward RNN (history-based) vs. backward RNN (hindsight-based): The backward RNN captures future dependencies but requires complete trajectories
  - Full hindsight vs. 1-step hindsight: More future information may help but increases computational complexity
  - Stop-gradient target vs. online target: Stop-gradient prevents collapse but may slow convergence

- Failure signatures:
  - High KL-divergence loss: Encoder not matching transition model predictions
  - CMI matrix with values near threshold: Unclear causal structure
  - Reward prediction loss not decreasing: Hidden state encoding not capturing reward-relevant information

- First 3 experiments:
  1. Train with history-based encoder only to establish baseline performance
  2. Train with current and 1-step future hindsight encoder to compare with DVAE approach
  3. Train with full hindsight (current and all future) to test if additional future information helps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DVAE-based hindsight encoders scale with the number of cascaded hidden factors in the transition graph?
- Basis in paper: [inferred] The paper notes that while the DVAE 1-step Hindsight Encoder was sufficient for a single hidden factor, extending it to scenarios with multiple cascaded hidden factors may require additional future information for effective latent identification.
- Why unresolved: The experiments only tested a single hidden factor scenario, leaving the scalability to multiple cascaded hidden factors unexplored.
- What evidence would resolve it: Empirical results comparing the performance of DVAE-based encoders with varying numbers of cascaded hidden factors in the transition graph.

### Open Question 2
- Question: How does the DVAE-based hindsight approach perform in continuous state-action spaces compared to discrete ones?
- Basis in paper: [explicit] The paper mentions that expanding the approach to continuous state-action spaces would link the work to DVAE research on latent dynamics in stochastic driven dynamical systems, but does not provide experimental results.
- Why unresolved: The experiments were conducted in a discrete state-action space (modulo environment), and the authors only suggest the potential extension to continuous spaces.
- What evidence would resolve it: Empirical results comparing the performance of DVAE-based hindsight encoders in continuous state-action space environments.

### Open Question 3
- Question: How does the performance of DVAE-based hindsight encoders compare to other state-of-the-art methods for learning causal dynamics in POMDPs?
- Basis in paper: [inferred] The paper compares the performance of DVAE-based hindsight encoders to history-based and earlier hindsight-based methods, but does not compare to other state-of-the-art methods for learning causal dynamics in POMDPs.
- Why unresolved: The paper focuses on comparing the proposed method to specific baselines, but does not provide a comprehensive comparison to other state-of-the-art methods in the field.
- What evidence would resolve it: Empirical results comparing the performance of DVAE-based hindsight encoders to other state-of-the-art methods for learning causal dynamics in POMDPs, such as those based on deep reinforcement learning or causal inference techniques.

## Limitations
- The method requires complete trajectories for backward RNN processing, limiting applicability to streaming or online settings
- Evaluation is limited to a single synthetic environment (Modulo environment) with discrete states and actions
- Several critical implementation details remain unspecified, including exact neural network architectures and hyperparameter values
- Generalization to continuous state-action spaces and more complex environments remains unexplored

## Confidence
- High Confidence: The core mechanism that incorporating future observations improves hidden state inference in factored POMDPs is well-supported by both theoretical reasoning and empirical results in the Modulo environment. The use of conditional mutual information for causal structure discovery is a standard and well-established technique.
- Medium Confidence: The specific design choices for the DVAE architecture (backward RNN + MLP combination) and the stop-gradient technique for preventing representational collapse are reasonable but may not be optimal. The paper provides theoretical justification but limited ablation studies on alternative designs.
- Low Confidence: The generalization of results to more complex environments, particularly those with continuous states/actions, high-dimensional observations, or non-deterministic transition dynamics. The paper's evaluation on a single synthetic environment limits confidence in broader applicability.

## Next Checks
1. **Architecture Sensitivity Analysis**: Systematically vary the neural network architecture (number of layers, hidden units, activation functions) and measure impact on transition graph accuracy and state decoding performance. This will determine whether the proposed architecture is critical or if simpler alternatives suffice.

2. **Generalization to Continuous Domains**: Implement the DVAE-based hindsight encoder in a continuous-state, continuous-action environment (e.g., modified MuJoCo tasks) and evaluate whether the same performance gains hold. This will test the method's applicability beyond discrete environments.

3. **Online/Streaming Extension**: Modify the backward RNN to handle streaming data by maintaining a sliding window of future observations, and evaluate performance degradation compared to batch processing. This will determine practical applicability to real-world scenarios where complete trajectories are unavailable.