---
ver: rpa2
title: Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial
  Reasoning?
arxiv_id: '2409.17080'
source_url: https://arxiv.org/abs/2409.17080
tags:
- vlms
- task
- tasks
- learning
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a new benchmark called Spatial Visual Ambiguity Tasks
  (SVAT) to evaluate whether vision-language models (VLMs) can learn novel visuospatial
  concepts from visual demonstrations. SVAT consists of classification tasks where
  the goal is to identify the correct spatial decision boundary in synthesized images
  based on limited ambiguous text and visual demonstrations.
---

# Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?

## Quick Facts
- arXiv ID: 2409.17080
- Source URL: https://arxiv.org/abs/2409.17080
- Authors: Bowen Zhao; Leo Parker Dirac; Paulina Varshavskaya
- Reference count: 40
- Primary result: State-of-the-art VLMs fail at SVAT tasks in zero-shot settings, but curriculum learning improves performance by 14.2%-34.2% relative accuracy compared to direct fine-tuning on the most challenging tasks.

## Executive Summary
This paper introduces the Spatial Visual Ambiguity Tasks (SVAT) benchmark to evaluate whether vision-language models (VLMs) can learn novel visuospatial concepts from visual demonstrations. SVAT consists of binary classification tasks where models must identify correct spatial decision boundaries from synthesized images based on limited ambiguous text and visual demonstrations. The benchmark systematically varies task difficulty through object complexity, background complexity, and number of distracting objects.

The key finding is that while VLMs fail at SVAT tasks in zero-shot settings, applying curriculum learning—progressively increasing task difficulty during training—leads to significantly better results than direct fine-tuning. Specifically, curriculum learning achieves 14.2% to 34.2% relative accuracy gains compared to direct fine-tuning on the most challenging SVAT tasks. This demonstrates that curriculum learning is an effective approach for enabling VLMs to learn ambiguous visuospatial reasoning from visual demonstrations.

## Method Summary
The authors created SVAT, a synthetic benchmark for evaluating VLMs' ability to learn spatial reasoning from visual demonstrations. They generated 50 task families with varying difficulty levels parameterized by background complexity, object category variety, number of distractors, and text guidance. Five VLMs were evaluated across three settings: zero-shot, direct fine-tuning, and curriculum learning. For curriculum learning, they implemented four strategies (CI, CC, CM, Call) that progressively increase task difficulty by varying background complexity, object variety, or distractor count. All fine-tuning used LoRA adapters with 3 epochs per task family.

## Key Results
- VLMs fail at SVAT tasks in zero-shot settings without fine-tuning
- Direct fine-tuning improves performance by 5.8%-27.3% across different models
- Curriculum learning achieves 14.2% to 34.2% relative accuracy gains compared to direct fine-tuning on the most challenging SVAT tasks
- The CI (background complexity) curriculum strategy shows the most consistent improvement across models

## Why This Works (Mechanism)

### Mechanism 1: Curriculum Learning for Spatial Concept Acquisition
VLMs can learn novel visuospatial concepts by progressively increasing task difficulty during training. CL starts with simpler task families where the decision boundary is easier to infer from ICL examples, building foundational spatial reasoning capabilities before introducing more complex tasks with distractors and ambiguous text.

### Mechanism 2: Controlled Task Parameterization
SVAT's parameterization (φ = (I, C, M, T)) enables systematic study of VLM spatial reasoning failures by varying background complexity, object category variety, distractors, and text guidance independently, revealing whether failures stem from visual ambiguity, object recognition, or text interpretation.

### Mechanism 3: ICL Limitations in Novel Spatial Reasoning
Zero-shot VLM failures on SVAT reveal limitations in multimodal ICL rather than format following. Despite pretraining on large multimodal datasets, VLMs cannot infer spatial decision boundaries from ICL examples when the boundary is not explicitly stated in text, suggesting they may be limited to adapting output formats rather than learning novel visual concepts.

## Foundational Learning

- **Visual In-context Learning (ICL) formulation**: Understanding how VLMs process demonstration examples and queries is essential for designing SVAT and interpreting results. *Quick check: What are the components of an input prompt x in visual ICL according to Eq. (1)?*

- **Curriculum Learning (CL) principles**: CL is the key technique that enables VLMs to learn SVAT tasks, so understanding its design and implementation is crucial. *Quick check: How does the SVAT curriculum C(φ) differ from naive data mixing according to Section 2.4?*

- **Spatial decision boundary inference**: The core challenge in SVAT is inferring an implicit decision boundary P from ICL examples, which is fundamentally different from standard classification. *Quick check: How is the decision boundary P defined in SVAT and why can't it be inferred from text alone?*

## Architecture Onboarding

- **Component map**: VLM backbone (LLaVA-Next, Idefics2, VILA-1.5, InternVL2, MiniCPM-V-2.6) → LoRA adapter for fine-tuning → Dataset generator (SVAT) → Prompt generator → Evaluation metric (accuracy)

- **Critical path**: Task family selection (φ) → Dataset generation → Model fine-tuning (direct or CL) → Prompt generation → Model inference → Accuracy calculation

- **Design tradeoffs**: SVAT uses synthetic data for control but may not reflect real-world complexity; CL requires multiple training stages but improves performance; simpler tasks may not transfer to complex ones

- **Failure signatures**: Zero-shot accuracy near 50% (random guessing); failure to follow ICL output format; CL performance worse than direct fine-tuning (indicates wrong curriculum ordering)

- **First 3 experiments**:
  1. Run zero-shot evaluation on φ = (I5, Ceasy, 1, Tnone) to establish baseline VLM spatial reasoning capability
  2. Implement direct fine-tuning on the same task family to verify if VLMs can learn from SVAT data at all
  3. Apply curriculum learning with CI strategy (varying background complexity) to test if staged learning improves performance over direct fine-tuning

## Open Questions the Paper Calls Out

1. **Transfer to real-world tasks**: Does curriculum learning remain effective for ambiguous visual-spatial reasoning when the task complexity increases beyond what was tested in SVAT? The paper notes that no realistic data was considered and suggests combining SVAT with ambiguous, realistic multimodal data as a future direction.

2. **Model scale limitations**: Are larger VLMs (beyond the 7-8B scale tested) able to achieve significantly better performance on ambiguous spatial reasoning tasks without curriculum learning? The paper states that larger models have more potential for conducting visuospatial reasoning, especially under in-context learning setups.

3. **Synthetic-to-real transfer**: How transferable is knowledge learned from synthetic ambiguous spatial reasoning tasks like SVAT to real-world visual tasks with ambiguous spatial components? The paper explicitly states that what knowledge can be transferred between synthetic datasets like SVAT and real-world datasets remains under-explored.

## Limitations

- **Synthetic data generalization**: SVAT uses synthetically generated images that may not capture the full complexity and ambiguity present in real-world visual demonstrations.
- **Curriculum design specificity**: The effectiveness of the specific CI→CC→CM→Call curriculum ordering versus alternative progressions is not thoroughly explored or validated.
- **Performance ceiling**: Even with curriculum learning, performance on the most difficult tasks remains below 60% accuracy, suggesting fundamental limitations in VLMs' ability to learn from visual demonstrations.

## Confidence

- **High Confidence**: The zero-shot failure results are well-supported by multiple experiments across five different VLMs.
- **Medium Confidence**: The curriculum learning improvements are demonstrated, but the magnitude of gains depends heavily on the specific curriculum design.
- **Low Confidence**: The claim that CL enables VLMs to learn novel visuospatial concepts assumes that observed improvements represent genuine concept learning rather than pattern matching or memorization.

## Next Checks

1. **Cross-dataset validation**: Test whether VLMs trained on SVAT synthetic data can transfer to real-world ambiguous spatial reasoning tasks to validate whether the synthetic data captures genuine spatial reasoning challenges.

2. **Curriculum ablation study**: Systematically test alternative curriculum orderings and progressions to determine whether the specific CI→CC→CM→Call progression is optimal or if benefits come from curriculum learning in general.

3. **Zero-shot prompting variation**: Experiment with alternative few-shot learning approaches (varying shot counts, different demonstration selection strategies, prompt engineering) to determine whether zero-shot failures represent fundamental limitations or can be mitigated through better prompting strategies.