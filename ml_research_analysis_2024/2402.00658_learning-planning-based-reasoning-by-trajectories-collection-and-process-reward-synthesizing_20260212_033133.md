---
ver: rpa2
title: Learning Planning-based Reasoning by Trajectories Collection and Process Reward
  Synthesizing
arxiv_id: '2402.00658'
source_url: https://arxiv.org/abs/2402.00658
tags:
- reasoning
- process
- chosen
- observation
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework to learn planning-based
  reasoning through Direct Preference Optimization (DPO) on collected trajectories,
  which are ranked according to synthesized process rewards. The method addresses
  the problem of hallucination and flaws in reasoning process of Large Language Models
  (LLMs) by introducing offline simulation and trajectory collection to induce planning-based
  reasoning.
---

# Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing

## Quick Facts
- arXiv ID: 2402.00658
- Source URL: https://arxiv.org/abs/2402.00658
- Authors: Fangkai Jiao; Chengwei Qin; Zhengyuan Liu; Nancy F. Chen; Shafiq Joty
- Reference count: 26
- Key outcome: A 7B model can surpass GPT-3.5-Turbo on challenging logical reasoning benchmarks using synthesized process supervision

## Executive Summary
This paper introduces pDPO, a novel framework for learning planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories. The method addresses LLM hallucination and reasoning flaws by using offline simulation to estimate intermediate step rewards, then training a Process Reward Model to assess reasoning quality. By synthesizing process supervision from outcome labels rather than requiring expensive human annotations, the framework enables more efficient training of reasoning capabilities.

## Method Summary
The pDPO framework operates through a multi-stage pipeline: first collecting seed trajectories from LLMs, then sampling intermediate reasoning states and generating multiple completions from each state to estimate expected returns; these estimates train a Process Reward Model; finally, DPO optimizes the policy using preference pairs constructed from trajectories ranked by the PRM. The approach enables learning from process supervision without expensive human annotations by backpropagating outcome supervision to intermediate states through simulation-based reward estimation.

## Key Results
- 7B pDPO model outperforms GPT-3.5-Turbo on challenging logical reasoning benchmarks
- The method improves both the quality and conciseness of generated rationales
- Synthesized process supervision reduces reliance on human annotations while maintaining reasoning performance
- pDPO demonstrates effectiveness across four challenging logical reasoning datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline simulation allows accurate estimation of intermediate step rewards without costly online search.
- Mechanism: The method samples multiple completions from each intermediate reasoning state and uses the fraction of completions that reach the correct answer as an estimate of the expected return for that state.
- Core assumption: The frequency of correct completions from an intermediate state correlates with the quality of that state for guiding reasoning.
- Evidence anchors: [abstract] "we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to our synthesized process rewards"; [section 3.2] "we propose a simulation based method to estimate the expected value by starting from an intermediate point in a trajectory and exploring the received rewards after reaching the terminal states"
- Break condition: If the policy model's capability is too low, the simulation-based estimates become unreliable due to noise in the sampled completions.

### Mechanism 2
- Claim: Synthesizing process supervision from outcome supervision reduces annotation costs while improving reasoning quality.
- Mechanism: Ground truth outcome labels are backpropagated to intermediate states through offline simulation, then used to train a Process Reward Model that assesses intermediate reasoning steps.
- Core assumption: Outcome supervision contains sufficient information to infer the quality of intermediate reasoning steps.
- Evidence anchors: [abstract] "We introduce ground-truth outcome supervision that we backpropagate to intermediate states instead of relying on LLMs for process assessment"; [section 3.2] "we aim at introducing process supervision (Lightman et al., 2023a), which, however, is hard to obtain in most reasoning cases. We propose a simulation based method to estimate the expected value"
- Break condition: If intermediate reasoning steps are too context-dependent, outcome labels alone cannot provide sufficient information about intermediate step quality.

### Mechanism 3
- Claim: Direct Preference Optimization with synthesized process rewards improves rationale quality more than outcome-only supervision.
- Mechanism: The policy model is optimized using preference pairs constructed from trajectory-level rewards that incorporate both outcome correctness and intermediate step quality.
- Core assumption: The synthesized process rewards provide meaningful signal for discriminating between different reasoning paths to the same correct answer.
- Evidence anchors: [abstract] "we optimize the LLMs to learn a better policy for generating reliable rationales through Direct Preference Optimization (Rafailov et al., 2023), where the contrastive trajectory pairs are annotated by the PRM"; [section 3.5] "we can further consider the pair-wise relationship among those trajectories with correct predictions: Dp = {x(i), τ (i) a , τ (i) b |rp(τ (i) a ) − rp(τ (i) b ) > σ, }"
- Break condition: If the synthesized process rewards are too noisy or fail to capture meaningful differences between reasoning paths, preference optimization provides no benefit over outcome-only methods.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper builds on RLHF methodology but substitutes human process annotations with synthesized rewards
  - Quick check question: What is the key difference between traditional RLHF and the approach proposed in this paper?

- Concept: Process Reward Models (PRMs)
  - Why needed here: PRMs are trained to assess intermediate reasoning steps, enabling supervision of the reasoning process rather than just final outcomes
  - Quick check question: How does a PRM differ from a standard reward model in terms of what it evaluates?

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: The paper's offline simulation approach is conceptually inspired by MCTS's forward exploration to evaluate states
  - Quick check question: What is the fundamental trade-off that MCTS addresses that this paper's method also addresses?

## Architecture Onboarding

- Component map: Data collection pipeline (seeds trajectories → intermediate state sampling → multiple completions) → Process Reward Model training (estimated returns → classification model) → Preference dataset construction (full trajectories + PRM rewards → preference pairs) → Policy optimization (DPO training with preference pairs)
- Critical path: Data collection → PRM training → Preference annotation → DPO optimization
- Design tradeoffs:
  - More simulation samples per intermediate state improves reward quality but increases cost
  - Higher confidence thresholds (σ) for preference pairs reduce noise but limit training data
  - Using logits vs probabilities from PRM affects reward accumulation behavior
- Failure signatures:
  - Low correlation between simulation-based rewards and actual reasoning quality
  - DPO training instability or lack of improvement over baseline
  - Preference pairs not discriminating between different reasoning qualities
- First 3 experiments:
  1. Verify that simulation-based reward estimates correlate with reasoning quality on a small validation set
  2. Test PRM training with different numbers of simulation samples to find optimal balance
  3. Compare DPO performance with different confidence thresholds (σ) for constructing preference pairs

## Open Questions the Paper Calls Out
None explicitly mentioned in the paper.

## Limitations
- The simulation-based reward estimation method may fail when the base policy model is too weak to generate diverse, high-quality completions from intermediate reasoning states.
- The effectiveness of synthesizing process supervision from outcome labels depends on the assumption that intermediate reasoning steps can be meaningfully evaluated without explicit human annotations.
- The method's performance may degrade on problems where reasoning quality is highly context-dependent and cannot be inferred from final outcomes alone.

## Confidence
- **High Confidence**: The framework's overall architecture (offline simulation → process reward model → DPO optimization) is technically sound and follows established RLHF methodologies.
- **Medium Confidence**: The claim that simulation-based reward synthesis reduces annotation costs while maintaining or improving reasoning quality requires empirical validation on diverse reasoning tasks.
- **Low Confidence**: The claim that a 7B model can surpass GPT-3.5-Turbo on challenging logical reasoning benchmarks may be sensitive to specific benchmark characteristics and evaluation protocols.

## Next Checks
1. **Reward Correlation Validation**: Conduct a systematic analysis comparing simulation-based reward estimates with human-annotated process quality scores on a held-out validation set to quantify the reliability of the synthesized rewards.

2. **Policy Model Capability Analysis**: Test the framework's performance across different base model sizes (7B, 13B, 33B) to determine whether the simulation-based approach scales effectively with model capacity.

3. **Ablation on Simulation Parameters**: Perform controlled experiments varying the number of simulation samples per intermediate state and the confidence threshold (σ) for preference pair construction to identify optimal hyperparameter settings and quantify their impact on final reasoning quality.