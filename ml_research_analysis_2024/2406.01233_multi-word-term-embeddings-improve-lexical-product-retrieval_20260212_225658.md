---
ver: rpa2
title: Multi-word Term Embeddings Improve Lexical Product Retrieval
arxiv_id: '2406.01233'
source_url: https://arxiv.org/abs/2406.01233
tags:
- search
- retrieval
- product
- information
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents H1, a semantic model designed for offline term
  indexing of product descriptions at e-commerce platforms. The model improves upon
  existing approaches by processing multi-word product terms as single tokens (e.g.,
  treating "New Balance" as one token instead of two separate words), which increases
  precision without affecting recall.
---

# Multi-word Term Embeddings Improve Lexical Product Retrieval

## Quick Facts
- **arXiv ID**: 2406.01233
- **Source URL**: https://arxiv.org/abs/2406.01233
- **Authors**: Viktor Shcherbakov; Fedor Krasnov
- **Reference count**: 0
- **Primary result**: H1 hybrid search system achieves mAP@12 = 56.1% and R@1k = 86.6% on WANDS dataset

## Executive Summary
This paper presents H1, a semantic model designed for offline term indexing of product descriptions at e-commerce platforms. The model improves upon existing approaches by processing multi-word product terms as single tokens (e.g., treating "New Balance" as one token instead of two separate words), which increases precision without affecting recall. H1 uses a BERT-based Dual Encoder architecture combined with a similarity function similar to ColBERT, but operates within a hybrid search system that leverages both lexical and semantic methods.

The approach includes enriching the tokenizer's vocabulary with semantically rich terms, particularly brand names, to better capture user search intent. When evaluated on the WANDS public dataset, the hybrid search system with H1 achieved mAP@12 = 56.1% and R@1k = 86.6%, outperforming other state-of-the-art models. The authors also conducted an ablation study comparing different tokenization methods and model architectures, demonstrating that H1 with BPE tokenization and brand names consistently produced the best results.

## Method Summary
H1 is a semantic model for e-commerce product search that uses a BERT-based Dual Encoder architecture with a ColBERT-like similarity function. The key innovation is treating multi-word product terms (especially brand names) as single tokens during indexing. The model is trained with margin-based loss on positive and negative product-query pairs from the WANDS dataset. The tokenizer vocabulary is enriched with brand names, and the evaluation mimics hybrid search systems using pre-computed embeddings and an index with threshold-based matching. The approach aims to improve precision without affecting recall by preserving semantic coherence of multi-word terms.

## Key Results
- H1 hybrid search system achieves mAP@12 = 56.1% and R@1k = 86.6% on WANDS dataset
- Multi-word tokenization with brand names improves precision while maintaining recall
- BPE tokenization with brand name vocabulary enrichment consistently outperforms other methods in ablation studies
- The model outperforms other production semantic models in product search accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating multi-word brand terms as single tokens improves semantic precision without affecting recall
- Mechanism: By merging semantically coherent multi-word terms (like "New Balance") into single tokens, the system preserves brand identity during indexing and retrieval, preventing false matches from individual words
- Core assumption: User search intent for brand terms relies on the full multi-word expression, not its component words
- Evidence anchors:
  - [abstract] "compared to other production semantic models, H1 paired with the proposed approach stands out due to its ability to process multi-word product terms as one token"
  - [section] "we augmented the tokenizer's vocabulary with a carefully selected list of brand names... when a customer searches for 'new balance shoes', their intent is not to explore products related to the terms 'new' and 'balance' independently"

### Mechanism 2
- Claim: The ColBERT-inspired similarity function computes token-level similarities instead of mean vector comparisons
- Mechanism: By calculating maximum similarity scores for each query token against all document tokens (rather than averaging), the system captures more nuanced semantic relationships and maintains fine-grained matching capabilities
- Core assumption: Token-level matching provides better semantic discrimination than vector averaging in product search contexts
- Evidence anchors:
  - [abstract] "H1 uses a BERT-based Dual Encoder architecture combined with a similarity function similar to ColBERT"
  - [section] "the novelty of ColBERT lies in computing the similarity scores token-wise, instead of comparing the mean vectors"

### Mechanism 3
- Claim: Hybrid search combining lexical and semantic methods leverages strengths of both approaches
- Mechanism: The system uses lexical term indexing for fast retrieval while semantic embeddings provide ranking refinement, achieving both low latency and high relevance
- Core assumption: Lexical methods excel at exact matching while semantic methods handle vocabulary mismatches and synonymy
- Evidence anchors:
  - [abstract] "hybrid search system that incorporates the advantages of lexical methods for product retrieval and semantic embedding-based methods"
  - [section] "the evaluation approach employed... mimics the product search implemented with a simple term index-based hybrid search system"

## Foundational Learning

- **Concept**: Tokenization methods (BPE vs unigram vs word)
  - Why needed here: Different tokenization approaches affect how product terms are represented in the index, directly impacting retrieval accuracy
  - Quick check question: How does BPE tokenization differ from word tokenization in handling multi-word brand names?

- **Concept**: Dual Encoder architecture vs Single Encoder
  - Why needed here: Understanding the architectural choice between separate query/document encoders versus shared encoders affects model design decisions
  - Quick check question: What's the key difference between H1's Dual Encoder approach and ColBERT's Single Encoder architecture?

- **Concept**: Evaluation metrics for product search (mAP@k, R@k with equivalence relation)
  - Why needed here: Product search has unique requirements for precision and recall compared to document retrieval, necessitating specialized metrics
  - Quick check question: Why does product search require equivalence relations when calculating precision and recall?

## Architecture Onboarding

- **Component map**: Tokenizer -> BERT Encoders -> Index Builder -> Search Engine -> Evaluation Pipeline
- **Critical path**: 
  1. Tokenizer processes product descriptions and queries
  2. BERT encoders generate token embeddings
  3. Index builder creates token-product mappings with similarity thresholds
  4. Search engine performs lexical lookup followed by semantic ranking
  5. Results are evaluated using product-specific metrics
- **Design tradeoffs**:
  - Embedding dimensionality vs memory usage: 768 dimensions provides good balance
  - Vocabulary size vs coverage: Adding brand names improves precision but increases index size
  - Threshold selection: Lower thresholds increase recall but may reduce precision
  - Online vs offline processing: Semantic encoding happens offline to maintain search latency
- **Failure signatures**:
  - Low precision: Missing brand names in tokenizer vocabulary or incorrect multi-word tokenization
  - Low recall: Insufficient vocabulary coverage or overly strict similarity thresholds
  - High latency: Excessive vocabulary size or inefficient indexing structure
  - Inconsistent results: Threshold instability or embedding space misalignment
- **First 3 experiments**:
  1. Tokenization ablation: Compare BPE, unigram, and word tokenization with and without brand names on a small product set
  2. Threshold sensitivity: Evaluate mAP@12 and R@1k across different similarity thresholds (0.5, 0.7, 0.9) on validation data
  3. Brand name coverage: Test retrieval performance with varying percentages of brand names in the tokenizer vocabulary (10%, 50%, 100%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the H1 model perform on other e-commerce datasets beyond WANDS, and what impact does the domain specificity of the data have on its effectiveness?
- Basis in paper: [explicit] The paper mentions WANDS dataset but doesn't explore other e-commerce datasets or discuss domain specificity effects.
- Why unresolved: The evaluation was limited to a single public dataset, which may not fully represent the diversity of e-commerce search scenarios or different product categories.
- What evidence would resolve it: Testing H1 on multiple e-commerce datasets with varying characteristics (product types, query styles, brand importance) and comparing performance across domains would demonstrate generalizability.

### Open Question 2
- Question: What is the optimal balance between lexical and semantic components in hybrid search systems, and how does this balance vary based on query characteristics or product catalog size?
- Basis in paper: [inferred] The paper presents a hybrid approach but doesn't systematically explore how to optimize the balance between lexical and semantic methods or how this balance should adapt to different conditions.
- Why unresolved: The authors mention hybridization as an advantage but don't provide a framework for determining the optimal weighting or adaptation mechanisms for different scenarios.
- What evidence would resolve it: Controlled experiments varying the contribution of lexical vs. semantic components across different query types and catalog sizes, measuring performance trade-offs, would establish guidelines for system configuration.

### Open Question 3
- Question: How does the H1 model's performance scale when processing catalogs with billions of products, and what are the computational bottlenecks at extreme scale?
- Basis in paper: [explicit] The paper mentions product catalogs with billions of items but doesn't evaluate H1 at such scale or identify specific computational bottlenecks.
- Why unresolved: While the evaluation methodology mentions practical limitations, the actual scaling behavior and performance degradation points are not explored.
- What evidence would resolve it: Performance testing of H1 on progressively larger datasets, measuring latency, memory usage, and recall/precision degradation, would identify scalability limits and optimization opportunities.

## Limitations

- The evaluation is limited to a single e-commerce dataset (WANDS), which may not generalize to other domains or product categories
- The paper does not address computational efficiency trade-offs or memory requirements for maintaining the enriched vocabulary at scale
- Optimal threshold values (0.7 for mAP@12, 0.9 for R@1k) are specific to this dataset and may vary in different contexts

## Confidence

- **High Confidence**: The architectural design of using a BERT-based Dual Encoder with ColBERT-like similarity function is well-established in the literature
- **Medium Confidence**: The specific claim that treating multi-word brand terms as single tokens improves precision without affecting recall is supported by ablation study results on WANDS
- **Low Confidence**: The scalability claims and practical deployment considerations are not thoroughly addressed, lacking detailed analysis of indexing time, memory usage, or search latency

## Next Checks

1. **Dataset Generalization Test**: Evaluate the H1 model on at least two additional e-commerce datasets with different product categories and query distributions to assess whether the multi-word tokenization benefit holds across domains

2. **Vocabulary Scalability Analysis**: Conduct experiments varying the percentage of brand names and multi-word terms in the tokenizer vocabulary (e.g., 10%, 25%, 50%, 75%, 100%) to measure the precision-recall trade-off and computational overhead

3. **Threshold Sensitivity Validation**: Systematically test the model across a wider range of similarity thresholds (e.g., 0.3 to 0.95 in 0.05 increments) to identify optimal thresholds for different performance metrics and understand the stability of the model's performance across the threshold spectrum