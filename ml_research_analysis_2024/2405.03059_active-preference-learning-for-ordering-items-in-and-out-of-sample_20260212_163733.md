---
ver: rpa2
title: Active Preference Learning for Ordering Items In- and Out-of-sample
arxiv_id: '2405.03059'
source_url: https://arxiv.org/abs/2405.03059
tags:
- learning
- comparisons
- ordering
- items
- guro
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses active preference learning for ordering items
  based on pairwise comparisons, focusing on efficient in-sample ordering and generalization
  to new items. The authors propose a contextual logistic preference model and derive
  an upper bound on the expected ordering error based on collected comparisons.
---

# Active Preference Learning for Ordering Items In- and Out-of-sample

## Quick Facts
- **arXiv ID**: 2405.03059
- **Source URL**: https://arxiv.org/abs/2405.03059
- **Authors**: Herman Bergström; Emil Carlsson; Devdatt Dubhashi; Fredrik D. Johansson
- **Reference count**: 40
- **Primary result**: GURO and hybrid variants outperform baselines on image and text ordering tasks with human feedback

## Executive Summary
This paper introduces GURO (Greedy Uncertainty Reduction for Ordering), an active preference learning algorithm that efficiently orders items based on pairwise comparisons. The method combines contextual logistic preference modeling with uncertainty sampling that balances aleatoric and epistemic uncertainty. A key contribution is deriving an upper bound on expected ordering error that motivates the GURO sampling criterion. The framework also includes a hybrid model with per-item parameters to handle model misspecification. Evaluated across diverse image and text datasets with human annotator feedback, GURO demonstrates superior sample efficiency, generalization to new items, and robustness compared to non-contextual ranking approaches and active preference learning baselines.

## Method Summary
The method uses a contextual logistic preference model where comparison probabilities follow p(Cij=1) = σ(θ⊤(xi-xj)) for items with attribute vectors xi. GURO actively selects comparison pairs to minimize an upper bound on expected ordering error by maximizing the product of aleatoric uncertainty (˙σ(z⊤ijθ)) and epistemic uncertainty (∥zij∥H⁻¹t(θt)). The Hessian is updated using the Sherman-Morrison formula after each comparison. A hybrid variant adds per-item parameters ζi to correct for model misspecification. BayesGURO provides a Bayesian interpretation by sampling pairs that maximize posterior variance. The framework supports both in-sample ordering and generalization to new items.

## Key Results
- GURO and BayesGURO achieve lower ordering error than TrueSkill and BALD across all tested datasets
- Hybrid variants outperform purely contextual models when initial features are noisy or incomplete
- GURO demonstrates superior out-of-sample generalization compared to non-contextual baselines
- The method maintains strong performance in continual learning settings with new items added to the pool

## Why This Works (Mechanism)

### Mechanism 1
GURO actively reduces the epistemic uncertainty in the model parameter θ in the direction of the attribute difference vectors zij between items. In each round, GURO selects the item pair (i, j) that maximizes the product of aleatoric uncertainty ˙σ(z⊤ijθt) and epistemic uncertainty ∥zij∥H⁻¹t(θt). By updating the Hessian using the Sherman-Morrison formula after each comparison, this selection minimizes the term maxᵢ,ⱼ ˙σ(z⊤ijθT)∥zij∥H⁻¹T(θT) in Theorem 1, directly reducing the upper bound on expected ordering error. Core assumption: The logistic preference model is sufficiently well-specified that minimizing this bound correlates with minimizing actual ordering error.

### Mechanism 2
The hybrid model fθ,ζ(xi, xj) = θ⊤(ϕ(xi) − ϕ(xj)) + (ζi − ζj) combines the benefits of contextual learning (fast convergence, generalization) with per-item parameters (robustness to misspecification). The contextual term θ⊤(ϕ(xi) − ϕ(xj)) provides a shared structure across items, enabling sample-efficient learning and out-of-sample generalization. The per-item term (ζi − ζj) corrects residual errors due to model misspecification or noise, allowing the model to fit the in-sample ordering even when ϕ is poor. Core assumption: The residual correction term ζi − ζj is small if the context captures most relevant information and the functional form is nearly well-specified.

### Mechanism 3
The Bayesian variant BayesGURO samples pairs to maximize the posterior variance of comparison probabilities, which under first-order Taylor expansion is equivalent to GURO's criterion. By sampling pairs that maximize ˆVθ|Dt−1[σ(θ⊤zij)], BayesGURO targets high-uncertainty comparisons under the current posterior, effectively reducing epistemic uncertainty in the directions that matter most for ordering. The equivalence to GURO under first-order Taylor expansion justifies its use as an alternative implementation. Core assumption: The Laplace approximation for the posterior variance is accurate enough that maximizing it approximates minimizing the upper bound on ordering error.

## Foundational Learning

- **Logistic regression and MLE**: The core model assumes comparisons follow a logistic model p(Cij = 1) = σ(θ⊤zij), and θt is estimated via MLE to fit the observed comparisons. Quick check: Given observations D = {(1,2,1), (2,3,0)} with z12 = (1,0)⊤ and z23 = (0,1)⊤, what is the MLE estimate θ̂ that maximizes the log-likelihood?

- **Fisher information and Hessian matrix**: The Hessian Ht(θ) = Pt s=1 ˙σ(z⊤sθ)zsz⊤s represents the observed Fisher information, used to quantify epistemic uncertainty and update the model after each comparison. Quick check: For a single comparison with z = (1,1)⊤ and c = 1, compute the Hessian H(θ) at θ = (0,0)⊤.

- **Aleatoric vs. epistemic uncertainty**: Aleatoric uncertainty (˙σ(z⊤ijθ)) captures inherent noise in comparisons, while epistemic uncertainty (∥zij∥H⁻¹t(θt)) captures uncertainty in the model parameter. GURO's criterion balances both. Quick check: For two item pairs with the same attribute difference z but different comparison outcomes, which has higher aleatoric uncertainty?

## Architecture Onboarding

- **Component map**: Attribute extraction -> Logistic preference model -> Active sampling (GURO/BayesGURO) -> Parameter update (MLE/MAP) -> Ordering error evaluation

- **Critical path**: Extract features from items → Initialize logistic model with contextual features → Apply GURO sampling to select informative pairs → Update model parameters via MLE/MAP → Evaluate ordering accuracy in-sample and out-of-sample

- **Design tradeoffs**: Fully contextual vs. hybrid model (contextual models are sample-efficient but sensitive to misspecification; hybrid models add per-item parameters for robustness at increased dimensionality cost); GURO vs. BayesGURO (GURO is deterministic and directly minimizes the bound; BayesGURO introduces stochasticity useful for batched algorithms); Computational complexity (O(n²) per iteration can be mitigated by subsampling m ≪ n² pairs)

- **Failure signatures**: Poor ordering error despite many comparisons (likely model misspecification or uninformative attributes); Slow convergence (high aleatoric uncertainty or insufficient epistemic uncertainty reduction); Overfitting (per-item parameters ζi capturing noise instead of correcting misspecification)

- **First 3 experiments**:
  1. Synthetic data: Generate 100 items with 10-dimensional attributes, simulate comparisons using a known θ∗, run GURO and baselines, compare ordering error.
  2. Image dataset: Use X-RayAge dataset, extract features with pre-trained DenseNet, run GURO Hybrid and compare to TrueSkill.
  3. Text dataset: Use WiscAds dataset, extract features with Sentence-BERT, evaluate in-sample and out-of-sample ordering error for GURO vs. BALD.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal fixed-budget sampling strategy for minimizing the expected ordering error, and how does it compare to the greedy GURO approach? Basis: The paper discusses limitations of current fixed-budget lower bounds and the need for constructing a tight algorithm-specific upper bound. Unresolved because the current upper bound is retrospective and not directly usable as a sampling criterion in the fixed-budget setting. Evidence: A rigorous proof of a lower bound on expected ordering error for any fixed-budget strategy, along with a matching upper bound for GURO, would demonstrate its optimality.

### Open Question 2
How does the performance of GURO and its variants change when applied to a streaming data setting where new items are continuously added to the pool? Basis: The paper includes limited experiments with new items but is constrained to offline data. Unresolved because current experiments don't capture true streaming challenges with concept drift. Evidence: Experiments in a realistic streaming setting with continuous item addition and changing distributions would provide insights into performance and limitations.

### Open Question 3
How can representation learning be incorporated into GURO to improve its performance when the initial feature representations are not optimal? Basis: The paper mentions that performance depends on feature quality and suggests updating representations during exploration. Unresolved because the paper doesn't explore learning representations during active learning. Evidence: Experiments comparing GURO with and without representation learning, using different initial features and varying data amounts, would demonstrate potential benefits.

## Limitations

- The framework assumes pairwise comparison outcomes follow a logistic model with contextual attributes, which may not hold for subjective judgments or when attributes don't capture all relevant information
- The hybrid model's per-item parameters increase dimensionality and may lead to overfitting when the number of items is large relative to comparisons
- The upper bound on ordering error (Theorem 1) is derived under model specification assumptions, and its tightness in real-world scenarios remains empirical

## Confidence

- **High confidence**: The core GURO algorithm's theoretical foundation and empirical superiority over non-contextual baselines
- **Medium confidence**: The hybrid model's effectiveness in mitigating misspecification, demonstrated on synthetic but needing more real-world validation
- **Medium confidence**: The Bayesian justification for BayesGURO through first-order Taylor expansion, relying on Laplace approximation accuracy

## Next Checks

1. **Robustness to attribute quality**: Systematically vary the informativeness of contextual attributes in synthetic experiments to quantify impact on ordering performance and identify failure thresholds

2. **Scalability analysis**: Evaluate computational efficiency and ordering accuracy as item count scales from hundreds to thousands, measuring per-iteration complexity and total comparisons needed

3. **Real-world deployment study**: Conduct a longitudinal study on one image/text dataset with sequential comparisons, measuring learning curves and adaptation to concept drift over time beyond just final accuracy