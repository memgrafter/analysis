---
ver: rpa2
title: Sequential Editing for Lifelong Training of Speech Recognition Models
arxiv_id: '2406.17935'
source_url: https://arxiv.org/abs/2406.17935
tags:
- data
- task
- learning
- editing
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Sequential Model Editing for lifelong training
  of speech recognition models, addressing the challenge of catastrophic forgetting
  when adapting models to new domains without access to prior datasets. The method
  involves fine-tuning the model on new data and then creating a task vector by taking
  the difference between the fine-tuned and original model parameters.
---

# Sequential Editing for Lifelong Training of Speech Recognition Models

## Quick Facts
- arXiv ID: 2406.17935
- Source URL: https://arxiv.org/abs/2406.17935
- Reference count: 0
- Key outcome: Sequential Model Editing achieves up to 15% WER reduction over fine-tuning baseline on CommonVoice English multi-accent dataset

## Executive Summary
This paper introduces Sequential Model Editing as a novel approach for lifelong training of speech recognition models without access to prior datasets. The method addresses catastrophic forgetting by storing domain-specific updates as parameter differences rather than overwriting original parameters. Through experiments on the CommonVoice English multi-accent dataset, the approach demonstrates superior performance compared to traditional fine-tuning, achieving up to 15% WER reduction. The TIES-Merging variant, which retains only the top-k% values in the task vector, further improves performance by reducing interference from low-magnitude components.

## Method Summary
Sequential Model Editing works by fine-tuning the model on new data and creating a task vector through the difference between the fine-tuned and original model parameters. This task vector is then merged back into the original model using a scaling factor λ. The approach preserves the original model parameters while applying a scaled difference that captures domain-specific information. The TIES-Merging variant enhances this by retaining only the top-k% magnitude values in the task vector, reducing noise from low-magnitude components that may cause interference between tasks. The method is evaluated on the CommonVoice English multi-accent dataset, showing significant improvements over fine-tuning baselines.

## Key Results
- Task Arithmetic with λ=0.4 achieves 9.1% WER reduction over fine-tuning baseline
- TIES-Merging with λ=0.6 and k=0.5 achieves 15% WER reduction over fine-tuning baseline
- Sequential editing maintains performance across multiple domains without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential Model Editing reduces catastrophic forgetting by storing domain-specific updates as parameter differences rather than overwriting original parameters.
- Mechanism: The method computes a task vector τ_t = θ̂_t - θ_t-1, which captures the difference between the fine-tuned model on new data and the original model. This task vector is then merged back into the original model using a scaling factor λ. By keeping the original model intact and only applying a scaled difference, the approach preserves knowledge from previous domains.
- Core assumption: The task vector effectively encodes domain-specific information without interfering with previously learned knowledge when merged with the original model.
- Evidence anchors: [abstract] "We propose Sequential Model Editing as a novel method to continually learn new domains in ASR systems. Different than previous methods, our approach does not necessitate access to prior datasets or the introduction of extra parameters."

### Mechanism 2
- Claim: TIES-Merging improves upon basic Task Arithmetic by reducing interference from low-magnitude task vector components.
- Mechanism: The TIES-Merging procedure retains only the top-k% values in the task vector based on magnitude, while setting the bottom (100-k)% to zero. This reduces noise from small parameter changes that may cause interference between tasks.
- Core assumption: Low-magnitude values in the task vector are more likely to cause interference between tasks, while high-magnitude values represent more important domain-specific information.
- Evidence anchors: [abstract] "The TIES-Merging variant, which retains only the top-k% values in the task vector, performs better than the basic Task Arithmetic method."

### Mechanism 3
- Claim: Sequential editing maintains performance across multiple domains by treating each domain update as an additive adjustment rather than a replacement.
- Mechanism: At each time step t, the model parameters θ_t are updated as θ_t = θ_t-1 + λ · (τ_t). This additive approach ensures that each domain's contribution is preserved as a distinct adjustment to the original model, rather than being overwritten by subsequent updates.
- Core assumption: Parameter space is approximately linear for domain adaptation, allowing domain-specific updates to be represented as additive vectors.
- Evidence anchors: [section] "θ_t = θ_t-1 + λ · (τ_t)" and "This additive approach ensures that each domain's contribution is preserved as a distinct adjustment to the original model, rather than being overwritten by subsequent updates."

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: The paper explicitly addresses catastrophic forgetting as the core problem being solved. Understanding this concept is essential to grasp why sequential editing is necessary.
  - Quick check question: What happens to a neural network's performance on previous tasks when it is fine-tuned on a new task without any special techniques?

- Concept: Task Vectors and Model Editing
  - Why needed here: The entire method is built on the concept of task vectors as representations of domain-specific adjustments. Without understanding this, the proposed approach cannot be implemented.
  - Quick check question: How is a task vector defined in the context of model editing, and what information does it capture?

- Concept: Parameter Space and Linear Combinations
  - Why needed here: The approach assumes that domain adaptations can be represented as linear combinations in parameter space. This mathematical foundation is crucial for understanding why the method works.
  - Quick check question: Why might it be reasonable to assume that domain-specific adaptations can be represented as linear combinations of model parameters?

## Architecture Onboarding

- Component map:
  Base Conformer-CTC ASR model (12 layers, 8 attention heads) -> Task vector computation module (parameter difference calculator) -> TIES-Merging module (top-k% value selector) -> Scaling factor λ (hyperparameter controller) -> Evaluation pipeline (WER calculation on multiple accents)

- Critical path:
  1. Initialize base model θ_0 on D_0 (US accent)
  2. For each new domain t:
     - Fine-tune θ_t-1 on D_t to get θ̂_t
     - Compute task vector τ_t = θ̂_t - θ_t-1
     - Apply TIES-Merging if enabled
     - Update θ_t = θ_t-1 + λ · τ_t
     - Evaluate on all seen domains

- Design tradeoffs:
  - λ = 1 gives maximum adaptation but causes catastrophic forgetting
  - λ = 0 preserves original model but prevents learning
  - k = 100% in TIES-Merging preserves all information but may include noise
  - k = 0% in TIES-Merging prevents interference but loses all information

- Failure signatures:
  - Performance degradation on previously seen domains indicates catastrophic forgetting
  - Poor performance on new domains indicates insufficient adaptation
  - No improvement over fine-tuning baseline suggests the task vector mechanism isn't working
  - Inconsistent WER across different accents suggests domain interference

- First 3 experiments:
  1. Implement Task Arithmetic with λ = 0.4 and verify 9.1% WERR over fine-tuning on the CommonVoice dataset
  2. Add TIES-Merging with k = 50% and λ = 0.6, verify 15% WERR over fine-tuning
  3. Test different λ values (0.2, 0.4, 0.6, 0.8) on a single domain addition to find optimal scaling factor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal scaling factor λ vary across different sequential steps and tasks in Sequential Model Editing?
- Basis in paper: [explicit] The paper notes that λ = 0.4 was used for Task Arithmetic and λ = 0.6 with k = 0.5 for TIES-Merging, but found through ablation that λ = 0.2 was optimal for t = 2 in Task Arithmetic.
- Why unresolved: The study used fixed values of λ and k across all stages, leaving the potential for improved performance by adapting these parameters for each stage or task unexplored.
- What evidence would resolve it: Experiments varying λ and k at each sequential step, comparing WER performance across different values to determine optimal settings for each stage.

### Open Question 2
- Question: What are the theoretical foundations that make model editing effective for lifelong learning in ASR?
- Basis in paper: [inferred] The paper demonstrates superior performance of model editing over baselines but does not provide theoretical analysis of why this approach works.
- Why unresolved: While empirical results show effectiveness, the paper does not explore the underlying mechanisms that enable model editing to mitigate catastrophic forgetting.
- What evidence would resolve it: Theoretical analysis or experiments examining how task vectors capture task-specific information and how their integration preserves previous knowledge while acquiring new capabilities.

### Open Question 3
- Question: How do recently proposed editing techniques like Drop And REscale and Soft Merging of Experts compare to the Task Arithmetic and TIES-Merging approaches?
- Basis in paper: [explicit] The paper mentions these techniques as future work to explore, noting their potential for further enhancements.
- Why unresolved: The study focused on Task Arithmetic and TIES-Merging but did not evaluate newer editing methods that have been proposed since.
- What evidence would resolve it: Direct comparison of these newer editing techniques with the methods used in the study, measuring WER performance and efficiency in the lifelong learning setting.

## Limitations

- Performance heavily depends on the scaling factor λ, which requires careful tuning and may not generalize across different datasets
- TIES-Merging procedure lacks detailed implementation specifications, particularly for handling sign conflicts and small weights
- Evaluation is limited to English accents on CommonVoice dataset, raising questions about applicability to other languages or acoustic conditions

## Confidence

- **High Confidence**: The core mechanism of using task vectors to prevent catastrophic forgetting is well-supported by the theoretical framework and basic experimental results. The 9.1% WER reduction for Task Arithmetic over fine-tuning baseline is a robust finding.
- **Medium Confidence**: The TIES-Merging variant's superiority (15% WER reduction) is supported by experimental results but relies on implementation details that are not fully specified in the paper. The optimal scaling factors (λ=0.4 and λ=0.6) are presented without systematic exploration of the hyperparameter space.
- **Low Confidence**: Claims about the method's generalization to other languages, model architectures, or more diverse acoustic conditions are not supported by the experimental evidence, which is limited to English accents on CommonVoice data.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the scaling factor λ (0.1 to 0.9 in increments of 0.1) and the TIES-Merging retention rate k (10% to 90% in increments of 10%) to identify the robustness of performance improvements across different settings and to validate the suggested optimal values.

2. **Cross-Dataset Generalization**: Implement the method on a different speech recognition dataset (e.g., LibriSpeech or TED-LIUM) with a different language or more diverse acoustic conditions to assess whether the 9-15% WER reduction holds beyond the CommonVoice English accents.

3. **Computational Overhead Measurement**: Measure the actual memory usage and inference latency when storing task vectors for multiple domains (5-10+) to quantify the trade-off between improved performance and increased resource requirements, particularly for deployment scenarios.