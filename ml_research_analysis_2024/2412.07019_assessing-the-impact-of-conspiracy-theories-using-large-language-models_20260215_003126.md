---
ver: rpa2
title: Assessing the Impact of Conspiracy Theories Using Large Language Models
arxiv_id: '2412.07019'
source_url: https://arxiv.org/abs/2412.07019
tags:
- llms
- impact
- assessment
- bias
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel approach to assessing the impact
  of conspiracy theories using large language models (LLMs). By leveraging human-annotated
  datasets and simulating human-like reasoning processes, the research demonstrates
  that LLMs can effectively evaluate CT impacts through tailored prompting strategies.
---

# Assessing the Impact of Conspiracy Theories Using Large Language Models

## Quick Facts
- arXiv ID: 2412.07019
- Source URL: https://arxiv.org/abs/2412.07019
- Reference count: 40
- LLMs can assess CT impact through multi-agent debating and iterative reasoning, achieving Spearman's correlation up to 0.62

## Executive Summary
This study introduces a novel approach to assessing conspiracy theory (CT) impact using large language models (LLMs). By leveraging human-annotated datasets and simulating human-like reasoning processes, the research demonstrates that LLMs can effectively evaluate CT impacts through tailored prompting strategies. The most effective method combines multi-agent debating with iterative reasoning, achieving strong correlation with human assessments. However, LLMs exhibit significant biases including positional preference and sensitivity to wording and verbosity that can degrade performance.

## Method Summary
The study collected 12 CTs from a YouGov survey with human-annotated impact rankings based on 1,000 U.S. adults. The researchers created bias-augmented variants (position, wording, verbosity) and tested five prompting strategies across eight LLMs ranging from 8B to 70B+ parameters: Vanilla Ranking, Chain-of-Thought, Pairwise Comparison, Individual Scoring, and Multi-Agent Debating. Performance was evaluated using Spearman's rank correlation, Kendall's Tau, and nDCG metrics compared to human-annotated rankings.

## Key Results
- Multi-agent debating combined with iterative reasoning achieved Spearman's correlation up to 0.62 and Kendall's Tau of 0.51
- Larger LLMs (>70B parameters) significantly outperformed smaller counterparts across all metrics and strategies
- LLMs showed strong positional bias, ranking CTs appearing earlier in prompts higher regardless of actual impact
- Smaller LLMs benefited most from structured prompting, particularly multi-agent debating frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-step reasoning with multi-agent debating improves LLM accuracy by simulating human-like deliberation
- Mechanism: Multiple specialized agents (affirmative debater, negative debater, moderator) generate, critique, and refine impact rankings through iterative debate
- Core assumption: LLMs can effectively simulate distinct reasoning roles and improve outputs through adversarial interaction
- Evidence anchors: Abstract states multi-step reasoning produces accurate results; section 4.3 shows multi-agent debating significantly enhances performance for smaller LLMs

### Mechanism 2
- Claim: Structured prompting strategies enable deliberate slow thinking that outperforms instinctive fast thinking
- Mechanism: Forcing models to generate intermediate reasoning steps or compare CTs pairwise engages systematic analysis rather than heuristic judgments
- Core assumption: LLMs possess latent reasoning capabilities that can be activated through explicit prompting
- Evidence anchors: Abstract mentions multi-step reasoning; section 5.3 shows slow thinking mode improves performance particularly for smaller LLMs

### Mechanism 3
- Claim: LLM size correlates with CT impact assessment capability due to more extensive knowledge bases
- Mechanism: Larger LLMs demonstrate superior performance because they can draw from broader training data and maintain more complex reasoning chains
- Core assumption: Model capacity directly translates to better performance on complex social reasoning tasks
- Evidence anchors: Section 5.2 shows larger LLMs dominate across all metrics; section 5.3 confirms larger LLMs achieve significantly better results

## Foundational Learning

- Concept: Spearman's Rank Correlation
  - Why needed here: Evaluates how well LLM rankings match human-annotated ground truth rankings
  - Quick check question: What value of Spearman's correlation indicates perfect agreement between two rankings?

- Concept: Multi-agent debating framework
  - Why needed here: Enables systematic reasoning through adversarial and collaborative interactions among specialized agents
  - Quick check question: What are the three roles in the multi-agent debating framework described?

- Concept: Prompting bias types
  - Why needed here: Identifies systematic errors in LLM output based on input formatting and presentation
  - Quick check question: Which prompting bias causes LLMs to assign higher rankings to CTs appearing earlier in the prompt?

## Architecture Onboarding

- Component map:
  Survey data → Bias augmentation (position, wording, verbosity) → Human-annotated ground truth
  Multiple model sizes (8B, 70B+) → Prompt strategy selector → Output formatter
  Ranking metrics (Spearman, Kendall, nDCG) → Bias impact analysis

- Critical path:
  Input CT list → Apply prompt strategy → Generate LLM ranking → Compare to ground truth → Calculate metrics → Analyze bias impact

- Design tradeoffs:
  - Model size vs. cost: Larger models perform better but are more expensive to run
  - Bias mitigation vs. complexity: More sophisticated bias handling increases system complexity
  - Prompt strategy vs. generalizability: Specialized strategies may not transfer well to new domains

- Failure signatures:
  - Position bias: Rankings correlate strongly with input order
  - Wording sensitivity: Different phrasings of same CT produce different rankings
  - Verbosity confusion: Addition of irrelevant details significantly changes output

- First 3 experiments:
  1. Run all eight LLMs with Vanilla Ranking on original dataset to establish baseline performance
  2. Test position bias by shuffling CT order and measuring performance degradation across models
  3. Compare CoT vs. Vanilla Ranking performance for each model to quantify reasoning strategy impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified biases (position, wording, verbosity) interact with each other during conspiracy theory impact assessment?
- Basis in paper: The paper identifies three distinct biases and notes their individual effects, but does not explore their potential interactions or compounding effects
- Why unresolved: The experimental design tested each bias separately, leaving their combined or interactive effects unexplored
- What evidence would resolve it: Experiments systematically varying multiple biases simultaneously would reveal whether they amplify, cancel, or create new patterns of error in LLM assessments

### Open Question 2
- Question: Can smaller LLMs be effectively fine-tuned or augmented to match the performance of larger models in conspiracy theory impact assessment?
- Basis in paper: The paper notes that smaller LLMs perform significantly worse than larger models, with Multi-Agent Debating being the most effective strategy for them, but leaves open whether additional techniques could close the performance gap
- Why unresolved: The study only tested prompting strategies and did not explore fine-tuning, retrieval augmentation, or other architectural improvements for smaller models
- What evidence would resolve it: Comparative experiments showing performance improvements in smaller models through fine-tuning on conspiracy theory datasets or through retrieval-augmented approaches would address this question

### Open Question 3
- Question: How generalizable are the findings to conspiracy theories outside the United States or to newly emerging conspiracy theories?
- Basis in paper: The paper acknowledges that its dataset focuses on US conspiracy theories and does not include emerging theories, limiting generalizability
- Why unresolved: The study used a specific YouGov survey dataset and did not test cross-cultural applicability or temporal generalizability to new conspiracy theories
- What evidence would resolve it: Testing the same prompting strategies and LLM models on conspiracy theories from different cultural contexts or on newly emerging conspiracy theories would demonstrate generalizability

## Limitations

- Study is based on a relatively small dataset of 12 CTs, limiting generalizability to broader conspiracy theory landscape
- Model performance may degrade when assessing CTs outside the specific demographic context (U.S. adults) used in training data
- Multi-agent debating framework requires substantial computational resources and careful implementation, potentially limiting scalability

## Confidence

- **High Confidence**: Identification of prompting biases and their impact on LLM performance - well-supported by systematic testing across multiple models and variants
- **Medium Confidence**: Superiority of larger LLMs (>70B parameters) for CT impact assessment - while demonstrated, may be partially attributable to better instruction-following rather than genuine reasoning capability
- **Medium Confidence**: Effectiveness of multi-agent debating framework - shown to improve performance but requires significant computational overhead and may not scale efficiently

## Next Checks

1. **Bias Robustness Testing**: Test proposed bias mitigation strategies (randomized position prompts, neutral wording templates, relevance filters) across larger, more diverse CT datasets to verify effectiveness and identify new emergent biases

2. **Cross-Demographic Validation**: Evaluate model performance on CT datasets from different geographic regions and demographic groups to assess generalizability beyond U.S. adult population

3. **Computational Efficiency Analysis**: Compare performance-cost tradeoff between multi-agent debating framework and simpler prompting strategies across varying model sizes to determine optimal configurations for real-world deployment