---
ver: rpa2
title: 'Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability
  is Complicated'
arxiv_id: '2406.18259'
source_url: https://arxiv.org/abs/2406.18259
tags:
- human
- text
- texts
- detectors
- gptzero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study challenges the traditional binary classification of\
  \ machine-generated text detection by introducing a ternary classification scheme\
  \ that includes an \u201Cundecided\u201D category for ambiguous cases. Through experiments\
  \ on four new datasets spanning multiple state-of-the-art LLMs and human-authored\
  \ texts, the researchers identified GPTZero, Sapling, and Binoculars as the top-performing\
  \ detectors."
---

# Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability is Complicated

## Quick Facts
- arXiv ID: 2406.18259
- Source URL: https://arxiv.org/abs/2406.18259
- Reference count: 10
- This study challenges binary classification by introducing ternary classification (human, machine, undecided) and shows current detectors struggle with ambiguous texts.

## Executive Summary
This research challenges the traditional binary approach to detecting machine-generated text by introducing a ternary classification framework that includes an "undecided" category for ambiguous cases. Through experiments on four new datasets spanning multiple state-of-the-art LLMs and human-authored texts, the researchers identified GPTZero, Sapling, and Binoculars as the top-performing detectors. The study reveals that current binary detectors systematically misclassify ambiguous texts as machine-generated, highlighting the need for improved explainability in detection systems. Human annotators provided richer, more interpretable explanations than automated detector metrics, suggesting that future detector designs should prioritize natural language explanations over abstract statistical measures.

## Method Summary
The researchers collected four custom datasets containing texts from GPT-4o, Gemini Pro, LLaMA3.3-70B, Qwen2-72B, and human sources. They conducted binary classification experiments using multiple detectors including GPTZero, Sapling, and Binoculars, then implemented a ternary classification scheme through human annotation. Human annotators classified texts into human, machine, or undecided categories while providing qualitative explanations based on linguistic fluency, stylistic tone, structure, content depth, personal elements, and bias. The study compared detector performance across binary and ternary frameworks and analyzed the interpretability of automated explainability metrics versus human explanations.

## Key Results
- GPTZero, Sapling, and Binoculars emerged as the top-performing detectors for machine-generated text classification
- Current binary detectors systematically misclassify ambiguous texts as machine-generated, with significant performance degradation in ternary classification
- Human explanations based on linguistic fluency, stylistic tone, structure, content depth, personal elements, and bias provide more interpretable insights than abstract detector metrics like perplexity and burstiness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ternary classification improves detection explainability by explicitly accounting for ambiguous cases that binary classifiers mislabel.
- Mechanism: By adding an "undecided" category, detectors are forced to acknowledge uncertainty rather than force a false binary decision, allowing human users to better interpret borderline cases.
- Core assumption: Ambiguous texts share overlapping features of both human and machine generation, making binary classification inherently lossy for such cases.
- Evidence anchors:
  - [abstract] "This research shifts the paradigm from merely classifying to explaining machine-generated texts, emphasizing need for detectors to provide clear and understandable explanations to users."
  - [section] "Our results highlight why 'undecided' category is much needed from the viewpoint of explainability."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.479, average citations=0.0. Top related titles: Exploring the Limitations of Detecting Machine-Generated Text.
- Break condition: If the cost of maintaining an additional class outweighs the benefit of improved explainability, or if the dataset lacks sufficient ambiguous examples to justify the third class.

### Mechanism 2
- Claim: Human annotators provide richer, more interpretable explanations than automated detector metrics.
- Mechanism: Human explanations categorize texts using linguistic fluency, stylistic tone, structural patterns, content depth, personal elements, and bias—features that align with intuitive understanding of authorship.
- Core assumption: Human perception of text characteristics is more aligned with end-user needs than abstract statistical metrics.
- Evidence anchors:
  - [section] "The human annotation results revealed that, although some automated MGT detectors have achieved very good performance in predicting ground truth labels, human annotators were clearly not convinced by the cases falling into the 'undecided' category."
  - [section] "Human annotators' explanation notes are predominantly qualitative, yet quantitative measures can also be applied, particularly for aspects like spelling and grammatical errors, perplexity, and readability."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.479, average citations=0.0.
- Break condition: If automated systems can generate explanations that match human interpretability or if human explanations are too subjective for scalable deployment.

### Mechanism 3
- Claim: Current detectors' explainability metrics (e.g., GPTZero's six features) are insufficiently interpretable for lay users.
- Mechanism: Abstract metrics like "readability," "perplexity," and "burstiness" do not convey why a text is classified as machine- or human-generated in human-understandable terms.
- Core assumption: End users require explanations in natural language categories rather than statistical abstractions.
- Evidence anchors:
  - [section] "The metrics used by GPTZero has limited explanatory power because they are too abstract. For instance, all the six metrics are marked as 'Medium', which does not explain why the final judgment is AI."
  - [section] "Table 4 shows an example, comparing the six explainability metrics used by GPTZero and the explanation notes given by our human annotators."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.479, average citations=0.0.
- Break condition: If future detector designs incorporate natural language explanations or if users become trained to interpret statistical metrics.

## Foundational Learning

- Concept: Ternary classification framework
  - Why needed here: To handle ambiguous texts that cannot be confidently classified as purely human or machine-generated, improving both detection accuracy and user trust.
  - Quick check question: What are the three categories in the ternary classification system and when should each be used?

- Concept: Explainability in machine learning models
  - Why needed here: To bridge the gap between model predictions and user understanding, especially for borderline cases where automated decisions may be questionable.
  - Quick check question: Why are abstract metrics like perplexity and burstiness less useful for end users compared to human-readable explanations?

- Concept: Confusion matrix analysis in multi-class classification
  - Why needed here: To evaluate how detectors perform across all three classes and identify biases, particularly the tendency to misclassify "undecided" texts as machine-generated.
  - Quick check question: How does the confusion matrix reveal detector bias toward classifying ambiguous texts as machine-generated?

## Architecture Onboarding

- Component map: Text → Feature extraction → Detector prediction → Human annotation (for ground truth) → Ternary classification evaluation → Explainability analysis
- Critical path: Text → Feature extraction → Detector prediction → Human annotation (for ground truth) → Ternary classification evaluation → Explainability analysis
- Design tradeoffs: Binary classification offers simplicity and established metrics, while ternary classification improves explainability at the cost of increased complexity and potential class imbalance
- Failure signatures: High misclassification rates for "undecided" texts, detectors showing consistent bias toward machine classification for ambiguous cases, or human annotators failing to reach consensus
- First 3 experiments:
  1. Implement ternary classification evaluation using the existing four datasets to measure performance impact of the "undecided" category
  2. Compare detector explainability by mapping abstract metrics to human-readable categories using the annotated dataset
  3. Test whether incorporating human explanation categories as features improves detector performance on ambiguous texts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the explainability of machine-generated text detectors be significantly improved to provide more human-understandable explanations?
- Basis in paper: [explicit] The paper discusses the limitations of current detectors' explainability and emphasizes the need for enhanced explainability in MGT detectors to improve user trust.
- Why unresolved: While the paper identifies the need for improved explainability, it does not provide a detailed methodology or framework for achieving this enhancement.
- What evidence would resolve it: Development and testing of new explainability techniques that significantly improve user understanding and trust in detector outputs.

### Open Question 2
- Question: What are the specific criteria that can be established to better distinguish between human-generated texts, machine-generated texts, and undecided texts in the ternary classification framework?
- Basis in paper: [inferred] The paper introduces a ternary classification scheme but acknowledges the difficulty in establishing precise criteria for distinguishing between the three categories.
- Why unresolved: The current criteria for classification are not sufficiently detailed or objective, leading to ambiguity in classification.
- What evidence would resolve it: Development of a comprehensive set of objective criteria or a scoring system that can be consistently applied to classify texts into the three categories.

### Open Question 3
- Question: How can detectors be designed to handle the "capybara problem," where both prompts and responses with high perplexity can lead to misjudgments about text origin?
- Basis in paper: [explicit] The paper mentions the "capybara problem" and suggests that addressing it involves creating prompts that encourage LLMs to produce features typical of HGTs.
- Why unresolved: The paper does not provide a detailed solution or methodology for addressing this issue in detector design.
- What evidence would resolve it: Successful implementation and testing of detector designs that can accurately classify texts with high perplexity, regardless of their origin.

## Limitations

- The study's datasets were specifically constructed to include ambiguous texts, which may not reflect real-world distribution where such cases are rarer
- Human annotation process may suffer from group conformity effects rather than genuine agreement, despite showing near-complete consensus after discussion
- Explainability analysis relies on qualitative human annotations that remain inherently subjective without quantitative validation measures

## Confidence

- High confidence: The core finding that current binary detectors struggle with ambiguous texts (supported by confusion matrix analysis showing systematic misclassification of "undecided" cases)
- Medium confidence: The superiority of ternary classification for explainability (limited by dataset selection bias toward ambiguous examples)
- Medium confidence: Human explanations provide better interpretability than detector metrics (supported by qualitative examples but lacking quantitative validation)
- Low confidence: Generalizability of results across different domains and text types (only tested on academic/technical writing samples)

## Next Checks

1. **External Dataset Validation**: Test the ternary classification approach on real-world corpora with naturally occurring ambiguous texts rather than artificially constructed datasets to verify robustness.
2. **Quantitative Explainability Assessment**: Develop and apply statistical measures comparing human versus detector explanations, such as inter-annotator agreement scores and automated semantic similarity metrics.
3. **User Study Implementation**: Conduct controlled experiments with end-users to measure whether ternary classification and human-style explanations actually improve user trust and decision-making compared to binary classification outputs.