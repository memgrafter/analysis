---
ver: rpa2
title: Reconfigurable Intelligent Surface Assisted VEC Based on Multi-Agent Reinforcement
  Learning
arxiv_id: '2406.11318'
source_url: https://arxiv.org/abs/2406.11318
tags:
- uni00000003
- uni00000052
- uni00000048
- uni00000013
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses joint optimization of offloading power, local
  power allocation, and RIS phase-shift in RIS-assisted VEC networks to minimize power
  consumption and buffer length. It proposes a two-stage approach: first using BCD
  to optimize RIS phase-shifts, then employing a modified MADDPG algorithm for multi-agent
  power allocation.'
---

# Reconfigurable Intelligent Surface Assisted VEC Based on Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.11318
- Source URL: https://arxiv.org/abs/2406.11318
- Reference count: 15
- The paper addresses joint optimization of offloading power, local power allocation, and RIS phase-shift in RIS-assisted VEC networks to minimize power consumption and buffer length.

## Executive Summary
This paper proposes a two-stage approach to optimize power allocation and RIS phase-shift in vehicular edge computing (VEC) networks. The solution combines block coordinate descent (BCD) for RIS phase-shift optimization with a modified multi-agent deep deterministic policy gradient (MADDPG) algorithm for power allocation. The framework achieves superior performance compared to centralized DDPG and random schemes across varying task arrival rates and RIS element counts.

## Method Summary
The proposed method employs a two-stage optimization approach: first using BCD to optimize RIS phase-shifts analytically under fixed communication channels, then employing a modified MADDPG algorithm for multi-agent power allocation. The BCD algorithm exploits the slower time-scale of RIS phase-shift optimization, while MADDPG handles the stochastic multi-agent power allocation problem. The modified MADDPG incorporates twin-delay deterministic policy gradient to improve stability and uses shared experience replay across agents.

## Key Results
- The joint BCD and modified MADDPG approach outperforms centralized DDPG and random schemes
- Achieves better convergence stability, lower power consumption, and reduced buffer length
- Demonstrates effectiveness across varying task arrival rates and RIS element counts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint BCD and modified MADDPG decompose a non-convex VEC optimization into tractable subproblems.
- **Mechanism:** The BCD algorithm optimizes RIS phase-shifts analytically under fixed communication channels, then MADDPG handles the stochastic multi-agent power allocation. This separation exploits the slower time-scale of RIS phase-shift vs. faster time-scale of power control.
- **Core assumption:** Channel states remain quasi-static within a time slot; RIS phase-shifts change slower than power allocation.

### Mechanism 2
- **Claim:** Twin-delay deterministic policy gradient in MADDPG improves stability over vanilla MADDPG.
- **Mechanism:** Twin critics (Q1, Q2) reduce overestimation bias; delayed target updates dampen oscillations; global reward plus per-agent local rewards balance cooperation and individual performance.
- **Core assumption:** Overestimation bias is significant in multi-agent scenarios; individual reward signals are insufficient alone.

### Mechanism 3
- **Claim:** Experience replay across agents improves sample efficiency and stability.
- **Mechanism:** Agents share transitions in a common replay buffer, enabling each to learn from others' experiences, reducing non-stationarity.
- **Core assumption:** Agents' actions affect each other's environment states, so learning from other agents' transitions helps predict others' behavior.

## Foundational Learning

- **Concept:** Non-convex optimization in wireless networks
  - Why needed here: The joint RIS phase-shift and power allocation problem is non-convex; understanding convex relaxations, BCD, and alternating optimization is essential.
  - Quick check question: What guarantees does BCD provide for convergence in non-convex settings?

- **Concept:** Multi-agent reinforcement learning (MARL)
  - Why needed here: Multiple vehicles act as agents; MARL handles decentralized decision-making with partial observability and potential conflicts.
  - Quick check question: How does the centralized critic in MADDPG overcome non-stationarity in multi-agent environments?

- **Concept:** Reconfigurable intelligent surfaces (RIS) channel modeling
  - Why needed here: RIS phase-shift directly affects channel gain; accurate modeling of LoS, path loss, and Rician fading is needed for realistic simulations.
  - Quick check question: What is the impact of discrete phase-shift resolution on achievable beamforming gain?

## Architecture Onboarding

- **Component map:** Environment simulator -> BCD optimizer -> MARL agents -> Replay buffer -> Twin global critic networks -> Local critics and actors -> Target networks
- **Critical path:** At each time slot → Environment generates state → BCD computes optimal RIS phase-shift → Agents observe states → Agents act (power allocation) → Rewards computed → Transitions stored → MARL updates networks
- **Design tradeoffs:**
  - Joint BCD+MADDPG vs. fully centralized DDPG: Joint approach exploits slow RIS dynamics, reduces dimensionality, improves scalability.
  - Twin critics vs. single critic: Twin critics reduce overestimation bias but double computation.
  - Shared replay vs. per-agent replay: Shared replay improves sample efficiency but may mix non-i.i.d. transitions.
- **Failure signatures:**
  - BCD failure: If channel estimates are poor, BCD produces sub-optimal phase-shifts, degrading communication.
  - MARL failure: If rewards are not well-shaped (too sparse), agents may not learn meaningful policies.
  - MARL instability: Oscillating rewards or divergence of actor/critic losses indicate learning instability.
- **First 3 experiments:**
  1. **BCD ablation:** Compare reward curves with and without BCD optimization to quantify RIS phase-shift contribution.
  2. **Twin critic ablation:** Run with single critic to see if overestimation bias hurts performance.
  3. **Replay buffer size sweep:** Vary buffer size to find sweet spot for sample efficiency vs. non-stationarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed MARL with BCD scheme scale with the number of VUs (K) in large-scale vehicular networks?
- Basis in paper: The paper states "in scenarios with multiple VUs, the centralized algorithms do not have advantages any more" and demonstrates performance with K=8 VUs, but doesn't explore scaling beyond this.
- Why unresolved: The paper only evaluates performance for K=8 VUs and doesn't provide analysis of how the scheme performs as the number of VUs increases significantly.
- What evidence would resolve it: Systematic experiments varying K from small to large values (e.g., 10, 50, 100) showing reward convergence, power consumption, and buffer length trends, along with computational complexity analysis.

### Open Question 2
- Question: How robust is the proposed scheme to dynamic channel conditions and vehicle mobility patterns that change rapidly between time slots?
- Basis in paper: The paper assumes "quasi-static scenarios where the channel conditions keep constant within a time slot but may change between different time slots" but doesn't evaluate performance under rapidly varying conditions.
- Why unresolved: The model assumes channel stability within time slots, but real-world vehicular scenarios involve rapid changes that could affect learning stability and performance.
- What evidence would resolve it: Experiments with rapidly varying channel conditions, different mobility patterns (high-speed vehicles, frequent direction changes), and comparison of performance degradation compared to the quasi-static assumption.

### Open Question 3
- Question: What is the impact of different RIS phase-shift quantization levels (parameter b) on the trade-off between hardware complexity and system performance?
- Basis in paper: The paper mentions "phase-shift can only be selected from a finite discrete value set" and uses b=3 in experiments, but doesn't explore how performance changes with different b values.
- Why unresolved: While b=3 is used in simulations, the relationship between quantization granularity, hardware cost, and performance isn't quantified or optimized.
- What evidence would resolve it: Systematic experiments varying b (e.g., 1, 2, 3, 4, 5) showing the performance-cost trade-off, along with analysis of how coarse quantization affects the BCD algorithm's effectiveness and overall system performance.

## Limitations
- Performance scaling with increasing number of vehicles remains unexplored
- Limited evaluation of robustness to rapidly changing channel conditions
- Impact of different RIS phase-shift quantization levels on hardware complexity vs. performance trade-off not quantified

## Confidence

**High:** BCD algorithm effectiveness in optimizing RIS phase-shifts
**Medium:** Twin-delayed MADDPG improvement over standard MADDPG
**Low:** Real-world scalability to dynamic channel conditions

## Next Checks

1. **Channel Dynamics Stress Test:** Vary vehicle speeds and task arrival rates to test BCD stability under rapid channel changes
2. **RIS Element Sensitivity:** Sweep RIS element count to quantify beamforming gain scaling
3. **MARL Transferability:** Test trained agents on unseen vehicle distributions to assess generalization capability