---
ver: rpa2
title: An Exploration of Pattern Mining with ChatGPT
arxiv_id: '2412.16814'
source_url: https://arxiv.org/abs/2412.16814
tags:
- pattern
- data
- patterns
- llms
- ordances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using ChatGPT for pattern mining, proposing
  an eight-step collaborative process between human experts and AI to extract patterns
  from known uses. The method is demonstrated by creating a pattern language for integrating
  LLMs with data sources and tools, adding affordances as a new pattern description
  element.
---

# An Exploration of Pattern Mining with ChatGPT

## Quick Facts
- arXiv ID: 2412.16814
- Source URL: https://arxiv.org/abs/2412.16814
- Reference count: 15
- Result: ChatGPT successfully extracted seven patterns from three application scenarios for LLM integrations

## Executive Summary
This paper explores using ChatGPT for pattern mining through a collaborative process between human experts and AI. The author proposes an eight-step method where human experts provide application scenarios and guide ChatGPT to extract solutions, problems, and affordances, then manually refine the resulting patterns. The approach is demonstrated by creating a pattern language for integrating LLMs with data sources and tools, successfully extracting seven patterns from three examples. While promising, the method requires human refinement for quality and faces scalability challenges due to context limitations.

## Method Summary
The method involves a human expert working iteratively with ChatGPT through eight steps: identifying application scenarios, extracting recurring solutions, defining problems, distilling patterns in Alexandrian format, identifying affordances of components, relating patterns to affordances, exploring dependencies, and consolidating the pattern language. The process leverages ChatGPT's ability to recognize structural similarities in narrative descriptions and generate initial pattern elements, which the human expert then refines for accuracy, terminology, and completeness.

## Key Results
- ChatGPT successfully extracted seven distinct patterns from three application scenarios
- Human refinement was essential for improving pattern names, context descriptions, and terminology
- Adding affordances as pattern elements provided clearer explanations of why patterns work
- The collaborative process produced a coherent pattern language for LLM integration scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can identify recurring solutions across application scenarios through pattern recognition on narrative descriptions
- Mechanism: When provided with multiple application scenarios describing how LLMs, data sources, and tools are integrated, ChatGPT uses its training on large text corpora to identify common solution patterns by recognizing structural similarities in the described approaches
- Core assumption: The narrative descriptions of application scenarios contain sufficient structural information about solutions that can be abstracted into patterns
- Evidence anchors:
  - [abstract] "ChatGPT successfully extracted seven patterns from three application scenarios"
  - [section] "What are the recurring solutions across these examples? The result is a list of solutions and their descriptions"
  - [corpus] Weak evidence - related papers focus on LLM applications but not pattern extraction from narratives
- Break condition: If application scenarios are too dissimilar or described in code/design diagrams rather than narrative format

### Mechanism 2
- Claim: ChatGPT can co-create pattern descriptions through iterative refinement with human domain experts
- Mechanism: The human expert guides ChatGPT through structured prompts to extract pattern elements (context, problem, forces, solution, known uses), then manually refines the output to improve accuracy, terminology, and completeness
- Core assumption: The collaborative process between human expertise and AI capabilities produces better patterns than either working alone
- Evidence anchors:
  - [section] "Instead of asking ChatGPT to combine the various pattern elements it produced... I chose to combine them manually. This gave me a chance to digest what ChatGPT had produced, and 'humanize' the pattern descriptions"
  - [abstract] "human refinement was needed to improve pattern names, context descriptions, and terminology"
  - [corpus] Moderate evidence - related papers show human-AI collaboration but not specifically for pattern mining
- Break condition: If human expert lacks sufficient domain knowledge to evaluate and refine AI-generated patterns

### Mechanism 3
- Claim: Adding affordances as pattern elements helps explain why patterns work by mapping solutions to component capabilities
- Mechanism: By identifying and cross-referencing the affordances (capabilities) of LLMs, databases, and external tools with the patterns that use them, the solution descriptions become more actionable and the pattern language more coherent
- Core assumption: Solutions can be effectively described as configurations of component affordances, making patterns more transferable across contexts
- Evidence anchors:
  - [section] "Documenting the affordances actualized by each pattern allows us to describe a solution in terms of the capabilities of the underlying components it uses or builds on"
  - [abstract] "argues for adding affordances of the underlying components as a new element of pattern descriptions"
  - [corpus] Weak evidence - related papers don't discuss affordances in pattern descriptions
- Break condition: If patterns cannot be meaningfully mapped to component affordances or if this adds unnecessary complexity

## Foundational Learning

- Pattern structure (context, problem, forces, solution, known uses, resulting context)
  - Why needed here: Understanding this format is essential for both extracting patterns from examples and creating the initial prompts
  - Quick check question: What are the five main sections of an Alexandrian pattern format?

- Large Language Model capabilities (content generation, semantic understanding, adaptive learning)
  - Why needed here: The patterns specifically leverage LLM affordances, so understanding what LLMs can and cannot do is crucial
  - Quick check question: Which LLM capability allows it to generate contextually appropriate responses even when definitive answers aren't available?

- Affordance theory in design
  - Why needed here: The paper introduces affordances as a new pattern element, so understanding this concept is necessary to implement the proposed process
  - Quick check question: How does the paper define "affordances" in the context of pattern descriptions?

## Architecture Onboarding

- Component map:
  - Human domain expert: Provides application scenarios, evaluates pattern quality, refines pattern descriptions
  - ChatGPT (LLM): Extracts solutions, problems, and affordances from examples; generates initial pattern descriptions
  - Pattern elements: Context, problem, forces, solution, known uses, resulting context, affordances
  - Process steps: Identify examples → Extract solutions → Define problems → Distill patterns → Identify affordances → Relate patterns to affordances → Refine iteratively → Consolidate patterns

- Critical path:
  1. Expert prepares 3-5 application scenarios with sufficient narrative detail
  2. Expert prompts ChatGPT to extract recurring solutions
  3. Expert prompts ChatGPT to identify problems for each solution
  4. Expert prompts ChatGPT to format as patterns
  5. Expert manually refines pattern names, context, and terminology
  6. Expert identifies and cross-references affordances
  7. Expert explores pattern dependencies and adds resulting context
  8. Expert creates pattern stories to demonstrate application

- Design tradeoffs:
  - Prompt quality vs. LLM context limits: More detailed prompts improve results but consume context tokens
  - Pattern granularity vs. pattern language coherence: More granular patterns capture more specific design choices but may fragment the language
  - Human refinement vs. automation: More human involvement improves quality but reduces scalability

- Failure signatures:
  - Generic or overly broad patterns suggest insufficient variety in application scenarios
  - Missing patterns indicate gaps in either the examples or the extraction process
  - Inconsistent terminology suggests need for more domain-specific prompting
  - Missing dependencies suggest insufficient exploration of pattern relationships

- First 3 experiments:
  1. Test pattern extraction with 3 simple application scenarios that have clear solution differences
  2. Test human refinement process by comparing initial ChatGPT output with expert-refined version
  3. Test affordance cross-referencing by mapping a single pattern to its component affordances and evaluating the added explanatory value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed pattern mining process be adapted to handle larger numbers of known uses given the context limitations of LLMs?
- Basis in paper: [explicit] "Making known uses available" section discusses context limitations
- Why unresolved: The paper identifies the context size limitation as a challenge but does not provide a concrete solution for scaling the process
- What evidence would resolve it: A case study demonstrating the adapted process with a larger dataset, showing improved pattern extraction compared to the original method

### Open Question 2
- Question: What is the optimal granularity for patterns extracted using this collaborative process, and how does it compare to patterns extracted by human experts?
- Basis in paper: [explicit] "Future Work" section mentions comparing pattern granularity
- Why unresolved: The paper suggests this comparison but does not conduct the experiment
- What evidence would resolve it: A comparative study showing pattern granularity differences between AI-assisted and human-only extraction methods using the same known uses

### Open Question 3
- Question: How reliable are the prompts used in the process, and can their effectiveness be improved by incorporating more explicit pattern knowledge?
- Basis in paper: [explicit] "Quality of the prompts" section in Limitations
- Why unresolved: The paper acknowledges potential prompt issues but doesn't test alternative prompts
- What evidence would resolve it: An experiment testing different prompt variations and measuring their impact on pattern extraction quality and consistency

## Limitations

- The approach requires high-quality, diverse application scenarios to produce meaningful patterns
- Manual human refinement is essential but introduces subjectivity and reduces scalability
- Context limitations of LLMs constrain the number of examples that can be processed effectively

## Confidence

- High confidence: ChatGPT can extract solutions from narrative descriptions when given well-structured application scenarios
- Medium confidence: The collaborative refinement process produces more coherent patterns than AI-generated output alone
- Low confidence: The scalability of this approach to larger pattern languages and the generalizability to other domains

## Next Checks

1. **Reproducibility test**: Conduct the full pattern mining process with different human experts using the same application scenarios to assess consistency in pattern extraction and refinement
2. **Scalability experiment**: Apply the method to a larger set of 10-15 application scenarios to evaluate how pattern quality and quantity change with more examples
3. **Cross-domain validation**: Test the approach in a different domain (e.g., software design patterns or user experience patterns) to assess generalizability beyond LLM integrations