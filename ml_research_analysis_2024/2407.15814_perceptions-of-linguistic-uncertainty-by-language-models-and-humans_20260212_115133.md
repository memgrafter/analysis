---
ver: rpa2
title: Perceptions of Linguistic Uncertainty by Language Models and Humans
arxiv_id: '2407.15814'
source_url: https://arxiv.org/abs/2407.15814
tags:
- uncertainty
- likely
- statements
- numerical
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how large language models (LLMs) map linguistic\
  \ expressions of uncertainty to numerical probabilities, focusing on their ability\
  \ to employ theory of mind\u2014understanding another agent's uncertainty independently\
  \ of their own beliefs. Through experiments with 94 humans and 10 LLMs, the study\
  \ finds that 7 out of 10 LLMs can map uncertainty expressions to numerical responses\
  \ in a human-like manner, with larger and newer models like GPT-4 and LLama3 (70B)\
  \ performing especially well."
---

# Perceptions of Linguistic Uncertainty by Language Models and Humans

## Quick Facts
- arXiv ID: 2407.15814
- Source URL: https://arxiv.org/abs/2407.15814
- Reference count: 40
- Large language models map uncertainty expressions to probabilities in human-like ways but show systematic bias based on prior knowledge

## Executive Summary
This paper investigates how large language models (LLMs) interpret linguistic expressions of uncertainty and map them to numerical probabilities, examining their ability to understand another agent's uncertainty independently of their own beliefs. Through experiments with 94 humans and 10 LLMs, the study finds that 7 out of 10 models can map uncertainty expressions to numerical responses in a human-like manner, with larger and newer models like GPT-4 and LLama3-70B performing especially well. However, a key finding is that LLMs are substantially more susceptible to bias based on their prior knowledge than humans, systematically assigning higher numerical responses to uncertainty expressions when they believe the associated statement is true versus false.

The research highlights important implications for human-AI and AI-AI communication, particularly in contexts where accurate uncertainty communication is critical, such as summarizing scientific reports or writing news articles. The systematic bias observed in LLMs raises concerns about their reliability in tasks involving uncertainty expression, suggesting that while models can understand and use uncertainty language, their responses may be influenced by factors that don't affect human judgment in the same way. This work calls for improved model behavior in understanding and expressing uncertainty to ensure more effective communication between humans and AI systems.

## Method Summary
The study employed a mixed-methods approach combining human surveys with LLM evaluations. Researchers presented 94 human participants and 10 different LLMs with statements expressing uncertainty (e.g., "I believe," "It's likely," "There's a good chance") paired with binary true/false prompts. Participants and models were asked to map these linguistic expressions to numerical probabilities. The experiment tested various uncertainty expressions across different contexts to examine how well LLMs could understand another agent's perspective independently of their own knowledge. The study compared model responses to human responses using correlation metrics and analyzed systematic biases in how models interpreted uncertainty based on their prior knowledge about the truth value of statements.

## Key Results
- 7 out of 10 tested LLMs mapped uncertainty expressions to numerical probabilities in a human-like manner, with GPT-4 and LLama3-70B showing the best performance
- LLMs demonstrated systematic bias, assigning higher numerical values to uncertainty expressions when they believed the associated statement was true versus false
- This bias was particularly pronounced for certain uncertainty expressions and was substantially more severe in LLMs compared to human participants
- Larger and newer models performed better at human-like uncertainty mapping, suggesting scaling and recency effects in LLM capabilities

## Why This Works (Mechanism)
The mechanism underlying LLMs' ability to map linguistic uncertainty to numerical probabilities appears to stem from their training on vast corpora of human-written text that naturally contains uncertainty expressions. During training, models learn statistical associations between phrases and their implied confidence levels through pattern recognition. The successful models likely capture nuanced contextual cues that humans use when expressing uncertainty. However, the observed bias suggests that LLMs' internal representations of truth and uncertainty are more tightly coupled than in humans, possibly because training data often presents information with implied certainty levels that models overgeneralize. The better performance of larger and newer models indicates that scale and recent training approaches may improve the models' ability to separate their own knowledge from the expressed uncertainty of others, though not completely.

## Foundational Learning

**Theory of Mind** - The ability to attribute mental states, beliefs, intents, and knowledge to oneself and others, and to understand that others have beliefs, desires, intentions, and perspectives that are different from one's own. *Why needed*: The study specifically tests whether LLMs can understand another agent's uncertainty independently of their own beliefs, which is a core theory of mind capability. *Quick check*: Can the model distinguish between what it knows and what another agent knows or believes?

**Uncertainty Calibration** - The process of aligning linguistic expressions of uncertainty with their corresponding numerical probability values. *Why needed*: The core experimental task requires mapping phrases like "I believe" or "It's likely" to specific probability values. *Quick check*: Does the model consistently assign similar numerical values to the same uncertainty expression across different contexts?

**Prior Knowledge Bias** - The systematic influence of pre-existing beliefs or information on judgment and decision-making. *Why needed*: The study reveals that LLMs show stronger prior knowledge bias than humans when interpreting uncertainty expressions. *Quick check*: Does the model's response change based on its knowledge of a statement's truth value, even when that knowledge shouldn't affect the interpretation of uncertainty language?

## Architecture Onboarding

**Component Map**: Language Model (input processing) -> Context Encoding -> Uncertainty Expression Mapping -> Numerical Probability Output -> Bias Detection Module

**Critical Path**: Input statement containing uncertainty expression → Context analysis → Knowledge retrieval about statement truth → Uncertainty probability assignment → Final numerical output

**Design Tradeoffs**: The models must balance between capturing nuanced human uncertainty expressions (requiring sophisticated language understanding) and maintaining consistency in probability assignments (requiring stable internal representations). Larger models achieve better performance but at computational cost. The coupling between knowledge and uncertainty expression mapping creates both capabilities and vulnerabilities.

**Failure Signatures**: Systematic over-assignment of numerical values when the model believes statements are true, inconsistent probability assignments for the same uncertainty expression across different contexts, and inability to separate personal knowledge from expressed uncertainty in third-party statements.

**First Experiments**:
1. Test the same uncertainty expressions with explicitly neutral prompts that don't mention truth values to see if bias persists
2. Evaluate model performance on uncertainty expressions from specialized domains (legal, medical) not included in the original study
3. Conduct ablation studies removing different amounts of training data to determine whether the bias stems from specific training patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 94 human participants and only 10 LLMs may not capture full diversity of human uncertainty expression or LLM behaviors
- Binary true/false prompt framing could introduce artificial effects not present in naturalistic settings
- The study doesn't fully explore the root causes of observed biases or provide comprehensive theoretical explanations for why certain uncertainty expressions are more vulnerable to bias
- Operational definition of "human-like" mapping remains somewhat loose, primarily based on correlation metrics rather than deeper qualitative analysis

## Confidence

**Key Finding: LLM uncertainty mapping performance** - High
The finding that certain LLMs (particularly GPT-4 and LLama3-70B) can map uncertainty expressions to numerical probabilities in human-like ways is well-supported by the data and aligns with broader literature on scaling laws in LLMs.

**Key Finding: Systematic bias based on prior knowledge** - Medium
While the effect is statistically significant, the study doesn't fully explore whether this bias stems from training data patterns, architectural limitations, or experimental setup, limiting confidence in the explanation.

**Key Finding: Model size and recency effects** - Medium
The correlation between model size/recency and performance is observed, but the study doesn't control for all potential confounding variables that might explain these effects.

## Next Checks
1. Replicate the study with a larger, more diverse human participant pool (n > 300) across multiple cultural contexts to verify the universality of human uncertainty expression patterns
2. Test additional uncertainty expressions not included in the original study, particularly those from specialized domains like legal, medical, or technical contexts
3. Conduct intervention studies where models are explicitly fine-tuned on uncertainty calibration to determine whether the observed biases are remediable through training