---
ver: rpa2
title: A Generative Adversarial Attack for Multilingual Text Classifiers
arxiv_id: '2401.08255'
source_url: https://arxiv.org/abs/2401.08255
tags:
- language
- have
- adversarial
- multilingual
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a generative adversarial attack method for
  multilingual text classifiers. The core idea is to fine-tune a pre-trained multilingual
  paraphrasing model with an adversarial objective, incorporating semantic similarity
  and language consistency models.
---

# A Generative Adversarial Attack for Multilingual Text Classifiers

## Quick Facts
- arXiv ID: 2401.08255
- Source URL: https://arxiv.org/abs/2401.08255
- Authors: Tom Roth; Inigo Jauregi Unanue; Alsharif Abuadbba; Massimo Piccardi
- Reference count: 12
- Key outcome: Generative adversarial attack method for multilingual text classifiers using fine-tuned mT5 with adversarial objective, vocabulary-mapping matrices, and component models. Achieves validated success rate up to 0.8 with 32 queries.

## Executive Summary
This paper introduces a novel generative adversarial attack method for multilingual text classifiers that fine-tunes a pre-trained mT5 model through two stages: first for multilingual paraphrasing, then with an adversarial objective. The approach incorporates semantic similarity and language consistency models connected via vocabulary-mapping matrices to ensure end-to-end differentiability. Experiments on MARC and TSM datasets across five languages demonstrate superior performance compared to baselines, particularly in query efficiency, with validated success rates reaching 0.8 using only 32 queries versus 0.6 for best baselines.

## Method Summary
The method employs a two-stage approach where mT5 is first pre-trained on a multilingual paraphrasing dataset to acquire cross-lingual transformation capabilities, then fine-tuned with an adversarial objective. The training incorporates component models for victim classification, semantic similarity (multilingual Sentence-BERT), and language detection (Lingua library), connected through vocabulary-mapping matrices that enable seamless integration across different vocabularies while preserving differentiability. The loss function balances adversarial strength with text quality through weighted components including victim model score, semantic similarity, language consistency, KL divergence, and diversity penalty, with coefficients tuned to achieve optimal trade-offs.

## Key Results
- Validated success rate reaches 0.8 for proposed approach versus 0.6 for best baseline using maximum 32 queries
- Superior query efficiency compared to existing baselines on MARC and TSM multilingual datasets
- Maintains semantic similarity (BERTScore F1) and language consistency across English, German, French, Spanish, and Arabic
- Model shows lower performance on Arabic due to morphological complexity and vocabulary limitations

## Why This Works (Mechanism)

### Mechanism 1
The generative model learns to produce multilingual adversarial examples by incorporating a multilingual paraphrasing objective followed by adversarial fine-tuning. Initially pre-trained mT5 is fine-tuned on a multilingual paraphrasing dataset to acquire diverse text-to-text transformation capabilities across multiple languages, then fine-tuned with an adversarial objective that leverages a victim model, semantic similarity, and language consistency models. The core assumption is that the pre-trained mT5 model's ability to generate paraphrases across many languages can be effectively adapted for adversarial purposes through fine-tuning.

### Mechanism 2
Vocabulary-mapping matrices enable seamless integration of models with different vocabularies, preserving differentiability. For each component model (victim, similarity, language detection), the token probability distributions from the generative model are weighted with embedding matrices and connected via vocabulary-mapping matrices. This allows any model to be used in the training objective without sacrificing differentiability. The core assumption is that the constructed vocabulary-mapping matrices accurately map tokens between vocabularies of different models while maintaining semantic equivalence.

### Mechanism 3
The training objective balances adversarial strength with text quality through carefully weighted loss components. The loss function combines victim model score, semantic similarity score, language consistency score, and KL divergence, each with adjustable coefficients. Threshold clipping operators ensure balanced optimization. The core assumption is that the coefficients can be tuned to achieve a trade-off between attack strength and text quality that is both effective and acceptable.

## Foundational Learning

- Concept: Multilingual text generation and paraphrasing
  - Why needed here: The approach relies on generating diverse paraphrases across multiple languages as a foundation for adversarial example generation.
  - Quick check question: How does the model ensure that generated paraphrases maintain semantic meaning while varying the wording?

- Concept: Adversarial training in NLP
  - Why needed here: Understanding how adversarial objectives are incorporated into the training of NLP models is crucial for adapting the generative model to produce adversarial examples.
  - Quick check question: What are the key components of an adversarial training objective for text classification models?

- Concept: Vocabulary mapping and cross-lingual embeddings
  - Why needed here: The approach uses vocabulary-mapping matrices to connect models with different vocabularies, requiring an understanding of how to map tokens while preserving semantic information.
  - Quick check question: How do vocabulary-mapping matrices handle tokens that do not have direct equivalents across different vocabularies?

## Architecture Onboarding

- Component map: Input text → Generative model (mT5) → Vocabulary-mapping matrices → Victim model, Semantic similarity model, Language detection model → Loss function → Backpropagation to generative model
- Critical path: Input text → Generative model → Vocabulary-mapping matrices → Component models → Loss function → Backpropagation to generative model
- Design tradeoffs:
  - Memory vs. performance: Storing large vocabulary-mapping matrices and multiple models requires significant memory
  - Attack strength vs. text quality: The loss function coefficients must be carefully tuned to balance these objectives
  - Generalization vs. specificity: The approach aims to work across multiple languages but may not be optimal for any single language
- Failure signatures:
  - High similarity scores but low attack success: The model is not generating effective adversarial examples
  - Low language consistency scores: The model is generating text in the wrong language
  - High KL divergence: The model is deviating too much from the original paraphrase distribution
- First 3 experiments:
  1. Train the generative model on the multilingual paraphrasing dataset and evaluate its ability to generate diverse paraphrases across the five languages
  2. Integrate the victim model and semantic similarity model, and train with the adversarial objective, monitoring the balance between attack strength and text quality
  3. Add the language detection model and evaluate the model's ability to generate adversarial examples that maintain language consistency

## Open Questions the Paper Calls Out

### Open Question 1
How would the proposed generative adversarial attack perform on languages with significantly different morphological and syntactic structures, such as Japanese or Chinese, compared to the five languages tested? The paper mentions that all methods struggled with Arabic due to its highly inflected and morphologically distinct nature, and suggests that specialist adversarial algorithms may be required for such languages. This implies that the proposed approach might face similar challenges with other linguistically diverse languages. Conducting experiments with the proposed approach on Japanese and Chinese datasets, comparing performance metrics with the results obtained for the five languages in the paper, would resolve this question.

### Open Question 2
Would the use of parameter-efficient fine-tuning methods, such as LoRA (Low-Rank Adaptation), mitigate the GPU memory constraints associated with the proposed approach? The authors mention that the method requires use of a GPU with a substantial amount of memory to store all the component models during training, and suggest that the use of parameter-efficient fine-tuning methods may be able to mitigate this issue. Implementing the proposed approach using LoRA or other parameter-efficient fine-tuning methods, and comparing the GPU memory usage and training time with the original implementation, would resolve this question.

### Open Question 3
How would the performance of the proposed generative adversarial attack change if the training objective was modified to incorporate additional component models, such as a model for controlling the topic of the generated adversarial examples? The authors mention that prior work has incorporated a downstream model to control the topic of the generated adversarial candidates at inference time, suggesting that this could be a potential direction for improving the proposed approach. Modifying the training objective to include an additional component model for topic control, and comparing the performance of the proposed approach with and without this modification on the same datasets used in the paper, would resolve this question.

## Limitations

- The model shows lower performance on Arabic compared to other languages due to morphological complexity and vocabulary limitations
- Exact implementation details for vocabulary-mapping matrices between SentencePiece and WordPiece vocabularies are not fully specified
- Manual tuning of loss function coefficients introduces uncertainty in reproducing exact performance results
- Memory constraints require substantial GPU resources to store all component models during training

## Confidence

**High Confidence Claims**:
- The two-stage training approach (multilingual paraphrasing pre-training followed by adversarial fine-tuning) is effective for generating adversarial examples
- Vocabulary-mapping matrices successfully enable end-to-end differentiability between models with different vocabularies
- The method outperforms existing baselines in terms of query efficiency on the tested datasets

**Medium Confidence Claims**:
- The model maintains semantic similarity and language consistency across all five languages
- The adversarial examples generated are of sufficient quality to evade detection by robustness-aware classifiers
- The approach generalizes well across different multilingual text classification tasks

**Low Confidence Claims**:
- The method's effectiveness extends to languages beyond the five tested
- The approach remains effective against more sophisticated defense mechanisms not evaluated in the paper
- The model's performance on extremely long sequences (beyond 32 tokens) would be comparable to the results presented

## Next Checks

**Check 1: Vocabulary Mapping Validation**
Implement and validate the vocabulary-mapping matrices between mT5 and victim models. Create test cases that map known tokens between vocabularies and measure the semantic drift introduced by the mapping. Compare the performance of the attack with and without proper vocabulary mapping to quantify its impact on attack success rate.

**Check 2: Coefficient Sensitivity Analysis**
Systematically vary the α and β coefficients in the loss function around the reported ranges. Measure how changes in these coefficients affect the balance between attack success rate, semantic similarity, and language consistency. Identify the optimal coefficient settings for each dataset and language combination.

**Check 3: Cross-Lingual Generalization Test**
Evaluate the model's performance on an additional multilingual dataset containing languages not included in the original study (e.g., Chinese, Hindi, or Russian). Measure the attack success rate, semantic similarity, and language consistency to assess the model's ability to generalize to new languages beyond the five it was trained on.