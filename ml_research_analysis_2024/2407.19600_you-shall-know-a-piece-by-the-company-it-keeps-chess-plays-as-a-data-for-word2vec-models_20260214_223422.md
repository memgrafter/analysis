---
ver: rpa2
title: You shall know a piece by the company it keeps. Chess plays as a data for word2vec
  models
arxiv_id: '2407.19600'
source_url: https://arxiv.org/abs/2407.19600
tags:
- moves
- move
- chess
- piece
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies linguistic methods of analysis, specifically
  word2vec embeddings, to chess game notations as a form of non-linguistic data. The
  author treats chess moves as "words" and game sequences as "sentences," building
  vector models to capture semantic relationships between moves.
---

# You shall know a piece by the company it keeps. Chess plays as a data for word2vec models

## Quick Facts
- arXiv ID: 2407.19600
- Source URL: https://arxiv.org/abs/2407.19600
- Authors: Boris Orekhov
- Reference count: 6
- Primary result: Word2vec models applied to chess moves successfully capture semantic relationships between moves, distinguishing game phases and grouping similar moves by piece and position

## Executive Summary
This paper applies word2vec embeddings to chess game notations, treating chess moves as words and game sequences as sentences. Five types of models were constructed using 5.4 million games from Lichess: based on moves alone, positions plus moves, and variants focusing on different game phases and outcomes. Results show that models meaningfully group similar moves (usually by the same piece or to the same square), distinguish between game phases, and capture the dual nature of castling involving both king and rook. While these models are unlikely to help choose optimal chess moves, they demonstrate that chess game data contains significant semantic structure even without explicit position consideration.

## Method Summary
The author parsed 5.4 million chess games from Lichess into move sequences and trained word2vec models using different contexts: moves-only, positions-plus-moves, and subsets focusing on openings, midgame, endgame, and game outcomes. The models were evaluated through visual inspection of tSNE clustering, "odd one out" tests where models identified dissimilar moves, and analogy solving capabilities. The study compared models with and without explicit position information to determine how much positional context could be implicitly learned through move co-occurrence patterns.

## Key Results
- Models successfully group moves by piece and square, with endgame moves clustering very clearly by piece type in tSNE visualizations
- The approach distinguishes between different game phases, with opening moves showing less distinct clustering than endgame moves
- Position-agnostic models capture the dual nature of castling, correctly associating both king and rook moves
- "Odd one out" tests successfully identify dissimilar moves, and the models can solve simple move analogies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chess moves treated as words form meaningful vector spaces due to their distributional context
- Mechanism: Word2vec learns semantic similarity from co-occurrence patterns; moves appearing in similar game contexts receive similar vector representations
- Core assumption: The "meaning" of a chess move is captured by the sequence of moves surrounding it in games
- Evidence anchors:
  - [abstract] "word embeddings (word2vec) can work on chess game texts instead of natural language texts"
  - [section] "Chess moves, like words in natural language, receive their interpretation from context"
  - [corpus] Weak evidence: No corpus-based distributional statistics provided, but 5.4M games suggest sufficient data volume
- Break condition: If chess moves don't follow predictable patterns based on game context (e.g., in highly creative or non-standard openings), the model would fail to capture meaningful relationships

### Mechanism 2
- Claim: Different game phases create distinct move distributions that the model can distinguish
- Mechanism: Vector models trained on different move subsets (openings, midgame, endgame) show clear clustering patterns specific to each phase
- Core assumption: Moves in different game phases have systematically different contexts and therefore cluster separately
- Evidence anchors:
  - [abstract] "distinguish between different game phases"
  - [section] "opening moves do not cluster well" vs "endgame moves cluster very clearly, as in a textbook—distinctly by each piece"
  - [corpus] Weak evidence: No quantitative clustering metrics reported, but qualitative visualization described
- Break condition: If players frequently use moves from one phase in another phase context, phase-specific clustering would blur

### Mechanism 3
- Claim: Position-agnostic models still capture positional information through move co-occurrence patterns
- Mechanism: Even without explicit board positions, the model learns that certain moves frequently appear together in similar game situations, implicitly encoding positional constraints
- Core assumption: The sequential nature of chess games preserves enough positional information through move patterns
- Evidence anchors:
  - [abstract] "the model, based on the available data, has managed to learn that castling involves the movement of both the king and the rook"
  - [section] "moves with the same piece from the same square are grouped" even in models without explicit positions
  - [corpus] Weak evidence: No position reconstruction experiments described
- Break condition: If move sequences become too random or if games deviate significantly from standard patterns, the implicit positional encoding would break down

## Foundational Learning

- Concept: Distributional semantics and the distributional hypothesis
  - Why needed here: The entire approach relies on the idea that words/moves with similar contexts have similar meanings
  - Quick check question: Can you explain why "king + woman - man ≈ queen" works in word embeddings?

- Concept: Vector space mathematics and cosine similarity
  - Why needed here: The model uses cosine distance to measure move similarity and perform analogical reasoning
  - Quick check question: What does a cosine distance of 0.4 between two moves indicate about their relationship?

- Concept: tSNE and dimensionality reduction
  - Why needed here: The visualization technique that reveals clustering patterns in the high-dimensional move space
  - Quick check question: What parameter in tSNE controls how distinct clusters appear in the visualization?

## Architecture Onboarding

- Component map: PGN parser -> move extraction -> vectorization -> word2vec training -> analysis tools
- Critical path:
  1. Parse chess games into move sequences
  2. Prepare different context types (moves-only, positions + moves, game-phase subsets)
  3. Train word2vec models with appropriate parameters
  4. Analyze similarity patterns and cluster visualizations
  5. Validate findings through controlled experiments

- Design tradeoffs:
  - Window size vs. computational cost: Larger windows capture more context but increase training time
  - Move granularity vs. semantic clarity: Including square information creates more distinct vectors but may fragment related moves
  - Model size vs. generalization: More dimensions can capture subtle distinctions but may overfit to game-specific patterns

- Failure signatures:
  - Random or uniform clustering in tSNE visualizations
  - Low cosine similarity between semantically related moves
  - Inability to distinguish between different game phases
  - Poor performance on "odd one out" and analogy tasks

- First 3 experiments:
  1. Test move similarity: Query the model for nearest neighbors of common opening moves (e.g., Pe2e4) and verify they're similar moves by the same piece
  2. Validate phase distinction: Compare tSNE visualizations of opening vs. endgame models to confirm phase-specific clustering
  3. Test analogical reasoning: Verify that king's castling vectors correctly include both king and rook moves as nearest neighbors

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies heavily on qualitative visual inspection rather than quantitative metrics for evaluating clustering quality and model performance
- Corpus of 5.4 million games from Lichess may introduce bias toward amateur play patterns rather than professional-level strategies
- Models cannot distinguish between different opening systems (e.g., Sicilian vs. French Defense) despite treating all openings as a single phase

## Confidence
- **High confidence**: Models successfully group similar moves by piece and square, and distinguish between game phases (opening vs. endgame) - supported by clear tSNE visualizations and intuitive "odd one out" test results
- **Medium confidence**: The model captures the dual nature of castling involving both king and rook - demonstrated through nearest neighbor queries but not quantitatively validated
- **Low confidence**: Claims about semantic structure encoding without explicit positions - no position reconstruction experiments to verify this capability

## Next Checks
1. **Quantitative clustering validation**: Apply silhouette scores or other clustering metrics to tSNE visualizations to objectively measure separation between move clusters across different game phases

2. **Position reconstruction test**: Feed the model sequences of moves from known board positions and evaluate whether it can accurately predict the resulting position or suggest moves consistent with that position

3. **Professional vs. amateur distinction**: Train separate models on professional games versus amateur games and compare their ability to distinguish between different opening systems and strategic patterns