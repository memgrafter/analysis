---
ver: rpa2
title: 'Longhorn: State Space Models are Amortized Online Learners'
arxiv_id: '2407.14207'
source_url: https://arxiv.org/abs/2407.14207
tags:
- longhorn
- online
- learning
- arxiv
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Longhorn, a novel state space model (SSM)
  architecture designed for sequence modeling tasks. The key idea is to view SSMs
  as meta-modules for specific online learning problems, with state transition rules
  derived from solving these objectives.
---

# Longhorn: State Space Models are Amortized Online Learners

## Quick Facts
- arXiv ID: 2407.14207
- Source URL: https://arxiv.org/abs/2407.14207
- Reference count: 10
- Key outcome: Introduces Longhorn, an SSM architecture achieving 1.8x improvement in sample efficiency over Mamba with capability to extrapolate to contexts up to 16x longer than training context

## Executive Summary
Longhorn introduces a novel perspective on state space models (SSMs) by viewing them as meta-modules for specific online learning problems. The core innovation is a closed-form solution for an online associative recall problem that serves as the recurrence update, naturally incorporating a forgetting mechanism without a separately parameterized forget gate. This approach outperforms state-of-the-art SSMs including Mamba on standard sequence modeling benchmarks, language modeling, and vision tasks. Longhorn achieves significant improvements in sample efficiency (1.8x) and demonstrates the unique capability to extrapolate to contexts up to 16x longer than seen during training.

## Method Summary
Longhorn is a state space model architecture derived from solving an online associative recall problem through implicit online learning. The key innovation is a closed-form update rule obtained by minimizing an online regression objective that balances stability and plasticity. This update replaces the traditional SSM recurrence with a diagonal approximation that enables parallel computation. The architecture maintains the same overall structure as Mamba but uses the novel Longhorn recurrence, which does not require careful initialization of state transition parameters. The model is trained with AdamW optimizer, cosine learning rate decay, and gradient clipping.

## Key Results
- Achieves 1.8x improvement in sample efficiency compared to Mamba
- Can extrapolate to contexts up to 16x longer than training context during inference
- Outperforms state-of-the-art SSMs including Mamba on standard sequence modeling benchmarks, language modeling, and vision tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Longhorn's recurrence update solves an online associative recall problem through implicit online learning
- Mechanism: The state transition matrix is derived as the closed-form solution to minimizing an online regression objective, naturally incorporating a forgetting mechanism without a separately parameterized forget gate
- Core assumption: The online learning objective formulation captures the essential dynamics needed for effective sequence modeling
- Evidence anchors:
  - [abstract] "The proposed method outperforms state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks"
  - [section] "Longhorn's recurrent update is obtained by the closed-form solution to the online learning objective"
  - [corpus] Weak - no direct citations to associative recall literature
- Break condition: If the online regression objective fails to capture the true data distribution, or if the diagonal approximation becomes too restrictive

### Mechanism 2
- Claim: Longhorn achieves 1.8x improvement in sampling efficiency compared to Mamba
- Mechanism: The closed-form solution provides a stable recurrence without requiring careful initialization of state transition parameters, leading to more efficient training and better sample utilization
- Core assumption: The implicit update's stability translates directly to training efficiency gains
- Evidence anchors:
  - [abstract] "Longhorn achieves a 1.8x improvement in sample efficiency compared to Mamba"
  - [section] "The closed-form solution in Equation 7 does not need any specific initialization. In contrast, Mamba requires careful initialization of the A and ∆t"
  - [corpus] Weak - no comparative efficiency studies in neighbors
- Break condition: If the assumed stability doesn't translate to actual training efficiency, or if the diagonal approximation introduces significant approximation error

### Mechanism 3
- Claim: Longhorn can extrapolate to contexts up to 16x longer than training context without significant performance degradation
- Mechanism: The online learning framework provides a principled approach to handling longer sequences, as the model continues to "learn" online even during inference
- Core assumption: The online learning formulation inherently supports extrapolation beyond training context lengths
- Evidence anchors:
  - [abstract] "can extrapolate over contexts that are up to 16x longer during inference"
  - [section] "Unlike DeltaNet, which... cannot extrapolate beyond the training context, Longhorn demonstrates the capability to successfully extrapolate to contexts up to 16x longer"
  - [corpus] Weak - no direct citations to extrapolation studies
- Break condition: If the model's online learning dynamics break down for very long sequences, or if catastrophic forgetting becomes problematic at extreme lengths

## Foundational Learning

- Concept: Online convex optimization and Bregman divergences
  - Why needed here: The SSM recurrence is derived from solving an online learning objective using Bregman divergences as stability constraints
  - Quick check question: What is the role of the parameter β_t in balancing stability vs. plasticity in the online learning formulation?

- Concept: Sherman-Morrison formula and matrix inversion
  - Why needed here: The closed-form solution derivation relies on matrix inversion techniques to simplify the update equation
  - Quick check question: How does the Sherman-Morrison formula help simplify the matrix inversion in the implicit update?

- Concept: Kronecker and Hadamard products
  - Why needed here: The Longhorn update uses both outer products (b) and elementwise products (d) to handle the state and input dimensions efficiently
  - Quick check question: What is the dimensional relationship between the state St, input xt, and key kt in the Longhorn update?

## Architecture Onboarding

- Component map:
  Input preprocessing (linear projection + Conv1d) → Query/key/beta computation (linear layers) → State update (elementwise and outer products) → Output projection
  Replaces Mamba's SSM block while keeping all other Mamba architecture components unchanged

- Critical path:
  State update computation: (1 - ∆t b kd^2) elementwise with previous state, then add (∆t d xt) outer product with kt
  This must be highly optimized as it's the core recurrence operation

- Design tradeoffs:
  No separate forget gate saves parameters but relies on key vector for forgetting mechanism
  Diagonal approximation enables parallel scan but may limit expressiveness
  Closed-form solution provides stability but may be less flexible than iterative methods

- Failure signatures:
  Training instability or divergence (suggests issues with online objective formulation)
  Poor performance on long sequences (suggests diagonal approximation breaking down)
  Inefficient training compared to Mamba (suggests closed-form solution not providing expected efficiency gains)

- First 3 experiments:
  1. Verify the closed-form solution by comparing numerical results against iterative optimization of the online objective
  2. Benchmark training efficiency and convergence against Mamba on a small dataset
  3. Test extrapolation capability by training on short contexts and evaluating on progressively longer contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Longhorn's online learning objective compare to other potential online learning objectives in terms of performance and efficiency?
- Basis in paper: [explicit] The paper proposes using an online associative recall problem as the objective for Longhorn, but acknowledges that exploring other objectives is an intriguing avenue for future research
- Why unresolved: The paper focuses on demonstrating the effectiveness of the chosen objective, but does not compare it to other potential objectives
- What evidence would resolve it: Systematic experiments comparing Longhorn's performance using different online learning objectives on various sequence modeling tasks

### Open Question 2
- Question: How does the combination of Longhorn with sliding-window attention, as suggested by recent studies, affect its performance compared to Mamba?
- Basis in paper: [explicit] The paper mentions that incorporating sliding-window attention with Mamba improves performance and anticipates similar benefits for Longhorn
- Why unresolved: The paper does not provide experimental results on combining Longhorn with sliding-window attention
- What evidence would resolve it: Experiments comparing the performance of Longhorn with and without sliding-window attention on various sequence modeling tasks

### Open Question 3
- Question: What is the impact of Longhorn's state size on its performance and efficiency compared to other state-space models?
- Basis in paper: [explicit] The paper mentions that Longhorn saves parameters by not requiring a separately parameterized forget gate, but does not provide a detailed analysis of the impact of state size
- Why unresolved: The paper does not conduct a comprehensive study on the relationship between state size and performance for Longhorn
- What evidence would resolve it: Experiments varying Longhorn's state size and comparing its performance and efficiency to other state-space models with different state sizes

## Limitations
- Limited empirical validation of the theoretical connection between associative recall and real-world sequence modeling tasks
- Claims about 1.8x sample efficiency improvement lack supporting training curves or convergence comparisons
- Extrapolation capability demonstrated primarily on synthetic data rather than real-world tasks

## Confidence

- **High**: The mathematical derivation of the closed-form solution and the connection between SSMs and online learning
- **Medium**: The performance claims on standard benchmarks (relative to other SSMs, but absolute performance comparisons are limited)
- **Low**: The 1.8x sample efficiency claim and the extrapolation capability claims (lack of training curves and real-world validation)

## Next Checks

1. **Training Efficiency Verification**: Run controlled experiments comparing Longhorn and Mamba with identical initialization schemes, architectures, and hyperparameters to isolate the impact of the closed-form solution on training efficiency. Plot training curves showing both convergence speed and final performance.

2. **Online Learning Objective Validation**: Design an ablation study where Longhorn is trained with different online learning objectives (varying the stability-plasticity tradeoff β) to demonstrate that the chosen formulation provides optimal performance for the intended tasks.

3. **Real-World Extrapolation Testing**: Evaluate Longhorn's extrapolation capability on actual language modeling tasks by training on short contexts (e.g., 1024 tokens) and testing on progressively longer sequences (up to 16x longer), comparing against Mamba's performance on the same setup.