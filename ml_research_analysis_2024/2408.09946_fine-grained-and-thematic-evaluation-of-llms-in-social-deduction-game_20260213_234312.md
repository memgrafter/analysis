---
ver: rpa2
title: Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game
arxiv_id: '2408.09946'
source_url: https://arxiv.org/abs/2408.09946
tags:
- llms
- metrics
- citizens
- game
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models' (LLMs) ability to perform
  obscured communication in social deduction games, addressing limitations in prior
  coarse-grained evaluations. The authors propose six fine-grained metrics targeting
  subtext inference and deceptive control, alongside a thematic analysis to identify
  reasoning failures.
---

# Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game

## Quick Facts
- arXiv ID: 2408.09946
- Source URL: https://arxiv.org/abs/2408.09946
- Authors: Byungjun Kim; Dayeon Seo; Minju Kim; Bugeun Kim
- Reference count: 38
- Primary result: Fine-grained metrics and thematic analysis provide deeper insights into LLM performance in social deduction games than traditional win-rate evaluations

## Executive Summary
This study addresses limitations in evaluating LLMs' ability to perform obscured communication in social deduction games by introducing six fine-grained metrics and conducting thematic analysis of reasoning errors. The authors tested four LLMs (GPT-4, GPT-3.5, Gemini-pro, and Llama2) in a modified SpyGame environment, finding that GPT-4 outperformed others in subtext inference while Llama2 struggled with deceptive control. The thematic analysis identified four major reasoning failures—memory distortion, dissociation, exposure, and character ambiguity—that correlate with lower performance. This approach demonstrates that decomposing performance into specific metrics and analyzing qualitative errors provides richer insights than traditional aggregate measures.

## Method Summary
The study modified the Spyfall game into SpyGame to avoid pretrained knowledge, using turn counts instead of time limits. Four LLMs played as spies across 168 games (24 per location across 7 locations), with game logs collected for analysis. Six fine-grained metrics were computed: Guess Success Rate, Information Catching Rate, Information Deduction Rate (subtext inference); Caught Rate, Vote Rate, Vote Entropy (deceptive control). Thematic analysis was conducted on 506 reasoning steps from 15 randomly selected games to identify four reasoning error categories. Results were compared against baseline coarse-grained metrics like win rate and living rounds.

## Key Results
- GPT-4 demonstrated superior subtext inference capabilities compared to other models
- Llama2 showed particular difficulty with deceptive control aspects of the game
- Thematic analysis revealed systematic reasoning errors that explain quantitative performance differences
- Four error categories (memory distortion, dissociation, exposure, character ambiguity) correlated with lower metric scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained metrics capture specific reasoning behaviors that coarse-grained metrics miss
- Mechanism: Decomposing performance into six targeted metrics isolates distinct skills like information capture, information deduction, and deceptive control
- Core assumption: Social deduction game performance can be meaningfully decomposed into separate, measurable components
- Evidence anchors: Abstract mentions six fine-grained metrics resolving coarse-grained evaluation issues; section describes analyzing spy's guessing outcomes
- Break condition: If metrics overlap significantly or underlying skills are too interdependent to isolate

### Mechanism 2
- Claim: Thematic analysis reveals systematic reasoning failures that explain quantitative results
- Mechanism: Structured qualitative coding of gameplay logs identifies categories like memory distortion, dissociation, and character ambiguity
- Core assumption: Observable reasoning errors in gameplay logs can be reliably categorized and correlated with metric outcomes
- Evidence anchors: Abstract discusses thematic analysis identifying four major reasoning failures; section defines three types of reasoning errors through coding
- Break condition: If annotator agreement is too low or identified categories don't correlate with quantitative performance

### Mechanism 3
- Claim: SpyGame environment isolates LLM communication skills from game-specific knowledge
- Mechanism: Renaming the game and using turn counts prevents LLMs from relying on memorized rules
- Core assumption: LLM performance reflects reasoning ability rather than memorization of game-specific knowledge
- Evidence anchors: Section explains changing game name to avoid pretrained knowledge; section describes replacing time budget with turn counts
- Break condition: If LLMs still demonstrate knowledge of original game mechanics despite renaming

## Foundational Learning

- Concept: Social deduction games as evaluation environments
  - Why needed here: Understanding why social deduction games provide adversarial contexts for testing obscured communication skills
  - Quick check question: What distinguishes social deduction games from cooperative dialogue in terms of information flow and trust dynamics?

- Concept: Thematic analysis methodology
  - Why needed here: Grasping how qualitative coding can systematically identify reasoning failures in LLM behavior
  - Quick check question: How does thematic analysis differ from simple observation in identifying patterns across gameplay logs?

- Concept: Fine-grained vs. coarse-grained evaluation metrics
  - Why needed here: Recognizing when aggregate metrics obscure important behavioral details
  - Quick check question: What specific information does win rate fail to capture about LLM reasoning in social deduction games?

## Architecture Onboarding

- Component map: Data collection → Thematic analysis → Fine-grained metric computation → Statistical comparison → Result interpretation
- Critical path: Game log collection (SpyGame environment) → Qualitative error identification → Quantitative metric development → Cross-validation of findings
- Design tradeoffs: Detailed metrics provide better insights but require more complex computation; thematic analysis adds qualitative depth but introduces subjectivity
- Failure signatures: Low annotator agreement, lack of correlation between qualitative and quantitative findings, metrics not discriminating between models
- First 3 experiments:
  1. Replicate analysis with different LLM models to test metric generalizability
  2. Apply thematic analysis framework to logs from different game environments
  3. Conduct ablation study removing specific metrics to assess their individual contribution to overall findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' performance in social deduction games translate to real-world applications requiring obscured communication?
- Basis in paper: The paper discusses LLMs' abilities in obscured communication but focuses on game-based evaluation
- Why unresolved: The study's context is limited to a controlled game environment, which may not fully represent the complexities of real-world scenarios
- What evidence would resolve it: Empirical studies comparing LLM performance in game-based and real-world obscured communication tasks would provide insights

### Open Question 2
- Question: What are the long-term effects of LLMs' reasoning errors on their ability to maintain effective obscured communication over extended interactions?
- Basis in paper: The paper identifies reasoning errors such as memory distortion and dissociation that impact LLMs' performance
- Why unresolved: The study's analysis is based on short-term gameplay, not accounting for prolonged interactions
- What evidence would resolve it: Longitudinal studies tracking LLM performance over multiple extended sessions would clarify the impact of reasoning errors

### Open Question 3
- Question: How do different cultural contexts influence LLMs' effectiveness in obscured communication within social deduction games?
- Basis in paper: The paper does not consider cultural variations, which could affect how obscured communication is perceived and executed
- Why unresolved: The study's framework does not account for cultural differences in communication styles
- What evidence would resolve it: Cross-cultural studies involving diverse participant groups would reveal how cultural contexts affect LLM performance

## Limitations

- The thematic analysis relies on coding only 15 randomly selected games, which may not capture full diversity of reasoning errors
- Six fine-grained metrics lack external validation from independent studies to confirm their effectiveness
- The modified SpyGame environment's success in isolating reasoning ability from pretrained knowledge is assumed without direct empirical evidence

## Confidence

**High Confidence:** The core finding that fine-grained metrics provide more detailed insights than coarse-grained win rates is well-supported by the data structure and analysis approach.

**Medium Confidence:** The thematic analysis identifying four reasoning error categories shows promise but depends heavily on annotation quality and sample representativeness.

**Low Confidence:** The claim that the modified SpyGame environment successfully isolates reasoning ability from pretrained knowledge lacks direct empirical evidence.

## Next Checks

1. Conduct inter-annotator reliability assessment with multiple independent coders applying the thematic analysis framework to the same 15 games.

2. Perform statistical tests (chi-square, correlation coefficients) between frequency of identified reasoning errors and six fine-grained metric scores.

3. Test whether LLMs still demonstrate knowledge of original Spyfall mechanics despite renaming and rule modifications by comparing responses to both modified and original game descriptions.