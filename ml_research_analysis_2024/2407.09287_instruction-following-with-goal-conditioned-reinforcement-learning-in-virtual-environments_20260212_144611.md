---
ver: rpa2
title: Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual
  Environments
arxiv_id: '2407.09287'
source_url: https://arxiv.org/abs/2407.09287
tags:
- language
- agent
- environment
- learning
- subtasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IGOR is a hierarchical framework for instruction following in virtual
  environments that combines large language models (LLMs) for high-level planning
  with reinforcement learning (RL) agents for task execution. The approach decomposes
  complex language instructions into subtasks using an LLM, which are then executed
  by an RL agent in a goal-conditioned manner.
---

# Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments

## Quick Facts
- arXiv ID: 2407.09287
- Source URL: https://arxiv.org/abs/2407.09287
- Reference count: 40
- Primary result: IGOR achieves 60% success rate in Crafter environment and F1 score of 0.52 in IGLU competition

## Executive Summary
IGOR is a hierarchical framework for instruction following in virtual environments that combines large language models (LLMs) for high-level planning with reinforcement learning (RL) agents for task execution. The approach decomposes complex language instructions into subtasks using an LLM, which are then executed by an RL agent in a goal-conditioned manner. IGOR achieved 60% success rate in the modified Crafter environment, outperforming Dynalang (36.4%), and achieved F1 scores of 0.52 overall in IGLU, surpassing competition winners with 0.36. The modular design enables integration of curriculum learning and data augmentation techniques, resulting in improved performance on complex spatial reasoning tasks.

## Method Summary
IGOR uses a hierarchical architecture where a Language Module (LLM) translates natural language instructions into sequences of subtasks, a Task Manager encodes these subtasks into RL-observable format and monitors completion, and a Policy Module (RL agent) executes actions in the virtual environment to achieve the subtasks. The framework incorporates curriculum learning to dynamically adjust task difficulty based on agent performance, and data augmentation techniques to improve the LLM's generalization. Training involves fine-tuning the LLM on environment-specific datasets with augmented instructions, and training the RL agent using goal-based learning with a curriculum task sampler.

## Key Results
- IGOR achieved 60% success rate in the modified Crafter environment, outperforming Dynalang (36.4%)
- IGOR achieved F1 scores of 0.52 overall in IGLU competition, surpassing competition winners with 0.36
- The modular design enabled effective integration of curriculum learning and data augmentation techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hierarchical decomposition of complex language instructions into subtasks via LLM enables the RL agent to focus on simpler, goal-conditioned learning tasks.
- Mechanism: By translating a high-level instruction into a sequence of discrete subtasks, the RL agent can train on individual goals rather than the entire instruction at once. This decomposition reduces the action space complexity and aligns with the agent's observation space by providing subtask encodings.
- Core assumption: The LLM can accurately decompose instructions into executable subtasks, and the RL agent can generalize from individual subtask training to perform the full instruction sequence.
- Evidence anchors:
  - [abstract] "IGOR is a hierarchical framework for instruction following in virtual environments that combines large language models (LLMs) for high-level planning with reinforcement learning (RL) agents for task execution."
  - [section] "The framework relies on two main components: a Language Module, which translates instructions in natural language into a high-level action plan with generated subgoals, and a Policy Module, tasked with executing this plan and achieving subgoals."
  - [corpus] Weak - related papers discuss instruction following but don't specifically anchor the decomposition mechanism.
- Break condition: If the LLM fails to decompose instructions correctly, the RL agent receives incorrect or unachievable subtasks, leading to poor performance or failure to complete instructions.

### Mechanism 2
- Claim: Curriculum learning dynamically adjusts task difficulty based on agent performance, improving learning efficiency for complex subtasks.
- Mechanism: The curriculum sampler modifies selection probabilities for subtasks based on average reward and variability. Tasks exceeding a success threshold are deprioritized, while challenging tasks with high variability are sampled more frequently, focusing learning on areas needing improvement.
- Core assumption: The reward signal accurately reflects task difficulty and progress, and the agent can benefit from focusing on challenging tasks before easier ones.
- Evidence anchors:
  - [section] "To enhance the quality of training for the RL agent, we incorporate a curriculum that controls the complexity of the list of subtasks."
  - [section] "Specifically, the selection probability for each task i in T is modified according to: qi = 1/d if ri ≥ τ; 1 + (δi · d) if ri < τ"
  - [corpus] Weak - curriculum learning is mentioned in related work but not specifically tied to this mechanism.
- Break condition: If the reward signal is noisy or uninformative, the curriculum may prioritize the wrong tasks or fail to provide appropriate challenge levels.

### Mechanism 3
- Claim: Data augmentation and format modification of subtasks improve the LLM's generalization and ability to handle diverse instructions.
- Mechanism: Techniques include using ChatGPT to rewrite instructions with modified subtasks, decomposing subtasks into primitives, and rotating objects in instructions. These augmentations increase dataset diversity and help the LLM learn robust mappings from instructions to subtasks.
- Core assumption: The LLM can generalize from augmented data to unseen instructions, and the augmentation techniques preserve semantic meaning while increasing variability.
- Evidence anchors:
  - [section] "Our experiments demonstrate that these methods effectively enhance training quality by ensuring the model can generalize well from smaller, varied linguistic datasets."
  - [section] "We explored two methods for creating primitives... This approach aims to enhance the coherence between the instructional context and the subtask structure."
  - [corpus] Weak - data augmentation is mentioned in related papers but not specifically for instruction following in virtual environments.
- Break condition: If augmentations introduce unrealistic or contradictory instructions, the LLM may learn incorrect mappings or fail to generalize to real instructions.

## Foundational Learning

- Concept: Hierarchical task decomposition
  - Why needed here: Complex instructions in virtual environments require breaking down multi-step actions into manageable subtasks for the RL agent to execute.
  - Quick check question: Can you explain how decomposing "build a red tower with 5 blocks" into individual block placement subtasks simplifies the learning problem for the RL agent?

- Concept: Goal-conditioned reinforcement learning
  - Why needed here: The RL agent must learn to achieve specific subtask goals provided by the Task Manager, rather than learning a single policy for all possible instructions.
  - Quick check question: How does providing the RL agent with subtask encodings as part of its observation space enable goal-conditioned learning?

- Concept: Curriculum learning in RL
  - Why needed here: Complex virtual environments have varying task difficulties; curriculum learning helps the agent progressively master easier tasks before tackling harder ones.
  - Quick check question: Why would dynamically adjusting subtask sampling probabilities based on performance help the agent learn more efficiently than uniform sampling?

## Architecture Onboarding

- Component map: Language Module (LLM) -> Task Manager -> Policy Module (RL agent) -> Virtual Environment
- Critical path: Instruction → Language Module → Task Manager → Policy Module → Environment → Reward → Policy Update
- Design tradeoffs:
  - Independent module training vs. joint optimization: Allows flexibility in training techniques but may create misalignment between modules
  - Subtask granularity: Finer subtasks enable focused learning but increase sequence length and computational complexity
  - Curriculum vs. uniform sampling: Curriculum improves efficiency but requires careful reward signal design
- Failure signatures:
  - LLM produces incorrect subtasks → RL agent attempts impossible or wrong actions
  - Task Manager encoding mismatch → RL agent cannot interpret subtasks correctly
  - Curriculum selects wrong tasks → Agent wastes time on easy tasks or struggles with too-hard tasks
  - Reward signal noise → Curriculum and RL agent learn incorrect value estimates
- First 3 experiments:
  1. Test Language Module alone: Feed known instructions and verify subtask output matches expected decomposition
  2. Test Policy Module alone: Provide hand-crafted subtask sequences and verify agent can complete individual subtasks
  3. Integration test: Run full pipeline on simple instructions in Crafter environment, verify subtask completion and overall success

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of IGOR change when using different LLM models for the language module, such as GPT-4 or Claude, instead of Flan-T5?
- Basis in paper: [inferred] The paper uses Flan-T5 as the base model for the language module and demonstrates its effectiveness, but does not explore alternative LLM models.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the IGOR framework using Flan-T5 and does not compare the performance of different LLM models.
- What evidence would resolve it: Conducting experiments with different LLM models for the language module and comparing their performance in terms of success rate and F1 score in the IGLU and Crafter environments.

### Open Question 2
- Question: How does the modular design of IGOR affect its ability to handle complex, multi-step instructions compared to end-to-end approaches?
- Basis in paper: [explicit] The paper emphasizes the modular design of IGOR, which separates the language module and policy module, allowing for easier integration of techniques like curriculum learning and data augmentation.
- Why unresolved: While the paper demonstrates the effectiveness of the modular design, it does not directly compare IGOR's performance with end-to-end approaches on complex, multi-step instructions.
- What evidence would resolve it: Conducting experiments comparing IGOR's performance with end-to-end approaches on complex, multi-step instructions in both the IGLU and Crafter environments.

### Open Question 3
- Question: How does the performance of IGOR scale with the size and complexity of the virtual environment, such as larger grids or more intricate structures in IGLU?
- Basis in paper: [inferred] The paper demonstrates IGOR's effectiveness in the IGLU and Crafter environments, but does not explore its performance in larger or more complex virtual environments.
- Why unresolved: The paper focuses on the effectiveness of IGOR in the given environments but does not investigate its scalability to larger or more complex environments.
- What evidence would resolve it: Conducting experiments with larger grids or more intricate structures in the IGLU environment and measuring the performance of IGOR in terms of success rate and F1 score.

## Limitations
- Limited experimental validation to only two environments (Crafter and IGLU)
- No ablation studies isolating contributions of individual components like curriculum learning or data augmentation
- 60% success rate in Crafter still leaves significant room for improvement, suggesting struggles with complex instructions

## Confidence
- High confidence in the overall hierarchical architecture design and its theoretical benefits
- Medium confidence in the empirical performance claims due to limited experimental scope
- Low confidence in the robustness of the approach across diverse instruction types and environments

## Next Checks
1. Conduct ablation studies removing curriculum learning and data augmentation to quantify their individual contributions to performance gains
2. Test the framework on a held-out set of more complex instructions in Crafter to evaluate scalability and identify failure modes
3. Analyze LLM subtask decomposition quality by comparing generated subtasks against ground truth for a sample of instructions, measuring accuracy and identifying systematic errors