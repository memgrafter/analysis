---
ver: rpa2
title: Unveiling the Role of Pretraining in Direct Speech Translation
arxiv_id: '2409.18044'
source_url: https://arxiv.org/abs/2409.18044
tags:
- training
- encoder
- source
- translation
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training dynamics of direct speech translation
  systems and finds that models trained from scratch struggle to incorporate encoder
  information early in training due to the complexity of learning both acoustic and
  semantic modeling simultaneously. To address this, the authors propose WeRC (Weighted
  Residual Connection), a modification to the decoder's cross-attention layer that
  forces earlier integration of source information.
---

# Unveiling the Role of Pretraining in Direct Speech Translation

## Quick Facts
- arXiv ID: 2409.18044
- Source URL: https://arxiv.org/abs/2409.18044
- Authors: Belen Alastruey; Gerard I. Gállego; Marta R. Costa-jussà
- Reference count: 7
- Primary result: WeRC modification enables models trained from scratch to achieve comparable BLEU scores to pretrained models while reducing training time

## Executive Summary
This paper analyzes the training dynamics of direct speech translation (ST) systems and identifies why models trained from scratch struggle to incorporate encoder information early in training. The authors find that the simultaneous learning burden of acoustic and semantic modeling creates a dual learning challenge that slows encoder training progress. To address this, they propose WeRC (Weighted Residual Connection), a modification to the decoder's cross-attention layer that forces earlier integration of source information by scaling cross-attention outputs. With WeRC, models trained from scratch achieve comparable BLEU scores to pretrained models while reducing training time, validated on English-to-German, English-to-Spanish, and English-to-French MuST-C datasets with +1.3 BLEU point improvements.

## Method Summary
The study proposes WeRC (Weighted Residual Connection), a modification to the decoder's cross-attention layer in S2T-Transformer architectures. WeRC scales the cross-attention output relative to the residual stream using a fixed weighting factor (λ = 0.65) and adds layer normalization to ensure both components have comparable norms before weighting. This forces the decoder to use encoder information from the beginning of training, preventing convergence toward language modeling alone. The method was evaluated on MuST-C dataset for En-De, En-Es, and En-Fr language pairs, comparing models trained from scratch with and without WeRC to pretrained baselines.

## Key Results
- Models trained from scratch achieve +1.3 BLEU points improvement over baseline models when using WeRC
- WeRC enables models trained from scratch to achieve comparable BLEU scores to pretrained models
- Training time is reduced compared to pretraining approaches while maintaining translation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models trained from scratch struggle to integrate encoder information early in training due to the simultaneous learning burden of acoustic and semantic modeling.
- Mechanism: The encoder must learn to extract meaningful acoustic features from speech while also learning semantic representations, creating a dual learning challenge that slows encoder training progress.
- Core assumption: The encoder's output quality directly impacts the decoder's ability to generate accurate translations.
- Evidence anchors:
  - [abstract] "Throughout the training, the randomly initialized model struggles to incorporate information from the speech inputs for its predictions."
  - [section 3.1] "Unlike an encoder in text translation, which solely requires semantic modeling, a ST encoder learns both acoustic and semantic modeling."
  - [corpus] Weak - no direct evidence in corpus neighbors about this dual learning burden.

### Mechanism 2
- Claim: The Weighted Residual Connection (WeRC) forces earlier integration of source information by scaling cross-attention outputs.
- Mechanism: By increasing the weight of cross-attention outputs relative to residual streams, WeRC ensures that the decoder must use encoder information from the beginning of training, preventing convergence toward language modeling alone.
- Core assumption: The optimal source contribution ratio is approximately 65% based on comparison with pretrained models.
- Evidence anchors:
  - [section 4.1] "We aim to approximately match the proportion of source contribution found in Section 3.1, hence we set λ = 0.65."
  - [section 4.1] "Our goal is to increase the information flow coming from the source, so we scale these two components giving a higher weight to the output of the cross-attention."
  - [corpus] Weak - no direct evidence in corpus neighbors about residual connection scaling.

### Mechanism 3
- Claim: Layer normalization in WeRC prevents cross-attention from converging to small-norm vectors that would minimize contribution regardless of weighting.
- Mechanism: Adding layer normalization ensures both cross-attention and residual stream have comparable norms before weighting, maintaining the intended information flow proportion.
- Core assumption: Without normalization, the model could learn to minimize cross-attention outputs to bypass the forced source integration.
- Evidence anchors:
  - [section 4.1] "A potential issue of this approach is that the cross-attention block could converge towards producing small-norm vectors, so that they would still have a small contribution regardless of the weighting."
  - [section 4.1] "This ensures both tensors have the same norm before the weighting. Therefore they will contribute to the sum with the target proportion."
  - [corpus] Weak - no direct evidence in corpus neighbors about normalization effects on attention outputs.

## Foundational Learning

- Concept: Transformer attention mechanisms and residual connections
  - Why needed here: Understanding how cross-attention layers combine source and target information is crucial for implementing and debugging WeRC.
  - Quick check question: What happens to the information flow when you scale cross-attention outputs relative to residual streams in a transformer decoder?

- Concept: Training dynamics and convergence patterns
  - Why needed here: Recognizing the three-stage training process helps diagnose whether models are learning appropriately and whether WeRC is having the intended effect.
  - Quick check question: How does the source contribution metric change during the three training phases in text vs. speech translation?

- Concept: Pretraining strategies and transfer learning
  - Why needed here: Understanding why pretraining works for speech translation helps validate the hypothesis that WeRC is addressing the same underlying problem.
  - Quick check question: What specific capability does ASR pretraining provide to the encoder that allows it to skip the initial training stage?

## Architecture Onboarding

- Component map: Encoder (12 layers) -> Cross-attention layer (with WeRC) -> Decoder (6 layers) -> Translation output
- Critical path:
  1. Encoder processes speech input to extract acoustic and semantic features
  2. Cross-attention layer (with WeRC) combines encoder outputs with decoder state
  3. Decoder generates translation tokens using weighted combination
  4. Loss computed against target text
- Design tradeoffs:
  - Fixed weighting (λ = 0.65) vs. learnable weighting
  - Layer normalization addition vs. potential performance impact
  - Simplicity of modification vs. generality across tasks
  - Training from scratch vs. pretraining efficiency
- Failure signatures:
  - If BLEU scores don't improve despite source contribution changes, check if encoder representations are meaningful
  - If training becomes unstable, verify layer normalization parameters are correctly set
  - If source contribution doesn't stabilize at 65%, reconsider the optimal ratio
- First 3 experiments:
  1. Implement WeRC with λ = 0.5 to verify that scaling affects source contribution as expected
  2. Remove layer normalization from WeRC to confirm it prevents small-norm convergence
  3. Test different λ values (0.4, 0.7, 0.8) to find optimal source contribution ratio for each language pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed WeRC method work equally well across different speech-to-text translation model architectures beyond the S2T-Transformer used in this study?
- Basis in paper: [inferred] The paper states "We believe different results could be obtained in other settings" and only tests on one specific architecture
- Why unresolved: The study only evaluates WeRC on the Fairseq S2T-Transformer architecture, leaving open whether the benefits generalize to other architectures like conformer, hybrid CTC-attention models, or different encoder-decoder configurations
- What evidence would resolve it: Testing WeRC on multiple architectures (conformer, hybrid models, etc.) across the same datasets and comparing performance to baselines trained from scratch and with pretraining

### Open Question 2
- Question: What is the optimal value of λ (the weight for cross-attention output) in the WeRC method, and how does it vary across different language pairs and data conditions?
- Basis in paper: [explicit] The paper sets λ = 0.65 based on observed optimal source contribution in Figure 1, but notes this may not be universal
- Why unresolved: The paper fixes λ at 0.65 without exploring whether this value is optimal across different language pairs, data sizes, or training conditions, or whether it could be made adaptive during training
- What evidence would resolve it: Systematic ablation studies varying λ across multiple language pairs and training conditions, or experiments with adaptive λ that changes during training

### Open Question 3
- Question: How does WeRC perform in low-resource speech translation scenarios where both ASR and ST training data are limited?
- Basis in paper: [explicit] The limitations section states "We believe different results could be obtained in other settings, such as low resource speech translation"
- Why unresolved: The study focuses on medium-resource MuST-C datasets and doesn't evaluate scenarios where data scarcity is the primary challenge, which is when pretraining is typically most beneficial
- What evidence would resolve it: Experiments on genuinely low-resource datasets with limited ASR and ST data, comparing WeRC to both pretrained and scratch baselines to determine if it maintains its advantages

## Limitations

- The analysis relies on a specific interpretability method (ALTI+) that is referenced but not fully described, creating uncertainty about the universality of the observed three-stage training process
- The experimental scope is limited to three language pairs from the MuST-C dataset, all involving English as source language, raising questions about generalization to other language pairs
- The fixed weighting parameter (λ = 0.65) is chosen based on empirical observation rather than theoretical justification, without exploring whether this ratio is optimal across different conditions

## Confidence

**High Confidence**: The observation that models trained from scratch show lower BLEU scores compared to pretrained models is well-supported by standard evaluation metrics and aligns with established findings in the speech translation literature.

**Medium Confidence**: The claim about the three-stage training process and the difficulty of incorporating encoder information early in training is supported by source contribution measurements, but the methodology for measuring this contribution introduces some uncertainty.

**Low Confidence**: The assertion that 65% is the optimal source contribution ratio and that WeRC specifically addresses the root cause of training difficulties is based on empirical observation rather than theoretical analysis.

## Next Checks

1. **Source Contribution Measurement Validation**: Reproduce the source contribution analysis using alternative interpretability methods (e.g., attention visualization, feature importance scores) to verify whether the three-stage training process and the 65% target ratio are consistent across different measurement approaches.

2. **Architecture Ablation Study**: Implement WeRC modifications with different residual connection architectures (e.g., gated residual connections, learnable weighting parameters) to determine whether the specific formulation is critical or whether the benefits come from the general principle of enforcing early source integration.

3. **Cross-Lingual Generalization Test**: Apply the WeRC method to language pairs beyond the MuST-C dataset, particularly those with different linguistic properties (e.g., tonal languages, languages with rich morphology, distant language pairs), to assess whether the 65% source contribution target remains optimal across diverse translation scenarios.