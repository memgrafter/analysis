---
ver: rpa2
title: Boost Your Human Image Generation Model via Direct Preference Optimization
arxiv_id: '2405.20216'
source_url: https://arxiv.org/abs/2405.20216
tags:
- images
- image
- stage
- arxiv
- hg-dpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating realistic human
  images using text-to-image diffusion models. The proposed method, HG-DPO, incorporates
  real images as winning images in the Direct Preference Optimization (DPO) framework,
  encouraging the model to produce outputs that resemble real images rather than generated
  ones.
---

# Boost Your Human Image Generation Model via Direct Preference Optimization

## Quick Facts
- arXiv ID: 2405.20216
- Source URL: https://arxiv.org/abs/2405.20216
- Authors: Sanghyeon Na; Yonggyu Kim; Hyunjoon Lee
- Reference count: 40
- Primary result: Introduces HG-DPO, a three-stage curriculum learning approach that uses real images as winning images in Direct Preference Optimization to generate realistic human images

## Executive Summary
This paper addresses the challenge of generating realistic human images using text-to-image diffusion models by introducing HG-DPO (Human image Generation through DPO). The key innovation is using real images as winning images in the Direct Preference Optimization framework, which encourages the model to produce outputs that resemble real images rather than generated ones. To overcome the difficulty of directly optimizing with real images, the authors propose a three-stage curriculum learning approach that gradually transitions from generated to real images through intermediate domains. Experimental results demonstrate that HG-DPO significantly outperforms existing methods on various metrics including human preference scores, image-text alignment, and image realism.

## Method Summary
HG-DPO employs a three-stage curriculum learning framework to generate realistic human images. The easy stage uses generated images as winning images to learn basic human preferences, the normal stage introduces intermediate domains to improve visual quality, and the hard stage refines fine details using real images. The method uses Direct Preference Optimization (DPO) with real images as winning images and generated images as losing images. Statistics matching loss is applied in the easy stage to prevent color shift artifacts. LoRA is used for efficient training of both the U-Net and text encoder. The training proceeds through 300k steps in the easy stage, 20k in the normal stage, and 20k in the hard stage.

## Key Results
- HG-DPO outperforms existing methods on human preference metrics (P-Score, HPS, I-Reward, AES)
- Achieves superior image-text alignment (CLIP scores) and realism metrics (FID, CI-Q, CI-S, ATHEC)
- Effectively adapts to personalized text-to-image tasks without additional training
- Successfully reduces color shift artifacts through statistics matching loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using real images as winning images in DPO drives the model to generate outputs that resemble real images rather than generated ones
- Mechanism: The preference structure penalizes outputs that resemble generated (losing) images while encouraging outputs more similar to real (winning) images, effectively guiding the model toward real image characteristics
- Core assumption: The model can learn to generate images that share statistical properties with real images through preference-based training
- Evidence anchors:
  - [abstract] "propose an enhanced DPO approach that incorporates high-quality real images as winning images, encouraging outputs to resemble real images rather than generated ones"
  - [section] "HG-DPO penalizes outputs that resemble generated (losing) images, while encouraging outputs more similar to real (winning) images"
- Break condition: If the domain gap between generated and real images is too large for the model to bridge through preference optimization alone

### Mechanism 2
- Claim: Curriculum learning enables gradual improvement by starting with easier tasks and progressing to harder ones
- Mechanism: The three-stage approach (easy→normal→hard) uses increasingly challenging winning image domains, allowing the model to incrementally learn human preferences, visual quality, and fine details
- Core assumption: Gradual progression from generated to real images reduces training difficulty compared to direct optimization with real images
- Evidence anchors:
  - [abstract] "our approach, HG-DPO (Human image Generation through DPO), employs a novel curriculum learning framework that gradually improves the output of the model toward greater realism"
  - [section] "HG-DPO training consists of three stages: easy, normal, and hard. To create tasks of varying difficulty at each stage, each stage utilizes a dataset constructed in a different manner"
- Break condition: If intermediate domains don't effectively bridge the gap between generated and real images

### Mechanism 3
- Claim: Statistics matching loss prevents color shift artifacts that occur when model latent statistics diverge from the base model
- Mechanism: By matching channel-wise mean statistics between latents sampled by the updated model and the base model, the method maintains color consistency while allowing other improvements
- Core assumption: Color shift artifacts are caused by divergence in latent statistics between the updated and base models
- Evidence anchors:
  - [section] "This issue arises from the statistics of latents sampled by ϵE diverging from those sampled by ϵbase. To address this, we design a statistics matching loss to prevent this divergence"
  - [section] "Lstat only matches the mean because it sufficiently resolves the color shift artifacts"
- Break condition: If mean statistics alone are insufficient to prevent visual artifacts

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Provides the framework for training with preferred (winning) and non-preferred (losing) image pairs
  - Quick check question: How does DPO differ from traditional reinforcement learning approaches in terms of reward modeling?

- Concept: Curriculum Learning
  - Why needed here: Enables gradual progression from easy to hard tasks, essential for bridging the domain gap between generated and real images
  - Quick check question: What are the key differences between the three stages (easy, normal, hard) in terms of winning image domains?

- Concept: Diffusion Models and Latent Space
  - Why needed here: Understanding how images are represented and manipulated in latent space is crucial for implementing the statistics matching loss
  - Quick check question: Why is matching latent statistics important for maintaining visual consistency during model updates?

## Architecture Onboarding

- Component map: Base model -> U-Net with LoRA layers -> Text encoder with LoRA layers -> Statistics matching loss (easy stage only) -> Curriculum learning pipeline (easy→normal→hard)

- Critical path: Image generation → scoring → dataset construction → LoRA training → statistics matching (easy stage only) → text encoder enhancement

- Design tradeoffs:
  - Using LoRA instead of full fine-tuning for efficiency
  - Three-stage curriculum vs. single-stage training
  - Mean-only statistics matching vs. full covariance matching

- Failure signatures:
  - Color shift artifacts (indicates need for statistics matching)
  - Unrealistic poses/anatomy (indicates easy stage issues)
  - Poor fine details (indicates hard stage issues)
  - Weak image-text alignment (indicates text encoder issues)

- First 3 experiments:
  1. Implement easy stage with generated winning images and evaluate for color shift artifacts
  2. Add statistics matching loss and verify artifact reduction
  3. Implement normal stage with intermediate domains and evaluate visual quality improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HG-DPO compare when using real images as winning images directly versus using intermediate domain images (t1) in the hard stage?
- Basis in paper: [explicit] The paper states that using images from the intermediate domain t1 as winning images in the hard stage leads to slightly better quantitative performance compared to using real images directly.
- Why unresolved: The paper does not provide a detailed comparison of the performance between these two approaches. It only mentions that the results are similar, with a slight improvement in CI-Q scores for the intermediate domain approach.
- What evidence would resolve it: A direct quantitative comparison of the performance of HG-DPO using real images versus intermediate domain images (t1) as winning images in the hard stage, across all relevant metrics.

### Open Question 2
- Question: What is the optimal value of N (number of images per prompt in the image pool) for the easy stage of HG-DPO?
- Basis in paper: [inferred] The paper discusses the effectiveness of using an image pool (N > 2) compared to generating exactly two images per prompt (N = 2) in the easy stage. It shows that N > 2 leads to better results, but it does not explore the optimal value of N.
- Why unresolved: The paper does not provide a systematic analysis of how the performance of HG-DPO changes with different values of N in the easy stage.
- What evidence would resolve it: A comprehensive analysis of the performance of HG-DPO with varying values of N (e.g., N = 2, 5, 10, 20) in the easy stage, across all relevant metrics.

### Open Question 3
- Question: How does the curriculum learning approach in HG-DPO compare to other curriculum learning methods in terms of improving human image generation?
- Basis in paper: [explicit] The paper compares HG-DPO to Curriculum-DPO, which also employs curriculum learning but uses only generated images. HG-DPO outperforms Curriculum-DPO, but it does not compare to other curriculum learning methods.
- Why unresolved: The paper does not explore how the specific curriculum learning approach in HG-DPO (using real images as winning images and generated images as losing images) compares to other curriculum learning strategies in the context of human image generation.
- What evidence would resolve it: A comparative study of HG-DPO's curriculum learning approach against other curriculum learning methods (e.g., different task difficulty progressions, different winning/losing image selection strategies) in terms of their effectiveness in improving human image generation.

## Limitations

- Three-stage curriculum approach requires careful dataset construction and may not generalize to domains beyond human image generation
- Effectiveness of intermediate domains (SDRecon outputs) as a bridge between generated and real images is promising but unproven outside this specific context
- Method's reliance on large-scale paired datasets (300k images) may limit its applicability to data-constrained scenarios

## Confidence

- **High confidence**: The effectiveness of DPO with real images as winning images (Mechanism 1) - supported by strong experimental evidence across multiple metrics
- **Medium confidence**: The curriculum learning framework (Mechanism 2) - theoretically sound but effectiveness depends on intermediate domain quality
- **Medium confidence**: The statistics matching loss for preventing color shift (Mechanism 3) - empirically validated but may not address all visual artifacts

## Next Checks

1. Test the method's generalization to other domains (e.g., animal images, vehicles) to verify curriculum learning effectiveness beyond human images
2. Compare full covariance matching versus mean-only statistics matching to determine if additional visual artifacts can be prevented
3. Evaluate the method with smaller dataset sizes (50k, 100k images) to assess scalability and data efficiency