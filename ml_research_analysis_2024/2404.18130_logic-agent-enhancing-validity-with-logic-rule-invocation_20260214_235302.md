---
ver: rpa2
title: 'Logic Agent: Enhancing Validity with Logic Rule Invocation'
arxiv_id: '2404.18130'
source_url: https://arxiv.org/abs/2404.18130
tags:
- reasoning
- logical
- logic
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Logic Agent (LA) framework to improve
  the logical reasoning capabilities of large language models (LLMs). LA guides LLMs
  through complex reasoning tasks by leveraging propositional logic and a set of predefined
  functions that apply deduction rules, ensuring structured and coherent inference
  chains.
---

# Logic Agent: Enhancing Validity with Logic Rule Invocation

## Quick Facts
- arXiv ID: 2404.18130
- Source URL: https://arxiv.org/abs/2404.18130
- Authors: Hanmeng Liu; Zhiyang Teng; Chaoli Zhang; Yue Zhang
- Reference count: 11
- One-line primary result: LA improves logical reasoning accuracy across diverse models and tasks compared to direct answering and CoT

## Executive Summary
This paper introduces the Logic Agent (LA) framework to enhance the logical reasoning capabilities of large language models (LLMs). LA leverages propositional logic and a set of predefined inference functions to guide LLMs through complex reasoning tasks, ensuring valid and coherent inference chains. Experiments with models like GPT-4, GPT-3.5, LLaMA-2, and Mixtral on datasets such as ReClor, AR-LSAT, LogiQA22, ConTRoL, NaN-NLI, RuleTaker, and ProofWriter demonstrate consistent performance gains when using LA compared to direct answering and chain-of-thought approaches.

## Method Summary
LA transforms LLMs into logic agents by converting natural language inputs into structured logic forms and applying predefined inference rules through callable functions. The framework uses propositional logic constructs (Atoms, Not, And, Or, Implies, etc.) and inference rules (Contrapositive, Transitive, De Morgan's, etc.) to guide the LLM's reasoning process. By constraining the LLM's output to follow valid logic chains derived from these rules, LA ensures logical coherence and interpretability. The approach scales across model sizes, with even base models showing improvements in logical reasoning accuracy.

## Key Results
- GPT-3.5 with LA achieves 71.30% accuracy on RuleTaker, compared to 55.33% with direct answering.
- GPT-4 with LA reaches 65.84% on RuleTaker, outperforming both its direct and chain-of-thought variants.
- LA consistently improves performance across diverse models (GPT-4, GPT-3.5, LLaMA-2, Mixtral) and tasks (multi-choice reading comprehension, natural language inference, true-or-false).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LA transforms LLMs into logic agents by using predefined functions that apply propositional logic rules, ensuring valid reasoning chains.
- Mechanism: Natural language inputs are first converted into structured logic forms, then specific rule functions like `Contrapositive` and `Transitive` are called to derive valid inferences. The LLM acts as a decision-maker, choosing which rule to apply at each step.
- Core assumption: LLMs can accurately parse natural language into formal logic and reliably select the correct inference rule when prompted.
- Evidence anchors:
  - [abstract] "LA transforms LLMs into logic agents that dynamically apply propositional logic rules"
  - [section] "We let an LLM serve as a decision-making agent and make a callable symbolic reasoning agent by assembling a set of essential formal logical rules"
  - [corpus] Weak: No corpus neighbor directly discusses the specific agent-based logic rule invocation mechanism.
- Break condition: If the LLM mis-parses the input logic or chooses an incorrect rule, the resulting inference chain will be invalid despite the functions being correct.

### Mechanism 2
- Claim: LA's rule-guided generation enforces logical coherence by constraining the LLM's output to follow valid inference paths.
- Mechanism: After parsing and rule application, the LLM is prompted to generate conclusions strictly based on the outputs of the rule functions. This prevents the model from making unsupported leaps in reasoning.
- Core assumption: The LLM can reliably follow structured prompts that limit generation to valid logic chains.
- Evidence anchors:
  - [abstract] "LA is designed to steer Large Language Models (LLMs) toward a trajectory of enhanced logical coherence and interpretability by introducing symbolic reasoning"
  - [section] "We prompt LLMs to discern and decide upon the most appropriate rule to apply in varying states of reasoning"
  - [corpus] Weak: No corpus neighbor directly discusses constrained generation for logical reasoning.
- Break condition: If the LLM ignores the constraints or generates outputs that don't follow from the rule outputs, logical coherence is lost.

### Mechanism 3
- Claim: LA scales across model sizes and improves logical reasoning accuracy compared to direct answering and CoT.
- Mechanism: By offloading the complexity of rule application to predefined functions, even smaller or base models can perform valid logical reasoning without needing to internalize all rules.
- Core assumption: The overhead of function calls and structured prompting does not overwhelm the model's capacity, and the benefits of valid reasoning outweigh any added complexity.
- Evidence anchors:
  - [abstract] "demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning"
  - [section] "Across the three datasets, we observed a noticeable decrease in accuracy when the models were deprived of the constrained generation process"
  - [corpus] Weak: No corpus neighbor directly discusses scaling logic reasoning across diverse model sizes.
- Break condition: If the function call overhead or prompt complexity exceeds the model's context or reasoning capacity, performance may degrade.

## Foundational Learning

- Concept: Propositional logic syntax and semantics
  - Why needed here: LA relies on translating natural language into propositional logic (e.g., `Implies`, `Not`, `And`, `Or`) and applying rules to these forms.
  - Quick check question: Given "If it rains, the ground gets wet", what is the propositional logic form using `Implies` and `Atom`?
- Concept: Inference rules (contrapositive, transitive, De Morgan's, etc.)
  - Why needed here: LA uses these rules as callable functions to derive valid inferences from parsed logic.
  - Quick check question: What is the contrapositive of `Implies(P, Q)`?
- Concept: Logic parsing from natural language
  - Why needed here: Accurate translation from context to logic is the first step in LA's pipeline.
  - Quick check question: How would you parse "A person can only be humble if they recognize their shortcomings" into logic functions?

## Architecture Onboarding

- Component map:
  - Input parser -> Logic form generator -> Rule function library -> LLM agent -> Output formatter
- Critical path:
  1. Parse input into logic forms
  2. LLM selects rule based on current state
  3. Apply rule function to generate new logic
  4. LLM generates conclusion from updated logic
  5. Format and return answer
- Design tradeoffs:
  - Function granularity: Finer functions (e.g., separate `Contrapositive` and `Transitive`) give more control but increase prompt complexity
  - Prompt design: Must balance clarity and brevity to avoid overwhelming the LLM
  - Parser choice: Default parser is simpler; GPT-4 parser is more accurate but adds cost
- Failure signatures:
  - Incorrect parsing → invalid logic forms → wrong answers
  - Wrong rule selection → invalid inference chain
  - LLM ignoring constraints → unsupported conclusions
- First 3 experiments:
  1. Run LA on a simple entailment task (e.g., RuleTaker) with a base model (e.g., LLaMA-2) to verify basic parsing and rule application.
  2. Compare LA vs. direct vs. CoT on a multi-choice reading comprehension dataset (e.g., ReClor) to measure accuracy gains.
  3. Test LA with GPT-4 as the parser on a challenging dataset (e.g., LogiQA22) to evaluate the benefit of neural parsing.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the Logic Agent framework be extended to handle more complex logical reasoning tasks, such as those involving predicate logic or modal logic?
  - Basis in paper: [inferred] The paper mentions that LA currently focuses on propositional logic and certain deduction rules, but its applicability to other forms of logic has not been extensively explored.
  - Why unresolved: The paper does not provide a clear path for extending LA to handle more complex logical reasoning tasks, such as those involving predicate logic or modal logic.
  - What evidence would resolve it: A detailed analysis of the challenges and potential solutions for extending LA to handle more complex logical reasoning tasks, along with experimental results demonstrating the effectiveness of the extended framework.

- **Open Question 2**: How can the performance of the Logic Agent framework be improved for base models, such as DAVINCI-002 and LLaMA-2-13B, which currently show less pronounced improvements compared to advanced models like GPT-4?
  - Basis in paper: [explicit] The paper mentions that while LA improves the performance of base models, the improvements are not as pronounced as those seen with advanced models like GPT-4.
  - Why unresolved: The paper does not provide a clear explanation for why base models show less improvement and what specific strategies could be employed to enhance their performance with LA.
  - What evidence would resolve it: A comprehensive analysis of the factors limiting the performance of base models with LA, along with experimental results demonstrating the effectiveness of proposed strategies for improving their performance.

- **Open Question 3**: How can the Logic Agent framework be adapted to handle real-world scenarios or domains beyond those tested in the paper, such as medical diagnosis or legal reasoning?
  - Basis in paper: [inferred] The paper mentions that the generalizability of LA to real-world scenarios or domains beyond those tested remains an area for future investigation.
  - Why unresolved: The paper does not provide a clear methodology for adapting LA to handle real-world scenarios or domains beyond those tested.
  - What evidence would resolve it: A detailed analysis of the challenges and potential solutions for adapting LA to handle real-world scenarios or domains beyond those tested, along with experimental results demonstrating the effectiveness of the adapted framework.

## Limitations
- Exact prompt templates and examples used to guide LLMs in applying logic rules are not specified, which could significantly impact reproducibility.
- Logic parser's implementation details, particularly its handling of ambiguous or complex natural language statements, are unclear.
- Specific performance improvements over CoT are difficult to fully evaluate without access to the exact prompt templates and parser implementations.

## Confidence
- **High confidence**: The general mechanism of using predefined logic rule functions to guide LLM reasoning is sound and well-supported by the results.
- **Medium confidence**: The scalability claims across diverse model sizes are based on the reported experiments but would benefit from additional ablation studies.
- **Low confidence**: The specific performance improvements over CoT are difficult to fully evaluate without access to the exact prompt templates and parser implementations.

## Next Checks
1. Recreate the prompt templates and logic parser based on the paper's description, then test on a simple dataset like RuleTaker to verify basic functionality.
2. Conduct an ablation study comparing LA with and without each rule function to quantify their individual contributions to performance gains.
3. Test LA on out-of-domain logical reasoning tasks not covered in the paper to assess generalization beyond the reported datasets.