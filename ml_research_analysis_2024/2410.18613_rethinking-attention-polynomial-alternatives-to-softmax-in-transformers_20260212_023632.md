---
ver: rpa2
title: 'Rethinking Attention: Polynomial Alternatives to Softmax in Transformers'
arxiv_id: '2410.18613'
source_url: https://arxiv.org/abs/2410.18613
tags:
- softmax
- attention
- norm
- scale
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the conventional view that softmax attention's
  success in transformers stems from its properties of positivity, normalization,
  and sparsity. Instead, it argues that softmax's effectiveness arises from its implicit
  regularization of the Frobenius norm of the attention matrix, which stabilizes training.
---

# Rethinking Attention: Polynomial Alternatives to Softmax in Transformers

## Quick Facts
- arXiv ID: 2410.18613
- Source URL: https://arxiv.org/abs/2410.18613
- Reference count: 40
- Primary result: Polynomial activations can match or outperform softmax attention by providing implicit regularization of the attention matrix's Frobenius norm

## Executive Summary
This paper challenges the conventional understanding of why softmax attention works in transformers, arguing that its success stems not from its properties of positivity, normalization, and sparsity, but rather from its implicit regularization of the attention matrix's Frobenius norm. Based on this insight, the authors propose polynomial activations as alternatives to softmax. Extensive experiments across multiple transformer architectures and tasks demonstrate that certain polynomials can match or exceed softmax performance while violating traditional softmax properties. The work suggests that the three conventional properties of softmax are not necessary for effective attention mechanisms.

## Method Summary
The authors propose replacing softmax with polynomial activations in transformer attention mechanisms. The key insight is that softmax implicitly regularizes the Frobenius norm of the attention matrix, which stabilizes training. They derive theoretical conditions under which polynomials can provide similar regularization effects while violating traditional softmax properties. The method involves selecting appropriate polynomial functions that approximate the regularization behavior of softmax while potentially offering computational advantages or improved performance characteristics.

## Key Results
- Polynomial activations can match or outperform softmax across multiple tasks including image classification, object detection, and text classification
- The three conventional properties of softmax (positivity, normalization, sparsity) are not necessary for effective attention
- Certain polynomial functions provide implicit regularization equivalent to softmax's Frobenius norm control
- Results demonstrate consistent improvements across vision transformers and smaller-scale models

## Why This Works (Mechanism)
The paper argues that softmax's effectiveness in transformers stems from its implicit regularization of the attention matrix's Frobenius norm, which stabilizes training. Polynomial activations can be designed to induce similar regularization effects while violating traditional softmax properties. This regularization prevents the attention matrix from becoming too large or too sparse during training, leading to more stable optimization dynamics.

## Foundational Learning
- Frobenius norm regularization: Why needed - controls attention matrix magnitude to stabilize training; Quick check - measure attention matrix norm during training
- Polynomial activation functions: Why needed - provide alternative to softmax with similar regularization properties; Quick check - verify polynomial satisfies theoretical regularization conditions
- Transformer attention mechanism: Why needed - context for understanding where polynomials replace softmax; Quick check - trace attention computation flow

## Architecture Onboarding
Component map: Query/Key -> Polynomial Activation -> Attention Matrix -> Weighted Sum
Critical path: Input embeddings → Linear projections → Polynomial activation → Attention computation → Output
Design tradeoffs: Polynomials vs softmax - computational efficiency vs established behavior; Degree of polynomial vs regularization strength
Failure signatures: Unstable training, attention collapse, poor gradient flow
First experiments:
1. Replace softmax with simple quadratic polynomial in ViT on CIFAR-10
2. Test polynomial degree sensitivity on attention matrix norm stability
3. Compare polynomial vs softmax training curves on attention matrix statistics

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on simplifying assumptions about polynomial-regularization relationships
- Experiments predominantly use vision transformers and smaller-scale models
- Limited testing on large language models raises questions about generalizability
- Analysis of why certain polynomials work better than others remains somewhat heuristic

## Confidence
- Theoretical framework connecting polynomial activations to regularization: Medium
- Empirical performance claims: High
- Generalization to large-scale language models: Low

## Next Checks
1. Test polynomial activations on large language models (Llama, GPT-style architectures) to assess scalability and performance retention
2. Conduct ablation studies varying polynomial degrees and coefficients systematically to map the design space
3. Analyze failure cases where polynomial attention underperforms softmax to identify limitations and edge cases