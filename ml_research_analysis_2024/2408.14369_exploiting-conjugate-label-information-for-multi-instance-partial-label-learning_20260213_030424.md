---
ver: rpa2
title: Exploiting Conjugate Label Information for Multi-Instance Partial-Label Learning
arxiv_id: '2408.14369'
source_url: https://arxiv.org/abs/2408.14369
tags:
- label
- learning
- loss
- mipl
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-instance partial-label learning (MIPL),
  where each training sample is a bag of instances associated with a candidate label
  set containing one true label and several false positives. Existing MIPL algorithms
  primarily focus on mapping bags to candidate labels, ignoring the intrinsic properties
  of the label space and supervised information from non-candidate labels.
---

# Exploiting Conjugate Label Information for Multi-Instance Partial-Label Learning

## Quick Facts
- arXiv ID: 2408.14369
- Source URL: https://arxiv.org/abs/2408.14369
- Authors: Wei Tang; Weijia Zhang; Min-Ling Zhang
- Reference count: 10
- Key outcome: ELIMIPL exploits conjugate label information to significantly outperform existing MIPL algorithms with average accuracy improvements of at least 0.016-0.17 on benchmark datasets

## Executive Summary
This paper addresses multi-instance partial-label learning (MIPL), where each training sample is a bag of instances associated with a candidate label set containing one true label and several false positives. Existing MIPL algorithms primarily focus on mapping bags to candidate labels, ignoring the intrinsic properties of the label space and supervised information from non-candidate labels. The authors propose ELIMIPL, which exploits conjugate label information (CLI) from both candidate and non-candidate label sets, along with the sparsity of the true label matrix. The method uses a scaled additive attention mechanism to aggregate bag-level features and introduces three key components: a mapping loss to disambiguate candidate labels, a sparsity loss to capture mutually exclusive relationships among candidates, and an inhibition loss to suppress predictions on non-candidate labels.

## Method Summary
ELIMIPL addresses MIPL by exploiting conjugate label information through a three-component loss framework. The method employs a scaled additive attention mechanism to aggregate instance features into bag representations, using a softmax with scaling factor 1/√l to distinguish positive and negative instances. The mapping loss disambiguates candidate labels by encouraging the model to assign higher probabilities to true labels. The sparsity loss enforces mutually exclusive relationships among candidate labels by minimizing the ℓ1 norm of predicted probabilities, promoting one-hot like outputs. The inhibition loss suppresses predictions on non-candidate labels by explicitly penalizing high probabilities for incorrect classes. The model is trained with SGD optimizer using momentum 0.9 and weight decay 0.0001, with learning rate selected from {0.01, 0.05} with cosine annealing for 100 epochs.

## Key Results
- ELIMIPL achieves average accuracy improvements of at least 0.016 over DEMIPL and 0.032-0.17 over MIPL GP on MNIST-MIPL and FMNIST-MIPL datasets
- Performance gains are even larger on more challenging datasets including Birdsong-MIPL, SIV AL-MIPL, and CRC-MIPL
- Ablation studies confirm the importance of CLI, showing substantial performance boosts when exploiting both candidate and non-candidate label information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The scaled additive attention mechanism distinguishes positive and negative instances more effectively than sigmoid-based attention by producing a wider range of attention scores.
- Mechanism: Instead of using sigmoid to constrain attention scores to (0,1), the method applies a softmax with a scaling factor of 1/√l. This increases the dynamic range of scores, allowing the model to better differentiate between relevant and irrelevant instances within a bag.
- Core assumption: The scaling factor prevents vanishing gradients while still maintaining discriminative power.
- Evidence anchors:
  - [section]: "we introduce an additive attention mechanism calculating attention scores by the softmax function to distinguish instances, equipped with a scaling factor to prevent vanishing gradients [Vaswani et al., 2017]."
  - [corpus]: No direct corpus evidence for this specific scaling choice; assumed from attention literature.
- Break condition: If the scaling factor is set too high, gradients may explode; if too low, scores may collapse and lose discriminative power.

### Mechanism 2
- Claim: Incorporating non-candidate label information via an inhibition loss improves disambiguation by suppressing incorrect class predictions.
- Mechanism: The inhibition loss explicitly penalizes the model for assigning high probabilities to labels known not to be correct for a given bag. This complements the candidate label loss by providing negative supervision.
- Core assumption: The non-candidate label set is accurate and can be trusted as hard negative examples.
- Evidence anchors:
  - [abstract]: "introduce an inhibition loss to diminish the model's predictions on the non-candidate labels."
  - [section]: "we introduce an inhibition loss as follows: Lin(Z, ¯S) = − 1/m Σ_i Σ_¯c∈¯Si log(1 − f¯c(zi)), where f¯c(·) denotes the classifier's prediction probability over the non-candidate label ¯c."
  - [corpus]: Weak corpus support; no cited prior work on this exact inhibition loss in MIPL.
- Break condition: If non-candidate labels are noisy or incomplete, the inhibition loss could mislead training.

### Mechanism 3
- Claim: The sparsity loss enforces mutually exclusive relationships among candidate labels by encouraging the model to predict one label with high probability and others with near-zero probability.
- Mechanism: By minimizing the ℓ1 norm of the product of predicted probabilities and the candidate label mask, the model is pushed toward sparse outputs, reflecting the assumption that exactly one label in the candidate set is true.
- Core assumption: Each bag has exactly one true label among its candidates, and the sparsity loss can approximate this constraint without explicit label knowledge.
- Evidence anchors:
  - [section]: "we employ the ℓ1 norm as a surrogate for the ℓ0 norm, promoting sparsity while allowing for efficient optimization [Wright and Ma, 2022]."
  - [corpus]: No direct corpus evidence for this sparsity loss in MIPL; assumed from prior PLL literature.
- Break condition: If the assumption of a single true label is violated (e.g., multi-label bags), sparsity loss could harm performance.

## Foundational Learning

- Concept: Multi-instance learning (MIL)
  - Why needed here: MIPL builds on MIL by extending it to handle partial labels; understanding bag-level vs instance-level supervision is critical.
  - Quick check question: In MIL, how is a bag labeled if it contains at least one positive instance?
- Concept: Partial-label learning (PLL)
  - Why needed here: PLL provides the foundation for handling ambiguous labels; MIPL merges this with MIL.
  - Quick check question: In PLL, what distinguishes a candidate label from the true label?
- Concept: Attention mechanisms in deep learning
  - Why needed here: The scaled additive attention mechanism is central to aggregating instance features into bag representations.
  - Quick check question: What is the difference between sigmoid-based and softmax-based attention in terms of score distribution?

## Architecture Onboarding

- Component map: Instance-level feature extractor (ψ1, ψ2) → Scaled additive attention → Bag-level representation → Classifier → Losses (mapping, sparsity, inhibition)
- Critical path: Feature extraction → Attention aggregation → Loss computation → Parameter update via SGD
- Design tradeoffs:
  - Attention mechanism choice: sigmoid vs softmax with scaling; softmax allows sharper distinctions but risks instability.
  - Loss weighting: µ and γ control sparsity and inhibition contributions; poor tuning can lead to over- or under-suppression.
  - Model capacity: Too shallow may underfit; too deep may overfit on small datasets.
- Failure signatures:
  - Low attention variance: indicates attention mechanism not distinguishing instances.
  - Equal prediction across candidate and non-candidate labels: suggests inhibition loss ineffective.
  - Sparse loss dominating training: may suppress true label predictions.
- First 3 experiments:
  1. Ablation study: Remove sparsity loss and inhibition loss separately to measure impact.
  2. Attention score distribution: Visualize attention scores for positive vs negative instances in test bags.
  3. Loss weight sweep: Tune µ and γ to find optimal balance between sparsity and inhibition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaled additive attention mechanism compare to other attention mechanisms (e.g., self-attention, sigmoid-based attention) in terms of disambiguation performance for MIPL?
- Basis in paper: [explicit] The paper introduces a scaled additive attention mechanism as a key component of ELIMIPL, claiming it provides better distinction between instances compared to existing sigmoid-based attention mechanisms used in previous MIPL algorithms like DEMIPL.
- Why unresolved: The paper does not provide a direct comparison between the scaled additive attention and other attention mechanisms. It only shows that ELIMIPL outperforms DEMIPL, which uses a sigmoid-based attention mechanism, but this could be due to other factors in the algorithm as well.
- What evidence would resolve it: A controlled experiment comparing the scaled additive attention mechanism with other attention mechanisms (e.g., self-attention, sigmoid-based attention) on the same MIPL datasets would be needed to determine its relative performance in terms of disambiguation.

### Open Question 2
- Question: What is the theoretical justification for using the ℓ1 norm as a surrogate for the ℓ0 norm in the sparsity loss, and how does this choice impact the performance and convergence of ELIMIPL?
- Basis in paper: [explicit] The paper mentions using the ℓ1 norm as a surrogate for the ℓ0 norm in the sparsity loss to promote sparsity while allowing for efficient optimization, but does not provide a theoretical justification or analyze the impact of this choice.
- Why unresolved: While the ℓ1 norm is a common convex relaxation of the ℓ0 norm, its effectiveness in this specific context (MIPL with conjugate label information) is not theoretically established. The impact on performance and convergence is also not explored.
- What evidence would resolve it: A theoretical analysis of the approximation error introduced by using the ℓ1 norm instead of the ℓ0 norm in the MIPL setting, along with empirical studies comparing different norms (e.g., ℓ0, ℓ1, ℓ2) in the sparsity loss, would be needed to understand the implications of this choice.

### Open Question 3
- Question: How does the performance of ELIMIPL vary with different levels of sparsity in the true label matrix, and what is the impact of this sparsity on the effectiveness of the conjugate label information?
- Basis in paper: [explicit] The paper mentions that the true label matrix is sparse and incorporates this sparsity into the algorithm through the sparsity loss, but does not analyze how different levels of sparsity affect the performance or the effectiveness of the conjugate label information.
- Why unresolved: The relationship between the sparsity of the true label matrix and the performance of ELIMIPL is not explored. It is unclear how the algorithm would perform if the true label matrix were less sparse, and whether the conjugate label information would still be as effective.
- What evidence would resolve it: Experiments varying the sparsity of the true label matrix in the MIPL datasets and analyzing the corresponding performance of ELIMIPL would be needed to understand the impact of sparsity on the algorithm's effectiveness. Additionally, a theoretical analysis of the role of sparsity in the conjugate label information framework would provide further insights.

## Limitations
- The computational complexity of the scaled additive attention mechanism and multiple loss functions could become prohibitive on extremely large datasets
- The method's reliance on accurate non-candidate label information introduces potential brittleness if label quality degrades
- While ablation studies show the importance of CLI, the relative contributions of candidate vs non-candidate label information are not fully disentangled

## Confidence
- High: Improved accuracy over baseline methods on benchmark datasets
- Medium: Mechanism explanations, particularly the scaling factor's role in attention
- Low: Generalizability to real-world applications with noisy labels

## Next Checks
1. Conduct experiments on larger-scale datasets to evaluate computational scalability and potential performance degradation.
2. Perform sensitivity analysis on the quality of non-candidate label information to assess robustness to label noise.
3. Design controlled experiments to separately quantify the contributions of candidate label information versus non-candidate label information in the CLI framework.