---
ver: rpa2
title: Experience-driven discovery of planning strategies
arxiv_id: '2412.03111'
source_url: https://arxiv.org/abs/2412.03111
tags:
- strategy
- strategies
- learning
- planning
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how people discover new planning strategies
  through metacognitive reinforcement learning (MCRL). The authors designed a novel
  experiment using the Mouselab-MDP paradigm, where participants had to learn a highly
  specific planning strategy that was unlikely to be part of their existing mental
  toolbox.
---

# Experience-driven discovery of planning strategies

## Quick Facts
- arXiv ID: 2412.03111
- Source URL: https://arxiv.org/abs/2412.03111
- Authors: Ruiqi He; Falk Lieder
- Reference count: 4
- Only about 29% of participants discovered the novel planning strategy after 120 trials

## Executive Summary
This paper investigates how people discover new planning strategies through metacognitive reinforcement learning (MCRL). The authors designed a novel experiment using the Mouselab-MDP paradigm, where participants had to learn a highly specific planning strategy that was unlikely to be part of their existing mental toolbox. Empirical results showed that while strategy discovery occurred, only about 29% of participants discovered the novel strategy after 120 trials. The authors then developed MCRL models and demonstrated their capability for strategy discovery. When fitted to human data, these models provided a better explanation of human strategy discovery than alternative learning mechanisms. However, the models exhibited a slower discovery rate than humans, suggesting room for improvement. The study highlights the importance of metacognitive learning in strategy discovery and contributes to understanding human cognition and developing adaptive AI models.

## Method Summary
The study used a novel Mouselab-MDP paradigm with 420 participants who completed 120 trials each to discover an optimal planning strategy. The experiment employed computational models based on metacognitive reinforcement learning (MCRL) that updated strategy parameters through reinforcement signals. The MCRL framework represented strategies using 63 features across six broader categories. Models were fitted to human data using Bayesian optimization for 60,000 iterations and compared against alternative learning mechanisms (RSSL, mental habit, non-learning) using Bayesian model selection with family-level aggregation.

## Key Results
- Only 29% of participants discovered the novel planning strategy after 120 trials
- MCRL models provided better explanations of human strategy discovery than alternative learning mechanisms
- When fitted to human data, MCRL models exhibited slower discovery rates than humans
- 61% of participants were best explained by metacognitive Reinforce models, while 39% were better accounted for by a value-free mental habit model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strategy discovery occurs through metacognitive reinforcement learning where participants update weights of features representing planning strategies based on feedback.
- Mechanism: Participants engage in metacognitive learning where they internally evaluate planning operations (clicks) and update strategy parameters through reinforcement signals (rewards/punishments from scores).
- Core assumption: The brain can learn to approximate optimal meta-decision-making through reinforcement learning mechanisms that operate on internal mental operations.
- Evidence anchors:
  - [abstract] "new planning strategies are discovered through metacognitive reinforcement learning"
  - [section] "metcognitive reinforcement learning (MCRL) framework to study the problem of deciding how to decide"
  - [corpus] Weak - corpus doesn't contain direct evidence for MCRL mechanism, though related concepts appear in neighboring papers
- Break condition: When feature representation fails to capture relevant strategies or when learning rate/exploration parameters prevent convergence to optimal strategy

### Mechanism 2
- Claim: Participants discover strategies by experiencing "Eureka moments" where they suddenly recognize environmental structure through exhaustive exploration.
- Mechanism: Some participants examine all nodes in early trials, rapidly learn the environment's structure, and then adopt the optimal strategy, leading to sudden performance improvements.
- Core assumption: Humans can engage in insight-based learning that allows rapid strategy discovery through complete information gathering.
- Evidence anchors:
  - [section] "21 out of 121 habitual participants examined all the nodes early on...thereby quickly learned the environment's structure, and then adopted the optimal strategy"
  - [section] "This type of insight-like learning was much less pronounced in other participant groups"
  - [corpus] Weak - corpus neighbors don't discuss insight-based learning mechanisms
- Break condition: When environmental complexity prevents complete exploration or when cognitive load inhibits sudden recognition of patterns

### Mechanism 3
- Claim: Strategy discovery involves a spectrum of learning mechanisms where different participants use different approaches (reinforcement learning, mental habits, or sudden insight).
- Mechanism: Individual differences in learning mechanisms lead to varied discovery patterns - some learn gradually through reinforcement, others through habitual repetition, and some through sudden insight.
- Core assumption: Multiple learning mechanisms can operate simultaneously or that different individuals default to different mechanisms.
- Evidence anchors:
  - [section] "Participants best explained by the mental habit model...performed better than those explained by the hybrid Reinforce mechanism"
  - [section] "This counterintuitive observation can, however, be explained by some participants classified as habitual learners showing rapid improvement"
  - [corpus] Moderate - neighboring papers discuss individual differences in cognitive mechanisms and metacognition
- Break condition: When model selection fails to distinguish between mechanisms or when participant behavior doesn't fit any existing model framework

## Foundational Learning

- Concept: Reinforcement Learning
  - Why needed here: The paper's models are based on reinforcement learning frameworks that update strategy parameters based on feedback
  - Quick check question: How does the Q-value update rule work in standard reinforcement learning, and how is it modified for metacognitive reinforcement learning?

- Concept: Feature Representation of Strategies
  - Why needed here: Strategies are represented as weighted combinations of features, requiring understanding of how features capture different aspects of planning behavior
  - Quick check question: What are the six broader categories of features used to represent planning strategies, and why is each category important?

- Concept: Bayesian Model Selection
  - Why needed here: The paper uses Bayesian model selection to compare different learning mechanisms and determine which best explains human data
  - Quick check question: How does exceedance probability help determine which model family best explains the data, and what does it mean when a model has 100% exceedance probability?

## Architecture Onboarding

- Component map: Experiment -> Data Collection -> Strategy Classification -> Model Fitting -> Model Selection -> Results Analysis
- Critical path: Participants complete Mouselab-MDP trials -> Strategies classified using 63 features -> Models fitted via Bayesian optimization -> Model selection using Bayesian criteria -> Results analyzed
- Design tradeoffs: Granular feature representation provides better strategy capture but increases computational complexity; hybrid models capture more variance but require more parameters; exhaustive exploration enables insight but may be cognitively expensive
- Failure signatures: Slow discovery rates suggest feature representation gaps; misclassification of insight learners as habit learners indicates model limitation; individual differences suggest need for mechanism diversity
- First 3 experiments:
  1. Test whether feature set captures all relevant intermediate strategies by simulating agent behavior with varying strategy representations
  2. Compare learning curves of models with and without model-based metareasoning features to isolate their contribution
  3. Test whether adding active learning mechanisms improves discovery rate to match human performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific features or mechanisms could capture "insight-like" learning moments that lead to sudden performance improvements in strategy discovery?
- Basis in paper: [explicit] The paper notes that some participants showed rapid improvement in the first 20 trials followed by a flattened learning curve, which current models don't capture
- Why unresolved: Current MCRL models exhibit gradual improvements rather than sudden jumps in performance
- What evidence would resolve it: Developing and testing models that incorporate mechanisms for sudden insight-based learning or active exploration, then comparing their fit to human data

### Open Question 2
- Question: How can we determine if the current feature set comprehensively captures all possible intermediate strategies participants might adopt during the discovery process?
- Basis in paper: [inferred] The paper states that while the current feature set effectively represents the optimal strategy, "we cannot ensure that it encompasses all potential intermediate strategies participants might have adopted"
- Why unresolved: The feature space may be incomplete, missing strategies that humans actually use during the learning process
- What evidence would resolve it: Analyzing human click patterns to identify novel strategies not captured by current features, then expanding the feature set and testing model performance

### Open Question 3
- Question: What factors contribute to the individual differences in metacognitive learning mechanisms observed in the study?
- Basis in paper: [explicit] The paper found that 61% of participants were better explained by metacognitive Reinforce while 39% were better accounted for by a value-free mental habit model
- Why unresolved: The paper doesn't investigate what characteristics (cognitive, demographic, or experiential) predict which learning mechanism an individual uses
- What evidence would resolve it: Conducting additional experiments to correlate individual differences in learning mechanisms with various participant characteristics and performing systematic analysis of these relationships

### Open Question 4
- Question: How can the MCRL models be modified to achieve discovery rates comparable to humans while maintaining their explanatory power?
- Basis in paper: [explicit] The paper found that when fitted to human data, the MCRL models exhibited a slower discovery rate than humans
- Why unresolved: The current models are too slow in discovering strategies despite having appropriate hyperparameters
- What evidence would resolve it: Developing enhanced versions of the models with improved learning mechanisms, then testing whether they can match human discovery rates while still accurately explaining human behavior patterns

## Limitations

- Only 29% of participants successfully discovered the novel strategy, raising questions about the robustness of MCRL as a general mechanism
- The distinction between insight-based learning and reinforcement-based learning remains unclear, as models struggled to capture rapid improvements
- The computational complexity of the 63-feature representation may not fully capture cognitive processes underlying strategy discovery

## Confidence

- High confidence: The experimental design effectively isolates strategy discovery from general planning competence, and the MCRL framework provides a plausible computational model for metacognitive learning.
- Medium confidence: The claim that multiple learning mechanisms operate simultaneously is supported by model selection results, but the distinction between mechanisms could be clearer.
- Low confidence: The assertion that the MCRL models capture human-like strategy discovery is undermined by the slower discovery rates in simulations compared to human participants.

## Next Checks

1. Test the feature representation's ability to capture intermediate strategies by analyzing the weight trajectories of participants who partially discovered the strategy.
2. Conduct ablation studies to determine which feature categories contribute most to strategy discovery and whether removing certain features affects model performance.
3. Compare the MCRL models against alternative cognitive architectures (e.g., hierarchical reinforcement learning) to assess whether simpler models can explain the data equally well.