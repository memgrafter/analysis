---
ver: rpa2
title: Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large
  Language Models
arxiv_id: '2402.16568'
source_url: https://arxiv.org/abs/2402.16568
tags:
- temporal
- question
- knowledge
- answer
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses temporal knowledge graph question answering
  (TKGQA), a task that requires answering questions with temporal constraints over
  dynamic structured knowledge. The authors propose a two-stage generative framework,
  GenTKGQA, that uses large language models (LLMs) for this task.
---

# Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models

## Quick Facts
- arXiv ID: 2402.16568
- Source URL: https://arxiv.org/abs/2402.16568
- Reference count: 24
- Key outcome: Proposes GenTKGQA, a two-stage framework that improves TKGQA performance by 11.3% in Hits@1 using subgraph retrieval and virtual knowledge indicators to bridge GNN and LLM representations.

## Executive Summary
This paper addresses temporal knowledge graph question answering (TKGQA), a task that requires answering questions with temporal constraints over dynamic structured knowledge. The authors propose a two-stage generative framework, GenTKGQA, that uses large language models (LLMs) for this task. In the first stage, subgraph retrieval, the LLM mines temporal constraints and structural links from questions to narrow down the search space of relevant facts. In the second stage, answer generation, virtual knowledge indicators are designed to fuse graph neural network signals of the subgraph with LLM text representations, enabling deep understanding of temporal order and structural dependencies. Experiments on two widely used datasets show that GenTKGQA significantly outperforms existing methods, especially on complex question types, achieving 11.3% improvement in Hits@1 metric. The framework effectively leverages LLM's intrinsic knowledge and enables accurate temporal reasoning over structured data.

## Method Summary
GenTKGQA is a two-stage generative framework for TKGQA that uses LLMs. In the first stage, subgraph retrieval, the LLM is prompted with in-context learning (ICL) to mine temporal constraints and structural links from the question, retrieving a relevant subgraph from the TKG. In the second stage, answer generation, a temporal GNN (T-GNN) encodes the subgraph into embeddings, which are then fused with LLM text representations via virtual knowledge indicators [SUB], [REL], [OBJ]. The LLM is fine-tuned with instruction tuning (IT) to generate answers using the evidence subgraph. The framework is evaluated on CronQuestions and TimeQuestions datasets using Hits@1 and Hits@10 metrics.

## Key Results
- GenTKGQA achieves 11.3% improvement in Hits@1 over existing methods on TKGQA datasets.
- The two-stage approach significantly outperforms single-stage methods, especially on complex question types.
- Virtual knowledge indicators effectively bridge GNN and LLM representations, enabling deep understanding of temporal order and structural dependencies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage approach improves LLM performance on temporal reasoning by first reducing the search space and then deeply integrating graph structure.
- Mechanism: Subgraph retrieval phase uses LLM's intrinsic knowledge to mine temporal constraints and structural links, reducing noise. Answer generation phase fuses GNN embeddings with LLM text representations via virtual knowledge indicators, enabling non-shallow understanding of temporal order and dependencies.
- Core assumption: LLMs can extract relevant structural and temporal constraints from questions without training, and deep fusion of GNN and text representations improves reasoning over shallow concatenation.
- Evidence anchors:
  - [abstract] "we exploit LLM’s intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space"
  - [section 4.1] "we utilize the LLM’s internal knowledge to mine structural connections between entities and time constraints in the problem without extra training"
  - [corpus] Weak - corpus papers discuss KGQA but not specifically this two-stage TKGQA approach with deep fusion.
- Break condition: If the LLM cannot accurately extract temporal constraints or structural links, the subgraph retrieval fails and noise is not reduced, hurting performance.

### Mechanism 2
- Claim: Virtual knowledge indicators bridge GNN and LLM representations to enable structured reasoning.
- Mechanism: [SUB], [REL], [OBJ] tokens are designed to carry GNN embeddings enriched with temporal info. Linear projection aligns them to LLM's text space. This non-shallow fusion helps LLMs understand graph structure.
- Core assumption: Adding special tokens with GNN embeddings and projecting them into LLM's embedding space allows structured knowledge to influence reasoning.
- Evidence anchors:
  - [section 4.2.2] "we design three novel virtual knowledge indicators to bridge the links between pre-trained GNN signals of the temporal subgraph and text representations of the LLM"
  - [section 4.2.2] "we design three knowledge indicators to link graph signals and input prompt text, namely [SUB], [REL] and [OBJ]"
  - [corpus] Weak - corpus discusses KGQA with LLMs but not this specific virtual indicator approach.
- Break condition: If the projection layer is not learned well or indicators are not positioned correctly in prompt, the fusion fails to convey structure.

### Mechanism 3
- Claim: Instruction tuning with evidence prompts enables LLM to leverage retrieved subgraph for answer generation.
- Mechanism: Fine-tuning open-source LLM with instruction template containing question and evidence set, optimized to generate answers. This teaches model to use subgraph context.
- Core assumption: Fine-tuning with instruction tuning format and evidence context teaches LLM to reason over structured knowledge.
- Evidence anchors:
  - [section 4.2.2] "we fine-tune the open-source LLM to deeply understand the temporal order and structural dependencies of the retrieved query-relevant facts"
  - [section 4.2.2] "we employ a simple linear layer Wp to project them into the textual representation space of the LLM. The final input prompt sequence S = V : I : Q : A"
  - [corpus] Weak - corpus discusses LLMs for KGQA but not this specific instruction tuning with evidence.
- Break condition: If fine-tuning data is too small or template is not clear, model fails to learn to use evidence.

## Foundational Learning

- Concept: Temporal Knowledge Graphs (TKGs)
  - Why needed here: The task involves answering questions over TKGs with time constraints, so understanding TKG structure and how facts are represented with timestamps is essential.
  - Quick check question: What is the format of a fact in a TKG and how does it differ from a regular KG?

- Concept: Graph Neural Networks (GNNs) for TKGs
  - Why needed here: The method uses a T-GNN to encode subgraph structure and temporal info into embeddings that are fused with LLM representations. Understanding GNN message passing and how temporal info is incorporated is key.
  - Quick check question: How does the T-GNN compute messages between nodes and what temporal info is included?

- Concept: Large Language Models (LLMs) for structured reasoning
  - Why needed here: The method leverages LLM's reasoning ability and intrinsic knowledge for subgraph retrieval and uses instruction tuning to teach it to reason over structured data. Understanding LLM prompting and fine-tuning is important.
  - Quick check question: What are the key differences between in-context learning and instruction tuning for adapting LLMs to new tasks?

## Architecture Onboarding

- Component map:
  Input: Natural language question
  Subgraph Retrieval: LLM with prompt templates for relation ranking and time mining
  T-GNN: Encodes retrieved subgraph into embeddings
  Virtual Knowledge Indicators: Bridge GNN and LLM representations
  Instruction Tuning: Fine-tune LLM to generate answers using evidence
  Output: List of 10 most relevant answers

- Critical path:
  1. Question → Relation Ranking (LLM) → Top-k relations
  2. Question + relations → Time Mining (LLM) → Temporal constraints
  3. Relations + constraints → Subgraph retrieval from TKG
  4. Subgraph → T-GNN encoding → Virtual indicators
  5. Question + indicators → Instruction tuning → Answer generation

- Design tradeoffs:
  - Using LLM for subgraph retrieval vs rule-based: LLM can handle complex questions but may be less precise than rules. Using few-shot prompts keeps cost low.
  - Deep fusion via indicators vs shallow concatenation: Deep fusion may capture more structure but adds complexity. Indicators are simple to implement.
  - Instruction tuning vs zero-shot: Tuning improves performance but requires data and compute. Few-shot prompts could be an alternative.

- Failure signatures:
  - Poor Hits@1/10: Subgraph retrieval is not accurate or indicators are not conveying structure well.
  - Hallucinations in answers: Instruction tuning is not effective or evidence is not being used properly.
  - Slow inference: LLM calls in subgraph retrieval are expensive. Could cache or use smaller model.

- First 3 experiments:
  1. Ablation: Remove subgraph retrieval, feed all facts to LLM, measure performance drop.
  2. Ablation: Remove virtual indicators, use shallow concatenation, measure performance drop.
  3. Ablation: Remove instruction tuning, use zero-shot prompting, measure performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the GenTKGQA framework perform on multi-hop complex temporal questions over TKG, and what modifications would be needed to handle such cases?
- Basis in paper: Inferred from the limitations section, which states that the current framework is designed for single-hop facts and mentions exploring more datasets for multi-hop complex temporal problems.
- Why unresolved: The current datasets and experiments focus on single-hop facts, and the framework's ability to handle multi-hop reasoning is not tested or validated.
- What evidence would resolve it: Testing the framework on a dataset with multi-hop complex temporal questions and evaluating its performance compared to existing methods.

### Open Question 2
- Question: How does the design of different templates for in-context learning affect the consistency of results with manual outputs, and what are the potential biases introduced by template variations?
- Basis in paper: Inferred from the limitations section, which mentions that different templates may result in incomplete consistency with manual results.
- Why unresolved: The paper does not provide a detailed analysis of how template variations impact the results or the consistency with manual outputs.
- What evidence would resolve it: Conducting experiments with different templates and comparing the results to manual outputs to identify biases and inconsistencies.

### Open Question 3
- Question: What are the computational costs and scalability challenges of the GenTKGQA framework, especially when dealing with large-scale TKG datasets?
- Basis in paper: Inferred from the implementation details, which mention the use of NVIDIA A100 GPUs and the fine-tuning of Llama 2-7B, suggesting potential scalability issues.
- Why unresolved: The paper does not provide a detailed analysis of the computational costs or scalability challenges of the framework.
- What evidence would resolve it: Analyzing the computational costs and scalability of the framework on larger TKG datasets and comparing it to other methods.

## Limitations
- Heavy reliance on synthetic temporal queries derived from fixed templates may not capture real-world diversity.
- Absence of human evaluation makes it difficult to assess semantic correctness of generated answers.
- Strong dependency on ChatGPT for subgraph retrieval introduces potential reliability and generalization concerns.

## Confidence
- **High Confidence**: The architectural design of using virtual knowledge indicators to bridge GNN and LLM representations is technically sound and well-justified by the empirical results showing significant improvements over baseline methods.
- **Medium Confidence**: The claim that the two-stage approach outperforms existing methods is supported by experimental results, but the reliance on synthetic query generation and absence of human evaluation introduces uncertainty about real-world applicability.
- **Medium Confidence**: The assertion that LLMs can effectively mine temporal constraints without training is plausible given the few-shot learning setup, but the specific effectiveness may vary depending on the complexity and diversity of temporal queries.

## Next Checks
1. **Human Evaluation**: Conduct human assessment of answer quality on a subset of questions to verify that high Hits@1 scores correspond to semantically correct answers, not just surface matches.
2. **Robustness Testing**: Evaluate the framework's performance when using alternative LLMs or when ChatGPT's capabilities change, to assess dependency risks and generalization.
3. **Real-world Query Testing**: Apply the framework to questions generated from real user query logs or diverse temporal scenarios beyond the fixed template-based generation to test practical applicability.