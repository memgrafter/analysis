---
ver: rpa2
title: Contrastive Learning to Fine-Tune Feature Extraction Models for the Visual
  Cortex
arxiv_id: '2410.06067'
source_url: https://arxiv.org/abs/2410.06067
tags:
- rois
- visual
- alexnet
- encoding
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for encoding visual stimuli
  in the human brain using contrastive learning (CL) to fine-tune a convolutional
  neural network (CNN) for ROI-specific feature extraction. The authors adapt CL to
  maximize the similarity between image features and fMRI responses while contrasting
  with responses to other images.
---

# Contrastive Learning to Fine-Tune Feature Extraction Models for the Visual Cortex

## Quick Facts
- arXiv ID: 2410.06067
- Source URL: https://arxiv.org/abs/2410.06067
- Authors: Alex Mulrooney; Austin J. Brockmeier
- Reference count: 20
- Primary result: CL-tuned models show 0.9% average correlation improvement in early visual ROIs and 7.1% improvement on external dataset

## Executive Summary
This paper introduces a novel approach for encoding visual stimuli in the human brain using contrastive learning (CL) to fine-tune a convolutional neural network (CNN) for ROI-specific feature extraction. The authors adapt CL to maximize the similarity between image features and fMRI responses while contrasting with responses to other images. They fine-tune AlexNet, pretrained on ImageNet, for each subject and ROI, then use linear models to predict voxel responses. Results show that CL-tuned models significantly improve encoding accuracy in early visual ROIs (V1-V4) compared to the untuned baseline and regression-tuned models, with an average correlation improvement of 0.9%. The CL-tuned models also demonstrate cross-subject transferability and generalize well to a lower-resolution external dataset (NOD), improving encoding accuracy by 7.1%. Furthermore, the models preserve most classification performance on ImageNet, Caltech256, and Places365 datasets, with minimal accuracy drops. Model landscapes and salience maps reveal meaningful organization of ROIs and lateralization effects in early visual areas.

## Method Summary
The method involves adapting contrastive learning to fine-tune a pretrained AlexNet model for brain encoding tasks. The authors first extract image features from AlexNet layers, then use these features to predict voxel responses in specific brain regions of interest (ROIs) using linear regression. The contrastive learning objective is designed to maximize the similarity between the model's predicted features and actual fMRI responses while contrasting with responses to different images. This process is applied individually for each subject and ROI, creating personalized encoding models. The fine-tuned models are then evaluated on their ability to predict brain responses to new visual stimuli, compared against baseline untuned models and regression-tuned alternatives. The approach is tested on both the original fMRI dataset and a lower-resolution external dataset (NOD) to assess generalization capabilities.

## Key Results
- CL-tuned models achieve 0.9% average correlation improvement in early visual ROIs (V1-V4) compared to baseline
- Models demonstrate 7.1% improvement in encoding accuracy on external lower-resolution dataset (NOD)
- Classification performance preserved on ImageNet, Caltech256, and Places365 with minimal accuracy drops

## Why This Works (Mechanism)
The contrastive learning approach works by learning to align the CNN's feature representations with the brain's actual neural responses to visual stimuli. By maximizing similarity between predicted features and fMRI responses while contrasting with responses to different images, the model learns a more brain-aligned feature space. This alignment is particularly effective in early visual areas (V1-V4) where the retinotopic organization and basic visual processing are more directly mappable to CNN feature hierarchies. The method's success in cross-subject transferability suggests that the learned feature representations capture fundamental visual processing patterns that are consistent across individuals, despite individual anatomical differences.

## Foundational Learning
- fMRI preprocessing and analysis: Essential for understanding how brain activity data is collected and processed; quick check involves understanding BOLD signal processing
- Convolutional neural networks for vision: Core to understanding the feature extraction mechanism; quick check involves understanding hierarchical feature representations
- Contrastive learning fundamentals: Critical for grasping the fine-tuning approach; quick check involves understanding similarity maximization and negative sampling
- Brain encoding models: Necessary background for understanding the prediction task; quick check involves understanding voxel-wise linear regression
- Visual cortex organization: Important for interpreting ROI-specific results; quick check involves understanding hierarchical visual processing
- Transfer learning and fine-tuning: Relevant for understanding how pretrained models are adapted; quick check involves understanding layer-wise adaptation strategies

## Architecture Onboarding

Component Map:
fMRI data acquisition -> Feature extraction (AlexNet) -> Contrastive fine-tuning -> Linear encoding model -> Voxel response prediction

Critical Path:
The critical path flows from fMRI data through the fine-tuned CNN features to the linear encoding model. The contrastive fine-tuning step is the innovation that differentiates this approach from traditional encoding models.

Design Tradeoffs:
- Choice of AlexNet vs newer architectures: Balances computational efficiency with representational power
- Contrastive vs regression tuning: Contrastive provides better brain alignment but requires more complex training
- Subject-specific vs general models: Subject-specific provides better accuracy but loses generalizability
- ROI-specific tuning: Increases model specificity but requires more computational resources

Failure Signatures:
- Poor encoding accuracy indicating misalignment between model features and brain responses
- Overfitting to subject-specific data reducing cross-subject transferability
- Loss of classification performance indicating degradation of general visual features

First Experiments:
1. Verify baseline encoding accuracy with untuned AlexNet on a subset of data
2. Test contrastive fine-tuning on a single ROI and subject before scaling
3. Compare encoding accuracy on original vs external dataset to validate generalization

## Open Questions the Paper Calls Out
None provided in the input.

## Limitations
- Results primarily validated on early visual ROIs (V1-V4), limiting generalizability to higher-order areas
- fMRI dataset characteristics and preprocessing pipeline not fully detailed, affecting reproducibility
- Comparison lacks exploration of intermediate tuning approaches between pure CL and regression methods

## Confidence
- Encoding accuracy improvements (0.9%): Medium confidence - statistically significant but small effect size
- Cross-subject transferability (7.1%): Medium confidence - shows promise but needs broader validation
- Classification performance preservation: High confidence - comprehensive evaluation across multiple datasets

## Next Checks
1. Replicate encoding accuracy improvements on a different fMRI dataset with independent visual stimuli
2. Test method's effectiveness on higher-order visual areas beyond V1-V4
3. Compare CL-tuned models against alternative fine-tuning strategies including mixed approaches and different CNN architectures