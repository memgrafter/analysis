---
ver: rpa2
title: Optimizing Language Models for Human Preferences is a Causal Inference Problem
arxiv_id: '2402.14979'
source_url: https://arxiv.org/abs/2402.14979
tags:
- outcome
- language
- texts
- text
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper treats language model fine-tuning for human preferences
  as a causal inference problem, where the goal is to optimize text generation to
  cause desired outcomes rather than merely correlate with them. The authors propose
  Causal Preference Optimization (CPO) that uses importance weighting on randomized
  crowdsourced datasets to learn an unbiased surrogate objective, and extend it with
  Doubly Robust CPO (DR-CPO) that reduces variance by incorporating outcome modeling
  while maintaining strong bias guarantees.
---

# Optimizing Language Models for Human Preferences is a Causal Inference Problem

## Quick Facts
- arXiv ID: 2402.14979
- Source URL: https://arxiv.org/abs/2402.14979
- Authors: Victoria Lin; Eli Ben-Michael; Louis-Philippe Morency
- Reference count: 36
- Primary result: DR-CPO achieves win rates >50% against offline RLHF and fine-tuning baselines for optimizing Llama 2 to reduce hate speech and increase persuasion

## Executive Summary
This paper reframes language model fine-tuning for human preferences as a causal inference problem, arguing that traditional methods optimize for correlations rather than causation. The authors propose Causal Preference Optimization (CPO) that uses importance weighting on randomized crowdsourced datasets to learn an unbiased objective, and extend it with Doubly Robust CPO (DR-CPO) that reduces variance through outcome modeling while maintaining strong bias guarantees. Empirical results demonstrate DR-CPO outperforms baselines on optimizing Llama 2 for less hateful text and more persuasive Hong Kong democracy texts, with win rates exceeding 50% against both offline RLHF and simple fine-tuning.

## Method Summary
The method treats language model optimization as a causal inference problem where the goal is to optimize text generation to cause desired outcomes. CPO uses importance weighting based on randomized crowdsourced datasets to estimate the value function by weighting observed outcomes by the ratio P_f(X)/P_R(X). DR-CPO extends this by adding an outcome modeling term that reduces variance while maintaining unbiasedness guarantees if either the importance weights or outcome model is correct. The approach leverages large non-randomized datasets for outcome modeling while correcting for confounding through importance weighting.

## Key Results
- DR-CPO achieves win rates exceeding 50% against offline RLHF and fine-tuning baselines
- Outperforms baselines on optimizing Llama 2 for less hateful text and more persuasive Hong Kong democracy texts
- Demonstrates robustness under outcome model confounding while OO-RLHF degrades significantly
- Reduces variance through outcome modeling while maintaining strong bias guarantees

## Why This Works (Mechanism)

### Mechanism 1
CPO corrects bias in text generation by importance weighting based on randomization probability. In randomized crowdsourced datasets, the relationship between text and outcome is causal because texts are independent of potential outcomes. CPO uses importance weighting to estimate the value function by weighting observed outcomes by the ratio P_f(X)/P_R(X), effectively simulating outcomes under the model's text distribution.

### Mechanism 2
DR-CPO reduces variance while maintaining bias guarantees by combining importance weighting with outcome modeling. The doubly robust formulation ensures unbiasedness if either the importance weights or the outcome model is correct, allowing the outcome model to reduce variance when importance weights are accurate.

### Mechanism 3
Outcome modeling in DR-CPO can leverage large non-randomized datasets to improve variance reduction. The outcome modeling term can be trained on large datasets where the causal relationship between text and outcome may be confounded, because the importance weighting term corrects for any bias. This allows DR-CPO to benefit from more data than CPO alone.

## Foundational Learning

- **Concept**: Causal inference and potential outcomes framework
  - Why needed here: The paper frames language model optimization as a causal problem where we want to intervene on text distribution to cause desired outcomes, not just correlate with them
  - Quick check question: What is the difference between E[Y|X] and E[Y(X)] in the potential outcomes framework?

- **Concept**: Importance weighting and inverse probability weighting
  - Why needed here: CPO uses importance weighting to estimate the value function by weighting observed outcomes by the ratio of model distribution to randomization distribution
  - Quick check question: How does importance weighting correct for sampling bias in causal inference?

- **Concept**: Doubly robust estimation
  - Why needed here: DR-CPO combines importance weighting and outcome modeling to reduce variance while maintaining bias guarantees
  - Quick check question: What are the two conditions under which a doubly robust estimator remains unbiased?

## Architecture Onboarding

- **Component map**: Base language model -> Importance weighting estimator (CPO) -> Outcome model (for DR-CPO) -> Randomized dataset -> Non-randomized dataset -> Monte Carlo sampling from reference model

- **Critical path**: 
  1. Obtain randomized dataset with known P_R
  2. Implement importance weighting to estimate V(f)
  3. (Optional) Train outcome model on non-randomized data
  4. Combine IPW and outcome modeling for DR-CPO
  5. Optimize language model using the (DR-)CPO objective

- **Design tradeoffs**:
  - CPO vs DR-CPO: DR-CPO can use more data but requires training an outcome model
  - Choice of f_0: Pre-trained vs fine-tuned affects diversity of texts for outcome modeling
  - Sample size of randomized data vs non-randomized data: Balance between bias correction and variance reduction

- **Failure signatures**:
  - High variance in value function estimates
  - Poor win rates against baselines
  - Outcome model predictions uncorrelated with true outcomes
  - Importance weights with extreme values

- **First 3 experiments**:
  1. Implement CPO on a simple binary outcome dataset (e.g., hate speech) and verify it outperforms fine-tuning
  2. Add outcome modeling to create DR-CPO and compare variance reduction
  3. Test robustness by training outcome model on confounded data and verifying DR-CPO maintains performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the doubly robust formulation provide benefits beyond variance reduction, such as improved generalization to out-of-distribution text?
- Basis in paper: The paper mentions that DR-CPO can leverage large amounts of confounded data while maintaining unbiasedness guarantees, suggesting potential benefits beyond variance reduction.
- Why unresolved: The experiments primarily focus on variance reduction and robustness to confounding. The paper does not explicitly test generalization to out-of-distribution text.
- What evidence would resolve it: Experiments comparing the performance of CPO and DR-CPO on out-of-distribution text, measuring both variance and generalization error.

### Open Question 2
- Question: How does the choice of the reference language model f0 affect the performance of DR-CPO and OO-RLHF?
- Basis in paper: The paper discusses the choice of f0, noting that a pre-trained model is used to leverage the diversity of generated texts. However, the impact of this choice on performance is not fully explored.
- Why unresolved: The paper does not conduct experiments varying the choice of f0 or analyzing its impact on performance.
- What evidence would resolve it: Experiments comparing the performance of DR-CPO and OO-RLHF using different reference language models, such as pre-trained models, fine-tuned models, or a combination of both.

### Open Question 3
- Question: Can DR-CPO be extended to handle paired completion data, and would it provide similar benefits as observed in the direct outcome setting?
- Basis in paper: The paper mentions that future work may wish to extend DR-CPO to the paired completion data setting, suggesting that it is a potential direction for research.
- Why unresolved: The paper focuses on the direct outcome setting and does not explore the application of DR-CPO to paired completion data.
- What evidence would resolve it: Experiments applying DR-CPO to paired completion data, comparing its performance to existing methods like DPO and RLHF, and analyzing the impact of the doubly robust formulation in this setting.

## Limitations
- Practical implementation of true randomization at scale for large language models remains unclear
- The empirical results may depend heavily on the specific nature and degree of confounding in the outcome model training data
- All experiments use binary outcome datasets, limiting generalizability to multi-class or continuous outcomes

## Confidence
- **High Confidence**: The core causal inference framing and importance weighting mechanism are mathematically sound given the assumptions
- **Medium Confidence**: The claim that DR-CPO maintains strong performance with confounded outcome models is supported but may depend on specific confounding characteristics
- **Low Confidence**: Practical feasibility of obtaining large-scale randomized datasets and generalizability to non-binary outcome settings remain uncertain

## Next Checks
1. **Scale-up Experiment**: Implement CPO and DR-CPO on larger language models (e.g., Llama 2 13B or 70B) and test whether importance weighting remains stable when model distribution diverges significantly from reference distribution. Measure variance in value function estimates as model size increases.

2. **Confounding Robustness Analysis**: Systematically vary the degree and type of confounding in outcome model training data and measure how this affects DR-CPO performance relative to CPO and baselines. Quantify the trade-off between outcome model quality and importance weighting accuracy.

3. **Multi-class Extension**: Extend DR-CPO framework to handle multi-class outcomes and evaluate whether doubly robust formulation maintains theoretical guarantees. Test whether outcome model can be effectively trained on confounded data for multi-class settings.