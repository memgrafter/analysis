---
ver: rpa2
title: 'SELU: Self-Learning Embodied MLLMs in Unknown Environments'
arxiv_id: '2410.03303'
source_url: https://arxiv.org/abs/2410.03303
tags:
- critic
- mllm
- actor
- environment
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SELU, a self-learning paradigm for multimodal
  large language models (MLLMs) in unknown environments. Unlike prior approaches that
  rely on external feedback (human or environmental), SELU employs an actor-critic
  framework where both components learn solely from the model's own interactions.
---

# SELU: Self-Learning Embodied MLLMs in Unknown Environments

## Quick Facts
- arXiv ID: 2410.03303
- Source URL: https://arxiv.org/abs/2410.03303
- Reference count: 40
- This paper introduces SELU, a self-learning paradigm for multimodal large language models (MLLMs) in unknown environments, achieving approximately 28% critic improvement and 20% actor improvement.

## Executive Summary
This paper presents SELU, a self-learning paradigm for MLLMs in unknown environments that operates without external feedback. The method employs an actor-critic framework where both components learn solely from the model's own interactions. The critic evaluates trajectories using self-asking and hindsight relabeling to provide feedback to improve the actor's decision-making. Tested in AI2-THOR and VirtualHome environments, SELU demonstrates effective self-learning through mutual enhancement between environmental understanding and decision-making capabilities.

## Method Summary
SELU implements a self-learning actor-critic framework for MLLMs in unknown environments. The actor module interacts with the environment to collect trajectories, while the critic module evaluates these trajectories using self-asking (targeted questioning about object states) and hindsight relabeling (repurposing failed trajectories for alternative tasks). The framework builds training datasets iteratively through interaction, evaluation, and dataset building cycles. The method uses LLaVA-Mistral-7B or Qwen-VL models with LoRA fine-tuning at different learning rates for actor and critic components.

## Key Results
- Achieves approximately 28% improvement in critic performance
- Achieves approximately 20% improvement in actor performance
- Successfully grounds MLLMs in unknown environments without external supervision

## Why This Works (Mechanism)

### Mechanism 1
The actor-critic self-learning framework enables mutual improvement between environmental comprehension and decision-making in MLLMs. The critic module evaluates trajectories and provides feedback to improve the actor's decision-making, while the actor's improved performance generates better trajectories for critic optimization. This relies on the assumption that MLLMs have stronger discriminative (evaluation) abilities than generative (decision-making) abilities, allowing the critic to effectively guide the actor.

### Mechanism 2
Self-asking corrects critic errors by focusing on specific object states within trajectories. When the critic initially judges a trajectory as failure, it is prompted to examine the state of task-related objects, potentially correcting its initial assessment. This mechanism assumes that MLLMs can correct their own errors through targeted questioning about specific visual elements.

### Mechanism 3
Hindsight relabeling increases sample efficiency by repurposing failed trajectories for other tasks. Trajectories that fail the original task are relabeled as successful for alternative tasks based on what objects were actually manipulated. This relies on the assumption that failed trajectories often complete subtasks or alternative tasks that can be identified and relabeled.

## Foundational Learning

- **Concept: Reinforcement Learning Actor-Critic Framework**
  - Why needed here: SELU is directly inspired by actor-critic architecture where two components mutually improve each other
  - Quick check question: What are the two main components in actor-critic methods and how do they interact?

- **Concept: Multimodal Large Language Models (MLLMs)**
  - Why needed here: The method specifically targets MLLMs for embodied decision-making in unknown environments
  - Quick check question: How do MLLMs differ from standard LLMs in terms of

- **Concept: Self-Supervised Learning**
  - Why needed here: SELU eliminates the need for external feedback by generating its own training signals
  - Quick check question: What distinguishes self-supervised learning from supervised and unsupervised learning paradigms?

## Architecture Onboarding

**Component Map:**
Actor -> Environment -> Critic -> Dataset Builder -> Actor

**Critical Path:**
Actor interaction → Trajectory collection → Critic evaluation (with self-asking and hindsight relabeling) → Dataset building → Actor fine-tuning → Improved actor interaction

**Design Tradeoffs:**
- Using same model for actor and critic (SELU-One) vs. separate models: SELU-One shows performance degradation due to different learning rate requirements and dataset characteristics
- First-person vs. third-person perspectives: Actor uses first-person for interaction, critic uses third-person for evaluation
- Learning rates: 2e-5 for actor vs. 2e-6 for critic reflect different training dynamics

**Failure Signatures:**
- Actor performance degradation when critic provides inaccurate feedback
- Insufficient dataset size leading to poor convergence
- Performance plateau when self-asking cannot correct critic errors

**First Experiments:**
1. Test basic actor-critic interaction in simple environment without self-asking or hindsight relabeling
2. Evaluate critic performance on trajectory success detection before and after implementing self-asking
3. Measure sample efficiency gains from hindsight relabeling on trajectory reuse

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SELU scale with larger MLLMs, and what are the computational trade-offs of using larger models for self-learning in unknown environments? The paper focuses on small-scale MLLMs due to training costs and computational constraints, but does not investigate how performance might change with larger models.

### Open Question 2
What is the optimal balance between self-asking and hindsight relabeling in the critic module, and how does this balance affect the overall learning efficiency and final performance? The ablation study shows that removing either component degrades performance, but does not explore the relative importance or optimal weighting of these techniques.

### Open Question 3
How does SELU generalize to more complex, long-horizon tasks that require sequential decision-making and multi-step planning, and what modifications would be needed to handle such tasks effectively? The paper focuses on short-horizon tasks (max 10 steps) and acknowledges that more complex tasks are a limitation.

## Limitations
- Lack of empirical validation for the core assumption that MLLMs possess stronger discriminative than generative abilities
- Performance in more complex, longer-horizon tasks remains unclear
- Reliance on specific prompt templates and potential for hallucination in critic's self-feedback mechanism

## Confidence
- **High confidence**: The actor-critic framework architecture and its general applicability to self-learning in embodied environments
- **Medium confidence**: The effectiveness of hindsight relabeling for increasing sample efficiency, supported by related work in reinforcement learning
- **Low confidence**: The specific claim about MLLMs' superior discriminative vs generative abilities, due to lack of empirical evidence

## Next Checks
1. **Empirical Validation of MLLM Capability Gap**: Conduct controlled experiments comparing the same MLLM's performance on evaluation tasks versus generation tasks to verify the assumed capability gap that underpins the SELU approach.

2. **Prompt Template Robustness Analysis**: Systematically test variations in the self-asking and hindsight relabeling prompt templates to assess their sensitivity and identify optimal formulations that minimize hallucination while maximizing correction accuracy.

3. **Long-Horizon Task Performance**: Extend evaluation to multi-step tasks requiring sequential decision-making (e.g., "make coffee" involving multiple object interactions) to assess whether SELU's self-learning capabilities scale to more complex reasoning requirements.