---
ver: rpa2
title: Can I understand what I create? Self-Knowledge Evaluation of Large Language
  Models
arxiv_id: '2406.06140'
source_url: https://arxiv.org/abs/2406.06140
tags:
- arxiv
- self-knowledge
- generate
- paragraph
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-knowledge evaluation framework for
  large language models (LLMs) and large multimodal models (LMMs), inspired by Feynman's
  principle of understanding through creation. The framework evaluates models on their
  ability to comprehend and respond to self-generated questions, making it easy to
  implement and resource-efficient compared to existing benchmarks.
---

# Can I understand what I create? Self-Knowledge Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2406.06140
- Source URL: https://arxiv.org/abs/2406.06140
- Reference count: 40
- One-line primary result: Self-knowledge evaluation framework reveals significant gaps in LLMs/LMMs' ability to comprehend their own generated content

## Executive Summary
This paper introduces a self-knowledge evaluation framework for large language models (LLMs) and large multimodal models (LMMs), inspired by Feynman's principle that understanding requires creation. The framework tests models on their ability to comprehend and respond to self-generated questions through a two-step process: first generating content, then immediately evaluating comprehension of that content. Testing seven LLMs and two LMMs across nine tasks reveals that modern models struggle significantly with self-knowledge tasks, suggesting fundamental limitations in their ability to process and understand their own generated content.

The authors analyze why models fail at self-knowledge tasks, finding that models become more similar to human attention mechanisms when achieving higher self-knowledge scores. Attention score concentration correlates with better performance, indicating that models with more focused attention patterns better understand their own outputs. The framework also demonstrates that expert-based prompts improve self-knowledge ability while chain-of-thought prompting does not consistently help. Fine-tuning on self-generated math tasks can improve performance on related benchmarks like GSM-8k, suggesting potential applications for self-knowledge evaluation in model improvement.

## Method Summary
The self-knowledge evaluation framework uses a two-step process: first generating content using a question-generating prompt, then immediately testing comprehension using a corresponding question-verifying prompt. The framework tests seven LLMs (GPT-3.5, GPT-4, Llama3-8B-Instruct, Llama2-7B-Chat, Mistral-7B-Instruct-v0.2, Gemma-1.1-7B-Instruct, Qwen1.5-7B-Chat) and two LMMs (Gill, SEED-LLaMa) across nine tasks including word counting, math problems, theorem proving, and multimodal perception. Self-knowledge scores are calculated as the accuracy of the model's response to self-generated questions. The evaluation includes context-aware assessment and noise injection experiments to test robustness. Attention scores are extracted during generation to analyze correlation with self-knowledge performance.

## Key Results
- Modern LLMs and LMMs show poor self-knowledge performance across all tested tasks
- GPT-4 and Gemma achieve perfect accuracy in context-aware evaluation, with performance decreasing when exposed to noise
- Models with higher self-knowledge scores exhibit attention mechanisms more similar to human attention patterns
- Expert-based prompts improve self-knowledge ability while chain-of-thought prompting does not consistently help
- Fine-tuning on self-generated math tasks improves GSM-8k performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-knowledge evaluation works by testing whether models can consistently answer questions about content they themselves generated.
- Mechanism: The framework generates content using a question-generating prompt, then immediately tests comprehension using a corresponding question-verifying prompt. Consistency between the original answer (known to the evaluator) and the model's self-generated answer indicates self-knowledge.
- Core assumption: Models can accurately process and understand their own generated content when the generation and verification happen in separate runs.
- Evidence anchors:
  - [abstract] "evaluating models on their ability to comprehend and respond to self-generated questions"
  - [section 3] "First generate, then evaluate" describes the two-step process
  - [corpus] Weak evidence - corpus contains related work on self-knowledge but no direct mechanism validation
- Break condition: Performance degrades significantly when noise is introduced between generation and verification, suggesting the model cannot maintain comprehension without contextual continuity.

### Mechanism 2
- Claim: Attention mechanisms in models correlate with self-knowledge performance, similar to human attention patterns.
- Mechanism: When models perform better on self-knowledge tasks, their attention scores concentrate more on relevant tokens (like humans focusing on keywords), indicating a more human-like attention mechanism.
- Core assumption: Attention score distribution can serve as a proxy for understanding and focus during text generation.
- Evidence anchors:
  - [section 6.1] Analysis of designated word counting task shows models with higher self-knowledge scores have more concentrated attention
  - [section 6.1] "models become much similar to the human-inspired attention-based mechanisms when the model gets a higher self-knowledge score"
  - [corpus] No direct evidence in corpus for this specific mechanism
- Break condition: If attention scores remain diffuse even when self-knowledge performance is high, the correlation would break.

### Mechanism 3
- Claim: Fine-tuning on self-generated content can improve model performance on related tasks.
- Mechanism: Models learn patterns and reasoning from their own generated examples, leading to improved performance on similar tasks like GSM-8k math problems.
- Core assumption: Self-generated content contains valuable patterns that can enhance model capabilities when used for fine-tuning.
- Evidence anchors:
  - [section 6.3] "fine-tuning the data generated by the self-knowledge math task may improve the performance on GSM-8k"
  - [section 6.3] Results show GSM-8k accuracy improvements after fine-tuning on self-generated math data
  - [corpus] No direct evidence in corpus for this specific mechanism
- Break condition: If fine-tuning on self-generated content leads to performance degradation or overfitting, the mechanism would break.

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how attention scores correlate with self-knowledge performance
  - Quick check question: Can you explain how attention heads work in transformer models and why their distribution might matter for self-knowledge tasks?

- Concept: Prompt engineering and instruction following
  - Why needed here: The framework relies heavily on carefully crafted prompts for both generation and verification
  - Quick check question: How would you design a question-generating prompt versus a question-verifying prompt for the same task?

- Concept: Knowledge distillation and fine-tuning strategies
  - Why needed here: The paper discusses improving models through fine-tuning on self-generated data
  - Quick check question: What are the key differences between supervised fine-tuning and knowledge distillation?

## Architecture Onboarding

- Component map:
  - Prompt generator -> Model interface -> Attention analyzer -> Evaluation engine -> Fine-tuning pipeline

- Critical path:
  1. Generate content with question-generating prompt
  2. Extract attention scores during generation
  3. Verify comprehension with question-verifying prompt
  4. Calculate self-knowledge score
  5. (Optional) Fine-tune on successful examples

- Design tradeoffs:
  - Using in-context evaluation vs. separate runs affects performance and resource usage
  - Expert prompts vs. standard prompts trade simplicity for potential performance gains
  - Including attention analysis adds computational overhead but provides insights

- Failure signatures:
  - Low self-knowledge scores despite high general performance indicate attention mechanism issues
  - Performance improvement with noise suggests stochastic resonance effects
  - Chain-of-thought prompting not consistently helping indicates task-specific prompt requirements

- First 3 experiments:
  1. Run word counting task with and without expert prompts to measure performance difference
  2. Extract attention scores during designated word generation and analyze concentration patterns
  3. Fine-tune a model on self-generated math content and test on GSM-8k benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design more sophisticated self-verifying strategies beyond directly asking the model to respond?
- Basis in paper: [inferred] The paper mentions that "more sophisticated self-verifying strategies like [34] are left for future work."
- Why unresolved: The current framework uses a simple self-evaluation strategy, and the paper suggests that more advanced methods could be explored.
- What evidence would resolve it: Development and testing of more complex self-verification methods that improve the accuracy and reliability of self-knowledge evaluation.

### Open Question 2
- Question: What are the underlying reasons for the improvements in self-knowledge scores when using expert prompts, given that neither generation nor verification accuracy improves?
- Basis in paper: [explicit] The paper states that "none of the real generative accuracy or verifying accuracy is improved when using the expert prompt, showing a deep underlying reason behind the improvements in self-knowledge score."
- Why unresolved: The paper notes that expert prompts improve self-knowledge scores without improving actual performance, suggesting a deeper mechanism at play.
- What evidence would resolve it: Investigation into the cognitive or attention-based mechanisms that explain why expert prompts enhance self-knowledge scores without improving task performance.

### Open Question 3
- Question: Can AI agents autonomously generate questions beyond human-designed ones, and how would this impact self-verification and self-improvement without human supervision?
- Basis in paper: [explicit] The paper discusses using AI agents to generate questions and states that "This shows that agents have the potential to work without human supervision, we leave the detailed investigation in this direction as future work."
- Why unresolved: While the paper demonstrates that AI agents can generate template questions, it does not explore the full potential of autonomous question generation for self-improvement.
- What evidence would resolve it: Development of AI agents capable of generating diverse and complex questions autonomously, leading to improved self-verification and self-improvement in LLMs and LMMs.

## Limitations

- Prompt templates are not fully specified, requiring researchers to make assumptions about their design
- The framework currently relies on relatively simple tasks that may not capture complex self-knowledge scenarios
- Fine-tuning experiments lack sufficient detail about implementation, hyperparameters, and statistical significance

## Confidence

- **High confidence**: The basic two-step self-knowledge evaluation methodology is clearly described and reproducible. The observation that models struggle with self-knowledge tasks is well-supported by the experimental results across multiple models and tasks.
- **Medium confidence**: The correlation between attention mechanisms and self-knowledge performance requires more evidence. While the analysis shows patterns, the causal relationship between attention concentration and understanding is not fully established.
- **Low confidence**: The fine-tuning improvements on GSM-8k tasks lack sufficient detail about implementation, hyperparameters, and statistical significance to be confidently reproduced or generalized.

## Next Checks

1. **Prompt template validation**: Implement and test multiple variations of the question-generating and question-verifying prompts to assess how sensitive self-knowledge scores are to prompt design choices.

2. **Attention correlation analysis**: Conduct a systematic study across more model architectures comparing attention score distributions during self-generated content creation with self-knowledge performance, controlling for model size and task complexity.

3. **Fine-tuning reproducibility**: Replicate the self-knowledge math task fine-tuning experiments with detailed documentation of training procedures, hyperparameters, and statistical analysis of GSM-8k performance improvements.