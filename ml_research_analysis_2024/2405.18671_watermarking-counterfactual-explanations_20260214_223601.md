---
ver: rpa2
title: Watermarking Counterfactual Explanations
arxiv_id: '2405.18671'
source_url: https://arxiv.org/abs/2405.18671
tags:
- explanations
- extraction
- should
- data
- watermarking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CFMark is a model-agnostic watermarking framework designed to detect
  unauthorized model extraction attacks that exploit counterfactual explanations.
  The framework embeds imperceptible watermarks into counterfactual explanations using
  a bi-level optimization approach, balancing detectability and usability.
---

# Watermarking Counterfactual Explanations

## Quick Facts
- **arXiv ID**: 2405.18671
- **Source URL**: https://arxiv.org/abs/2405.18671
- **Reference count**: 40
- **Primary result**: CFMark achieves ~0.89 F1-score in detecting unauthorized model extraction attacks with minimal explanation quality degradation

## Executive Summary
CFMark introduces a model-agnostic watermarking framework that embeds imperceptible watermarks into counterfactual explanations to detect unauthorized model extraction attacks. The framework employs a bi-level optimization approach that balances watermark detectability against usability degradation of the explanations. Detection is performed through pairwise t-tests comparing model predictions on watermarked versus unwatermarked counterfactual explanations. The method demonstrates strong performance across multiple datasets and extraction methods while maintaining high explanation quality.

## Method Summary
CFMark embeds watermarks into counterfactual explanations through a bi-level optimization framework. The outer optimization maximizes detectability by finding perturbations that create distinguishable prediction patterns on extracted models, while the inner optimization simulates the adversary's model extraction process. A regularization term prevents false positives by ensuring the watermark doesn't affect benign models. Data augmentation with proprietary training data improves generalizability. Detection uses pairwise t-tests to identify models trained on watermarked explanations.

## Key Results
- Achieves F1-score of approximately 0.89 in detecting unauthorized model extraction
- Maintains explanation validity with only ~1.3% degradation
- Preserves explanation proximity with ~1.6% degradation
- Demonstrates effectiveness across diverse datasets and extraction methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bi-level optimization jointly maximizes detectability and minimizes usability degradation
- Mechanism: The outer maximization seeks to embed a perturbation θ that increases the difference in predictions between watermarked and unwatermarked CF explanations on the extracted model, while the inner minimization trains the extracted model to fit both original and watermarked data
- Core assumption: The adversary uses watermarked CF explanations in training the extracted model, enabling detection
- Evidence anchors:
  - [abstract] "Our novel framework solves a bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks that rely on these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme."
  - [section] "We formalize our detectability objective as follows: (i) the probability output of fw* on the watermarked CF explanation fw*(xcf + θ) should be higher than the probability output of fw*(xcf)."

### Mechanism 2
- Claim: Regularization prevents overfitting to a specific extracted model, reducing false positives
- Mechanism: A regularization term penalizes the difference in predictions between watermarked and unwatermarked CF explanations on a benign model not trained on watermarked data
- Core assumption: The regularization strength λ is properly tuned to balance detectability and false positive rate
- Evidence anchors:
  - [section] "This regularization term is similar to our poison loss objective - it aims to minimize the difference in probability output between watermarked and un-watermarked CF explanations by a benign ML model fw*2 that has not been trained using watermarked CF explanations."
  - [section] "At a high level, this regularization term ensures that our learned θ-perturbation does not result in our watermarking detection system falsely flagging benign ML models that have not been trained on watermarked CF explanations."

### Mechanism 3
- Claim: Data augmentation improves generalizability by training against diverse extracted models
- Mechanism: Including sampled training data from the defender's proprietary dataset Dt in the inner optimization problem creates an ensemble of extracted models
- Core assumption: The defender has access to their proprietary training data for augmentation
- Evidence anchors:
  - [section] "Furthermore, to ensure that the learned θ-perturbation does not overfit to our extracted ML model...we enrich the training data used to train the extracted ML model by including sampled data points from the defender's proprietary training dataset Dt."
  - [section] "In addition, we experiment with using an ensemble of extraction models inside the bi-level formulation so that the generated watermarks can be optimized against a diverse ensemble of extracted models."

## Foundational Learning

- Concept: Bi-level optimization
  - Why needed here: CFMark needs to optimize both the watermark perturbation and the adversary's extraction process simultaneously
  - Quick check question: What is the difference between the outer and inner problems in CFMark's bi-level formulation?

- Concept: Null hypothesis significance testing (NHST)
  - Why needed here: CFMark uses pairwise t-tests to determine if a suspicious model was trained on watermarked CF explanations
  - Quick check question: What is the null hypothesis in CFMark's ownership verification framework?

- Concept: Counterfactual explanations and their utility metrics
  - Why needed here: CFMark must embed watermarks without significantly degrading validity and proximity metrics
  - Quick check question: How does CFMark measure the quality degradation of watermarked CF explanations?

## Architecture Onboarding

- Component map: Counterfactual Generator -> Bi-level Optimization Solver -> Watermark Embedding -> Detection System -> Pairwise t-test

- Critical path:
  1. Generate CF explanations for input data
  2. Solve bi-level optimization to find optimal watermark perturbation θ
  3. Embed watermark by adding θ to CF explanations
  4. Distribute watermarked CF explanations
  5. When suspicious model detected, query model for predictions on watermarked vs unwatermarked CF explanations
  6. Perform pairwise t-test to verify unauthorized extraction

- Design tradeoffs:
  - Detectability vs usability: Higher watermark strength improves detection but degrades explanation quality
  - Regularization strength λ: Balances false positive rate against true positive detection
  - Data augmentation: Improves generalizability but requires access to proprietary training data

- Failure signatures:
  - High false positive rate: Likely insufficient regularization or overfitting to specific extraction methods
  - Low true positive rate: Watermark too subtle or adversary using only original CF explanations
  - Significant validity/proximity degradation: Watermark perturbation magnitude too large

- First 3 experiments:
  1. Verify bi-level optimization converges and produces meaningful watermark perturbations
  2. Test detectability on extracted models trained with watermarked CF explanations
  3. Evaluate usability degradation (validity and proximity metrics) with different watermark strengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the watermarking framework be extended to handle non-tabular data modalities like images, text, or audio?
- Basis in paper: [inferred] from the discussion section stating the focus on tabular datasets and suggesting future research should explore other data modalities
- Why unresolved: The paper explicitly limits its scope to tabular datasets, leaving the applicability to other modalities as an open research question
- What evidence would resolve it: Experimental results demonstrating successful extension of CFMark to image, text, or audio datasets with maintained watermark detectability and explanation quality

### Open Question 2
- Question: What is the impact of CFMark on the interpretability of counterfactual explanations from a human-centered perspective?
- Basis in paper: [inferred] from the discussion section mentioning the lack of user study evaluation for interpretability impact
- Why unresolved: The paper only evaluates quantitative metrics and acknowledges the need for human-centered evaluation to understand interpretability impact
- What evidence would resolve it: User study results comparing perceived interpretability of watermarked vs. unwatermarked counterfactual explanations, including metrics like user trust and understanding

### Open Question 3
- Question: How does CFMark perform against adaptive attackers who attempt to detect and remove watermarks?
- Basis in paper: [inferred] from the discussion of model extraction attacks without considering adaptive adversaries who might develop watermark detection/removal techniques
- Why unresolved: The paper assumes attackers use standard extraction methods but doesn't explore scenarios where attackers specifically target watermark removal
- What evidence would resolve it: Experiments showing CFMark's robustness against attackers who attempt watermark detection and removal techniques, including metrics for false negatives in watermark detection

## Limitations

- Framework assumes adversaries will use watermarked counterfactual explanations, which may not hold in practice
- Effectiveness depends heavily on the adversary's extraction methodology and whether they can identify and avoid watermarked explanations
- Regularization mechanism's effectiveness across diverse model architectures and extraction techniques remains uncertain

## Confidence

- **High Confidence**: The bi-level optimization framework is mathematically sound and the pairwise t-test methodology for detection is well-established
- **Medium Confidence**: The claim that regularization prevents false positives, as effectiveness depends heavily on proper λ tuning and may vary across datasets
- **Medium Confidence**: The generalizability claim through data augmentation, since results depend on access to proprietary training data and may not extend to all domains

## Next Checks

1. Test CFMark's effectiveness when adversaries use mixed training data (some watermarked, some original CF explanations) to determine detection robustness under realistic attack scenarios
2. Evaluate false positive rates on benign models with similar architectures but different training data to validate regularization effectiveness
3. Assess performance when the defender lacks access to proprietary training data for augmentation, testing the framework's generalizability limitations