---
ver: rpa2
title: 'Large Language Models for Medicine: A Survey'
arxiv_id: '2405.13055'
source_url: https://arxiv.org/abs/2405.13055
tags:
- medical
- llms
- language
- data
- medicine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of large language models
  (LLMs) in medicine, covering their development, applications, benefits, challenges,
  and future directions. The authors review the evolution of LLMs from generative
  models to pre-training and autoregressive models, highlighting their characteristics
  compared to traditional NLP technologies.
---

# Large Language Models for Medicine: A Survey

## Quick Facts
- arXiv ID: 2405.13055
- Source URL: https://arxiv.org/abs/2405.13055
- Authors: Yanxin Zheng; Wensheng Gan; Zefeng Chen; Zhenlian Qi; Qian Liang; Philip S. Yu
- Reference count: 40
- One-line primary result: Comprehensive survey of LLMs in medicine covering development, applications, benefits, challenges, and future directions

## Executive Summary
This survey provides a comprehensive overview of large language models (LLMs) in medicine, examining their evolution from early generative models to modern autoregressive architectures. The paper analyzes how LLMs are uniquely positioned to address medical challenges through knowledge integration, personalized treatment, and enhanced diagnostic capabilities. It categorizes medical LLM applications across treatment and diagnosis, drug design, medical imaging, doctor-patient communication, and multimodal approaches, while addressing critical challenges including computational demands, data privacy, and ethical considerations.

## Method Summary
The paper conducts a comprehensive literature review examining the development of LLMs in medicine from historical perspectives through current applications. The authors synthesize findings from existing research to categorize medical LLM applications, analyze their characteristics and requirements, and identify challenges and future directions. The survey methodology involves collecting and analyzing published works on LLM development, applications, and deployment in healthcare settings, followed by systematic organization of findings into thematic categories.

## Key Results
- Medical LLMs can integrate diverse data sources (literature, guidelines, case reports) to provide comprehensive information through pre-training and fine-tuning on biomedical corpora
- Key applications include supplementary treatment and diagnosis, drug design, medical image segmentation, and doctor-patient communication with specialized fine-tuning approaches
- Major challenges include computational resource demands, data-related issues, practical implementation barriers, and ethical concerns requiring careful consideration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medical LLMs integrate diverse data sources (literature, guidelines, case reports) to provide comprehensive and multifaceted information.
- Mechanism: By pre-training on large biomedical corpora and fine-tuning on domain-specific datasets, the models learn to extract and synthesize knowledge across multiple data modalities.
- Core assumption: Sufficient quality and quantity of labeled medical data exists for fine-tuning.
- Evidence anchors:
  - [abstract] "They can integrate diverse medical data sources, including medical literature, clinical guidelines, and case reports, offering comprehensive and multifaceted information."
  - [section] "Common pre-training models include Word2Vec [57], Embeddings from Language Models (ELMo) [58], Generative Pre-trained Transformer (GPT) [59], Bidirectional Encoder Representations from Transformers (BERT) [60],andothers."
- Break Condition: If labeled data is insufficient or highly imbalanced, model generalization will degrade.

### Mechanism 2
- Claim: Autoregressive LLMs generate coherent text by sequentially predicting the next word based on context.
- Mechanism: The model uses attention mechanisms to weigh the importance of previous tokens and produce contextually relevant predictions.
- Core assumption: Attention weights accurately capture long-range dependencies in text.
- Evidence anchors:
  - [section] "The autoregressive model is trained through supervised learning and requires a large amount of labeled data."
  - [corpus] "A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine" indicates similar approaches.
- Break Condition: If sequence length exceeds model capacity, attention becomes ineffective and predictions degrade.

### Mechanism 3
- Claim: Fine-tuning on medical dialogues improves model performance in doctor-patient communication.
- Mechanism: Supervised fine-tuning on real-world dialogue data aligns model outputs with clinical communication norms and improves relevance.
- Core assumption: Dialogue datasets are representative of actual clinical interactions.
- Evidence anchors:
  - [section] "To ensure patient confidentiality, this dataset undergoes rigorous privacy protection and personal information cleaning."
  - [corpus] "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?" suggests similar fine-tuning approaches.
- Break Condition: If training data lacks diversity in patient conditions or communication styles, model responses may be biased.

## Foundational Learning

- Concept: Attention Mechanisms
  - Why needed here: Enable the model to focus on relevant parts of input sequences when making predictions.
  - Quick check question: What is the difference between self-attention and cross-attention in transformer models?

- Concept: Transfer Learning
  - Why needed here: Allows models pre-trained on general language tasks to be adapted for specialized medical domains.
  - Quick check question: How does fine-tuning differ from full retraining in transfer learning?

- Concept: Data Preprocessing
  - Why needed here: Medical text requires normalization, tokenization, and handling of domain-specific terminology.
  - Quick check question: What are common challenges in preprocessing medical text data?

## Architecture Onboarding

- Component map: Data ingestion → Preprocessing → Pre-training → Fine-tuning → Evaluation → Deployment
- Critical path: Data ingestion → Preprocessing → Pre-training → Fine-tuning → Evaluation → Deployment
- Design tradeoffs: Model size vs. inference speed, general vs. domain-specific knowledge, data privacy vs. model performance
- Failure signatures: Poor generalization on rare conditions, biased predictions, slow inference times
- First 3 experiments:
  1. Evaluate model performance on medical question-answering tasks using benchmark datasets
  2. Test model interpretability by analyzing attention weights on clinical text
  3. Assess data privacy compliance by checking for potential leakage of sensitive information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can medical LLMs be designed to maintain both high computational efficiency and real-time processing capabilities in resource-constrained clinical environments?
- Basis in paper: [explicit] The paper discusses computational resource demands as a fundamental challenge for medical LLMs, noting that models like GPT-3, GPT-4, or custom medical models require powerful computing hardware (GPUs/TPUs) and incur high costs for acquisition, maintenance, and operation.
- Why unresolved: The paper identifies the problem but doesn't provide specific architectural or algorithmic solutions that balance computational efficiency with performance requirements in clinical settings.
- What evidence would resolve it: Comparative studies showing inference times, resource usage, and clinical decision-making accuracy across different LLM architectures optimized for medical applications under various hardware constraints.

### Open Question 2
- Question: What specific technical approaches can ensure interpretability of medical LLMs while protecting patient privacy and maintaining model performance?
- Basis in paper: [explicit] The paper identifies interpretability and trustworthiness as critical challenges, while also noting data privacy concerns in healthcare. It mentions differential privacy as a potential technique but doesn't elaborate on how to balance these competing requirements.
- Why unresolved: There's a fundamental tension between making models interpretable (which may require access to sensitive patient data) and protecting privacy, with no clear technical pathway forward.
- What evidence would resolve it: Empirical demonstrations of interpretable medical LLM architectures that successfully integrate privacy-preserving techniques while maintaining diagnostic accuracy and explainability comparable to non-privacy-preserving versions.

### Open Question 3
- Question: How can medical LLMs be effectively adapted to handle multilingual and multicultural healthcare contexts without compromising accuracy across different regions and populations?
- Basis in paper: [explicit] The paper identifies multilingual and multicultural adaptation as a practical challenge, noting that healthcare practices, beliefs, and medical standards vary across regions, requiring precise model adaptations for different cultural contexts.
- Why unresolved: The paper acknowledges the complexity but doesn't provide concrete strategies for achieving effective adaptation across diverse linguistic and cultural healthcare contexts.
- What evidence would resolve it: Cross-cultural validation studies showing consistent diagnostic accuracy and culturally appropriate recommendations across multiple languages and healthcare systems, along with specific adaptation methodologies that maintain performance across different cultural contexts.

## Limitations

- The survey does not provide quantitative performance benchmarks for specific medical LLM applications, making it difficult to assess real-world effectiveness
- Limited discussion of failure cases and model limitations in clinical settings, with most claims based on theoretical potential rather than empirical evidence
- Data privacy and ethical considerations are mentioned but not deeply explored with concrete implementation guidelines

## Confidence

- **High confidence**: The historical development of LLMs and their general characteristics are well-established and accurately represented
- **Medium confidence**: Claims about specific medical applications and benefits are supported by cited literature but lack systematic validation across different clinical contexts
- **Low confidence**: Predictions about future directions (e.g., integration with Metaverse, blockchain) are speculative and not grounded in current technological feasibility

## Next Checks

1. Examine published clinical trials or real-world deployment studies of medical LLMs to verify claimed benefits in practice
2. Investigate documented cases of LLM failures or errors in medical contexts to understand failure modes and limitations
3. Review regulatory guidelines and standards for AI/ML in healthcare to assess the feasibility of proposed future directions regarding privacy and security