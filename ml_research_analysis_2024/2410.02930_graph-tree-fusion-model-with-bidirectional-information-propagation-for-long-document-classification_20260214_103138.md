---
ver: rpa2
title: Graph-tree Fusion Model with Bidirectional Information Propagation for Long
  Document Classification
arxiv_id: '2410.02930'
source_url: https://arxiv.org/abs/2410.02930
tags:
- document
- long
- information
- classification
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel Graph-Tree Fusion Model to tackle
  the challenges of long document classification, particularly addressing issues related
  to token limits and hierarchical relationships. The approach combines syntax trees
  for sentence encodings and document graphs for document encodings, utilizing Tree
  Transformers and Graph Attention Networks (GATs) to capture both local and global
  dependencies.
---

# Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification

## Quick Facts
- arXiv ID: 2410.02930
- Source URL: https://arxiv.org/abs/2410.02930
- Authors: Sudipta Singha Roy; Xindi Wang; Robert E. Mercer; Frank Rudzicz
- Reference count: 16
- Introduces novel Graph-Tree Fusion Model for long document classification

## Executive Summary
This paper addresses the challenge of long document classification by introducing a novel Graph-Tree Fusion Model that combines syntax trees for sentence encodings and document graphs for document encodings. The model employs Tree Transformers and Graph Attention Networks (GATs) to capture both local and global dependencies while utilizing a bidirectional information propagation mechanism to enhance contextual representation. The approach effectively handles arbitrarily long contexts without token limit constraints, demonstrating significant improvements across various long document classification tasks including binary, multi-class, and multi-label classification.

## Method Summary
The proposed Graph-Tree Fusion Model leverages the complementary strengths of syntax trees and document graphs to represent long documents. Syntax trees capture local dependencies within sentences, while document graphs model global relationships between sentences. Tree Transformers process the syntactic structures, and GATs handle the document-level graph representations. A bidirectional information propagation mechanism enables information flow from word-to-sentence-to-document and vice versa, enriching the contextual representation at each level. The model architecture effectively overcomes traditional token limit constraints by processing documents hierarchically rather than as flat sequences.

## Key Results
- Significant performance improvements across multiple long document classification tasks
- Outperforms existing methods in both accuracy and macro-F1 scores
- Effectively handles arbitrarily long contexts without token limit constraints
- Demonstrates success in binary, multi-class, and multi-label classification scenarios

## Why This Works (Mechanism)
The model's effectiveness stems from its ability to capture hierarchical document structure through complementary representations. The syntax tree provides fine-grained local context within sentences, while the document graph captures broader semantic relationships between sentences. The bidirectional propagation mechanism allows information to flow both upward (from words to sentences to document) and downward (from document to sentences to words), creating rich, context-aware representations at each level. This hierarchical approach naturally handles long documents by breaking them into manageable components rather than treating them as flat sequences limited by token constraints.

## Foundational Learning
- **Syntax Trees**: Hierarchical sentence representations needed to capture local grammatical and semantic dependencies
  *Quick check*: Parse sentences into constituency or dependency trees before processing
- **Graph Attention Networks**: Node-level feature learning for document graphs to model relationships between sentences
  *Quick check*: Ensure proper message passing between connected nodes in the graph
- **Tree Transformers**: Adapt transformer architecture to tree-structured data for encoding syntactic information
  *Quick check*: Verify tree traversal order preserves structural information
- **Bidirectional Propagation**: Dual information flow direction to enrich contextual representations at all levels
  *Quick check*: Track information flow from words → sentences → document and back

## Architecture Onboarding

**Component Map**
Document -> Document Graph + Syntax Trees (per sentence) -> Tree Transformers + GATs -> Bidirectional Propagation -> Classification Layer

**Critical Path**
1. Document parsing and segmentation
2. Syntax tree construction for each sentence
3. Document graph construction
4. Tree Transformer encoding of syntax trees
5. GAT encoding of document graph
6. Bidirectional information propagation
7. Classification layer

**Design Tradeoffs**
- Hierarchical processing vs. flat sequence processing: better long document handling but increased complexity
- Bidirectional propagation vs. unidirectional: richer context but higher computational cost
- Tree+Graph fusion vs. single representation: complementary strengths but integration complexity

**Failure Signatures**
- Poor parsing quality leading to incorrect syntax trees
- Inadequate graph connectivity failing to capture document relationships
- Information loss during bidirectional propagation
- Classification layer unable to integrate multi-level representations effectively

**3 First Experiments**
1. Test with varying document lengths to verify token limit handling
2. Compare performance with unidirectional vs. bidirectional propagation
3. Evaluate contribution of syntax trees vs. document graphs through ablation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on classification tasks without exploring other potential applications
- Bidirectional information propagation increases computational complexity and may face scalability challenges
- Relies on pre-trained encoders, potentially limiting adaptability to specialized domains
- Assumes availability of reliable parsing tools which may not perform consistently across all languages or document domains

## Confidence
**High confidence:** The core architectural innovations (graph-tree fusion, bidirectional propagation) are technically sound and the reported performance improvements are substantial across multiple benchmark datasets.

**Medium confidence:** The claim of handling "arbitrarily long contexts without token limit constraints" requires verification, as practical limitations may exist in terms of memory and computation time.

**Medium confidence:** The superiority over existing methods is demonstrated, but the comparison may be influenced by specific implementation choices and hyperparameter tuning.

## Next Checks
1. Conduct scalability testing with documents significantly longer than those in the current evaluation to verify practical token limit handling and measure computational resource requirements.

2. Perform ablation studies to quantify the individual contributions of the graph component, tree component, and bidirectional propagation mechanism to overall performance.

3. Test the model on diverse document domains (e.g., legal, scientific, technical) to assess generalization beyond the current benchmark datasets and evaluate robustness to parsing quality variations.