---
ver: rpa2
title: 'MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question
  Generation'
arxiv_id: '2410.12893'
source_url: https://arxiv.org/abs/2410.12893
tags:
- question
- scores
- human
- evaluation
- feedback-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIRROR is a feedback-based system that uses two large language
  models (LLMs) to iteratively evaluate open-ended questions generated by automated
  systems. The approach involves initial scoring followed by iterative refinement,
  where one LLM provides feedback (strengths and flaws) to another, improving evaluation
  scores over multiple rounds until convergence.
---

# MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation

## Quick Facts
- **arXiv ID**: 2410.12893
- **Source URL**: https://arxiv.org/abs/2410.12893
- **Reference count**: 27
- **Primary result**: MIRROR improves LLM evaluation scores and correlation with human judgments through iterative feedback between two models

## Executive Summary
MIRROR introduces a novel feedback-based approach for evaluating open-ended questions generated by automated systems. The system uses two large language models (LLMs) in an iterative process where one model provides initial evaluations and identifies strengths and flaws, while the other refines the evaluation based on this feedback. This process continues until scores converge. Experiments demonstrate that MIRROR improves both human evaluation metric scores (relevance, appropriateness, novelty, complexity, grammaticality) and Pearson's correlation between LLM evaluations and human expert judgments compared to direct prompting approaches.

## Method Summary
MIRROR operates through an iterative feedback loop between two LLMs. First, LLM1 evaluates a generated question on five human evaluation metrics and identifies strengths and flaws. These scores and feedback are then provided to LLM2, which refines the evaluation. The process iterates, with each model alternately evaluating and providing feedback, until scores converge (become identical for two consecutive iterations). The approach was tested with GPT-4, Gemini, and Llama2-70b on EduProbe and SciQ datasets, comparing feedback-based evaluation against direct prompting methods.

## Key Results
- GPT-4 shows higher correlation scores with human experts when using MIRROR compared to direct evaluation across all metrics
- Evaluation scores on human metrics (relevance, appropriateness, novelty, complexity, grammaticality) improve with MIRROR, tending toward human baseline scores
- GPT-4 performs closest to human baseline in the feedback-based approach, with high scores in grammaticality and appropriateness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The feedback-based iterative refinement between two LLMs improves evaluation scores by allowing one model to benefit from the strengths and weaknesses identified by another.
- Mechanism: LLM1 provides initial evaluation scores and identifies strengths and flaws. LLM2 then uses this feedback to re-evaluate the same question, updating the scores. This process iterates until scores converge, leading to refined and more accurate evaluations.
- Core assumption: LLMs can effectively critique and improve upon each other's evaluations when provided with structured feedback about strengths and flaws.
- Evidence anchors:
  - [abstract] "We observed that the scores of human evaluation metrics, namely relevance, appropriateness, novelty, complexity, and grammaticality, improved when using the feedback-based approach called MIRROR, tending to be closer to the human baseline scores."
  - [section 4.3] "The process then enters an iterative loop designed to refine these initial assessments. In each iteration, the identified strengths and flaws (S1, F1) are provided as feedback to the second LLM, LLM2."
- Break condition: The process breaks when scores from LLM1 and LLM2 become identical for two consecutive iterations, indicating convergence.

### Mechanism 2
- Claim: The iterative feedback process improves the alignment between LLM evaluations and human expert judgments, as measured by Pearson's correlation coefficient.
- Mechanism: By iteratively refining evaluations through feedback, the LLMs' scores become more consistent with how human experts would evaluate the questions, particularly in areas like relevance and appropriateness.
- Core assumption: Human evaluation metrics such as relevance and appropriateness can be reliably approximated by LLMs when given sufficient feedback and context.
- Evidence anchors:
  - [abstract] "Furthermore, we observed that Pearson's correlation coefficient between GPT-4 and human experts improved when using our proposed feedback-based approach, MIRROR, compared to direct prompting for evaluation."
  - [section 5] "For the EduProbe and SciQ datasets, GPT-4 shows higher correlation scores with human experts when using the feedback-based approach compared to the direct approach across all metrics."
- Break condition: The correlation improvement plateaus when further iterations do not significantly change the scores.

### Mechanism 3
- Claim: Different LLMs have varying strengths in evaluating different aspects of question quality, and using multiple models can leverage these differences for better overall evaluation.
- Mechanism: By using different LLMs (GPT-4, Gemini, Llama2-70b) in the feedback process, the system can capitalize on each model's strengths. For example, GPT-4 might be better at evaluating relevance, while another model might excel at assessing complexity.
- Core assumption: Different LLMs have unique capabilities and biases that can complement each other when used in a feedback loop.
- Evidence anchors:
  - [section 5] "GPT-4 performs closest to the human baseline, especially in the feedback-based approach, with high scores in grammaticality and appropriateness. Gemini consistently scores slightly lower than GPT-4 across all metrics and evaluation methods."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.433, average citations=0.0." (Weak corpus evidence for multi-model approaches)
- Break condition: The feedback process breaks when the combined evaluation from multiple models does not significantly improve upon the best individual model's performance.

## Foundational Learning

- Concept: Pearson's correlation coefficient
  - Why needed here: To measure how closely the LLM evaluations align with human expert judgments, providing a quantitative measure of the system's effectiveness.
  - Quick check question: If human experts give scores of [4, 3, 5, 2] for a set of questions, and an LLM gives [4, 3, 5, 1], what would be the Pearson correlation coefficient? (Answer: 0.98 - very high correlation)

- Concept: Iterative refinement
  - Why needed here: The core mechanism of MIRROR relies on repeatedly improving evaluations through feedback, similar to how iterative algorithms converge to solutions.
  - Quick check question: If the initial score is 3 and each iteration improves it by 0.5, how many iterations are needed to reach a score of 4.5? (Answer: 3 iterations)

- Concept: Prompt engineering
  - Why needed here: The quality of LLM evaluations depends heavily on how the evaluation criteria and feedback are presented in the prompts.
  - Quick check question: What is the difference between asking "Rate this question's relevance" versus "How relevant is this question to the given context, on a scale of 1-5?" (Answer: The second prompt provides clearer context and a specific scale)

## Architecture Onboarding

- Component map:
  Context and question input -> LLM1 for initial evaluation and feedback generation -> LLM2 for refined evaluation using feedback -> Feedback loop controller (manages iterations and convergence) -> Score aggregation and output

- Critical path:
  1. Input context and generated question
  2. LLM1 evaluates and generates strengths/flaws
  3. LLM2 refines evaluation using feedback
  4. Compare scores for convergence
  5. Output final scores if converged, else repeat from step 3

- Design tradeoffs:
  - Single LLM vs. dual LLM approach: Using two different LLMs may provide more diverse perspectives but increases computational cost and complexity.
  - Iteration limit vs. convergence-based stopping: Setting a maximum number of iterations ensures termination but may not achieve optimal refinement.

- Failure signatures:
  - Oscillation: Scores fluctuate without converging, suggesting the feedback isn't leading to improvement.
  - Score degradation: Subsequent iterations produce worse scores than initial ones, indicating the feedback process is counterproductive.
  - Stuck at local optimum: Scores converge to a suboptimal value, suggesting the feedback isn't exploring the full evaluation space.

- First 3 experiments:
  1. Run MIRROR on a small set of questions with GPT-4 as both LLM1 and LLM2, comparing scores to human evaluations.
  2. Test different iteration limits (5, 10, 20) to find the optimal balance between refinement and efficiency.
  3. Compare MIRROR's performance using GPT-4 vs. Gemini as LLM1, keeping LLM2 constant, to assess model-specific strengths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MIRROR scale when applied to longer and more complex contexts compared to the EduProbe and SciQ datasets?
- Basis in paper: [explicit] The authors state "So far, we have focused on short and medium-sized contexts, whereas future work will focus on applying MIRROR to longer contexts."
- Why unresolved: The current experiments only cover short to medium-length contexts, and the authors acknowledge this as a limitation for future exploration.
- What evidence would resolve it: Experiments applying MIRROR to datasets with longer, more complex contexts, comparing evaluation accuracy and correlation with human baselines across varying context lengths.

### Open Question 2
- Question: What is the impact of using different LLMs (beyond GPT-4, Gemini, and Llama2-70b) on the effectiveness of the MIRROR feedback loop, and are there specific model characteristics that make some LLMs more suitable for this approach?
- Basis in paper: [inferred] The paper only experiments with three specific LLMs, leaving open the question of how other models would perform in the feedback-based approach.
- What evidence would resolve it: Systematic testing of MIRROR with a diverse range of LLMs, including newer and specialized models, measuring evaluation accuracy and convergence rates to identify optimal model characteristics.

### Open Question 3
- Question: Can the MIRROR approach be extended to handle multi-turn or dialog-based question generation scenarios, where questions build upon previous interactions?
- Basis in paper: [inferred] The current methodology focuses on single-context question evaluation without considering conversational dynamics or question sequences.
- What evidence would resolve it: Development and testing of MIRROR in conversational settings where questions form a coherent dialogue, measuring how well the feedback loop maintains consistency and quality across multiple turns.

## Limitations

- The study focuses only on short to medium-sized contexts, with longer contexts identified as future work
- Limited exploration of different LLM combinations beyond the three tested models (GPT-4, Gemini, Llama2-70b)
- No direct human validation study to confirm that improved correlation translates to better absolute evaluation quality

## Confidence

**High confidence**: The observation that MIRROR improves correlation with human evaluations is well-supported by the presented Pearson correlation coefficients across multiple datasets and metrics. The iterative refinement mechanism itself is clearly described and reproducible.

**Medium confidence**: The claim that MIRROR produces "more accurate" evaluations is supported by correlation improvements but lacks direct validation against ground truth quality assessments beyond human correlations. The convergence behavior is observed but not fully characterized across all question types.

**Low confidence**: The assertion that different LLMs have complementary strengths for different evaluation aspects is based on limited comparisons. The corpus evidence showing weak citation signals for multi-model approaches suggests this area may not be well-established in the literature.

## Next Checks

1. **Convergence analysis**: Track score trajectories across all iterations for 100 random questions to identify patterns of convergence vs. oscillation and characterize the types of questions that resist convergence.

2. **Role reversal experiment**: Run MIRROR with GPT-4 and Gemini swapping roles (which one provides initial feedback vs. refined evaluation) to determine if the asymmetric setup is optimal or arbitrary.

3. **Human validation study**: Have human experts rate a subset of questions evaluated by both direct prompting and MIRROR to directly measure whether the improved correlation translates to better absolute evaluation quality, not just better alignment with other LLM scores.