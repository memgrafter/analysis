---
ver: rpa2
title: Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors
arxiv_id: '2410.13776'
source_url: https://arxiv.org/abs/2410.13776
tags:
- annotator
- prior
- performance
- llms
- annotators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether annotation artifacts introduced
  during data aggregation affect the performance of large language models (LLMs) on
  subjective tasks like emotion and morality classification. The authors hypothesize
  that aggregation of low-agreement, disparate annotations creates noise in the prompt
  that leads LLMs to ignore the input-label mapping.
---

# Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors

## Quick Facts
- arXiv ID: 2410.13776
- Source URL: https://arxiv.org/abs/2410.13776
- Reference count: 19
- Key outcome: Aggregation artifacts in subjective tasks hinder LLM performance by creating inconsistent input-label mappings that models interpret as noise

## Executive Summary
This paper investigates how aggregation artifacts in subjective task datasets affect large language model performance. The authors hypothesize that aggregating low-agreement annotations creates inconsistent patterns that LLMs interpret as noise, causing them to ignore the input-label mapping. Through controlled experiments comparing individual annotator labels versus aggregated labels across emotion and morality classification datasets, they find strong evidence that aggregation artifacts hinder modeling. Individual annotators often outperform the aggregate, and models amplify minority annotator perspectives more than majority ones. While aggregation explains some performance gaps, it doesn't account for all differences between LLMs and traditional algorithms.

## Method Summary
The authors conduct controlled experiments using MFRC and GoEmotions datasets with individual annotator labels and aggregated labels. They employ in-context learning (ICL) and Chain-of-Thought (CoT) prompting with multiple LLM models (Llama-2, Llama-3, GPT-3.5 Turbo, GPT-4o mini). The experimental design compares model performance using individual annotator labels versus aggregated labels, measuring performance with Jaccard Score, Micro F1, and Macro F1. They analyze correlations between annotator similarity to the aggregate/prior and performance improvements, and test whether explicitly encoding annotator biases in reasoning chains improves performance.

## Key Results
- Individual annotators often outperform aggregated labels in ICL settings
- Models amplify minority annotator perspectives more than majority annotators
- Aggregation artifacts explain some but not all performance differences between LLMs and traditional algorithms
- Chain-of-thought prompting can improve performance when reasoning chains explicitly describe consistent annotator biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregation artifacts create inconsistent input-label mappings that cause LLMs to ignore the prompt mapping as noise
- Mechanism: Low-agreement annotations create inconsistent patterns across examples, which LLMs interpret as noise rather than signal
- Core assumption: LLMs are highly sensitive to consistent patterns and will disregard inconsistent mappings
- Evidence anchors: [abstract] aggregation might lead to annotation artifacts that create detrimental noise; [section 4.4.1] aggregate mapping has inconsistencies that inject sufficient noise

### Mechanism 2
- Claim: Majority annotators are more similar to the aggregate, making them less aligned with LLM priors and receiving smaller performance improvements
- Mechanism: Majority annotators better represent the problematic aggregate, receiving less improvement from ICL
- Core assumption: The aggregate's poor performance stems from aggregation artifacts
- Evidence anchors: [section 4.4.1] strongly negative trends show idiosyncratic annotators have better performance and are more amplified; more an annotator resembles majority, less aligned with model priors

### Mechanism 3
- Claim: LLMs can learn from consistent patterns in the prompt when the mapping is interpretable and clear
- Mechanism: Explicit reasoning chains describing consistent annotator biases can be detected and learned by LLMs
- Core assumption: LLMs can detect and learn explicit patterns when made clear in the prompt
- Evidence anchors: [section 4.5] gains in Authority F1 score are consistent and significant across models; indicates ability to learn and revise priors from consistent mappings

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: The paper's entire experimental setup relies on ICL where models learn from demonstrations in the prompt without parameter updates
  - Quick check question: What distinguishes ICL from traditional fine-tuning approaches?

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: The paper compares standard ICL with CoT prompting to see if explicit reasoning improves performance on subjective tasks
  - Quick check question: How does CoT prompting differ from standard ICL in terms of prompt structure?

- Concept: Prior knowledge vs. evidence in LLMs
  - Why needed here: The paper investigates whether LLMs rely more on their pre-trained knowledge (priors) or the evidence provided in the prompt
  - Quick check question: What experimental design would reveal whether a model is using prior knowledge versus prompt evidence?

## Architecture Onboarding

- Component map: Dataset preparation with individual/aggregated labels -> Prompt construction with ICL and CoT formats -> LLM inference with multiple models -> Evaluation using Jaccard Score and F1 scores
- Critical path: Dataset preparation → Prompt construction → Model inference → Result analysis
- Design tradeoffs: Individual annotator labels provide cleaner signal but reduce dataset size; aggregation increases coverage but introduces artifacts; ICL vs CoT involves tradeoff between prompt length and reasoning clarity
- Failure signatures: If performance correlates with annotator similarity to aggregate rather than prior knowledge, indicates aggregation artifacts; if CoT fails to improve performance despite clear reasoning, suggests LLMs cannot process subjective task reasoning effectively
- First 3 experiments:
  1. Replicate prior vs. posterior performance comparison for a single annotator group
  2. Test different aggregation methods (weighted voting, median aggregation) to reduce artifacts
  3. Evaluate whether explicitly encoding known annotator biases in reasoning chains improves performance across multiple annotator groups

## Open Questions the Paper Calls Out

Open Question 1
- Question: How do different aggregation methods (e.g., majority vote, weighted vote, probabilistic aggregation) affect LLM performance on subjective tasks?
- Basis in paper: The paper suggests simple majority-based aggregation introduces inconsistencies that confuse LLMs
- Why unresolved: The paper only examines majority-based aggregation and does not explore other aggregation methods
- What evidence would resolve it: Experiments comparing LLM performance using different aggregation methods on the same datasets

Open Question 2
- Question: Can LLM performance on subjective tasks be improved by incorporating additional information about annotators (e.g., demographics, expertise, past agreement rates) during aggregation?
- Basis in paper: The paper suggests annotator-level modeling is beneficial
- Why unresolved: The paper does not explore the impact of incorporating annotator information during aggregation
- What evidence would resolve it: Experiments comparing LLM performance using aggregation methods that incorporate annotator information with those that do not

Open Question 3
- Question: How does the size and diversity of the annotator pool affect LLM performance on subjective tasks?
- Basis in paper: The paper suggests individual annotators often outperform the aggregate
- Why unresolved: The paper does not explore the impact of annotator pool size and diversity
- What evidence would resolve it: Experiments comparing LLM performance using datasets with different annotator pool sizes and diversities

Open Question 4
- Question: Can LLM performance on subjective tasks be improved by explicitly modeling the reasoning process of individual annotators during aggregation?
- Basis in paper: The paper suggests interpretable and consistent mappings can be modeled by LLMs
- Why unresolved: The paper does not explore the impact of explicitly modeling annotator reasoning during aggregation
- What evidence would resolve it: Experiments comparing LLM performance using aggregation methods that explicitly model annotator reasoning with those that do not

## Limitations
- The paper cannot fully separate aggregation artifact effects from task complexity or inherent annotator quality differences
- Assumes individual annotator labels provide "cleaner" signal, ignoring potential biases or inconsistencies in individual annotations themselves
- Corpus analysis reveals moderate relevance but lacks papers specifically addressing aggregation artifacts, suggesting this is a relatively unexplored phenomenon

## Confidence
**High confidence**: Individual annotator labels generally outperform aggregated labels in ICL settings
**Medium confidence**: Aggregation artifacts create inconsistent input-label mappings that LLMs interpret as noise
**Medium confidence**: Releasing individual annotations would benefit subjective task modeling

## Next Checks
1. Test alternative aggregation methods: Compare performance using weighted voting, median aggregation, or consensus-based methods against simple majority voting
2. Cross-dataset validation: Replicate individual vs. aggregate performance comparison on additional subjective task datasets
3. Controlled annotation study: Conduct human study where annotators provide labels on same examples with and without aggregation instructions, then measure how aggregation processes affect label consistency and LLM performance