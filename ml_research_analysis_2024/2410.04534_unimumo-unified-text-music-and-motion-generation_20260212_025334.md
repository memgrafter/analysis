---
ver: rpa2
title: 'UniMuMo: Unified Text, Music and Motion Generation'
arxiv_id: '2410.04534'
source_url: https://arxiv.org/abs/2410.04534
tags:
- music
- motion
- generation
- text
- music-motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces UniMuMo, the first unified multimodal model
  capable of generating text, music, and motion in any combination. To overcome the
  lack of synchronized multimodal data, the authors align unpaired music and motion
  sequences using rhythmic patterns via beat detection and dynamic time warping.
---

# UniMuMo: Unified Text, Music and Motion Generation

## Quick Facts
- arXiv ID: 2410.04534
- Source URL: https://arxiv.org/abs/2410.04534
- Reference count: 40
- Primary result: First unified multimodal model capable of generating text, music, and motion in any combination

## Executive Summary
UniMuMo introduces the first unified multimodal model capable of generating text, music, and motion in arbitrary combinations. The key innovation addresses the lack of synchronized multimodal data by aligning unpaired music and motion sequences using rhythmic patterns via beat detection and dynamic time warping. The model employs a three-stage training approach: joint tokenization using a shared music-motion codebook, fine-tuning a pre-trained MusicGen decoder with a parallel generation scheme, and training a T5 decoder for captioning. UniMuMo achieves competitive results across all unidirectional generation benchmarks, including text-to-music (CLAP similarity 0.27), music-to-text captioning (Bleu 0.261), motion-to-text captioning (R-Precision 0.520), and music/motion-conditioned dance generation (Distg 9.37, Beat Align. 0.25).

## Method Summary
UniMuMo uses a three-stage training pipeline: (1) Joint tokenization stage that encodes motion into the same feature space as music using a shared codebook derived from a pre-trained audio tokenizer (Encodec), (2) Music-motion decoder stage that employs a parallel generation scheme within a single transformer decoder architecture, allowing simultaneous generation of both modalities with cross-modal conditioning, and (3) T5 decoder stage for captioning tasks using the music-motion decoder as a feature extractor. The model aligns unpaired music and motion data using rhythmic patterns, then synthesizes textual descriptions from metadata and large language models to create training data for all three modalities.

## Key Results
- Text-to-music generation achieves CLAP similarity of 0.27
- Music-to-text captioning achieves Bleu score of 0.261
- Motion-to-text captioning achieves R-Precision of 0.520
- Music/motion-conditioned dance generation achieves Distg of 9.37 and Beat Alignment of 0.25

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint codebook encoding bridges the modality gap by mapping motion into the same feature space as music
- Mechanism: The authors reuse the frozen music codebook from Encodec and train a motion encoder that outputs features in the same embedding space, then quantize these features using the shared codebook
- Core assumption: The music codebook, trained on richer and more complex audio signals, contains sufficient representational capacity to encode motion sequences effectively
- Evidence anchors:
  - [abstract]: "We propose encoding motion with a music codebook, mapping motion into the same feature space as music"
  - [section 4.1]: "Inspired by these facts, we introduce an efficient and effective way to encode music and motion into a joint latent space"
  - [corpus]: Weak evidence - corpus papers focus on unified architectures but don't specifically address shared codebook approaches
- Break condition: If the music codebook lacks sufficient diversity to represent the full range of motion dynamics, or if the motion-to-music feature space mapping introduces significant information loss

### Mechanism 2
- Claim: Music-motion parallel generation scheme enables synchronized bidirectional generation within a single architecture
- Mechanism: The transformer decoder is trained on a joint task where it predicts next tokens for both music and motion simultaneously, using cross-modal causal attention masks that allow each modality to attend to its own history and the other modality's history
- Core assumption: Autoregressive generation can be effectively parallelized across modalities while maintaining temporal coherence and cross-modal conditioning
- Evidence anchors:
  - [abstract]: "We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture"
  - [section 4.2]: "To enable the autoregressive generation of music and motion within a unified framework, we propose training on the task of music-motion joint generation"
  - [corpus]: Moderate evidence - corpus papers discuss unified architectures but lack specific details on parallel autoregressive schemes
- Break condition: If the cross-modal attention masks fail to maintain proper temporal dependencies, or if the joint training objective creates gradient conflicts between modalities

### Mechanism 3
- Claim: Fine-tuning pre-trained single-modality models significantly reduces computational demands while maintaining performance
- Mechanism: The authors initialize the music-motion decoder with pre-trained MusicGen weights and add modality-specific components (separate embedders, MoE layers) that are also initialized from MusicGen, then fine-tune on the joint task
- Core assumption: Pre-trained weights contain useful representations that transfer well to the multimodal setting, and modality-specific initialization prevents interference between music and motion features
- Evidence anchors:
  - [abstract]: "Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands"
  - [section 4.2]: "With the reuse of Encodec and joint encoding of motion in the previous stage, the current stage can be effectively achieved by fine-tuning the pre-trained text-to-music model"
  - [corpus]: Weak evidence - corpus focuses on unified architectures but doesn't specifically address pre-training transfer strategies
- Break condition: If the pre-trained weights are too specialized to music generation and don't transfer well to multimodal generation, or if the added modality-specific components create architectural conflicts

## Foundational Learning

- Concept: Dynamic Time Warping (DTW)
  - Why needed here: Used to align unpaired music and motion sequences based on rhythmic patterns when paired data is unavailable
  - Quick check question: How does DTW find the optimal alignment between two sequences of different lengths, and what distance metric does it minimize?

- Concept: Vector Quantization (VQ) and VQ-VAE
  - Why needed here: Forms the basis of the joint tokenization approach, allowing both music and motion to be encoded into discrete tokens that can be processed by transformers
  - Quick check question: What is the role of the commitment loss in VQ-VAE training, and how does it affect the stability of the codebook learning?

- Concept: Cross-modal attention and causal masking
  - Why needed here: Enables the transformer to process both music and motion tokens while maintaining proper temporal dependencies and allowing cross-modal conditioning
  - Quick check question: How does the cross-modal causal attention mask differ from standard causal attention, and why is this important for joint generation?

## Architecture Onboarding

- Component map: Music4All audio → Demucs vocal removal → Tokenization (stage 1) → Parallel generation training (stage 2) → Captioning training (stage 3) → Inference pipeline

- Critical path: Music4All audio → Demucs vocal removal → Tokenization (stage 1) → Parallel generation training (stage 2) → Captioning training (stage 3) → Inference pipeline

- Design tradeoffs: Shared codebook vs separate tokenizers (efficiency vs potential information loss), parallel generation vs sequential (speed vs control), pre-trained initialization vs training from scratch (efficiency vs customization)

- Failure signatures: Poor audio quality despite good quantitative scores (likely data quality issues), degraded performance on text-to-music vs other tasks (data imbalance), motion quality degradation when paired with music (cross-modal interference)

- First 3 experiments:
  1. Test music-motion alignment accuracy by calculating L1 distance between aligned vs unaligned beats on 300 random pairs
  2. Validate joint codebook encoding by comparing reconstruction quality of motion tokens using shared vs separate codebooks
  3. Test parallel generation capability by running joint music-motion generation and measuring beat alignment scores between generated outputs

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and discussion, several unresolved issues emerge around the generalizability of the shared codebook approach to motion types beyond dance, the impact of automatic text synthesis quality on captioning performance, and the scalability of the approach to additional modalities.

## Limitations

- Data quality and alignment reliability concerns due to automatic rhythmic alignment between unpaired music and motion data
- Limited evaluation scope focused primarily on dance motion, raising questions about generalization to other motion types
- Significant computational overhead despite claims of efficiency through pre-training, with unclear memory and training time requirements

## Confidence

**High Confidence (80-100%)**:
- The unified architecture can generate all three modalities (text, music, motion) in any combination
- The three-stage training approach is technically sound and implementable
- The methodology for aligning unpaired music and motion using rhythmic patterns is valid

**Medium Confidence (40-80%)**:
- The shared codebook approach effectively bridges the modality gap between music and motion
- Pre-trained initialization significantly reduces computational demands compared to training from scratch
- The parallel generation scheme maintains temporal coherence across modalities

**Low Confidence (0-40%)**:
- The quantitative metrics accurately reflect perceptual quality across all tasks
- The approach generalizes well to motion types beyond dance
- The automatic text synthesis from metadata produces high-quality captions

## Next Checks

1. **Alignment Accuracy Validation**: Run the music-motion alignment algorithm on 300 randomly selected pairs from the dataset and calculate the L1 distance between aligned beats versus unaligned beats. This will quantify whether the rhythmic alignment is sufficiently accurate for cross-modal generation.

2. **Codebook Representation Capacity**: Train a baseline motion tokenizer with a separate codebook and compare reconstruction quality (using metrics like L2 distance or perceptual similarity) against the shared codebook approach. This will reveal if the music codebook adequately represents motion dynamics.

3. **Cross-Modal Generation Coherence**: Generate paired music-motion outputs using the parallel generation scheme and compute beat alignment scores between the generated music and motion sequences. Compare these scores against a baseline that generates modalities sequentially without cross-modal conditioning.