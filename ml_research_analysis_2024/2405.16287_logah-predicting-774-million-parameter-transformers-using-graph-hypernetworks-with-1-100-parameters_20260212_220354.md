---
ver: rpa2
title: 'LoGAH: Predicting 774-Million-Parameter Transformers using Graph HyperNetworks
  with 1/100 Parameters'
arxiv_id: '2405.16287'
source_url: https://arxiv.org/abs/2405.16287
tags:
- parameters
- logah
- ghn-3
- dataset
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of scaling Graph HyperNetworks
  (GHNs) to predict parameters for very wide neural networks, which is challenging
  due to the exponential increase in parameters required. The authors propose LoGAH,
  a GHN with a low-rank parameter decoder that expands to significantly wider networks
  without requiring as excessive an increase in parameters as previous methods.
---

# LoGAH: Predicting 774-Million-Parameter Transformers using Graph HyperNetworks with 1/100 Parameters

## Quick Facts
- **arXiv ID**: 2405.16287
- **Source URL**: https://arxiv.org/abs/2405.16287
- **Authors**: Xinyu Zhou; Boris Knyazev; Alexia Jolicoeur-Martineau; Jie Fu
- **Reference count**: 20
- **Key outcome**: LoGAH achieves better performance than random initialization by up to 6.47% on CIFAR-100 and 2.16% on CIFAR-10 for ViT models, and achieves a perplexity score of 18.79 on WikiText-103 for GPT-2 models

## Executive Summary
LoGAH addresses the challenge of scaling Graph HyperNetworks (GHNs) to predict parameters for very wide neural networks, which traditionally requires exponentially increasing parameters. The method introduces a low-rank parameter decoder that expands to significantly wider networks without the prohibitive parameter growth seen in previous approaches. By using low-rank decomposition to predict parameters, LoGAH reduces the number of parameters needed from O(d³) to O(d²), enabling prediction of parameters for 774-million-parameter transformers using only 2.5 million parameters in the GHN. The approach is evaluated on ViT and GPT-2 models, demonstrating superior performance compared to random initialization and existing hypernetworks, with promising transfer learning results across different model scales.

## Method Summary
LoGAH is a Graph HyperNetwork that predicts parameters for wide neural networks using a low-rank parameter decoder. The method processes computational graphs of neural networks using Graphormer layers, then applies a low-rank decomposition W = AB to generate large weight matrices from smaller intermediate representations. This reduces parameter growth from O(d³) to O(d²), allowing the prediction of parameters for networks with 2048 channels while keeping the GHN parameter count low (2.5M for LoGAH-Tiny vs 105M+ for GHN-3). The GHN is trained on datasets of small ViT and GPT-2 architectures (VITS-1K and GPT S-1K) and demonstrates the ability to generalize to predict parameters for much larger models through transfer learning.

## Key Results
- LoGAH outperforms random initialization by up to 6.47% on CIFAR-100 and 2.16% on CIFAR-10 for ViT models
- Achieves perplexity score of 18.79 on WikiText-103 dataset for GPT-2 models
- Demonstrates successful transfer learning, with models trained on small datasets effectively initializing larger models
- Four variants: LoGAH-Tiny (2.5M params), LoGAH-Small (21.4M), LoGAH-Base (78.2M), LoGAH-Large (289.4M)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank decomposition reduces parameter growth from O(d³) to O(d²), enabling prediction of much wider networks without prohibitive parameter costs
- Mechanism: Instead of predicting full weight tensors, LoGAH uses low-rank decomposition W = AB to generate large weight matrices from smaller intermediate representations
- Core assumption: The rank r can be chosen such that r ≈ d/2 while maintaining sufficient expressiveness for accurate parameter prediction
- Evidence anchors:
  - [abstract]: "LoGAH achieves this by using a low-rank decomposition to predict parameters, reducing the number of parameters needed from O(d^3) to O(d^2)"
  - [section 3.1]: "Theoretically, we can fix r as a much smaller constant hyperparameter than d, then Eqn. (5) would be in O(d2), less than the complexity of original GHN's decoder O(d3)"

### Mechanism 2
- Claim: LoGAH generalizes from small training architectures to predict parameters for much larger models, enabling transfer learning across scales
- Mechanism: By training on diverse small ViT and GPT-2 architectures, LoGAH learns to predict parameters for larger unseen models through the GHN's graph representation and low-rank decoder
- Core assumption: The training dataset of small architectures covers the architectural space sufficiently for the GHN to generalize to larger models
- Evidence anchors:
  - [abstract]: "Furthermore, we show promising transfer learning results w.r.t. training LoGAH on small datasets and using the predicted parameters to initialize for larger tasks"
  - [section 5.4]: "In this section, we explore the setting when LoGAH is trained on one dataset, but is used to produce a parameter initialization for another (potentially more difficult) dataset"

### Mechanism 3
- Claim: Predicted parameters from LoGAH provide better initialization than random initialization, leading to improved convergence and final performance
- Mechanism: LoGAH learns to predict parameters that encode useful inductive biases from the training data, resulting in initializations that start closer to good solutions
- Core assumption: The optimization problem solved during GHN training produces parameters that encode meaningful patterns for the target architectures
- Evidence anchors:
  - [abstract]: "We show that vision and language models (i.e., ViT and GPT-2) initialized with LoGAH achieve better performance than those initialized randomly or using existing hypernetworks"
  - [section 5.1.1]: "LOGAH generally outperforms RAND INIT, ORTHINIT and GHN-3" with specific performance improvements listed in Table 2

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and their application to neural architecture representation
  - Why needed here: LoGAH processes computational graphs of neural networks using Graphormer layers
  - Quick check question: How do Graphormer layers encode edge features and positional information in the node representations?

- **Concept**: Low-rank matrix factorization and its approximation properties
  - Why needed here: The core innovation relies on decomposing large weight matrices into products of smaller matrices (W = AB)
  - Quick check question: What is the relationship between the rank r and the approximation error when factorizing a weight matrix using low-rank decomposition?

- **Concept**: Transfer learning and generalization across model scales
  - Why needed here: LoGAH is trained on small models but must generalize to predict parameters for much larger models
  - Quick check question: What properties of the training data distribution ensure that a model trained on small architectures can effectively generalize to larger architectures?

## Architecture Onboarding

- **Component map**: Computational graph -> Node embeddings -> Graph neural network processing -> Low-rank decoder -> Predicted parameters
- **Critical path**: Graph representation -> Node embeddings through Graphormer layers -> Low-rank parameter prediction -> Weight matrix generation
- **Design tradeoffs**: Higher rank r improves prediction quality but increases parameter count; larger meta-batch sizes improve stability but increase memory requirements
- **Failure signatures**: Poor performance on target tasks indicates either insufficient rank, inadequate training data coverage, or optimization issues during GHN training
- **First 3 experiments**:
  1. Train LoGAH-Tiny on VITS-1K and evaluate parameter prediction quality on held-out small ViT architectures
  2. Use LoGAH-Tiny to initialize ViT-Small on CIFAR-10 and compare performance against random initialization
  3. Test transfer learning by training LoGAH on CIFAR-10 and using it to initialize ViT on CIFAR-100

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LoGAH's performance scale with model size when predicting parameters for architectures beyond GPT-2-Large?
- Basis in paper: [explicit] The paper states LoGAH-Tiny can predict parameters for GPT-2-Large (774M parameters) but doesn't explore larger models
- Why unresolved: The paper focuses on ViT and GPT-2 models but doesn't test the upper limits of LoGAH's parameter prediction capability
- What evidence would resolve it: Experiments showing LoGAH's performance on architectures larger than GPT-2-Large, such as GPT-2-XL or even larger models

### Open Question 2
- Question: What is the impact of different low-rank decomposition values (r) on LoGAH's performance and parameter efficiency?
- Basis in paper: [explicit] The paper sets r ≈ d/2 but doesn't explore how different r values affect performance
- Why unresolved: The choice of r is heuristic and the paper doesn't investigate the sensitivity of LoGAH to this hyperparameter
- What evidence would resolve it: Systematic experiments varying r across a range of values and measuring both performance and parameter count

### Open Question 3
- Question: How does LoGAH's transfer learning capability compare when transferring between different domains (e.g., vision to language tasks)?
- Basis in paper: [inferred] The paper demonstrates transfer learning within domains but doesn't explore cross-domain transfer
- Why unresolved: Cross-domain transfer learning is a challenging problem and the paper doesn't address whether LoGAH's learned initialization is domain-specific or generalizable
- What evidence would resolve it: Experiments showing LoGAH's performance when transferring from vision tasks to language tasks or vice versa

## Limitations

- The paper's claims about generalization across scales are partially validated, as experiments primarily demonstrate transfer within the same model family (ViT to ViT, GPT-2 to GPT-2)
- The critical assumption that low-rank decomposition with r ≈ d/2 provides sufficient expressiveness lacks empirical validation of approximation quality across different rank values
- Performance on more diverse or complex tasks remains unexplored, particularly for language models beyond GPT-2

## Confidence

- **High confidence**: The computational complexity reduction from O(d³) to O(d²) through low-rank decomposition is mathematically sound and well-supported by theoretical analysis
- **Medium confidence**: The empirical results showing LoGAH outperforming random initialization (up to 6.47% on CIFAR-100) are convincing but based on limited datasets and model variations
- **Medium confidence**: The transfer learning claims are supported by experiments but lack comprehensive ablation studies on what architectural features enable successful generalization

## Next Checks

1. **Rank sensitivity analysis**: Systematically vary the rank parameter r from d/4 to d/2 to d and measure the impact on parameter prediction quality and final task performance to validate the choice of r ≈ d/2

2. **Cross-architecture transfer**: Test LoGAH trained on ViT architectures for parameter prediction on entirely different architectures (e.g., ResNets or CNNs) to evaluate the generality of learned inductive biases

3. **Scalability stress test**: Evaluate LoGAH's performance when predicting parameters for models significantly larger than those in the training distribution (e.g., predicting for 10B parameter models when trained on 10M parameter models) to identify the limits of scale generalization