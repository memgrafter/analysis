---
ver: rpa2
title: 'TernaryLLM: Ternarized Large Language Model'
arxiv_id: '2406.07177'
source_url: https://arxiv.org/abs/2406.07177
tags:
- quantization
- feature
- llms
- weights
- floating-point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TernaryLLM, a method for ternarizing large
  language models (LLMs) to reduce memory usage and enable energy-efficient inference.
  The approach addresses two main challenges: (1) the asymmetric distribution of weights
  in LLMs, which traditional symmetric ternarization methods fail to handle well,
  and (2) the significant information loss caused by extreme low-bit quantization.'
---

# TernaryLLM: Ternarized Large Language Model

## Quick Facts
- arXiv ID: 2406.07177
- Source URL: https://arxiv.org/abs/2406.07177
- Reference count: 40
- Key outcome: Outperforms previous low-bit quantization methods with 5.8 perplexity improvement on C4 and 8.2% accuracy gain on zero-shot tasks for LLaMA-3

## Executive Summary
TernaryLLM addresses the challenge of ternarizing large language models by introducing Dual Learnable Ternarization (DLT) and Outlier-Friendly Feature Knowledge Distillation (OFF). Traditional symmetric ternarization methods struggle with asymmetric weight distributions in LLMs, while extreme low-bit quantization destroys semantic clustering of related words. The proposed method uses learnable scales and shifts per weight group to adapt to asymmetric distributions and cosine similarity-based distillation to recover mutual information between floating-point and ternarized features. Experiments demonstrate significant improvements over previous low-bit quantization approaches across various LLM families.

## Method Summary
TernaryLLM quantizes LLM weights into three values {-1, 0, 1} using group-wise quantization with Dual Learnable Ternarization that introduces learnable scale and shift parameters per group. The method employs Outlier-Friendly Feature Knowledge Distillation using cosine similarity to maximize mutual information between floating-point and ternarized features, addressing the information loss caused by ternary quantization. Training uses AdamW optimizer with zero weight decay, cosine learning rate decay, batch size of 16, and 10,000 training steps with 500-step warm-up. The approach is evaluated on OPT and LLaMA family models ranging from 125M to 7B parameters.

## Key Results
- W1.58A16 configuration achieves 5.8 perplexity improvement on C4 dataset compared to W2A16 baseline
- 8.2% increase in average accuracy on zero-shot tasks including PIQA, ARC, BoolQ, HellaSwag, and Winogrande
- Outperforms previous low-bit quantization methods across various LLM families including LLaMA and OPT
- Successfully ternarizes models up to LLaMA-3-8B and OPT-6.7B while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric outliers in LLM weight groups degrade symmetric ternarization performance
- Mechanism: Traditional symmetric ternarization fails on LLM weight groups with asymmetric outliers, where some groups have negative mean with positive outliers and vice versa. DLT introduces learnable scales and shifts per group to better fit these asymmetric distributions
- Core assumption: Weight group asymmetries significantly impact quantization error and accuracy
- Evidence: Internal analysis shows asymmetric weight distributions in LLM groups; previous symmetric methods lead to suboptimal trade-off between clamping and rounding errors

### Mechanism 2
- Claim: Extreme low-bit quantization destroys semantic clustering of related words in LLM features
- Mechanism: Ternarization narrows feature representation range and eliminates dominant channel prominence, disrupting the tight clustering of semantically related words developed during pretraining. OFF uses cosine similarity to recover mutual information without being overwhelmed by outliers
- Core assumption: Semantic clustering structure in LLM features is critical for maintaining performance and is lost under ternary quantization
- Evidence: Internal observations show that ternarization destroys the semantic structure where related words are tightly clustered in feature space

### Mechanism 3
- Claim: MSE knowledge distillation fails under ternary quantization due to outlier-induced gradient dominance
- Mechanism: Ternary quantization introduces discrete weights, making exact feature alignment impossible. Large outlier activations dominate MSE gradients, causing training instability. OFF mitigates this using scale-invariant, outlier-insensitive cosine similarity
- Core assumption: Heterogeneity between floating-point and ternary feature spaces creates large gradients from outlier channels that overwhelm MSE training
- Evidence: Internal analysis shows MSE suffers from severe oscillations when large channel values dominate the loss

## Foundational Learning

- Concept: Group-wise quantization and asymmetric weight distributions
  - Why needed here: TernaryLLM quantizes weights per group; asymmetric distributions within groups degrade symmetric ternarization accuracy
  - Quick check question: If a weight group has mean=-0.5 and outliers at +2.0, what error would symmetric threshold=0.7 cause?

- Concept: Knowledge distillation and mutual information maximization
  - Why needed here: OFF uses cosine similarity to maximize mutual information between floating-point and ternarized features, recovering semantic structure lost to quantization
  - Quick check question: How does cosine similarity differ from MSE when one feature channel is an outlier 100x larger than others?

- Concept: Straight-through estimator (STE) for gradient approximation
  - Why needed here: DLT uses STE to approximate gradients for ternary weights, enabling end-to-end training with discrete weight values
  - Quick check question: What gradient does STE provide for a ternary weight equal to 0 when its floating-point value is +0.3?

## Architecture Onboarding

- Component map: Input -> DLT (Dual Learnable Ternarization) -> Ternary Weights -> OFF (Outlier-Friendly Feature Knowledge Distillation) -> Output

- Critical path:
  1. Group weights by channel or predefined size
  2. Apply DLT: threshold to {-1,0,1}, update learnable scale/shift via STE
  3. Forward pass with ternary weights (2 additions per multiply)
  4. Compute cosine similarity between features for OFF loss
  5. Backpropagate through STE to update scales/shifts and other parameters

- Design tradeoffs:
  - DLT adds learnable parameters but reduces quantization error for asymmetric groups
  - OFF trades MSE's direct error measurement for cosine similarity's outlier insensitivity
  - Ternary weights enable 2 additions instead of 1 multiply but limit representational capacity

- Failure signatures:
  - Training instability/oscillations (indicates MSE failure, switch to OFF)
  - Degradation in perplexity or zero-shot accuracy (indicates insufficient recovery of semantic structure)
  - Slow convergence (indicates poor scale/shift initialization in DLT)

- First 3 experiments:
  1. Implement DLT on a small LLM (e.g., OPT-125M) with symmetric baseline; compare perplexity
  2. Replace MSE with cosine similarity in feature distillation; observe training stability
  3. Test ternary inference speed using floating-point additions vs. original multiplications

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TernaryLLM scale with model size beyond the tested range?
- Basis in paper: Tested on models up to LLaMA-3-8B and OPT-6.7B, but does not explore performance on larger models like GPT-3 or beyond
- Why unresolved: Authors did not conduct experiments on models larger than those tested
- What evidence would resolve it: Experiments demonstrating performance on models significantly larger than those tested in the paper

### Open Question 2
- Question: What is the impact of TernaryLLM on inference latency in real-world deployment scenarios?
- Basis in paper: Discusses theoretical benefits of ternary quantization but lacks empirical data on actual inference speed improvements
- Why unresolved: Paper focuses on memory efficiency and accuracy but lacks real-world latency benchmarks
- What evidence would resolve it: Benchmark results showing inference time improvements on various hardware platforms

### Open Question 3
- Question: How does TernaryLLM perform in multilingual or specialized domain tasks compared to general NLP benchmarks?
- Basis in paper: Experiments focus on general NLP tasks and datasets like WikiText2 and C4
- Why unresolved: Paper does not provide data on multilingual or specialized domain tasks
- What evidence would resolve it: Results from multilingual benchmarks or domain-specific datasets demonstrating performance across diverse tasks

### Open Question 4
- Question: What are the long-term stability and degradation effects of TernaryLLM under continuous usage?
- Basis in paper: Does not address durability of TernaryLLM models under prolonged usage or varying input distributions
- Why unresolved: No mention of long-term performance monitoring or degradation analysis
- What evidence would resolve it: Longitudinal studies tracking model performance over time and under different input distributions

## Limitations
- Mechanism claims about asymmetric weight distributions and semantic clustering disruption lack external validation
- Computational overhead of per-group statistics may become prohibitive for extremely large models
- The specific contribution of cosine similarity versus general outlier robustness to performance gains needs ablation study

## Confidence

**High Confidence:** Empirical performance claims (5.8 perplexity improvement on C4, 8.2% accuracy gain on zero-shot tasks) are well-supported by experimental results and sound evaluation methodology.

**Medium Confidence:** Theoretical framework for OFF using cosine similarity is mathematically coherent, but practical necessity and optimality versus other outlier-robust metrics remains uncertain without ablation studies.

**Low Confidence:** Mechanism claims about semantic clustering destruction and the specific role of asymmetric outliers in degrading symmetric ternarization are primarily based on internal observations without external validation or comprehensive ablation studies.

## Next Checks
1. Conduct ablation study comparing OFF (cosine similarity) against MSE with outlier clipping, Huber loss, and other robust metrics to quantify the specific contribution of cosine similarity.

2. Perform comprehensive statistical analysis of weight group asymmetries across different LLM families and scales to validate whether asymmetric outliers are consistently problematic.

3. Implement quantitative measures of feature space semantic structure to directly test whether ternary quantization destroys semantic clustering and whether OFF effectively recovers it.