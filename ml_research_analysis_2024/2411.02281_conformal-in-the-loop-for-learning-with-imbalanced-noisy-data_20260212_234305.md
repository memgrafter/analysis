---
ver: rpa2
title: Conformal-in-the-Loop for Learning with Imbalanced Noisy Data
arxiv_id: '2411.02281'
source_url: https://arxiv.org/abs/2411.02281
tags:
- class
- learning
- training
- noise
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Conformal-in-the-Loop (CitL) introduces a novel uncertainty-aware
  training framework that leverages conformal prediction to address the dual challenges
  of class imbalance and label noise. The method dynamically adjusts sample weights
  and prunes unreliable examples by evaluating prediction set sizes derived from conformal
  prediction.
---

# Conformal-in-the-Loop for Learning with Imbalanced Noisy Data

## Quick Facts
- arXiv ID: 2411.02281
- Source URL: https://arxiv.org/abs/2411.02281
- Authors: John Brandon Graham-Knight; Jamil Fayyad; Nourhan Bayasi; Patricia Lasserre; Homayoun Najjaran
- Reference count: 40
- Primary result: CitL achieves up to 6.1% accuracy improvement on CIFAR-10 and 5.0 mIoU improvement on CityScapes

## Executive Summary
Conformal-in-the-Loop (CitL) introduces a novel uncertainty-aware training framework that leverages conformal prediction to address the dual challenges of class imbalance and label noise. The method dynamically adjusts sample weights and prunes unreliable examples by evaluating prediction set sizes derived from conformal prediction. Experimental results demonstrate that CitL significantly improves model performance across both multiclass classification and semantic segmentation tasks while maintaining minimal computational overhead.

## Method Summary
CitL uses prediction set sizes from conformal prediction (specifically the Least Ambiguous Classifier) to weight training losses and prune unreliable examples. The method operates by evaluating sample uncertainty through prediction set size, where larger sets indicate higher uncertainty. During training, it assigns higher weights to uncertain examples (typically minority classes) early in training, then reduces weights as uncertainty decreases. A single hyperparameter α controls the balance between weighting uncertain examples and pruning highly uncertain ones. The framework works with standard neural network architectures and adds minimal computational overhead by caching non-conformity scores.

## Key Results
- CIFAR-10: Up to 6.1% accuracy improvement over baseline, with 18.3% gain for Airplane class at 50% noise level
- CityScapes semantic segmentation: 5.0 mIoU improvement over baseline, with some classes showing >10 mIoO gains
- Computational efficiency: Only 11% additional training time per step for classification, 4% for segmentation tasks

## Why This Works (Mechanism)

### Mechanism 1
Conformal prediction-based uncertainty quantification enables dynamic reweighting and pruning that addresses both label noise and class imbalance simultaneously. The method uses prediction set size (|P|) from conformal prediction to weight training losses, where larger prediction sets indicate higher uncertainty. This allows selective emphasis on hard-to-learn minority class samples while pruning potentially mislabeled high-uncertainty examples.

### Mechanism 2
Early training emphasis on uncertain examples followed by gradual reduction creates effective curriculum learning for imbalanced datasets. The method initially assigns high weights to uncertain examples (typically minority classes), then reduces weights as training progresses and uncertainty decreases, preventing overfitting to noise while ensuring minority classes are learned.

### Mechanism 3
The single hyperparameter α effectively balances between weighting uncertain examples and pruning noisy ones without requiring dataset-specific tuning. α controls the conformal prediction quantile threshold - lower α values increase prediction set sizes (more weighting, less pruning) while higher α values decrease set sizes (more pruning, less weighting).

## Foundational Learning

- **Conformal prediction and prediction set construction**: Forms the core uncertainty quantification mechanism that drives both weighting and pruning decisions. Quick check: How does the Least Ambiguous Classifier (LAC) differ from Adaptive Prediction Set (APS) in handling empty prediction sets?

- **Curriculum learning and training dynamics**: Understanding how uncertainty evolves during training is crucial for interpreting the method's behavior and failure modes. Quick check: Why does the method assign higher weights to uncertain examples early in training but reduce them later?

- **Class imbalance metrics and evaluation**: Proper evaluation requires understanding metrics like macro accuracy and mIoU that account for class imbalance. Quick check: How does macro accuracy differ from overall accuracy in imbalanced datasets, and why is it more appropriate here?

## Architecture Onboarding

- **Component map**: Data → Base model → Conformal prediction → Weight calculation → Modified loss → Parameter update → Evaluation
- **Critical path**: Data flows through base model to generate predictions, conformal prediction module computes uncertainty, weights are calculated and applied to loss function, parameters are updated, and validation occurs periodically
- **Design tradeoffs**: Single α parameter provides simplicity but may not capture dataset-specific nuances; using only softmax outputs limits flexibility but ensures minimal overhead; caching non-conformity scores trades memory for computational efficiency
- **Failure signatures**: Poor performance with very high noise levels (>10%) where minority classes get preferentially pruned; failure to reduce uncertainty for clean samples leading to curriculum breakdown; suboptimal α selection causing either excessive pruning or insufficient noise handling
- **First 3 experiments**:
  1. Run baseline cross-entropy training on CIFAR-10 with no noise to establish performance baseline
  2. Apply CitL with α=0.15 on CIFAR-10 with 10% synthetic noise to verify improvement over baseline
  3. Test CitL on CityScapes with α=0.02 to validate segmentation performance gains

## Open Questions the Paper Calls Out

### Open Question 1
How does Conformal-in-the-Loop (CitL) perform when applied to multi-label classification tasks with imbalanced and noisy data? The paper focuses on multiclass classification and semantic segmentation but does not address multi-label classification tasks, leaving the effectiveness of CitL in such contexts unexplored.

### Open Question 2
What is the optimal strategy for dynamically determining the hyperparameter α in CitL without relying on grid search? The paper acknowledges the sensitivity of CitL to the hyperparameter α and suggests exploring heuristic methods for dynamic determination, but does not provide a concrete heuristic or method.

### Open Question 3
How does CitL handle extreme noise levels beyond 50% in classification tasks, and what are its limitations in such scenarios? The paper evaluates CitL up to 50% noise in CIFAR-10 classification but does not explore higher noise levels, which may occur in real-world datasets.

## Limitations
- Performance depends critically on proper α hyperparameter selection through grid search, which may not generalize well to unseen datasets
- The approach assumes that prediction set size reliably indicates label quality, but this relationship may break down with adversarial examples or complex feature distributions
- Computational overhead, while reported as minimal, still increases training time by 4-11% and requires additional memory for caching non-conformity scores

## Confidence
- **High Confidence**: Claims about improved performance on CIFAR-10 (6.1% accuracy increase) and CityScapes (5.0 mIoU improvement) are directly supported by experimental results
- **Medium Confidence**: The single-parameter balancing mechanism (α) is effective across datasets, though optimal values vary significantly between tasks
- **Medium Confidence**: The curriculum learning mechanism works as described, but relies on assumptions about uncertainty reduction that may not hold for all datasets

## Next Checks
1. Test CitL on datasets with different noise patterns (instance-dependent noise) to verify robustness beyond synthetic uniform noise
2. Evaluate the method's performance when class imbalance ratios exceed those tested in the paper (extreme imbalance scenarios)
3. Compare CitL against recent state-of-the-art methods for noisy label training that don't use conformal prediction to isolate the contribution of the conformal component