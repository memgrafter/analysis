---
ver: rpa2
title: 'UnibucLLM: Harnessing LLMs for Automated Prediction of Item Difficulty and
  Response Time for Multiple-Choice Questions'
arxiv_id: '2404.13343'
source_url: https://arxiv.org/abs/2404.13343
tags:
- llms
- difficulty
- answers
- response
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using LLMs to predict item difficulty and response
  time for USMLE MCQs. It augments the dataset with answers from zero-shot LLMs (Falcon,
  Meditron, Mistral) and trains transformer-based models on six feature combinations.
---

# UnibucLLM: Harnessing LLMs for Automated Prediction of Item Difficulty and Response Time for Multiple-Choice Questions

## Quick Facts
- arXiv ID: 2404.13343
- Source URL: https://arxiv.org/abs/2404.13343
- Reference count: 3
- Primary result: LLM-augmented features improve difficulty and response time prediction for medical MCQs

## Executive Summary
This paper presents UnibucLLM, a system that leverages large language models (LLMs) to predict item difficulty and response time for multiple-choice questions from the United States Medical Licensing Examination (USMLE). The approach augments a small dataset of 104 questions with answers from three zero-shot LLMs (Falcon, Meditron, Mistral) and trains transformer-based models on six different feature combinations. The study demonstrates that predicting item difficulty is more challenging than predicting response time, and that including question text as a feature consistently improves model performance. The best results are achieved using post-competition linear probing methods with Support Vector Regression (SVR) and BERT, achieving a Kendall τ correlation of 0.1592 for difficulty prediction.

## Method Summary
The study uses a dataset of 104 USMLE multiple-choice questions, each annotated with actual difficulty (percentage of correct answers) and average response time. The authors generate zero-shot answers for each question using three LLMs: Falcon, Meditron, and Mistral. They create six feature sets combining various elements: question text, LLM-generated answers, and metadata. For each feature set, they train transformer-based models and evaluate them using cross-validation. Post-competition, they implement linear probing methods using SVR with BERT embeddings. The models are evaluated using Kendall's τ correlation for ranking quality and Spearman's ρ correlation for overall performance.

## Key Results
- Difficulty prediction (Kendall τ = 0.1592) is significantly harder than response time prediction
- Question text features are essential for both difficulty and response time prediction tasks
- LLM answer variability improves model performance across different feature combinations
- Post-competition SVR+BERT linear probing methods outperform fine-tuning approaches on the small dataset

## Why This Works (Mechanism)
The success of LLM-augmented features stems from their ability to capture semantic patterns in question-answer relationships that correlate with human response patterns. By generating multiple zero-shot answers from different LLMs, the system creates a richer feature space that captures the inherent ambiguity and complexity of medical questions. The question text provides critical context that helps models distinguish between different levels of difficulty and expected response times. The linear probing approach with pre-trained BERT embeddings is particularly effective on small datasets because it avoids overfitting while still leveraging powerful contextual representations.

## Foundational Learning
- **Zero-shot LLM generation**: Creating answers without fine-tuning allows capturing baseline model behavior that correlates with human difficulty perceptions. Quick check: Compare zero-shot vs. few-shot performance to validate this assumption.
- **Feature engineering for MCQs**: Combining question text, LLM answers, and metadata creates a multi-modal representation of each item. Quick check: Ablation studies showing which feature combinations are most predictive.
- **Transformer-based ranking**: Using transformers for ordinal regression tasks requires careful loss function design and normalization. Quick check: Verify that Kendall's τ is appropriate for the skewed difficulty distribution.
- **Linear probing with SVR**: This technique avoids overfitting on small datasets by freezing pre-trained weights and only training a shallow regressor. Quick check: Compare with full fine-tuning to confirm generalization benefits.

## Architecture Onboarding

**Component Map:**
Dataset (104 USMLE items) -> LLM zero-shot generation (Falcon, Meditron, Mistral) -> Feature engineering (6 combinations) -> Transformer models (BERT variants) -> Linear probing (SVR+BERT) -> Evaluation (Kendall's τ, Spearman's ρ)

**Critical Path:**
1. Dataset preparation and LLM answer generation
2. Feature set creation and model training
3. Cross-validation evaluation and post-competition analysis

**Design Tradeoffs:**
The choice between fine-tuning and linear probing represents a fundamental tradeoff between model capacity and generalization on small datasets. Fine-tuning offers higher potential performance but risks overfitting with only 104 items. Linear probing with SVR provides better generalization but may miss complex nonlinear relationships. The six feature combinations represent another tradeoff between model complexity and interpretability.

**Failure Signatures:**
- Poor generalization when fine-tuning on small datasets (evidenced by performance drops in cross-validation)
- Suboptimal performance when excluding question text features (consistently worse across all models)
- Limited improvement when using only single LLM features versus combined LLM outputs

**3 First Experiments:**
1. Ablation study removing individual LLM features to quantify their specific contributions
2. Comparison of different transformer architectures (RoBERTa, DistilBERT) for the same feature sets
3. Analysis of feature importance using SHAP values to understand which LLM characteristics drive predictions

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset consists of only 104 items, which constrains model generalizability and raises overfitting concerns
- Performance metrics show considerable variability across different feature sets and evaluation folds
- The competition evaluation framework may not fully represent real-world generalization scenarios

## Confidence
- **High confidence**: Difficulty prediction is more challenging than response time prediction; question text features are crucial for both tasks
- **Medium confidence**: Superiority of post-competition linear probing methods over fine-tuning is demonstrated but may be specific to small dataset context
- **Low confidence**: Specific contribution of individual LLM features to prediction performance is difficult to isolate

## Next Checks
1. External validation: Test best-performing models on a larger, independent dataset of medical MCQs to assess true generalization capability
2. Ablation study: Systematically remove individual LLM features to quantify their specific contributions
3. Longitudinal analysis: Evaluate model performance across different medical domains and question types within USMLE