---
ver: rpa2
title: 'Vision language models are blind: Failing to translate detailed visual features
  into words'
arxiv_id: '2407.06581'
source_url: https://arxiv.org/abs/2407.06581
tags:
- vlms
- circles
- image
- accuracy
- sonnet-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the visual perception capabilities of
  large vision-language models (VLMs) on low-level geometric tasks. The authors design
  BlindTest, a benchmark suite of seven tasks involving simple 2D geometric primitives,
  and evaluate four state-of-the-art VLMs: GPT-4o, Gemini-1.5 Pro, Claude-3 Sonnet,
  and Claude-3.5 Sonnet.'
---

# Vision language models are blind: Failing to translate detailed visual features into words

## Quick Facts
- **arXiv ID**: 2407.06581
- **Source URL**: https://arxiv.org/abs/2407.06581
- **Reference count**: 40
- **Primary result**: VLMs achieve only 77.84% accuracy on low-level geometric tasks, far below expected human performance

## Executive Summary
This paper investigates whether large vision-language models (VLMs) can accurately perceive and interpret low-level visual features in geometric tasks. The authors introduce BlindTest, a benchmark suite of seven tasks using simple 2D geometric primitives, and evaluate four state-of-the-art VLMs. Results reveal that VLMs struggle significantly with tasks requiring precise spatial information, such as detecting overlapping circles and counting intersections. Linear probing experiments demonstrate that vision encoders contain sufficient information to solve these tasks, but language models fail to decode this information into correct answers. The study highlights a fundamental limitation in how VLMs translate visual features into language outputs.

## Method Summary
The paper evaluates four pre-trained VLMs (GPT-4o, Gemini-1.5 Pro, Claude-3 Sonnet, Claude-3.5 Sonnet) on BlindTest, a benchmark of seven geometric tasks involving 2D primitives. The benchmark includes 7,968 images testing tasks like detecting overlapping circles, counting line intersections, identifying circled letters, and counting nested shapes. VLMs are evaluated directly without fine-tuning, using specified prompts to extract answers. Linear probing experiments assess whether frozen vision encoder features contain sufficient information by training logistic regression classifiers on these features. Simplified versions of tasks with increased spacing between elements are also tested to investigate spatial precision requirements.

## Key Results
- VLMs achieve only 77.84% average accuracy across all BlindTest tasks, far below human baseline of 100%
- Linear probing shows vision encoders achieve ≥99.47% accuracy on two-circle and line-intersection tasks, indicating the bottleneck is in language model decoding
- Increasing spacing between geometric elements significantly improves VLM performance, suggesting spatial precision is a key challenge
- Claude-3.5 Sonnet performs best at 77.84% accuracy, while other models score between 66.58% and 74.28%

## Why This Works (Mechanism)

### Mechanism 1
- Vision encoders in VLMs already contain the low-level visual information needed to solve BlindTest tasks, but language models fail to decode it into correct answers
- Linear probing experiments show that frozen vision encoder features achieve ≥99.47% accuracy on two-circle and line-intersection tasks, while full VLMs perform far worse (e.g., 33.14% on two circles)
- Core assumption: The bottleneck is in the language model's ability to interpret spatial features, not in the vision encoder's extraction of those features

### Mechanism 2
- VLMs struggle with low-level geometric tasks when shapes are close together, overlapping, or nested because their "late fusion" architecture processes visual features before considering the textual question
- Vision encoders extract features independently of the question context, so precise spatial relationships (e.g., tiny gaps between circles) are lost before the language model processes the question
- Core assumption: Late fusion prevents the model from focusing visual extraction on task-relevant spatial details

### Mechanism 3
- VLMs have difficulty counting overlapping or nested shapes because they lack the ability to trace individual shapes through intersections or nesting, unlike humans who can mentally separate them
- When shapes overlap or nest, VLMs cannot maintain shape identity across shared boundaries, leading to undercounting or overcounting
- Core assumption: VLMs lack explicit shape-tracing or segmentation capabilities needed for complex geometric configurations

## Foundational Learning

- **Linear probing and frozen feature evaluation**: Used to determine whether vision encoders contain sufficient information for BlindTest tasks without language model interference
  - Quick check: If a logistic regression classifier trained on frozen vision encoder features achieves >99% accuracy on a task, what does this imply about the bottleneck in VLM performance?

- **Late fusion vs. early fusion in multimodal architectures**: Critical for understanding why vision encoders might lose spatial precision when processing images independently of questions
  - Quick check: In a late fusion VLM, at what stage are visual features combined with language representations, and how might this affect spatial detail preservation?

- **Shape perception and counting in computer vision**: Essential for understanding the specific challenges VLMs face with overlapping, nested, or closely spaced geometric primitives
  - Quick check: What visual processing capabilities (e.g., segmentation, contour tracing) would help a model accurately count overlapping circles?

## Architecture Onboarding

- **Component map**: Vision encoder (e.g., CLIP, SigLIP) → Projection layer → Language model → Output
- **Critical path**: Image → Vision encoder features → Language model reasoning → Answer generation
- **Design tradeoffs**: Late fusion (vision independent of question) vs. early fusion (vision conditioned on question); frozen vision encoders vs. end-to-end training
- **Failure signatures**: High accuracy on disjoint shapes but low accuracy on overlapping/nested shapes; improvement when shapes are separated; linear probe success but VLM failure
- **First 3 experiments**:
  1. Train logistic regression on frozen vision encoder features for two-circle task and measure accuracy
  2. Vary spacing between shapes in BlindTest tasks and measure VLM accuracy changes
  3. Implement early fusion variant where vision encoding is conditioned on the question and test on BlindTest

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do VLMs exhibit a systematic bias toward specific numbers (like 5 in the Olympic logo task) when counting objects, and if so, what architectural or training factors cause this bias?
- **Basis**: The paper demonstrates that Gemini-1.5 consistently predicts "5" circles even when the actual count is 6, 7, 8, or 9, showing a strong bias toward the Olympic logo configuration
- **Why unresolved**: While the paper identifies the bias, it does not investigate whether this stems from training data distribution, architectural limitations, or other factors
- **What evidence would resolve it**: Ablation studies testing different training datasets, architectural modifications, or controlled experiments with varying number distributions could determine the source of this bias

### Open Question 2
- **Question**: Can fine-tuning VLMs on the specific visual tasks in BlindTest significantly improve their performance, or are these failures indicative of deeper architectural limitations?
- **Basis**: The paper reports that fine-tuning Bunny on the two-circle task showed improvements with smaller datasets but failed to generalize with larger ones, suggesting overfitting without solving the underlying issue
- **Why unresolved**: The paper only tests one model (Bunny) on one task, leaving open whether different fine-tuning approaches or other VLMs might perform better
- **What evidence would resolve it**: Systematic fine-tuning experiments across multiple VLMs, tasks, and approaches (including multi-task learning) would clarify whether these failures can be remedied through training or require architectural changes

### Open Question 3
- **Question**: Does the language decoder in VLMs significantly constrain their visual accuracy, or is the limitation primarily in the vision encoder?
- **Basis**: The paper shows that LLaVA-OneV with larger language decoders (72B vs 0.5B) performs significantly better on tasks like counting shapes and identifying circled letters, suggesting the language decoder plays a crucial role
- **Why unresolved**: While the paper demonstrates this correlation, it doesn't establish causation or explore whether the limitation is in decoding visual features into language or in the vision encoder's representation itself
- **What evidence would resolve it**: Comparative experiments with frozen vision encoders but different language decoders, or vice versa, could isolate whether the bottleneck is in encoding or decoding stages

## Limitations
- Linear probing experiments assume logistic regression classifiers can adequately evaluate vision encoder sufficiency, but more complex decoders might perform differently
- Study focuses only on 2D geometric primitives, which may not generalize to more complex visual scenarios or natural images
- Paper does not investigate whether fine-tuning vision encoders on BlindTest-like tasks would improve performance

## Confidence
- **High Confidence**: Mechanisms 1 and 2 have strong quantitative support showing ≥99% linear probe accuracy versus <35% VLM accuracy on the same tasks
- **Medium Confidence**: Mechanism 3 has supporting evidence through performance patterns but lacks direct experimental validation

## Next Checks
1. Test whether vision encoder features can be decoded by more sophisticated models than logistic regression (e.g., small MLPs or attention-based decoders) to confirm the bottleneck is truly in language model interpretation rather than feature representation quality
2. Implement an early fusion variant where the vision encoder processes images conditioned on the question text, then evaluate whether this architecture improves performance on BlindTest tasks compared to the late fusion baseline
3. Conduct ablation studies where vision encoder features are progressively corrupted or spatial information is removed to identify which specific visual features are critical for VLM success on geometric tasks