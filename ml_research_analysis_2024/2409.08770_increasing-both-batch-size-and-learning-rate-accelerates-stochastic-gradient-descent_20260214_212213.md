---
ver: rpa2
title: Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient
  Descent
arxiv_id: '2409.08770'
source_url: https://arxiv.org/abs/2409.08770
tags:
- learning
- batch
- rate
- increasing
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence of mini-batch stochastic gradient
  descent (SGD) under various batch size and learning rate scheduling schemes. It
  shows that increasing both batch size and learning rate can accelerate convergence
  compared to traditional constant batch size and decaying learning rate approaches.
---

# Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2409.08770
- Source URL: https://arxiv.org/abs/2409.08770
- Authors: Hikaru Umeda; Hideaki Iiduka
- Reference count: 40
- Key outcome: This paper shows that increasing both batch size and learning rate accelerates convergence compared to traditional constant batch size and decaying learning rate approaches.

## Executive Summary
This paper analyzes mini-batch stochastic gradient descent (SGD) under various batch size and learning rate scheduling schemes. It provides theoretical upper bounds on the expected full gradient norm for four scenarios: constant batch size with decaying learning rate, increasing batch size with decaying learning rate, increasing batch size with increasing learning rate, and increasing batch size with warm-up decaying learning rate. The analysis demonstrates that increasing both batch size and learning rate leads to faster convergence rates than traditional approaches. Numerical results on training ResNet-18 on CIFAR100 support the theoretical findings, showing that schemes with increasing batch sizes and learning rates minimize the full gradient norm faster than schemes with constant or decaying learning rates.

## Method Summary
The paper analyzes mini-batch SGD convergence with four scheduling scenarios. It considers constant batch size with decaying learning rate (Case i), increasing batch size with decaying learning rate (Case ii), increasing batch size with increasing learning rate (Case iii), and increasing batch size with warm-up decaying learning rate (Case iv). The analysis provides theoretical upper bounds on the expected full gradient norm for each scenario. Numerical experiments are conducted using CIFAR100 dataset with 50000 training samples and ResNet-18 architecture, comparing full gradient norm, empirical loss, and test accuracy across the different scheduling schemes.

## Key Results
- Schemes with increasing batch sizes and learning rates (iii and iv) achieve faster convergence rates than traditional approaches
- Using increasing batch sizes with decaying learning rates (scheme ii) guarantees convergence while constant batch size with decaying learning rate (scheme i) may not
- Increasing both batch size and learning rate leads to O(γ^(-M/2)) convergence rate, better than the O(1/√M) rate of constant learning rate schemes
- Warm-up decaying learning rates (scheme iv) accelerate convergence by combining initial acceleration with later stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing both batch size and learning rate accelerates convergence compared to traditional constant batch size and decaying learning rate approaches.
- Mechanism: When batch size increases, keeping learning rates large is beneficial. Using increasing batch sizes and increasing learning rates leads to O(γ^(-M/2)) convergence rate, better than O(1/√M) with constant learning rates. Larger batch sizes provide more stable gradient estimates, allowing larger learning rates without compromising convergence.
- Core assumption: The learning rate schedule satisfies γ² < δ to guarantee convergence.
- Evidence anchors: [abstract]: "Schemes (iii) and (iv) achieve faster convergence rates than (i) and (ii)"; [section]: "From Case (ii), when batch sizes increase, keeping learning rates large is useful for training DNNs"; [corpus]: Weak evidence - related papers focus on adaptive batch size schedulers but don't directly address simultaneous increase of both batch size and learning rate
- Break condition: The acceleration mechanism breaks when γ² ≥ δ, violating the convergence condition.

### Mechanism 2
- Claim: Decaying learning rates with warm-up minimizes the full gradient norm faster than using only decaying or only increasing learning rates.
- Mechanism: The warm-up period accelerates convergence during early training, while subsequent decay ensures convergence stability. This combines rapid initial progress with stable convergence.
- Core assumption: The warm-up period is sufficiently long to provide acceleration benefits before transitioning to decay.
- Evidence anchors: [abstract]: "schemes (iii) and (iv) accelerate mini-batch SGD"; [section]: "One way to guarantee fast convergence of mini-batch SGD with increasing batch sizes is to increase learning rates (acceleration period; Case (iii)) during the first epochs and then decay the learning rates (convergence period; Case (ii))"; [corpus]: Missing evidence - related papers discuss warm-up strategies but not in combination with increasing batch sizes
- Break condition: The mechanism breaks if the warm-up period is too short to provide meaningful acceleration or too long causing instability in later training phases.

### Mechanism 3
- Claim: Increasing batch size while keeping learning rates large reduces the variance term in convergence bounds, leading to faster minimization of the empirical loss.
- Mechanism: As batch size increases, the variance term in the convergence bound (inversely proportional to batch size) decreases. When combined with appropriately scaled learning rates, this results in faster reduction of the expected full gradient norm.
- Core assumption: The learning rate increase compensates for the reduced gradient noise, maintaining sufficient progress toward minima.
- Evidence anchors: [abstract]: "using scheduler (iii) or (iv) minimizes the full gradient norm of the empirical loss faster than using scheduler (i) or (ii)"; [section]: "Theorem 3.1 indicates that the bias term including BT converges to 0 as O(1/T), whereas the variance term including VT does not always converge to 0"; [corpus]: Weak evidence - related papers discuss variance reduction but not in the context of simultaneous batch size and learning rate increases
- Break condition: The mechanism fails when learning rates become too large relative to batch size increases, causing instability or divergence.

## Foundational Learning

- Concept: Convergence rate analysis for stochastic optimization
  - Why needed here: Understanding the theoretical guarantees for different scheduling schemes is crucial for interpreting the paper's main claims about acceleration
  - Quick check question: What is the difference between the convergence rates O(1/√M) and O(γ^(-M/2))?

- Concept: Lipschitz continuity and smoothness assumptions
  - Why needed here: These mathematical properties are fundamental to the convergence analysis and the derivation of upper bounds on gradient norms
  - Quick check question: Why does the paper assume the loss functions have Lipschitz continuous gradients?

- Concept: Variance reduction in mini-batch SGD
  - Why needed here: The paper's analysis heavily relies on understanding how batch size affects the variance of gradient estimates, which directly impacts convergence
  - Quick check question: How does increasing batch size affect the variance term in the convergence bound?

## Architecture Onboarding

- Component map: Theoretical analysis module -> Numerical validation module -> Scheduler configuration module -> Performance evaluation module
- Critical path: 1. Define scheduling scheme; 2. Run theoretical analysis to derive convergence bounds; 3. Implement corresponding scheduler in training code; 4. Execute training experiments with CIFAR100/ResNet-18; 5. Compare convergence metrics across scheduling schemes
- Design tradeoffs:
  - Batch size vs. learning rate: Larger batch sizes allow larger learning rates but require more memory
  - Warm-up duration: Longer warm-up provides more acceleration but delays the stability of decaying phase
  - Computational cost: Increasing batch size reduces the number of parameter updates but increases per-update computation
- Failure signatures:
  - Divergence: Learning rates too large relative to batch size increases
  - Slow convergence: Insufficient batch size increase or learning rate decay too aggressive
  - Memory overflow: Batch size increases beyond hardware limits
- First 3 experiments:
  1. Implement scheduler (iii) with exponential batch size increase (doubling every 30 epochs) and linearly increasing learning rate, measure convergence on CIFAR100
  2. Compare scheduler (iv) with warm-up (30 epochs) followed by decay against pure scheduler (iii) on the same dataset
  3. Test scheduler (iii) with different γ values (1.080, 1.196, 1.292) while keeping δ=2 to verify the O(γ^(-M/2)) convergence rate claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of mini-batch SGD with increasing batch sizes and learning rates compare to other state-of-the-art optimizers like Adam or AdaGrad in practical deep learning tasks?
- Basis in paper: [inferred] The paper focuses on mini-batch SGD with various batch size and learning rate scheduling schemes, but does not compare its performance to other optimizers like Adam or AdaGrad.
- Why unresolved: The paper only provides theoretical analyses and numerical results for mini-batch SGD with increasing batch sizes and learning rates, without comparing its performance to other optimizers.
- What evidence would resolve it: Empirical comparisons of mini-batch SGD with increasing batch sizes and learning rates to other optimizers like Adam or AdaGrad on various deep learning tasks and datasets.

### Open Question 2
- Question: How does the choice of batch size and learning rate scheduling schemes affect the generalization performance of deep neural networks?
- Basis in paper: [explicit] The paper mentions that the performance of mini-batch SGD strongly depends on setting the batch size and learning rate, and provides theoretical analyses and numerical results for various scheduling schemes. However, it does not explicitly discuss the impact on generalization performance.
- Why unresolved: The paper focuses on the convergence rate of mini-batch SGD with increasing batch sizes and learning rates, but does not investigate how the choice of scheduling schemes affects the generalization performance of deep neural networks.
- What evidence would resolve it: Empirical studies on the generalization performance of deep neural networks trained with mini-batch SGD using different batch size and learning rate scheduling schemes.

### Open Question 3
- Question: How does the proposed scheduling scheme (increasing batch size and learning rate) perform in distributed or federated learning settings where communication costs are a concern?
- Basis in paper: [inferred] The paper does not discuss the application of the proposed scheduling scheme in distributed or federated learning settings, where communication costs can be a significant factor.
- Why unresolved: The paper focuses on the convergence rate of mini-batch SGD with increasing batch sizes and learning rates in a centralized setting, without considering the communication costs in distributed or federated learning scenarios.
- What evidence would resolve it: Empirical studies on the performance of the proposed scheduling scheme in distributed or federated learning settings, considering the trade-off between convergence rate and communication costs.

## Limitations
- Theoretical analysis relies on Lipschitz continuity and smoothness assumptions that may not hold for all deep neural network architectures
- Analysis focuses on convex loss functions while many modern deep learning applications involve non-convex optimization landscapes
- Theoretical convergence bounds are asymptotic and may not fully capture practical training dynamics

## Confidence
- Claim about acceleration mechanism (Mechanism 1): High confidence - well-supported by theoretical analysis and numerical results
- Claim about warm-up decay benefits (Mechanism 2): Medium confidence - theoretical support is reasonable but lacks strong empirical validation
- Claim about variance reduction (Mechanism 3): Low confidence - theoretical reasoning is present but empirical evidence is limited

## Next Checks
1. Test the proposed scheduling schemes (iii) and (iv) on additional datasets and architectures beyond CIFAR100/ResNet-18 to verify generalizability
2. Implement a systematic ablation study varying γ and δ parameters to identify the optimal range for convergence acceleration
3. Measure and compare the full gradient norm reduction rate empirically across all four scheduling schemes to validate the claimed O(γ^(-M/2)) convergence rate for scheme (iii)