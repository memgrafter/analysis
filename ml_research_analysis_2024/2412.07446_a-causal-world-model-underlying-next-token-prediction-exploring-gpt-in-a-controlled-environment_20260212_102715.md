---
ver: rpa2
title: 'A Causal World Model Underlying Next Token Prediction: Exploring GPT in a
  Controlled Environment'
arxiv_id: '2412.07446'
source_url: https://arxiv.org/abs/2412.07446
tags:
- causal
- sequence
- attention
- sequences
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether GPT models, trained only to predict
  the next token, implicitly learn a causal world model from which sequences are generated.
  The authors propose a causal interpretation of the attention mechanism in GPT, relating
  it to structural causal models (SCMs).
---

# A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment

## Quick Facts
- arXiv ID: 2412.07446
- Source URL: https://arxiv.org/abs/2412.07446
- Authors: Raanan Y. Rohekar; Yaniv Gurwicz; Sungduk Yu; Estelle Aflalo; Vasudev Lal
- Reference count: 34
- Primary result: GPT models implicitly learn causal world models that can be leveraged for generating valid sequences in out-of-distribution settings.

## Executive Summary
This paper explores whether GPT models, trained only for next-token prediction, implicitly learn causal world models from which sequences are generated. The authors propose a causal interpretation of the attention mechanism, relating it to structural causal models (SCMs), where the attention matrix acts as a transitive closure of the causal graph. They develop a method for zero-shot causal structure learning from GPT attention matrices using a modified iterative causal discovery (ICD) algorithm and introduce a confidence scoring function based on entropy of p-values from conditional independence tests. Empirical tests on Othello and Chess games show that GPT models are more likely to generate legal next moves for sequences where a causal structure can be learned with high confidence, suggesting implicit learning of causal world models.

## Method Summary
The authors propose that GPT's attention matrices encode causal structures that can be extracted through zero-shot causal discovery. They compute covariance matrices from attention matrices and perform conditional independence tests to learn causal graphs using a modified ICD algorithm. A confidence score R(A) = H_ind - H_dep measures how well the attention matrix captures causal structure based on entropy of p-values from independence and dependence tests. The method is tested on pre-trained GPT models for Othello and Chess, using out-of-distribution test sequences of randomly generated legal moves to evaluate whether structural confidence correlates with legal move generation accuracy.

## Key Results
- Legal move generation accuracy increases with structural confidence score R(A) for both Othello and Chess games
- When GPT generates illegal moves, it fails to capture a causal structure with high confidence
- Zero-shot causal structure learning from attention matrices is feasible and correlates with model performance on out-of-distribution data
- The entropy-based confidence score effectively distinguishes between sequences where causal structure is well-represented versus poorly represented

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT's masked self-attention implicitly encodes causal structures because the attention matrix acts as a transitive closure of the causal graph.
- **Mechanism:** Each element $A(i,j)$ in the attention matrix reflects the cumulative influence of token $j$ on token $i$ through all directed paths in the underlying causal graph. This cumulative effect mirrors $(I-G)^{-1}$ in a linear-Gaussian SCM, where $G$ encodes direct causal edges.
- **Core assumption:** The attention weights approximate the causal influence weights when the model is trained to predict the next token, and the causal Markov property holds.
- **Evidence anchors:**
  - [abstract] "the attention matrix can be viewed as a transitive closure of a causal graph"
  - [section 4.1] "An element $A(i,j)$ in the attention matrix reflects the 'attention' given to token $j$ when computing the embedding of token $i$. This corresponds to the influence that node $j$ has on node $i$ through all directed paths in the causal graph"
- **Break condition:** If the attention matrix is corrupted by noise or if the model relies heavily on positional encodings rather than learned causal dependencies, the transitive closure analogy fails.

### Mechanism 2
- **Claim:** Zero-shot causal structure learning from GPT attention matrices is possible by leveraging conditional independence tests derived from the attention-derived covariance matrix.
- **Mechanism:** The covariance matrix $C = (D^{-1}A)(D^{-1}A)^\top$ is computed from the attention matrix, then partial correlations are extracted to perform conditional independence tests. Removing edges between conditionally independent nodes reveals the causal skeleton and orientations.
- **Core assumption:** The causal Markov property and faithfulness hold, and the attention matrix faithfully captures the true covariance structure.
- **Evidence anchors:**
  - [abstract] "propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences"
  - [section 4.2] "A causal structure for a particular output sequence can be inferred in a zero-shot manner directly from the attention matrix in the last layer"
- **Break condition:** If the model does not encode the true causal structure or if the covariance estimate from attention is inaccurate, the CI tests will not recover the correct graph.

### Mechanism 3
- **Claim:** The confidence score $R(A) = H_{ind} - H_{dep}$ reliably indicates whether a sequence's causal structure is well-represented in the attention matrix.
- **Mechanism:** Entropy of p-values for independence relations ($H_{ind}$) should be higher when the structure is well-captured, while entropy for dependence relations ($H_{dep}$) should be lower. Their difference measures the contrast and thus the confidence.
- **Core assumption:** Under the null hypothesis, p-values are uniformly distributed; therefore, high confidence corresponds to a bimodal distribution of p-values (many near 0 and many near 1).
- **Evidence anchors:**
  - [section 4.3] "We define the following confidence score, given an attention matrix: $R(A) = H_{ind} - H_{dep}$, which captures the contrast between dependence and independence relations"
  - [section 5.3] "From Figure 6, for Othello and Chess, it is evident that the legal move generation accuracy increases with the structural confidence score R"
- **Break condition:** If the p-value distribution does not follow the expected uniform/null behavior, or if the model generates sequences that violate the assumed causal Markov property, the confidence score loses interpretability.

## Foundational Learning

- **Concept:** Structural Causal Models (SCMs) and the causal Markov property
  - Why needed here: The paper's core claim is that GPT attention matrices implicitly encode an SCM; understanding how SCMs represent causal relations is essential to interpreting the mechanism.
  - Quick check question: In an SCM, what does it mean for a variable to be independent of all others except its effects, conditional on its direct causes?

- **Concept:** Conditional independence (CI) testing and faithfulness
  - Why needed here: CI tests are the operational tool used to learn the causal structure from attention-derived covariance; faithfulness determines whether CI relations in the data match the graph structure.
  - Quick check question: If two variables are dependent but become independent when conditioning on a third, what does that imply about the causal structure between them?

- **Concept:** Attention mechanisms in Transformers and masking
  - Why needed here: The triangular masking enforces a causal order in token generation; the mechanism relies on interpreting this masked attention as encoding causal influence.
  - Quick check question: Why does masking the upper triangle of the attention matrix enforce a causal (no-backward-influence) structure in sequence generation?

## Architecture Onboarding

- **Component map:**
  GPT model with masked self-attention -> Attention matrix extraction -> Covariance matrix computation -> CI test engine -> Causal discovery algorithm -> Confidence scoring function -> Legal move generator -> Test harness

- **Critical path:**
  1. Input sequence → GPT forward pass → extract attention matrix $A$
  2. Compute covariance $C$ → run CI tests → learn causal graph $G$
  3. Compute confidence score $R(A)$
  4. Generate next token using GPT
  5. Check if token is legal → aggregate results by $R(A)$

- **Design tradeoffs:**
  - Using only marginal independence (CI conditioning size 0) is fast but loses causal information; full CI tests are slower but more accurate.
  - Pruning low-confidence heads improves accuracy but may reduce diversity in generated sequences.
  - The entropy-based confidence score is unsupervised but may not generalize to domains where the causal Markov property is violated.

- **Failure signatures:**
  - Confidence scores do not correlate with legal move accuracy → suggests attention does not encode causal structure.
  - High variance in $R(A)$ across heads for the same sequence → suggests attention heads capture different (non-causal) patterns.
  - CI tests fail to orient edges correctly → suggests violations of faithfulness or Markov assumptions.

- **First 3 experiments:**
  1. Run causal discovery on a synthetic sequence with known causal structure and verify recovered graph matches ground truth.
  2. Compare legal move accuracy when using full CI tests vs. only marginal tests across multiple sequence lengths.
  3. Perform head pruning based on $R(A)$ and measure the tradeoff between accuracy and diversity of generated sequences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT's implicit causal learning rely on specific architectural features beyond the attention mechanism, such as layer depth or head specialization?
- Basis in paper: [inferred] The paper shows that attention matrices can encode causal structures and that pruning low-confidence heads affects legal move generation, but doesn't test whether architectural modifications (e.g., fewer layers, different head counts) would impair this capability.
- Why unresolved: The experiments use standard GPT models without systematically varying architectural hyperparameters to isolate the role of specific design choices.
- What evidence would resolve it: Ablation studies comparing causal structure learning and legal move generation across models with varying numbers of layers, attention heads, or embedding dimensions.

### Open Question 2
- Question: Can the confidence scoring method based on p-value entropy distinguish between causal structures that are merely correlated versus those that represent true underlying generative mechanisms?
- Basis in paper: [explicit] The authors introduce a confidence score using entropy differences between dependence and independence p-value distributions, but acknowledge it measures compatibility rather than objective likelihood.
- Why unresolved: The confidence metric correlates with legal move generation but doesn't validate whether the learned structures reflect genuine causal relationships versus spurious correlations in the data.
- What evidence would resolve it: Testing whether high-confidence causal structures transfer to novel domains or tasks where surface correlations differ but underlying causal mechanisms remain consistent.

### Open Question 3
- Question: How does GPT's implicit causal learning scale to domains with more complex temporal dependencies or longer-range causal relationships?
- Basis in paper: [inferred] Experiments are limited to Othello and Chess games with relatively short sequences, showing GPT can learn causal structures within these constrained environments.
- Why unresolved: The paper doesn't explore whether the causal interpretation holds for domains with longer sequences, more complex state spaces, or non-Markovian dependencies where attention might struggle to capture long-range effects.
- What evidence would resolve it: Evaluating GPT's causal structure learning and legal move generation on domains with extended temporal horizons or hierarchical causal relationships, such as multi-agent systems or continuous control tasks.

## Limitations

- The causal interpretation of GPT attention as a transitive closure of SCMs lacks direct empirical validation from prior work
- The modified ICD algorithm's implementation details are underspecified, making exact reproduction challenging
- The entropy-based confidence score methodology is novel but untested in other contexts beyond Othello and Chess

## Confidence

- **High confidence**: The empirical finding that legal move accuracy correlates with structural confidence scores R(A)
- **Medium confidence**: The causal interpretation of attention matrices as transitive closures of SCMs
- **Low confidence**: The general applicability of the entropy-based confidence score R(A) beyond the Othello/Chess domains tested

## Next Checks

1. Test the causal discovery algorithm on synthetic sequences with known causal structures (e.g., linear SCMs with Gaussian noise) to verify that the modified ICD algorithm can correctly recover ground truth graphs before applying it to GPT attention matrices.

2. Apply the same methodology to a different sequential domain (e.g., text generation or protein folding sequences) to test whether the causal attention interpretation holds beyond board games.

3. Perform detailed analysis of individual attention heads' contribution to causal structure learning by computing R(A) per head and examining whether pruning low-confidence heads selectively improves performance on specific sequence positions or move types.