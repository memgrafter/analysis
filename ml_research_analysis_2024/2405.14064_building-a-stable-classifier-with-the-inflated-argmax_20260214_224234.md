---
ver: rpa2
title: Building a stable classifier with the inflated argmax
arxiv_id: '2405.14064'
source_url: https://arxiv.org/abs/2405.14064
tags: []
core_contribution: This paper addresses algorithmic stability for multiclass classification,
  where the standard argmax operation leads to unstable predictions. The authors propose
  a novel framework combining bagging with a stable relaxation of argmax called the
  "inflated argmax".
---

# Building a stable classifier with the inflated argmax

## Quick Facts
- arXiv ID: 2405.14064
- Source URL: https://arxiv.org/abs/2405.14064
- Reference count: 3
- Key outcome: Novel framework combining bagging with "inflated argmax" to achieve selection stability in multiclass classification without distributional assumptions

## Executive Summary
This paper addresses the instability problem in multiclass classification where standard argmax operations lead to erratic predictions. The authors propose a two-stage approach: first using bagging (either bootstrapping or subbagging) to produce stable continuous probability scores, then applying a novel "inflated argmax" operator to convert these scores into stable sets of candidate labels. The method provides theoretical guarantees of selection stability without requiring distributional assumptions or depending on the number of classes or covariate dimensionality.

## Method Summary
The method consists of a two-stage pipeline: (1) Bagging with B bags of size m (either m=n for bootstrapping or m=n/2 for subbagging) to create stable probability estimates by averaging base classifier outputs across subsamples, (2) Applying the inflated argmax operator with margin ε to convert stable probability scores into sets of candidate labels. The inflated argmax includes all labels whose scores are within ε of the maximum, ensuring that small perturbations cannot cause the set of candidate labels to change completely. The framework is theoretically justified through tail stability of the bagged classifier and ε-compatibility of the inflated argmax operator.

## Key Results
- Fashion-MNIST experiments demonstrate that inflated argmax provides necessary protection against unstable classifiers without loss of accuracy
- The method outperforms simpler alternatives like fixed-margin selection in returning smaller candidate sets while maintaining stability
- Selection stability guarantees hold for any base classifier and any data distribution without requiring distributional assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bagging stabilizes continuous probability scores by reducing variance across subsamples.
- Mechanism: Bootstrapping or subbagging creates multiple training sets of reduced size. Averaging the base classifier outputs over these sets yields smoother probability estimates, lowering the variance of predictions for any given test point.
- Core assumption: The base classifier's predictions are sufficiently consistent across subsamples that averaging reduces instability without introducing bias.
- Evidence anchors:
  - [abstract]: "using bagging (i.e., resampling and averaging) to produce stable continuous scores"
  - [section]: "we construct a bagged version of the base learning algorithm A... ensures that the resulting bagged algorithm has tail stability"
- Break condition: If the base classifier is highly unstable or produces wildly different predictions across subsamples, averaging may not converge to a stable estimate.

### Mechanism 2
- Claim: The inflated argmax converts stable continuous scores into a set of candidate labels that is guaranteed to overlap across small perturbations.
- Mechanism: The inflated argmax defines a margin around the predicted probabilities; any label whose score is within this margin of the maximum is included in the candidate set. This margin ensures that small perturbations in scores cannot exclude all labels from both perturbed and original sets.
- Core assumption: The margin ε is large enough to tolerate typical prediction score perturbations induced by data resampling.
- Evidence anchors:
  - [abstract]: "using a stable relaxation of argmax, which we call the 'inflated argmax,' to convert these scores to a set of candidate labels"
  - [section]: "argmaxε is ε-compatible... j ∈ argmaxε(v) ∩ argmaxε(w) ≠ ∅ when ∥v − w∥ < ε"
- Break condition: If ε is too small, perturbations can still lead to disjoint sets; if too large, the candidate sets become overly inclusive and lose informativeness.

### Mechanism 3
- Claim: Combining bagging with the inflated argmax yields selection stability with no distributional assumptions.
- Mechanism: Bagging produces stable probability estimates; the inflated argmax transforms them into overlapping candidate sets; by Proposition 4, this combination guarantees that the probability of disjoint sets across leave-one-out samples is bounded.
- Core assumption: Tail stability of the bagged classifier and ε-compatibility of the inflated argmax together ensure the stability bound holds for any base learner and any data distribution.
- Evidence anchors:
  - [abstract]: "The resulting stability guarantee places no distributional assumptions on the data"
  - [section]: "Combining Theorem 7, Theorem 8, and Proposition 4 immediately implies our main result, Theorem 5"
- Break condition: If the bagged classifier fails to achieve tail stability (e.g., due to extreme overfitting or underfitting), the final stability guarantee may not hold.

## Foundational Learning

- Concept: Algorithmic stability in classification
  - Why needed here: The paper builds on classical algorithmic stability to extend the notion to set-valued classification outputs, not just real-valued predictions.
  - Quick check question: What is the difference between tail stability and selection stability, and why is the latter more relevant for multiclass classification?

- Concept: Bagging (bootstrapping and subbagging)
  - Why needed here: Bagging is used to stabilize the base classifier's probability estimates by averaging over perturbed training sets.
  - Quick check question: How does the choice of subsample size m affect the trade-off between stability and accuracy in bagging?

- Concept: Set-valued classification and argmax relaxation
  - Why needed here: Instead of returning a single label, the framework returns a set of candidate labels, requiring a stable mechanism to choose this set.
  - Quick check question: Why is the standard argmax discontinuous, and how does the inflated argmax address this?

## Architecture Onboarding

- Component map: Data → Base Classifier (A) → Bagged Classifier (eAm) → Inflated Argmax (argmaxε) → Candidate Label Set
- Critical path: Subsampling → Training Base Classifier → Averaging → Applying Inflated Argmax
- Design tradeoffs:
  - Smaller subsample size m → higher stability but lower accuracy
  - Larger inflation margin ε → higher stability but less informative candidate sets
  - Finite number of bags B → faster computation but weaker stability guarantees
- Failure signatures:
  - If candidate sets are always singletons but accuracy drops → margin ε too large
  - If stability δ is high but sets are very large → margin ε too small or B too small
  - If training is very slow → too many bags B or too large m
- First 3 experiments:
  1. Run base classifier on full training set and measure selection stability without bagging or inflated argmax.
  2. Apply bagging with B=200, m=n/2, then measure selection stability and set size with standard argmax.
  3. Apply bagging with B=200, m=n/2, ε=0.05, then measure selection stability and set size with inflated argmax; compare to experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal values of the inflation parameter ε for different types of data distributions and base classifiers?
- Basis in paper: [inferred] The paper mentions that the inflated argmax allows stable classification without distributional assumptions, but does not investigate optimal ε values for different scenarios.
- Why unresolved: The paper focuses on proving theoretical properties of the inflated argmax rather than empirical tuning of its parameters.
- What evidence would resolve it: Systematic experiments varying ε across different datasets, classifiers, and data distributions to identify patterns in optimal ε selection.

### Open Question 2
- Question: How does the computational efficiency of the inflated argmax compare to simpler selection rules like fixed-margin selection in high-dimensional classification problems?
- Basis in paper: [explicit] The paper mentions that the inflated argmax is computationally efficient and compares it theoretically to the fixed-margin rule, but does not provide empirical runtime comparisons.
- Why unresolved: The paper proves theoretical properties but does not benchmark actual computation times across different problem sizes.
- What evidence would resolve it: Empirical timing experiments comparing inflated argmax to fixed-margin and other selection rules on datasets with varying numbers of classes and feature dimensions.

### Open Question 3
- Question: Can the stability framework be extended to structured output spaces beyond simple multiclass classification, such as hierarchical labels or sequence predictions?
- Basis in paper: [explicit] The paper mentions in the discussion section that extending the framework to "more structured discrete outputs" is an open problem.
- Why unresolved: The current framework is specifically designed for multiclass classification and does not address more complex output structures.
- What evidence would resolve it: Theoretical extensions of the stability definitions and inflated argmax to structured output spaces, along with empirical validation on hierarchical or sequential prediction tasks.

## Limitations
- Empirical validation limited to Fashion-MNIST dataset with a single base classifier architecture
- Computational cost scales with both number of bags B and number of leave-one-out models needed for stability evaluation
- Choice of inflation parameter ε requires tuning without clear systematic selection criteria

## Confidence
- **High confidence**: The theoretical framework combining bagging with inflated argmax for selection stability is mathematically sound, with proofs provided for the key stability guarantees (Theorems 5, 7, 8).
- **Medium confidence**: The empirical results showing improved stability without accuracy loss on Fashion-MNIST, though limited to a single dataset and classifier.
- **Medium confidence**: The claim that the method works "for any base classifier" is supported theoretically but not thoroughly validated across diverse classifier types in experiments.

## Next Checks
1. **Classifier diversity test**: Validate the method across multiple base classifier architectures (random forests, neural networks with different depths, SVMs) to confirm the "any base classifier" claim holds in practice.
2. **Margin sensitivity analysis**: Systematically vary ε across a range of values and measure the trade-off between stability, precision, and set size to develop principled selection criteria.
3. **Scalability evaluation**: Test the method on larger datasets (ImageNet, CIFAR-100) to assess computational feasibility and whether stability guarantees hold with increased class diversity and data volume.