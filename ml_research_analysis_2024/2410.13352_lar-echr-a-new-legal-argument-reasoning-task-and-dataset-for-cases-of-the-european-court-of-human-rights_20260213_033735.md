---
ver: rpa2
title: 'LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of the
  European Court of Human Rights'
arxiv_id: '2410.13352'
source_url: https://arxiv.org/abs/2410.13352
tags:
- legal
- arguments
- reasoning
- court
- lar-echr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAR-ECHR, a new dataset and task for evaluating
  legal argument reasoning in Large Language Models. The task requires selecting the
  correct next statement in a chain of legal arguments from court proceedings, given
  case facts.
---

# LAR-ECHR: A New Legal Argument Reasoning Task and Dataset for Cases of the European Court of Human Rights

## Quick Facts
- arXiv ID: 2410.13352
- Source URL: https://arxiv.org/abs/2410.13352
- Authors: Odysseas S. Chlapanis; Dimitrios Galanis; Ion Androutsopoulos
- Reference count: 21
- Key outcome: Introduces LAR-ECHR dataset requiring selection of correct next legal argument in court proceedings, showing model rankings align with LegalBench despite different legal systems, with best model achieving only 75.8% accuracy.

## Executive Summary
This paper introduces LAR-ECHR, a novel dataset and task for evaluating legal argument reasoning in Large Language Models. The task requires models to select the correct next statement in a chain of legal arguments from court proceedings, given case facts. Constructed using arguments from the European Court of Human Rights, the dataset contains 403 instances with facts, preceding arguments, and multiple choice continuations. The authors evaluate seven general-purpose LLMs on LAR-ECHR alongside two other benchmarks, demonstrating that LAR-ECHR better distinguishes top models while maintaining alignment with established legal reasoning benchmarks despite being based on EU law rather than US law.

## Method Summary
LAR-ECHR was constructed using arguments from the European Court of Human Rights (ECtHR) cases. Target arguments were selected based on specific criteria: ECHR actor, Application to concrete case type, and first argument per issue. Distractors were algorithmically selected using cosine similarity with a threshold of τ = 0.8 to ensure challenging but not identical alternatives. Seven general-purpose LLMs were evaluated using zero-shot Chain-of-Thought prompting with analysis, explanation, and answer steps. Performance was measured using classification accuracy and compared against LegalBench and MMLU-Law benchmarks.

## Key Results
- Model rankings on LAR-ECHR align with LegalBench rankings despite different legal systems (EU vs US)
- LAR-ECHR better distinguishes top models, with larger performance gaps between models compared to LegalBench
- Even the best model (GPT-4o) achieves only 75.8% accuracy, indicating significant room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LAR-ECHR provides clearer discrimination between top models than LegalBench despite being based on EU law.
- Mechanism: The task requires selecting the correct next legal argument from a chain of judicial reasoning, demanding both legal knowledge and deeper contextual understanding. Human-generated distractors from similar cases create more nuanced distractors than synthetic ones.
- Core assumption: Legal reasoning chains are more difficult to predict than general commonsense reasoning or classification tasks, even when models have seen legal text during pretraining.
- Evidence anchors:
  - [abstract] "LAR-ECHR distinguishes top models more clearly, compared to LegalBench"
  - [section 4.3] "Mistral (L) is the second best model on LAR-ECHR and LegalBench, with a larger performance gap from the best model (6 pp), compared to the corresponding gap on LegalBench (2.5 pp)"
  - [corpus] Weak corpus evidence for this specific mechanism

### Mechanism 2
- Claim: LAR-ECHR rankings align with LegalBench rankings despite different legal systems (EU vs US).
- Mechanism: Legal reasoning requires similar cognitive skills across different legal systems, so models that excel at US legal reasoning also excel at EU legal reasoning. The task structure (predicting next argument in reasoning chain) is universal across legal domains.
- Core assumption: The cognitive processes required for legal reasoning are similar across different legal systems, even though the specific laws differ.
- Evidence anchors:
  - [abstract] "the ranking of the models is aligned with that of LegalBench, an established US-based legal reasoning benchmark, even though LAR-ECHR is based on EU law"
  - [section 4.3] "The rankings (in square brackets) of all models on LAR-ECHR are identical to those of LegalBench"
  - [corpus] Weak - no direct corpus evidence supporting cross-system reasoning alignment

### Mechanism 3
- Claim: Even the best model (GPT-4o) achieves only 75.8% accuracy, indicating significant room for improvement.
- Mechanism: The task is genuinely difficult because it requires understanding the logical flow of legal arguments, not just pattern matching. The combination of complex legal terminology, case-specific facts, and the need to distinguish between semantically similar distractors creates a challenging benchmark.
- Core assumption: Legal argument prediction requires reasoning beyond what current LLMs can achieve, even when fine-tuned on legal data.
- Evidence anchors:
  - [abstract] "even the best model (GPT-4o) obtains 75.8% accuracy on LAR-ECHR, indicating significant potential for further model improvement"
  - [section 4.3] "even the best model (GPT-4o) obtains 75.8% accuracy on LAR-ECHR, similar to the top accuracy on LegalBench (73.3%)"
  - [corpus] Weak - no direct corpus evidence about why 75.8% is significant

## Foundational Learning

- Concept: Legal argument structure and reasoning chains
  - Why needed here: The task requires understanding how judges construct arguments and how each statement logically follows from previous ones
  - Quick check question: Can you explain why the correct answer in Table 1 is (A) rather than the other options, based on the logical flow of arguments?

- Concept: Legal terminology and case law interpretation
  - Why needed here: The dataset contains complex legal terms and references to specific articles and procedures that must be understood
  - Quick check question: What is the significance of Article 8 of the European Convention on Human Rights in the context of this dataset?

- Concept: Embedding similarity and cosine distance metrics
  - Why needed here: The distractor selection algorithm relies on embedding similarity to find challenging but not identical distractors
  - Quick check question: If the cosine similarity threshold τ is set too high, what problem occurs with the distractor selection?

## Architecture Onboarding

- Component map: LAM:ECHR arguments → ECtHR B facts → LAR-ECHR instances → Zero-shot CoT prompting → Accuracy metrics
- Critical path: Data construction → Model evaluation → Analysis of results
  The most critical component is the data construction pipeline, as the quality of LAR-ECHR directly determines the validity of all subsequent evaluations.
- Design tradeoffs:
  - Zero-shot vs few-shot: Zero-shot enables fair comparison across models but may underestimate true capabilities
  - Complete vs summarized facts: Complete facts improve accuracy but limit model applicability; summarized facts enable broader testing but may introduce bias
  - Human vs LLM-generated distractors: Human-generated distractors avoid hallucinations but require more complex selection algorithms
- Failure signatures:
  - Low discrimination between models: Suggests distractors are too easy or too hard
  - Rankings that don't match LegalBench: Indicates potential bias in the EU-specific dataset
  - Extremely high accuracy (>90%): Suggests the task is too easy or data leakage occurred
  - Extremely low accuracy (<50%): Suggests the task is too difficult or distractors are poorly chosen
- First 3 experiments:
  1. Test different cosine similarity thresholds (τ) for distractor selection to optimize difficulty
  2. Compare zero-shot results with few-shot results using the 5 provided samples
  3. Evaluate model performance using complete facts vs summarized facts to measure bias introduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LAR task's effectiveness depend on selecting only the first argument of the judges (per issue) as the target argument, or would later arguments be equally or more challenging to predict?
- Basis in paper: [explicit] The paper states that "in our experience, among the arguments of the judges, the first one (per issue) is the most difficult to predict; we leave an experimental validation of this claim for future work."
- Why unresolved: The authors acknowledge this as an area for future investigation but do not provide empirical evidence to support their claim about the difficulty of predicting the first argument.
- What evidence would resolve it: Experimental results comparing model performance on first versus later judge arguments would clarify whether the selection of the first argument is crucial for task difficulty.

### Open Question 2
- Question: How does the choice of cosine similarity threshold (τ) for selecting distractors impact the difficulty and quality of the LAR-ECHR dataset?
- Basis in paper: [explicit] The paper describes the selection of τ = 0.8 based on manual inspection of a few development examples, noting that "for similarity scores above 0.9, the two texts were almost identical" and that "for lower similarity scores, no such issues were visible."
- Why unresolved: The authors selected τ based on a small sample and subjective judgment. The impact of different threshold values on dataset quality and model performance is not explored.
- What evidence would resolve it: Systematic experiments varying τ across a range of values and measuring resulting model performance and distractor quality would reveal the optimal threshold.

### Open Question 3
- Question: Would using the complete facts instead of summarized facts significantly improve model performance, and would this improvement vary across different model families?
- Basis in paper: [explicit] The paper presents results showing improved performance with complete facts for both GPT and Llama models, noting that "Llama models benefit more than GPT models."
- Why unresolved: While the paper shows differences in performance with complete versus summarized facts, it does not explore why these differences exist or whether they would hold with different summarization techniques or model architectures.
- What evidence would resolve it: Detailed ablation studies comparing various summarization methods and their impact on different model families would clarify the relationship between fact presentation and model performance.

## Limitations
- Cross-system validation limited to 7 models, lacking broader verification across multiple legal domains
- Claims about discrimination power lack external validation with other legal benchmarks
- Interpretation of 75.8% accuracy as "significant room for improvement" lacks human baseline comparison

## Confidence

- **High confidence**: The dataset construction methodology (using ECtHR arguments, specific selection criteria, and embedding-based distractor generation) is well-documented and reproducible.
- **Medium confidence**: Claims about model ranking alignment and discrimination power are supported by current results but require validation across more legal systems and model types.
- **Low confidence**: The interpretation of 75.8% accuracy as indicating "significant potential for improvement" lacks comparative human baseline data and may not generalize to future model generations.

## Next Checks
1. **Cross-system validation**: Test LAR-ECHR with models fine-tuned on non-EU legal datasets to verify whether the EU-to-US ranking alignment holds for domain-adapted models, not just general-purpose LLMs.
2. **Human performance benchmark**: Recruit legal experts to complete a subset of LAR-ECHR tasks to establish a human baseline, enabling proper interpretation of whether 75.8% represents significant room for improvement or approaches human-level performance.
3. **Distractor quality audit**: Manually review 20 randomly selected instances to verify that distractors are neither too similar to target arguments (making the task too hard) nor too dissimilar (making it too easy), and assess whether the cosine similarity threshold τ = 0.8 consistently produces appropriate distractors across all instances.