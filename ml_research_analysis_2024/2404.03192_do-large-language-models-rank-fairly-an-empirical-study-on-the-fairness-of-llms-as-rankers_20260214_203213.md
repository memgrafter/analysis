---
ver: rpa2
title: Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of
  LLMs as Rankers
arxiv_id: '2404.03192'
source_url: https://arxiv.org/abs/2404.03192
tags:
- fairness
- llms
- ranking
- protected
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts an empirical study on the fairness of large
  language models (LLMs) as rankers. It evaluates popular LLMs like GPT-3.5, GPT-4,
  Mistral-7b, and Llama2-13b on the TREC Fair Ranking dataset, focusing on representation
  of binary protected attributes like gender and geographic location.
---

# Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers

## Quick Facts
- arXiv ID: 2404.03192
- Source URL: https://arxiv.org/abs/2404.03192
- Authors: Yuan Wang; Xuyang Wu; Hsin-Tai Wu; Zhiqiang Tao; Yi Fang
- Reference count: 15
- Primary result: Pairwise evaluation reveals deeper biases in LLM rankers favoring certain protected groups

## Executive Summary
This paper conducts an empirical study on the fairness of large language models (LLMs) as rankers, evaluating popular models including GPT-3.5, GPT-4, Mistral-7b, and Llama2-13b on the TREC Fair Ranking dataset. The study employs both listwise and pairwise evaluation methods to assess fairness from user and content perspectives, focusing on binary protected attributes like gender and geographic location. Results reveal that while listwise evaluations suggest relative fairness, pairwise evaluations uncover deeper biases favoring certain protected groups.

## Method Summary
The study evaluates LLM ranking fairness using the TREC Fair Ranking dataset with both listwise and pairwise evaluation methods. Listwise evaluation measures overall ranking quality across protected and unprotected groups, while pairwise evaluation directly compares relevance judgments between protected and unprotected groups. The research employs fairness metrics including fairness ratios to quantify bias, and proposes a mitigation strategy using LoRA fine-tuning to address observed fairness issues in pairwise evaluations.

## Key Results
- Listwise evaluations suggest LLMs exhibit relative fairness in ranking
- Pairwise evaluations reveal deeper biases favoring certain protected groups
- LoRA fine-tuning improves fairness ratios closer to 1.0, indicating more equitable treatment

## Why This Works (Mechanism)
The study's findings emerge from the fundamental differences between listwise and pairwise evaluation approaches. Listwise methods aggregate overall ranking quality, which can mask subgroup-specific biases by averaging across the entire result set. Pairwise comparisons directly expose disparities by examining how protected and unprotected groups are ranked relative to each other. This granular comparison reveals that while LLMs may produce reasonable overall rankings, they systematically favor certain protected groups when evaluated at the pairwise level, suggesting that fairness issues are not apparent in aggregate metrics but become visible through direct comparison.

## Foundational Learning
1. **Listwise vs Pairwise Evaluation**
   - Why needed: To understand different approaches for assessing ranking fairness
   - Quick check: Listwise evaluates overall ranking quality; pairwise compares specific pairs of items

2. **Fairness Metrics and Ratios**
   - Why needed: To quantify and measure bias in ranking systems
   - Quick check: Fairness ratio of 1.0 indicates equal treatment of protected and unprotected groups

3. **LoRA Fine-tuning**
   - Why needed: To understand the mitigation technique used to improve fairness
   - Quick check: LoRA enables efficient fine-tuning by modifying low-rank adaptations of the model

## Architecture Onboarding

**Component Map:**
Data Collection -> Fairness Evaluation (Listwise + Pairwise) -> Bias Detection -> LoRA Fine-tuning -> Re-evaluation

**Critical Path:**
The evaluation pipeline follows: data preprocessing → fairness metric computation → pairwise comparison → bias quantification → mitigation application → post-mitigation assessment

**Design Tradeoffs:**
The study balances evaluation comprehensiveness (using both listwise and pairwise methods) against computational complexity, and chooses binary protected attributes for simplicity while acknowledging this limits real-world applicability.

**Failure Signatures:**
Listwise fairness metrics may hide subgroup biases; pairwise evaluation reveals disparities but may not reflect real-world search behaviors; binary attribute assumptions oversimplify complex fairness scenarios.

**First 3 Experiments:**
1. Run both listwise and pairwise evaluations on a small subset to verify the methodology
2. Test LoRA fine-tuning on a single LLM model to validate the mitigation approach
3. Compare fairness metrics before and after fine-tuning on a controlled dataset

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation constrained to binary protected attributes on a single dataset
- Pairwise evaluation may not fully represent real-world user search behaviors
- LoRA fine-tuning effectiveness lacks evaluation of long-term stability and ranking quality trade-offs

## Confidence

| Claim | Confidence |
|-------|------------|
| Listwise evaluation findings | High |
| Pairwise evaluation findings | Medium |
| LoRA fine-tuning effectiveness | Medium |
| Generalizability to other domains | Low |

## Next Checks

1. Replicate the study using intersectional attributes (e.g., gender AND geographic location combined) to assess compound bias effects

2. Conduct user studies to validate whether improved fairness ratios correlate with perceived fairness in real-world search scenarios

3. Test the LoRA fine-tuning approach across multiple ranking tasks and datasets to evaluate robustness and potential trade-offs with ranking performance