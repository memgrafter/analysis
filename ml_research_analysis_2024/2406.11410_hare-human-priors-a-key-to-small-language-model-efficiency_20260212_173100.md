---
ver: rpa2
title: 'HARE: HumAn pRiors, a key to small language model Efficiency'
arxiv_id: '2406.11410'
source_url: https://arxiv.org/abs/2406.11410
tags:
- data
- training
- datasets
- arxiv
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training efficient small
  language models (SLMs) in resource-constrained environments. The core method idea
  is to incorporate human priors into data construction by creating a training dataset
  that balances semantic diversity and data quality consistency while avoiding benchmark
  data leakage.
---

# HARE: HumAn pRiors, a key to small language model Efficiency

## Quick Facts
- arXiv ID: 2406.11410
- Source URL: https://arxiv.org/abs/2406.11410
- Authors: Lingyun Zhang; Bin jin; Gaojian Ge; Lunhui Liu; Xuewen Shen; Mingyong Wu; Houqian Zhang; Yongneng Jiang; Shiqi Chen; Shi Pu
- Reference count: 16
- Primary result: HARE-1.1B achieves competitive performance against state-of-the-art SLMs while maintaining lower data leakage risk

## Executive Summary
This paper addresses the challenge of training efficient small language models (SLMs) in resource-constrained environments by incorporating human priors into data construction. The authors propose a novel approach that balances semantic diversity and data quality consistency while avoiding benchmark data leakage. Their 1.1B parameter model, HARE-1.1B, is trained on 600 billion tokens using 16 GPUs over 30 days and demonstrates competitive performance against state-of-the-art SLMs on benchmark datasets. The model also shows strong zero-shot capabilities and practical utility in real-world scenarios such as chat and Android API calling applications.

## Method Summary
The authors construct a training dataset by combining heuristic rule-based cleaning of open-source pre-training corpora (RedPajama, Pile, OpenWebMath, etc.), LLM-based data synthesis using Mixtral-8×7B and Qwen1.5-32B, and NLP task data construction in natural language form. They implement strict data decontamination procedures to avoid benchmark data leakage. The HARE-1.1B model uses the Mistral architecture with 1.1B parameters and is trained on 16 Nvidia-H800 GPUs using DeepSpeed and Flash-Attention. The training process involves two stages: 52B tokens on D1 dataset, followed by 600B tokens on the final training dataset (D1, D2, D3).

## Key Results
- HARE-1.1B performs favorably against state-of-the-art SLMs on large-scale benchmark datasets
- The model achieves competitive results while maintaining a lower risk of data leakage compared to models relying heavily on web-scraped data
- HARE-1.1B demonstrates strong zero-shot capabilities and can be fine-tuned for practical applications like chat and Android API calling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating human priors into data construction improves SLM training efficiency by enhancing semantic diversity and data quality consistency.
- Mechanism: Human priors guide the creation of training data that balances semantic diversity and data quality consistency, avoiding benchmark data leakage.
- Core assumption: Human priors can effectively guide data construction to improve model performance without introducing bias or leakage.
- Evidence anchors:
  - [abstract] Human priors play a crucial role in efficiently utilizing data in deep learning.
  - [section 2] This process enhances semantic diversity, reflecting the human prior that diversity is crucial for improving model generalization. Additionally, using LLMs to filter and govern web-scraped data ensures consistent data quality.
  - [corpus] Weak evidence. The corpus mentions related works on SLMs but does not provide direct evidence for this specific mechanism.
- Break condition: If human priors are not carefully selected or if they introduce bias, the model's performance may degrade.

### Mechanism 2
- Claim: Using LLMs to synthesize data based on human-prior-guided prompts enhances semantic diversity while maintaining data quality.
- Mechanism: LLMs generate synthetic data using diverse prompts and topic-specific data, ensuring both semantic diversity and consistent data quality.
- Core assumption: LLMs can effectively generate high-quality synthetic data that aligns with human priors.
- Evidence anchors:
  - [abstract] The success of (Li et al., 2023) and (Ben Allal et al., 2024) can be attributed to incorporating human priors in data construction.
  - [section 2] This approach significantly enhances semantic diversity through diverse topics and prompts, while using the same LLM for data synthesis ensures consistent data quality.
  - [corpus] Weak evidence. The corpus mentions related works on data synthesis but does not provide direct evidence for this specific mechanism.
- Break condition: If the LLM used for data synthesis is not high-quality or if the prompts are not diverse enough, the synthetic data may not effectively enhance semantic diversity.

### Mechanism 3
- Claim: Constructing NLP task data in natural language form enhances the model's ability to solve NLP tasks.
- Mechanism: NLP task data is created using prompts and seed data, allowing the model to learn task-specific solving capabilities.
- Core assumption: NLP task data in natural language form can effectively teach the model to solve NLP tasks.
- Evidence anchors:
  - [abstract] For SLMs to acquire the same capability from limited data, they must rely on the injection of human priors.
  - [section 2] To enhance NLP-task solving capabilities, we construct a substantial amount of NLP-task data in natural language form.
  - [corpus] Weak evidence. The corpus mentions related works on NLP tasks but does not provide direct evidence for this specific mechanism.
- Break condition: If the NLP task data is not diverse or if it does not cover a wide range of tasks, the model's ability to solve NLP tasks may be limited.

## Foundational Learning

- Concept: Data cleaning and preprocessing
  - Why needed here: To ensure high-quality training data and avoid issues like data leakage and noise.
  - Quick check question: What are the key steps in data cleaning and preprocessing for training SLMs?

- Concept: Synthetic data generation
  - Why needed here: To enhance semantic diversity and maintain data quality consistency.
  - Quick check question: How can LLMs be used to generate synthetic data that aligns with human priors?

- Concept: NLP task data construction
  - Why needed here: To enable the model to learn task-specific solving capabilities.
  - Quick check question: What are the best practices for constructing NLP task data in natural language form?

## Architecture Onboarding

- Component map:
  Data construction pipeline -> Model architecture -> Training setup
  Heuristic rule-based cleaning -> Mistral architecture with 1.1B parameters -> DeepSpeed, Flash-Attention, 16 GPUs, 600B tokens
  LLM-based data synthesis -> Reduced parameters for efficiency
  NLP task data construction -> Two-stage training (52B tokens on D1, then 600B tokens on final dataset)

- Critical path:
  1. Clean and preprocess collected data using heuristic rules
  2. Synthesize data using LLMs based on human-prior-guided prompts
  3. Construct NLP task data in natural language form
  4. Decontaminate data to avoid benchmark data leakage
  5. Train the model using the constructed dataset

- Design tradeoffs:
  - Balancing semantic diversity and data quality consistency
  - Ensuring data decontamination to avoid benchmark data leakage
  - Using a reduced parameter model to maintain efficiency

- Failure signatures:
  - Poor model performance due to insufficient semantic diversity or data quality issues
  - Data leakage leading to overfitting on benchmark datasets
  - Inefficient training due to suboptimal data construction

- First 3 experiments:
  1. Evaluate the impact of different data construction methods on model performance
  2. Assess the effectiveness of data decontamination in avoiding benchmark data leakage
  3. Compare the performance of the model trained on the constructed dataset with state-of-the-art SLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of human priors affect the performance of small language models (SLMs) when incorporated into data construction?
- Basis in paper: [explicit] The paper mentions that the work explores the application of human priors in training SLMs but acknowledges that the quality of these priors was not discussed. It also notes that inappropriately selected or biased human priors could lead to suboptimal or misleading model outcomes.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis on how varying qualities of human priors impact SLM performance. This is a critical gap because the effectiveness of the proposed data construction method heavily relies on the quality of the human priors used.
- What evidence would resolve it: Conducting experiments with different qualities of human priors (e.g., high-quality vs. biased or low-quality priors) and measuring their impact on SLM performance would provide insights. This could involve comparing models trained with different sets of human priors and analyzing their performance on benchmark datasets.

### Open Question 2
- Question: What is the qualitative relationship between the number of parameters in SLMs and the amount of human prior knowledge required for optimal performance?
- Basis in paper: [inferred] The paper notes that due to computational resource constraints, the authors were unable to fully explore the constraints between the number of parameters in SLMs and human prior knowledge. This suggests that there might be a relationship between model size and the amount of human priors needed.
- Why unresolved: The paper does not provide a detailed analysis or experimental results that establish how the number of parameters in SLMs correlates with the amount of human prior knowledge required. This is important for understanding how to scale human priors effectively as models grow larger.
- What evidence would resolve it: Conducting a series of experiments with SLMs of varying parameter sizes, each trained with different amounts of human prior knowledge, would help establish this relationship. Analyzing the performance trends across these models could provide insights into the optimal balance between model size and human prior knowledge.

### Open Question 3
- Question: How can the risk of benchmark data leakage be further minimized while maintaining or improving model performance?
- Basis in paper: [explicit] The paper discusses the implementation of a rigorous data decontamination process to avoid benchmark data leakage, including statistical analyses and N-gram overlap checks. However, it also acknowledges that the models compared, such as Phi1.5, Qwen1.5, and Stablelm2, have significant ∆ values, indicating a higher probability of data leakage.
- Why unresolved: While the paper presents a method to reduce data leakage, it does not explore whether further improvements are possible or how these improvements might affect model performance. This is crucial for developing models that are both effective and ethically sound.
- What evidence would resolve it: Experimenting with additional decontamination techniques, such as more advanced N-gram analysis or synthetic data generation methods that avoid overlap with benchmarks, could help. Comparing the performance and leakage risk of models trained with these enhanced methods against the current approach would provide valuable insights.

### Open Question 4
- Question: How can the data construction method be adapted to incorporate human priors into the design of network architecture, loss functions, and regularization techniques for SLMs?
- Basis in paper: [inferred] The paper mentions that the authors hope this work will inspire further exploration in incorporating human priors through the design of network architecture, loss functions, and regularization to enhance SLM training efficiency, not just through efficient data utilization.
- Why unresolved: The paper focuses on incorporating human priors into data construction but does not explore how these priors can be integrated into other aspects of model design. This is an important area for further research as it could lead to more holistic improvements in SLM training.
- What evidence would resolve it: Conducting research to develop and test new network architectures, loss functions, and regularization techniques that explicitly incorporate human priors would be necessary. Evaluating the performance of models using these novel approaches against traditional methods would provide insights into their effectiveness.

## Limitations

- The paper lacks detailed specification for heuristic data cleaning rules and exact prompts used for LLM-based data synthesis, hindering faithful reproduction.
- Claims about achieving competitive results and maintaining lower data leakage risk are not substantiated with comparative analysis or detailed validation procedures.
- The paper does not discuss potential biases that could be introduced by human priors or limitations in handling diverse linguistic contexts.

## Confidence

- **High Confidence**: The general framework of incorporating human priors into data construction for SLM training.
- **Medium Confidence**: The specific mechanisms of using LLM-based data synthesis and NLP task data construction in natural language form.
- **Low Confidence**: Claims about achieving competitive results against state-of-the-art SLMs and maintaining lower data leakage risk.

## Next Checks

1. Implement the data construction pipeline using specified heuristic rules and LLM-based data synthesis to create a training dataset, then evaluate its semantic diversity and data quality consistency through quantitative metrics and qualitative analysis.

2. Conduct comprehensive statistical analyses and n-gram overlap calculations on the constructed dataset to detect potential data leakage, and compare the results with other SLM training methods that claim to address data leakage.

3. Train the HARE-1.1B model on the constructed dataset and evaluate its performance on a diverse set of benchmark datasets, comparing the results with state-of-the-art SLMs to validate the claims of competitive performance and practical utility.