---
ver: rpa2
title: Can Language Models Pretend Solvers? Logic Code Simulation with LLMs
arxiv_id: '2403.16097'
source_url: https://arxiv.org/abs/2403.16097
tags:
- logic
- llms
- code
- language
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces logic code simulation, a novel task for evaluating
  LLMs' ability to directly predict the outcomes of logical programs. The authors
  curate three datasets from solver communities and conduct extensive experiments
  using various LLMs and prompting techniques.
---

# Can Language Models Pretend Solvers? Logic Code Simulation with LLMs

## Quick Facts
- arXiv ID: 2403.16097
- Source URL: https://arxiv.org/abs/2403.16097
- Authors: Minyu Chen; Guoqiang Li; Ling-I Wu; Ruibang Liu; Yuxin Su; Xi Chang; Jianxin Xue
- Reference count: 40
- Key outcome: Introduces logic code simulation task and Dual Chains of Logic (DCoL) prompting method, achieving 7.06% accuracy improvement with GPT-4-Turbo

## Executive Summary
This paper introduces logic code simulation as a novel task for evaluating Large Language Models' (LLMs) ability to directly predict the outcomes of logical programs without external solvers. The authors curate three datasets from solver communities and conduct extensive experiments using various LLMs and prompting techniques. They propose a new prompting method, Dual Chains of Logic (DCoL), which encourages LLMs to consider both SAT and UNSAT hypotheses when reasoning about logic codes. Experiments show that GPT-4-Turbo with DCoL achieves a 7.06% improvement in accuracy compared to other prompting strategies. The study also investigates the strengths and pitfalls of LLM-based logic code simulation, finding that LLMs can simulate generated logic codes, are robust to syntax errors, and can leverage external knowledge for reasoning.

## Method Summary
The authors introduce logic code simulation as a task where LLMs directly predict the outcomes of logical programs written in Z3Py and SMT-LIB formats. They curate three datasets (Z3Tutorial, Z3Test, SMTSim) containing logic codes from solver communities. The study evaluates multiple LLM architectures (GPT-3.5 Turbo, GPT-4 Turbo, LLaMA-2-13B, Code LLaMA) using zero-shot prompting with baseline methods (Standard, Chain-of-Thought, Plan-and-Solve, Chain of Simulation) and their proposed Dual Chains of Logic (DCoL) method. DCoL forces the LLM to consider both SAT and UNSAT hypotheses separately before merging them. Performance is measured using accuracy metrics for predicting SAT/UNSAT outcomes.

## Key Results
- GPT-4-Turbo with DCoL achieves 7.06% higher accuracy compared to other prompting strategies
- LLMs demonstrate robustness to syntax errors in logic code, with minimal performance degradation
- Performance varies significantly across datasets, with the most complex dataset (SMTSim) showing the greatest room for improvement
- DCoL prompting consistently outperforms baseline methods across different LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can directly simulate the execution of logic codes without external solvers.
- Mechanism: By interpreting logic code and applying reasoning paths internally, LLMs emulate the behavior of symbolic solvers, predicting SAT/UNSAT outcomes.
- Core assumption: The LLM's internal reasoning capacity is sufficient to follow the logical flow of the code and derive the correct outcome.
- Evidence anchors:
  - [abstract] "This study delves into a novel aspect, namely logic code simulation, which forces LLMs to emulate logical solvers in predicting the results of logical programs."
  - [section] "Experiments show that GPT-4-Turbo with DCoL achieves a 7.06% improvement in accuracy compared to other prompting strategies."
  - [corpus] Weak - related papers focus on neuro-symbolic approaches but do not provide direct experimental evidence for code simulation without external solvers.
- Break condition: The logical code contains constructs or theories beyond the LLM's training distribution, or the reasoning chains become too long or complex for the model to follow.

### Mechanism 2
- Claim: Dual Chains of Logic (DCoL) prompting significantly improves LLM performance in logic code simulation.
- Mechanism: DCoL forces the LLM to consider both SAT and UNSAT hypotheses separately before merging them, reducing reasoning errors caused by incomplete exploration of the solution space.
- Core assumption: LLMs benefit from explicit exploration of both possible outcomes, leading to more robust reasoning and fewer errors in complex logic problems.
- Evidence anchors:
  - [abstract] "DCoL... has demonstrated state-of-the-art performance compared to other LLM prompt strategies, achieving a notable improvement in accuracy by 7.06% with GPT-4-Turbo."
  - [section] "DCoL... steers clear of falling into the erroneous reasoning trap... DCoL consistently provides a precise reasoning approach for obtaining accurate outcomes by thinking on the dual side of logic problems."
  - [corpus] Weak - no direct evidence in related works about dual-path prompting specifically for logic code simulation.
- Break condition: The LLM fails to follow the DCoL instructions correctly, or the SAT/UNSAT hypotheses are not clearly separable, leading to confusion rather than improved reasoning.

### Mechanism 3
- Claim: LLMs are robust to syntax errors in logic code, making them effective simulators even with imperfect inputs.
- Mechanism: Since LLMs predict outcomes without actually executing the code, they are less sensitive to minor syntax errors that would cause traditional solvers to fail.
- Core assumption: The LLM's understanding of the code's intent is not significantly impacted by small syntax errors, allowing it to still derive the correct logical outcome.
- Evidence anchors:
  - [section] "LLMs are robust simulators... Syntax errors had minimal influence on the effectiveness of our method, which demonstrates LLMs are robust simulators."
  - [section] "In contrast, syntax errors had minimal influence on the effectiveness of our method, which demonstrates LLMs are robust simulators."
  - [corpus] Weak - no direct evidence in related works about LLM robustness to syntax errors in the context of logic code simulation.
- Break condition: The syntax errors are severe enough to completely obscure the code's meaning or introduce ambiguity that the LLM cannot resolve.

## Foundational Learning

- Concept: Logic Solvers (SAT/UNSAT)
  - Why needed here: Understanding how logic solvers work is crucial for comprehending the logic code simulation task and evaluating LLM performance.
  - Quick check question: What are the possible outputs of a typical logic solver, and what do they mean?

- Concept: Prompt Engineering
  - Why needed here: Different prompting strategies, like DCoL, significantly impact LLM performance in logic code simulation.
  - Quick check question: How does the DCoL prompting strategy differ from standard Chain-of-Thought prompting?

- Concept: Transformer-based LLMs
  - Why needed here: The capabilities and limitations of LLMs, particularly their reasoning and code understanding abilities, are central to this research.
  - Quick check question: What are the key architectural features of transformer-based LLMs that enable them to perform tasks like code simulation?

## Architecture Onboarding

- Component map: LLM (e.g., GPT-4-Turbo) -> Logic Code Input -> Prompt Strategy (e.g., DCoL) -> Reasoning Process -> SAT/UNSAT Prediction Output

- Critical path: The LLM receives the logic code and prompt, interprets the code, applies reasoning paths guided by the prompt, and generates a final prediction of the code's outcome.

- Design tradeoffs: Using LLMs as simulators instead of external solvers trades off perfect accuracy for increased flexibility and robustness to syntax errors. The choice of prompting strategy involves balancing the complexity of the prompt with the potential for improved reasoning.

- Failure signatures: Incorrect predictions, unknown outputs, or failure to follow the prompting instructions are key indicators of problems in the system.

- First 3 experiments:
  1. Evaluate the baseline performance of different LLMs on a simple logic code dataset using standard prompting.
  2. Test the effectiveness of DCoL prompting compared to other strategies on the same dataset.
  3. Assess the robustness of LLMs to syntax errors by introducing controlled errors into the logic code and observing the impact on performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures compare in their ability to simulate logic codes, and what architectural features are most critical for this task?
- Basis in paper: [explicit] The paper evaluates multiple LLM architectures (GPT-3.5 Turbo, GPT-4 Turbo, LLaMA-2-13B, and Code LLaMA) on logic code simulation tasks.
- Why unresolved: While the paper compares these models, it doesn't perform a systematic analysis of which architectural features (e.g., parameter count, attention mechanisms, training data composition) are most critical for logic code simulation performance.
- What evidence would resolve it: A comprehensive ablation study comparing models with different architectural features (e.g., varying parameter counts, attention mechanisms, training objectives) on the same logic code simulation tasks.

### Open Question 2
- Question: How does the performance of LLM-based logic code simulation scale with problem complexity, and at what point do LLMs begin to fail on increasingly complex logical programs?
- Basis in paper: [explicit] The paper introduces the SMTSim dataset containing complex logic codes and reports performance drops on this dataset.
- Why unresolved: The paper provides initial insights but doesn't systematically explore the relationship between problem complexity (e.g., number of variables, constraints, logical theories) and LLM performance. The exact failure points and scaling behavior remain unclear.
- What evidence would resolve it: A detailed analysis correlating LLM performance on logic code simulation tasks with various complexity metrics (e.g., number of variables, constraints, logical theories) and identifying specific complexity thresholds where performance degrades significantly.

### Open Question 3
- Question: Can LLM-based logic code simulation be effectively combined with traditional solvers to create hybrid systems that leverage the strengths of both approaches?
- Basis in paper: [inferred] The paper discusses the potential of LLMs to handle theories beyond traditional solvers' capabilities and their robustness to syntax errors, suggesting possible synergies with traditional solvers.
- Why unresolved: While the paper hints at potential synergies, it doesn't explore or evaluate hybrid approaches that combine LLM-based simulation with traditional solvers.
- What evidence would resolve it: Experiments comparing hybrid systems (e.g., using LLMs for initial problem analysis and traditional solvers for detailed execution) against pure LLM-based or pure solver-based approaches on a variety of logic code simulation tasks.

## Limitations

- The paper lacks a comparison with actual logic solvers as a gold standard baseline for evaluating LLM performance
- The robustness to syntax errors claim is not fully validated across different types and severities of syntax errors
- The 7.06% accuracy improvement figure lacks context regarding the baseline performance and whether it approaches solver-level accuracy

## Confidence

- Central claim (LLMs can simulate logic code execution without external solvers): Low confidence
- DCoL prompting effectiveness: Medium confidence
- Robustness to syntax errors: Medium confidence

## Next Checks

1. Implement the same logic code simulation task using actual Z3/SMT solvers as a gold standard to establish what percentage of problems can be solved perfectly, providing context for LLM performance.

2. Systematically vary syntax error types (typos, missing parentheses, wrong operators) and severity to map the boundary where LLM performance degrades, testing the claimed robustness claim.

3. Test whether models trained on Z3Tutorial can generalize to unseen problem types in SMTSim, or if performance improvements are dataset-specific artifacts of the DCoL approach.