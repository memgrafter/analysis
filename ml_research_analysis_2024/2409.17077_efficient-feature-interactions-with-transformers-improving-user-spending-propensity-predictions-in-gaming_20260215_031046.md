---
ver: rpa2
title: 'Efficient Feature Interactions with Transformers: Improving User Spending
  Propensity Predictions in Gaming'
arxiv_id: '2409.17077'
source_url: https://arxiv.org/abs/2409.17077
tags:
- data
- features
- tabular
- transformer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of predicting user spending propensity
  in a fantasy sports gaming platform. The authors propose a novel deep learning architecture
  called Proximity-Aware Contextual Transformer, designed specifically for tabular
  data.
---

# Efficient Feature Interactions with Transformers: Improving User Spending Propensity Predictions in Gaming

## Quick Facts
- arXiv ID: 2409.17077
- Source URL: https://arxiv.org/abs/2409.17077
- Authors: Ved Prakash; Kartavya Kothari
- Reference count: 18
- Primary result: Proximity-Aware Contextual Transformer achieves MAE of 37.13 and MSE of 351.01, improving MAE by 2.5% and MSE by 21.8% over FT Transformer

## Executive Summary
This paper addresses the challenge of predicting user spending propensity in fantasy sports gaming platforms, where large tabular datasets contain hundreds of heterogeneous features. The authors propose a novel deep learning architecture called Proximity-Aware Contextual Transformer that incorporates proximity-aware contextual relationships between input features. The model demonstrates superior performance compared to existing approaches including MLP, ResNet, TabTransformer, FT Transformer, XGBoost, and CatBoost on a large-scale dataset with millions of rows.

The key innovation lies in transforming proximity-aware contextual relationships into embeddings that capture feature interactions, then applying transformer layers to these enriched representations. The proposed architecture shows significant improvements in prediction accuracy (2.5% MAE reduction, 21.8% MSE reduction) while demonstrating robustness to uninformative features and faster convergence with smaller datasets compared to state-of-the-art approaches.

## Method Summary
The Proximity-Aware Contextual Transformer is a deep learning architecture specifically designed for tabular data prediction tasks. The model consists of three main components: a feature tokenizer that converts raw features (categorical and numerical) into embeddings, proximity-aware contextual embeddings that capture relationships between features including statistical correlations, temporal patterns, and contextual proximity, and a transformer stack that processes these enriched embeddings through multiple attention layers. The architecture is trained using hyperparameter optimization with Optuna and evaluated using Mean Absolute Error (MAE) and Mean Squared Error (MSE) metrics. The model is tested on a large fantasy sports gaming dataset with millions of rows and hundreds of features, demonstrating superior performance compared to existing approaches.

## Key Results
- Proximity-Aware Contextual Transformer achieves MAE of 37.13 and MSE of 351.01 on test data
- Outperforms FT Transformer by 2.5% MAE reduction and 21.8% MSE reduction
- Demonstrates faster convergence with smaller datasets compared to FT Transformer
- Shows robustness to uninformative features common in large tabular datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Proximity-Aware Contextual Transformer improves generalization by embedding statistical correlations, temporal patterns, and contextual proximity relationships between input features.
- Mechanism: The model transforms proximity-aware contextual relationships into embeddings that capture feature interactions, then applies transformer layers to these enriched embeddings.
- Core assumption: Feature relationships (statistical, temporal, contextual) contain predictive signal that standard feature processing misses.
- Evidence anchors:
  - [abstract] "incorporating proximity-aware contextual relationships between input features"
  - [section] "Our adaptation is motivated by the fact that the existing deep learning architectures are not effective in the presence of a large number of features"
- Break condition: If feature relationships are random or uncorrelated with target, proximity embeddings add noise rather than signal.

### Mechanism 2
- Claim: The architecture mitigates the impact of non-rotationally-invariant and uninformative features common in tabular data.
- Mechanism: By breaking rotational invariance through embedding layers and filtering irrelevant information early in processing, the model maintains high performance even with uninformative features.
- Core assumption: Standard MLPs and transformers degrade with uninformative features, but this architecture's design prevents that degradation.
- Evidence anchors:
  - [section] "tabular data features are typically non-rotationally-invariant... When a model incorporates these useless feature interactions, it can harm network generalization"
  - [section] "neural networks, including traditional MLPs and some transformer models, often struggle with uninformative features"
- Break condition: If most features are uninformative or if feature selection is already optimal, the architecture provides diminishing returns.

### Mechanism 3
- Claim: The model achieves faster convergence and better performance with smaller datasets compared to FT Transformer by efficiently capturing feature relationships.
- Mechanism: Proximity-aware contextual embeddings provide richer feature representations that allow the transformer to learn more effectively from limited data.
- Core assumption: The additional structure from proximity-aware embeddings provides a learning advantage that compounds with dataset size.
- Evidence anchors:
  - [section] "model is able to converge faster than FT Transformer and attain the best performance of FT Transformer with smaller data size"
  - [section] "Performance does improve with the increase in the dataset size"
- Break condition: If dataset is extremely small (underfitting regime) or if features lack meaningful proximity relationships.

## Foundational Learning

- Concept: Feature embedding techniques for categorical and numerical data
  - Why needed here: The model transforms raw features into embeddings before applying transformer layers, which is critical for handling heterogeneous tabular data
  - Quick check question: How does the embedding size scale with the number of unique categorical values?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The model applies stacked transformer layers over feature embeddings to capture complex interactions
  - Quick check question: What is the difference between self-attention and intersample attention in this context?

- Concept: Proximity-aware contextual relationships in tabular data
  - Why needed here: The core innovation relies on understanding how features relate to each other temporally and contextually
  - Quick check question: How would you define "proximity" between gaming rounds in a fantasy sports context?

## Architecture Onboarding

- Component map: Feature → Tokenizer → Proximity Embeddings → Transformer → Prediction
- Critical path: Raw features are tokenized, enriched with proximity-aware contextual embeddings, processed through transformer layers, and output spending propensity predictions
- Design tradeoffs: More complex feature engineering vs. improved performance; increased model size vs. better generalization
- Failure signatures: Poor performance on datasets with mostly uninformative features; overfitting with limited data
- First 3 experiments:
  1. Compare MAE/MSE on a small validation set with and without proximity embeddings
  2. Test different embedding sizes for categorical features
  3. Evaluate performance on datasets with varying proportions of uninformative features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Proximity-Aware Contextual Transformer architecture perform on datasets with a high proportion of uninformative features compared to traditional MLPs and FT Transformers?
- Basis in paper: [explicit] The paper discusses that the model demonstrates robustness against uninformative features, but does not provide empirical evidence comparing performance degradation rates across different models.
- Why unresolved: The paper mentions theoretical advantages but does not conduct controlled experiments varying the proportion of uninformative features to quantify performance differences.
- What evidence would resolve it: Controlled experiments where uninformative features are systematically added to datasets and performance metrics (MAE, MSE) are tracked across models to measure degradation rates.

### Open Question 2
- Question: What is the computational overhead of training the Proximity-Aware Contextual Transformer compared to FT Transformer and GBDT models on large-scale datasets?
- Basis in paper: [inferred] While the paper compares predictive performance, it does not discuss training time, memory usage, or inference efficiency differences between the proposed model and baselines.
- Why unresolved: The paper focuses on predictive accuracy but omits practical deployment considerations that would influence model selection in real-world applications.
- What evidence would resolve it: Comprehensive benchmarking of training times, memory requirements, and inference speeds across different dataset sizes and hardware configurations.

### Open Question 3
- Question: How sensitive is the Proximity-Aware Contextual Transformer to hyperparameter choices compared to other deep learning models for tabular data?
- Basis in paper: [explicit] The paper mentions using Optuna for hyperparameter optimization but does not analyze sensitivity or provide insights into how different hyperparameters affect model performance.
- Why unresolved: The paper presents optimized results but does not explore the stability of these results across different hyperparameter configurations or initialization seeds.
- What evidence would resolve it: Sensitivity analysis showing performance variance across different hyperparameter ranges, including ablation studies on key architectural choices like embedding sizes and transformer layer configurations.

## Limitations
- The exact implementation details of proximity-aware contextual embeddings are not fully specified, making independent validation challenging
- Performance benefits appear dataset-specific, showing advantages with large-scale fantasy sports data containing temporal relationships
- The paper does not provide computational overhead comparisons with baseline models for practical deployment considerations

## Confidence

- High confidence: Experimental results showing superior performance over baseline models (MAE improvement of 2.5% and MSE improvement of 21.8% compared to FT Transformer) are well-documented with proper statistical validation using 10 random seeds
- Medium confidence: Architectural claims about proximity-aware contextual embeddings are supported by results but lack sufficient implementation detail for full verification
- Medium confidence: Claim about faster convergence with smaller datasets is supported by results but would benefit from more detailed convergence analysis

## Next Checks
1. Replicate the model architecture with a simplified version of proximity-aware contextual embeddings (e.g., using basic temporal lags) to verify if performance gains persist without full proximity-aware implementation
2. Conduct ablation studies removing different components of the architecture (transformer layers, proximity embeddings, feature tokenizer) to quantify contribution of each to overall performance improvement
3. Test the model on a different tabular dataset with similar characteristics (large-scale, temporal relationships) but different domain to assess generalizability beyond fantasy sports context