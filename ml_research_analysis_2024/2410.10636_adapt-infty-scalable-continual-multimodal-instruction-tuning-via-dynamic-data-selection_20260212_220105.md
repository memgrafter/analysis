---
ver: rpa2
title: 'Adapt-$\infty$: Scalable Continual Multimodal Instruction Tuning via Dynamic
  Data Selection'
arxiv_id: '2410.10636'
source_url: https://arxiv.org/abs/2410.10636
tags:
- data
- datasets
- samples
- adapt
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses lifelong multimodal instruction tuning, where\
  \ a pre-trained multimodal model must continually adapt to a growing sequence of\
  \ diverse instruction datasets. It proposes Adapt-\u221E, a dynamic data selection\
  \ method that clusters samples by gradient similarity into pseudo-task clusters,\
  \ then selects the most influential samples per cluster using a multi-way scoring\
  \ approach."
---

# Adapt-$\infty$: Scalable Continual Multimodal Instruction Tuning via Dynamic Data Selection

## Quick Facts
- arXiv ID: 2410.10636
- Source URL: https://arxiv.org/abs/2410.10636
- Reference count: 40
- One-line primary result: Achieves >100% relative gains over sequential training with 0.9% catastrophic forgetting on multimodal instruction tuning

## Executive Summary
This paper addresses lifelong multimodal instruction tuning, where a pre-trained multimodal model must continually adapt to a growing sequence of diverse instruction datasets. It proposes Adapt-∞, a dynamic data selection method that clusters samples by gradient similarity into pseudo-task clusters, then selects the most influential samples per cluster using a multi-way scoring approach. The method also includes permanent pruning of redundant samples to control computational cost. Experiments on a sequence of five visual instruction datasets show that Adapt-∞ achieves >100% relative gains over sequential training, reduces catastrophic forgetting to 0.9%, and outperforms strong baselines like random replay, EL2N, entropy, and recent pruning methods.

## Method Summary
Adapt-∞ implements a three-stage pipeline for lifelong multimodal instruction tuning. First, it constructs pseudo-skill clusters by computing gradient vectors from the model and applying k-means clustering to group similar samples. Second, it employs a multi-way data selection approach that evaluates multiple scoring functions (perplexity, image grounding, EL2N, entropy) on each cluster and selects the one with highest entropy distribution for balanced sampling. Third, it applies permanent cluster-wise data pruning to remove redundant samples based on cosine similarity in the hidden state space, maintaining computational efficiency as the dataset grows. The method is trained sequentially on five multimodal instruction datasets while minimizing catastrophic forgetting.

## Key Results
- Achieves >100% relative gain in performance compared to sequential training baseline
- Reduces catastrophic forgetting to just 0.9% across all tasks
- Outperforms strong baselines including random replay, EL2N, entropy-based methods, and recent pruning approaches
- Particularly excels at retaining rare tasks like multilingual multimodal skills which typically suffer severe forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic pseudo-skill clustering using gradient vectors prevents task imbalance during lifelong instruction tuning.
- Mechanism: Gradient vectors capture skill-specific information more effectively than hidden states, enabling better separation of tasks in the data pool. Adapt-∞ uses k-means clustering on concatenated gradient vectors across layers to form pseudo-task clusters, then selects the best scoring function per cluster based on entropy maximization.
- Core assumption: Skill-specific information is more prominent in gradient vectors than in hidden state representations.
- Evidence anchors:
  - [abstract] "We first construct pseudo-skill clusters by grouping gradient-based sample vectors."
  - [section] "As shown in Figure 2B and Figure 4, we find that gradient vectors are significantly more separable by skills than hidden state outputs of the model."
  - [corpus] "Found 25 related papers...Top related titles: MLLM-CBench...SwitchCIT...Federated Continual Instruction Tuning." (corpus provides weak direct support for this specific mechanism)

### Mechanism 2
- Claim: Multi-way data selection using entropy-based scoring function selection balances task representation across diverse skills.
- Mechanism: Adapt-∞ evaluates multiple scoring functions (perplexity, image grounding, EL2N, entropy) on each pseudo-task cluster and selects the one with highest entropy distribution, indicating better discrimination across the full score range. This ensures balanced representation of easy, difficult, and ambiguous samples.
- Core assumption: Higher entropy in scoring function distribution indicates better ability to assess sample uncertainty and maintain balanced task representation.
- Evidence anchors:
  - [abstract] "we propose a new multi-way data selection approach that chooses the best scoring function from a pool of experts based on its discriminativeness (measured by entropy)."
  - [section] "Since the resulting pseudo-task clusters may vary in size, we define a training budget T and divide it uniformly over the k clusters."
  - [corpus] "MLLM-CBench...a comprehensive benchmark for continual instruction tuning" (corpus provides context but not direct evidence for this specific entropy-based mechanism)

### Mechanism 3
- Claim: Permanent cluster-wise data pruning maintains computational efficiency while preserving task diversity in growing dataset pools.
- Mechanism: After each training step, Adapt-∞ computes pairwise cosine similarities within each cluster using hidden layer outputs and removes most redundant samples, keeping computational requirements manageable while maintaining diverse representation.
- Core assumption: Hidden layer outputs effectively represent semantic similarity for redundancy detection, and removing highly similar samples doesn't eliminate critical task information.
- Evidence anchors:
  - [abstract] "To prevent excessive computation as the data pool grows, Adapt-∞ adds permanent data pruning that removes semantically redundant samples from the data pool at the end of each time step."
  - [section] "The similarity is computed in the semantic space, which is well represented using hidden layer outputs of the model as shown in Figure 4."
  - [corpus] "Dynamic Prompt Allocation and Tuning for Continual Test-Time Adaptation" (corpus provides related context but not direct evidence for this specific pruning mechanism)

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Adapt-∞ explicitly addresses catastrophic forgetting, which is a central problem in lifelong multimodal instruction tuning where models forget previously learned skills when trained on new datasets
  - Quick check question: What is the forgetting rate for sequential training in the experiments, and how does Adapt-∞ reduce it?

- Concept: Gradient-based data representation for task separation
  - Why needed here: The method relies on gradient vectors to create pseudo-task clusters, requiring understanding of how gradients capture task-specific information
  - Quick check question: According to the paper, which layer's gradients work best for clustering, and why might this be the case?

- Concept: Entropy as a measure of score function discriminativeness
  - Why needed here: The multi-way selection mechanism uses entropy to choose the best scoring function per cluster, requiring understanding of entropy's relationship to uncertainty and discrimination
  - Quick check question: How does the paper define the "best" scoring function for each cluster, and what property of the score distribution determines this?

## Architecture Onboarding

- Component map:
  - Input: Sequence of multimodal instruction datasets with image-text pairs
  - Gradient extraction module: Computes gradients from model layers for sample representation
  - Pseudo-task clustering: k-means clustering on gradient vectors to form skill clusters
  - Multi-way scoring: Evaluates multiple scoring functions, selects best per cluster using entropy
  - Data selection: Samples from clusters using selected scoring functions
  - Training module: Fine-tunes model on selected subset
  - Pruning module: Removes redundant samples using cosine similarity on hidden states
  - Output: Model with reduced forgetting and improved forward transfer

- Critical path: Dataset → Gradient extraction → Clustering → Scoring selection → Data selection → Training → Pruning → Next dataset

- Design tradeoffs:
  - Memory vs. accuracy: Using gradients from all layers provides better separation but increases memory requirements; middle layer gradients offer good balance
  - Computational cost vs. diversity: More aggressive pruning reduces computation but may harm task diversity; LITE-Adapt-∞ offers configurable pruning levels
  - Static vs. dynamic selection: Single scoring function is computationally cheaper but less effective than multi-way selection

- Failure signatures:
  - High forgetting rate (>10%) indicates poor skill retention, possibly due to ineffective clustering or pruning
  - Task imbalance in selected subsets suggests scoring functions aren't discriminating well or clusters aren't meaningful
  - Computational bottleneck during gradient extraction suggests need for dimensionality reduction or layer selection

- First 3 experiments:
  1. Verify gradient-based clustering separates tasks: Extract gradients from middle layer, perform t-SNE visualization, check if different skills form distinct clusters
  2. Test entropy-based scoring function selection: Apply multiple scoring functions to a cluster, compute entropy distributions, verify the highest entropy scorer provides most balanced sample selection
  3. Evaluate pruning effectiveness: After training, compute cosine similarity in hidden state space, verify redundant samples are correctly identified and removed while maintaining task diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which gradient-based pseudo-task clustering in Adapt-∞ leads to superior skill retention compared to semantic embeddings or hidden layer outputs?
- Basis in paper: [explicit] The paper demonstrates that gradient vectors are more separable by skills than hidden state outputs, and that using gradients from the middle layer yields the best performance. It also shows that Adapt-∞ outperforms embedding-based selection methods like SemDeDup and DBP.
- Why unresolved: While the paper provides empirical evidence of gradient superiority, it does not fully explain the underlying reasons for this phenomenon. The authors hypothesize that gradients represent skills while hidden layer outputs represent semantics, but a deeper theoretical understanding is lacking.
- What evidence would resolve it: Further theoretical analysis and ablation studies isolating the impact of different layers and gradient types (e.g., zero-order vs. backpropagated) would clarify the precise mechanism.

### Open Question 2
- Question: How does the choice of scoring functions within the multi-way data selection framework impact the balance between task diversity and difficulty in lifelong multimodal instruction tuning?
- Basis in paper: [explicit] The paper proposes a multi-way data selection approach that chooses the best scoring function from a pool of experts based on entropy. It demonstrates that different scoring functions emphasize different data quality attributes and that the combination leads to superior performance compared to single-score methods.
- Why unresolved: The paper does not exhaustively explore the impact of different scoring function combinations on task balance and learning efficiency. The choice of scoring functions could significantly impact the model's ability to learn both common and rare tasks.
- What evidence would resolve it: Systematic ablation studies varying the scoring function pool and analyzing their impact on task-specific performance and overall accuracy would provide insights into optimal scoring function combinations.

### Open Question 3
- Question: What are the long-term implications of using Adapt-∞ for lifelong multimodal instruction tuning on the model's ability to generalize to unseen tasks and adapt to evolving data distributions?
- Basis in paper: [inferred] The paper demonstrates that Adapt-∞ minimizes catastrophic forgetting and promotes forward transfer, but does not explicitly investigate its impact on generalization to unseen tasks or adaptability to changing data distributions over extended periods.
- Why unresolved: While Adapt-∞ shows promising results in the short term, its long-term impact on the model's robustness and adaptability remains unclear. Continuous training on a growing dataset pool could lead to overfitting or bias towards previously encountered tasks.
- What evidence would resolve it: Long-term experiments evaluating the model's performance on unseen tasks and its ability to adapt to new data distributions after extended training with Adapt-∞ would provide insights into its long-term effectiveness.

## Limitations

- The method relies heavily on gradient-based clustering working effectively across diverse multimodal tasks, but doesn't thoroughly explore cases where tasks share similar gradients or when gradient representations fail to capture task-specific information
- The pruning mechanism, while reducing computational cost, could potentially eliminate rare but important samples if cosine similarity thresholds are too aggressive
- The multi-way scoring function selection assumes that entropy maximization reliably identifies the best scorer, but this relationship isn't empirically validated across all possible task combinations

## Confidence

- **High confidence**: The catastrophic forgetting reduction claim (>0.9% forgetting) and relative performance gains (>100% over sequential training) are well-supported by experimental results across multiple datasets and baselines
- **Medium confidence**: The effectiveness of gradient-based clustering for task separation is supported by visualizations, but the method's robustness to tasks with overlapping gradients remains unclear
- **Medium confidence**: The entropy-based scoring function selection appears theoretically sound, but the paper doesn't provide ablation studies showing how performance degrades when using fixed scoring functions versus dynamic selection

## Next Checks

1. **Gradient clustering robustness test**: Apply Adapt-∞ to a synthetic dataset where tasks are designed to have similar gradients, then measure how well clustering separates them and whether task imbalance occurs
2. **Pruning sensitivity analysis**: Systematically vary the pruning threshold and measure the trade-off between computational savings and skill retention, particularly for rare tasks that might be disproportionately affected
3. **Scoring function ablation**: Replace the entropy-based multi-way selection with fixed scoring functions across all clusters and measure the degradation in task balance and overall performance to quantify the value of dynamic selection