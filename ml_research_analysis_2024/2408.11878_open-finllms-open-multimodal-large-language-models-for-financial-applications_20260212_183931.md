---
ver: rpa2
title: 'Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications'
arxiv_id: '2408.11878'
source_url: https://arxiv.org/abs/2408.11878
tags:
- financial
- data
- tasks
- table
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Open-FinLLMs is the first open-source multimodal financial LLM
  suite, comprising FinLLaMA (continual pre-trained on 52B tokens of text, tabular,
  and time-series data), FinLLaMA-Instruct (fine-tuned on 573K financial instructions),
  and FinLLaV A (enhanced with 1.43M multimodal tuning pairs). Evaluated across 14
  financial tasks (30 datasets) and 4 multimodal tasks, the models achieve state-of-the-art
  performance, surpassing GPT-4 on 3 tasks and closed-source models like GPT-4o and
  Gemini-1.5-pro on tabular tasks.
---

# Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications

## Quick Facts
- arXiv ID: 2408.11878
- Source URL: https://arxiv.org/abs/2408.11878
- Reference count: 40
- First open-source multimodal financial LLM suite achieving state-of-the-art performance on financial tasks

## Executive Summary
Open-FinLLMs is the first open-source multimodal financial LLM suite, comprising FinLLaMA (continual pre-trained on 52B tokens of text, tabular, and time-series data), FinLLaMA-Instruct (fine-tuned on 573K financial instructions), and FinLLaV A (enhanced with 1.43M multimodal tuning pairs). Evaluated across 14 financial tasks (30 datasets) and 4 multimodal tasks, the models achieve state-of-the-art performance, surpassing GPT-4 on 3 tasks and closed-source models like GPT-4o and Gemini-1.5-pro on tabular tasks. FinLLaMA achieves an 82.10 F1 score in zero-shot NER and 67.67% cumulative return in single-asset trading, while FinLLaV A attains 72.4% accuracy on TableBench, outperforming commercial models. The suite demonstrates strong zero-shot, few-shot, and multimodal reasoning capabilities, making it a robust tool for real-world financial applications. All models and code are publicly released under OSI-approved licenses.

## Method Summary
The Open-FinLLMs suite builds upon the LLaMA3-8B backbone through three sequential training stages: continual pretraining on 52B tokens of financial text, tabular, and time-series data (mixed with general-domain data in a 3:1 ratio to prevent catastrophic forgetting), instruction fine-tuning on 573K financial instruction samples, and multimodal fine-tuning on 1.43M pairs including images, text, charts, and tabular data. The models are trained using DeepSpeed on 64 A100 80GB GPUs for pretraining (250 hours per epoch) and Qlora on 8 A100 80GB GPUs for fine-tuning (6 hours). The approach leverages continual pretraining to acquire financial domain knowledge, instruction tuning to improve task performance, and multimodal extension to handle diverse financial data types.

## Key Results
- FinLLaMA achieves 82.10 F1 score in zero-shot NER and 67.67% cumulative return in single-asset trading
- FinLLaV A attains 72.4% accuracy on TableBench, outperforming commercial models
- Surpasses GPT-4 on 3 tasks and closed-source models like GPT-4o and Gemini-1.5-pro on tabular tasks

## Why This Works (Mechanism)

### Mechanism 1
Continual pretraining on domain-specific data (52B tokens) combined with general data (18B tokens) improves FinLLaMA's financial knowledge without catastrophic forgetting. Mixing financial and general-domain data during pretraining allows the model to retain general language capabilities while acquiring specialized financial understanding. The 3:1 ratio optimizes knowledge transfer.

### Mechanism 2
Instruction tuning with 573K financial instructions significantly improves FinLLaMA-Instruct's performance on downstream financial tasks. Fine-tuning with diverse, high-quality financial instructions enhances the model's ability to follow instructions and understand financial domain knowledge, leading to better performance across various financial tasks.

### Mechanism 3
Multimodal instruction tuning with 1.43M pairs, including tabular and chart data, enables FinLLaV A to outperform commercial models on financial multimodal tasks. Training on diverse multimodal data types (images, text, charts, tabular) through instruction tuning allows the model to effectively interpret and process complex financial data across modalities.

## Foundational Learning

- Concept: Continual pretraining
  - Why needed here: Allows the model to build upon existing knowledge while acquiring domain-specific expertise, avoiding the need to train from scratch
  - Quick check question: What is the key difference between continual pretraining and training from scratch in terms of data efficiency?

- Concept: Instruction tuning
  - Why needed here: Transforms the pretrained model into one that can effectively follow instructions and perform specific tasks in the financial domain
  - Quick check question: How does instruction tuning differ from standard supervised fine-tuning in terms of dataset requirements?

- Concept: Multimodal learning
  - Why needed here: Financial data comes in various forms (text, tables, charts), requiring models to process and integrate information across different modalities
  - Quick check question: What are the key challenges in aligning visual and textual information in multimodal models?

## Architecture Onboarding

- Component map: FinLLaMA (base model) -> FinLLaMA-Instruct (instruction-tuned) -> FinLLaV A (multimodal extension)
- Critical path: Pretraining → Instruction tuning → Multimodal extension → Evaluation across multiple task types
- Design tradeoffs: Smaller model size (8B) vs. performance, focus on financial domain vs. general capabilities, inclusion of multimodal data vs. complexity
- Failure signatures: Poor performance on zero-shot tasks may indicate insufficient pretraining, while failures on instruction-following tasks suggest inadequate fine-tuning
- First 3 experiments:
  1. Evaluate zero-shot performance on basic financial sentiment analysis tasks
  2. Test instruction-following capabilities on financial question-answering
  3. Assess multimodal understanding on simple tabular data tasks

## Open Questions the Paper Calls Out

- How does the performance of Open-FinLLMs compare when applied to non-English financial texts? The paper states "our experiments focused exclusively on English, highlighting the need to expand to multilingual settings to better serve global financial markets."
- What is the impact of increasing model size beyond 8B parameters on the performance of Open-FinLLMs? "The models are currently limited to a size of 8B parameters, and future work should explore both smaller models for efficiency and larger models for enhanced performance."
- How do Open-FinLLMs perform on broader industrial use cases beyond the financial tasks tested in the paper? "The tasks we addressed, such as trading-related scenarios, represent only a subset of potential applications, and future research should investigate broader industrial use cases like financial auditing, risk management, and regulatory compliance."

## Limitations

- Limited empirical validation for key design decisions, particularly the optimal ratio of financial to general-domain data during pretraining
- Lack of ablation studies showing how different dataset sizes or compositions affect performance
- Reliance on synthetic tabular data for fine-tuning may limit real-world applicability

## Confidence

**High Confidence Claims:**
- The overall training pipeline (continual pretraining → instruction tuning → multimodal extension) is technically sound and follows established practices
- Performance improvements over baseline models on the reported benchmarks
- The release of open-source models and code under OSI-approved licenses

**Medium Confidence Claims:**
- Claims of state-of-the-art performance across all 14 financial tasks (30 datasets)
- Specific numerical results (82.10 F1 score, 67.67% cumulative return) without detailed statistical analysis
- Comparisons with commercial models (GPT-4, GPT-4o, Gemini-1.5-pro) that may have access to different data or capabilities

**Low Confidence Claims:**
- Claims about preventing catastrophic forgetting without extensive ablation studies
- The generalizability of results to completely unseen financial domains
- The long-term robustness of the models without ongoing evaluation

## Next Checks

1. Conduct ablation study on pretraining data composition by systematically varying the ratio of financial to general-domain data to identify the optimal balance that prevents catastrophic forgetting while maintaining financial specialization.

2. Replicate the TableBench and ChartBench evaluations using different datasets and evaluation protocols to verify the claimed 72.4% accuracy and assess performance consistency across diverse financial multimodal tasks.

3. Evaluate model performance over time on dynamic financial datasets to assess how well the models maintain accuracy as financial markets and data distributions evolve, particularly focusing on zero-shot and few-shot capabilities.