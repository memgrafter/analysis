---
ver: rpa2
title: Reinforcement Learning with Token-level Feedback for Controllable Text Generation
arxiv_id: '2403.11558'
source_url: https://arxiv.org/abs/2403.11558
tags:
- text
- methods
- generation
- which
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning method for controllable
  text generation that uses token-level rewards rather than sentence-level rewards.
  The key innovation is formulating rewards based on the probability shift of an attribute
  classifier before and after generating each token.
---

# Reinforcement Learning with Token-level Feedback for Controllable Text Generation

## Quick Facts
- arXiv ID: 2403.11558
- Source URL: https://arxiv.org/abs/2403.11558
- Reference count: 31
- Key outcome: Proposed RL method with token-level rewards achieves superior attribute control accuracy and text quality compared to baselines on sentiment control, detoxification, and multi-attribute control tasks.

## Executive Summary
This paper proposes a reinforcement learning method for controllable text generation that uses token-level rewards rather than sentence-level rewards. The key innovation is formulating rewards based on the probability shift of an attribute classifier before and after generating each token. This provides finer-grained guidance to the model. The method also includes a "first quantize, then noise" procedure to enhance robustness. Experiments on sentiment control, detoxification, and multi-attribute control tasks show the proposed method achieves superior performance compared to various baselines including post-processing, fine-tuning, and RL methods, with better attribute control accuracy and text quality.

## Method Summary
The proposed method formulates controllable text generation as a reinforcement learning problem where a language model generates tokens sequentially, receiving token-level rewards based on the probability shift of an attribute classifier. The reward is calculated as the difference in attribute probability before and after each token generation. To enhance robustness, rewards are first quantized into intervals and then noise is injected within each interval. For multi-attribute control, a small "weigher" module learns to dynamically weight contributions from different attribute scorers based on hidden states. The policy is updated using PPO-style objectives with entropy and KL penalties.

## Key Results
- Token-level reward approach achieves higher attribute accuracy than sentence-level reward baselines on sentiment control tasks
- The quantization and noise injection procedure improves robustness and generalization compared to non-robust methods
- The "weigher" module enables effective multi-attribute control, outperforming simple averaging approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level rewards provide finer-grained feedback than sentence-level rewards by capturing semantic shifts within sentences.
- Mechanism: The reward function is formulated as the probability shift of an attribute classifier before and after generating each token, allowing the model to adjust generation step-by-step.
- Core assumption: The semantic content of a sentence can change significantly within a single sentence, and different tokens contribute differently to the target attribute.
- Evidence anchors:
  - [abstract] "formulating rewards based on the probability shift of an attribute classifier before and after generating each token. This provides finer-grained guidance to the model."
  - [section 3.2] "The token-level reward function can be formulated as the probability shift before and after the word is generated."
- Break condition: If the attribute classifier cannot reliably predict attribute probability changes at the token level, the reward signal becomes noisy or uninformative.

### Mechanism 2
- Claim: The "first quantize, then noise" procedure enhances robustness by preventing overfitting to classifier scoring patterns.
- Mechanism: Rewards are quantized into intervals, and noise is injected within each interval to disrupt fixed scoring patterns while preserving relative order.
- Core assumption: Without noise, the model may learn to optimize for specific scoring patterns rather than generalizing to the target attribute.
- Evidence anchors:
  - [section 3.3] "First, we quantize the rewards within D, and acquire q-quantiles, which divide the reward range into q intervals. Then, we inject noise into each reward while ensuring each reward stays in the original interval."
  - [section 4.4] "Our algorithm can achieve higher attribute accuracy since the noising procedure can promote the generalization of models."
- Break condition: If noise amplitude is too high, the reward signal becomes too random to guide learning effectively.

### Mechanism 3
- Claim: The "weigher" module enables effective multi-attribute control by balancing contributions of different attribute scorers.
- Mechanism: A small-scale neural network learns to weight token-level rewards from multiple attribute classifiers based on hidden states, allowing different parts of a sentence to address different attributes.
- Core assumption: Different tokens within a sentence may contribute to different attributes, requiring dynamic weighting rather than simple averaging.
- Evidence anchors:
  - [section 3.4] "To consider multiple constraints simultaneously, we should combine multiple reward groups from different scorers. Simple aggregations or averages cannot provide appropriate token-level guidance."
  - [section 4.3] "Table 3 shows that ablating 'weigher' leads to a performance decrease."
- Break condition: If attribute classifiers are highly correlated or contradictory, the weigher may struggle to find meaningful balance.

## Foundational Learning

- Concept: Reinforcement learning with Markov Decision Processes (MDPs)
  - Why needed here: The problem is formulated as an MDP where states are partial sequences and actions are next tokens, with rewards guiding the policy.
  - Quick check question: What are the state, action, and reward in this text generation MDP?

- Concept: Bayesian factorization and probability shifts
  - Why needed here: The token-level reward design is inspired by an alternative perspective on Bayesian factorization, focusing on probability shifts rather than absolute probabilities.
  - Quick check question: How does the probability shift P(c|y≤i)/P(c|y≤i-1) differ from just using P(c|y≤i) as a reward?

- Concept: Quantile-based discretization and noise injection
  - Why needed here: The quantization and noise procedure prevents overfitting to classifier patterns while maintaining relative reward ordering.
  - Quick check question: What is the purpose of quantizing rewards before adding noise, rather than just adding noise directly?

## Architecture Onboarding

- Component map:
  - Policy model (LLM) -> Attribute scorer -> Data pool -> Quantizer & noise injector -> Learning module

- Critical path:
  1. Generate token with policy
  2. Score attribute probability shift → reward
  3. Store in data pool
  4. Periodically quantize & noise rewards
  5. Update policy using processed rewards
  6. Repeat

- Design tradeoffs:
  - Token-level vs. sentence-level rewards: finer guidance vs. computational overhead
  - Noise amplitude: prevents overfitting vs. signal clarity
  - Quantile number: granularity of quantization vs. stability
  - Weigher complexity: expressiveness vs. computational cost

- Failure signatures:
  - Low attribute accuracy despite training: reward signal may be too noisy or classifier poorly calibrated
  - High PPL scores: exploration/entropy coefficient too low or KL penalty too high
  - Poor multi-attribute performance: weigher not learning meaningful weights or attribute classifiers conflicting

- First 3 experiments:
  1. Run with sentence-level rewards (baseline) vs. token-level rewards on sentiment control task
  2. Test different quantile numbers (q=3,5,7,9) on convergence speed
  3. Implement ablation of weigher in multi-attribute setting to verify its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different quantization methods (e.g., quantiles, equal width bins) on the performance of TOLE?
- Basis in paper: [inferred] The paper mentions using quantiles for quantization, but does not explore other quantization methods.
- Why unresolved: The paper only experiments with one quantization method, so the impact of other methods is unknown.
- What evidence would resolve it: Experiments comparing the performance of TOLE using different quantization methods (e.g., quantiles vs. equal width bins) on the same tasks would provide evidence for the impact of the quantization method.

### Open Question 2
- Question: How does the performance of TOLE scale with the number of attributes being controlled?
- Basis in paper: [inferred] The paper only experiments with controlling up to three attributes, so the scalability to more attributes is unknown.
- Why unresolved: The paper does not provide evidence for the performance of TOLE when controlling more than three attributes.
- What evidence would resolve it: Experiments evaluating the performance of TOLE on tasks requiring control of more than three attributes would provide evidence for its scalability.

### Open Question 3
- Question: How does the performance of TOLE compare to other state-of-the-art methods on large-scale language models (e.g., GPT-3, PaLM)?
- Basis in paper: [inferred] The paper only experiments with GPT-2, so the performance on larger models is unknown.
- Why unresolved: The paper does not provide evidence for the performance of TOLE on larger language models.
- What evidence would resolve it: Experiments comparing the performance of TOLE to other methods on large-scale language models would provide evidence for its effectiveness on these models.

## Limitations
- The effectiveness of token-level rewards depends on the attribute classifier's ability to reliably predict attribute probability changes at the token level, which may not hold for all attributes or text domains.
- The quantization and noise injection process could potentially introduce noise that obscures meaningful reward signals, especially if not properly calibrated.
- The experiments are conducted on relatively small datasets, which may not fully capture the method's performance on larger, more diverse corpora.

## Confidence
- High confidence: The overall framework of using token-level rewards for controllable text generation is sound and well-motivated by the need for finer-grained guidance compared to sentence-level rewards.
- Medium confidence: The specific mechanisms for enhancing robustness (quantization and noise injection) and handling multi-attribute control (the "weigher" module) are supported by ablation studies, but lack of detailed implementation specifics warrant further investigation.
- Low confidence: The paper does not provide sufficient evidence to fully support claims about the method's generalizability to other attributes, text domains, or larger datasets.

## Next Checks
1. Test the robustness of the token-level reward signal by conducting experiments with different levels of noise injection and quantile numbers to assess sensitivity to these hyperparameters.
2. Evaluate the "weigher" module's contribution through ablation studies comparing multi-attribute control performance with and without the module, and analyze the learned weights.
3. Assess generalizability to other attributes and text domains by applying the method to additional controllable text generation tasks beyond sentiment control and detoxification.