---
ver: rpa2
title: 'UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit
  Reward Function'
arxiv_id: '2410.21438'
source_url: https://arxiv.org/abs/2410.21438
tags:
- alignment
- mistral
- data
- qwen
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of performance degradation in large
  language models (LLMs) that occurs when supervised fine-tuning (SFT) and alignment
  techniques are applied sequentially. SFT enhances the model's ability to answer
  questions, while alignment techniques like RLHF, DPO, and UNA address ethical concerns
  by teaching the model to reject harmful requests.
---

# UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function

## Quick Facts
- **arXiv ID**: 2410.21438
- **Source URL**: https://arxiv.org/abs/2410.21438
- **Reference count**: 40
- **Primary result**: Unified Fine-Tuning (UFT) prevents performance degradation when fine-tuning LLMs by integrating SFT and alignment techniques into a single training stage using a generalized implicit reward function.

## Executive Summary
The paper addresses performance degradation in large language models that occurs when supervised fine-tuning (SFT) and alignment techniques like RLHF, DPO, and UNA are applied sequentially. These sequential stages cause models to lose capabilities acquired in earlier phases. To solve this, the paper introduces UFT, which unifies SFT and alignment into a single training stage using the same objective and loss functions through a generalized implicit reward function. This approach prevents performance degradation and maintains model capabilities across tasks. Experiments demonstrate that UFT outperforms SFT when fine-tuning on instruction-tuning data alone, and when combining instruction-tuning data with alignment data, UFT effectively prevents performance degradation while showing clear advantages over sequential methods.

## Method Summary
UFT integrates supervised fine-tuning (SFT) and alignment techniques into a single training stage by converting SFT data into alignment format and using a generalized implicit reward function from UNA. The method transforms instruction-tuning data (prompt-response pairs) into alignment data with feedback score r=1, allowing both data types to be trained on simultaneously. The unified training uses a loss function that includes KL divergence minimization from the pretrained model, maintaining capabilities while optimizing for new objectives. UFT is tested on Mistral 7B-v0.1 and Qwen 32B models with LoRA, using 20k samples each from UltraChat (instruction-tuning) and HelpSteer2 (alignment) datasets.

## Key Results
- UFT outperforms SFT on downstream tasks when fine-tuning using only instruction-tuning dataset
- UFT effectively prevents catastrophic forgetting when combining instruction-tuning data with alignment data
- Significant improvements in ifeval task for instruction-following and truthful-qa task for factuality compared to sequential methods
- Performance degradation observed in some tasks (e.g., musr) when data distribution is not properly balanced

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** UFT prevents performance degradation by unifying SFT and alignment into a single training stage.
- **Mechanism:** By converting SFT data into a format compatible with alignment training and using a generalized implicit reward function, UFT trains on both instruction-tuning and alignment data simultaneously, avoiding the sequential stage losses that cause degradation.
- **Core assumption:** The transformation of SFT data into a format with a score of 1 (highest reward) allows it to be effectively merged with alignment data for unified training.
- **Evidence anchors:** [abstract] "UFT effectively prevents catastrophic forgetting across these two stages and shows a clear advantage over sequentially applying SFT and alignment."

### Mechanism 2
- **Claim:** UFT outperforms SFT by incorporating KL divergence minimization from the pretrained model.
- **Mechanism:** The UFT loss function includes a KL divergence term that maintains proximity to the pretrained model's capabilities while optimizing for the new objectives, which SFT lacks.
- **Core assumption:** Minimizing divergence from the pretrained model preserves capabilities while allowing fine-tuning on new data.
- **Evidence anchors:** [section 2.3] "we attribute to the KL divergence term that focuses on minimization with the pretrained model, a factor ignored in the SFT processes."

### Mechanism 3
- **Claim:** UFT achieves the same goal as SFT by maximizing the probability of generating responses.
- **Mechanism:** Through mathematical equivalence, UFT's loss function drives the implicit reward to positive infinity, which maximizes πθ(y|x), achieving the same objective as SFT's cross-entropy loss.
- **Core assumption:** The mathematical transformation from SFT's cross-entropy loss to UFT's implicit reward formulation preserves the optimization objective.
- **Evidence anchors:** [section 2.3] "A heuristic proof why UFT can replace SFT is provided by arguing that they achieve the same goal of maximizing the probability of πθ(y|x)"

## Foundational Learning

- **Concept: Generalized Implicit Reward Function**
  - Why needed here: UFT relies on UNA's generalized implicit reward function to handle multiple feedback types (pairwise, binary, score-based) within a unified framework.
  - Quick check question: What are the three types of feedback that the generalized implicit reward function can handle?

- **Concept: Catastrophic Forgetting**
  - Why needed here: Understanding this phenomenon explains why sequential SFT followed by alignment causes performance degradation and why UFT's unified approach is beneficial.
  - Quick check question: In the context of LLM training, what specific capability is typically lost when alignment is applied after SFT?

- **Concept: Cross-Entropy Loss vs. Reward-Based Loss**
  - Why needed here: The paper argues that UFT mathematically achieves the same objective as SFT while incorporating alignment, requiring understanding of how these different loss formulations relate.
  - Quick check question: How does maximizing an implicit reward function mathematically relate to maximizing the probability of generating a response?

## Architecture Onboarding

- **Component map:** Data preprocessing (convert SFT to alignment format) -> Unified training with implicit reward function -> Evaluation on downstream tasks
- **Critical path:** Data preprocessing → Unified training with implicit reward → Evaluation on downstream tasks. The critical path involves ensuring the SFT data transformation is correct and that the implicit reward function properly handles both data types.
- **Design tradeoffs:** UFT trades off the simplicity of separate SFT and alignment stages for unified training that prevents degradation. This may increase complexity in data handling but reduces the risk of losing capabilities.
- **Failure signatures:** Performance degradation on specific tasks (like musr in some experiments), misalignment of data distribution causing bias toward one data type, or failure to properly convert SFT data to the alignment format.
- **First 3 experiments:**
  1. Compare UFT vs SFT on instruction-tuning data alone using UltraChat dataset (20k samples) on Mistral 7B-v0.1 and Qwen 32B.
  2. Compare UFT vs SFT+Alignment sequential methods using HelpSteer2 dataset (20k samples) with DPO, KTO, and UNA.
  3. Test different data distributions by varying the proportion of instruction-tuning vs alignment data while keeping alignment constant at 20k samples.

## Open Questions the Paper Calls Out
1. **How does the performance of UFT vary when applied to multilingual datasets compared to the current English-only evaluation?**
   - Basis in paper: [explicit] The paper mentions that the study is restricted to English and recommends evaluating multilingual capabilities in future research.
   - Why unresolved: The paper does not provide any multilingual evaluation or comparison, leaving the performance of UFT on non-English languages unexplored.
   - What evidence would resolve it: Testing UFT on multilingual datasets and comparing its performance with SFT and alignment methods across different languages.

2. **What is the optimal ratio of instruction-tuning data to alignment data for maximizing UFT performance across diverse tasks?**
   - Basis in paper: [explicit] The paper discusses the impact of different data distributions but notes that further investigation into the optimal ratio is warranted.
   - Why unresolved: The experiments show varying performance with different data ratios, but a definitive optimal ratio is not established.
   - What evidence would resolve it: Conducting extensive experiments with various ratios of instruction-tuning to alignment data to determine the ratio that consistently yields the best performance.

3. **How does UFT perform on industrial datasets compared to academic datasets like UltraChat and HelpSteer?**
   - Basis in paper: [explicit] The paper recommends testing UFT on industrial datasets to enhance applicability, as current evaluations are based on academic datasets.
   - Why unresolved: The paper does not include evaluations on industrial datasets, leaving the performance of UFT in real-world applications uncertain.
   - What evidence would resolve it: Applying UFT to industrial datasets and comparing its performance with SFT and alignment methods in practical scenarios.

## Limitations
- The mathematical equivalence proof between UFT and SFT is provided heuristically rather than rigorously, creating uncertainty about whether UFT truly achieves the same optimization objective as SFT.
- The generalized implicit reward function's implementation details are not fully specified, which could affect reproducibility.
- Experiments primarily use two base models (Mistral 7B-v0.1 and Qwen 32B), limiting generalizability to other model architectures.

## Confidence

- **High confidence**: UFT prevents performance degradation compared to sequential SFT and alignment methods (supported by experimental results across multiple tasks and datasets)
- **Medium confidence**: UFT mathematically achieves the same goal as SFT through implicit reward maximization (based on heuristic proof with supporting but not definitive evidence)
- **Low confidence**: The generalized implicit reward function can effectively handle all three feedback types (pairwise, binary, score-based) without modification (limited corpus evidence)

## Next Checks
1. Rigorously prove the mathematical equivalence between UFT's implicit reward formulation and SFT's cross-entropy loss through formal analysis
2. Test UFT on additional base model architectures beyond Mistral 7B-v0.1 and Qwen 32B to assess generalizability
3. Conduct ablation studies on the implicit reward function to determine which components are essential for preventing performance degradation