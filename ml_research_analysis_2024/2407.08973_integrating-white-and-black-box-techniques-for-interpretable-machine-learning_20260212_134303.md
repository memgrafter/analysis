---
ver: rpa2
title: Integrating White and Black Box Techniques for Interpretable Machine Learning
arxiv_id: '2407.08973'
source_url: https://arxiv.org/abs/2407.08973
tags:
- classifier
- decision
- base
- grader
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the accuracy-interpretability trade-off in
  machine learning by proposing an ensemble method that combines white-box (interpretable)
  and black-box (non-interpretable but powerful) classifiers. The method uses a three-component
  system: a base classifier (decision tree) for "easy" inputs, a deferral classifier
  (random forest) for "hard" inputs, and a grader (decision tree) to classify inputs
  as easy or hard.'
---

# Integrating White and Black Box Techniques for Interpretable Machine Learning

## Quick Facts
- arXiv ID: 2407.08973
- Source URL: https://arxiv.org/abs/2407.08973
- Authors: Eric M. Vernon; Naoki Masuyama; Yusuke Nojima
- Reference count: 20
- Primary result: Ensemble method combining white-box and black-box classifiers significantly improves accuracy while maintaining interpretability for most predictions

## Executive Summary
This paper addresses the accuracy-interpretability trade-off in machine learning by proposing an ensemble method that combines interpretable (white-box) and powerful but non-interpretable (black-box) classifiers. The method uses three components: a base classifier (decision tree) for "easy" inputs, a deferral classifier (random forest) for "hard" inputs, and a grader (decision tree) to classify inputs as easy or hard. Experiments on 10 real-world datasets showed significant accuracy improvements compared to using only the interpretable base classifier, while maintaining interpretability for most predictions.

## Method Summary
The proposed ensemble method trains three classifiers independently: a decision tree base classifier, a random forest deferral classifier, and a decision tree grader. Training data is relabeled as "easy" (correctly classified by base) or "hard" (misclassified), with SMOTE resampling to balance classes for the grader. During inference, new inputs are first evaluated by the grader; easy inputs are classified by the base, hard by the deferral. The method was evaluated using 10-fold cross-validation with 5 repetitions on 10 real-world datasets from the OpenML repository.

## Key Results
- Accuracy significantly improved compared to using only the base classifier (e.g., Gas Sensor Array Drift: 72.96% → 95.50%)
- Maintained interpretability for most predictions (e.g., 77% of points classified by interpretable base classifier)
- Effective across diverse datasets with varying numbers of classes and features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble improves accuracy by using random forest for inputs misclassified by decision tree
- Mechanism: Grader identifies "hard" inputs that decision tree misclassifies, routing them to random forest which handles complex patterns better
- Core assumption: Random forest outperforms decision tree on difficult patterns
- Evidence anchors: Accuracy improvement from 72.96% to 95.50% on Gas Sensor Array Drift dataset; 23% of points routed to black box deferral classifier

### Mechanism 2
- Claim: Grader provides interpretability for deferral decisions by being itself interpretable
- Mechanism: Decision tree grader classifies inputs as easy/hard, allowing users to understand why inputs were deferred to black box
- Core assumption: Grader is sufficiently accurate at distinguishing easy from hard inputs
- Evidence anchors: Method offers "explanations for both classification results and deferral decisions"; transparency maintained even when black box selected

### Mechanism 3
- Claim: Resampling addresses class imbalance in grader's training data
- Mechanism: SMOTE oversamples minority class after relabeling data as easy/hard, creating balanced training data for grader
- Core assumption: Class imbalance significantly impacts grader performance
- Evidence anchors: Grader trained with highly imbalanced dataset; resampling ensures equal number of easy and hard training samples

## Foundational Learning

- Concept: Decision Trees and Random Forests
  - Why needed here: Base and grader are decision trees, deferral is random forest; understanding their behavior is essential
  - Quick check question: How does increasing tree depth affect a decision tree's accuracy and interpretability?

- Concept: Ensemble Methods
  - Why needed here: Three classifiers working together form an ensemble
  - Quick check question: What are the benefits and drawbacks of combining multiple classifiers in an ensemble?

- Concept: Class Imbalance and Resampling Techniques
  - Why needed here: Grader's training data is imbalanced between easy and hard patterns
  - Quick check question: Why does training on imbalanced data often lead to poor minority class performance?

## Architecture Onboarding

- Component map: Input -> Grader (Decision Tree) -> [Easy] -> Base Classifier (Decision Tree) OR [Hard] -> Deferral Classifier (Random Forest) -> Output

- Critical path:
  1. New input arrives
  2. Grader evaluates if input is easy or hard
  3. If easy → Base classifier processes input
  4. If hard → Deferral classifier processes input
  5. Output classification result

- Design tradeoffs:
  - Shallow decision trees (max depth 4) balance interpretability with complexity
  - Random forest provides better accuracy but sacrifices interpretability
  - Resampling adds computational overhead but improves grader performance

- Failure signatures:
  - Grader becomes trivial (always predicts easy or always predicts hard)
  - Deferral classifier doesn't outperform base classifier on hard patterns
  - Overfitting in any of the three components

- First 3 experiments:
  1. Train the ensemble on a simple 2D dataset and visualize decision boundaries of all three classifiers
  2. Test with different maximum depths for the decision trees to find the interpretability-accuracy sweet spot
  3. Compare performance with and without SMOTE resampling to quantify its impact on grader accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ensemble method perform when using different combinations of white-box and black-box classifiers beyond the decision tree and random forest used in the experiments?
- Basis in paper: Authors plan to experiment with many different types of classification algorithms, such as rule-based classifiers for white box and gradient boosted trees for black box
- Why unresolved: Only tested decision trees and random forests, leaving performance of other classifier combinations unexplored
- What evidence would resolve it: Empirical results comparing accuracy-interpretability trade-off across various combinations of white-box and black-box classifiers

### Open Question 2
- Question: What is the impact of varying the maximum depth of the decision trees used for the base classifier and grader on overall performance?
- Basis in paper: Fixed maximum depth of 4 used, but no exploration of how different depths affect accuracy and interpretability
- Why unresolved: Optimal depth for balancing interpretability and performance is likely dataset-dependent and was not investigated
- What evidence would resolve it: Sensitivity analysis showing how accuracy and deferral rates change as maximum tree depth varies

### Open Question 3
- Question: How does the ensemble method scale with high-dimensional datasets, particularly those with hundreds or thousands of features?
- Basis in paper: Experiments included datasets with up to 128 features, but no discussion of performance on higher dimensionality
- Why unresolved: Computational complexity and interpretability of decision trees may degrade significantly in very high-dimensional spaces
- What evidence would resolve it: Performance metrics on datasets with 1000+ features, along with analysis of feature selection or dimensionality reduction techniques

## Limitations

- No ablation studies to isolate contribution of each component (grader, resampling, deferral classifier)
- Decision tree depth (max depth 4) appears arbitrary without sensitivity analysis
- SMOTE resampling parameters unspecified, which could affect reproducibility

## Confidence

- High confidence in ensemble's effectiveness based on statistically significant accuracy improvements across 10 datasets
- Medium confidence in generalizability due to limited hyperparameter exploration and lack of diverse dataset types
- Low confidence in optimal parameter choices (tree depths, resampling rates, random forest configurations)

## Next Checks

1. Perform ablation studies removing each component (grader, resampling, deferral classifier) to quantify individual contributions
2. Conduct sensitivity analysis varying decision tree depths and random forest parameters to find optimal configurations
3. Test the ensemble on additional dataset types including high-dimensional data, imbalanced datasets, and categorical features to assess robustness