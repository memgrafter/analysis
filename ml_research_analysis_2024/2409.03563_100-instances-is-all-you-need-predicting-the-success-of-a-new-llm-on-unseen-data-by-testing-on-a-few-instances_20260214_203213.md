---
ver: rpa2
title: '100 instances is all you need: predicting the success of a new LLM on unseen
  data by testing on a few instances'
arxiv_id: '2409.03563'
source_url: https://arxiv.org/abs/2409.03563
tags:
- performance
- instances
- llms
- assessor
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to predict the performance of a new
  LLM on individual task instances by leveraging the evaluation results of previously
  tested LLMs. The core idea is to evaluate the new LLM on a small set of reference
  instances and train a generic assessor that predicts performance on a new instance
  using the LLM's performance on the reference set combined with instance-specific
  features.
---

# 100 instances is all you need: predicting the success of a new LLM on unseen data by testing on a few instances

## Quick Facts
- **arXiv ID**: 2409.03563
- **Source URL**: https://arxiv.org/abs/2409.03563
- **Reference count**: 40
- **Primary result**: A generic assessor trained on a few reference instances can predict LLM performance on new instances, but performance degrades significantly out-of-distribution

## Executive Summary
This paper introduces a method to predict how well a new large language model (LLM) will perform on individual task instances by testing it on just 100 reference examples. The approach trains a generic assessor that combines the LLM's performance on these reference instances with instance-specific features to predict outcomes on new data. The method is evaluated on HELM-Lite and a novel reasoning dataset collection, showing comparable accuracy to LLM-specific assessors in-distribution but significant degradation out-of-distribution, highlighting inherent limitations in LLM predictability.

## Method Summary
The core idea is to evaluate a new LLM on a small set of reference instances and train a generic assessor that predicts performance on new instances using the LLM's performance on the reference set combined with instance-specific features. The method involves two main steps: (1) collecting performance data from multiple LLMs on a reference set of instances, and (2) training a generic assessor that maps instance features and reference performance to predicted accuracy on new instances. The authors compare this approach to LLM-specific assessors and several baselines across in-distribution and out-of-distribution settings.

## Key Results
- A generic assessor trained on 100 reference instances performs comparably to LLM-specific assessors when predicting performance on in-distribution data
- Out-of-distribution, the generic assessor's performance degrades significantly, suggesting fundamental limits to LLM performance prediction across unseen datasets
- The method requires far fewer evaluations than LLM-specific assessors, making it more efficient for practical deployment

## Why This Works (Mechanism)
The method works by leveraging the observation that LLM performance on a new instance can be predicted from its performance on a small, carefully selected reference set combined with instance-specific features. By training on multiple LLMs' behaviors across diverse instances, the generic assessor learns patterns that generalize within the same data distribution. The approach assumes that instance difficulty and LLM capabilities are correlated enough across instances that a small reference set captures sufficient information for prediction.

## Foundational Learning
- **Instance-specific feature extraction**: Needed to characterize the properties of each task instance that might influence LLM performance. Quick check: Verify that extracted features capture meaningful dimensions of instance complexity.
- **Reference set selection**: Critical for ensuring the small set represents the full task distribution. Quick check: Confirm reference set diversity through statistical coverage analysis.
- **Cross-LLM generalization**: Required for the generic assessor to work across different models. Quick check: Validate assessor performance across a diverse set of LLM architectures and sizes.
- **In-distribution vs out-of-distribution distinction**: Fundamental for understanding when prediction is reliable. Quick check: Test performance on held-out datasets from the same and different distributions.

## Architecture Onboarding

**Component Map**
Reference instances -> LLM evaluation -> Feature extraction -> Generic assessor training -> Performance prediction on new instances

**Critical Path**
1. Select 100 diverse reference instances
2. Evaluate multiple LLMs on reference set
3. Extract instance features for all instances
4. Train generic assessor using reference performance + instance features
5. Apply trained assessor to predict new instance performance

**Design Tradeoffs**
- Reference set size (100) vs prediction accuracy
- Feature complexity vs computational efficiency
- Generic vs LLM-specific assessors (generalization vs accuracy)
- In-distribution focus vs out-of-distribution robustness

**Failure Signatures**
- Poor reference set diversity leading to inaccurate predictions
- Feature extraction missing key performance predictors
- Overfitting to specific LLM behaviors in training data
- Distribution shift between training and test instances

**First 3 Experiments**
1. Ablation study: Test performance with reference sets of 10, 50, 100, and 200 instances to validate the "100 instances" claim
2. Feature importance analysis: Determine which instance features most strongly predict performance across different task types
3. Cross-task validation: Train a single generic assessor across all task types and test its ability to predict performance on new, unseen tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance degradation when predicting on out-of-distribution data suggests fundamental limits to LLM predictability
- The 100-instance reference set, while empirically effective, lacks theoretical justification for why this size works across different tasks
- The study focuses on zero-shot and few-shot settings without examining chain-of-thought or other prompting strategies that could affect predictability

## Confidence
- **High confidence**: Predicting LLM performance on individual instances is feasible with small reference sets when data distribution is preserved
- **Medium confidence**: The claim that 100 instances is sufficient across diverse tasks, though validation on broader domains is needed
- **Low confidence**: The assertion that out-of-distribution performance degradation is an "inherent limitation" rather than a result of dataset selection or feature engineering choices

## Next Checks
1. Test the methodology on at least 5 additional benchmark suites from different domains (e.g., biomedical, legal, creative writing) to validate generalizability claims and better characterize the boundaries of predictability
2. Evaluate whether a single generic assessor trained across all task types can predict performance on new tasks, rather than requiring task-specific assessors
3. Systematically test whether including chain-of-thought or other prompting variations in the reference set improves out-of-distribution prediction accuracy