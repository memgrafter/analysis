---
ver: rpa2
title: 'Language models emulate certain cognitive profiles: An investigation of how
  predictability measures interact with individual differences'
arxiv_id: '2406.04988'
source_url: https://arxiv.org/abs/2406.04988
tags:
- surprisal
- reading
- entropy
- cognitive
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines how predictability measures from language models
  (surprisal and entropy) interact with individual cognitive differences to predict
  reading times. The authors use eye-tracking data from 61 German speakers who also
  completed psychometric tests.
---

# Language models emulate certain cognitive profiles: An investigation of how predictability measures interact with individual differences

## Quick Facts
- **arXiv ID**: 2406.04988
- **Source URL**: https://arxiv.org/abs/2406.04988
- **Reference count**: 27
- **Primary result**: Language models better predict reading times for individuals with lower verbal intelligence scores

## Executive Summary
This study investigates how individual cognitive differences modulate the relationship between language model predictability measures (surprisal and entropy) and reading times. Using eye-tracking data from 61 German speakers who completed psychometric tests, the authors find that incorporating cognitive scores into predictability models significantly improves prediction accuracy. Critically, all tested language models (GPT-2, Llama, Mixtral) show better predictive power for readers with lower verbal intelligence scores, suggesting inherent model biases in how they represent cognitive processing.

## Method Summary
The study uses the Individual Differences Corpus (InDiCo) containing eye-tracking data and psychometric scores from 61 German speakers. Language model surprisal and entropy values are computed for each word using pre-trained German models (GPT-2 base/large, Llama 2 7B/13B, Mixtral). Linear-mixed models predict log-transformed first-pass reading times from word-level predictors (length, frequency) and cognitive scores, with interaction terms testing how cognitive performance modulates predictability effects. Models are evaluated by comparing predictive power (ΔLL) between baseline and interaction models.

## Key Results
- Incorporating cognitive scores as interaction terms significantly improves prediction of reading times from surprisal and entropy
- Higher cognitive performance correlates with lower sensitivity to predictability effects on reading times
- All tested language models better predict reading times for individuals with lower verbal intelligence scores
- The effect is particularly strong for reading fluency and working memory measures, weaker for cognitive control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Individual cognitive profiles modulate the relationship between predictability measures and reading times.
- Mechanism: Cognitive capacities interact with surprisal and entropy to alter their predictive power on reading times.
- Core assumption: Cognitive scores can be meaningfully incorporated as interaction terms in regression models.
- Evidence anchors:
  - [abstract] "incorporating information of language users' cognitive capacities"
  - [section] "we define a baseline model Mb with predictors xb including the word-level predictors li, fi, si, hi, and the subject-level predictor cj denoting the test score of a specific psychometric test"
  - [corpus] Evidence is weak: corpus only contains psychometric scores, not causal evidence of cognitive modulation.
- Break condition: If cognitive scores do not significantly improve predictive power when added as interactions.

### Mechanism 2
- Claim: Higher cognitive performance correlates with lower sensitivity to predictability effects.
- Mechanism: Individuals with higher cognitive scores exhibit smaller surprisal and entropy effects on reading times.
- Core assumption: Negative interaction coefficients indicate reduced sensitivity to predictability.
- Evidence anchors:
  - [abstract] "high performance in the psychometric tests is associated with lower sensitivity to predictability effects"
  - [section] "individuals with higher scores show lower surprisal effects as shown in Table 1"
  - [corpus] Evidence is weak: corpus contains only correlational data, not causal evidence of sensitivity changes.
- Break condition: If interaction coefficients are not negative or not significant.

### Mechanism 3
- Claim: Language models emulate readers with lower verbal intelligence.
- Mechanism: LM surprisal and entropy predict reading times better for individuals with lower verbal intelligence scores.
- Core assumption: Better predictive power indicates closer emulation of cognitive profile.
- Evidence anchors:
  - [abstract] "all tested models emulate the processing behaviour of individuals with low verbal intelligence"
  - [section] "surprisal estimates across all tested models predicted RTs better for the group of individuals with low verbal intelligence scores"
  - [corpus] Evidence is weak: corpus only shows correlation, not causation or emulation mechanism.
- Break condition: If predictive power is not higher for low verbal intelligence groups.

## Foundational Learning

- Concept: Linear-mixed models
  - Why needed here: To predict reading times while accounting for both fixed effects (word-level predictors) and random effects (subject-level variability).
  - Quick check question: What is the difference between fixed and random effects in linear-mixed models?

- Concept: Surprisal and contextual entropy
  - Why needed here: These predictability measures quantify how predictable a word is given its context and are hypothesized to correlate with reading effort.
  - Quick check question: How are surprisal and contextual entropy mathematically defined in terms of probability distributions?

- Concept: Interaction terms in regression
  - Why needed here: To model how cognitive scores modulate the relationship between predictability measures and reading times.
  - Quick check question: What does an interaction term between a cognitive score and surprisal represent in the regression model?

## Architecture Onboarding

- Component map: Data collection (eye-tracking + psychometric tests) → LM estimation (surprisal/entropy) → Regression modeling (baseline + interactions) → Prediction and analysis
- Critical path: Collect cognitive scores → Extract predictability measures from LMs → Fit regression models with interactions → Evaluate predictive power differences
- Design tradeoffs: Using interaction terms increases model complexity but may improve prediction; splitting subjects at median is simple but may not capture nuanced cognitive profiles.
- Failure signatures: Non-significant interaction terms, lack of improvement in predictive power, inconsistent results across models.
- First 3 experiments:
  1. Replicate baseline surprisal and entropy predictive power on reading times without cognitive interactions.
  2. Add individual cognitive score interactions to the model and test if predictive power improves.
  3. Split subjects by cognitive score medians and compare LM predictive power between high and low performing groups.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do language models show significantly better predictive power for reading times of individuals with lower verbal intelligence scores?
- Basis in paper: [explicit] The authors state "all tested models emulate the processing behaviour of individuals with low verbal intelligence" and "surprisal estimates across all tested models predicted RTs better for the group of individuals with low verbal intelligence scores."
- Why unresolved: The paper suggests this is "surprising since a language model has been exposed to billions of tokens" but does not provide a definitive explanation for why models would better predict processing effort in less cognitively capable readers.
- What evidence would resolve it: Experimental comparison of predictability measures from models trained on different vocabulary distributions, or analysis of model behavior on rare vs common words across different intelligence levels.

### Open Question 2
- Question: Do different mechanisms underlie the relationship between working memory capacity and surprisal effects compared to other cognitive measures?
- Basis in paper: [explicit] The authors note that results regarding working memory "are more difficult to contextualize" and contrast their findings with O'Rourke (2013) showing high working memory individuals exhibit stronger P600 effects.
- Why unresolved: The paper presents evidence that high working memory individuals show lower surprisal effects, which seems to contradict findings about garden path effects and re-analysis processes.
- What evidence would resolve it: Controlled experiments manipulating memory load and surprisal independently, or EEG studies measuring different ERP components across working memory levels.

### Open Question 3
- Question: How do individual differences in reading comprehension processes vary across different psychometric measures, and what underlying cognitive mechanisms drive these differences?
- Basis in paper: [explicit] The authors find differential predictive power across cognitive domains, with reading fluency and working memory showing stronger effects than cognitive control measures, and note the complex relationships between these measures.
- Why unresolved: While the paper identifies that certain cognitive profiles show stronger predictability effects, it does not explain the underlying cognitive mechanisms that cause these differences or why some measures show stronger effects than others.
- What evidence would resolve it: Detailed cognitive modeling linking specific psychometric measures to underlying reading processes, or neuroimaging studies identifying neural correlates of these individual differences.

## Limitations
- The study's correlational design limits causal interpretations of how cognitive profiles interact with predictability measures
- Median splits to categorize cognitive performance may oversimplify continuous cognitive variation and reduce statistical power
- The approximation of word-level entropy by summing sub-word token entropies could introduce measurement error

## Confidence
**High confidence**: The methodological approach of incorporating cognitive scores as interaction terms is sound and well-established. The general finding that cognitive performance modulates predictability effects on reading times is supported by significant interaction terms in the regression models.

**Medium confidence**: The specific pattern that all tested language models better predict reading times for lower verbal intelligence readers is robust across models, but the underlying mechanism (whether due to model architecture, training data, or other factors) requires further investigation.

**Low confidence**: The claim that language models "emulate" cognitive profiles is more metaphorical than literal, as the models are not designed to simulate human cognition. The practical implications of these findings for model development or cognitive theory remain speculative.

## Next Checks
1. **Test alternative cognitive score categorizations**: Instead of median splits, use continuous cognitive scores or percentile-based groupings to verify that the pattern of lower verbal intelligence readers being better predicted is robust to different analytical approaches.

2. **Validate with alternative language models**: Test whether this pattern holds across a broader range of language models (including smaller models, different architectures, and models trained on different corpora) to determine if this is a general property of language models or specific to the tested models.

3. **Conduct out-of-sample prediction**: Split the dataset temporally or by subjects to test whether models trained on high verbal intelligence readers can predict reading times for low verbal intelligence readers (and vice versa), which would provide stronger evidence for differential predictive patterns.