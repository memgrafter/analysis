---
ver: rpa2
title: Comparison of Machine Learning Classification Algorithms and Application to
  the Framingham Heart Study
arxiv_id: '2402.15005'
source_url: https://arxiv.org/abs/2402.15005
tags:
- training
- testing
- equal
- positive
- proportional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the generalizability of machine learning classification
  algorithms using the Framingham Heart Study data. The research addresses how to
  convert regression models into classifiers by selecting an optimal probability cutoff
  and compares eight classification algorithms under four training/testing scenarios
  to assess their robustness to data distribution shifts.
---

# Comparison of Machine Learning Classification Algorithms and Application to the Framingham Heart Study

## Quick Facts
- arXiv ID: 2402.15005
- Source URL: https://arxiv.org/abs/2402.15005
- Reference count: 40
- Primary result: Double Discriminant Scoring of Type 1 (DDS1) consistently outperforms other classification algorithms across all training/testing scenarios in the Framingham Heart Study.

## Executive Summary
This study examines the generalizability of machine learning classification algorithms using the Framingham Heart Study data to predict coronary heart disease (CHD). The research addresses how to convert regression models into classifiers by selecting an optimal probability cutoff and compares eight classification algorithms under four training/testing scenarios to assess their robustness to data distribution shifts. The study introduces the Double Discriminant Scoring of Type 1, which consistently outperforms other algorithms across all scenarios. Additionally, a methodology is presented to extract an optimal variable hierarchy for classification algorithms, reducing the number of required sampling distribution tests from 2^p - 1 to at most p(p+1)/2. The findings highlight the importance of considering data distribution and algorithmic choice to mitigate biases in healthcare applications.

## Method Summary
The study compares eight classification algorithms (logistic regression, random forest, XGBoost, SVM, linear/quadratic discriminant analysis, and double discriminant scoring) using the Framingham CHD dataset with seven features. Four training/testing scenarios are evaluated: proportional, equal, proportional+equal, and equal+equal splits with Ï„=0.8 ratio. The methodology converts regression outputs to classifiers using training prevalence-based cutoffs, runs 100 simulations per configuration, and extracts optimal variable hierarchies using Bellman recursion to reduce computational complexity.

## Key Results
- DDS1 consistently outperforms other classification algorithms regardless of training/testing scenario.
- Prevalence-based probability cutoffs (matching training dataset prevalence) provide balanced sensitivity and specificity for imbalanced medical data.
- Optimal variable hierarchy satisfies Bellman principle, reducing search space from 2^p - 1 to at most p(p+1)/2 tests.
- XGBoost and SVM show poor performance on proportional training scenarios, highlighting their sensitivity to data distribution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DDS1 consistently outperforms other classification algorithms because it combines linear and quadratic discriminant functions in a "liberal" way, assigning a case to the positive group if either model assigns it positively.
- Mechanism: By using a union-based decision rule, DDS1 captures both linear and non-linear separability in the feature space, increasing true positive detection while tolerating some false positives. This compensates for the high cost of missing positive cases in medical contexts.
- Core assumption: Both linear and quadratic discriminant assumptions (equal vs. unequal covariance) capture different but valid patterns in the data; their union increases coverage.
- Evidence anchors:
  - [abstract] states that DDS1 "consistently outperforms the other classification algorithms regardless of the training/testing scenario."
  - [section] defines DDS1 as assigning to Group 1 "if either the linear or quadratic discriminant models assign the tested patient to Group 1."
  - [corpus] shows related work on heart disease classification using multiple ML algorithms, supporting the general approach.
- Break condition: If the underlying distributions are severely non-Gaussian or the covariance structures are so different that neither model is reliable, the union rule could accumulate noise and degrade performance.

### Mechanism 2
- Claim: Selecting a classifier cutoff equal to the training dataset prevalence balances sensitivity and specificity for imbalanced medical data.
- Mechanism: In imbalanced datasets (e.g., 15% prevalence), a 50% cutoff forces the model to favor the majority class, missing most positives. Using the training prevalence as the cutoff aligns the decision threshold with the actual class distribution, improving true positive detection without overly inflating false positives.
- Core assumption: The training and testing prevalences are similar enough that prevalence-matching generalizes across splits.
- Evidence anchors:
  - [section] shows that "a good and balanced classifier cutoff for both the logistic and random forest regression models appears to be the prevalence of the training dataset."
  - [section] reports consistent equilibrium cutoffs across multiple simulations for both logistic and random forest classifiers.
  - [corpus] lacks direct citations but the claim is consistent with general ML literature on threshold selection for imbalanced data.
- Break condition: If the training and testing prevalences differ significantly (e.g., in domain shift), the prevalence-matching cutoff will no longer be optimal.

### Mechanism 3
- Claim: The optimal variable hierarchy satisfies the Bellman principle, reducing the search space from 2^p - 1 to at most p(p+1)/2 tests.
- Mechanism: By recursively selecting the next variable that maximizes the performance metric given the current hierarchy, the algorithm avoids testing all subsets. Once a variable is added, all supersets containing it are implicitly considered optimal if the incremental gain is positive.
- Core assumption: The performance metric is monotonic with respect to variable inclusion in the optimal sequence; once a variable is excluded, adding it later never improves performance.
- Evidence anchors:
  - [section] states that "this optimal variable hierarchy satisfies the Bellman principle of optimality, leading then to the reduction of sampling distribution tests from 2^p - 1 iterations to at most p(p+1)/2."
  - [section] illustrates this on Framingham CHD data, finding a consistent hierarchy across scenarios.
  - [corpus] lacks direct examples of Bellman-based variable selection but the principle is standard in dynamic programming.
- Break condition: If the performance metric is non-monotonic or interaction effects dominate, the recursive assumption fails and the hierarchy may miss optimal subsets.

## Foundational Learning

- Concept: Logistic regression as a probabilistic model for binary outcomes.
  - Why needed here: The paper converts logistic regression outputs to classifiers using a cutoff; understanding the link function and probability interpretation is essential.
  - Quick check question: If a logistic regression outputs 0.3 probability for a patient, and the cutoff is 0.15, what class is assigned?

- Concept: Imbalanced classification and threshold selection.
  - Why needed here: The Framingham CHD dataset has ~15% prevalence; naive 50% cutoffs lead to poor sensitivity. Adjusting thresholds based on prevalence is key.
  - Quick check question: In a dataset with 90% negatives, what is the risk of using a 50% cutoff for classification?

- Concept: Discriminant analysis assumptions (normality, covariance equality).
  - Why needed here: DDS1 combines linear and quadratic discriminant functions, each with different distributional assumptions; understanding these is necessary to interpret results.
  - Quick check question: What assumption distinguishes linear from quadratic discriminant analysis?

## Architecture Onboarding

- Component map: Data preprocessing -> Train/test split (four scenarios) -> Model training (8 algorithms) -> Cutoff calibration (prevalence-based) -> Performance evaluation -> Variable hierarchy extraction -> Cross-validation (100 simulations)
- Critical path: Train/test split -> Model training -> Cutoff calibration -> Performance aggregation -> Hierarchy extraction
- Design tradeoffs: Using prevalence-based cutoffs improves sensitivity but may increase false positives; DDS1's union rule boosts recall at the cost of precision; full variable sets maximize coverage but increase variance.
- Failure signatures: Extreme Gradient Boosting and SVM failing on proportional splits; variable hierarchies not improving after adding a variable; performance metrics diverging across training/testing ratios.
- First 3 experiments:
  1. Run logistic and random forest with prevalence-based cutoffs on proportional vs. equal training splits; compare true positive rates.
  2. Implement DDS1 and compare its true positive rate to other algorithms across all four training/testing scenarios.
  3. Apply the Bellman-based variable hierarchy extraction on the Framingham CHD data and verify monotonic improvement in true positive rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal variable hierarchy for predicting coronary heart disease in different demographic groups (e.g., age, gender, ethnicity) beyond the Framingham data?
- Basis in paper: [explicit] The study derives an optimal variable hierarchy for the overall, male, and female Framingham CHD data, but does not explore other demographic factors.
- Why unresolved: The paper focuses on age and sex as demographic factors but does not extend the analysis to other potential variables like ethnicity, socioeconomic status, or geographic location.
- What evidence would resolve it: Analyzing the optimal variable hierarchy across diverse demographic groups using different datasets and validating the findings with external populations.

### Open Question 2
- Question: How do the proposed classification algorithms (e.g., Double Discriminant Scoring of Type 1) perform when applied to imbalanced datasets with different prevalence rates?
- Basis in paper: [explicit] The study highlights that Extreme Gradient Boosting and Support Vector Machine are flawed when trained on unbalanced datasets but does not extensively explore other algorithms' performance under varying prevalence conditions.
- Why unresolved: While the study addresses the performance of some algorithms on unbalanced datasets, it does not provide a comprehensive analysis of all proposed algorithms under different prevalence scenarios.
- What evidence would resolve it: Conducting extensive simulations and real-world testing of all proposed algorithms on datasets with varying prevalence rates and imbalance levels.

### Open Question 3
- Question: What is the impact of including additional variables (e.g., lifestyle factors, genetic markers) on the performance of the classification algorithms?
- Basis in paper: [inferred] The study uses seven explanatory variables from the Framingham data and suggests that adding more variables could improve model performance, but does not empirically test this hypothesis.
- Why unresolved: The paper does not explore the inclusion of additional variables beyond those used in the Framingham dataset, leaving the potential impact on model performance untested.
- What evidence would resolve it: Experimenting with the inclusion of various additional variables (e.g., lifestyle factors, genetic markers) in the classification algorithms and comparing their performance metrics.

## Limitations

- Algorithmic specificity: The exact mathematical formulation of Double Discriminant Scoring (DDS1/DDS2) is described conceptually but not explicitly coded, creating implementation ambiguity.
- Parameter tuning: Hyperparameter settings for XGBoost and SVM are not specified, which could significantly affect comparative performance.
- Prevalence matching assumption: The prevalence-based cutoff approach assumes stable class distributions across training and testing splits, which may not hold under domain shift.

## Confidence

- **High confidence**: Prevalence-based threshold selection improves sensitivity for imbalanced medical data; DDS1 consistently outperforms other algorithms in reported scenarios.
- **Medium confidence**: The Bellman principle-based variable hierarchy reduces search space effectively; the performance advantage of DDS1 holds across different training/testing distributions.
- **Low confidence**: The generalizability of DDS1 to datasets with non-Gaussian distributions or severe covariance heterogeneity; the optimal variable hierarchy may miss important interaction effects.

## Next Checks

1. Implement DDS1 with exact union rule logic and verify it achieves higher true positive rates than individual discriminant functions on simulated Gaussian data.
2. Test prevalence-based cutoffs across training/testing splits with different class ratios to confirm robustness to distribution shifts.
3. Validate the Bellman-based variable hierarchy by comparing its performance against full subset enumeration on synthetic datasets with known optimal subsets.