---
ver: rpa2
title: 'Large Language Models for Automated Literature Review: An Evaluation of Reference
  Generation, Abstract Writing, and Review Composition'
arxiv_id: '2412.13612'
source_url: https://arxiv.org/abs/2412.13612
tags:
- llms
- literature
- references
- task
- title
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a framework to automatically evaluate large
  language models (LLMs) for their ability to perform key literature review tasks:
  reference generation, abstract writing, and literature review composition. The evaluation
  employs external tools to assess reference hallucination rates and semantic coverage
  and factual consistency against human-written content.'
---

# Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition

## Quick Facts
- arXiv ID: 2412.13612
- Source URL: https://arxiv.org/abs/2412.13612
- Authors: Xuemei Tang; Xufeng Duan; Zhenguang G. Cai
- Reference count: 4
- Primary result: Claude-3.5-Sonnet achieved highest accuracy in reference generation and abstract writing, but all models still produce hallucinated references.

## Executive Summary
This study introduces a framework to automatically evaluate large language models (LLMs) for literature review automation tasks. The framework assesses reference generation accuracy, abstract writing quality, and literature review composition through external validation tools measuring hallucination rates and semantic consistency. Results show that even state-of-the-art models like Claude-3.5-Sonnet, GPT-4o, and Qwen-2.5 generate hallucinated references, with performance varying significantly across academic disciplines. The study demonstrates that contextual constraints during literature review writing improve reference accuracy compared to standalone reference generation tasks.

## Method Summary
The study collected 1,106 literature reviews from 51 journals across five disciplines from Annual Reviews. Three tasks were designed for LLMs: reference generation, abstract writing, and literature review composition. External tools were used for evaluation: Semantic Scholar API verified reference accuracy, while NLI models (TRUE and GPT-4o) assessed factual consistency. The framework measured reference hallucination rates, semantic similarity between generated and human-written abstracts, and factual consistency across disciplines. Claude-3.5-Sonnet was evaluated against GPT-4o and Qwen-2.5 for performance comparison.

## Key Results
- Claude-3.5-Sonnet achieved the highest accuracy scores and title search rates across all evaluation tasks
- Reference hallucination rates remained significant across all models, even for the best-performing Claude-3.5-Sonnet
- Model performance varied substantially by discipline, with Mathematics showing highest accuracy and Chemistry/Technology showing lowest accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Claude-3.5-Sonnet generates more accurate references due to superior training on academic corpora.
- Mechanism: Claude-3.5's architecture and training data include more domain-specific academic texts, improving its ability to generate accurate reference metadata.
- Core assumption: The model's performance advantage is directly tied to the quality and scope of its training data.
- Evidence anchors:
  - [abstract] "Claude-3.5-Sonnet outperformed others in accuracy and factual consistency, particularly excelling in reference generation and abstract writing."
  - [section] "Claude-3.5-Sonnet achieves the highest accuracy score and title search rate."
- Break condition: If newer models trained on larger or more specialized datasets outperform Claude-3.5, this mechanism no longer holds.

### Mechanism 2
- Claim: Generating references within the context of a literature review reduces hallucination rates.
- Mechanism: The act of writing a literature review provides contextual constraints that guide the LLM to cite real sources, reducing hallucination compared to standalone reference generation.
- Core assumption: The textual context of a literature review anchors reference generation to real-world sources.
- Evidence anchors:
  - [section] "Compared to the accuracy scores in Task 1, all LLMs exhibit a significant increase in accuracy when generating references in Task 3."
  - [section] "When LLMs are tasked with generating references for the literature review, the accuracy of the generated references increases significantly."
- Break condition: If the context provided in Task 3 is removed or altered, and hallucination rates increase, this mechanism would be invalidated.

### Mechanism 3
- Claim: Academic disciplines with structured formats (e.g., Mathematics) yield higher reference generation accuracy.
- Mechanism: Disciplines with standardized citation formats and metadata (like Mathematics) reduce ambiguity, improving LLM accuracy in generating correct references.
- Core assumption: Structured academic fields have more consistent reference metadata, making it easier for LLMs to generate accurate citations.
- Evidence anchors:
  - [section] "Almost all models exhibit the highest accuracy in the Mathematics discipline."
  - [section] "The accuracy of reference generation in Task 1 for Claude-3.5, GPT-4o, and Qwen-2.5 shows a consistent trend across all dimensions, with the highest accuracy observed in the title dimension."
- Break condition: If reference generation accuracy is similar across disciplines when using structured citation templates, this mechanism would be weakened.

## Foundational Learning

- Concept: Hallucination in LLMs
  - Why needed here: Understanding why LLMs generate fake references is critical to evaluating their reliability in literature reviews.
  - Quick check question: What is the difference between open-domain and closed-domain hallucinations in LLMs?

- Concept: Semantic similarity metrics
  - Why needed here: Evaluating the quality of generated abstracts requires measuring how closely they match human-written content.
  - Quick check question: How does cosine similarity differ from ROUGE in measuring semantic coverage?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG can enhance LLMs' domain-specific knowledge, which is relevant to improving literature review generation.
  - Quick check question: How does RAG reduce hallucination in LLM-generated references?

## Architecture Onboarding

- Component map: Data collection -> LLM task execution -> Evaluation pipeline -> Cross-disciplinary analysis
- Critical path:
  1. Collect and preprocess dataset from Annual Reviews
  2. Generate outputs for all three tasks using selected LLMs
  3. Evaluate reference accuracy using external search APIs
  4. Assess semantic similarity and factual consistency
  5. Analyze performance across disciplines
- Design tradeoffs:
  - Using human-written reviews as ground truth simplifies evaluation but may not capture all nuances of high-quality literature reviews
  - Relying on external APIs (e.g., Semantic Scholar) for reference validation introduces dependency on third-party services
  - Evaluating only title and first author accuracy may overlook errors in other metadata fields
- Failure signatures:
  - Low reference accuracy across all models suggests issues with training data or task design
  - Inconsistent performance across disciplines may indicate model bias or lack of domain-specific training
  - High semantic similarity but low factual consistency suggests models generate fluent but inaccurate content
- First 3 experiments:
  1. Test LLM performance on a smaller, manually curated dataset to validate evaluation pipeline
  2. Compare hallucination rates with and without RAG augmentation to measure its impact
  3. Evaluate model performance on structured vs. unstructured citation formats to test disciplinary bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be optimized to reduce hallucination rates in reference generation across different academic disciplines?
- Basis in paper: [explicit] The paper highlights that hallucination rates in references remain a significant challenge for LLMs, with performance varying across disciplines.
- Why unresolved: The study identifies the problem but does not propose specific optimization strategies or training techniques to address hallucinations in references.
- What evidence would resolve it: Comparative experiments testing various optimization methods (e.g., fine-tuning on discipline-specific datasets, integrating external verification tools) and their impact on hallucination rates.

### Open Question 2
- Question: What factors contribute to the observed differences in LLM performance across academic disciplines, particularly the higher accuracy in Mathematics versus lower accuracy in Chemistry and Technology?
- Basis in paper: [explicit] The paper notes significant performance differences across disciplines, with higher accuracy in Mathematics and lower accuracy in Chemistry and Technology.
- Why unresolved: The study identifies these differences but does not explore underlying causes such as training data composition, terminology complexity, or citation practices.
- What evidence would resolve it: Analysis of training data distribution across disciplines and correlation studies between training data characteristics and performance metrics.

### Open Question 3
- Question: How does the inclusion of references in the literature review generation task impact the factual consistency of the generated content?
- Basis in paper: [explicit] The paper observes that generating references during literature review writing improves reference accuracy, suggesting a relationship between reference generation and factual consistency.
- Why unresolved: The study demonstrates this correlation but does not investigate the causal mechanisms or optimal reference integration strategies.
- What evidence would resolve it: Controlled experiments comparing factual consistency with and without reference generation, and analysis of how reference placement and density affect overall content quality.

## Limitations

- The evaluation framework relies heavily on external tools (Semantic Scholar API, NLI models) whose performance characteristics are not fully characterized
- The focus on title and first author accuracy for reference evaluation may miss errors in other critical metadata fields like journal names or publication years
- The corpus of 1,106 literature reviews may not represent all academic disciplines or publication types comprehensively

## Confidence

- **High Confidence**: The finding that Claude-3.5-Sonnet outperforms other models in reference generation and abstract writing is supported by robust evaluation metrics across multiple tasks
- **Medium Confidence**: The claim that context reduces hallucination rates is well-supported by experimental results but requires further validation with different contextual prompts
- **Low Confidence**: The assertion about disciplinary differences in reference accuracy is based on limited data and may reflect dataset biases rather than inherent model limitations

## Next Checks

1. Test the evaluation framework on a manually curated dataset with ground-truth metadata to validate the accuracy of external tool assessments
2. Implement a blind review process where human experts evaluate generated literature reviews without knowing which model produced them
3. Conduct ablation studies to isolate the impact of different prompt components on reference hallucination rates