---
ver: rpa2
title: 'Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey'
arxiv_id: '2412.18619'
source_url: https://arxiv.org/abs/2412.18619
tags:
- arxiv
- image
- multimodal
- language
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews the Next Token Prediction (NTP)
  paradigm as a unified framework for multimodal intelligence, encompassing both understanding
  and generation tasks across text, image, video, and audio modalities. The survey
  introduces a comprehensive taxonomy covering multimodal tokenization (discrete and
  continuous), MMNTP model architectures (compositional and unified), unified task
  representation, datasets and evaluation, and open challenges.
---

# Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey

## Quick Facts
- **arXiv ID**: 2412.18619
- **Source URL**: https://arxiv.org/abs/2412.18619
- **Reference count**: 40
- **Primary result**: Introduces a unified framework for multimodal intelligence using Next Token Prediction (NTP) paradigm across text, image, video, and audio modalities

## Executive Summary
This comprehensive survey systematically examines the Next Token Prediction (NTP) paradigm as a unified framework for multimodal intelligence, covering both understanding and generation tasks across text, image, video, and audio modalities. The survey provides a detailed taxonomy of multimodal tokenization methods (discrete and continuous), MMNTP model architectures (compositional and unified), unified task representation, datasets and evaluation frameworks. It demonstrates how NTP extends language modeling to multimodal domains by transforming raw multimodal data into sequential tokens, enabling large models to handle diverse tasks through a single autoregressive objective. The survey serves as a foundational reference for advancing multimodal intelligence research.

## Method Summary
The survey employs a systematic review methodology to analyze the MMNTP paradigm, covering tokenization methods, model architectures, training objectives, datasets, and evaluation benchmarks. It introduces comprehensive taxonomies categorizing multimodal tokenization (discrete vs continuous approaches), MMNTP model architectures (compositional vs unified designs), and unified task representation. The survey analyzes scaling challenges, modality interference issues, and computational inefficiencies in large-scale multimodal pretraining. Through extensive coverage of training datasets and evaluation benchmarks, it provides a holistic view of the current state and future directions in multimodal intelligence research.

## Key Results
- Comprehensive taxonomy covering multimodal tokenization (discrete and continuous), MMNTP model architectures (compositional and unified), unified task representation, datasets and evaluation
- Detailed analysis of modality interference, token granularity balancing, and computational inefficiency challenges in MMNTP
- Extensive coverage of training datasets, evaluation benchmarks, and emerging research trends in multimodal intelligence

## Why This Works (Mechanism)
The MMNTP paradigm works by transforming raw multimodal data into sequential tokens that can be processed by autoregressive models using a unified next-token prediction objective. This approach leverages the success of language modeling by extending it to multiple modalities through appropriate tokenization methods. The unified framework enables cross-modal learning where information from one modality can inform predictions in others, creating synergies that improve overall multimodal understanding and generation capabilities.

## Foundational Learning
1. **Autoregressive modeling** - Sequential prediction where each token is predicted based on previous tokens; needed for consistent token-by-token generation across modalities; quick check: verify model can generate coherent sequences without skipping or repeating tokens.

2. **Tokenization methods** - Discrete (tokenizers like BPE, SentencePiece) vs continuous (latent representations, VAEs); needed to convert raw multimodal data into model-compatible formats; quick check: ensure tokenized sequences preserve semantic information across modalities.

3. **Cross-modal fusion** - Integration of information from multiple modalities during training; needed to enable multimodal reasoning and generation; quick check: verify model can use information from one modality to improve predictions in another.

4. **Scaling laws** - Relationship between model size, dataset size, and performance; needed to understand computational requirements and performance ceilings; quick check: measure performance gains as model and data scale increase.

5. **Modality interference** - Degraded performance when learning across multiple modalities; needed to understand limitations and design better architectures; quick check: monitor performance degradation when adding new modalities.

## Architecture Onboarding

**Component Map**: Raw multimodal data -> Tokenization -> Sequential tokens -> MMNTP model (compositional/unified) -> Next token prediction -> Generated output

**Critical Path**: Tokenization → MMNTP model architecture → Training objective → Evaluation

**Design Tradeoffs**: 
- Discrete tokenization offers interpretability but may lose fine-grained information
- Continuous tokenization preserves more information but reduces interpretability
- Compositional architectures offer modularity but may suffer from modality interference
- Unified architectures enable better cross-modal fusion but are more complex to train

**Failure Signatures**:
- Mode collapse when one modality dominates training
- Loss of fine-grained details in continuous representations
- Poor cross-modal generalization due to insufficient shared representation space
- Computational inefficiency at scale limiting practical deployment

**First 3 Experiments**:
1. Compare discrete vs continuous tokenization on a multimodal understanding task (e.g., image-text retrieval)
2. Ablation study of cross-modal attention mechanisms in unified MMNTP architectures
3. Scaling analysis measuring performance gains vs computational costs across different model sizes

## Open Questions the Paper Calls Out
The survey identifies several fundamental challenges including modality interference during cross-modal learning, difficulty balancing token granularity across modalities, and computational inefficiency of large-scale multimodal pretraining. It notes that the proposed taxonomy may not capture emerging approaches that deviate from the autoregressive paradigm, and acknowledges that theoretical understanding of scaling laws in multimodal models remains limited despite empirical observations.

## Limitations
- Acknowledges modality interference and computational inefficiency as fundamental challenges in MMNTP
- Taxonomy may not capture emerging approaches that deviate from autoregressive paradigm
- Rapidly evolving nature of multimodal datasets means some recent developments may not be fully represented
- Scaling laws analysis remains largely empirical with limited theoretical understanding

## Confidence
- Taxonomy of multimodal tokenization methods (Discrete vs Continuous): High
- Classification of MMNTP model architectures (Compositional vs Unified): High
- Coverage of training objectives and unified task representation: Medium
- Analysis of modality interference and computational challenges: Medium
- Evaluation of current benchmarks and datasets: Medium

## Next Checks
1. Conduct systematic replication studies comparing different tokenization approaches (discrete vs continuous) on standardized multimodal understanding and generation tasks to quantify trade-offs in performance and efficiency.

2. Perform ablation studies on MMNTP model architectures to empirically validate claims about modality interference and identify optimal design patterns for cross-modal fusion.

3. Design and execute controlled experiments testing the survey's claims about scaling limitations by training MMNTP models at different scales while measuring performance gains, computational costs, and modality-specific degradation patterns.