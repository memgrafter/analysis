---
ver: rpa2
title: 'Chasing Random: Instruction Selection Strategies Fail to Generalize'
arxiv_id: '2410.15225'
source_url: https://arxiv.org/abs/2410.15225
tags:
- selection
- random
- cost
- budget
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Data selection methods for instruction-tuning are evaluated across
  datasets, budgets, and benchmarks. No strategy consistently outperforms random selection;
  selection costs often exceed full-dataset training costs.
---

# Chasing Random: Instruction Selection Strategies Fail to Generalize

## Quick Facts
- arXiv ID: 2410.15225
- Source URL: https://arxiv.org/abs/2410.15225
- Reference count: 40
- No strategy consistently outperforms random selection; selection costs often exceed full-dataset training costs

## Executive Summary
This paper investigates whether instruction selection strategies can improve instruction-tuning performance compared to random sampling. Across multiple datasets, budgets, and benchmarks, the authors find that no selection strategy consistently outperforms random selection. Selection costs often exceed the cost of training on the full dataset, with only marginal performance gains. The study reveals that different benchmarks disagree on strategy performance trends, making it difficult to identify optimal approaches. Random sampling emerges as a surprisingly competitive and cost-effective baseline.

## Method Summary
The authors evaluate five selection strategies (Alpagasus, Cherry, DEITA, Longest, Random) across four source datasets (FLAN, DOLLY, EVOL, ALPACA) at three different budget sizes (1000, 5000, 10000 samples). They fine-tune LLaMa-7B using three hyperparameter configurations and evaluate performance across four benchmarks (IFE VAL, ALPACA EVAL, LLMBAR, OPENLLM). The study systematically compares selection costs against performance gains and analyzes correlation between different evaluation metrics.

## Key Results
- No selection strategy consistently outperforms random selection across all datasets, budgets, and benchmarks
- Selection costs often exceed the cost of training on the full dataset while yielding only marginal performance improvements
- Different benchmarks show poor correlation in strategy performance rankings, with some showing negative correlation at larger budgets
- Random selection proves surprisingly competitive, matching or exceeding specialized strategies in many conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selection strategy performance varies significantly with the choice of evaluation benchmark.
- Mechanism: Different benchmarks measure different aspects of instruction-following ability. ALPACA EVAL focuses on length-controlled win rates, IFE VAL on prompt and instruction-level accuracy, and OPENLLM on task-specific accuracy. A strategy that excels at one may underperform on another due to these different focus areas.
- Core assumption: Instruction-following competence is multidimensional and not captured uniformly across benchmarks.
- Evidence anchors:
  - [abstract] "Benchmarks disagree on strategy performance trends, making it difficult to identify optimal approaches."
  - [section] "Measuring instruction following capabilities is generally more complex than task-specific accuracy evaluation as instruction following models are expected to demonstrate a wide range of capabilities."
  - [corpus] Weak - related papers don't directly address benchmark disagreement in strategy performance.

### Mechanism 2
- Claim: Selection strategies fail to consistently outperform random baselines across different budgets and datasets.
- Mechanism: Selection strategies are sensitive to their experimental setup - changing budget or source dataset can dramatically alter their relative performance. Random sampling remains consistently competitive because it avoids the setup-dependent weaknesses of specialized strategies.
- Core assumption: The effectiveness of selection strategies is not stable across reasonable variations in experimental conditions.
- Evidence anchors:
  - [abstract] "No strategy consistently outperforms random selection; selection costs often exceed full-dataset training costs."
  - [section] "We observe similarly poor correlations between the performance trends of selection strategies when we do a pair wise comparison between other studied benchmarks like OPENLLM and LLMBAR."
  - [corpus] Weak - related papers focus on proposing new selection strategies rather than analyzing their generalization failures.

### Mechanism 3
- Claim: The cost of selection often exceeds the cost of training on the full dataset, with marginal gains.
- Mechanism: Selection strategies require computational resources for scoring, clustering, or similarity calculations across the entire dataset. These costs accumulate and can surpass the cost of simply training on all available data, especially when the gains over random selection are small.
- Core assumption: The computational overhead of selection is significant relative to the dataset size and the performance gains achieved.
- Evidence anchors:
  - [abstract] "selection costs often exceed full-dataset training costs, yielding only marginal and sometimes no gains"
  - [section] "In Figure 6, we plot the cost of selection per dataset compared to the performances of Mselected on IFEVAL (all budgets are included in §B.3 in the Appendix)."
  - [corpus] Weak - related papers mention efficiency but don't analyze the cost-benefit tradeoff as thoroughly.

## Foundational Learning

- Concept: Instruction tuning and its goals
  - Why needed here: The paper evaluates selection strategies for instruction tuning datasets, so understanding what instruction tuning aims to achieve is crucial for interpreting results.
  - Quick check question: What is the primary goal of instruction tuning in language models?

- Concept: Data selection strategies and their diversity
  - Why needed here: The paper compares multiple selection strategies (Alpagasus, Longest, Cherry, DEITA) with random baselines, requiring understanding of how these strategies differ.
  - Quick check question: How does the DEITA strategy differ from the Longest strategy in selecting instructions?

- Concept: Evaluation metrics for instruction-following
  - Why needed here: Different benchmarks use different metrics (win rate, accuracy, etc.), and understanding these is essential for interpreting why strategies perform differently across benchmarks.
  - Quick check question: What does prompt-level accuracy measure in the IFE VAL benchmark?

## Architecture Onboarding

- Component map: Source datasets (FLAN, DOLLY, EVOL, ALPACA) -> Selection Strategies (Alpagasus, Cherry, DEITA, Longest, Random) -> Model Training (LLaMa-7B fine-tuning) -> Evaluation Benchmarks (IFE VAL, ALPACA EVAL, LLMBAR, OPENLLM)
- Critical path: Dataset → Selection Strategy → Model Training → Evaluation Benchmark. The selection step is the critical path element being evaluated.
- Design tradeoffs: Choosing between selection strategies involves balancing potential performance gains against computational costs and the risk of poor generalization. Random selection offers consistency and low cost but may miss opportunities for improvement.
- Failure signatures: A selection strategy fails when it underperforms random selection, when its performance varies drastically across benchmarks, or when its selection cost exceeds training cost on the full dataset.
- First 3 experiments:
  1. Replicate the comparison between random selection and each strategy on a single dataset (e.g., DOLLY) at a fixed budget (e.g., 1000 samples) using one benchmark (e.g., ALPACA EVAL) to verify the basic finding.
  2. Test the same setup across multiple budgets (e.g., 1000, 5000, 10000) to observe how strategy performance changes with scale.
  3. Evaluate the same selected subsets across multiple benchmarks (e.g., ALPACA EVAL and IFE VAL) to confirm that strategy performance varies by evaluation metric.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do instruction selection strategies fail to generalize across different datasets and selection budgets?
- Basis in paper: [explicit] The paper states "Our results indicate that selection strategies generalize poorly, often failing to consistently outperform even random baselines."
- Why unresolved: The paper identifies this as a key finding but does not explore the underlying reasons why strategies perform inconsistently across different experimental conditions.
- What evidence would resolve it: Comparative analysis of strategy performance across varying dataset characteristics and budgets, identifying specific factors that influence strategy effectiveness.

### Open Question 2
- Question: How can we reliably predict which instruction selection strategy will work best for a given dataset and budget?
- Basis in paper: [explicit] "Our conclusions on the performance of random baselines in this setting can be considered aligned to contemporary work demonstrating the unreasonable effectiveness of random baselines in several other domains."
- Why unresolved: The paper finds that random baselines are competitive but does not provide a framework for predicting when non-random strategies might be superior.
- What evidence would resolve it: Development of a predictive model that considers dataset properties, budget size, and benchmark type to recommend optimal selection strategies.

### Open Question 3
- Question: Why do different instruction-following benchmarks disagree on the performance trends of selection strategies?
- Basis in paper: [explicit] "In Figure 4, we show the correlation between the Win-Rates for Mselected and their IFE VAL accuracy. The performance trends on both benchmarks appear very weakly correlated for our lowest budget, and show almost negative correlation after scaling Mselected to the largest budget."
- Why unresolved: The paper observes this phenomenon but does not investigate the root causes of benchmark disagreement.
- What evidence would resolve it: Systematic analysis of benchmark design, evaluation criteria, and task coverage to understand why performance trends diverge across benchmarks.

## Limitations
- The computational cost analysis depends heavily on specific infrastructure assumptions (8 A6000 GPUs, 128 CPUs) that may not reflect real-world deployments.
- The evaluation relies on only four benchmarks, which may not capture the full diversity of instruction-following capabilities.
- The paper doesn't explore whether ensemble methods or hybrid strategies could overcome individual strategy limitations.

## Confidence
- **High confidence**: The finding that random selection remains competitive with specialized strategies across multiple conditions is well-supported by the experimental data and consistent across different analysis approaches.
- **Medium confidence**: The claim that benchmark disagreement makes strategy evaluation difficult is supported by the data but could benefit from additional benchmarks or correlation analysis to strengthen the argument.
- **Low confidence**: The assertion that selection costs often exceed training costs is based on specific infrastructure assumptions that may not generalize to all settings.

## Next Checks
1. **Benchmark Diversity Validation**: Test the same selection strategies on additional instruction-following benchmarks not included in the original study to determine if the observed benchmark disagreement extends to a broader range of evaluation metrics.

2. **Infrastructure Cost Validation**: Replicate the cost calculations using different hardware configurations (e.g., cloud vs. on-premise, different GPU types) to verify whether the cost-benefit tradeoff remains consistent across deployment scenarios.

3. **Strategy Ensemble Validation**: Implement and evaluate hybrid selection strategies that combine multiple approaches (e.g., DEITA for initial filtering followed by Longest for diversity) to test whether ensemble methods can overcome the limitations of individual strategies while maintaining reasonable computational costs.