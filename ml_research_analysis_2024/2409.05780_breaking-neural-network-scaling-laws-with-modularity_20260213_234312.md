---
ver: rpa2
title: Breaking Neural Network Scaling Laws with Modularity
arxiv_id: '2409.05780'
source_url: https://arxiv.org/abs/2409.05780
tags:
- task
- modular
- training
- module
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generalizing on high-dimensional
  tasks with limited training data, which is difficult for standard nonmodular neural
  networks due to their exponential sample complexity with respect to input dimensionality.
  The authors propose a modular learning framework that exploits the compositional
  structure of tasks to break this exponential scaling law.
---

# Breaking Neural Network Scaling Laws with Modularity

## Quick Facts
- arXiv ID: 2409.05780
- Source URL: https://arxiv.org/abs/2409.05780
- Authors: Akhilan Boopathy; Sunshine Jiang; William Yue; Jaedong Hwang; Abhiram Iyer; Ila Fiete
- Reference count: 40
- One-line primary result: Modular networks with kernel-initialized projections achieve better generalization on high-dimensional tasks with limited data compared to monolithic and randomly initialized modular networks

## Executive Summary
This paper addresses the challenge of generalizing on high-dimensional tasks with limited training data, which is difficult for standard nonmodular neural networks due to their exponential sample complexity with respect to input dimensionality. The authors propose a modular learning framework that exploits the compositional structure of tasks to break this exponential scaling law. Their core method involves a novel learning rule that learns initializations for module projections by optimizing a kernel-based objective, aligning the network's modules with the task's underlying modular structure. This initialization is then refined during task training.

## Method Summary
The method combines modular network architecture with a kernel-based initialization technique. The modular architecture consists of K modules, each receiving a low-dimensional projection of the input, processing it through a sub-network, and summing the outputs. The kernel-based initialization learns these module projections by optimizing a kernel objective that minimizes the norm of module parameters needed to fit the training data. This initialization is then refined during standard task training. The approach is tested on synthetic sine wave regression tasks and Compositional CIFAR-10, demonstrating improved generalization over monolithic and randomly initialized modular baselines.

## Key Results
- Modular networks avoid exponential sample complexity with task dimensionality by exploiting bottleneck projections that reduce effective input dimension per module
- Kernel-based initialization aligns module projections with true task structure, improving generalization on both in-distribution and out-of-distribution test data
- Theoretical linear approximation of neural networks accurately predicts generalization trends and validates the modular analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular networks avoid exponential sample complexity with task dimensionality by exploiting bottleneck projections that reduce effective input dimension per module.
- Mechanism: Each module receives a low-dimensional projection of the full input, limiting the number of effective parameters needed to generalize per module. The full task can then be solved by combining these low-dimensional modules without exponential growth in sample requirements.
- Core assumption: Module projections create an input bottleneck (low b relative to full m) that isolates the exponential dependence on dimensionality to the bottleneck size, not the full task dimension.
- Evidence anchors:
  - [abstract]: "modular networks' sample complexity is independent of task dimensionality"
  - [section 4.1]: Theorem 2 shows test loss depends linearly on m only through the bottleneck term mb, not exponentially
  - [corpus]: Weak—no direct corpus evidence on modular bottleneck analysis
- Break condition: If module projections are not low-dimensional (b ≈ m) or if modules cannot be combined additively, the exponential scaling may reappear.

### Mechanism 2
- Claim: Learning module projections via a kernel-based initialization aligns module inputs with the true modular structure, improving generalization.
- Mechanism: By minimizing the norm of module parameters needed to fit the training data across different projection directions, the method encourages module projections to align with the true task modules, leading to more efficient learning.
- Core assumption: True task modules can be approximated by low-dimensional projections, and the kernel optimization finds projections that minimize parameter norm.
- Evidence anchors:
  - [section 4.2]: "we optimize ˆUi to minimize the squared norm of θi" and "we hypothesize that if the ˆUi is far from Ui, then the norm of theθi will be large"
  - [section 4.3]: Empirical validation shows learned module projections cluster around true modules (Fig 5)
  - [corpus]: Weak—no direct corpus evidence on this specific kernel initialization method
- Break condition: If the kernel is not well-suited to the task structure or if the number of modules is mis-specified, initialization may not align with true modules.

### Mechanism 3
- Claim: The theoretical model treating NNs as linear functions of parameters accurately predicts generalization trends, validating the modular analysis.
- Mechanism: By linearizing the NN output as a function of parameters, exact closed-form expressions for training and test error can be derived, capturing key phenomena like double descent and exponential scaling with dimensionality.
- Core assumption: NN learning can be approximated as linear regression on parameters, at least in certain regimes (e.g., infinite width, early training).
- Evidence anchors:
  - [section 3.2]: Theorem 1 provides closed-form expressions matching empirical trends (Fig 2)
  - [section 3.3]: Empirical validation on sine wave task shows theory matches training/test loss trends
  - [corpus]: Moderate—prior work on neural tangent kernel and linearized models supports this assumption
- Break condition: If the linearization assumption breaks down (e.g., for very deep networks, late training stages), the theoretical predictions may diverge from empirical results.

## Foundational Learning

- Concept: Linear approximation of neural network training dynamics (Neural Tangent Kernel regime)
  - Why needed here: The theoretical analysis relies on treating the NN as a linear function of its parameters to derive closed-form generalization bounds.
  - Quick check question: What assumption about the NN's training dynamics allows treating it as linear regression on parameters?

- Concept: Sample complexity scaling with input dimensionality
  - Why needed here: The paper's central claim is that modular networks break the exponential scaling law with dimensionality that affects monolithic networks.
  - Quick check question: How does the sample complexity of a monolithic network scale with the intrinsic dimensionality of the input according to prior work?

- Concept: Kernel methods and kernel ridge regression
  - Why needed here: The module initialization method uses kernel-based optimization to align module projections with the task structure.
  - Quick check question: What is the relationship between minimizing parameter norm in kernel regression and finding relevant features?

## Architecture Onboarding

- Component map: Input x → K module projections (ˆUj) → K sub-networks (ˆyj) → Summed outputs
- Critical path: 1) Construct dataset with modular structure, 2) Initialize module projections using kernel method, 3) Train all module parameters with task loss
- Design tradeoffs: More modules (larger K) can better capture task structure but increase computational cost and risk of overfitting. Kernel hyperparameters (σ) must be tuned for the task.
- Failure signatures: Poor generalization compared to baselines may indicate misalignment of module projections or insufficient module capacity. Instability during training may suggest issues with the kernel initialization or module architecture.
- First 3 experiments:
  1. Reproduce the sine wave regression experiment comparing monolithic, modular baseline, and our method on a low-dimensional task (k=m=5).
  2. Test the modular method on the nonlinear sine wave regression variant to verify it extends beyond linear projections.
  3. Apply the method to Compositional CIFAR-10 with k=2 to observe the effect of modularity on a real image dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the theoretical advantage of modular networks (sample complexity independent of task dimension) hold for more general modular architectures beyond the specific form analyzed in the paper (sum of module outputs with linear or nonlinear projections)?
- Basis in paper: [inferred] The paper restricts analysis to a specific modular setting (linear projections, summed outputs) and states "we expect that future analyses can demonstrate the benefits of modularity more widely."
- Why unresolved: The paper only provides theoretical analysis for a specific modular architecture and does not prove the result for general modular architectures like those with routing mechanisms or attention-based modules.
- What evidence would resolve it: Extending the theoretical analysis to more general modular architectures and comparing their sample complexity to monolithic networks for modular tasks.

### Open Question 2
- Question: How does the choice of kernel function in the modular learning rule affect the quality of module initialization and subsequent generalization performance?
- Basis in paper: [explicit] The paper uses a specific kernel function tailored to the task structure but mentions "we tailor κ to the modular structure of the problem we consider" suggesting other choices are possible.
- Why unresolved: The paper only tests one kernel function per task type and does not systematically explore how different kernel choices impact performance.
- What evidence would resolve it: Systematic experiments comparing different kernel functions on the same tasks to measure their impact on module initialization quality and final generalization.

### Open Question 3
- Question: Can the modular learning rule be extended to tasks where the modular structure is not known a priori or is only partially known?
- Basis in paper: [inferred] The paper assumes knowledge of the number of modules K and uses a kernel that assumes a specific modular structure, but mentions "meta-learning algorithms can discover and learn the modules without prespecifying them."
- Why unresolved: The current method requires specifying the number of modules and uses a kernel designed for a specific modular structure, which may not generalize to tasks with unknown or partially known modular structure.
- What evidence would resolve it: Developing and testing an extension of the modular learning rule that can automatically discover the modular structure of a task or handle cases where only partial information about the modular structure is available.

## Limitations
- The kernel-based initialization method's effectiveness heavily depends on choosing appropriate kernel functions and hyperparameters, which may not generalize across diverse tasks
- The theoretical analysis assumes infinite-width networks and linearized training dynamics, which may not hold for practical finite-width architectures
- The Compositional CIFAR-10 experiment uses only k=2 modules, which is a relatively simple compositional structure

## Confidence
- **High confidence**: Modular networks can avoid exponential sample complexity when modules receive low-dimensional projections
- **Medium confidence**: Kernel-based initialization can align module projections with true task structure
- **Medium confidence**: The linear approximation accurately predicts generalization trends

## Next Checks
1. **Cross-task kernel validation**: Test the kernel-based initialization method on a suite of tasks with varying compositional structures (e.g., k=3-5 modules) using the same kernel function to assess robustness and identify when kernel selection becomes critical.

2. **Finite-width scaling study**: Systematically vary network width in both theoretical predictions and empirical results to quantify when the infinite-width assumption breaks down and how this affects the claimed sample complexity advantages.

3. **Nonlinear module architectures**: Replace the linear projection modules with small multilayer perceptrons and evaluate whether the theoretical benefits persist, testing the assumption that linear bottlenecks are sufficient for breaking exponential scaling.