---
ver: rpa2
title: 'Turning Trash into Treasure: Accelerating Inference of Large Language Models
  with Token Recycling'
arxiv_id: '2408.08696'
source_url: https://arxiv.org/abs/2408.08696
tags:
- tokens
- decoding
- tree
- draft
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Token Recycling is a speculative decoding method that accelerates
  large language model inference by reusing candidate tokens discarded during greedy
  decoding. It stores these candidate tokens in an adjacency matrix and uses a BFS-like
  algorithm to construct draft trees, which are then validated using tree attention.
---

# Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling

## Quick Facts
- arXiv ID: 2408.08696
- Source URL: https://arxiv.org/abs/2408.08696
- Reference count: 23
- Primary result: Achieves approximately 2x speedup across all model sizes using <2MB additional storage

## Executive Summary
Token Recycling introduces a novel approach to accelerate large language model inference by storing and reusing candidate tokens that are typically discarded during greedy decoding. The method maintains an adjacency matrix of top-k candidate tokens for each vocabulary token and uses a BFS-like algorithm to construct draft trees for speculative decoding. By leveraging tree attention for verification and continuously updating the matrix with new candidate tokens, the approach achieves significant speedup without requiring model training. The method demonstrates superior performance compared to both previous train-free approaches and even some training-based methods, while maintaining a minimal storage footprint of less than 2MB.

## Method Summary
Token Recycling accelerates LLM inference by storing candidate tokens rejected during greedy decoding in an adjacency matrix, then reusing these tokens to construct draft trees for speculative decoding. The method uses a static, imbalanced tree structure with 81 nodes across 6 layers and employs a BFS-like retrieval algorithm to efficiently access candidate tokens. Draft sequences are verified using tree attention, which modifies the attention mask to validate multiple sequences simultaneously. The adjacency matrix is continuously updated with new candidate tokens from the decoding process, allowing the system to adapt to different generation patterns. A hot start initialization strategy leverages patterns from previous generations to improve initial performance.

## Key Results
- Achieves approximately 2x speedup across all model sizes (7B, 13B, 33B, 70B parameters)
- Requires less than 2MB of additional storage
- Outperforms previous train-free approaches by over 30%
- Exceeds a widely used training-based method by 25%

## Why This Works (Mechanism)

### Mechanism 1: Candidate Token Reuse
Candidate tokens from rejected tokens contain useful patterns for future generations. Rejected tokens are stored in an adjacency matrix and reused as draft tokens when their corresponding tokens reappear in future sequences. Core assumption: Tokens rejected in one generation may be accepted in future generations due to different context.

### Mechanism 2: Efficient Tree Construction
BFS-like algorithm efficiently constructs draft trees using adjacency matrix. Static and imbalanced tree structure allows parallel retrieval of candidate tokens with minimal computational overhead. Core assumption: The tree structure can be predetermined and remains effective across different decoding steps.

### Mechanism 3: Hot Start Initialization
Hot start initialization significantly improves performance. Inheriting from existing adjacency matrix provides better starting point than random or zero initialization. Core assumption: Common patterns captured in previous generations generalize well to new queries.

## Foundational Learning

- **Adjacency matrix data structure**
  - Why needed here: Efficiently stores and retrieves candidate tokens for each vocabulary token
  - Quick check question: How does the adjacency matrix differ from other data structures like tries in terms of parallel processing capabilities?

- **Breadth-First Search (BFS) algorithm**
  - Why needed here: Constructs draft trees by exploring all candidate tokens at each level before moving deeper
  - Quick check question: Why is BFS more suitable than DFS for constructing draft trees in speculative decoding?

- **Tree attention mechanism**
  - Why needed here: Verifies multiple token sequences simultaneously by modifying the attention mask
  - Quick check question: How does tree attention differ from standard causal attention in terms of computational efficiency?

## Architecture Onboarding

- **Component map**: Adjacency matrix (storage: <2MB) -> Static tree structure (81 nodes, 6 layers) -> BFS-like retrieval algorithm -> Tree attention verification -> Matrix update mechanism

- **Critical path**: 
  1. Retrieve draft tree from adjacency matrix
  2. Verify draft tokens using tree attention
  3. Update adjacency matrix with new candidate tokens
  4. Select longest correct sequence

- **Design tradeoffs**:
  - Static vs dynamic tree structure: Static provides better performance but less flexibility
  - Matrix size vs coverage: Larger matrix covers more patterns but increases memory usage
  - Update frequency vs accuracy: More frequent updates provide better adaptation but increase computational overhead

- **Failure signatures**:
  - Performance degradation when sequences contain few repeated patterns
  - Memory overflow if matrix size increases significantly
  - Latency increase if tree depth or breadth becomes too large

- **First 3 experiments**:
  1. Compare performance with different adjacency matrix sizes (k=4, 8, 16)
  2. Test with different tree structures (balanced vs imbalanced)
  3. Evaluate hot start vs zero initialization across different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Token Recycling change with different candidate token top-k values?
- Basis in paper: The paper uses k=8 for the adjacency matrix but doesn't explore different values
- Why unresolved: The paper only uses k=8 as the default and doesn't provide an analysis of how varying k affects performance
- What evidence would resolve it: Experiments comparing MAT, Tokens/s, and speedup ratio across different k values (e.g., k=4, 8, 16) would clarify the optimal setting and trade-offs

### Open Question 2
- Question: Can dynamic tree structures outperform the static tree used in Token Recycling?
- Basis in paper: The paper acknowledges that dynamic trees could be employed but uses a static tree structure for efficiency
- Why unresolved: The paper only uses a static tree structure and mentions dynamic trees as future work without empirical comparison
- What evidence would resolve it: Direct comparison of static vs dynamic tree structures in terms of MAT, Tokens/s, and acceleration would determine if the additional complexity of dynamic trees is justified

### Open Question 3
- Question: How does Token Recycling perform on tasks with very low redundancy or high novelty requirements?
- Basis in paper: The paper shows good performance on SpecBench and MBPP, but doesn't explore extreme cases
- Why unresolved: The evaluation focuses on general benchmarks and code generation, but doesn't test scenarios with minimal repetition or highly novel content
- What evidence would resolve it: Testing on tasks like creative writing, highly technical documentation, or novel scientific discovery would reveal limitations in low-redundancy contexts

## Limitations

- Static tree structure may not generalize well across diverse sequence types and could limit effectiveness for complex language patterns
- Hot start initialization relies on assumption that patterns from previous generations transfer effectively, without thorough analysis of failure scenarios
- Claims of "negligible latency" for matrix preprocessing need more rigorous validation across different hardware configurations and sequence lengths

## Confidence

**High Confidence**: The core mechanism of storing rejected candidate tokens and reusing them for draft tree construction is well-defined and theoretically sound. The adjacency matrix approach for storage and the BFS-like retrieval algorithm are clearly specified with reasonable computational complexity.

**Medium Confidence**: The empirical speedup claims of approximately 2x across all model sizes are supported by experimental results, but the evaluation setup lacks detail about specific model architectures, sequence lengths, and hardware configurations. The comparison with training-based methods, while impressive, doesn't fully account for potential differences in implementation optimizations.

**Low Confidence**: The paper's claims about outperforming previous train-free approaches by over 30% and exceeding training-based methods by 25% are based on a limited set of benchmarks. The generalization across diverse domains, model sizes, and hardware platforms remains uncertain without more extensive validation.

## Next Checks

1. **Generalization Testing**: Evaluate Token Recycling performance across diverse sequence types (code, technical documentation, creative writing) and model sizes (7B, 13B, 70B parameters) to verify the claimed universal 2x speedup holds across all scenarios.

2. **Edge Case Analysis**: Systematically test sequences with minimal token repetition, highly unique vocabulary, and non-standard language patterns to identify break conditions where the token recycling mechanism fails to provide acceleration benefits.

3. **Resource Overhead Validation**: Measure actual memory usage and preprocessing latency on different hardware platforms (CPU, GPU, different memory bandwidths) to verify the claimed <2MB storage requirement and negligible preprocessing overhead hold true in practice.