---
ver: rpa2
title: Parameter Efficient Fine-tuning via Explained Variance Adaptation
arxiv_id: '2410.07170'
source_url: https://arxiv.org/abs/2410.07170
tags:
- lora
- rank
- tasks
- initialization
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Explained Variance Adaptation (EVA), a new
  initialization method for LoRA-based fine-tuning that uses incremental SVD on activation
  vectors to provably maximize the expected gradient signal. Unlike existing SVD-based
  approaches that rely on weight or gradient matrices, EVA captures the directions
  of highest activation variance, enabling faster convergence and better performance.
---

# Parameter Efficient Fine-tuning via Explained Variance Adaptation

## Quick Facts
- arXiv ID: 2410.07170
- Source URL: https://arxiv.org/abs/2410.07170
- Authors: Fabian Paischer; Lukas Hauzenberger; Thomas Schmied; Benedikt Alkin; Marc Peter Deisenroth; Sepp Hochreiter
- Reference count: 40
- Primary result: EVA improves average performance while reducing trainable parameters compared to LoRA and other PEFT variants

## Executive Summary
This paper introduces Explained Variance Adaptation (EVA), a new initialization method for LoRA-based fine-tuning that uses incremental SVD on activation vectors to provably maximize the expected gradient signal. Unlike existing SVD-based approaches that rely on weight or gradient matrices, EVA captures the directions of highest activation variance, enabling faster convergence and better performance. By globally sorting right-singular vectors by explained variance and redistributing ranks across weight matrices, EVA achieves adaptive rank allocation within a fixed budget, reducing trainable parameters. Across language generation, understanding, image classification, and reinforcement learning tasks, EVA consistently improves average performance while reducing parameter count compared to LoRA and other PEFT variants.

## Method Summary
EVA performs incremental SVD on minibatches of activation vectors to identify directions of maximum variance, then globally sorts right-singular vectors by explained variance ratio and redistributes ranks across weight matrices before initializing LoRA adapters. The method provably maximizes expected gradient signal magnitude under a statistical independence assumption between activations and gradients, and initializes matrix A with top variance-capturing singular vectors while setting B=0.

## Key Results
- EVA consistently improves average performance across language, vision, and RL tasks while reducing trainable parameters
- Global sorting of singular vectors by explained variance enables adaptive rank allocation that reduces parameter count
- Faster convergence observed compared to standard LoRA initialization
- EVA establishes new Pareto frontier in accuracy and efficiency for parameter-efficient fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initializing A with right-singular vectors from activation variance maximizes expected gradient signal magnitude.
- Mechanism: The top r right-singular vectors of activation data capture the directions of highest variance. Initializing LoRA matrix A with these directions aligns the parameter update space with the most informative directions in activation space, leading to larger expected gradient norms at the onset of fine-tuning.
- Core assumption: The gradient with respect to B can be approximated as statistically independent of the input activation vector x.
- Evidence anchors:
  - [abstract] "provably maximizing the expected gradient signal"
  - [section] "Theorem 3.2...maximizes the expected squared gradient norm: E[∥∂L/∂B∥²_F] ∝ Tr(AΣA⊤)"
  - [corpus] Weak - SVD-based LoRA initialization methods are cited in corpus but do not claim gradient signal maximization.
- Break condition: If the statistical independence assumption Cov(x,g)=0 does not hold, the gradient amplification effect diminishes.

### Mechanism 2
- Claim: Incremental SVD on minibatches of activations enables variance-optimal initialization without excessive memory overhead.
- Mechanism: Instead of computing SVD on the full activation matrix, the method performs incremental SVD updates on successive minibatches. This allows the algorithm to converge to the principal components of the entire dataset while keeping memory usage constant.
- Core assumption: The components of the SVD converge when the cosine similarity between successive updates exceeds a threshold.
- Evidence anchors:
  - [abstract] "performs incremental SVD on minibatches of activation vectors"
  - [section] "incrementally update V_i:r,: following the approach of Ross et al. (2008)...check whether V_i has converged by cosine similarity"
  - [corpus] Weak - incremental SVD is not discussed in corpus; only batch SVD on gradients or weights.
- Break condition: If the activation distribution changes significantly between minibatches, convergence may be slow or components may not capture the full variance.

### Mechanism 3
- Claim: Global sorting of right-singular vectors by explained variance enables adaptive rank allocation that reduces total trainable parameters.
- Mechanism: After computing SVD components for all weight matrices, the method globally sorts all right-singular vectors by their normalized singular values (explained variance ratio). This allows the algorithm to assign more ranks to weight matrices whose variance is spread across many components and fewer ranks to those concentrated in few components, staying within a fixed rank budget.
- Core assumption: The explained variance ratio is a good proxy for how many ranks each weight matrix needs to capture its activation variance.
- Evidence anchors:
  - [abstract] "by selecting the directions that capture the most activation-variance for a given rank budget, EVA accommodates adaptive ranks that reduce the number of trainable parameters"
  - [section] "we sort right singular vectors obtained for all weight matrices according to their explained variance ratio"
  - [corpus] Weak - other methods like AdaLoRA and LoRA-GA perform rank redistribution but do not claim using explained variance from activation SVD.
- Break condition: If the variance distribution changes drastically across layers or tasks, the global sorting may not optimally allocate ranks.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is the core mathematical operation used to identify the directions of maximum activation variance for initialization.
  - Quick check question: Given a matrix X, what do the right-singular vectors represent in terms of the data's variance structure?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The paper connects explained variance initialization to NTK theory, showing that initializing along dominant NTK directions can minimize generalization error.
  - Quick check question: How does the NTK relate to the speed of learning different directions in a neural network?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: EVA is a method for initializing LoRA adapters; understanding LoRA's mechanics is essential for implementing EVA.
  - Quick check question: In LoRA, what is the relationship between matrices A and B, and how do they modify the pre-trained weights?

## Architecture Onboarding

- Component map: Incremental SVD module -> Global sorting module -> Rank redistribution module -> LoRA initialization module
- Critical path: 1. Forward pass through model on minibatch to collect activations. 2. Incremental SVD update on collected activations. 3. Check convergence of SVD components. 4. After convergence, globally sort all right-singular vectors by explained variance. 5. Redistribute ranks across weight matrices. 6. Initialize LoRA adapters with selected components. 7. Proceed with standard LoRA fine-tuning.
- Design tradeoffs:
  - Batch size vs. initialization speed: Smaller batches reduce memory but increase SVD computation time.
  - Rank redistribution vs. uniformity: Adaptive ranks can reduce parameters but may hurt performance if variance distribution is not informative.
  - Scale factor α: Setting α=1 simplifies scaling but may require rank adjustment for optimal performance.
- Failure signatures:
  - SVD components not converging: Check batch size, variance threshold, or activation distribution stability.
  - Poor performance despite correct initialization: Verify that the dataset used for SVD is representative of the fine-tuning data.
  - Unexpected rank distribution: Ensure global sorting is correctly implemented and that explained variance ratios are properly normalized.
- First 3 experiments:
  1. Run EVA on a small model (e.g., RoBERTa-base) on a single GLUE task with rank=2, compare to LoRA baseline.
  2. Test EVA with ρ=1 (uniform ranks) vs ρ=2 (adaptive ranks) on the same task to observe rank redistribution effect.
  3. Measure SVD convergence time and component stability across different batch sizes (4, 8, 16) on the same task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EV A perform when fine-tuning models on sequential tasks where data arrives incrementally rather than in a fixed static dataset?
- Basis in paper: [inferred] The paper states "EV A assumes access to a fixed static downstream dataset which may not always be available."
- Why unresolved: The current EV A implementation requires collecting activation vectors from a fixed dataset to compute the SVD, but the paper doesn't explore how the method would adapt to streaming or incremental data scenarios.
- What evidence would resolve it: Experiments comparing EV A's performance on streaming tasks versus standard LoRA, and analysis of how the incremental SVD algorithm would need to be modified for online learning scenarios.

### Open Question 2
- Question: What is the optimal strategy for initializing matrix B when using EV A, and how does this affect final model performance compared to initializing B=0?
- Basis in paper: [explicit] The paper follows Hu et al. (2022) in initializing B=0, but notes "All other initialization methods initialize B≠0, which requires altering the pre-trained model weights" and suggests this could be "a driving factor for improved performance."
- Why unresolved: While the paper demonstrates EV A's superiority over methods with non-zero B initialization, it doesn't systematically investigate whether initializing B with learned values (e.g., from gradient information) would further improve performance.
- What evidence would resolve it: Controlled experiments comparing EV A with various B initialization strategies (zero, gradient-based, random) across multiple task domains, measuring both final performance and convergence speed.

### Open Question 3
- Question: How does the choice of explained variance measure (eigenvalues vs. normalized singular values) impact EV A's performance across different model architectures and task types?
- Basis in paper: [explicit] The paper shows that "EV A-Raw and EV A-Max slightly improve upon LoRA, they perform worse on average than EV A" where these variants use raw eigenvalues and normalized by maximum eigenvalue respectively.
- Why unresolved: The paper only compares three specific variants of explained variance measures, leaving open the question of whether other measures (e.g., variance explained ratio, entropy-based measures) might yield better performance for specific model types or tasks.
- What evidence would resolve it: Comprehensive ablation studies testing multiple explained variance measures across diverse model architectures (transformers, CNNs, decision transformers) and task domains (language, vision, reinforcement learning), with analysis of which measures work best for which scenarios.

## Limitations

- Core theoretical claims rely on statistical independence assumption between activations and gradients, which may not hold across all model architectures and fine-tuning scenarios
- Global rank redistribution strategy introduces complexity that could lead to suboptimal allocations if variance patterns are not representative
- Incremental SVD convergence depends on activation distribution stability across minibatches, which may not hold for tasks with high domain shift

## Confidence

**High Confidence**: The mechanism of using activation variance directions for initialization is well-grounded mathematically, and the empirical results across multiple task types demonstrate consistent improvements over LoRA baselines.

**Medium Confidence**: The convergence properties of the incremental SVD and the effectiveness of global rank redistribution are supported by experimental results, but the theoretical guarantees have assumptions that may not always hold in practice.

**Low Confidence**: The paper does not provide systematic analysis of failure cases, particularly for scenarios where the independence assumption breaks down or when activation distributions vary significantly across training epochs.

## Next Checks

1. **Independence Assumption Validation**: Design controlled experiments to measure the actual covariance between activations and gradients during fine-tuning. Test whether the independence assumption holds across different layers, model sizes, and task types, and quantify the impact on EVA's effectiveness when this assumption is violated.

2. **Convergence Robustness Analysis**: Systematically vary batch sizes (from 2 to 32) and measure SVD convergence rates and component stability. Track how changes in activation distribution across epochs affect the initialization quality, particularly for tasks with domain shift or non-stationary data.

3. **Rank Redistribution Sensitivity**: Compare EVA's adaptive rank allocation against fixed-rank baselines across models with different layer counts and activation variance patterns. Measure the tradeoff between parameter reduction and performance drop when variance-based allocation is suboptimal.