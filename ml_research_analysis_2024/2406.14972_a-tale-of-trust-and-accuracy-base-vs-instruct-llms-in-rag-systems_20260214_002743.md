---
ver: rpa2
title: 'A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems'
arxiv_id: '2406.14972'
source_url: https://arxiv.org/abs/2406.14972
tags:
- answer
- documents
- instruct
- llama
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the common practice of using instructed large
  language models (LLMs) in retrieval-augmented generation (RAG) systems. It conducts
  a principled evaluation comparing base LLMs to their instructed counterparts across
  two QA datasets.
---

# A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems

## Quick Facts
- arXiv ID: 2406.14972
- Source URL: https://arxiv.org/abs/2406.14972
- Reference count: 36
- Base LLMs outperform instructed models by up to 59% in accuracy for RAG tasks

## Executive Summary
This paper challenges the prevailing assumption that instructed large language models (LLMs) are superior for retrieval-augmented generation (RAG) systems. Through a principled evaluation across two QA datasets, the study reveals that base models consistently outperform their instructed counterparts in accuracy by up to 59%, while instructed models show better instruction compliance. This creates a fundamental trade-off between accuracy and trustworthiness in RAG systems, suggesting the need for new evaluation methodologies and mechanisms that allow users to manage this trade-off based on their specific needs.

## Method Summary
The study compares base and instructed LLMs in RAG pipelines using NQ-open and TriviaQA-unfiltered datasets with English Wikipedia as the corpus. A Contriever dense passage retriever retrieves top-k documents, which are then used as context for Llama2, Llama3, Mistral, and Falcon models. The evaluation measures accuracy (normalized ground truth match) and negative rejection rate (NO-RES compliance). The experiments systematically vary the number of retrieved documents (k=1 to 10) and test models with and without instruction templates to understand how different components affect RAG performance.

## Key Results
- Base LLMs outperform instructed models by up to 59% in accuracy for RAG tasks
- Instructed models show superior adherence to task instructions (e.g., responding with NO-RES when no answer exists)
- The use of templates in instructed models can override short-answer task instructions, causing verbose, less accurate responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Base LLMs rely more heavily on parametric memory during RAG tasks, allowing them to provide correct answers even when retrieved documents lack the answer.
- Mechanism: Base models retain broader, less constrained access to knowledge learned during pre-training, which they can draw upon when context documents are insufficient or misleading.
- Core assumption: The base model's pre-training phase exposed it to sufficient examples of the target QA pairs, allowing it to "remember" correct answers.
- Evidence anchors:
  - [abstract] "base models frequently manage to provide the correct answer even when there is none in the retrieved documents, suggesting that they 'know' the answer from prior training."
  - [section 5.2] "base models frequently manage to provide the correct answer even when there is none in the retrieved documents, suggesting that they 'know' the answer from prior training."
- Break condition: If the answer was not present in the training corpus or if fine-tuning removed that knowledge, base models would not be able to recall it.

### Mechanism 2
- Claim: Instruct models, due to alignment and supervised fine-tuning, are more likely to adhere to task instructions (e.g., responding with NO-RES when no answer is present), but this adherence reduces their ability to leverage parametric memory.
- Mechanism: Alignment techniques (RLHF/DPO) explicitly train models to follow instructions and avoid hallucinations, which increases instruction compliance but constrains the model's natural tendency to draw on parametric knowledge.
- Evidence anchors:
  - [abstract] "base models show higher accuracy but lower adherence to task instructions (e.g., responding with NO-RES when no answer exists), while instructed models are more reliable but less effective at RAG."
  - [section 5.1] "In most cases, the models fail to comply with the instruction to answer with NO-RES when the answer is absent... the instruct version of Llama 2 responds with NO-RES only 30.23% of the time when the answer is not in the one document context."
- Break condition: If the task instruction does not explicitly penalize drawing on parametric memory, instruct models might still leverage it.

### Mechanism 3
- Claim: The presence of templates in instruct models can override short-answer task instructions, causing verbose, less accurate responses.
- Mechanism: Templates used during instruct fine-tuning are designed for conversational, open-ended responses. When applied to extractive QA tasks, they bias the model toward generating longer, less focused answers.
- Evidence anchors:
  - [abstract] "investigate the underlying factors affecting RAG models' performance and the impact of the additional training techniques (i.e., SFT and Alignment) on these systems."
  - [section 4.3] "instruct models face in answering the question when the recommended template is used... models in this setting override this specification and produce overly verbose responses, damaging their accuracy."
- Break condition: If templates are adapted to the task (e.g., extractive QA), the verbosity issue may be mitigated.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) combines a retrieval phase (finding relevant documents) with a generative phase (LLM producing answers conditioned on retrieved context).
  - Why needed here: The paper's central comparison is between base and instruct LLMs in a RAG pipeline; understanding RAG is essential to interpret results.
  - Quick check question: In RAG, what are the two main phases, and how do they interact?

- Concept: Parametric vs. non-parametric memory in LLMs.
  - Why needed here: The paper contrasts base models' reliance on parametric memory with instruct models' emphasis on following instructions using provided context.
  - Quick check question: What is the difference between parametric and non-parametric memory in LLMs, and how does this distinction matter for RAG?

- Concept: Instruction fine-tuning and alignment (RLHF/DPO).
  - Why needed here: The paper evaluates how these fine-tuning stages affect RAG performance, particularly instruction compliance vs. accuracy.
  - Quick check question: What are the two main stages after pre-training in instruct LLMs, and what is the primary goal of each?

## Architecture Onboarding

- Component map: Query → Retriever (top-k docs) → LLM (prompt with task instruction + docs + query) → Answer → Evaluation

- Critical path: Query → Retriever (top-k docs) → LLM (prompt with task instruction + docs + query) → Answer → Evaluation

- Design tradeoffs:
  - Base vs. instruct: Base offers higher accuracy but lower instruction compliance; instruct offers better compliance but may miss correct answers via parametric memory
  - Template usage: Templates can enforce format but may bias toward verbose answers in extractive tasks
  - Retriever quality: Higher top-k accuracy improves context relevance but also introduces more distractors

- Failure signatures:
  - Base model: High accuracy but frequent failure to answer NO-RES when appropriate (hallucination risk)
  - Instruct model: Lower accuracy, especially with templates; may over-reject (NO-RES when answer exists)
  - Template model: Verbose, off-topic answers; poor performance on short-answer tasks

- First 3 experiments:
  1. Run base and instruct models on a small NQ subset, record accuracy and NO-RES rates
  2. Add templates to instruct models, re-run, compare verbosity and accuracy
  3. Remove NO-RES requirement, re-run, observe change in accuracy and parametric memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do base models maintain their accuracy advantage over instructed models in RAG tasks when evaluated on longer context lengths beyond 10 documents?
- Basis in paper: [inferred] The paper shows base models outperform instructed models with up to 10 retrieved documents, but does not test beyond this point.
- Why unresolved: The experiments only tested up to 10 retrieved documents. The paper notes that base models' parametric memory recall decreases as more documents are added, suggesting their advantage might diminish with longer contexts.
- What evidence would resolve it: Experiments evaluating base vs. instructed models with 15-20+ retrieved documents, measuring both accuracy and parametric memory recall rates.

### Open Question 2
- Question: What specific aspects of the instruction fine-tuning process cause the degradation in RAG performance observed in instructed models?
- Basis in paper: [explicit] The paper states "base models, without the additional instruct-specific fine-tuning, outperform the instruct models on the task of RAG" and that "supervised fine-tuning and alignment detrimentally impact the model's capabilities in RAG."
- Why unresolved: The paper identifies a performance gap but does not isolate which components of instruction fine-tuning (SFT vs. alignment) or which aspects of the training data are responsible for the degradation.
- What evidence would resolve it: Controlled experiments ablating specific components of instruction fine-tuning, such as training instructed models without human preference alignment or with different instruction templates.

### Open Question 3
- Question: How does the accuracy-trustworthiness tradeoff observed in this study manifest across different model sizes and architectures?
- Basis in paper: [inferred] The study only evaluates models up to 8B parameters, leaving open whether larger models exhibit similar tradeoffs between accuracy and instruction-following behavior.
- Why unresolved: The paper notes computational resource constraints prevented evaluation of larger models, and different architectures (e.g., non-transformer models) might handle the base vs. instructed tradeoff differently.
- What evidence would resolve it: Comparative studies of base vs. instructed models across a range of model sizes (1B, 30B, 70B+) and architectures, measuring both accuracy and instruction-following metrics.

## Limitations

- The study only tests up to 10 retrieved documents, leaving open whether base model advantages persist with longer contexts
- Results are based on English Wikipedia and may not generalize to other domains or languages
- The analysis doesn't isolate which specific components of instruction fine-tuning cause the observed performance degradation

## Confidence

- Base vs. instruct accuracy comparison: High
- Parametric memory mechanism: Medium
- Instruction compliance findings: High
- Template impact on accuracy: Low

## Next Checks

1. Test the base-instruct accuracy trade-off across multiple domains (e.g., scientific literature, news articles) to assess generalizability beyond Wikipedia-based datasets
2. Conduct ablation studies isolating the effect of each fine-tuning stage (SFT vs. alignment) on RAG performance to better understand the mechanisms driving the observed differences
3. Evaluate whether providing explicit instructions to base models about NO-RES compliance can bridge the instruction-following gap without sacrificing accuracy gains