---
ver: rpa2
title: Grammar-based Game Description Generation using Large Language Models
arxiv_id: '2407.17404'
source_url: https://arxiv.org/abs/2407.17404
tags:
- game
- description
- grammar
- rules
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating grammatically
  accurate Game Description Language (GDL) representations from natural language text
  descriptions. The authors propose a two-stage framework that first generates the
  minimal grammar required to construct a valid game description, then iteratively
  improves the generated description using grammar-guided decoding.
---

# Grammar-based Game Description Generation using Large Language Models

## Quick Facts
- arXiv ID: 2407.17404
- Source URL: https://arxiv.org/abs/2407.17404
- Authors: Tsunehiko Tanaka; Edgar Simo-Serra
- Reference count: 40
- Primary result: Grammar-guided iterative decoding significantly improves GDL generation accuracy, achieving 64.0% compilability and 56.7% functionality

## Executive Summary
This paper introduces a two-stage framework for generating grammatically accurate Game Description Language (GDL) representations from natural language text descriptions. The approach first generates the minimal grammar required to construct a valid game description, then iteratively improves the generated description using grammar-guided decoding. By constraining LLM output to valid grammar continuations and providing focused context through minimal grammar inclusion, the framework significantly outperforms baseline methods in both compilability and functionality metrics.

## Method Summary
The framework employs a two-stage approach: grammar generation and game description generation. In the first stage, it generates a minimal grammar by extracting undefined non-terminal symbols and retrieving corresponding rules from the original grammar. In the second stage, it uses grammar-guided iterative decoding with an Earley parser to identify valid subsequences and candidate symbols, gradually refining the LLM output to ensure grammatical correctness. The approach uses in-context learning with demonstration examples and optimizes context length by including only the minimal necessary grammar rather than the full grammar.

## Key Results
- 64.0% compilability vs 27.0% baseline (2.37× improvement)
- 56.7% functionality vs 26.3% baseline (2.16× improvement)
- 0.46 normalized concept distance vs 0.75 baseline (significant reduction)
- Competitive ROUGE scores maintained while improving grammatical accuracy

## Why This Works (Mechanism)

### Mechanism 1: Grammar-guided iterative decoding
The framework restricts LLM output space to valid subsequences by using a parser to identify valid continuations from candidate symbols. At each iteration, the LLM is constrained to choose from valid candidate symbols that follow the longest valid subsequence, gradually building grammatically correct output. This works because the minimal grammar contains all necessary rules, and incremental validation prevents compounding grammatical errors.

### Mechanism 2: Rule decoding stage
The framework iteratively defines undefined non-terminal symbols by extracting them from predicted grammar, retrieving corresponding rules from the original grammar, and prompting the LLM to generate only the necessary rules. This process repeats until all non-terminal symbols are defined or iteration limit is reached. This ensures generated grammar is a subset of the original grammar while maintaining completeness.

### Mechanism 3: Context length optimization
Instead of including the entire GDL grammar in prompts (15,442 tokens), the framework includes only the minimal grammar (average 1,031 tokens) necessary for the specific game description. This provides more focused context to the LLM, enabling better pattern recognition and generation while staying within context length limits.

## Foundational Learning

- **Context-Free Grammar (CFG) and EBNF**: The framework relies on parsing game descriptions using CFG rules and generating minimal grammars in EBNF style. Understanding how CFGs work is essential for implementing the parser and understanding grammar extraction. Quick check: Given a simple EBNF grammar for arithmetic expressions, can you write a parser that identifies valid subsequences and candidate symbols?

- **In-Context Learning (ICL) and prompt engineering**: The framework uses ICL by providing demonstration examples and minimal grammar in prompts. Understanding how to structure prompts for optimal LLM performance is crucial. Quick check: If you have a context length limit of 8192 tokens, how would you optimize the number and selection of demonstration examples to maximize ICL effectiveness?

- **Earley parsing algorithm**: The framework uses an Earley parser to identify the longest valid subsequence and candidate symbols from LLM responses. Understanding how this algorithm works is necessary for implementing the game description decoding stage. Quick check: Can you explain how the Earley parser would process the string "(game 'Tic-Tac-Toe'(players 2)" given a grammar that defines game descriptions?

## Architecture Onboarding

- **Component map**: Input query → Grammar generation (rule decoding) → Game description generation (game description decoding) → Earley parser validation → Evaluation metrics → Output

- **Critical path**: Natural language query → Grammar generation with iterative rule decoding → Game description generation with iterative game description decoding → Earley parser validation → Compilability/functionality testing → ROUGE/NCD evaluation → Final output

- **Design tradeoffs**:
  - Context length vs. grammar completeness: Including minimal grammar vs. full grammar in prompts
  - Iteration limits vs. accuracy: More iterations improve accuracy but increase computational cost
  - LLM size vs. performance: Larger models may perform better but are more expensive
  - Rule decoding complexity vs. grammar accuracy: More complex rule selection may improve grammar but increase LLM call complexity

- **Failure signatures**:
  - Low compilability scores indicate grammar generation issues
  - Low functionality scores indicate semantic issues despite grammatical correctness
  - High ROUGE but low other metrics suggest surface-level similarity without structural correctness
  - Long inference times may indicate iteration limits are too high or LLM calls are inefficient

- **First 3 experiments**:
  1. Baseline comparison: Implement GDG (direct game description generation without grammar) and compare compilability, functionality, ROUGE, and NCD scores against GGDG
  2. Grammar size impact: Vary the number of demonstration examples and measure how minimal grammar size affects generation quality
  3. Iteration limit optimization: Test different iteration limits for both rule decoding and game description decoding stages to find optimal balance between accuracy and computational cost

## Open Questions the Paper Calls Out

### Open Question 1: Category-specific effectiveness
The paper identifies performance differences across game categories (racing, mancala, puzzle, line, war games) but doesn't analyze which specific game characteristics drive these variations. Systematic experiments varying game structural features and correlating them with decoding performance could lead to adaptive decoding strategies tailored to different game types.

### Open Question 2: Optimal iteration limits
The authors experimentally determine iteration limits of 20 for rule decoding and 10 for game description decoding, but acknowledge this is a practical constraint rather than an optimal solution. Analysis showing how iteration limits affect performance across different game types and complexity levels could lead to a principled method for setting optimal iteration counts.

### Open Question 3: Handling longer descriptions
The authors note performance degradation with longer game descriptions and discuss the 8,192 token context limit as a practical constraint. Implementation and evaluation of techniques like hierarchical decomposition, chunking, or memory mechanisms could enable processing of longer descriptions while maintaining accuracy.

## Limitations

- The framework's effectiveness depends heavily on the quality of minimal grammar extracted from ground truth examples, with unclear robustness across diverse game types
- The approach doesn't address game descriptions that exceed current LLM context length limitations (8,192 tokens)
- The computational efficiency of the iterative decoding process for real-world deployment hasn't been thoroughly evaluated

## Confidence

**High Confidence**: The iterative grammar-guided decoding framework is novel and well-defined; the experimental methodology is sound; the two-stage approach is clearly superior to baseline GDG.

**Medium Confidence**: The specific performance improvements are accurately reported; the Earley parser integration effectively validates and guides the generation process; the context length optimization through minimal grammar inclusion is optimal.

**Low Confidence**: The framework's robustness across completely different game categories not represented in the test set; the scalability of the approach to significantly larger or more complex grammars; the computational efficiency of the iterative decoding process for real-world deployment.

## Next Checks

1. **Cross-Category Generalization Test**: Apply the framework to game descriptions from categories not represented in the original 100-test dataset (e.g., board games if the test set focused on card games) and measure whether the same performance improvements hold.

2. **Error Analysis on Iterative Decoding**: Systematically analyze the failure cases where the iterative decoding process doesn't converge to grammatically correct output, identifying whether failures stem from parser limitations, LLM selection errors, or grammar incompleteness.

3. **Computational Cost Evaluation**: Measure the actual inference time and token usage for the complete two-stage process across varying iteration limits, comparing this to the performance gains to establish practical deployment thresholds.