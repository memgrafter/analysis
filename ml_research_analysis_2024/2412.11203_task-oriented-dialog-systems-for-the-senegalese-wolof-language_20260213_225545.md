---
ver: rpa2
title: Task-Oriented Dialog Systems for the Senegalese Wolof Language
arxiv_id: '2412.11203'
source_url: https://arxiv.org/abs/2412.11203
tags:
- language
- wolof
- translation
- languages
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building task-oriented dialog
  systems for low-resource languages, specifically Wolof, by leveraging machine translation
  and annotation projection techniques. The authors propose a chatbot generation engine
  based on the Rasa framework, using a French-Wolof machine translation system to
  project annotations from a rich-resource language onto Wolof.
---

# Task-Oriented Dialog Systems for the Senegalese Wolof Language

## Quick Facts
- arXiv ID: 2412.11203
- Source URL: https://arxiv.org/abs/2412.11203
- Reference count: 12
- Low-resource language chatbot system achieves comparable intent classification F1 scores to resource-rich language baseline

## Executive Summary
This paper addresses the challenge of building task-oriented dialog systems for low-resource languages, specifically Wolof, by leveraging machine translation and annotation projection techniques. The authors propose a chatbot generation engine based on the Rasa framework, using a French-Wolof machine translation system to project annotations from a rich-resource language onto Wolof. Their method involves replacing labeled words with numbered identifiers marked by dollar signs, translating the sentences, and then backfilling the translated annotations. The approach demonstrates competitive performance, with the Wolof intent classifier achieving similar F1 scores to the French counterpart, suggesting the effectiveness of the annotation projection method for creating synthetic datasets in low-resource languages.

## Method Summary
The authors propose an annotation projection approach for building task-oriented dialog systems in low-resource languages. The method uses a French-Wolof machine translation system to project annotations from French to Wolof by replacing labeled words with numbered identifiers wrapped in dollar signs ($0N$), translating the sentences, and then backfilling the translated annotations. The chatbot generation engine is based on the Rasa framework, utilizing LaBSE embeddings in a language-agnostic pipeline with DIET classifier for intent and slot filling. The approach is tested on the Amazon Massive dataset, achieving comparable F1 scores to the French baseline.

## Key Results
- Wolof intent classifier achieves similar F1 scores to French baseline (resource-rich language)
- Annotation projection method successfully preserves semantic structure across languages
- Language-agnostic pipeline enables zero-shot transfer for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Annotation projection through marker-based translation preserves semantic structure across languages.
- Mechanism: The system replaces labeled words with numbered identifiers wrapped in dollar signs ($0N$), translates the sentence, and then backfills the translated annotations using stored mappings.
- Core assumption: Machine translation systems preserve the placeholder markers during translation.
- Evidence anchors:
  - [abstract] "We introduce a simple but effective approach to projecting annotations from a source language to a target one, using only the original machine translation system coupled with an intuitive parsing strategy."
  - [section 4] "We therefore combined the two approaches (markers and identifiers) by testing the markers presented earlier on the numbered identifiers. In the end, we found that only the dollar sign ($) managed to preserve the marked identifiers during translation on all the sample sentences we tested."
  - [corpus] Found 25 related papers; average neighbor FMR=0.427 suggests moderate semantic alignment across language pairs.
- Break condition: Translation model modifies or drops the dollar sign markers, breaking the identifier preservation.

### Mechanism 2
- Claim: Language-agnostic pipelines enable zero-shot transfer for low-resource languages.
- Mechanism: Using LaBSE embeddings in a fixed pipeline allows intent classifiers to operate across languages without retraining language-specific models.
- Core assumption: LaBSE embeddings provide sufficient semantic overlap between French and Wolof for intent classification.
- Evidence anchors:
  - [section 5] "We've used it to propose a simple but highly effective fixed pipeline... making the overall system scalable to other languages beyond Wolof."
  - [abstract] "We also show that this approach is extensible to other low-resource languages, thanks to the intent classifier's language-agnostic pipeline, simplifying the design of chatbots in these languages."
  - [corpus] Related papers on cross-lingual transfer show similar language-agnostic approaches (average FMR=0.427).
- Break condition: LaBSE embeddings fail to capture semantic nuances unique to Wolof, causing intent misclassification.

### Mechanism 3
- Claim: Synthetic datasets created through translation achieve competitive performance compared to source language data.
- Mechanism: Training on Wolof data generated from French through annotation projection yields F1 scores comparable to native French intent classifiers.
- Core assumption: The quality of the French-Wolof machine translation system is sufficient to preserve intent semantics.
- Evidence anchors:
  - [abstract] "After evaluating a generated chatbot trained on the Amazon Massive dataset, our Wolof Intent Classifier performs similarly to the one obtained for French, which is a resource-rich language."
  - [section 6] "We show the macro F1 scores on the source and synthetic data and we can observe that we get equivalent scores on both sides."
  - [corpus] Weak evidence for cross-lingual dataset quality transfer (average citations=0.0, no direct comparisons found).
- Break condition: Translation introduces semantic drift that degrades classifier performance below acceptable thresholds.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Enables leveraging rich-resource language data to bootstrap NLP systems for low-resource languages like Wolof.
  - Quick check question: What are the two main approaches for cross-lingual transfer in dialog systems mentioned in the related work section?

- Concept: Annotation projection
  - Why needed here: Provides a method to transfer labeled data from French to Wolof without manual annotation effort.
  - Quick check question: What specific marker was found to consistently preserve identifiers during translation?

- Concept: Language-agnostic embeddings
  - Why needed here: Allows intent classifiers to work across languages without language-specific training.
  - Quick check question: Which embedding model was selected for its language-agnostic properties in the experiments?

## Architecture Onboarding

- Component map:
  - Excel ontology files → Chatbot Engine → RASA project generation
  - Chatbot Engine: Marker-based annotation projection + French-Wolof MT system
  - RASA pipeline: LaBSE embeddings → DIET classifier for intent and slot filling

- Critical path:
  1. Parse French examples and replace labeled terms with $0N$ identifiers
  2. Translate parsed sentences to Wolof using MT system
  3. Backfill identifiers with translated annotations
  4. Generate RASA training data
  5. Train intent classifier using LaBSE embeddings
  6. Evaluate performance on Wolof test set

- Design tradeoffs:
  - Marker-based vs alignment-based projection: Marker approach simpler but depends on MT preserving markers
  - LaBSE vs language-specific embeddings: Language-agnostic but may sacrifice some language-specific performance
  - Synthetic vs human-annotated data: Cost-effective but quality depends on MT system

- Failure signatures:
  - Low F1 scores on Wolof despite high French performance indicates translation quality issues
  - Inconsistent marker preservation during translation suggests need for different marker or MT fine-tuning
  - High confidence but wrong predictions indicate embedding space misalignment

- First 3 experiments:
  1. Test marker preservation by translating sample sentences with different markers ($, {}, [], etc.)
  2. Compare F1 scores on French test set vs Wolof synthetic test set to establish baseline
  3. Evaluate intent confidence distributions to identify potential confusion patterns between specific intents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of annotation projection vary across different low-resource languages with varying degrees of linguistic similarity to French?
- Basis in paper: [inferred] The paper mentions that the approach is "extensible to other low-resource languages" and uses French as a resource-rich language due to its proximity to Wolof, but doesn't test this across multiple languages.
- Why unresolved: The paper only demonstrates the approach on Wolof, leaving the generalizability to other low-resource languages uncertain.
- What evidence would resolve it: Testing the annotation projection approach on multiple low-resource languages with varying degrees of similarity to French and comparing the results.

### Open Question 2
- Question: What is the impact of code-switching between Wolof and French on the performance of the chatbot engine?
- Basis in paper: [explicit] The paper mentions that "Wolof spoken in Senegal is also very much code-switched with French" but doesn't explore how this affects the system's performance.
- Why unresolved: The paper doesn't evaluate the chatbot engine's performance on code-switched utterances, which are common in real-world Wolof conversations.
- What evidence would resolve it: Evaluating the chatbot engine on a dataset containing code-switched Wolof-French utterances and comparing its performance to monolingual Wolof data.

### Open Question 3
- Question: How does the annotation projection method handle out-of-vocabulary words or expressions in the source language?
- Basis in paper: [inferred] The paper mentions that some annotations in the French dataset are expressions, not just words, and are translated separately, but doesn't discuss how the method handles words not present in the translation system's vocabulary.
- Why unresolved: The paper doesn't address the handling of out-of-vocabulary terms, which could be common in low-resource languages.
- What evidence would resolve it: Analyzing the performance of the annotation projection method on sentences containing out-of-vocabulary words in the source language and examining how these cases are handled.

## Limitations
- Dependence on quality of underlying machine translation system
- Evaluation focuses on intent classification but doesn't extensively address slot filling quality
- Requires access to parallel corpora for fine-tuning the MT system

## Confidence
**High confidence** in the core mechanism: The annotation projection approach using dollar sign markers is well-validated through explicit testing and produces measurable results comparable to the source language. The reported F1 scores and the clear methodology description support this confidence level.

**Medium confidence** in cross-lingual generalizability: While the language-agnostic pipeline using LaBSE embeddings theoretically enables zero-shot transfer, the evaluation is limited to one low-resource language pair. The moderate semantic alignment (FMR=0.427) in related work suggests this may not generalize universally without language-specific considerations.

**Low confidence** in the MT system details: The paper mentions using an in-house French-Wolof MT system but doesn't provide architecture details, training procedures, or quality metrics beyond the implicit evaluation through downstream task performance.

## Next Checks
1. **Marker preservation stress test**: Systematically test the dollar sign marker approach across different MT models (M2M100, NLLB) and sentence structures to identify edge cases where markers might be dropped or modified during translation.

2. **Error analysis of projected annotations**: Manually examine a sample of translated utterances to quantify translation-induced errors in both intent labels and slot annotations, measuring the correlation between MT quality and downstream task performance.

3. **Cross-lingual transfer scalability test**: Apply the same pipeline to a different low-resource language pair with available parallel data to validate whether the language-agnostic approach maintains competitive performance across language families with different linguistic structures.