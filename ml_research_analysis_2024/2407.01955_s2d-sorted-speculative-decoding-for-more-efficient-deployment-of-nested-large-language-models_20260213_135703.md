---
ver: rpa2
title: 'S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large
  Language Models'
arxiv_id: '2407.01955'
source_url: https://arxiv.org/abs/2407.01955
tags:
- draft
- target
- soft
- decoding
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sorted Speculative Decoding (S2D) introduces a novel multi-target
  approach for draft model deployment, enabling a single draft model to serve multiple
  target LLMs without requiring separate training for each. By using sorted fine-tuning
  to create sub-models within a draft architecture and applying adaptive draft selection,
  S2D achieves an average speedup ratio of 1.55 across targets ranging from 7B to
  70B parameters.
---

# S2D: Sorted Speculative Decoding For More Efficient Deployment of Nested Large Language Models

## Quick Facts
- arXiv ID: 2407.01955
- Source URL: https://arxiv.org/abs/2407.01955
- Authors: Parsa Kavehzadeh; Mohammadreza Pourreza; Mojtaba Valipour; Tinashu Zhu; Haoli Bai; Ali Ghodsi; Boxing Chen; Mehdi Rezagholizadeh
- Reference count: 20
- Key outcome: S2D achieves 1.55x average speedup across 7B-70B parameter targets without retraining

## Executive Summary
Sorted Speculative Decoding (S2D) introduces a novel multi-target approach for draft model deployment, enabling a single draft model to serve multiple target LLMs without requiring separate training for each. By using sorted fine-tuning to create sub-models within a draft architecture and applying adaptive draft selection, S2D achieves an average speedup ratio of 1.55 across targets ranging from 7B to 70B parameters. Compared to standard speculative decoding, S2D outperforms in smaller targets (Vicuna 7B, 13B) and maintains competitive performance in larger ones (LLaMA Chat 70B), demonstrating effective adaptation between draft latency and capacity based on target size.

## Method Summary
S2D uses sorted fine-tuning to create a draft architecture containing multiple sub-models (6, 9, and 12 layers) trained on the ShareGPT dataset. During inference, an adaptive draft selection mechanism iterates through these sub-models in order of increasing complexity, generating tokens with confidence scores. If a token's confidence exceeds the predefined threshold for that sub-model, it's accepted as the final draft token. The draft tokens are then verified by the target LLM in parallel. This approach enables a single draft model to serve multiple target models efficiently by balancing draft latency and capacity based on target requirements.

## Key Results
- Achieves 1.55x average speedup across target models ranging from 7B to 70B parameters
- Outperforms standard speculative decoding on smaller targets (Vicuna 7B, 13B) while maintaining competitive performance on larger targets (LLaMA Chat 70B)
- Reduces deployment complexity by eliminating the need for multiple draft models or target model retraining
- Demonstrates effective multi-target deployment with a single draft architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S2D achieves efficient multi-target inference by leveraging a single draft model that contains multiple sub-models, each optimized for different target LLMs.
- Mechanism: S2D uses sorted fine-tuning to create sub-models within a draft architecture, where each sub-model is trained to serve as a draft for different target models. During inference, an adaptive draft selection mechanism dynamically chooses the appropriate sub-model based on confidence thresholds, ensuring optimal performance for each target without requiring separate draft models.
- Core assumption: The sub-models within the draft architecture can effectively approximate the behavior of their corresponding target models, and the confidence-based selection can reliably choose the best sub-model for each target.
- Evidence anchors:
  - [abstract] "Sorted Speculative Decoding (S2D) introduces a novel multi-target approach for draft model deployment, enabling a single draft model to serve multiple target LLMs without requiring separate training for each."
  - [section 3.2] "To address the mentioned problems, we propose our solution called sorted speculative decoding (S2D). Sorted refers to the sorted-training (Valipour et al., 2023) approach in which a model and its selected sub-models can be trained on single or multiple tasks at the same time."
  - [corpus] Weak evidence; the related papers focus on speculative decoding variants but do not specifically address multi-target draft models.

### Mechanism 2
- Claim: The confidence-based early-exiting approach in S2D allows for efficient draft token generation by selecting the most appropriate sub-model based on its confidence score.
- Mechanism: During draft generation, S2D iterates through the draft sub-models in order of increasing complexity (6 layers, 9 layers, 12 layers). For each sub-model, it generates a token and its confidence score. If the confidence score exceeds a predefined threshold for that sub-model, the token is accepted as the final draft token, and the process moves to the next position. This approach ensures that simpler sub-models are used when possible, reducing latency, while more complex sub-models are employed when necessary for accuracy.
- Core assumption: The confidence scores generated by the sub-models are reliable indicators of their ability to produce accurate draft tokens, and the thresholds can be set to balance latency and accuracy effectively.
- Evidence anchors:
  - [section 3.3] "To generate one draft token given input sequence S, we start iterating over draft sub-models, starting from Nds. For each sub-model Ni ∈ LD, we have: t, c ∼ f (S; θNi) Where t and c are the draft token and its confidence sampled from draft sub-model Ni. We accept the token t as the final draft token if c ≥ τi."
  - [section 4.4.1] "To find the optimum confidence thresholds for SoFT draft sub-models in S2D algorithm, we assess different thresholds sets to see the algorithm's performance in each scenario."
  - [corpus] Weak evidence; the related papers do not discuss confidence-based early-exiting approaches in the context of speculative decoding.

### Mechanism 3
- Claim: S2D outperforms traditional speculative decoding methods by effectively balancing draft latency and capacity based on the target model size.
- Mechanism: S2D adapts the draft generation process based on the target model size. For smaller targets, it prioritizes lower-latency sub-models to minimize the overall inference time. For larger targets, it leverages the higher capacity of more complex sub-models to maintain a high acceptance ratio, ensuring that the speedup gains are not offset by an excessive number of verification failures.
- Core assumption: The relationship between draft latency, capacity, and target model size is such that the optimal balance can be achieved through the adaptive selection of sub-models based on their complexity and the target's requirements.
- Evidence anchors:
  - [abstract] "S2D achieves an average speedup ratio of 1.55 across targets ranging from 7B to 70B parameters. Compared to standard speculative decoding, S2D outperforms in smaller targets (Vicuna 7B, 13B) and maintains competitive performance in larger ones (LLaMA Chat 70B)."
  - [section 4.3] "S2D has also higher speedup compared to SD with medium size (Layer 9) of SoFT model in many tasks like MT-Bench and GSM8K. SD with the smallest sub-model (Layer 6) of SoFT model outperforms S2D in most tasks due to the fact that the smallest sub-model of our draft architecture enjoys 1/2 latency of the full architecture."
  - [corpus] Weak evidence; the related papers focus on speculative decoding variants but do not specifically address the balance between draft latency and capacity based on target model size.

## Foundational Learning

- Concept: Speculative Decoding
  - Why needed here: Speculative decoding is the foundational technique that S2D builds upon. Understanding how it works is crucial for grasping the improvements and modifications introduced by S2D.
  - Quick check question: In speculative decoding, what are the two main steps involved in generating tokens, and how do they contribute to the overall speedup?

- Concept: Sorted Fine-tuning
  - Why needed here: Sorted fine-tuning is the key technique used in S2D to create the sub-models within the draft architecture. Understanding how it works is essential for comprehending the multi-target capabilities of S2D.
  - Quick check question: How does sorted fine-tuning differ from traditional fine-tuning, and what advantage does it provide in the context of S2D?

- Concept: Adaptive Draft Selection
  - Why needed here: The adaptive draft selection mechanism is what enables S2D to efficiently serve multiple target models with a single draft architecture. Understanding how it works is crucial for grasping the multi-target capabilities of S2D.
  - Quick check question: In S2D, how does the adaptive draft selection mechanism determine which sub-model to use for a given target, and what factors influence this decision?

## Architecture Onboarding

- Component map:
  - Target LLMs -> Draft Architecture -> Sub-models (6, 9, 12 layers) -> Confidence Thresholds -> Adaptive Selection Mechanism

- Critical path:
  1. Input sequence is passed to the draft architecture
  2. For each position in the sequence, the adaptive selection mechanism iterates through the sub-models in order of increasing complexity
  3. Each sub-model generates a token and its confidence score
  4. If the confidence score exceeds the threshold for that sub-model, the token is accepted as the final draft token
  5. The draft tokens are then verified by the target LLM in parallel

- Design tradeoffs:
  - Number of sub-models: Increasing the number of sub-models can improve the accuracy of draft generation for a wider range of target models but also increases the complexity and memory requirements of the draft architecture
  - Confidence thresholds: Setting higher thresholds can improve the accuracy of draft generation but may also increase the latency by requiring more complex sub-models to be used more frequently
  - Sub-model complexity: Using more complex sub-models can improve the accuracy of draft generation but also increases the latency of the draft generation process

- Failure signatures:
  - Low speedup: If the draft tokens are frequently rejected by the target LLM, the overall speedup will be reduced
  - High latency: If the adaptive selection mechanism frequently chooses more complex sub-models, the draft generation latency will increase
  - Poor accuracy: If the sub-models fail to accurately represent their corresponding target models, the draft tokens may be inaccurate, leading to poor overall performance

- First 3 experiments:
  1. Compare the speedup and accuracy of S2D against traditional speculative decoding methods using a single target LLM
  2. Evaluate the performance of S2D with different numbers of sub-models in the draft architecture to determine the optimal balance between accuracy and complexity
  3. Test the impact of different confidence threshold settings on the speedup and accuracy of S2D to find the optimal balance between latency and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of sub-models to include in the SoFT draft architecture for maximizing speedup across different target sizes?
- Basis in paper: [inferred] The paper mentions using three sub-models (layers 6, 9, and 12) but suggests that exploring a different number of sub-models could offer deeper insights.
- Why unresolved: The authors did not conduct experiments with varying numbers of sub-models, leaving the optimal configuration untested.
- What evidence would resolve it: Comparative experiments testing different numbers of sub-models (e.g., 2, 3, 4) across various target sizes to determine which configuration yields the highest speedup.

### Open Question 2
- Question: How does the performance of S2D compare when applied to self-speculative decoding settings where the draft and target models are part of the same architecture?
- Basis in paper: [explicit] The paper states that S2D is applicable to self-speculative solutions with minor adjustments but does not provide experimental results in this setting.
- Why unresolved: The authors focused on the external draft model scenario and did not evaluate S2D in self-speculative decoding.
- What evidence would resolve it: Experimental results comparing S2D performance in self-speculative decoding versus traditional self-speculative methods across multiple target sizes.

### Open Question 3
- Question: What is the impact of varying confidence thresholds on the tradeoff between draft latency and token acceptance rate in S2D?
- Basis in paper: [explicit] The paper discusses confidence-based early-exiting and shows that smaller thresholds work better for smaller targets while larger thresholds are better for larger targets, but does not provide a systematic analysis of the threshold-latency-acceptance tradeoff.
- Why unresolved: The authors fixed the best thresholds for each target model but did not explore the full parameter space or provide guidelines for threshold selection.
- What evidence would resolve it: A comprehensive study varying confidence thresholds systematically to map the relationship between draft latency, token acceptance rate, and overall speedup across different target sizes.

## Limitations

- The paper only evaluates S2D on three specific target models (Vicuna 7B, 13B, LLaMA Chat 70B), limiting generalizability to other architectures and sizes
- The confidence-based early-exiting mechanism relies on the reliability of confidence scores without extensive validation of their correlation with token quality
- The threshold optimization process is described as empirical but lacks systematic sensitivity analysis across different tasks and targets

## Confidence

- **High Confidence**: The core architectural innovation of using sorted fine-tuning to create sub-models within a single draft architecture is well-supported by the methodology and experimental results
- **Medium Confidence**: The multi-target deployment advantage is demonstrated but primarily on three specific target models, with generalizability to arbitrary target models remaining uncertain
- **Low Confidence**: The paper claims to avoid retraining target models and eliminate the need for multiple draft models, but doesn't quantify the deployment complexity reduction or cost savings in real-world scenarios

## Next Checks

1. **Confidence Score Validation**: Conduct ablation studies measuring the correlation between confidence scores and actual token acceptance rates across different sub-models and target combinations to validate whether the confidence-based selection mechanism is working as intended.

2. **Cross-Target Generalization**: Test S2D on additional target models beyond the three evaluated (Vicuna 7B, 13B, LLaMA Chat 70B) to assess whether the multi-target capabilities hold for other architectures and sizes, particularly in the 1B-6B and 30B-65B parameter ranges.

3. **Threshold Sensitivity Analysis**: Perform systematic grid searches for confidence thresholds across all tasks and targets to determine whether the empirical thresholds used are near-optimal or if there's significant room for improvement through automated threshold tuning.