---
ver: rpa2
title: 'Large Language Models "Ad Referendum": How Good Are They at Machine Translation
  in the Legal Domain?'
arxiv_id: '2402.07681'
source_url: https://arxiv.org/abs/2402.07681
tags:
- translation
- language
- https
- legal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the machine translation (MT) quality of two
  large language models (GPT-4 and VICUNA) against a traditional neural MT system
  (Google Translate) in the legal domain across four language pairs. Automatic evaluation
  metrics (AEMs) and human evaluation (HE) by professional translators assessed translation
  ranking, fluency, and adequacy.
---

# Large Language Models "Ad Referendum": How Good Are They at Machine Translation in the Legal Domain?

## Quick Facts
- arXiv ID: 2402.07681
- Source URL: https://arxiv.org/abs/2402.07681
- Reference count: 18
- Large language models (GPT-4 and VICUNA) performed comparably or slightly better than traditional neural MT systems in human evaluation for legal translation, despite underperforming in automatic metrics

## Executive Summary
This study compared the machine translation quality of two large language models (GPT-4 and VICUNA) against a traditional neural MT system (Google Translate) in the legal domain across four language pairs. The research employed both automatic evaluation metrics and human evaluation by professional translators to assess translation ranking, fluency, and adequacy. While Google Translate generally outperformed the LLMs in AEMs, human evaluators rated GPT-4 comparably or slightly better than Google Translate in terms of contextually adequate and fluent translations, particularly for high-resourced languages (Spanish, Catalan, Brazilian Portuguese). This discrepancy highlights the importance of human evaluation methods in assessing MT quality and suggests LLMs' potential in handling specialized legal terminology and context.

## Method Summary
The study compared GPT-4 and VICUNA against Google Translate across four language pairs (Spanish-English, Catalan-English, Brazilian Portuguese-English) in the legal domain. Translations were evaluated using both automatic evaluation metrics (AEMs) and human evaluation (HE) by professional translators. The HE assessed translation ranking, fluency, and adequacy. The researchers used a combination of zero-shot and few-shot prompting strategies for the LLMs to test their performance in legal translation tasks.

## Key Results
- Google Translate outperformed both LLMs in automatic evaluation metrics (AEMs)
- Human evaluators rated GPT-4 comparably or slightly better than Google Translate for contextually adequate and fluent translations
- GPT-4 showed particular strength in handling specialized legal terminology and context
- Significant discrepancy observed between AEM and human evaluation results

## Why This Works (Mechanism)
The study's methodology combines both quantitative (AEMs) and qualitative (human evaluation) approaches to capture the nuanced nature of legal translation quality. By using professional translators for human evaluation, the study accounts for contextual understanding and domain-specific expertise that automated metrics may miss. The comparison across multiple language pairs and translation directions helps identify systematic patterns in LLM performance across different language combinations.

## Foundational Learning
- **Legal terminology**: Essential for understanding specialized vocabulary and context in legal documents
  - Why needed: Legal language contains unique terms and concepts that require precise translation
  - Quick check: Can the model accurately translate "force majeure" or "habeas corpus"?

- **Cross-linguistic legal concepts**: Understanding how legal concepts map across different legal systems
  - Why needed: Legal concepts don't always translate directly between jurisdictions
  - Quick check: Does the translation preserve the intended legal meaning across legal systems?

- **Translation quality metrics**: Understanding both automatic and human evaluation methods
  - Why needed: Different evaluation approaches can yield different results
  - Quick check: Can automatic metrics capture contextual adequacy as well as human evaluators?

## Architecture Onboarding
- **Component map**: Input text -> LLM/Google Translate -> Translation output -> AEM evaluation -> Human evaluation
- **Critical path**: Source text → Prompt engineering → Model inference → Output generation → Quality assessment
- **Design tradeoffs**: AEMs offer scalability and objectivity but may miss contextual nuances; human evaluation provides deeper insight but is resource-intensive and subjective
- **Failure signatures**: AEM underperformance paired with human evaluation success suggests models may be capturing context that metrics miss; vice versa indicates potential quality issues
- **3 first experiments**: 
  1. Test zero-shot vs few-shot prompting strategies on legal terminology accuracy
  2. Compare translation quality across different legal subdomains (contracts vs court documents)
  3. Evaluate translation consistency when processing similar legal documents

## Open Questions the Paper Calls Out
None

## Limitations
- Limited human evaluator pool (3 translators per language pair) may not represent broader evaluation variability
- Focus on only high-resource language pairs limits generalizability to low-resource languages
- Significant discrepancy between AEM and human evaluation results raises questions about evaluation method reliability
- Specific legal domain focus may not reflect broader legal translation needs

## Confidence
- **High confidence**: GPT-4's performance in human evaluation compared to Google Translate for high-resource languages
- **Medium confidence**: The discrepancy between AEM and human evaluation results
- **Medium confidence**: LLMs' potential for handling specialized legal terminology

## Next Checks
1. Replicate the study with a larger sample of professional translators (n > 10) per language pair to assess inter-rater reliability and reduce evaluator bias
2. Conduct the same evaluation across a broader range of language pairs, including low-resource languages, to test generalizability
3. Compare evaluation results when using different prompt engineering strategies for the LLMs, including zero-shot and few-shot prompts, to determine optimal prompting approaches for legal translation tasks