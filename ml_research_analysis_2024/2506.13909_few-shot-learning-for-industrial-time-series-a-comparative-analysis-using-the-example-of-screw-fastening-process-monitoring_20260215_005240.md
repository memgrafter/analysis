---
ver: rpa2
title: 'Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using
  the Example of Screw-Fastening Process Monitoring'
arxiv_id: '2506.13909'
source_url: https://arxiv.org/abs/2506.13909
tags:
- learning
- data
- moment
- network
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic few-shot learning study on screw-fastening
  process monitoring, using a 2,300-sample multivariate torque dataset with 16 defect
  types. The key contribution is a label-aware episodic sampler that converts multi-label
  sequences into single-label tasks, enabling few-shot learning while preserving label
  information.
---

# Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring

## Quick Facts
- arXiv ID: 2506.13909
- Source URL: https://arxiv.org/abs/2506.13909
- Reference count: 0
- Primary result: Label-aware episodic sampler with InceptionTime + Prototypical Network achieves 0.944 weighted F1 in multi-class screw-fastening defect classification

## Executive Summary
This paper presents a systematic few-shot learning study on screw-fastening process monitoring, using a 2,300-sample multivariate torque dataset with 16 defect types. The key contribution is a label-aware episodic sampler that converts multi-label sequences into single-label tasks, enabling few-shot learning while preserving label information. Experiments with Prototypical Networks and MAML, combined with 1D CNN, InceptionTime, and transformer-based Moment backbones, show that InceptionTime + Prototypical Network achieves 0.944 weighted F1 in multi-class and 0.935 in multi-label settings—outperforming Moment by up to 5.3% while requiring two orders of magnitude fewer parameters. Across all backbones, metric learning consistently surpasses MAML, and the label-aware sampler improves F1 by 1.7% over traditional sampling. Results challenge the assumption that large foundation models are always superior in data-scarce scenarios, demonstrating that lightweight CNNs with simple metric learning converge faster and generalize better. The dataset, code, and pretrained weights are released to support reproducible research.

## Method Summary
The method combines Prototypical Networks or MAML with three backbone architectures (1D CNN, InceptionTime, frozen Moment) for few-shot screw-fastening defect classification. The key innovation is a label-aware episodic sampler that converts multi-label time series samples into single-label tasks by duplicating samples for each active label, enabling standard few-shot learning frameworks while preserving combinatorial label information. Data preprocessing includes torque column extraction, invalid value reset, min-max normalization, downsampling by rate 20, and zero-padding to 920 points. The framework is evaluated on a 2,300-sample dataset split into 6-class/1,100-sample training, 3-class/500-sample validation, and 3-class/500-sample test sets, with Optuna hyperparameter optimization.

## Key Results
- InceptionTime + Prototypical Network achieves 0.944 weighted F1 in multi-class classification, outperforming Moment by up to 5.3%
- Label-aware episodic sampling improves F1 by 1.7% over traditional class-based sampling
- Lightweight CNNs (1D CNN, InceptionTime) require two orders of magnitude fewer parameters than Moment while achieving superior performance
- Prototypical Networks consistently outperform MAML across all backbone architectures
- The framework generalizes well to multi-label settings with 0.935 weighted F1

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Label-aware episodic sampling enables effective few-shot learning on multi-label time series by converting multi-label samples into single-label tasks while preserving combinatorial label information.
- **Mechanism:** Each multi-label sample (e.g., label (A,B)) is duplicated into separate single-label samples (A and B). During episode sampling, N-way classes are selected from these single labels, and support/query sets are formed as in standard multi-class FSL. The network output dimension is fixed at N-way, ensuring compatibility with existing FSL frameworks.
- **Core assumption:** Multi-label classification can be decomposed into multiple binary classification tasks without losing inter-label dependencies.
- **Evidence anchors:**
  - [abstract] "label-aware episodic sampler that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information."
  - [section] "For each episode τi with a support set Sτi and a query set Qτi, we first compute the activated label space Ai, excluding the empty set: Ai = ⋃(x,y)∈Sτi P(supp(y)) \∅"
- **Break condition:** If label dependencies are critical for accurate classification, decomposition may lose performance. If multi-label samples are rare, duplication may not provide enough diverse training examples.

### Mechanism 2
- **Claim:** Prototypical networks outperform MAML in few-shot time series classification due to simpler optimization and better generalization from limited data.
- **Mechanism:** Prototypical networks learn class prototypes as the mean of support set embeddings, then classify query samples by distance to these prototypes. This avoids complex second-order gradient updates required by MAML, reducing overfitting risk with limited samples.
- **Core assumption:** Distance-based classification in learned embedding space generalizes better than gradient-based adaptation for small datasets.
- **Evidence anchors:**
  - [abstract] "Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7% F1 over traditional class-based sampling."
  - [section] "Prototypical network [16] is a widely used metric-based approach for FSL. The model learns an embedding space in which each class is represented by a prototype, computed as the mean of the support set examples in that representation space."
- **Break condition:** If the embedding space cannot separate classes well, distance-based classification fails. If the dataset grows large enough, MAML's adaptation capability may become advantageous.

### Mechanism 3
- **Claim:** Lightweight CNNs with simple metric learning outperform large foundation models when data is scarce.
- **Mechanism:** 1D CNN and InceptionTime architectures capture local temporal patterns effectively without requiring massive parameter counts. Prototypical networks on these backbones achieve higher F1 scores than Moment (large transformer) models, which suffer from overfitting on limited data.
- **Core assumption:** For few-shot learning, model capacity should match data availability; larger models don't automatically generalize better with limited samples.
- **Evidence anchors:**
  - [abstract] "InceptionTime + Prototypical Network combination achieves a 0.944 weighted F1...outperforming finetuned Moment by up to 5.3% while requiring two orders of magnitude fewer parameters and training time."
  - [section] "InceptionTime offers the best overall balance, maintaining high scores across all metrics and class groupings."
- **Break condition:** If more labeled data becomes available, foundation models may surpass lightweight models. If the task requires long-range dependencies, transformer architectures may become necessary.

## Foundational Learning

- **Concept: Few-shot learning paradigm**
  - Why needed here: The screw-fastening dataset has only 2,300 samples across 16 defect types, making traditional supervised learning impractical for detecting new defects.
  - Quick check question: What is the difference between N-way K-shot and traditional supervised learning data organization?

- **Concept: Multi-label vs multi-class classification**
  - Why needed here: Screw-fastening defects can occur simultaneously (e.g., class 24 = [0,1,0,1,0,0,0] for offset joint partners and span in the thread), requiring the model to predict multiple active labels per sample.
  - Quick check question: How does the label-aware sampler handle a sample with labels (A,B,C) during episode construction?

- **Concept: Episodic training and meta-learning**
  - Why needed here: FSL models must learn to adapt quickly to new tasks; episodic training simulates this by repeatedly exposing the model to different support/query set combinations.
  - Quick check question: What is the purpose of having support and query sets within each episode?

## Architecture Onboarding

- **Component map:**
  Data preprocessing → Label-aware episodic sampler → Backbone (1D CNN/InceptionTime/Moment) → Prototypical network/MAML → Evaluation

- **Critical path:**
  1. Preprocess torque data: extract column, reset invalid values, normalize, downsample
  2. Convert multi-label to single-label via duplication for episode sampling
  3. Select N-way classes and form support/query sets
  4. Backbone processes sequences → embeddings
  5. Prototypical network: compute centroids → distance → softmax → cross-entropy loss
  6. MAML: clone meta-model → adapt on support set → evaluate on query set → update meta-model

- **Design tradeoffs:**
  - 1D CNN vs InceptionTime vs Moment: parameter count (1D CNN: ~10K, InceptionTime: ~100K, Moment: 341M), training time, ability to capture long-range dependencies
  - Prototypical network vs MAML: simplicity and generalization vs adaptation capability, computational cost (second-order gradients in MAML)
  - Label-aware sampling vs direct multi-label FSL: compatibility with existing libraries vs potential loss of label correlation information

- **Failure signatures:**
  - Poor validation F1 with increasing training epochs: overfitting, reduce model capacity or apply regularization
  - High variance across runs: insufficient data diversity, increase number of episodes or apply data augmentation
  - Degraded performance on multi-label classes: label-aware sampling losing critical label correlations, consider hybrid approaches

- **First 3 experiments:**
  1. **Baseline evaluation:** Run 1D CNN + Prototypical network on multi-class converted data (no label-aware sampling) to establish baseline performance
  2. **Label-aware sampling impact:** Compare 1D CNN + Prototypical network with and without label-aware sampling on multi-label data
  3. **Backbone comparison:** Evaluate all three backbones (1D CNN, InceptionTime, frozen Moment) with Prototypical network to identify best architecture for this dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the label-aware episodic sampler perform when scaling to datasets with more than 16 defect types or higher-dimensional multi-label combinations?
- Basis in paper: [explicit] The authors introduce a label-aware episodic sampler for multi-label screw-fastening data and note it improves F1 by 1.7% over traditional sampling, but do not test it beyond 16 classes or with more complex combinatorial label structures.
- Why unresolved: The paper's dataset is limited to 16 defect types, and experiments do not explore scalability or performance with denser multi-label patterns.
- What evidence would resolve it: Systematic experiments on synthetic or real-world datasets with >16 defect types and higher-order multi-label combinations, measuring F1, training stability, and computational overhead.

### Open Question 2
- Question: Why does Moment underperform lightweight CNNs like 1D CNN and InceptionTime in few-shot screw-fastening tasks, despite its strong performance on public benchmarks?
- Basis in paper: [explicit] The authors note Moment's underperformance (e.g., 5.3% lower F1 than InceptionTime + Prototypical Network) and speculate on data scarcity and downsampling as possible causes.
- Why unresolved: The paper does not conduct ablation studies on Moment's architecture, data augmentation, or fine-tuning strategy to isolate the root cause of underperformance.
- What evidence would resolve it: Controlled experiments varying data size, fine-tuning depth, and input preprocessing for Moment, compared against lightweight CNNs under identical conditions.

### Open Question 3
- Question: How robust is the best-performing InceptionTime + Prototypical Network combination to distribution shifts, such as new defect types or changes in screw-fastening process parameters?
- Basis in paper: [explicit] The authors report high F1 scores (0.944 weighted F1 in multi-class, 0.935 in multi-label) but do not evaluate out-of-distribution generalization or adaptation to novel defect types.
- Why unresolved: The experiments focus on fixed train/validation/test splits without simulating process changes or introducing unseen defect categories during evaluation.
- What evidence would resolve it: Transfer experiments where the model is evaluated on data from modified screw-fastening setups or with artificially introduced novel defect classes, measuring performance drop and adaptation speed.

## Limitations
- Limited generalizability beyond screw-fastening processes with fixed 16-defect structure
- Evaluation focuses on binary classification, potentially underestimating real-world complexity
- Label-aware sampling assumes decomposable multi-label problems, which may not hold for emergent defect combinations

## Confidence
- **High Confidence:** Prototypical Networks consistently outperform MAML across all backbone architectures; InceptionTime backbone provides optimal balance between performance and efficiency; Label-aware episodic sampling improves F1 scores by 1.7% over traditional methods
- **Medium Confidence:** Lightweight CNNs with metric learning outperform large foundation models in data-scarce scenarios; The 5.3% performance gap between InceptionTime and Moment is robust across metrics; Two-orders-of-magnitude parameter reduction translates to practical deployment advantages
- **Low Confidence:** Conclusions about industrial applicability beyond screw-fastening processes; Claims about foundation models' general inferiority in few-shot settings; Long-term performance stability across varying operational conditions

## Next Checks
1. **Cross-Industry Validation**: Test the proposed framework on a different industrial time series dataset (e.g., predictive maintenance from rotating machinery) to assess whether lightweight CNNs with Prototypical Networks maintain their advantage over foundation models when defect patterns and temporal characteristics differ significantly.

2. **Scalability Assessment**: Systematically increase the number of defect classes beyond 16 while maintaining few-shot constraints to determine at what point the label-aware sampling approach becomes ineffective or requires architectural modifications to preserve performance.

3. **Real-World Deployment Analysis**: Implement the best-performing configuration (InceptionTime + Prototypical Networks) in a live industrial setting with streaming data, measuring not just classification accuracy but also inference latency, memory usage, and ability to handle concept drift over extended operational periods.