---
ver: rpa2
title: Unsupervised Human Preference Learning
arxiv_id: '2410.03731'
source_url: https://arxiv.org/abs/2410.03731
tags:
- email
- preference
- rules
- ground
- truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces preference agents, small models trained to
  generate natural language rules that guide larger pre-trained language models to
  produce personalized outputs. This approach addresses the challenge of adapting
  large language models to individual user preferences without fine-tuning the large
  model directly.
---

# Unsupervised Human Preference Learning

## Quick Facts
- arXiv ID: 2410.03731
- Source URL: https://arxiv.org/abs/2410.03731
- Authors: Sumuk Shashidhar; Abhinav Chinta; Vaibhav Sahai; Dilek Hakkani-Tür
- Reference count: 40
- Primary result: Preference agents significantly outperform baseline personalization techniques, achieving up to 80% improvement in alignment with user preferences.

## Executive Summary
This paper introduces preference agents, small models trained to generate natural language rules that guide larger pre-trained language models to produce personalized outputs. The approach addresses the challenge of adapting large language models to individual user preferences without fine-tuning the large model directly. Preference agents learn from differences between zero-shot baseline outputs and ground truth, capturing preference information in structured natural language rules. Experiments on email, article, and review datasets demonstrate significant improvements over baseline personalization techniques, including fine-tuning and standard prompting.

## Method Summary
The method uses a modular architecture where a large language model (ML) generates zero-shot outputs and preference rules, while a small preference agent (MA) learns to generate these rules from training data. The preference agent is trained on input-rule pairs using QLoRA, capturing user preferences from the differences between ML's outputs and ground truth. These learned rules are then used to guide the ML's output toward personalized styles and content. The approach leverages natural language rules as an efficient learning signal, avoiding direct modification of the large model's weights.

## Key Results
- Preference agents achieve up to 80% improvement in alignment with user preferences compared to baselines
- Preference agents outperform naive fine-tuning while potentially hallucinating less on factual content
- Models within the same family (e.g., Llama) show higher semantic alignment, making rule-guided outputs more effective
- Incorporating thinking tokens into rule generation improves rule quality and effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The preference agent learns more effectively from structured preference rules than from direct input-output pairs because the rules are closer to the LM's existing output distribution.
- Mechanism: Naive fine-tuning on diverse input-output pairs forces the model to adapt to a new distribution that may differ significantly from its pre-trained style, especially the conversational "chatbot" style of instruction-tuned models. Structured preference rules, being natural language, are closer to the model's existing output distribution, allowing for more efficient adaptation using PEFT methods.
- Core assumption: The structured format of preference rules enables the model to discern patterns more easily than the diverse and potentially drastically different output styles of specific tasks.
- Evidence anchors:
  - [abstract]: "This modular architecture decouples the task of preference learning from the generic LLM, allowing users to efficiently fine-tune a small agent on their personal data without the need for modifying the weights of the larger model."
  - [section 5.1]: "Our experiments reveal that fine-tuning the preference agent on natural language rules, as opposed to directly on input-output pairs, leads to a more effective learning process. This result indicates that structured rules provide a more efficient learning signal for the preference agent."
  - [corpus]: Weak - no direct corpus evidence; this is primarily based on experimental findings within the paper.

### Mechanism 2
- Claim: Models within the same family (e.g., Llama) exhibit a higher degree of semantic alignment compared to models from different families (e.g., GPT-4), leading to more effective rule-guided outputs.
- Mechanism: The semantic understanding of natural language rules is model-specific. A preference agent trained on rules generated by a Llama model will likely be more effective at guiding another Llama model compared to a GPT-4 model, even if the rules appear well-structured to humans.
- Core assumption: Subtle differences in how models interpret language can significantly impact the effectiveness of rule-based guidance.
- Evidence anchors:
  - [section 5.2]: "Our findings suggest that models within the same family (e.g., Llama) exhibit a higher degree of semantic alignment compared to models from different families (e.g., GPT-4)."
  - [section 5.2]: "This indicates that semantic understanding, even when expressed through seemingly universal natural language, can be model-specific."
  - [corpus]: Weak - this is primarily based on experimental observations within the paper.

### Mechanism 3
- Claim: Incorporating "thinking tokens" into the rule generation process encourages the model to engage in more deliberate reasoning, leading to more insightful and effective preference rules.
- Mechanism: The thinking tokens function as a form of cognitive scaffolding, providing the model with a structured space to isolate and process critical preference information. By explicitly prompting the model to "think" before generating rules, we aim to enhance its ability to identify subtle patterns in user preferences and translate them into effective guidance for the larger model.
- Core assumption: Prompting LLMs to engage in step-by-step reasoning can significantly improve their performance on various tasks, including rule generation.
- Evidence anchors:
  - [section 5.3]: "Our empirical results demonstrate that incorporating these deliberative prompts leads to a notable improvement in the quality of generated rules, resulting in better alignment between the large LLM's outputs and individual user preferences."
  - [section 5.3]: "We hypothesize that these thinking tokens function as a form of cognitive scaffolding, providing the model with a structured space to isolate and process critical preference information."
  - [corpus]: Weak - this is primarily based on experimental findings within the paper.

## Foundational Learning

- Concept: Understanding the limitations of traditional personalization methods (ICL and PEFT) when applied to human preference learning with small, personal datasets.
  - Why needed here: The paper argues that these methods fall short in capturing the complexity of human preferences, especially given the small, personal datasets individuals possess. This motivates the need for the proposed preference agent approach.
  - Quick check question: What are the key challenges of using ICL and PEFT for personalization with small, personal datasets?

- Concept: Familiarity with Reinforcement Learning from Human Feedback (RLHF) and its variants (RLAIF, DPO) as common approaches for aligning language models with human preferences.
  - Why needed here: The paper positions its approach as an alternative to these traditional methods, highlighting the computational demands and data requirements of RLHF.
  - Quick check question: How does the proposed preference agent approach differ from RLHF in terms of computational efficiency and data requirements?

- Concept: Understanding the concept of semantic alignment between models and how it can impact the effectiveness of rule-based guidance.
  - Why needed here: The paper emphasizes that models within the same family exhibit higher semantic alignment, leading to more effective rule-guided outputs. This understanding is crucial for selecting appropriate models for rule generation and application.
  - Quick check question: Why might a preference agent trained on rules generated by a Llama model be more effective at guiding another Llama model compared to a GPT-4 model?

## Architecture Onboarding

- Component map:
  - User Input (X) -> Large Language Model (ML) (Zero-shot output Yz) -> ML (Preference rules P) -> Preference Agent (MA) training -> Preference Agent (MA) (Preference rules p) -> ML (Aligned output Ya)

- Critical path: User Input (X) → ML (Zero-shot output Yz) → ML (Preference rules P) → Preference Agent (MA) training → Preference Agent (MA) (Preference rules p) → ML (Aligned output Ya)

- Design tradeoffs:
  - Using a small preference agent allows for efficient personalization without modifying the large model's weights, but it introduces an additional inference step during rule generation.
  - The choice of rule generation strategy (with or without baseline) involves a tradeoff between performance and inference cost.
  - The use of thinking tokens can improve rule quality but adds to the inference cost.

- Failure signatures:
  - If the preference agent fails to learn effective rules, the aligned outputs may not differ significantly from the zero-shot outputs.
  - If the preference rules are too complex or ambiguous, the large model may struggle to interpret them correctly.
  - If the semantic alignment between the rule-generating model and the model being guided is low, the effectiveness of the rules may be limited.

- First 3 experiments:
  1. Evaluate the performance of the preference agent on a held-out test set using GPT-4o and human evaluation.
  2. Compare the performance of the preference agent against baselines (zero-shot, few-shot, naive fine-tuning) on different datasets (e.g., Enron, New Yorker, LAMP 3U).
  3. Investigate the impact of different rule generation strategies (with and without baseline, with and without thinking tokens) on the quality of the preference rules and the aligned outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do preference agents perform when scaled to larger language models, such as those with 100B+ parameters?
- Basis in paper: [inferred] The paper evaluates preference agents with models like Llama-3-70B, Claude 3.5 Sonnet, and Gemini 1.5 Pro, but does not explore their performance with even larger models.
- Why unresolved: The paper does not investigate the upper limits of model size for preference agents or whether their effectiveness plateaus or continues to improve with larger models.
- What evidence would resolve it: Experiments comparing preference agents' performance across a range of model sizes, including those exceeding 100B parameters, would clarify the scalability of the approach.

### Open Question 2
- Question: Can preference agents be effectively adapted for multimodal inputs, such as combining text with images or audio?
- Basis in paper: [explicit] The paper focuses solely on text-based preferences and does not explore multimodal applications.
- Why unresolved: The paper does not address the challenges or potential of extending preference agents to handle multimodal inputs, which could broaden their applicability.
- What evidence would resolve it: Testing preference agents on multimodal datasets and tasks, such as generating personalized responses to image-text pairs or audio-text combinations, would demonstrate their versatility.

### Open Question 3
- Question: How do preference agents handle conflicting preferences within the same user or dataset?
- Basis in paper: [inferred] The paper does not explicitly discuss how preference agents manage situations where a user's preferences are contradictory or when different users have opposing preferences.
- Why unresolved: The paper does not explore strategies for resolving conflicts in preferences, which could impact the reliability and adaptability of preference agents.
- What evidence would resolve it: Experiments that introduce conflicting preferences into the training and evaluation datasets, along with analysis of how preference agents adapt to or resolve these conflicts, would provide insights into their robustness.

## Limitations

- The paper's findings about semantic alignment within model families are based on limited comparisons and may not generalize across different model architectures or newer models.
- The human evaluation sample size (10-12 participants) is relatively small, and the study doesn't report inter-rater reliability or demographic information about evaluators.
- The automated evaluation using GPT-4o may be subject to its own biases and limitations in assessing stylistic alignment.

## Confidence

**High Confidence Claims:**
- The modular architecture of separating preference learning from the large LLM is technically sound and implementable
- The approach achieves measurable improvements over zero-shot and few-shot baselines on the tested datasets
- QLoRA enables efficient fine-tuning of preference agents on consumer hardware

**Medium Confidence Claims:**
- Preference agents outperform naive fine-tuning on stylistic alignment while potentially hallucinating less on factual content
- The rule generation strategy with baseline yields better results than without baseline
- Thinking tokens improve rule quality and effectiveness

**Low Confidence Claims:**
- The superiority of rule-based fine-tuning over naive fine-tuning is fundamentally due to distribution proximity rather than other factors
- Semantic alignment effects will hold consistently across different model families and generations
- The thinking tokens function as cognitive scaffolding rather than simply providing more tokens for the model to work with

## Next Checks

1. **Ablation Study on Rule Generation**: Conduct a systematic ablation study comparing rule-based fine-tuning against naive fine-tuning while controlling for training data volume, sequence length, and token diversity. This would help isolate whether the observed improvements stem from the structured rule format or other confounding factors like training efficiency.

2. **Cross-Family Semantic Alignment Test**: Expand the semantic alignment experiments to include more diverse model combinations (e.g., Llama→Mistral, Claude→GPT, and vice versa) and measure not just output quality but also rule interpretation consistency across models. This would test the generalizability of the semantic alignment findings.

3. **Long-Term Preference Stability Evaluation**: Implement a longitudinal study tracking preference agent performance over time with dynamic user preferences. This would validate whether the approach can adapt to evolving user styles and whether the learned rules remain effective as user preferences shift, addressing the real-world applicability beyond static datasets.