---
ver: rpa2
title: 'LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt
  Compression'
arxiv_id: '2403.12968'
source_url: https://arxiv.org/abs/2403.12968
tags:
- compression
- prompt
- original
- information
- llmlingua-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMLingua-2 addresses task-agnostic prompt compression by deriving
  compression knowledge through data distillation from GPT-4, creating an extractive
  text compression dataset, and formulating compression as a token classification
  task using a bidirectional Transformer encoder. The approach captures full context
  information, guarantees faithfulness to the original prompt, and achieves 3x-6x
  faster compression than existing methods.
---

# LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression

## Quick Facts
- arXiv ID: 2403.12968
- Source URL: https://arxiv.org/abs/2403.12968
- Reference count: 27
- Primary result: 3x-6x faster compression than existing methods with 2x-5x compression ratios

## Executive Summary
LLMLingua-2 introduces a task-agnostic prompt compression framework that addresses the growing need for efficient prompt processing in large language models. The method leverages data distillation from GPT-4 to create an extractive compression dataset, treating prompt compression as a token classification task using a bidirectional Transformer encoder. This approach achieves significant speedups while maintaining faithfulness to original prompts, with demonstrated improvements across both in-domain and out-of-domain datasets.

## Method Summary
The approach uses data distillation to extract compression knowledge from GPT-4, creating a training dataset where the oracle model's outputs guide the compression process. The compression task is formulated as token classification using a bidirectional Transformer encoder, which captures full contextual information from both directions. This extractive approach guarantees faithfulness to the original prompt while achieving 3x-6x faster compression compared to existing methods, with end-to-end latency improvements of 1.6x-2.9x.

## Key Results
- Achieves 3x-6x faster compression than existing methods
- Maintains 2x-5x compression ratios while preserving prompt semantics
- Improves end-to-end latency by 1.6x-2.9x across benchmark datasets
- Demonstrates significant performance gains over strong baselines on in-domain and out-of-domain datasets

## Why This Works (Mechanism)
The method works by leveraging data distillation from a powerful oracle model (GPT-4) to learn effective compression patterns, then applying these patterns through a bidirectional Transformer encoder that captures full contextual information. The extractive approach ensures faithfulness by selecting tokens directly from the original prompt rather than generating new content. The token classification formulation enables efficient processing compared to autoregressive generation methods.

## Foundational Learning

Token Classification: Understanding how to treat compression as a classification task where each token is labeled as keep/discard
- Why needed: Enables efficient, non-autoregressive compression processing
- Quick check: Verify classification accuracy on held-out data

Data Distillation: Process of extracting knowledge from a teacher model to create training data
- Why needed: Provides high-quality supervision signals for training
- Quick check: Compare distilled dataset quality against manual annotations

Bidirectional Context: Using bidirectional Transformer encoders to capture information from both directions
- Why needed: Ensures complete context understanding for accurate compression decisions
- Quick check: Test performance with unidirectional vs bidirectional encoders

## Architecture Onboarding

Component Map: GPT-4 (oracle) -> Data Distillation Pipeline -> Extractive Dataset -> Bidirectional Transformer Encoder -> Token Classification Output

Critical Path: Input prompt → Bidirectional encoder processing → Token classification scores → Compression mask → Output compressed prompt

Design Tradeoffs: Extractive vs abstractive compression (faithfulness vs potential for higher compression ratios), token classification vs generation (speed vs flexibility)

Failure Signatures: Over-compression losing critical information, under-compression providing minimal benefits, context misunderstanding leading to incorrect token selection

First Experiments:
1. Compare bidirectional vs unidirectional encoder performance on validation set
2. Test compression quality with different compression ratios
3. Evaluate faithfulness preservation using semantic similarity metrics

## Open Questions the Paper Calls Out

The paper notes that the assumption of extractive compression capturing full semantic content may not hold for highly abstractive or creative tasks. Additionally, the evaluation metrics may not fully capture semantic fidelity across all practical scenarios, particularly for specialized domains or languages not covered in the benchmark datasets.

## Limitations

The generalizability of the data distillation approach may be limited to specific GPT-4 setups, as compression dataset quality depends heavily on the oracle model's performance. The extractive nature of the approach may not preserve nuanced semantic relationships in all cases. Real-world speedups may vary based on implementation details and hardware configurations.

## Confidence

High confidence: Task-agnostic performance gains across multiple datasets, compression efficiency improvements (3x-6x faster), and faithfulness guarantees through extractive approach.

Medium confidence: Specific performance metrics may vary with different prompt types and model architectures; real-world implementation may affect observed speedups.

## Next Checks

1. Test the model on specialized domains (medical, legal, technical) to assess cross-domain robustness
2. Conduct ablation studies comparing bidirectional vs unidirectional Transformer architectures for this task
3. Evaluate compression performance on non-English prompts to test multilingual generalization