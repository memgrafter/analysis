---
ver: rpa2
title: On the Self-Verification Limitations of Large Language Models on Reasoning
  and Planning Tasks
arxiv_id: '2402.08115'
source_url: https://arxiv.org/abs/2402.08115
tags:
- object
- block
- vertex
- plan
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates whether large language models (LLMs) can
  improve their reasoning and planning performance through iterative self-critique.
  The core idea is to evaluate LLM performance across three domains (Game of 24, Graph
  Coloring, STRIPS planning) under three different setups: standard prompting, self-critique
  loops (LLM critiquing its own answers), and external sound verification (correct
  verifier providing feedback).'
---

# On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks

## Quick Facts
- arXiv ID: 2402.08115
- Source URL: https://arxiv.org/abs/2402.08115
- Reference count: 40
- One-line primary result: Self-verification consistently degrades LLM reasoning performance due to high false negative rates, while external sound verification significantly improves outcomes

## Executive Summary
This study investigates whether large language models can improve their reasoning and planning performance through iterative self-critique. The core finding is that self-verification consistently degrades performance due to high false negative rates when LLMs critique their own solutions. In contrast, external sound verification significantly improves outcomes by consistently accepting correct solutions. The paper demonstrates that even minimal feedback from a sound verifier maintains most performance gains, and iterative re-prompting without critique achieves similar results, suggesting LLMs are better treated as idea generators rather than autonomous reasoners.

## Method Summary
The study evaluates GPT-4 performance across three formal domains (Game of 24, Graph Coloring, STRIPS planning) under three different setups: standard prompting, self-critique loops (LLM critiquing its own answers), and external sound verification (correct verifier providing feedback). The methodology compares accuracy across these different prompting schemes while analyzing false positive and false negative rates in the verification process. Experiments were run with varying numbers of backprompt iterations (k=15, k=25) to measure performance scaling.

## Key Results
- Self-critique consistently degrades performance due to high false negative rates in LLM verification
- External sound verification significantly improves outcomes by properly accepting correct solutions
- Iterative re-prompting without any critique achieves similar performance gains as iterative critique with sound verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-verification fails because LLMs generate high false negative rates when critiquing their own solutions
- Mechanism: The same model that generates solutions also verifies them, leading to systematic rejection of correct answers due to model uncertainty or hallucinated errors
- Core assumption: Verification requires different cognitive processes than generation, and LLMs cannot simultaneously excel at both
- Evidence anchors:
  - [abstract] "We observe significant performance collapse with self-critique"
  - [section 5] "Our analysis reveals that the verifier LLM's false negative rate is significant across our domains"
  - [corpus] Weak - corpus contains related work but no direct verification accuracy metrics
- Break condition: If a model architecture could separate generation and verification capabilities (e.g., through specialized heads or different fine-tuning)

### Mechanism 2
- Claim: External sound verification succeeds because it provides consistent acceptance of correct solutions
- Mechanism: A separate verifier with sound logic (e.g., constraint checking, arithmetic evaluation) accepts all valid solutions, creating monotonic improvement through iterative refinement
- Core assumption: Sound verification is computationally easier than generation for formal reasoning tasks
- Evidence anchors:
  - [abstract] "significant performance gains with sound external verification"
  - [section 5] "When we replace the LLM verifier with a sound verifier, every correct answer will be accepted properly"
  - [corpus] Weak - corpus mentions verification but lacks specific sound verifier implementation details
- Break condition: If the sound verifier becomes a bottleneck due to computational complexity or if it introduces false positives

### Mechanism 3
- Claim: Iterative refinement without critique works because repeated guessing with sound verification eventually finds correct solutions
- Mechanism: The LLM generates multiple solutions independently, and the sound verifier filters for correctness, with performance improving as the number of attempts increases
- Core assumption: Solution space is large enough that repeated independent sampling will eventually hit correct answers
- Evidence anchors:
  - [abstract] "merely re-prompting with a sound verifier maintains most of the benefits"
  - [section 5.2] "This allows us to increase performance further by just increasing k further"
  - [corpus] Weak - corpus mentions sampling but doesn't provide specific iteration analysis
- Break condition: If solution space is too sparse or if sampling becomes computationally prohibitive

## Foundational Learning

- Concept: Computational complexity of verification vs. generation
  - Why needed here: The paper challenges the assumption that verification is easier than generation for LLMs, which is central to understanding why self-critique fails
  - Quick check question: Why does the classical complexity argument (verification easier than generation) not apply to LLMs in reasoning tasks?

- Concept: False positive vs. false negative rates in verification
  - Why needed here: The paper shows that false negatives (rejecting correct solutions) are the primary problem in self-verification, not false positives
  - Quick check question: What happens to system performance when a verifier has high false negative rates versus high false positive rates?

- Concept: Iterative refinement algorithms
  - Why needed here: Understanding how repeated attempts with verification can improve performance without requiring critique is key to the paper's main finding
  - Quick check question: How does performance scale with the number of independent attempts when using a sound verifier?

## Architecture Onboarding

- Component map: LLM (solution generator) → Sound Verifier → Accept/Reject → Feedback loop (if rejected) → Final solution
- Critical path: The sound verifier must be the bottleneck - if it's too slow or incorrect, the entire system fails
- Design tradeoffs: More attempts improve performance but increase computational cost; more complex verifiers are more accurate but slower
- Failure signatures: Performance collapse when using self-verification, plateauing performance with external verification, improvement with increased sampling attempts
- First 3 experiments:
  1. Compare self-verification vs. external verification on a simple formal task (e.g., Game of 24)
  2. Measure false positive vs. false negative rates of the LLM verifier on correct vs. incorrect solutions
  3. Test iterative refinement with external verification, measuring performance vs. number of attempts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance improvement from re-prompting without critique generalize to other reasoning domains beyond Game of 24, Graph Coloring, and STRIPS planning?
- Basis in paper: [explicit] The paper shows re-prompting with sound verifier achieves comparable gains to iterative critique in their three tested domains
- Why unresolved: The study only examined three specific formal reasoning tasks, leaving uncertainty about whether this finding extends to broader reasoning domains
- What evidence would resolve it: Testing re-prompting performance across a diverse set of reasoning domains (e.g., theorem proving, puzzle solving, logical inference) while comparing to iterative critique approaches

### Open Question 2
- Question: What is the minimum amount of feedback required from a sound verifier to maintain performance gains while minimizing token costs?
- Basis in paper: [explicit] The paper notes that performance gains can be achieved without any critique, suggesting that even minimal feedback might suffice
- Why unresolved: The study only compared binary feedback, first error feedback, and all errors feedback - leaving a gap in understanding the minimal effective feedback threshold
- What evidence would resolve it: Systematic testing of feedback granularity (e.g., single-bit vs multi-bit vs full explanation) while measuring both performance and token efficiency

### Open Question 3
- Question: How do the false negative rates of LLM verifiers compare across different prompting techniques (e.g., Chain of Thought, ReAct, Tree of Thoughts)?
- Basis in paper: [explicit] The paper shows high false negative rates in LLM verifiers but only tests basic prompting approaches
- Why unresolved: The study doesn't explore whether advanced prompting techniques can reduce the false negative rates that cause performance collapse
- What evidence would resolve it: Comparative analysis of false negative rates across multiple prompting techniques while maintaining consistent verification tasks and evaluation metrics

## Limitations
- Limited to GPT-4 model, uncertain if findings generalize to other LLMs
- Only tested three formal reasoning domains, may not represent broader reasoning tasks
- Analysis of false negative rates lacks deep investigation into underlying mechanisms

## Confidence
- High confidence: External sound verification improves performance while self-verification degrades it
- Medium confidence: False negatives as the primary failure mode in self-verification
- Medium confidence: Practical recommendation to treat LLMs as idea generators with external verification

## Next Checks
1. Cross-model validation: Test the same experimental setup with different LLMs (Claude, Gemini, open-source models) to determine if the self-verification limitations are universal or specific to GPT-4's architecture.
2. False negative rate decomposition: Conduct ablation studies to isolate whether high false negative rates stem from model uncertainty, hallucinated errors, or fundamental limitations in the model's verification capabilities.
3. Real-world task generalization: Apply the experimental framework to less formal reasoning tasks (legal reasoning, medical diagnosis, creative problem-solving) to test whether the self-verification limitations persist outside of structured formal domains.