---
ver: rpa2
title: Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning
  Recommendations
arxiv_id: '2403.03008'
source_url: https://arxiv.org/abs/2403.03008
tags:
- learning
- explanations
- explanation
- which
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method that leverages knowledge graphs to
  provide factual context for large language model (LLM) prompts, thereby improving
  the precision and relevance of generated explanations for learning recommendations.
  The approach extracts semantic relations and metadata from the knowledge graph to
  construct an enriched prompt for GPT-4, guiding it to generate more focused and
  informative explanations.
---

# Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations

## Quick Facts
- arXiv ID: 2403.03008
- Source URL: https://arxiv.org/abs/2403.03008
- Reference count: 23
- Primary result: KG-based contextualization improves LLM-generated explanation precision for learning recommendations.

## Executive Summary
This paper proposes using knowledge graphs (KGs) as context sources to enhance the precision and relevance of large language model (LLM) explanations for learning recommendations. By extracting semantic relations and metadata from the KG, the authors construct enriched prompts for GPT-4, guiding it to generate more focused and informative explanations. The approach is evaluated using ROUGE metrics and user studies, demonstrating improved recall, precision, and F1-scores compared to GPT-only explanations. The method also reduces irrelevant content in generated explanations, enhancing their pedagogical value.

## Method Summary
The method involves constructing a KG from educational materials, extracting semantic relations and metadata, and designing an LLM prompt with KG-based context. GPT-4 generates explanations using this prompt, which are then evaluated using ROUGE metrics against human-generated reference explanations. The approach also incorporates expert-in-the-loop design for explanation templates and user feedback to ensure pedagogical alignment and quality.

## Key Results
- KG-based contextualization significantly improves recall, precision, and F1-scores of LLM-generated explanations compared to GPT-only explanations.
- User studies confirm improved quality and reduced irrelevant content in KG-based explanations.
- The approach enhances the relevance of explanations to learning recommendation intent by providing factual semantic relations and metadata.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graph contextualization improves LLM-generated explanation precision by providing curated semantic relations and metadata.
- Mechanism: The KG supplies domain-specific facts, relations, and hierarchical structure as prompt context, which guides the LLM to generate explanations tightly aligned with the learning recommendation intent.
- Core assumption: The KG accurately captures relevant semantic relations between learning objects and their metadata.
- Evidence anchors:
  - [abstract] "We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations."
  - [section] "We utilize the features of KGs to enhance the querying process of LLMs, through extracting contextual knowledge from the KG and then performing an informed prompt engineering process."
  - [corpus] "Knowledge Graph-based recommendations have gained significant attention due to their ability to leverage rich semantic relationships."
- Break condition: If KG semantic relations are sparse, noisy, or misaligned with pedagogical goals, the LLM prompt context loses relevance and precision gains disappear.

### Mechanism 2
- Claim: Expert-in-the-loop prompt engineering ensures pedagogical alignment of LLM-generated explanations.
- Mechanism: Domain experts design explanation templates and influence prompt context construction, ensuring that the LLM outputs meet educational explainability standards.
- Core assumption: Expert involvement improves the relevance and quality of generated explanations beyond what automated methods alone achieve.
- Evidence anchors:
  - [abstract] "With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM."
  - [section] "The design of explanations for the learning recommendations is tightly connected to the pedagogical value of the explanation."
  - [corpus] "Researchers have therefore proposed solutions such as open learner models and letting learners select from ranked recommendations, which engage learners before or after the AI-supported..."
- Break condition: If expert inputs are inconsistent, outdated, or fail to capture evolving pedagogical best practices, the explanation quality will degrade.

### Mechanism 3
- Claim: Using ROUGE metrics on word and pattern overlap provides quantitative validation of explanation precision improvement.
- Mechanism: ROUGE-N, ROUGE-L, and ROUGE-Lsum scores compare LLM-generated explanations with expert reference explanations to measure precision and recall gains.
- Core assumption: Reference explanations accurately represent high-quality pedagogical explanations.
- Evidence anchors:
  - [abstract] "We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners."
  - [section] "We calculate the recall, precision, and f1-measure from four Rouge measures: Rouge-1, Rouge-2, Rouge-L, and Rouge-Lsum."
  - [corpus] "Knowledge Graph-based recommendations have gained significant attention due to their ability to leverage rich semantic relationships."
- Break condition: If reference explanations are of low quality or do not reflect expert consensus, ROUGE scores may misrepresent actual explanation quality.

## Foundational Learning

- Concept: Knowledge Graphs (KGs)
  - Why needed here: KGs provide structured semantic relations and metadata that serve as factual context for LLM prompts, reducing hallucinations and improving precision.
  - Quick check question: What are the two main components that define a knowledge graph triple?
    - Answer: Head entity, relation, tail entity (h, r, t).

- Concept: Prompt Engineering for LLMs
  - Why needed here: Prompt engineering shapes the LLM output by providing task instructions and contextual information, ensuring relevance and precision.
  - Quick check question: What are the two main parts of the GPT-4 prompt design in this approach?
    - Answer: Prompt body (task instructions) and contextual part (KG-derived facts and relations).

- Concept: Evaluation Metrics (ROUGE)
  - Why needed here: ROUGE metrics quantify the overlap between generated explanations and expert references, measuring precision, recall, and f1-score improvements.
  - Quick check question: Which ROUGE measure focuses on sentence-level overlap rather than word-level?
    - Answer: ROUGE-Lsum.

## Architecture Onboarding

- Component map: Knowledge Graph -> Relation Extraction Pipeline -> GPT-4 API Integration -> Expert-in-the-Loop Interface -> Evaluation Module

- Critical path:
  1. KG construction from educational materials.
  2. Relation extraction and community detection.
  3. Prompt context assembly with expert guidance.
  4. GPT-4 explanation generation.
  5. Template filling and user presentation.
  6. Evaluation via ROUGE and user studies.

- Design tradeoffs:
  - KG richness vs. maintenance cost: Richer KGs improve LLM context but require more curation effort.
  - Expert involvement vs. scalability: Expert inputs improve quality but limit scalability.
  - ROUGE evaluation vs. human judgment: ROUGE provides quantitative validation but may miss nuanced quality aspects.

- Failure signatures:
  - Low ROUGE scores despite KG presence: KG relations may be irrelevant or noisy.
  - Poor user feedback despite high ROUGE scores: Explanation phrasing or reflection depth may be insufficient.
  - LLM generates irrelevant or incorrect content: Prompt context may be incomplete or poorly structured.

- First 3 experiments:
  1. Build a small KG with 50 learning objects and test relation extraction accuracy.
  2. Compare GPT-4 outputs with and without KG context using ROUGE on a sample set of 10 explanations.
  3. Conduct a user study with 5 learners and 3 experts to gather qualitative feedback on explanation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the phrasing and sentence structure of LLM-generated explanations impact their effectiveness and comprehensibility for learners, beyond just the selection of relevant words and concepts?
- Basis in paper: [explicit] The paper mentions that domain experts emphasized the importance of phrasing and sentence structure in determining the meaning and effectiveness of explanations, beyond just word selection.
- Why unresolved: The current evaluation approach focuses on word choice and patterns but does not account for phrasing or writing style. Evaluating phrasing is complex and subjective, and the paper acknowledges this as a limitation.
- What evidence would resolve it: User studies comparing the effectiveness and comprehensibility of explanations with different phrasing styles, or developing automated metrics to evaluate the quality of explanation phrasing.

### Open Question 2
- How can user-specific data be incorporated into the knowledge graph-based contextualization approach to further personalize explanations while ensuring compliance with data protection regulations like GDPR?
- Basis in paper: [explicit] The paper mentions that user-specific data is not included in the current approach due to GDPR compliance concerns with third-party LLMs like GPT-4. It suggests using a local LLM for future evaluation of user data's role in enhancing explanation relevance.
- Why unresolved: Incorporating user data raises privacy concerns and requires careful consideration of data protection regulations. The paper does not provide a concrete solution or evaluation of this aspect.
- What evidence would resolve it: Case studies or pilot studies demonstrating the effectiveness of user-specific data incorporation while ensuring GDPR compliance, or developing techniques for anonymizing and securely using user data in the knowledge graph.

### Open Question 3
- How do different LLM architectures and training data impact the quality and relevance of generated explanations compared to GPT-4, and what are the trade-offs between model capabilities and contextualization requirements?
- Basis in paper: [explicit] The paper uses GPT-4 as the primary LLM for generating explanations and acknowledges the need for future comparison with other LLMs to measure performance deviations.
- Why unresolved: The paper focuses on GPT-4 and does not explore the impact of different LLM architectures or training data on explanation quality. The trade-offs between model capabilities and contextualization requirements are not explicitly discussed.
- What evidence would resolve it: Comparative studies evaluating the performance of different LLM architectures and training data on explanation generation tasks, considering factors like precision, recall, and relevance to user intent.

## Limitations
- The evaluation relies heavily on ROUGE metrics, which primarily measure surface-level lexical overlap and may not fully capture the pedagogical quality or learner comprehension of the explanations.
- The qualitative user studies with domain experts and learners provide valuable feedback but lack detail on sample size, study design, and statistical significance.
- The paper does not discuss potential biases in the KG construction or the impact of KG quality on explanation precision.

## Confidence

**Major uncertainties:**
The evaluation relies heavily on ROUGE metrics, which primarily measure surface-level lexical overlap and may not fully capture the pedagogical quality or learner comprehension of the explanations. The qualitative user studies with domain experts and learners provide valuable feedback but lack detail on sample size, study design, and statistical significance. Additionally, the paper does not discuss potential biases in the KG construction or the impact of KG quality on explanation precision.

**Confidence labels:**
- **High confidence** in the mechanism that KG-based contextualization improves LLM precision by providing factual semantic relations and metadata.
- **Medium confidence** in the expert-in-the-loop prompt engineering approach, as the impact of expert involvement on explanation quality is not quantitatively validated.
- **Medium confidence** in the quantitative evaluation using ROUGE metrics, as these measures may not fully reflect pedagogical effectiveness.

## Next Checks
1. Conduct a controlled experiment comparing KG-based explanations with alternative context sources (e.g., embeddings, taxonomies) to isolate the specific contribution of KGs to explanation precision.
2. Perform a user study with a larger sample size and standardized evaluation criteria to assess the pedagogical effectiveness of KG-based explanations on learner comprehension and engagement.
3. Analyze the impact of KG quality and coverage on explanation precision by systematically varying the richness and accuracy of the KG used for prompt contextualization.