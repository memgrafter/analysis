---
ver: rpa2
title: Is the neural tangent kernel of PINNs deep learning general partial differential
  equations always convergent ?
arxiv_id: '2412.06158'
source_url: https://arxiv.org/abs/2412.06158
tags:
- neural
- convergence
- when
- networks
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the Neural Tangent Kernel (NTK) for general
  partial differential equations (PDEs) using physics-informed neural networks (PINNs).
  The authors study the initialization and convergence conditions of NTK during training
  for general PDEs, finding that the homogeneity of differential operators plays a
  crucial role for NTK convergence.
---

# Is the neural tangent kernel of PINNs deep learning general partial differential equations always convergent ?

## Quick Facts
- **arXiv ID**: 2412.06158
- **Source URL**: https://arxiv.org/abs/2412.06158
- **Reference count**: 40
- **Primary result**: The NTK of PINNs converges to a deterministic limiting kernel in probability when the width N of the neural network tends to infinity and the homogeneity coefficient s is sufficiently large.

## Executive Summary
This paper analyzes the Neural Tangent Kernel (NTK) convergence for physics-informed neural networks (PINNs) solving general partial differential equations (PDEs). The authors establish that the convergence of NTK depends critically on the homogeneity of differential operators and the scaling parameter s. They prove that the initialized NTK converges in probability to a deterministic kernel when s ≥ 1/2 for boundary terms and s ≥ 1 for other terms, with convergence to a constant matrix during training when s > 1/4. The theoretical results are validated through numerical experiments on the sine-Gordon and KdV equations.

## Method Summary
The authors analyze PINNs with fully connected neural networks using smooth activation functions (specifically tanh) to solve general PDEs. They introduce a scaling parameter 1/N^s applied to the output layer and study the convergence of the Neural Tangent Kernel as the network width N approaches infinity. The analysis focuses on how the homogeneity of differential operators affects NTK convergence, with specific conditions derived for different components of the kernel. Numerical validation is performed on the sine-Gordon equation (initial value problem) and KdV equation (initial-boundary value problem) by varying the network width and convergence coefficient s across multiple independent experiments.

## Key Results
- The NTK of PINNs converges to a deterministic limiting kernel in probability when N → ∞ and s ≥ 1/2 for boundary terms (Kbb)
- For other kernel components (Kff and Kbf), convergence occurs when s ≥ 1 (except when all monomials are homogeneous, then s ≥ s1)
- When s > 1/4, the NTK remains a constant matrix during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NTK of PINNs converges to a deterministic limiting kernel in probability when the width N of the neural network tends to infinity and the homogeneity coefficient s is sufficiently large.
- Mechanism: The convergence of the NTK is determined by the balance between the number of neurons and the scaling parameter N^(-s). When s is sufficiently large, the law of large numbers ensures that the kernel converges to a deterministic limit.
- Core assumption: The activation function is smooth and its derivatives are bounded, and the differential operator F is continuous.
- Evidence anchors:
  - [abstract] "The theoretical results show that the homogeneity of differential operators plays a crucial role for the convergence of NTK."
  - [section] "The convergence of Kbb occurs in probability when s ≥ 1/2. In the case where F0[q, qx, qxx, ..., qnx] · F0[ ˆq, ˆqx, ˆqxx, ..., ˆqnx] ̸= 0 ( F0[q, qx, qxx, ..., qnx] ̸= 0), or if there exists a non-homogeneous monomial in the polynomial {Fi[q, qx, qxx, ..., qnx]}n i=1 (referred to as case A), the convergence of K f f (Kb f ) occurs in probability when s ≥ 1."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.447, average citations=0.0."
- Break condition: The convergence fails when s is too small, specifically when s < s1 (defined in Eq. 39) or s < s2 (defined in Eq. 47).

### Mechanism 2
- Claim: The NTK remains a constant matrix during training when N → ∞ and s > 1/4.
- Mechanism: Under certain assumptions, the NTK matrix remains close to its initial value throughout the training process, effectively acting as a constant matrix.
- Core assumption: The network parameters remain uniformly bounded during training, the derivatives of the equation are uniformly bounded, and the activation function is smooth with bounded derivatives.
- Evidence anchors:
  - [abstract] "When s > 1/4, the NTK remains a constant matrix during training"
  - [section] "Theorem 2.3 For the loss function (3), if the following assumptions are satisfied for any T > 0: (i) For t ∈ T, all parameters of the network are uniformly bounded, i.e., there exists a constant C > 0 (independent on N) such that sup t∈ [0,T] ∥θ(t)∥∞ ≤ C, (ii) The derivatives of the equation are uniformed bounded, i.e. there exists a constant C > 0 (independent on n) such that sup i∈{ 0,1,...,n} ∥Fi[q, qx, qxx, ..., qnx]∥∞ ≤ C,"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.447, average citations=0.0."
- Break condition: The NTK may not remain constant during training if the assumptions are violated, such as if the derivatives of the equation are unbounded.

### Mechanism 3
- Claim: The convergence of the NTK is influenced by the homogeneity of the differential operator and the presence of non-homogeneous terms.
- Mechanism: The presence of non-homogeneous terms in the differential operator can lead to divergence of the initialized NTK unless the scaling parameter s is sufficiently large.
- Core assumption: The differential operator F can be decomposed into homogeneous and non-homogeneous parts.
- Evidence anchors:
  - [abstract] "The theoretical results show that the homogeneity of differential operators plays a crucial role for the convergence of NTK."
  - [section] "In fact, in the examples in the first and third sections, we observe instances where the initialized NTK diverges."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.447, average citations=0.0."
- Break condition: The convergence fails when the differential operator contains non-homogeneous terms and the scaling parameter s is not sufficiently large.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: Understanding NTK is crucial for analyzing the training dynamics of PINNs and their convergence properties.
  - Quick check question: What is the relationship between the NTK and the training dynamics of neural networks?

- Concept: Physics-Informed Neural Networks (PINNs)
  - Why needed here: PINNs are the specific type of neural network used to solve PDEs in this paper, and their properties are central to the analysis.
  - Quick check question: How do PINNs incorporate physical laws into the loss function?

- Concept: Homogeneity of differential operators
  - Why needed here: The homogeneity of the differential operator is a key factor in determining the convergence of the NTK for PINNs.
  - Quick check question: How does the homogeneity of a differential operator affect the convergence of the NTK?

## Architecture Onboarding

- Component map: PINNs (neural network architecture) -> NTK (kernel matrix) -> Homogeneity coefficient s (scaling parameter) -> Differential operator F (PDE definition)

- Critical path:
  1. Initialize the PINN with appropriate weights and biases
  2. Compute the NTK matrix based on the current parameters
  3. Analyze the convergence of the NTK based on the homogeneity of the differential operator and the value of s
  4. Train the PINN using gradient descent, monitoring the behavior of the NTK

- Design tradeoffs:
  - Larger s values generally lead to better convergence but may require more computational resources
  - The choice of activation function can affect the convergence of the NTK
  - The complexity of the differential operator can impact the convergence behavior

- Failure signatures:
  - Divergence of the initialized NTK when s is too small
  - Failure of the NTK to remain constant during training when assumptions are violated
  - Poor performance of the PINN in solving the PDE

- First 3 experiments:
  1. Implement a PINN to solve a simple PDE with a known solution, and analyze the convergence of the NTK for different values of s
  2. Compare the performance of PINNs with different activation functions in terms of NTK convergence
  3. Investigate the impact of non-homogeneous terms in the differential operator on the convergence of the NTK

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does the Neural Tangent Kernel (NTK) of Physics-Informed Neural Networks (PINNs) fail to converge for general partial differential equations (PDEs)?
- Basis in paper: [explicit] The paper explicitly discusses that the original NTK theory for PINNs only applies to specific cases like the Poisson equation and does not account for general PDEs. The authors provide examples where the initialized NTK diverges, particularly when dealing with non-homogeneous terms in the PDEs.
- Why unresolved: The paper identifies that the convergence of NTK is influenced by the homogeneity of differential operators and the scaling coefficient, but it does not provide a comprehensive set of conditions under which NTK fails for all types of general PDEs.
- What evidence would resolve it: Detailed theoretical analysis and numerical experiments that systematically explore different types of PDEs (linear, nonlinear, homogeneous, non-homogeneous) and identify the specific conditions leading to NTK divergence.

### Open Question 2
- Question: How does the choice of activation function in PINNs affect the convergence of the Neural Tangent Kernel (NTK) during training?
- Basis in paper: [inferred] The paper mentions that the nonlinear activation function exhibits non-homogeneity, which affects the convergence of initialized NTK. It also discusses the convergence conditions for NTK during training when s > 1/4, implying that the activation function plays a role in NTK behavior.
- Why unresolved: While the paper acknowledges the impact of the activation function on NTK convergence, it does not provide a detailed analysis of how different activation functions (e.g., ReLU, Sigmoid, Tanh) specifically influence the NTK behavior during training.
- What evidence would resolve it: Comparative studies using various activation functions in PINNs, analyzing their impact on NTK convergence during training for different types of PDEs.

### Open Question 3
- Question: What is the optimal scaling coefficient s for ensuring the convergence of the Neural Tangent Kernel (NTK) in PINNs for a wide range of PDEs?
- Basis in paper: [explicit] The paper discusses the importance of the scaling coefficient s in determining the convergence of NTK. It provides specific conditions for convergence, such as s ≥ 1/2 for Kbb and s ≥ 1 for Kff and Kbf in certain cases, but does not offer a universal optimal value for s.
- Why unresolved: The paper identifies that the optimal value of s depends on the homogeneity of the differential operators and the presence of non-homogeneous terms, but it does not provide a comprehensive analysis to determine the best s for all types of PDEs.
- What evidence would resolve it: Systematic numerical experiments and theoretical analysis that explore the relationship between s and NTK convergence across a wide range of PDEs, leading to a set of guidelines for choosing s based on the characteristics of the PDE.

## Limitations
- The analysis assumes infinite-width neural networks and smooth activation functions, which may not reflect practical implementations
- The convergence conditions are proven under specific assumptions about differential operators that may not hold for all PDEs
- Numerical experiments are limited to only two specific equations (sine-Gordon and KdV), constraining generalizability

## Confidence
- **High**: The claim that NTK convergence depends on the homogeneity coefficient s is well-supported by both theoretical proofs and numerical experiments
- **Medium**: The specific threshold values (s ≥ 1/2, s ≥ 1, s > 1/4) are derived theoretically but may have narrower applicability than stated
- **Low**: The claim about NTK remaining constant during training for s > 1/4 relies heavily on the boundedness assumptions, which may not hold in practice

## Next Checks
1. Test the convergence conditions across a broader range of PDEs with varying degrees of homogeneity, including equations with fractional powers and non-polynomial terms
2. Implement the theoretical framework with different activation functions (ReLU, softplus) to verify if the convergence conditions remain valid beyond smooth activations
3. Analyze the practical implications by training PINNs with finite width on real-world PDE problems to assess the gap between theoretical convergence conditions and practical performance