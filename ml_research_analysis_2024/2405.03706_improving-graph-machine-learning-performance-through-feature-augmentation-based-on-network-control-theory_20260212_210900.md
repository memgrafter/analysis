---
ver: rpa2
title: Improving Graph Machine Learning Performance Through Feature Augmentation Based
  on Network Control Theory
arxiv_id: '2405.03706'
source_url: https://arxiv.org/abs/2405.03706
tags:
- graph
- node
- network
- controllability
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to improve graph machine learning
  performance by incorporating network control theory (NCT) into feature augmentation.
  The authors address the challenge of missing or sparse node features in graph neural
  networks (GNNs) by leveraging NCT metrics, including average controllability, closeness
  centrality, betweenness centrality, and eigenvector centrality.
---

# Improving Graph Machine Learning Performance Through Feature Augmentation Based on Network Control Theory

## Quick Facts
- **arXiv ID**: 2405.03706
- **Source URL**: https://arxiv.org/abs/2405.03706
- **Reference count**: 30
- **Primary result**: NCT-based feature augmentation improves GNN performance by up to 11% ROC AUC on social network datasets

## Executive Summary
This paper proposes NCT-EFA, a framework that improves graph machine learning performance by augmenting node features with network control theory metrics. The authors address the challenge of missing or sparse node features in GNNs by computing four NCT metrics (average controllability, closeness centrality, betweenness centrality, and eigenvector centrality) and integrating them into the feature set. They introduce a novel rank-based one-hot encoding scheme for average controllability. Evaluated on six GNN models across two social network datasets, the approach demonstrates significant performance improvements over one-hot-degree encoding baselines, with ROC AUC scores increasing by up to 11%.

## Method Summary
NCT-EFA computes network control theory metrics for each node from graph topology alone, then constructs augmented features by combining these metrics with a unique rank-based one-hot encoding of average controllability. The framework integrates these features into standard GNN architectures through feature augmentation. The authors evaluate their approach using 3-layer GNNs with 64 hidden units, Sort Aggregation, 2-layer 1D convolution with Max Pooling, and 2-layer MLP with 32 hidden neurons, trained with 10-fold cross-validation for 100 epochs at learning rate 1e-4 and weight decay 5e-2.

## Key Results
- NCT-EFA achieves up to 11% improvement in ROC AUC scores compared to one-hot-degree encoding
- Performance gains observed across six different GNN architectures (k-GNN, SAGE, GCN, UniMP, ResGatedGCN, GAT)
- Improvements demonstrated on two social network datasets: Reddit Threads and GitHub Stargazers
- Ablation study confirms effectiveness of the proposed rank encoding scheme for average controllability

## Why This Works (Mechanism)

### Mechanism 1
Augmenting node features with NCT metrics improves GNN expressiveness by encoding structural roles of nodes. Network control theory metrics capture node-level properties predictive for classification tasks. Core assumption: these metrics are computable from topology and differentiate nodes meaningfully. Evidence: evaluation shows 11% ROC AUC improvement. Break condition: if graph topology is too sparse or noisy, NCT metrics may not differentiate nodes.

### Mechanism 2
One-hot encoding of average controllability ranks provides discriminative features capturing relative influence ordering. By binning values into k ranks and one-hot encoding, each node gets a unique categorical feature indicating controllability hierarchy position. Core assumption: relative ranking is stable and meaningful for classification. Evidence: ablation study demonstrates effectiveness. Break condition: if distribution is too flat or peaked, encoding loses discriminative power.

### Mechanism 3
Combining NCT-EFA features with standard GNN message-passing improves node representation quality. NCT features act as additional attributes in message-passing update equations, allowing GNNs to leverage both local structure and global network control properties. Core assumption: GNNs can integrate external features without significant overfitting. Evidence: results show improved performance. Break condition: if dataset is large with already informative features, NCT augmentation may have negligible effect or hurt performance.

## Foundational Learning

- **Network Control Theory metrics computation**: NCT metrics (average controllability, closeness, betweenness, eigenvector centrality) are core augmentation features derived from graph topology. Quick check: Can you manually compute average controllability for each node using the controllability Gramian formulation on a small graph?

- **Graph Neural Network message passing and feature integration**: NCT-EFA augments node features and passes them into GNN; understanding how external features interact with message passing is key. Quick check: How does a GNN layer update node features when both topological messages and external attributes are present?

- **Feature encoding and representation (one-hot, rank-based)**: NCT-EFA uses rank-based one-hot encoding of average controllability; understanding this encoding is critical for correct feature construction. Quick check: Given average controllability values, how do you construct a rank-based one-hot feature vector for each node?

## Architecture Onboarding

- **Component map**: Graph topology (adjacency matrix A) -> NCT metrics + rank encoding -> augmented node features -> GNN layers with message passing -> node/graph embeddings -> classification output
- **Critical path**: Compute NCT metrics → Construct one-hot rank encodings → Augment node features → Feed into GNN → Train and evaluate
- **Design tradeoffs**: More NCT metrics → richer features but higher computational cost; more rank encoding bins → finer granularity but risk of sparsity; deeper GNN → higher capacity but overfitting risk
- **Failure signatures**: No performance gain over baseline → feature augmentation not informative; ROC AUC drops → overfitting or noisy features; NCT metrics fail to compute → need preprocessing
- **First 3 experiments**: 1) Baseline: One-hot degree encoding + GNN; 2) NCT-EFA with all four metrics + standard one-hot encoding; 3) NCT-EFA with only average controllability rank encoding

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implied by the limitations discussed.

## Limitations
- The paper lacks ablation studies isolating contributions of individual NCT metrics and the rank encoding scheme
- Computational complexity of NCT metrics (especially average controllability) is not discussed, raising scalability concerns
- The selection of four specific NCT metrics appears arbitrary without justification for why these over other available measures

## Confidence
- **High confidence**: The general approach of augmenting GNN features with structural network properties is well-supported by literature and empirical results
- **Medium confidence**: The specific mechanism by which rank encoding provides benefits is plausible but not rigorously proven; ablation study only tests combinations
- **Medium confidence**: Scalability claims are reasonable but unverified as computational costs are not reported

## Next Checks
1. Conduct ablation study measuring individual contribution of each NCT metric and the rank encoding scheme to isolate performance drivers
2. Evaluate computational complexity and runtime overhead of computing NCT metrics on larger graphs to assess practical scalability
3. Test the framework on additional benchmark datasets beyond the two social networks to verify generalizability