---
ver: rpa2
title: 'Enhancing Indonesian Automatic Speech Recognition: Evaluating Multilingual
  Models with Diverse Speech Variabilities'
arxiv_id: '2410.08828'
source_url: https://arxiv.org/abs/2410.08828
tags:
- speech
- indonesian
- data
- recognition
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of developing Indonesian automatic
  speech recognition (ASR) systems that can handle diverse speech variabilities including
  speaking style (read vs. spontaneous), speech context (formal vs.
---

# Enhancing Indonesian Automatic Speech Recognition: Evaluating Multilingual Models with Diverse Speech Variabilities

## Quick Facts
- **arXiv ID**: 2410.08828
- **Source URL**: https://arxiv.org/abs/2410.08828
- **Reference count**: 0
- **Primary result**: Fine-tuned Whisper model achieved WER of 14.85% and CER of 5.44% on Indonesian speech across speaking styles, contexts, and noise conditions

## Executive Summary
This study addresses the challenge of developing robust Indonesian automatic speech recognition (ASR) systems that can handle diverse speech variabilities. The researchers fine-tuned state-of-the-art multilingual models (MMS and Whisper) on a comprehensive Indonesian speech dataset containing 80.54 hours of data spanning different speaking styles (read vs. spontaneous), speech contexts (formal vs. informal), and background noise conditions (clean vs. moderate). The fine-tuned Whisper model demonstrated superior performance across all test conditions, with the highest accuracy on spontaneous informal clean speech. The study reveals that speaking style variability has the most significant impact on model performance, while background noise conditions show less influence on recognition accuracy.

## Method Summary
The research employed a fine-tuning approach using state-of-the-art multilingual models on a comprehensive Indonesian speech dataset. The dataset encompassed various speech variabilities including speaking style (read vs. spontaneous), speech context (formal vs. informal), and background noise conditions (clean vs. moderate). The study systematically evaluated model performance across these dimensions using both MMS and Whisper architectures. The fine-tuning process involved adapting pre-trained multilingual models to the specific characteristics of Indonesian speech data, with particular attention to the diverse speaking styles and contexts present in natural communication.

## Key Results
- Fine-tuned Whisper model achieved the best overall performance with WER of 14.85% and CER of 5.44%
- Highest accuracy observed on spontaneous informal clean speech with WER of 13.04% and CER of 5.85%
- Speaking style variability had the most significant influence on model performance
- MMS models showed degraded performance on read formal moderate data compared to pre-trained versions

## Why This Works (Mechanism)
The effectiveness of the fine-tuning approach stems from adapting multilingual models to capture the specific acoustic and linguistic patterns of Indonesian speech across diverse conditions. By training on data that includes both read and spontaneous speech, formal and informal contexts, and varying noise levels, the models learn to handle the full spectrum of real-world Indonesian speech scenarios. The Whisper model's superior performance likely results from its larger capacity and more robust pre-training on diverse multilingual data, enabling better generalization when fine-tuned on Indonesian-specific variations.

## Foundational Learning

**Automatic Speech Recognition (ASR)**: The technology that converts spoken language into written text, essential for voice interfaces and transcription services.
*Why needed*: Forms the foundation for understanding speech processing challenges in Indonesian language.
*Quick check*: Can the model accurately transcribe both formal speeches and casual conversations?

**Multilingual Models**: Pre-trained models that understand multiple languages and can be fine-tuned for specific language tasks.
*Why needed*: Leverages knowledge from multiple languages to improve performance on low-resource languages like Indonesian.
*Quick check*: Does fine-tuning improve performance compared to using the model without adaptation?

**Speech Variability**: The natural variations in speech including speaking style, context, and background conditions that affect recognition accuracy.
*Why needed*: Real-world speech is highly variable, and models must handle these differences to be practical.
*Quick check*: How does performance change when switching from read to spontaneous speech?

**Fine-tuning**: The process of adapting pre-trained models to specific tasks or domains by continuing training on domain-specific data.
*Why needed*: Allows leveraging pre-trained knowledge while specializing for Indonesian speech characteristics.
*Quick check*: Does fine-tuning improve WER compared to the original pre-trained model?

## Architecture Onboarding

**Component Map**: Pre-trained Multilingual Model -> Fine-tuning Dataset (IDSV) -> Fine-tuned Model -> Evaluation on Test Sets (varying speaking style, context, noise)

**Critical Path**: The most critical path involves the fine-tuning process where the model learns to handle Indonesian speech variabilities. The quality and diversity of the IDSV dataset directly impacts the model's ability to generalize across different speaking styles and contexts.

**Design Tradeoffs**: The study chose fine-tuning over training from scratch due to limited data (80.54 hours) and the availability of strong pre-trained multilingual models. This tradeoff prioritizes leveraging existing knowledge while accepting potential limitations in capturing purely Indonesian-specific patterns.

**Failure Signatures**: MMS models showed degradation on read formal moderate data, suggesting insufficient exposure to this combination during fine-tuning. This indicates that dataset balance and coverage of all variability combinations are crucial for robust performance.

**First Experiments**:
1. Test pre-trained vs. fine-tuned models on identical test sets to quantify fine-tuning benefits
2. Evaluate model performance on each speech variability dimension separately to identify weaknesses
3. Compare different fine-tuning strategies (learning rate, epochs) to optimize performance

## Open Questions the Paper Calls Out

None

## Limitations

- The relatively modest training dataset size of 80.54 hours may limit the models' ability to fully capture Indonesian linguistic diversity
- The study only examined clean and moderate noise conditions, without testing extreme noise scenarios that could reveal model limitations
- The analysis of why MMS models degraded on certain data types remains somewhat speculative without deeper investigation into the fine-tuning methodology

## Confidence

**High**: Speaking style variability as the dominant factor influencing performance, given systematic evaluation across controlled test sets.

**Medium**: Claim about background noise having minimal impact, as only clean and moderate conditions were tested without extreme noise scenarios.

**Medium**: Specific reasons behind MMS degradation on certain data types, as the analysis remains speculative without deeper investigation.

## Next Checks

1. Test the fine-tuned models on extreme noise conditions (SNR < 5dB) to validate the claimed robustness to background noise.
2. Expand the evaluation to include regional Indonesian dialects and code-switching scenarios to assess real-world generalization.
3. Conduct ablation studies varying the proportion of each speech variability type in the training data to quantify their individual contributions to model performance.