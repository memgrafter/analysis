---
ver: rpa2
title: The effect of fine-tuning on language model toxicity
arxiv_id: '2410.15821'
source_url: https://arxiv.org/abs/2410.15821
tags:
- fine-tuning
- arxiv
- toxicity
- language
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning open language models can inadvertently reverse safety
  mitigations, increasing toxic outputs even when using non-adversarial datasets.
  This study demonstrates that parameter-efficient fine-tuning with LoRA on benign
  data can significantly raise toxicity rates across Gemma, Llama, and Phi models.
---

# The effect of fine-tuning on language model toxicity

## Quick Facts
- arXiv ID: 2410.15821
- Source URL: https://arxiv.org/abs/2410.15821
- Reference count: 22
- Models can become more toxic after fine-tuning on benign data

## Executive Summary
Fine-tuning open language models, even with benign datasets and parameter-efficient methods like LoRA, can inadvertently increase toxic outputs. This study demonstrates that LoRA fine-tuning on non-adversarial datasets significantly raises toxicity rates across multiple model families including Gemma, Llama, and Phi. Community fine-tuned variants show unpredictable toxicity shifts, highlighting risks for users who assume models inherit safety properties from their base versions. The findings emphasize the need for comprehensive safety evaluations before and after fine-tuning, and improved documentation practices for community models.

## Method Summary
The study fine-tuned instruction-tuned language models using LoRA on the Dolly dataset, then evaluated toxicity using the roberta-hate-speech-dynabench-r4 metric on RealToxicityPrompts and Compositional Evaluation Benchmark datasets. Models tested included Gemma-2-2B, Llama-2-7B, Llama-3.1-8B, Phi-3-mini, and Phi-3.5-mini with their instruction-tuned variants. Community fine-tuned models were also analyzed. Bayesian estimation compared toxicity scores between model pairs, with temperature=0 and max_tokens=50 for deterministic generation.

## Key Results
- LoRA fine-tuning on benign data significantly increased toxicity rates across Gemma, Llama, and Phi models
- Community fine-tuned variants showed unpredictable toxicity shifts without clear patterns
- Safety alignment learned during instruction-tuning can be reversed by subsequent fine-tuning on unrelated benign data

## Why This Works (Mechanism)

### Mechanism 1
Non-adversarial fine-tuning data can still degrade toxicity mitigations in pre-trained models. LoRA updates small subsets of model parameters that participate in high-level behavioral patterns including safety alignment. Changing these parameters on benign data shifts the distribution of model outputs toward the fine-tuning data's style and context, which may reduce the effect of prior safety alignment.

### Mechanism 2
Community fine-tuning often lacks transparency, leading to unpredictable toxicity outcomes. Users fine-tune models without full disclosure of datasets or methods. Even benign-sounding goals can shift toxicity rates if the fine-tuning data or method alters distributional properties of outputs. Without safety evaluations, these shifts go unnoticed until deployment.

### Mechanism 3
Fine-tuning can overwrite or dilute prior safety alignment, causing models to revert toward base-model toxicity levels. Instruction-tuning reduces toxicity by adjusting parameters toward safer outputs. Subsequent LoRA fine-tuning on unrelated benign data can overwrite these adjustments if the new data's distribution conflicts with the safety-aligned distribution, effectively "forgetting" the prior alignment.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation) and parameter-efficient fine-tuning
  - Why needed here: Understanding how LoRA updates only a small subset of parameters explains why benign data can still shift model behavior
  - Quick check question: Does LoRA update all model parameters or just a small subset?

- Concept: Bayesian estimation for comparing model outputs
  - Why needed here: Standard significance tests can mislead with continuous toxicity scores; Bayesian methods provide credible differences
  - Quick check question: Why use Bayesian estimation instead of a t-test for toxicity comparisons?

- Concept: Toxicity metrics and thresholds
  - Why needed here: Knowing how the roberta-hate-speech-dynabench-r4 model defines and thresholds toxicity is critical for interpreting results
  - Quick check question: What threshold does the toxicity metric use to classify an output as toxic?

## Architecture Onboarding

- Component map: Hugging Face Transformers -> LoRA fine-tuning pipeline -> Toxicity evaluation pipeline -> Bayesian comparison module -> Result aggregation
- Critical path: Load base and instruction-tuned models -> Apply LoRA fine-tuning on Dolly dataset -> Generate outputs for prompt sets -> Score outputs for toxicity -> Perform Bayesian comparison -> Aggregate and interpret results
- Design tradeoffs: Using LoRA vs. full fine-tuning (faster, cheaper, but may still degrade safety); Fixed temperature=0 vs. sampling (ensures deterministic outputs for comparison, but may miss stochastic toxicity patterns); Single GPU (T4) vs. distributed (limits model size but matches typical user constraints)
- Failure signatures: Toxicity rates unexpectedly rise after LoRA fine-tuning on benign data; Community fine tuned models show widely varying toxicity without clear cause; Bayesian error bars overlap zero, indicating no credible difference, but practical impact may still exist
- First 3 experiments: Fine-tune Gemma-2-2B-IT on Dolly dataset with LoRA; measure toxicity before/after; Compare Llama-3.1-8B-Instruct to its LoRA-dolly variant on the severe toxicity subset; Load a popular community multilingual fine-tuned variant; compare its toxicity to the base instruction-tuned model

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanisms cause non-adversarial fine-tuning to inadvertently reverse safety mitigations in language models? The paper demonstrates that LoRA fine-tuning on benign datasets can significantly increase toxicity, but does not investigate the underlying causes of this phenomenon. Controlled experiments comparing fine-tuning effects on different layers of models, ablation studies to identify critical parameters, or theoretical analysis of how LoRA updates interact with safety-tuned weights would resolve this.

### Open Question 2
How does the magnitude of toxicity increase scale with different fine-tuning configurations (dataset size, epochs, learning rate)? The paper uses a single fine-tuning configuration but acknowledges that different configurations could yield different results. Systematic experiments varying fine-tuning hyperparameters and dataset characteristics to establish scaling relationships would resolve this uncertainty.

### Open Question 3
Can safety mitigations be made more robust to fine-tuning by incorporating specific architectural or training techniques? The paper identifies the problem but does not explore potential solutions or mitigation strategies. Experiments testing different safety training approaches to determine which techniques preserve safety during fine-tuning would resolve this.

## Limitations

- Study relies on a single toxicity metric and threshold, which may not capture all dimensions of toxic language
- LoRA fine-tuning experiments use only the Dolly dataset, a relatively small and homogeneous instruction-following corpus
- Community-tuned model analysis depends on models available at a specific time, potentially missing variants or changes in availability

## Confidence

- High Confidence: Fine-tuning open language models on benign data can increase toxicity rates, as demonstrated across multiple model families
- Medium Confidence: The unpredictability of toxicity changes in community fine tuned models is real but based on a snapshot of available models
- Medium Confidence: Safety alignment is not permanently fixed and can be degraded by subsequent fine-tuning, though specific mechanisms require further investigation

## Next Checks

1. Replicate with Multiple Toxicity Metrics: Run the same LoRA fine-tuning experiments using alternative toxicity detection models and different threshold values to verify whether toxicity increases are consistent across evaluation methods.

2. Vary Fine-tuning Parameters Systematically: Conduct controlled experiments varying LoRA rank, learning rate, and fine-tuning dataset size to determine the minimum conditions required to observe toxicity degradation.

3. Test Continual Fine-tuning Scenarios: Evaluate whether additional safety fine-tuning or regularization techniques can prevent toxicity degradation when models are fine-tuned on benign data, testing both interleaved safety training and parameter protection approaches.