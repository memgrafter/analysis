---
ver: rpa2
title: Hiding-in-Plain-Sight (HiPS) Attack on CLIP for Targetted Object Removal from
  Images
arxiv_id: '2410.13010'
source_url: https://arxiv.org/abs/2410.13010
tags:
- attack
- image
- adversarial
- attacks
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Hiding-in-Plain-Sight (HiPS) attacks, a\
  \ novel class of adversarial attacks that subtly modify model predictions by selectively\
  \ concealing target objects in images. Two HiPS variants\u2014HiPS-cls and HiPS-cap\u2014\
  are proposed, leveraging class labels and adversarial captions respectively to craft\
  \ attacks."
---

# Hiding-in-Plain-Sight (HiPS) Attack on CLIP for Targetted Object Removal from Images

## Quick Facts
- arXiv ID: 2410.13010
- Source URL: https://arxiv.org/abs/2410.13010
- Reference count: 40
- This paper introduces HiPS attacks that can remove target objects from image captions while preserving other content

## Executive Summary
This paper introduces Hiding-in-Plain-Sight (HiPS) attacks, a novel class of adversarial attacks that subtly modify model predictions by selectively concealing target objects in images. Two HiPS variants—HiPS-cls and HiPS-cap—are proposed, leveraging class labels and adversarial captions respectively to craft attacks. The method demonstrates high success in removing target objects from image captions while preserving other content, with HiPS-cls achieving 94% Attack Success Rate (ASR) and 100% Remaining Objects Retention Rate (RORR) using PGD with L∞ norm. HiPS-cap shows stronger semantic similarity to target captions (CSS ~0.75) but slightly lower TORR/RORR. The attacks effectively transfer to downstream captioning models like CLIP-Cap, highlighting new vulnerabilities in multi-modal systems.

## Method Summary
The HiPS attack framework consists of two variants: HiPS-cls uses class labels as targets while HiPS-cap uses adversarial captions. Both attacks leverage CLIP's contrastive learning alignment to manipulate cosine similarity in the shared embedding space. The method crafts adversarial images that hide specific objects by maximizing similarity to captions without the target object while minimizing similarity to the original caption. The attacks are evaluated on 50 manually sampled MS COCO images using metrics including Target Object Removal Rate (TORR), Remaining Objects Retention Rate (RORR), Attack Success Rate (ASR), and Caption Semantic Similarity (CSS).

## Key Results
- HiPS-cls achieved 94% ASR with 100% RORR using PGD(L∞) on MS COCO dataset
- HiPS-cap showed higher semantic similarity to target captions (CSS ~0.75) but slightly lower TORR/RORR
- Attacks successfully transferred to downstream captioning models like CLIP-Cap
- Image quality remained high (MSE, MAE, PSNR, SSIM) indicating subtle perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's contrastive learning alignment allows cosine similarity manipulation to selectively hide objects.
- Mechanism: CLIP embeds images and text in a shared space; by maximizing text similarity to captions without the target object while minimizing similarity to the original caption, the adversarial image is "pushed" away from representing the target object in CLIP's embedding space.
- Core assumption: CLIP's embedding space is continuous and smooth enough that small perturbations can shift object representations without drastically altering overall image semantics.
- Evidence anchors:
  - [abstract] "adversarial images designed to hide a particular object should cause an image captioning model to generate a caption as if the target object(s) was never present"
  - [section] "We calculate the cosine similarities between the image I and both the original caption C and the target caption ˜C as follows: SC = cos(fImage(I), fText(C)); S ˜C = cos(fImage(I), fText(˜C))"
  - [corpus] Weak evidence; no corpus neighbors directly address CLIP contrastive embedding manipulation.
- Break condition: If CLIP's embedding space has discontinuities or the target object is strongly semantically tied to other objects, perturbations may not isolate the target object's removal.

### Mechanism 2
- Claim: The adversarial loss structure (λ1S_j - λ2∑S_i≠j) directly steers the image embedding away from the target class representation.
- Mechanism: By penalizing the similarity to the target object's text embedding (negative λ1S_j) and rewarding similarity to other objects' text embeddings (positive λ2∑S_i≠j), the optimization process pushes the image embedding to align with a "target-object-absent" distribution.
- Core assumption: The model's text encoder can distinguish between images with and without the target object purely based on class labels, even if the visual features overlap.
- Evidence anchors:
  - [section] "Our goal is to perturb the image I in such a way that the cosine similarity score for the target object, Sj, is reduced (as if it is absent), while the scores for all other objects Ti (for all i≠j) are either increased or remain unchanged"
  - [section] "We define the HiPS-cls adversarial loss function as follows: LHiPS-cls = −λ1Sj + λ2∑i≠j Si"
  - [corpus] No direct evidence; corpus neighbors focus on adversarial patches and transferability, not contrastive class-label-based attacks.
- Break condition: If the target object's visual features are inseparable from other objects, reducing its similarity may also reduce similarity to related objects.

### Mechanism 3
- Claim: Downstream models inherit CLIP's vulnerability because they freeze the CLIP vision encoder during training.
- Mechanism: Models like CLIP-Cap use the frozen CLIP vision encoder, so adversarial perturbations crafted on CLIP transfer directly to these models without needing further adaptation.
- Core assumption: The downstream model's task-specific head does not significantly alter the embedding space in a way that would negate the adversarial perturbation.
- Evidence anchors:
  - [abstract] "We demonstrate that our HiPS attacks can effectively transfer to downstream image captioning models, such as CLIP-Cap"
  - [section] "Many of these models [14] integrate a pre-trained LLM with a large vision encoder like CLIP. For LLaVA, the vision encoder remains frozen during training"
  - [corpus] Weak evidence; corpus neighbors discuss transferability but not specifically frozen-encoder inheritance.
- Break condition: If the downstream model fine-tunes or modifies the vision encoder, the adversarial perturbation may not transfer effectively.

## Foundational Learning

- Concept: Contrastive learning and shared embedding spaces in multimodal models.
  - Why needed here: HiPS attacks exploit the geometry of CLIP's embedding space; understanding how images and text are aligned is essential to crafting effective attacks.
  - Quick check question: How does CLIP's contrastive loss encourage alignment between image and text embeddings?

- Concept: Adversarial attack optimization (FGSM, PGD) and Lp norms.
  - Why needed here: HiPS uses iterative attacks (PGD) under Lp constraints to generate subtle perturbations; knowing how these attacks work and their budget limits is critical for tuning.
  - Quick check question: What is the difference between FGSM and PGD in terms of perturbation granularity and success rates?

- Concept: Evaluation metrics for multimodal adversarial attacks (TORR, RORR, ASR, CSS).
  - Why needed here: Standard image classification metrics don't capture the subtlety of object removal; understanding these custom metrics is necessary to assess attack success properly.
  - Quick check question: How does TORR differ from standard accuracy in evaluating targeted object removal?

## Architecture Onboarding

- Component map: Input image → CLIP vision encoder → Image embedding → Cosine similarity calculation → Adversarial loss → PGD optimization → Perturbed image → Downstream model (CLIP-Cap) → Caption output
- Critical path:
  1. Compute CLIP image and text embeddings
  2. Calculate cosine similarities for target and non-target objects
  3. Compute adversarial loss and backpropagate to image
  4. Project perturbation onto Lp ball
  5. Feed adversarial image to downstream model
- Design tradeoffs:
  - L∞ norm: Smaller, more uniform perturbations but may require larger budgets for success
  - L1 norm: Sparse, localized changes but less stable across images
  - L2 norm: Smooth, distributed changes but may be more perceptible
  - λ1/λ2 balance: Higher λ1 improves target removal but may harm caption quality (CSS)
- Failure signatures:
  - ASR low but TORR high: Attack removes target but breaks other objects
  - ASR low but RORR high: Attack preserves other objects but fails to remove target
  - ASR low and CSS low: Attack is ineffective and produces poor captions
  - MSE/SSIM high: Perturbations are perceptible to humans
- First 3 experiments:
  1. Run HiPS-cls with PGD(L∞) on a simple two-object image; verify TORR ≈ 90% and RORR ≈ 100%.
  2. Compare HiPS-cls vs HiPS-cap on the same image; measure CSS difference and analyze caption quality.
  3. Vary λ1 from 0.1 to 2.0; plot ASR, CSS, and image quality to find optimal tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do HiPS attacks perform on multi-word objects (e.g., "teddy bear") compared to single-word objects?
- Basis in paper: [explicit] The paper mentions that the ASR metric struggles with multi-word objects and uses an averaging approach for embeddings, but notes this is a workaround.
- Why unresolved: The paper doesn't provide experimental results comparing HiPS performance on single vs. multi-word objects, and acknowledges this as a limitation of their evaluation method.
- What evidence would resolve it: Experiments measuring HiPS attack success rates (TORR, RORR, ASR) specifically for images containing multi-word objects versus single-word objects.

### Open Question 2
- Question: How transferable are HiPS attacks across different multimodal models beyond CLIP-Cap?
- Basis in paper: [explicit] The paper states "while in this work we focused on a single image captioning model, we believe that this attack can be used for other multimodal models (LLaVa, OpenFlamingo) as well as other downstream tasks (object detection, action recognition)."
- Why unresolved: The paper only tested transferability to one downstream model (CLIP-Cap) and acknowledges this as a future direction.
- What evidence would resolve it: Testing HiPS attacks generated with CLIP on a range of other multimodal models and downstream tasks, measuring attack success rates across these different systems.

### Open Question 3
- Question: What is the optimal balance between attack success and output quality when using different loss weightings (λ1) in HiPS attacks?
- Basis in paper: [explicit] The paper shows that different λ1 values affect ASR and CSS metrics differently, with HiPS-cls maintaining high performance across a wider range of λ1 values compared to HiPS-cap.
- Why unresolved: While the paper provides sensitivity analysis, it doesn't identify optimal λ1 values that balance attack success with maintaining high-quality, semantically similar outputs.
- What evidence would resolve it: Systematic evaluation of HiPS attacks across various λ1 values measuring not just ASR but also output quality metrics like grammatical correctness and semantic fidelity to identify optimal trade-offs.

## Limitations

- Small-scale evaluation on only 50 manually selected MS COCO images limits generalizability
- Limited testing of attack transferability to only one downstream model (CLIP-Cap)
- No investigation of potential defenses or robustness against real-world perturbations

## Confidence

- **High Confidence**: Technical implementation of HiPS attacks and effectiveness on tested dataset
- **Medium Confidence**: Generalizability to other datasets and downstream models
- **Low Confidence**: Practical impact in real-world scenarios without considering defenses

## Next Checks

1. **Scale Up Evaluation**: Test HiPS attacks on a larger and more diverse dataset (e.g., Flickr30k or Open Images) to assess generalizability and robustness across different image types and domains.

2. **Defense Mechanisms**: Investigate potential defenses against HiPS attacks, such as adversarial training, input preprocessing, or robust embedding spaces, to understand the practical implications and limitations of the attacks.

3. **Human Evaluation**: Conduct a human study to evaluate the semantic similarity of the generated captions and the perceptual quality of the adversarial images, complementing the automated metrics (CSS, MSE, PSNR, SSIM) with subjective assessments.