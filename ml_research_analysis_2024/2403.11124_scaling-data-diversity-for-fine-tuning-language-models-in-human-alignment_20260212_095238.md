---
ver: rpa2
title: Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment
arxiv_id: '2403.11124'
source_url: https://arxiv.org/abs/2403.11124
tags:
- arxiv
- prompts
- diversity
- responses
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the allocation of human annotation resources
  for fine-tuning large language models (LLMs) to align with human preferences. It
  quantitatively compares the impact of using diverse prompts versus diverse responses
  for LLM fine-tuning, finding that increasing the number of responses for each prompt
  yields greater improvements in human alignment than expanding the prompt set, given
  a fixed annotation budget.
---

# Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment

## Quick Facts
- arXiv ID: 2403.11124
- Source URL: https://arxiv.org/abs/2403.11124
- Reference count: 0
- Primary result: Increasing responses per prompt yields greater improvements in human alignment than expanding the prompt set, given a fixed annotation budget.

## Executive Summary
This work investigates how to allocate human annotation resources when fine-tuning large language models for human alignment. The authors compare the impact of diverse prompts versus diverse responses, finding that adding more response options per prompt improves alignment more effectively than adding more prompts when annotation budgets are fixed. They introduce an empirical metric to measure prompt diversity based on unique N-grams and demonstrate a linear correlation between this diversity and LLM performance. Additionally, they propose a data augmentation technique guided by this diversity metric that slightly enhances model performance.

## Method Summary
The study uses the HH-RLHF dataset and extends 2-ranking samples to 4-ranking through zero-shot augmentation. They construct subsets with controlled numbers of prompts and responses per prompt, ensuring equal total annotations across different configurations. The authors fine-tune OPT-1.3B and LLaMA-7B models using both Supervised Fine-tuning (SFT) and Preference Ranking Optimization (PRO). Performance is evaluated using reward scores from RMtrain/RMtest and GPT-4-as-a-judge comparisons. A novel diversity metric based on unique N-gram ratios is introduced and validated against performance outcomes.

## Key Results
- More responses per prompt yield better human alignment than more prompts given a fixed annotation budget
- Prompt diversity measured by unique N-gram ratios correlates linearly with LLM performance after fine-tuning
- Data augmentation guided by diversity-based filtering slightly improves prompt set diversity and LLM performance
- Longer response rankings (3-4) lead to better enhancement compared to expanding prompts, regardless of base models or algorithms used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: More responses per prompt yields better human alignment than more prompts, given a fixed annotation budget.
- Mechanism: Each prompt activates LLMs for alignment, but additional responses provide clearer pairwise preference signals that fine-tuning algorithms can leverage to optimize reward modeling.
- Core assumption: LLMs can learn alignment from a small number of prompts but need multiple response options to capture nuanced human preferences.
- Evidence anchors:
  - [abstract] "we find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment."
  - [section 3.6] "longer response rankings leads to better enhancement compared to expanding prompts, regardless of the backbones used or the values of N."
- Break condition: If the base model lacks the capacity to distinguish between response options, or if human preferences are not reliably captured by pairwise rankings.

### Mechanism 2
- Claim: Prompt diversity can be empirically quantified using unique N-gram ratios and correlates linearly with LLM performance after fine-tuning.
- Mechanism: By calculating the ratio of unique N-grams in the prompt set and combining it with a decay factor for diminishing returns, the diversity metric captures the richness of linguistic patterns and contexts in prompts, which scales linearly with reward scores.
- Core assumption: Token-level N-grams capture syntactic and contextual variation better than raw token counts, and diversity grows sublinearly with prompt quantity.
- Evidence anchors:
  - [section 4.1] "we introduce a novel formulation to empirically define prompt diversity based on N-grams."
  - [section 4.2] "we uncover a linear relationship between this diversity and the acquired reward scores by examining various scales of training sets, different base models, and algorithms."
- Break condition: If the decay index p is poorly chosen or if N-gram overlap does not reflect true semantic diversity.

### Mechanism 3
- Claim: Guided data augmentation using diversity-based filtering improves prompt set diversity and LLM performance.
- Mechanism: New prompts are sampled and retained only if they have low Jaccard overlap with a support set, thereby increasing the unique N-gram ratio and the overall diversity metric, which in turn improves reward scores after fine-tuning.
- Core assumption: Local diversity checks can approximate global diversity gains, and the resulting prompts are still valid for the target task.
- Evidence anchors:
  - [section 5.3] "we design the data augmentation as where there are existing samples that constitute a seed set... we introduce a locally greedy search process to filter new samples based on the supporting samples."
  - [section 5.3] "Implementing this method leads to an improvement in performance compared to randomly sampled data."
- Break condition: If the support set is too small or too homogeneous, the filtering may yield low-diversity prompts.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF) pipeline (SFT → RM → RL)
  - Why needed here: The work builds on RLHF as the baseline for human alignment, so understanding how SFT and preference ranking optimization differ is essential for interpreting results.
  - Quick check question: In the RLHF pipeline, what is the role of the reward model relative to the base model during fine-tuning?

- Concept: Tokenization and N-gram overlap
  - Why needed here: The diversity metric relies on counting unique N-grams in tokenized prompts; knowing how tokenization affects N-gram extraction is critical.
  - Quick check question: If a prompt is tokenized into [I, love, machine, learning], what are its 2-grams and 3-grams?

- Concept: Jaccard similarity for set overlap
  - Why needed here: The data augmentation step uses Jaccard index to measure overlap between supporting and candidate prompts; understanding its properties ensures correct implementation.
  - Quick check question: Given two sets X={a,b,c} and Y={b,c,d}, what is their Jaccard index?

## Architecture Onboarding

- Component map: Dataset builder -> Diversity calculator -> Fine-tuning runner -> Evaluator -> Augmenter
- Critical path: 1. Load HH-RLHF → split into N prompt sets 2. Compute runique and d for each set 3. Fine-tune LLM → validate every 500 steps 4. Evaluate with RMtest → cross-validate with GPT-4 5. If augmenting, filter new samples via Jaccard → fine-tune again
- Design tradeoffs:
  - Larger N improves diversity but risks redundancy; decay index p mitigates but must be tuned
  - Jaccard-based filtering is fast but only approximates global diversity; full pairwise computation is accurate but expensive
  - GPT-4 evaluation is human-like but costly; RM-based is cheaper but less nuanced
- Failure signatures:
  - Reward scores plateau despite more prompts → likely redundancy or poor p choice
  - GPT-4 win rates inconsistent with RM scores → evaluator disagreement or insufficient prompts
  - Augmenter produces near-duplicate prompts → support set too small or filtering threshold too loose
- First 3 experiments:
  1. Run quantitative comparison on OPT-1.3B with N=3000 and N=24000, record reward scores for 2/3/4 rankings
  2. Compute d for each subset, plot against reward scores, fit linear model, check MSE
  3. Augment D6000 subset with 6000 filtered prompts, compare d and reward scores to D12000 baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal decay index for different datasets and base models when measuring prompt diversity?
- Basis in paper: [explicit] The paper empirically sets the decay index to 0.5 for HH-RLHF but notes it could be found using grid search.
- Why unresolved: The decay index was chosen empirically for one specific dataset without exploring its sensitivity to different datasets or base models.
- What evidence would resolve it: A systematic study varying the decay index across multiple datasets and base models, showing its impact on the correlation between diversity and performance.

### Open Question 2
- Question: How does the proposed data augmentation method scale with larger values of n (number of supporting samples)?
- Basis in paper: [explicit] The paper uses n=2 for data augmentation experiments and mentions that larger n might amplify the effect, leaving it for future research.
- Why unresolved: The paper only tested the data augmentation with a small value of n, leaving its scalability unexplored.
- What evidence would resolve it: Experiments varying n across a wide range of values, demonstrating how the diversity metric and performance change with different n.

### Open Question 3
- Question: What is the theoretical foundation for the linear correlation between prompt diversity and LLM performance?
- Basis in paper: [explicit] The paper observes a linear correlation between prompt diversity and performance but does not provide a theoretical explanation.
- Why unresolved: The paper presents empirical evidence of the correlation but does not explore the underlying reasons for this relationship.
- What evidence would resolve it: A theoretical analysis or mathematical proof explaining why prompt diversity should linearly correlate with LLM performance in human alignment tasks.

## Limitations
- Dataset extension mechanism: The zero-shot augmentation process for converting 2-ranking to 4-ranking samples in HH-RLHF is not fully specified, which could affect reproducibility of the diversity comparisons
- Generalizability of diversity metric: While the N-gram based diversity metric shows promise, its applicability to other domains or languages remains untested
- Scalability of augmentation: The Jaccard-based filtering for data augmentation is computationally feasible for moderate datasets but may become a bottleneck for very large-scale applications

## Confidence
- High Confidence: The finding that more responses per prompt improves human alignment more than more prompts, given fixed annotation budgets
- Medium Confidence: The linear relationship between the proposed diversity metric and reward scores
- Low Confidence: The effectiveness of guided data augmentation for diversity improvement

## Next Checks
1. Apply the diversity metric and augmentation method to a different human preference dataset (e.g., Anthropic's HH-RLHF or Stanford's Human Preferences) to test generalizability
2. Systematically vary N (1, 2, 3, 4-grams) and decay index p to identify optimal configurations for different prompt types and languages
3. Analyze whether the diversity metric and augmentation method effectively handle rare or edge-case prompts that may be underrepresented in the original dataset