---
ver: rpa2
title: Benchmarking Generative Models on Computational Thinking Tests in Elementary
  Visual Programming
arxiv_id: '2406.09891'
source_url: https://arxiv.org/abs/2406.09891
tags:
- avatar
- code
- task
- tasks
- grid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a benchmark for evaluating generative models
  on computational thinking and problem-solving tasks in elementary visual programming.
  It finds that state-of-the-art models like GPT-4o and Llama3 perform only slightly
  better than average school students on these tasks.
---

# Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming

## Quick Facts
- **arXiv ID**: 2406.09891
- **Source URL**: https://arxiv.org/abs/2406.09891
- **Reference count**: 40
- **Primary result**: Fine-tuning Llama3-8B on synthetic data with symbolic explanations produces models achieving accuracy comparable to GPT-4o on computational thinking tests.

## Executive Summary
This paper introduces a benchmark for evaluating generative models on computational thinking and problem-solving tasks in elementary visual programming. The authors find that state-of-the-art models like GPT-4o and Llama3 perform only slightly better than average school students on these tasks. To address this gap, they develop a novel synthetic data generation methodology based on symbolic methods, producing a dataset covering various skill levels from basic visual element recognition to complex code synthesis. Fine-tuning Llama3-8B with this data produces models (LLAMA CT family) that achieve accuracy comparable to GPT-4o. Analysis shows that including symbolic explanations during fine-tuning and training on diverse task types significantly boosts performance.

## Method Summary
The authors create a benchmark using three existing computational thinking tests (Hour of Code, ACE, CT-TEST) and generate a synthetic dataset of 111,861 examples using symbolic methods. They fine-tune Llama3-8B using LoRA adapters on this synthetic data, training for 2 epochs with r=16 and alpha=32. The synthetic dataset covers solution synthesis, multi-choice questions, and fine-grained skills. Models are evaluated on both the benchmark tests and a synthetic test set, with performance compared to GPT-4o baselines and student data.

## Key Results
- State-of-the-art models like GPT-4o and Llama3 perform only slightly better than average school students on computational thinking tests
- Fine-tuning Llama3-8B with synthetic data produces models (LLAMA CT family) achieving accuracy comparable to GPT-4o
- Including symbolic explanations during fine-tuning and training on diverse task types significantly boosts performance
- Models still lag behind even grade 3 students on Hour of Code tasks, but approach average grade 7 performance on ACE tests

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning on synthetic data with symbolic explanations improves reasoning on computational thinking tasks.
- **Mechanism**: The model learns to produce correct intermediate traces and sequences of actions as explanations, which guide the generation of minimal solution code.
- **Core assumption**: Symbolic execution methods can reliably generate correct traces and explanations for synthetic tasks.
- **Evidence anchors**: [abstract], [section 4.1]
- **Break condition**: If symbolic execution produces incorrect explanations, or if the model cannot generalize from these explanations to unseen task structures.

### Mechanism 2
- **Claim**: Fine-tuning on fine-grained skills tasks improves performance on higher-level tasks.
- **Mechanism**: By training on basic skills, the model builds a robust understanding of grid and code interactions that transfers to more complex reasoning tasks.
- **Core assumption**: Skills learned at the fine-grained level will transfer to higher-level problem-solving through generalization.
- **Evidence anchors**: [abstract], [section 4.3]
- **Break condition**: If the model memorizes fine-grained patterns without understanding underlying concepts.

### Mechanism 3
- **Claim**: Providing multimodal input (text + visual) improves performance over single modality.
- **Mechanism**: Visual representations provide spatial context that text alone may not fully capture, especially for grid-based reasoning.
- **Core assumption**: Vision models can extract meaningful spatial relationships from grid images that improve reasoning.
- **Evidence anchors**: [section 5.2]
- **Break condition**: If the vision model misinterprets grid layouts or visual details.

## Foundational Learning

- **Concept**: Visual programming domain (grids, avatars, goals, walls, markers)
  - Why needed here: The entire benchmark is based on visual programming tasks where understanding grid layouts and avatar movements is essential.
  - Quick check question: Can you describe how an avatar moves from one cell to another in a grid-based visual programming environment?

- **Concept**: Domain-specific language (DSL) for code representation
  - Why needed here: The models must understand and generate code using a specific set of primitives (move, turnLeft, turnRight, etc.) and control structures (Repeat, RepeatUntil, If, IfElse).
  - Quick check question: What are the basic primitives available in the Hour of Code visual programming DSL?

- **Concept**: Computational thinking concepts (decomposition, pattern recognition, abstraction, algorithm design)
  - Why needed here: The benchmark assesses these higher-order thinking skills through tasks that require analyzing, evaluating, and creating code solutions.
  - Quick check question: How would you break down the problem of navigating an avatar to a goal into smaller, manageable steps?

## Architecture Onboarding

- **Component map**: Data generation pipeline -> Synthetic dataset (solution synthesis, MCQ, fine-grained skills) -> Model fine-tuning (LoRA adapters on Llama3-8B) -> Evaluation framework (real-world tests + synthetic evaluation segment)
- **Critical path**: Data generation → Synthetic dataset creation → Fine-tuning → Evaluation on benchmark
- **Design tradeoffs**: 
  - Using synthetic data allows for large-scale training but may not capture all real-world edge cases
  - Symbolic explanations provide strong training signals but require reliable execution methods
  - Multimodal input improves performance but increases computational cost and complexity
- **Failure signatures**:
  - Poor performance on solution synthesis tasks despite good MCQ results suggests inadequate understanding of code-to-trace mapping
  - Inability to generalize from fine-grained to higher-level tasks indicates memorization rather than understanding
  - Inconsistent results across seeds may indicate sensitivity to initialization or data ordering
- **First 3 experiments**:
  1. Generate a small synthetic dataset and fine-tune Llama3-8B to verify the basic pipeline works
  2. Compare performance of models trained with vs. without explanations on a subset of tasks
  3. Test the impact of including fine-grained skills data by training separate models with different data combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-modal models perform on computational thinking tests when fine-tuned specifically for these tasks?
- Basis in paper: [inferred] The paper mentions that multi-modal models are assessed but not fine-tuned to improve performance.
- Why unresolved: The current study only evaluates off-the-shelf multi-modal models without exploring their potential improvement through fine-tuning.
- What evidence would resolve it: Fine-tuning experiments with multi-modal models on the synthetic dataset and comparing their performance to unimodal models and baseline models.

### Open Question 2
- Question: Can generative models effectively interact with symbolic tools to obtain reasoning information at inference time rather than relying on pre-provided explanations?
- Basis in paper: [explicit] The paper suggests this as a future direction, noting that one technique "naively uses correct explanations provided at inference time" and proposing that "generative models interact with symbolic tools to obtain this kind of information at inference time."
- Why unresolved: The current approach for LLAMA CT:H OC+MCQ+A UGexp* simulates an ideal scenario where correct explanations are available at inference.
- What evidence would resolve it: Development and evaluation of a system where models can query symbolic tools or emulators during inference to verify their reasoning steps and obtain feedback.

### Open Question 3
- Question: What is the minimum amount of fine-grained skills data needed to achieve comparable performance to the full synthetic dataset?
- Basis in paper: [inferred] The paper mentions that including fine-grained skills data significantly improves performance but doesn't explore the optimal amount or combination of these skills.
- Why unresolved: The study uses the full fine-grained skills dataset but doesn't investigate whether smaller, more targeted subsets could achieve similar results with less computational cost.
- What evidence would resolve it: Ablation studies systematically removing different categories of fine-grained skills to determine which contribute most to performance gains.

## Limitations
- The synthetic data generation methodology's generalizability to other visual programming environments remains unproven
- No comparison with alternative fine-tuning approaches or synthetic data generation methods
- Limited evidence about transfer to novel task structures beyond the specific Hour of Code domain

## Confidence

**High Confidence**: The core finding that fine-tuning on synthetic data with symbolic explanations improves performance on computational thinking tasks. Well-supported by experimental results showing LLAMA CT models achieving accuracy comparable to GPT-4o.

**Medium Confidence**: The claim that including fine-grained skills during training improves higher-level task performance. While the paper shows correlation between diverse training data and improved results, the mechanism for transfer from fine-grained to complex tasks could be more rigorously established.

**Low Confidence**: The assertion that the symbolic data generation methodology can be generalized to other visual programming domains. The paper provides limited evidence beyond the specific Hour of Code environment.

## Next Checks

1. **Transfer Learning Validation**: Test the fine-tuned models on a completely different visual programming environment (e.g., Scratch or Code.org's broader curriculum) to assess generalization beyond the Hour of Code domain.

2. **Alternative Data Generation Comparison**: Implement and evaluate at least one alternative synthetic data generation approach to establish whether the symbolic methods provide unique advantages over other strategies.

3. **Longitudinal Student Performance Tracking**: Collect and analyze student performance data over time on the same benchmark tasks to better understand the gap between model and human performance, and identify specific areas where models struggle relative to learners.