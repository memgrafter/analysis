---
ver: rpa2
title: Semi-Periodic Activation for Time Series Classification
arxiv_id: '2412.09889'
source_url: https://arxiv.org/abs/2412.09889
tags:
- activation
- series
- leakysinelu
- time
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the gap in activation functions for time series
  neural networks by proposing a new semi-periodic activation called LeakySineLU.
  The method combines periodic and monotonic properties to handle both negative and
  positive time series values while maintaining non-linearity.
---

# Semi-Periodic Activation for Time Series Classification

## Quick Facts
- arXiv ID: 2412.09889
- Source URL: https://arxiv.org/abs/2412.09889
- Reference count: 40
- Primary result: LeakySineLU achieves best average ranking across 112 time series classification datasets, outperforming ReLU, PReLU, and Snake activations

## Executive Summary
This paper addresses a critical gap in time series neural network design by proposing LeakySineLU, a novel semi-periodic activation function that combines periodic and monotonic properties. The activation effectively handles both negative and positive time series values while maintaining non-linearity, solving problems found in traditional activations like ReLU (which loses information for negative values) and pure periodic functions (which can get stuck in local minima). Experimental evaluation demonstrates that LeakySineLU outperforms commonly used activations across diverse time series classification tasks.

## Method Summary
The study proposes LeakySineLU, a semi-periodic activation function designed specifically for time series neural networks. The method combines a positive-side linear term with a squared sine component, ensuring non-zero gradients for negative inputs while maintaining periodic derivatives that capture oscillatory patterns. The activation is evaluated using two architectures - Multi-Layer Perceptron (MLP) and Fully Convolutional Network (FCN) - across 112 benchmark datasets from the UCR repository. The MLP uses Adadelta optimizer with 1000 epochs, while the FCN employs Adam optimizer with 2000 epochs, both using cross-entropy loss and statistical significance testing.

## Key Results
- LeakySineLU achieves the best average ranking in all comparative scenarios across 112 benchmark datasets
- Outperforms traditional activations including ReLU, PReLU, and Snake on time series classification tasks
- Demonstrates superior information preservation for negative-valued time series while maintaining periodic pattern capture
- Shows consistent performance across both MLP and FCN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LeakySineLU solves the "dying ReLU" problem while preserving periodic signal structure in time series data.
- Mechanism: The activation combines a positive-side linear term with a squared sine component, ensuring non-zero gradients for negative inputs while maintaining a periodic derivative that captures oscillatory patterns.
- Core assumption: Time series data contains both monotonic trends and periodic components that require different handling in activation functions.
- Evidence anchors: [abstract] "proposed activation addresses issues found in existing functions like ReLU, which loses information for negative values, and periodic functions like sine, which can get stuck in local minima"
- Break condition: If time series data is purely monotonic with no periodic components, the periodic derivative may introduce unnecessary complexity without benefit.

### Mechanism 2
- Claim: The semi-periodic nature of LeakySineLU enables better gradient flow during training of time series models.
- Mechanism: By having a periodic derivative rather than a periodic function itself, LeakySineLU avoids the local minima problem of pure periodic activations while maintaining the ability to capture frequency information.
- Core assumption: Gradient-based optimization benefits from periodic derivatives that can navigate oscillatory loss landscapes common in time series problems.
- Evidence anchors: [abstract] "LeakySineLU achieves the best average ranking in all comparative scenarios"
- Break condition: If the optimization landscape is not oscillatory or if the learning rate is too high, the periodic derivative may cause training instability.

### Mechanism 3
- Claim: LeakySineLU's bounded-unbounded hybrid structure preserves information across the full range of time series values.
- Mechanism: The activation function has no upper or lower bounds while maintaining non-linearity through the sin²(x) component, preventing information loss that occurs with bounded functions like sigmoid or tanh.
- Core assumption: Time series data often contains extreme values that would be clipped by bounded activation functions, losing important signal information.
- Evidence anchors: [section] "LeakySineLU have no boundaries" with mathematical proof showing limits approach ±∞
- Break condition: If the network architecture is shallow or the dataset is small, unbounded activations may lead to overfitting or numerical instability.

## Foundational Learning

- Concept: Universal approximation theorem for neural networks
  - Why needed here: Understanding why unbounded activations are critical for time series where information preservation across the full value range is essential
  - Quick check question: Why do bounded activations like sigmoid and tanh potentially lose information in time series applications?

- Concept: Sub-differential calculus
  - Why needed here: LeakySineLU has a discontinuous derivative at x=0, requiring sub-gradient methods for proper optimization
  - Quick check question: How does the sub-differential at x=0 differ from traditional derivatives, and why is this important for LeakySineLU?

- Concept: Fourier series and periodic signal representation
  - Why needed here: The periodic derivative of LeakySineLU connects to Fourier analysis, enabling the capture of oscillatory patterns in time series data
  - Quick check question: What is the relationship between periodic derivatives and the ability to model frequency components in time series?

## Architecture Onboarding

- Component map: Input layer → LeakySineLU activation → hidden layers → output layer (softmax for multi-class, linear for binary). For MLP: 2 hidden layers with 500 neurons each. For FCN: 3 convolutional blocks with 128, 256, 128 channels and kernel sizes 8, 5, 3.

- Critical path: Data preprocessing → model forward pass with LeakySineLU → loss computation (cross-entropy or binary cross-entropy) → backpropagation using sub-gradients at x=0 → parameter updates with Adadelta/Adam → evaluation on test set.

- Design tradeoffs: LeakySineLU vs ReLU: Better information preservation vs slightly more complex computation. LeakySineLU vs pure periodic activations: Avoids local minima vs may not capture periodic patterns as strongly. LeakySineLU vs bounded activations: No information clipping vs potential for numerical instability.

- Failure signatures: Vanishing gradients when periodic component dominates, exploding gradients from unbounded nature, training instability at x=0 discontinuity, poor performance on purely monotonic time series.

- First 3 experiments:
  1. Compare LeakySineLU vs ReLU on a simple sine wave classification task to verify periodic pattern capture
  2. Test information preservation by measuring gradient flow on negative-valued time series inputs
  3. Benchmark training stability across different learning rates on datasets with varying periodicity strength

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LeakySineLU compare to other activation functions when applied to more complex time series architectures like InceptionTime and ResNet?
- Basis in paper: [explicit] The authors suggest future work should explore improving well-defined architectures like InceptionTime and ResNet using the proposed activation.
- Why unresolved: The experiments in the paper only tested LeakySineLU on MLP and FCN architectures, leaving its effectiveness on more advanced architectures unexplored.
- What evidence would resolve it: Comparative experiments on architectures like InceptionTime and ResNet using LeakySineLU versus other activation functions, measuring accuracy and other performance metrics.

### Open Question 2
- Question: What is the impact of LeakySineLU on time series tasks beyond classification, such as regression and forecasting?
- Basis in paper: [explicit] The authors mention exploring the impact of activation choices in other time series-related tasks like regression and forecasting as future work.
- Why unresolved: The study focuses solely on time series classification tasks, and its applicability to regression and forecasting remains untested.
- What evidence would resolve it: Experimental results comparing LeakySineLU with other activation functions on regression and forecasting tasks, using relevant datasets and evaluation metrics.

### Open Question 3
- Question: How does LeakySineLU perform on time series datasets with varying lengths, including those not covered in the UCR repository?
- Basis in paper: [inferred] The experiments were conducted on 112 equal-length datasets from the UCR repository, but the paper does not address performance on datasets with varying lengths.
- Why unresolved: The study does not explore the robustness of LeakySineLU on datasets with different time series lengths, which is a common scenario in real-world applications.
- What evidence would resolve it: Comparative experiments on time series datasets with varying lengths, evaluating the performance of LeakySineLU against other activation functions using appropriate metrics.

## Limitations

- Performance on purely monotonic or highly irregular time series remains untested
- Numerical stability at the x=0 discontinuity for the sub-differential handling is not explored
- Effectiveness on time series architectures beyond MLP and FCN is unverified

## Confidence

- High confidence in the mathematical formulation and basic properties of LeakySineLU
- Medium confidence in the empirical superiority across 112 datasets
- Low confidence in the activation's robustness to hyperparameter variations and edge cases

## Next Checks

1. Test LeakySineLU on synthetic time series with controlled periodic vs monotonic ratios to isolate its strengths and weaknesses
2. Evaluate numerical stability across different learning rates and batch sizes, particularly at the x=0 discontinuity
3. Compare performance on time series with extreme outliers to verify information preservation claims against bounded activations