---
ver: rpa2
title: Certified $\ell_2$ Attribution Robustness via Uniformly Smoothed Attributions
arxiv_id: '2405.06361'
source_url: https://arxiv.org/abs/2405.06361
tags:
- attribution
- attributions
- smoothed
- robustness
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a certified defense method for model attribution\
  \ robustness against \u21132 attacks using uniform smoothing. The method augments\
  \ attributions with noise sampled from a uniform distribution over an \u21132 ball\
  \ and provides theoretical guarantees that the cosine similarity between smoothed\
  \ attributions of original and perturbed samples is lower-bounded."
---

# Certified $\ell_2$ Attribution Robustness via Uniformly Smoothed Attributions

## Quick Facts
- arXiv ID: 2405.06361
- Source URL: https://arxiv.org/abs/2405.06361
- Reference count: 40
- This work proposes a certified defense method for model attribution robustness against ℓ2 attacks using uniform smoothing.

## Executive Summary
This paper introduces a novel approach to certifying attribution robustness against ℓ2 perturbations using uniform smoothing. The method provides theoretical guarantees that the cosine similarity between smoothed attributions of original and perturbed samples is lower-bounded. By deriving alternative formulations, the authors determine the maximum allowable perturbation size or minimum smoothing radius for a given similarity threshold. Experiments on MNIST, CIFAR-10, and ImageNet demonstrate effective protection across different network architectures and training schemes.

## Method Summary
The proposed method augments attributions with noise sampled from a uniform distribution over an ℓ2 ball and provides theoretical guarantees on the cosine similarity between smoothed attributions of original and perturbed samples. The approach involves computing volumes of hyperspherical caps to derive lower bounds on attribution robustness. Alternative formulations are developed to find the maximum perturbation size or minimum smoothing radius for a given similarity threshold. Monte Carlo integration is used to estimate the smoothed attributions, making the method scalable to large datasets.

## Key Results
- Provides the first theoretical guarantee for attribution robustness against ℓ2 attacks
- Effectively protects attributions across different network architectures (small CNN, ResNet-18, ResNet-50) and training schemes (standard, IG-NORM, TRADES, IGR)
- Demonstrates scalability to large datasets through Monte Carlo estimation
- Shows that uniform smoothing over ℓ2 balls creates volume-based lower bounds on cosine similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uniform smoothing over an ℓ2 ball creates a volume-based lower bound on cosine similarity between original and perturbed attributions.
- Mechanism: The intersection volume between two ℓ2 balls centered at the original and perturbed points determines the worst-case translation of the attribution vector, which directly constrains the minimum cosine similarity.
- Core assumption: Attribution function g(x) is upper-bounded by a constant M, and the smoothing distribution is uniform over an ℓ2 ball.
- Evidence anchors:
  - [abstract] "It is proved that, for all perturbations within the attack region, the cosine similarity between uniformly smoothed attribution of perturbed sample and the unperturbed sample is guaranteed to be lower bounded."
  - [section] "By solving the alternative problem, our result shows that the smoothed attribution is robust within the following half-angle of a spherical cone."
- Break condition: If g(x) is unbounded or the perturbation exceeds the smoothing radius such that the ball intersection becomes empty.

### Mechanism 2
- Claim: The smoothing radius r and attack budget ϵ have an inverse relationship in determining robustness - larger r allows larger ϵ while maintaining the same cosine similarity threshold.
- Mechanism: As r increases, the overlapping volume between the two ℓ2 balls grows, allowing more translation of the attribution vector before cosine similarity drops below the threshold. Conversely, for fixed r, larger ϵ reduces the overlap and tightens the bound.
- Core assumption: The relationship between volume overlap and cosine similarity is monotonic and can be computed via the regularized incomplete beta function.
- Evidence anchors:
  - [abstract] "We also derive alternative formulations of the certification that is equivalent to the original one and provides the maximum size of perturbation or the minimum smoothing radius such that the attribution can not be perturbed."
  - [section] "The lower bound also decreases when the attack budget ϵ increases."
- Break condition: When ϵ approaches 2r, the ball intersection becomes minimal and the bound becomes trivial.

### Mechanism 3
- Claim: Monte Carlo estimation of the smoothed attribution converges almost surely to the true integral, making the certification computationally scalable.
- Mechanism: By sampling N points uniformly from the ℓ2 ball and averaging the attributions, the estimator converges to the true smoothed attribution as N → ∞. The variance decreases as 1/N, making large N practical for certification.
- Core assumption: The attribution function g(x) is integrable over the ℓ2 ball and the Monte Carlo estimator has finite variance.
- Evidence anchors:
  - [section] "For large N, the estimator ˆh(x) almost surely converges to h(x) [18]; hence the convergence of ˆT to T can be obtained."
  - [section] "We used Monte Carlo Integration to calculate the integral in h(x), which estimates h(x) by sampling η from B."
- Break condition: If the attribution function has infinite variance over the ℓ2 ball or the sampling distribution is not uniform.

## Foundational Learning

- Concept: Volume of intersection between two ℓ2 balls (hyperspherical caps)
  - Why needed here: The robustness bound depends on computing the volume of the region where both original and perturbed smoothing balls overlap, which determines how much the attribution can change.
  - Quick check question: Given two ℓ2 balls of radius r with centers distance h apart, what is the volume of their intersection in terms of the regularized incomplete beta function?

- Concept: Cosine similarity as a measure of attribution robustness
  - Why needed here: Unlike ℓp norms, cosine similarity measures directional agreement between attributions, which is more appropriate for understanding if the attribution points to the same features even if magnitudes differ.
  - Quick check question: If two attribution vectors have angle θ between them, what is their cosine similarity and how does this relate to attribution robustness?

- Concept: Monte Carlo integration for high-dimensional uniform distributions
  - Why needed here: Computing the smoothed attribution requires integrating over an ℓ2 ball, which is intractable analytically in high dimensions. Monte Carlo provides a scalable approximation.
  - Quick check question: If you sample N points uniformly from an ℓ2 ball of radius r, what is the expected value and variance of the Monte Carlo estimator for the smoothed attribution?

## Architecture Onboarding

- Component map: Attribution function g(x) → Smoothed attribution h(x) via uniform sampling over ℓ2 ball → Robustness bound computation via volume formulas → Certification output

- Critical path:
  1. Load model and input sample
  2. Compute original attribution g(x)
  3. Sample N points from ℓ2 ball B(0,r)
  4. Compute smoothed attribution h(x) via Monte Carlo averaging
  5. Compute volumes VS and VU using beta function formulas
  6. Calculate lower bound T using Theorem 1 formula
  7. (Optional) Validate by attacking with IFIA and computing empirical cosine similarity

- Design tradeoffs:
  - Smoothing radius r vs. computational cost: Larger r gives better bounds but requires more samples for accurate Monte Carlo estimation
  - Number of Monte Carlo samples N vs. bound tightness: More samples reduce variance but increase computation time (currently ~15s per sample on ImageNet)
  - ℓ2 vs. other norms: ℓ2 provides clean volume formulas but other norms might be more appropriate for certain attack scenarios

- Failure signatures:
  - Bound T is very small (< 0.1): Likely r is too small relative to ϵ, or g(x) is unbounded
  - Monte Carlo estimator has high variance: May need more samples or the attribution function is too noisy
  - Empirical cosine similarity exceeds theoretical bound: Could indicate implementation error or that the attribution function doesn't satisfy the upper bound assumption

- First 3 experiments:
  1. Verify volume computation: Compute VU/VS for various r and ϵ values and check against known formulas for hyperspherical caps
  2. Test Monte Carlo convergence: Run with increasing N (1000, 10000, 100000) and plot variance reduction
  3. Validate certification: Apply ℓ2 IFIA attack on a simple model with known attribution bounds and verify that no perturbed sample exceeds the theoretical T value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound for the attribution robustness lower bound when the smoothing radius approaches infinity?
- Basis in paper: [explicit] The paper states "when the smoothing space is a ℓ2-norm ball, the above result can be derived by directly computing two volumes VU and VS" and discusses the behavior of the lower bound as smoothing radius r grows.
- Why unresolved: The paper mentions that "in extreme cases when r tends to infinity, the smoothing spaces of two samples completely overlap" but does not provide the exact mathematical derivation of the limit.
- What evidence would resolve it: A rigorous mathematical proof showing the limit of the lower bound expression as r approaches infinity would definitively answer this question.

### Open Question 2
- Question: How does the attribution robustness lower bound scale with the dimensionality of the input data?
- Basis in paper: [inferred] The paper mentions that "when ϵ increases beyond the constraint that h = r − ϵ/2 ≥ 0 and tends to infinity, the distance between two attributions will become further and their cosine similarity will tend to 0 in high dimensional space."
- Why unresolved: While the paper discusses the behavior in high dimensions, it does not provide a specific scaling law or relationship between the dimensionality and the lower bound.
- What evidence would resolve it: Empirical studies or theoretical analysis showing how the lower bound changes as a function of input dimensionality would provide clarity on this scaling relationship.

### Open Question 3
- Question: What is the computational complexity of computing the attribution robustness lower bound for large-scale datasets?
- Basis in paper: [explicit] The paper states "On the contrary, the previous works that approximately estimate the attribution robustness [ 46] require the computation of input Hessian and the corresponding eigenvalues and eigenvectors, which becomes intractable for larger size images on modern neural networks."
- Why unresolved: While the paper mentions that its method is scalable, it does not provide specific computational complexity analysis or runtime comparisons for large-scale datasets.
- What evidence would resolve it: Detailed runtime analysis and computational complexity calculations for different dataset sizes would quantify the scalability claims and provide insights into practical limitations.

## Limitations
- Theoretical guarantee relies heavily on the assumption that the attribution function is bounded by a constant M
- Computational cost of Monte Carlo integration scales poorly with input dimensionality
- ℓ2 norm constraint may not be optimal for all attack scenarios

## Confidence
- Theoretical framework and mathematical proofs: **High**
- Empirical validation on benchmark datasets: **Medium**
- Practical utility and computational efficiency: **Low**

## Next Checks
1. **Parameter sensitivity analysis**: Systematically vary the smoothing radius r and number of Monte Carlo samples N across different datasets to quantify their impact on both theoretical bounds and computational runtime, identifying optimal parameter ranges for practical deployment.

2. **Cross-norm comparison**: Implement and compare the certification method using ℓ1 and ℓ∞ norms to evaluate whether the ℓ2-specific volume analysis provides advantages or disadvantages compared to other norms for attribution robustness.

3. **Attribution method generalization**: Test the certification framework on attribution methods beyond integrated gradients (e.g., DeepLIFT, LIME, SHAP) to assess whether the bounded gradient assumption holds and whether the theoretical guarantees remain valid across different attribution approaches.