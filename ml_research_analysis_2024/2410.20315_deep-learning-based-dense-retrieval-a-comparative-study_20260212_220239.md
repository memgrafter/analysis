---
ver: rpa2
title: 'Deep Learning Based Dense Retrieval: A Comparative Study'
arxiv_id: '2410.20315'
source_url: https://arxiv.org/abs/2410.20315
tags:
- retrieval
- dense
- bert
- ance
- simcse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the vulnerability of dense retrieval models
  to poisoned tokenizers by simulating adversarial perturbations on query inputs.
  Models tested include BERT, DPR, Contriever, SimCSE, and ANCE.
---

# Deep Learning Based Dense Retrieval: A Comparative Study

## Quick Facts
- arXiv ID: 2410.20315
- Source URL: https://arxiv.org/abs/2410.20315
- Authors: Ming Zhong; Zhizhi Wu; Nanako Honda
- Reference count: 22
- Primary result: Dense retrieval models show significant vulnerability to tokenizer poisoning, with ANCE demonstrating highest resilience

## Executive Summary
This study evaluates how dense retrieval models respond to poisoned tokenizers by simulating adversarial perturbations on query inputs. Testing BERT, DPR, Contriever, SimCSE, and ANCE models on FiQA, HotpotQA, and Quora datasets reveals that even 5% tokenization perturbation causes notable performance degradation across all models. ANCE exhibits the most resilience, maintaining higher accuracy under minimal perturbations while showing systematic degradation only at higher perturbation rates (20%). The findings highlight critical vulnerabilities in dense retrieval systems and emphasize the need for robust defenses, particularly for high-stakes applications where retrieval accuracy is paramount.

## Method Summary
The study simulates adversarial attacks on dense retrieval models by introducing perturbations to tokenized input sequences. Researchers applied random integer noise to token IDs with perturbation rates ranging from 5% to 20%. Five models were evaluated: BERT, DPR, Contriever, SimCSE, and ANCE, using three datasets including Quora (400K+ question pairs), FiQA-2018 (financial opinion mining), and HotpotQA (multi-hop questions). Performance was measured using standard information retrieval metrics including Accuracy@k, Precision@k, Recall@k, NDCG@k, MRR@k, and MAP@k, comparing perturbed results against baseline performance.

## Key Results
- All models experienced performance degradation when tokenization was perturbed, with even 5% perturbation causing significant accuracy drops
- ANCE demonstrated highest resilience to tokenizer poisoning, maintaining better accuracy under minimal perturbations (5%) compared to other models
- Under 20% perturbation, all models showed notable performance decline, though ANCE exhibited the most systematic degradation pattern
- SimCSE showed significantly lower baseline accuracy and stability compared to other models under perturbation conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ANCE's use of dynamic negative sampling and approximate nearest neighbor (ANN) search provides greater robustness against tokenizer poisoning compared to supervised models like BERT and DPR.
- Mechanism: Dynamic negative sampling exposes the model to harder negative examples during training, improving its ability to distinguish similar queries and documents. ANN search allows effective retrieval even with altered queries, enhancing resilience to minor perturbations.
- Core assumption: The model's robustness stems from its training architecture and search techniques rather than its underlying language model architecture.
- Evidence anchors:
  - [abstract] "unsupervised models like ANCE show greater resilience"
  - [section] "ANCE employs approximate nearest neighbor (ANN) search techniques (e.g., Faiss)"
  - [corpus] "Dense retrievers utilize pre-trained backbone language models (e.g., BERT, LLaMA)" - weak evidence for specific ANN robustness
- Break condition: If the perturbation rate exceeds the model's ability to handle noisy inputs or if the poisoned tokens significantly alter semantic meaning beyond what ANN can compensate for.

### Mechanism 2
- Claim: Supervised models like BERT and DPR experience significant performance degradation when tokenizers are compromised due to their reliance on supervised fine-tuning.
- Mechanism: Supervised models are trained to optimize performance on specific tasks with clean, labeled data. When tokenizers introduce noise or bias, the models' learned representations become less reliable, leading to decreased retrieval accuracy.
- Core assumption: The performance degradation is primarily due to the mismatch between training data distribution and perturbed input distribution.
- Evidence anchors:
  - [abstract] "supervised models like BERT and DPR experience significant performance degradation when tokenizers are compromised"
  - [section] "We introduce random integer noise to the token IDs of tokenized text sequences input"
  - [corpus] "Unfortunately, they have recently been shown to be vulnerable to corpus poisoning attacks" - weak evidence for specific tokenizer poisoning
- Break condition: If the supervised models can be retrained or fine-tuned on perturbed data to regain performance.

### Mechanism 3
- Claim: Even small perturbations (5%) can severely impact retrieval accuracy across all models, highlighting the need for robust defenses in critical applications.
- Mechanism: Tokenization is a crucial step in the model's understanding and processing of text. Perturbations introduce errors that the model cannot easily recover from, leading to cascading failures in downstream tasks like retrieval.
- Core assumption: The models' performance is highly sensitive to tokenization quality and cannot effectively handle noisy or corrupted input.
- Evidence anchors:
  - [abstract] "even small perturbations can severely impact retrieval accuracy"
  - [section] "We experiment with perturbation rates ranging from 5% to 20%"
  - [corpus] "Despite their strong performance, Dense Passage Retrieval (DPR) models suffer from a lack of interpretability" - weak evidence for sensitivity to tokenization
- Break condition: If the models can be augmented with error correction or denoising mechanisms that mitigate the impact of tokenization noise.

## Foundational Learning

- Concept: Dense Retrieval vs. Sparse Retrieval
  - Why needed here: Understanding the difference between dense and sparse retrieval is crucial for grasping the significance of the study's findings and the potential impact of tokenizer poisoning on different types of retrieval systems.
  - Quick check question: What is the main difference between dense and sparse retrieval in terms of how they match texts?

- Concept: Tokenization and Its Role in NLP Models
  - Why needed here: Tokenization is a fundamental step in processing text for NLP models. Understanding its importance and potential vulnerabilities is essential for comprehending the study's focus on tokenizer poisoning.
  - Quick check question: Why is tokenization a critical component in NLP models, and how could poisoning affect model performance?

- Concept: Evaluation Metrics in Information Retrieval
  - Why needed here: The study uses various evaluation metrics (e.g., Accuracy, Precision, Recall, NDCG, MRR, MAP) to assess model performance. Understanding these metrics is necessary for interpreting the results and comparing model performance.
  - Quick check question: What do metrics like NDCG and MRR measure in the context of information retrieval, and why are they important?

## Architecture Onboarding

- Component map: Input Query -> Tokenizer -> Model (Dense Embeddings) -> Similarity Measure -> Retrieval -> Evaluation Metrics
- Critical path: 1. Tokenize input query and documents 2. Generate dense embeddings using the model 3. Compute similarity between query and document embeddings 4. Retrieve top-k most similar documents 5. Evaluate retrieval performance
- Design tradeoffs:
  - Model complexity vs. inference speed: More complex models may provide better performance but slower inference times.
  - Precision vs. recall: Higher precision may lead to lower recall and vice versa, depending on the application's needs.
  - Robustness vs. accuracy: Models optimized for robustness may sacrifice some accuracy on clean data.
- Failure signatures: Sudden drops in retrieval accuracy when input tokenization is perturbed, inconsistent results across different perturbation rates, lower performance on specific datasets or query types
- First 3 experiments:
  1. Evaluate baseline performance of all models on clean data using standard evaluation metrics.
  2. Introduce small perturbations (e.g., 5%) to input tokenization and measure the impact on retrieval accuracy.
  3. Gradually increase perturbation rates (e.g., 10%, 15%, 20%) and observe how each model's performance degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ANCE's dynamic negative sampling and ANN search techniques compare in effectiveness to other defense mechanisms against tokenizer poisoning in dense retrieval models?
- Basis in paper: [explicit] The paper explicitly states that ANCE's dynamic negative sampling and ANN search contribute to its robustness against tokenizer poisoning, but does not compare these techniques to other potential defense mechanisms.
- Why unresolved: The study focuses on evaluating model performance under perturbation attacks but does not explore alternative defense mechanisms or compare ANCE's techniques to other approaches.
- What evidence would resolve it: Comparative experiments testing ANCE's techniques against other defense mechanisms, such as adversarial training or input sanitization, across multiple dense retrieval models and datasets.

### Open Question 2
- Question: What is the impact of tokenizer poisoning on the computational efficiency of dense retrieval models during both training and inference phases?
- Basis in paper: [inferred] While the paper discusses performance degradation in accuracy metrics, it does not address the potential impact on computational efficiency when models process poisoned tokenizers.
- Why unresolved: The study focuses on accuracy and robustness metrics but does not measure or analyze changes in computational resources (e.g., time, memory) required to process poisoned inputs.
- What evidence would resolve it: Experiments measuring training and inference time, memory usage, and other computational resources for models processing both clean and poisoned tokenizers across different perturbation rates.

### Open Question 3
- Question: How does the severity of tokenizer poisoning affect the interpretability of dense retrieval models' decision-making processes?
- Basis in paper: [inferred] The paper does not address how poisoned tokenizers might impact the ability to interpret or explain model predictions, which is an important consideration for high-stakes applications.
- Why unresolved: The study focuses on quantitative performance metrics but does not explore qualitative aspects such as model interpretability or explainability under adversarial conditions.
- What evidence would resolve it: Analysis of attention weights, feature importance, or other interpretability techniques applied to models processing poisoned inputs, comparing results to clean input scenarios across different perturbation levels.

## Limitations

- The implementation details of the perturbation mechanism are not fully specified, particularly regarding how random integer noise is applied to token IDs while ensuring valid tokenization
- The paper lacks direct mechanistic evidence linking ANCE's architectural features to its observed robustness, without ablation studies isolating specific components
- No analysis of computational efficiency impacts when models process poisoned tokenizers, focusing solely on accuracy metrics without resource utilization measurements

## Confidence

- High: The general finding that all models experience performance degradation under perturbation is well-supported by the experimental design and results
- Medium: The comparative ranking of model robustness (ANCE > BERT > others) appears consistent but could be influenced by implementation-specific factors
- Low: The specific mechanisms proposed for ANCE's robustness require additional validation through controlled experiments

## Next Checks

1. Implement ablation studies removing ANCE's dynamic negative sampling to isolate its contribution to robustness
2. Test perturbation effects on a broader range of models with similar training architectures but different search mechanisms
3. Validate perturbation implementation by examining token distribution shifts and ensuring generated tokens remain valid within the vocabulary