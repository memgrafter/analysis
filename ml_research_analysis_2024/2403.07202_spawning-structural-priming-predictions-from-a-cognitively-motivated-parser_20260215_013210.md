---
ver: rpa2
title: SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser
arxiv_id: '2403.07202'
source_url: https://arxiv.org/abs/2403.07202
tags:
- priming
- sentence
- which
- predictions
- spawn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SPAWN, a cognitively motivated parser capable
  of generating quantitative priming predictions from syntactic theories. SPAWN integrates
  ACT-R-based cognitive principles with a flexible grammar formalism, enabling it
  to handle lexicalized theories of syntax and model a broader range of linguistic
  phenomena than prior models.
---

# SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser

## Quick Facts
- arXiv ID: 2403.07202
- Source URL: https://arxiv.org/abs/2403.07202
- Reference count: 11
- One-line primary result: SPAWN parser shows that Participial-Phase theory better captures human sentence representations than Whiz-Deletion theory for reduced relative clauses.

## Executive Summary
This work introduces SPAWN, a cognitively motivated parser that generates quantitative priming predictions from syntactic theories using ACT-R-based cognitive principles and CCG grammar formalism. The parser integrates incremental parsing, retrieval with activation updates, reanalysis with inhibition, and probabilistic null element prediction to model how humans process and learn syntactic structures. As a case study, SPAWN compared two theories of reduced relative clauses (Whiz-Deletion and Participial-Phase) while varying reanalysis mechanisms and prior knowledge, finding that only Participial-Phase with minimal prior knowledge matched empirical priming patterns.

## Method Summary
SPAWN implements a cognitively motivated parser with ACT-R-based mechanisms for retrieval, integration, reanalysis, and null element prediction using CCG grammar. The method involves generating training data from syntactic theory templates, training 12 model types with varying grammars, training data sizes, and reanalysis mechanisms, then testing predictions against human priming data from a web-based comprehension-to-production paradigm. Bayesian mixed-effects logistic regression is used to estimate priming effects and compare model predictions to human behavior.

## Key Results
- Only the Participial-Phase theory with minimal prior knowledge aligned with empirical priming patterns
- Whiz-Deletion theory failed to match human behavior under any tested condition
- Reanalysis mechanism and prior knowledge significantly impacted priming predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Priming effects emerge from incremental parsing combined with lexical and syntactic activation updates
- Mechanism: When parsing a prime sentence, categories are retrieved based on activation scores that combine base-level (frequency and recency), lexical (context-independent probability), and inhibition (from past reanalyses). After each sentence, activation counts are updated, increasing the likelihood of retrieving the same categories in similar contexts
- Core assumption: Parsing is incremental, with each word processed sequentially and categories integrated immediately if possible
- Evidence anchors: [abstract] "priming emerges as a consequence of parsing and learning"; [section] "After processing this sentence, the model is more likely to expect a "to" after the verb "sent" than "the"."

### Mechanism 2
- Claim: Reanalysis with inhibition prevents repeated retrieval of incorrect categories, refining parsing predictions over trials
- Mechanism: When a retrieved category fails integration, the parser backtracks to a prior word index, discards the categories for intervening words, and resets the parse state. Inhibition is increased for the discarded categories, lowering their future activation until they are overridden by base-level or lexical activation
- Core assumption: Inhibition is computed from counts of how often a category was discarded during reanalysis in the current sentence
- Evidence anchors: [section] "inhibition might be insufficient to suppress the relative activation of an incorrect category cij, it cij has very high base-level or lexical activation"; [section] "The parser keeps track of the categories that were discarded for all the words when processing a given sentence s"

### Mechanism 3
- Claim: Null (covert) elements are predicted probabilistically based on parse state and context-independent uncertainty
- Mechanism: For parse states that can be followed by null elements, the parser computes activation for each null category and a "not-null" category. The option with highest activation is selected, balancing base-level frequency, lexical activation, and inhibition
- Core assumption: Only certain parse states allow null elements, limiting the search space
- Evidence anchors: [section] "When the current parse state can be followed by a null element, there is uncertainty about the next parsing decision."; [section] "The activation for Niks is given by Equation 6... Lik is computed the same as lexical activation for syntactic categories (Equation 4), and Iiks the same as inhibition (Equation 5)"

## Foundational Learning

- Concept: Base-level activation in ACT-R (Equation 2)
  - Why needed here: Models prior exposure to syntactic categories; determines how often and recently a category was used
  - Quick check question: If a category was used 10 times in the last 100ms but not before, will its base-level activation be higher than a category used once 10s ago?

- Concept: Inhibition-based reanalysis
  - Why needed here: Prevents repeated selection of wrong categories; allows parser to explore alternatives without infinite loops
  - Quick check question: What happens if inhibition is set to zero for all categories during reanalysis?

- Concept: Incremental CCG integration
  - Why needed here: Enables on-the-fly combination of categories as words are processed; supports flexible grammar formalism
  - Quick check question: If a retrieved category cannot combine with the current parse state, what does the parser do next?

## Architecture Onboarding

- Component map: Declarative memory (lexical and syntax chunks) -> Procedural memory (retrieval, integration, reanalysis, null prediction) -> Buffers (current words, retrieved categories, parse state) -> Activation model (base-level, lexical, inhibition, noise)
- Critical path: Word -> Retrieve category -> Integrate with parse state -> If fail, reanalysis -> If allow null, predict null -> Next word
- Design tradeoffs:
  - Serial vs parallel parsing: Serial matches ACT-R cognitive realism but is slower
  - Full vs partial reanalysis: Full (back to start) is more stable but less incremental; partial (sample index) is more incremental but riskier
  - Noise vs determinism: Noise adds robustness but makes results stochastic; determinism is easier to debug
- Failure signatures:
  - Parser never completes a sentence: Likely inhibition decay too fast or noise too high
  - Parser always picks wrong categories: Base-level or lexical activation for incorrect categories too high; inhibition too low
  - Parser inserts null everywhere: Null-allowed states mis-specified; null activation too high
- First 3 experiments:
  1. Parse a simple sentence with one known category; verify activation update matches Equation 1
  2. Parse a sentence with a temporary ambiguity; check that reanalysis backtracks correctly and inhibition updates
  3. Parse a sentence with an optional null; confirm null prediction matches the activation comparison between null and not-null options

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different simplifying assumptions in SPAWN (e.g., serial parsing, reanalysis mechanisms, "giving up" criteria, null element predictions) impact its behavioral predictions?
- Basis in paper: [explicit] The paper acknowledges that SPAWN makes simplifying assumptions and that future work should systematically study their impact on predictions, particularly by evaluating fine-grained predictions with empirical eye-tracking data
- Why unresolved: The current study only modulated one simplifying assumption (reanalysis mechanism) and did not comprehensively explore the impact of other assumptions
- What evidence would resolve it: Systematic experiments evaluating SPAWN's predictions with empirical eye-tracking data under varying assumptions for serial vs. parallel parsing, reanalysis strategies, "give-up" thresholds, and null element predictions

### Open Question 2
- Question: Would supporting additional grammar formalisms in SPAWN (beyond CCG) reduce the need for translating between formalisms and improve its applicability to diverse syntactic theories?
- Basis in paper: [explicit] The paper notes that SPAWN is currently implemented using CCG and that supporting other formalisms could minimize translation needs, though CCG is flexible enough to express most syntactic theories
- Why unresolved: The current implementation only supports CCG, and the impact of adding other formalisms on SPAWN's predictions and usability remains untested
- What evidence would resolve it: Implementing SPAWN with additional grammar formalisms (e.g., Minimalist Grammar, HPSG) and comparing predictions and usability across formalisms for diverse syntactic theories

### Open Question 3
- Question: How do the predictions of SPAWN align with human behavior when applied to syntactic phenomena beyond relative clauses?
- Basis in paper: [explicit] The paper suggests that applying SPAWN to other empirical phenomena could shed light on model improvements, implying that its predictions for other phenomena are currently unknown
- Why unresolved: The case study only examined reduced relative clauses, leaving SPAWN's predictions for other syntactic phenomena untested
- What evidence would resolve it: Using SPAWN to generate and test predictions for priming patterns in other syntactic constructions (e.g., dative alternation, passive/active voice) and comparing these predictions to empirical data

## Limitations
- The comparison between syntactic theories is limited to reduced relative clauses and may not generalize to other syntactic structures
- Model predictions are sensitive to parameter tuning of ACT-R mechanisms like base-level and lexical activation
- Bayesian mixed-effects logistic regression assumes normal error distributions that may not capture full complexity of priming effects

## Confidence
- High confidence: The cognitive plausibility of SPAWN's parsing mechanisms and their grounding in ACT-R principles
- Medium confidence: The claim that Participial-Phase theory better explains human sentence representations than Whiz-Deletion, based on current priming data
- Medium confidence: The generalizability of SPAWN's framework to adjudicate between other competing syntactic theories

## Next Checks
1. Test SPAWN's predictions on additional syntactic phenomena (e.g., garden path sentences, filler-gap dependencies) to assess generalizability beyond reduced relative clauses
2. Conduct ablation studies removing individual cognitive mechanisms (inhibition, base-level activation) to quantify their contribution to priming predictions
3. Compare SPAWN's performance against non-cognitive parsers on the same priming tasks to evaluate the added value of cognitive constraints