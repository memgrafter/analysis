---
ver: rpa2
title: 'Characterizing Multimodal Long-form Summarization: A Case Study on Financial
  Reports'
arxiv_id: '2404.06162'
source_url: https://arxiv.org/abs/2404.06162
tags:
- report
- summary
- numeric
- sentence
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a computational framework to characterize multimodal
  long-form summarization using financial reports as a case study. The authors analyze
  summaries generated by Claude 2.0/2.1, GPT-4/3.5, and Cohere to understand their
  behavior and capabilities.
---

# Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports

## Quick Facts
- **arXiv ID**: 2404.06162
- **Source URL**: https://arxiv.org/abs/2404.06162
- **Reference count**: 29
- **Primary result**: A computational framework characterizes multimodal long-form summarization using financial reports, revealing Claude 2's superior capability in handling long multimodal inputs compared to GPT-4

## Executive Summary
This paper presents a computational framework to characterize multimodal long-form summarization using financial reports as a case study. The authors analyze summaries generated by Claude 2.0/2.1, GPT-4/3.5, and Cohere to understand their behavior and capabilities. The framework examines extractiveness, position bias, and numeric value utilization to provide comprehensive insights into how different LLMs handle long multimodal inputs. The study reveals significant differences between models, with Claude 2 demonstrating superior performance in using numbers and recognizing important information compared to GPT-4.

## Method Summary
The authors collected 1,000 HTML files of 10-K financial reports from the SEC EDGAR system, converting them to JSON format and extracting the Item 7 (MD&A) section. They used commercial LLMs (Claude 2.0/2.1, GPT-4/3.5, and Cohere) with provided prompts to generate summaries of these long multimodal documents. The analysis framework examined extractiveness (how much summary content comes directly from the input), position bias (whether information comes more from beginning or end of reports), numeric value utilization (how effectively numbers from text and tables are used), and numeric hallucinations (incorrect numeric values in summaries). The framework also tested prompt engineering approaches to improve model performance.

## Key Results
- GPT-3.5 and Cohere fail to meaningfully perform long-form multimodal summarization
- Extractive sentences represent 30-40% of summaries, with Claude 2.1 generating the most extractive content
- Claude 2's position bias disappears after shuffling input, suggesting it recognizes important information rather than relying on position
- Claude 2 demonstrates superior numeric value utilization compared to GPT-4, with higher tabular numbers utilization and number density
- LLMs hallucinate in only about 5% of numerical values, with context mismatch being the most common type of hallucination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed framework effectively characterizes multimodal long-form summarization by analyzing extractiveness, position bias, and numeric value utilization.
- Mechanism: The framework systematically examines how much of the summary is extractive, where extractive information comes from in the input, and how numeric data from both text and tables is utilized.
- Core assumption: The combination of extractive analysis, position tracking, and numeric categorization provides comprehensive insight into LLM behavior on long multimodal inputs.
- Evidence anchors:
  - [abstract] "We propose a computational framework for characterizing multimodal long-form summarization"
  - [section] "We propose a computational framework to characterize multimodal long-form summarization in summarizing financial reports"
  - [corpus] Weak evidence - the corpus provides related work but not direct evidence of this framework's effectiveness
- Break condition: If the framework fails to capture meaningful patterns in LLM behavior or cannot differentiate between models' approaches to the task.

### Mechanism 2
- Claim: Claude 2 demonstrates superior capability in handling long multimodal inputs compared to GPT-4, particularly in using numbers and recognizing important information.
- Mechanism: Claude 2 shows higher tabular numbers utilization, higher number density in summaries, and the position bias disappears after shuffling the input.
- Core assumption: The observed differences in behavior between Claude 2 and GPT-4 are meaningful indicators of superior capability.
- Evidence anchors:
  - [abstract] "Overall, our analyses highlight the strong capability of Claude 2 in handling long multimodal inputs compared to GPT-4"
  - [section] "We compare the behavior of Claude and GPT-4, and show that Claude demonstrates a stronger ability to use numbers and seems to recognize important information"
  - [corpus] Moderate evidence - the corpus shows related work on long-form summarization but doesn't directly compare Claude 2 and GPT-4
- Break condition: If the differences are due to factors unrelated to capability (e.g., different training data exposure or architectural differences not related to core capability).

### Mechanism 3
- Claim: Position bias in LLMs can be mitigated by shuffling input for Claude 2 but persists for GPT-4.
- Mechanism: When the input is shuffled, Claude 2's source information distribution becomes even across the report, while GPT-4 maintains preference for beginning sections.
- Core assumption: The disappearance of position bias after shuffling indicates that Claude 2 can recognize important information rather than relying on position.
- Evidence anchors:
  - [section] "the bias disappears for Claude after we shuffle the input, which suggests that Claude seems to recognize important information"
  - [abstract] "This position bias disappears after shuffling the input for Claude, which suggests that Claude seems to recognize important information"
  - [corpus] Weak evidence - the corpus doesn't directly address position bias mitigation through input shuffling
- Break condition: If the observed behavior is an artifact of the specific dataset or if other factors (like tokenization differences) explain the pattern.

## Foundational Learning

- Concept: Extractive summarization evaluation
  - Why needed here: The framework needs to quantify how much of the generated summary comes directly from the input text
  - Quick check question: How does the greedy match algorithm with quadratic bonus measure similarity between summary and report sentences?

- Concept: Position bias analysis in long contexts
  - Why needed here: Understanding whether models rely on position rather than content importance is crucial for evaluating their capability
  - Quick check question: What does it mean when Claude's position bias disappears after shuffling but GPT-4's persists?

- Concept: Numeric hallucination taxonomy
  - Why needed here: Financial reports contain many numbers, and distinguishing between correct usage and hallucinations is essential
  - Quick check question: What are the four types of numeric hallucinations identified in the framework?

## Architecture Onboarding

- Component map: Input → extractive similarity scoring → position distribution analysis → numeric extraction → hallucination annotation → prompt engineering testing
- Critical path: Input → extractive similarity scoring → position distribution analysis → numeric extraction → hallucination annotation → prompt engineering testing
- Design tradeoffs: Using manual annotation for hallucinations provides accuracy but limits scale; automatic similarity scoring enables large-scale analysis but may miss nuanced patterns.
- Failure signatures: If extractive percentages are consistently near 0% or 100%, position bias analysis shows no clear pattern, or numeric hallucination rates are abnormally high/low.
- First 3 experiments:
  1. Run extractive analysis on a small sample to verify the similarity scoring algorithm works as expected
  2. Test position bias analysis on shuffled vs. original reports for both Claude and GPT-4
  3. Validate numeric hallucination annotation protocol on a few examples to ensure consistent interpretation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Claude 3 compare to Claude 2 and GPT-4 in multimodal long-form summarization tasks?
- Basis in paper: [inferred] The paper notes that Claude 3 was released after the study began, so it was not included in the analysis.
- Why unresolved: The study only compared Claude 2.0/2.1 and GPT-4/3.5, missing the opportunity to evaluate the latest Claude model.
- What evidence would resolve it: Including Claude 3 in the analysis and comparing its performance metrics (e.g., extractiveness, numeric utilization, hallucination rates) with those of Claude 2 and GPT-4.

### Open Question 2
- Question: What are the specific factors that contribute to Claude's superior ability to use numbers and recognize important information compared to GPT-4?
- Basis in paper: [explicit] The paper suggests that Claude may have more exposure to enterprise data/applications involving structured data and business metrics, but this remains a hypothesis.
- Why unresolved: The study does not provide concrete evidence or a detailed analysis of the underlying causes for the observed performance differences.
- What evidence would resolve it: Conducting a comprehensive investigation into the training data, model architecture, and fine-tuning processes of both Claude and GPT-4 to identify the key factors contributing to their respective strengths in handling multimodal multimodal long-form inputs.

### Open Question 3
- Question: How do different prompt engineering strategies affect the performance of various LLMs in multimodal long-form summarization tasks?
- Basis in paper: [explicit] The paper explores the use of prompt engineering to improve GPT-4's use of numbers but notes limited success and the need for further investigation.
- Why unresolved: The study only tested a limited set of prompts and focused primarily on GPT-4, leaving open the question of how other models and prompt strategies might perform.
- What evidence would resolve it: Systematically testing a wide range of prompt engineering techniques (e.g., Chain-of-Thought, role-playing, example-based prompting) across multiple LLMs (e.g., Claude, GPT-4, Cohere) and evaluating their impact on various aspects of summarization performance (e.g., extractiveness, numeric utilization, hallucination rates).

## Limitations
- The analysis relies on commercial LLMs without fine-tuning, potentially conflating capability differences with architectural or training data variations
- The framework's metrics may not comprehensively capture all aspects of summarization quality beyond extractiveness, position bias, and numeric utilization
- Manual hallucination annotation limits scalability and introduces potential subjectivity in the analysis
- Financial report domain specificity may limit generalizability to other domains with different structural characteristics

## Confidence

- **High confidence**: Claude 2.1 generates the most extractive content (30-40% of summaries) and demonstrates superior numeric value utilization compared to GPT-4. The position bias finding for GPT-4 is robust across analyses.
- **Medium confidence**: The claim that Claude 2 demonstrates "stronger capability" overall is supported by specific metrics but may conflate capability with different architectural approaches to handling long contexts. The prompt engineering improvement for GPT-4's numeric usage is observed but may not fully bridge the capability gap.
- **Low confidence**: The generalizability of findings beyond financial reports and the specific models tested. The framework's ability to comprehensively characterize all aspects of multimodal long-form summarization.

## Next Checks

1. **Cross-domain validation**: Apply the same framework to a different domain (e.g., scientific articles or legal documents) to test whether the observed patterns in extractive behavior, position bias, and numeric utilization generalize beyond financial reports.

2. **Controlled model comparison**: Fine-tune smaller models on the same financial report summarization task to isolate whether observed differences between Claude 2 and GPT-4 stem from core capability versus architectural or training data differences.

3. **Human evaluation correlation**: Conduct a small-scale human evaluation of summary quality, relevance, and informativeness to determine whether the computational framework's metrics (extractiveness, position bias, numeric utilization) correlate with actual summary quality judgments.