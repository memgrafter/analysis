---
ver: rpa2
title: FACTS About Building Retrieval Augmented Generation-based Chatbots
arxiv_id: '2407.07858'
source_url: https://arxiv.org/abs/2407.07858
tags:
- chatbots
- llms
- data
- enterprise
- building
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for building enterprise-grade retrieval-augmented
  generation (RAG) chatbots, addressing challenges in content freshness, architecture,
  cost, testing, and security (FACTS). The authors identify 15 control points in RAG
  pipelines and propose solutions for each, including metadata enrichment, chunking,
  query rephrasing, reranking, hybrid search, and agentic architectures for complex
  queries.
---

# FACTS About Building Retrieval Augmented Generation-based Chatbots

## Quick Facts
- arXiv ID: 2407.07858
- Source URL: https://arxiv.org/abs/2407.07858
- Reference count: 17
- Key outcome: Enterprise RAG chatbot framework addressing content freshness, architecture, cost, testing, and security with 15 control points

## Executive Summary
This paper presents FACTS (Freshness, Architecture, Cost, Testing, Security), a comprehensive framework for building enterprise-grade retrieval-augmented generation (RAG) chatbots. The authors identify 15 critical control points in RAG pipelines and propose solutions for each, including metadata enrichment, chunking, query rephrasing, reranking, hybrid search, and agentic architectures for complex queries. The framework emphasizes the need for flexible architectures, robust testing, and security measures including guardrails for sensitive data and access control compliance.

## Method Summary
The authors developed a RAG pipeline with 15 control points for enterprise chatbot deployment, tested across three internal NVIDIA bots (NVInfo with ~500M documents, NVHelp with 2K documents, and Scout with 4K documents). They implemented metadata enrichment, chunking strategies, query rephrasing, reranking, hybrid search combining lexical and vector approaches, and agentic architectures for complex queries. The framework includes guardrails for security, document access controls, and multi-modal data handling. Empirical evaluation compared Llama3-70B against GPT-4 across accuracy and latency metrics.

## Key Results
- Metadata enrichment, chunking, query rephrasing, and reranking have the most impact on RAG pipeline quality
- Hybrid search combining lexical and vector approaches improves retrieval relevancy and coverage, especially for entity matching
- Llama3-70B can match or exceed GPT-4 performance in some aspects while maintaining acceptable latency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metadata enrichment, chunking, query rephrasing, and query reranking are the most impactful control points for improving RAG pipeline quality.
- Mechanism: These stages directly influence retrieval relevancy, which is the foundation for accurate LLM response generation. Poor metadata or chunking leads to irrelevant document retrieval, causing the LLM to generate inaccurate or hallucinated answers.
- Core assumption: Retrieval relevancy is the dominant factor in determining the quality of RAG-based chatbot responses.
- Evidence anchors:
  - [section] "We noticed that metadata enrichment, chunking, query rephrasing and query re-ranking stages of RAG pipeline have the most impact on the quality of Chatbot responses. LLM response generation quality is highly dependent on retrieval relevancy."
  - [corpus] No direct corpus evidence comparing these stages to other control points, but the claim is strongly supported by the paper's own experiments and observations.
- Break condition: If the retrieved documents are highly relevant but the LLM still generates poor responses, the bottleneck may lie in prompt engineering or the LLM itself rather than retrieval stages.

### Mechanism 2
- Claim: Hybrid search (combining lexical and vector-based search) improves retrieval relevancy and coverage, especially for entity matching.
- Mechanism: Vector databases excel at semantic similarity but struggle with exact entity matching (e.g., names, places). Lexical search handles exact matches well, so combining both methods captures both semantic and exact matches, improving overall retrieval quality.
- Core assumption: Entity matching is a significant challenge in enterprise RAG systems, and lexical search is effective for exact matches.
- Evidence anchors:
  - [section] "We noticed that Vector databases are not so good at handling matching entities (e.g., people names, places, company names etc.). Using a combination of Lexical search (e.g., elastic search) and vector search provided better retrieval relevancy and more coverage."
  - [corpus] No direct corpus evidence, but the claim aligns with known limitations of vector databases and the strengths of lexical search.
- Break condition: If the enterprise dataset contains few entities or if semantic matching is the primary requirement, the benefits of hybrid search may be minimal.

### Mechanism 3
- Claim: Smaller open-source LLMs can match or exceed the performance of larger commercial models like GPT-4 in some aspects while maintaining acceptable latency.
- Mechanism: Open-source models like Llama3-70B, when optimized with GPU libraries like TensorRT-LLM, can achieve comparable accuracy to larger models while being more cost-effective and faster due to reduced computational overhead.
- Core assumption: Open-source models are sufficiently capable for enterprise use cases and can be optimized for performance.
- Evidence anchors:
  - [abstract] "They also evaluate the accuracy-latency tradeoffs between large and small LLMs, finding that smaller open-source models like Llama3-70B can match or exceed the performance of larger models like GPT-4 in some aspects while maintaining acceptable latency."
  - [section] "Our results show that the Llama3-70B model excels in several aspects of answer quality while maintaining acceptable latency."
  - [corpus] No direct corpus evidence comparing Llama3-70B to GPT-4, but the claim is supported by the paper's empirical results.
- Break condition: If the enterprise use case requires extremely complex reasoning or long-form generation, larger models may still be necessary despite higher costs and latency.

## Foundational Learning

- Concept: Vector databases and semantic search
  - Why needed here: RAG systems rely on vector databases to retrieve relevant documents based on semantic similarity, which is crucial for providing accurate and context-aware responses in chatbots.
  - Quick check question: How does a vector database differ from a traditional keyword-based search engine?

- Concept: Prompt engineering and fine-tuning
  - Why needed here: The quality of LLM responses in RAG systems is highly dependent on the prompts used. Effective prompt engineering can significantly improve response accuracy, while fine-tuning can adapt the model to specific enterprise domains.
  - Quick check question: What are the key differences between prompt engineering and fine-tuning, and when would you choose one over the other?

- Concept: Multi-modal data handling
  - Why needed here: Enterprise data is often multi-modal, including text, images, tables, and other formats. RAG systems must be able to handle and retrieve relevant information from these diverse data types to provide comprehensive responses.
  - Quick check question: What challenges arise when dealing with multi-modal data in RAG systems, and how can they be addressed?

## Architecture Onboarding

- Component map: User Query -> Query Rephrasing -> Vector Database (with Embedding Model) -> Document Retrieval and Reranking -> Prompt Construction -> LLM Response Generation -> Guardrails and Security Checks -> User Feedback

- Critical path:
  1. User query â†’ Query rephrasing
  2. Semantic search in vector database
  3. Document retrieval and reranking
  4. Prompt construction with retrieved documents
  5. LLM response generation
  6. Guardrail checks and response refinement
  7. User feedback collection

- Design tradeoffs:
  - Accuracy vs. latency: Larger LLMs may provide more accurate responses but at the cost of increased latency.
  - Cost vs. performance: Smaller open-source models can be more cost-effective but may require more optimization to match the performance of larger models.
  - Complexity vs. maintainability: Agentic architectures can handle complex queries but add complexity to the system.

- Failure signatures:
  - Irrelevant or hallucinated responses: Likely due to poor retrieval relevancy or inadequate prompt engineering.
  - Slow response times: May indicate suboptimal LLM selection or inefficient vector database queries.
  - Security breaches: Could result from insufficient guardrails or improper access control implementation.

- First 3 experiments:
  1. Evaluate the impact of different chunking strategies on retrieval relevancy and response accuracy.
  2. Compare the performance of hybrid search (lexical + vector) against pure vector search for entity-heavy queries.
  3. Benchmark the accuracy-latency tradeoffs of different LLM models (e.g., GPT-4 vs. Llama3-70B) on a representative dataset of enterprise queries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are hybrid search approaches combining lexical and vector-based searches in improving retrieval accuracy and coverage compared to using either approach alone?
- Basis in paper: [explicit] The paper mentions hybrid search capabilities and states "Using a combination of Lexical search (e.g., elastic search) and vector search provided better retrieval relevancy and more coverage."
- Why unresolved: The paper does not provide quantitative comparisons or specific metrics to demonstrate the effectiveness of hybrid search approaches.
- What evidence would resolve it: Empirical studies comparing retrieval accuracy and coverage metrics between hybrid search, pure lexical search, and pure vector search approaches in enterprise RAG systems.

### Open Question 2
- Question: What is the optimal balance between using large commercial LLMs versus smaller open-source models in terms of cost, accuracy, and latency for enterprise chatbot deployments?
- Basis in paper: [explicit] The paper discusses cost economics and presents empirical results comparing Llama3-70B with GPT-4, finding that smaller models can match or exceed performance in some aspects while maintaining acceptable latency.
- Why unresolved: The paper does not provide a comprehensive framework or guidelines for determining the optimal balance for different enterprise use cases and requirements.
- What evidence would resolve it: A detailed analysis of cost, accuracy, and latency trade-offs across various enterprise chatbot scenarios, along with guidelines for selecting the appropriate model size and type based on specific use case requirements.

### Open Question 3
- Question: How can RAG pipelines be effectively optimized to handle complex, multi-part queries that require analytical reasoning and query decomposition?
- Basis in paper: [explicit] The paper mentions agentic architectures and presents a reference architecture for handling complex queries, but acknowledges that "IR systems and LLMs are insufficient for answering complex queries."
- Why unresolved: The paper does not provide a complete solution or evaluation of the effectiveness of agentic architectures in handling complex queries across different enterprise domains.
- What evidence would resolve it: Empirical studies comparing the performance of RAG systems with and without agentic architectures in handling complex queries, along with metrics for query decomposition accuracy and overall response quality.

## Limitations

- Limited external validation of claims, primarily based on internal NVIDIA implementations
- Missing detailed implementation specifications for the 15 control points
- Potential overfitting to enterprise-specific use cases without broader generalization testing

## Confidence

- Retrieval stage impact (metadata, chunking, query rephrasing): High - Well-established in RAG literature and strongly supported by the paper's own experiments
- Hybrid search benefits: Medium - The mechanism is sound but lacks external validation beyond the paper's observations
- Llama3-70B vs GPT-4 performance claims: Medium - Empirical results are presented but no direct corpus evidence comparing these specific models exists

## Next Checks

1. **Independent Benchmarking**: Replicate the accuracy-latency comparisons between Llama3-70B and GPT-4 using standardized enterprise datasets from multiple organizations to verify the performance claims

2. **Control Point Impact Analysis**: Systematically test each of the 15 control points individually on a public RAG benchmark to quantify their relative contributions to overall system performance

3. **Security Validation**: Conduct third-party security audits of the guardrail implementations using standard enterprise security frameworks to verify the claimed effectiveness of sensitive data protection mechanisms