---
ver: rpa2
title: 'CompAct: Compressing Retrieved Documents Actively for Question Answering'
arxiv_id: '2407.09014'
source_url: https://arxiv.org/abs/2407.09014
tags:
- documents
- information
- compact
- question
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CompAct, a novel framework that employs an
  active strategy to compress extensive retrieved documents without losing key information.
  CompAct addresses the challenge of handling long contexts by dynamically preserving
  query-related contexts and integrating information across multiple documents.
---

# CompAct: Compressing Retrieved Documents Actively for Question Answering

## Quick Facts
- arXiv ID: 2407.09014
- Source URL: https://arxiv.org/abs/2407.09014
- Reference count: 28
- Primary result: 7.0 F1 improvement on HotpotQA with 47x compression rate

## Executive Summary
CompAct introduces an active compression framework that addresses the challenge of processing long contexts in multi-document question answering. The framework dynamically compresses retrieved documents by iteratively analyzing previously compressed text with newly provided segments, while incorporating an early termination mechanism to halt compression when sufficient information is gathered. CompAct achieves significant performance improvements over existing compressors and demonstrates compatibility with various retrievers and readers as a plug-in module.

## Method Summary
CompAct employs a two-stage process: active compression and early termination. During compression, retrieved documents are segmented into fixed-size chunks (default 5 documents per segment) and processed iteratively. The model jointly analyzes the current segment with previously compressed context to generate a new compressed summary. An evaluation module determines whether the compressed context contains sufficient information to answer the question, outputting a [COMPLETE] token to terminate the process when ready. The framework is trained using supervised fine-tuning on a synthetic dataset generated by GPT-4o, with Mistral-7B as the backbone model.

## Key Results
- Achieves 7.0 F1 improvement on HotpotQA compared to existing compressors
- Outperforms long-context large language models on multi-document QA benchmarks
- Demonstrates higher compression rate (47x) while maintaining superior performance
- Shows plug-in compatibility with various retrievers (Contriever, BM25) and readers

## Why This Works (Mechanism)

### Mechanism 1: Iterative Context Refinement
The framework actively refines context by jointly analyzing previously compressed text with newly provided segments at each iteration. This allows the model to preserve essential information while discarding redundancy across multiple documents.

### Mechanism 2: Early Termination
After each compression step, the model evaluates whether the current context contains all necessary information to answer the question. If sufficient, it outputs a [COMPLETE] token and stops; otherwise, it continues processing.

### Mechanism 3: Segment-Based Processing
Retrieved documents are grouped into fixed-size segments to balance computational efficiency with completeness. This approach mitigates the quadratic computational cost growth associated with long sequences.

## Foundational Learning

- **Concept: Iterative context refinement**
  - Why needed here: Question answering over multiple documents requires synthesizing information spread across sources. Single-pass compression often loses critical links between documents.
  - Quick check question: If you compress documents in one step versus two steps (first half, then second half), which approach better preserves cross-document relationships and why?

- **Concept: Conditional generation with early stopping**
  - Why needed here: Processing all retrieved documents is computationally expensive. Early termination prevents unnecessary work while ensuring completeness.
  - Quick check question: In a compression task, what are the risks of stopping too early versus continuing too long, and how does the model's evaluation step mitigate these risks?

- **Concept: Segment-based processing for long sequences**
  - Why needed here: LLMs face quadratic computational costs with sequence length. Segmenting documents allows efficient processing while maintaining context flow.
  - Quick check question: If you have 30 documents and process them in segments of 5, how many iterations are needed, and what information must be passed between iterations to maintain coherence?

## Architecture Onboarding

- **Component map**: Retriever -> Segmenter -> Compressor -> Evaluator -> Reader
- **Critical path**: Retriever provides top-k documents → Segmenter groups into fixed-size chunks → Compressor iteratively generates compressed summaries → Evaluator determines completion condition → Reader generates final answer
- **Design tradeoffs**:
  - Segment size (j): Smaller values reduce computation but may miss cross-document relationships; larger values increase computation but improve completeness
  - Compression ratio target: Higher compression saves tokens but risks information loss; lower compression preserves more detail but increases cost
  - Evaluation strictness: Conservative evaluation ensures completeness but may extend runtime; lenient evaluation speeds processing but risks premature termination
- **Failure signatures**:
  - Incomplete answers: Often caused by early termination before gathering all necessary evidence
  - Redundant information: Indicates evaluator not identifying sufficient context
  - Performance degradation with higher k: Suggests compressor struggling with noisy contexts
  - High computational cost: May indicate segment size too small or evaluation too lenient
- **First 3 experiments**:
  1. Vary segment size (j=3,5,7) on HotpotQA and measure F1 vs computational cost to find optimal balance
  2. Compare strict vs lenient evaluation criteria on a validation set to quantify trade-off between completeness and efficiency
  3. Test with different retrievers (Contriever vs BM25) to verify plug-in compatibility and measure performance consistency

## Open Questions the Paper Calls Out

### Open Question 1
How does the active compression strategy in CompAct affect the performance of different types of questions, such as factual, causal, or comparative questions? The paper does not specify how the strategy performs across different question types, leaving open whether the active compression strategy is equally effective for all types of questions or if certain types benefit more.

### Open Question 2
What is the impact of the segment size (j) on the compression rate and end performance of CompAct? While the paper provides a default setup with j=5, it does not investigate how varying this parameter affects the model's performance or determine the optimal configuration.

### Open Question 3
How does CompAct's performance compare when using different backbone models for compression, such as larger or smaller models than Mistral-7B? The paper uses Mistral-7B due to resource limitations but does not explore how other models might affect performance or provide a comparative analysis of different models.

## Limitations

- Limited empirical validation scope confined to a single multi-hop QA benchmark (HotpotQA)
- Evaluation dependency concerns regarding the accuracy of the completion determination module
- Computational cost characterization gaps in scaling analysis across different configurations

## Confidence

- **High confidence**: Core claim of active compression with iterative refinement and early termination improving QA performance is well-supported by 7.0 F1 improvement on HotpotQA
- **Medium confidence**: Claims about plug-and-play compatibility are supported but limited to specific retriever and reader configurations
- **Low confidence**: Generalizability claims to other domains and tasks are weakly supported due to focus on single benchmark

## Next Checks

1. **Cross-dataset generalization test**: Evaluate CompAct on multiple QA benchmarks (Natural Questions, TriviaQA, MultiRC) to assess performance consistency across different document types, question complexities, and domain characteristics.

2. **Ablation study of synthetic data quality**: Replace the GPT-4o-generated synthetic dataset with human-annotated compression examples for a subset of questions, then measure performance differences to quantify synthetic data impact.

3. **Robustness analysis under retrieval noise**: Systematically inject irrelevant documents into retrieval results (0%, 25%, 50%, 75% noise levels) and measure how compression performance degrades to reveal resilience to imperfect retrieval.