---
ver: rpa2
title: 'Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers
  in Vision Language Models'
arxiv_id: '2411.04291'
source_url: https://arxiv.org/abs/2411.04291
tags:
- alignment
- image
- layers
- safety
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a safety vulnerability in vision-language
  models (VLMs) called Image enCoder Early-exiT (ICET), where early exits from intermediate
  layers of the image encoder increase harmful response generation. The authors propose
  Layer-wise Clip-PPO (L-PPO), a modified reinforcement learning from human feedback
  (RLHF) method, to address this issue.
---

# Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models

## Quick Facts
- arXiv ID: 2411.04291
- Source URL: https://arxiv.org/abs/2411.04291
- Authors: Saketh Bachu; Erfan Shayegani; Rohit Lal; Trishna Chakraborty; Arindam Dutta; Chengyu Song; Yue Dong; Nael Abu-Ghazaleh; Amit K. Roy-Chowdhury
- Reference count: 40
- Primary result: Layer-wise Clip-PPO reduces ICET attack success rate by up to 48% and toxicity score by 33.64% across three VLMs

## Executive Summary
This paper identifies a safety vulnerability in vision-language models called Image enCoder Early-exiT (ICET), where early exits from intermediate layers of the image encoder increase harmful response generation. The authors propose Layer-wise Clip-PPO (L-PPO), a modified reinforcement learning from human feedback (RLHF) method, to address this issue. Experiments across three VLMs (LLaVA-1.5, LLaVA-NeXT, Llama 3.2) show that L-PPO reduces attack success rate by up to 48% and toxicity score by 33.64% on safety datasets, while preserving VLM utility on vision tasks.

## Method Summary
The paper investigates ICET vulnerability by extracting intermediate layer embeddings from the image encoder and measuring safety degradation. L-PPO is implemented as a modified PPO algorithm that incorporates intermediate layer embeddings directly into the RL policy training process. The method uses LoRA-based fine-tuning on LLaVA-1.5 with reward models trained on HH-RLHF dataset. Safety evaluation is performed using automated metrics including Llama Guard, Perspective API, and custom reward models across multiple datasets.

## Key Results
- L-PPO reduces attack success rate by up to 48% on safety datasets
- Toxicity score decreases by 33.64% when using L-PPO
- VLM utility preserved with maintained accuracy on VQA-v2
- Effectiveness demonstrated across three VLMs: LLaVA-1.5, LLaVA-NeXT, and Llama 3.2

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intermediate layer embeddings create out-of-distribution inputs that weaken safety alignment when used with harmful text prompts.
- **Mechanism:** During training, the language backbone expects final-layer image embeddings (eL). When ICET uses earlier layer embeddings (el), the multimodal fusion (Pβ(el), eT) falls outside training distribution, leading to misalignment in safety responses.
- **Core assumption:** Safety alignment generalizes only to layer embeddings seen during training.
- **Evidence anchors:**
  - [abstract]: "skipping a certain set of layers and exiting early can increase the chance of the VLM generating harmful responses."
  - [section 3.1]: "we observe that early exit, i.e. utilizing intermediate layer embeddings, which were not included in the training process, creates an out-of-distribution (OOD) scenario where the language backbone interprets these embeddings differently and compromises safety alignment."
  - [corpus]: Weak, no direct evidence; mostly cites alignment studies, not OOD-layer-specific.
- **Break condition:** If the model is retrained on intermediate layer embeddings, this mechanism should break down.

### Mechanism 2
- **Claim:** Layer-wise Clip-PPO realigns the policy to safely handle ICET-generated embeddings by incorporating them directly into RLHF.
- **Mechanism:** L-PPO replaces the standard PPO policy input (Pβ(eL), eT) with (Pβ(el), eT) during training. The policy is then optimized to minimize harm while maintaining utility on ICET embeddings.
- **Core assumption:** The RL policy can be adapted to intermediate embeddings without harming overall performance.
- **Evidence anchors:**
  - [abstract]: "We introduce a simple modification of Clipped Proximal Policy Optimization (Clip-PPO) that effectively reduces the harmfulness of the VLM responses when early exiting from the image encoder."
  - [section 3.2]: "In L-PPO, we leverage this intermediate layer embedding el as input to the RL policy πRLϕ, aiming to systematically reduce the harmfulness of the VLM caused due to the intermediate layer l embeddings."
  - [corpus]: Weak, limited to general RLHF literature.
- **Break condition:** If the intermediate embeddings are too divergent from the final-layer embedding distribution, RLHF may fail to realign safely.

### Mechanism 3
- **Claim:** The monotone improvement theorem from RL ensures that L-PPO policy updates consistently reduce ICET vulnerability.
- **Mechanism:** By framing ICET vulnerability as expected regret between current and optimal policies, the L-PPO update rule (bounded by the KL penalty and clipping) guarantees non-decreasing expected performance.
- **Core assumption:** The optimal policy for ICET-l embeddings lies within the parameterized policy space Πϕ.
- **Evidence anchors:**
  - [section 3.3]: "The L-PPO algorithm ensures a reduction in ICET vulnerability of the VLM associated with exiting early from the l-th layer of the image encoder."
  - [corpus]: Weak, only tangentially related to layer-specific RL.
- **Break condition:** If the optimal policy for ICET-l embeddings is outside the current architecture's capacity, monotone improvement may not hold.

## Foundational Learning

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here:** L-PPO is an RLHF variant that aligns VLM behavior with human preferences on safety.
  - **Quick check question:** What is the role of the KL penalty in RLHF algorithms like PPO?

- **Concept:** Multimodal Fusion and Embedding Spaces
  - **Why needed here:** Understanding how image and text embeddings combine in VLMs is essential to grasp why ICET causes misalignment.
  - **Quick check question:** What happens to multimodal fusion when the image encoder layer changes mid-inference?

- **Concept:** Layer-wise Representations in Transformers
  - **Why needed here:** Different transformer layers capture different semantic levels; ICET exploits the mismatch between these and safety alignment.
  - **Quick check question:** How do early vs. late transformer layers differ in their learned representations?

## Architecture Onboarding

- **Component map:** Image → Eθ → El → Pβ → πRLϕ → Output
- **Critical path:** Image → Eθ → El → Pβ → πRLϕ → Output
- **Design tradeoffs:**
  - ICET vs. safety: Early exits improve efficiency but harm safety
  - L-PPO vs. SFT: L-PPO adapts to layer embeddings; SFT may overfit
  - KL penalty vs. freedom: Controls alignment drift
- **Failure signatures:**
  - Harmful outputs on safe inputs → ICET misalignment
  - Over-refusal on benign prompts → L-PPO over-alignment
  - Reward model bias → Misleading safety evaluation
- **First 3 experiments:**
  1. Test ICET-5 vs. ICET-20 on a fixed prompt/image pair; observe safety degradation in early layers
  2. Run L-PPO on ICET-5; verify ASR drop and preserved utility on VQA-v2
  3. Remove KL penalty; observe policy collapse or refusal loop

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in a dedicated section, but the discussion and limitations sections highlight several important directions:

## Limitations
- The paper lacks direct empirical evidence for the OOD mechanism - the claim that intermediate layer embeddings cause misalignment is largely theoretical with weak empirical support
- Safety evaluation relies heavily on automated metrics (Llama Guard, Perspective API) rather than comprehensive human evaluation
- The paper doesn't explore whether simply retraining on intermediate layer embeddings would achieve similar safety improvements

## Confidence
- **Medium confidence**: The core observation that ICET increases harmful responses is well-supported by the ASR and TS metrics across multiple VLMs and datasets
- **Low confidence**: The theoretical mechanism explaining why ICET causes misalignment (OOD embeddings) lacks direct empirical validation and remains largely speculative
- **Medium confidence**: The effectiveness of L-PPO in reducing ICET vulnerability is demonstrated, but the 48% ASR reduction and 33.64% TS reduction should be interpreted cautiously given the reliance on automated safety metrics

## Next Checks
1. **OOD validation**: Conduct a direct analysis comparing embedding distributions between final-layer (eL) and intermediate-layer (el) outputs using metrics like KL divergence or t-SNE visualization to empirically verify the OOD claim
2. **Human evaluation study**: Implement a comprehensive human evaluation of safety responses across ICET-0, ICET-vulnerable, and L-PPO-aligned models using diverse safety prompts to validate the automated metric findings
3. **Alternative alignment baseline**: Compare L-PPO against a simple fine-tuning baseline where the model is trained directly on intermediate layer embeddings to isolate whether the PPO mechanism or simply exposure to intermediate embeddings drives the safety improvements