---
ver: rpa2
title: Single- vs. Dual-Policy Reinforcement Learning for Dynamic Bike Rebalancing
arxiv_id: '2402.03589'
source_url: https://arxiv.org/abs/2402.03589
tags:
- rebalancing
- inventory
- routing
- demand
- station
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the dynamic bike rebalancing problem in bike-sharing
  systems by proposing two reinforcement learning (RL) approaches: Single-policy RL
  (SPRL) and Dual-policy RL (DPRL). Both models operate in a continuous-time framework,
  allowing vehicles to make independent and cooperative rebalancing decisions.'
---

# Single- vs. Dual-Policy Reinforcement Learning for Dynamic Bike Rebalancing

## Quick Facts
- arXiv ID: 2402.03589
- Source URL: https://arxiv.org/abs/2402.03589
- Reference count: 40
- Key outcome: DPRL reduces lost demand by up to 72.8% compared to best-performing MIP model

## Executive Summary
This study addresses dynamic bike rebalancing in bike-sharing systems using reinforcement learning approaches. The authors propose two frameworks: Single-policy RL (SPRL) that jointly learns inventory and routing decisions, and Dual-policy RL (DPRL) that decouples these decisions into separate networks. Both operate in a continuous-time framework enabling independent vehicle decisions. Using a high-fidelity simulator trained on synthetic trip data, DPRL significantly outperforms SPRL and multiple benchmarks, achieving substantial reductions in lost demand during morning peak periods.

## Method Summary
The method employs Deep Q-Networks (DQN) with experience replay for both SPRL and DPRL approaches. SPRL makes simultaneous inventory and routing decisions within a single network, while DPRL separates these into sequential decisions using two distinct networks. The continuous-time event-driven simulator processes rentals, returns, and rebalancing operations under first-arrive-first-serve rules. Models are trained on synthetic datasets (GT1 and GT2) with 60 stations each, using 100 days for training and 50 days for testing, evaluated on 4-hour morning peak periods with 4 rebalancing vehicles per fleet.

## Key Results
- DPRL reduces lost demand by up to 72.8% compared to best-performing MIP model
- DPRL outperforms SPRL by up to 23.8% in lost demand reduction
- DPRL achieves faster convergence and more stable learning than SPRL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPRL reduces lost demand by decoupling inventory and routing decisions, allowing each to adapt to real-time system changes independently.
- Mechanism: By separating the decision steps—first inventory, then routing—the model can respond to dynamic demand and inventory changes without waiting for global updates, unlike SPRL where both decisions are made simultaneously.
- Core assumption: Inventory rebalancing takes time and system state changes during this period; routing based on outdated state leads to suboptimal decisions.
- Evidence anchors: [abstract] "In contrast, the dual policy approach decouples node-level inventory decisions from arc-level vehicle routing, enhancing learning efficiency and adaptability."; [section] "In contrast, the dual policy approach introduces an additional binary indicator i_vk, leading to Hk = (bv_k, gv_k, pv_k, mv_k, ov_k, iv_k,∀v∈V)."

### Mechanism 2
- Claim: DPRL achieves faster convergence and more stable learning by allowing each sub-network to specialize in its domain (inventory vs routing).
- Mechanism: Training two separate DQNs allows each to focus on a smaller, more specific action space, reducing the complexity of the joint state-action space and improving learning efficiency.
- Core assumption: Specialization leads to better representation learning and faster convergence than a single, larger network handling both tasks.
- Evidence anchors: [abstract] "This decomposition enables a more granular modeling of network dynamics, significantly improving rebalancing efficiency."; [section] "The key innovation lies in the state type segmentation, which enables more targeted learning strategies for inventory rebalancing and vehicle routing."

### Mechanism 3
- Claim: The continuous-time framework allows vehicles to make independent, asynchronous decisions, reflecting realistic operational conditions and improving responsiveness.
- Mechanism: By processing events chronologically and allowing each vehicle to act upon arrival/departure without waiting for others, the system can adapt more quickly to demand fluctuations.
- Core assumption: Real-time responsiveness and independence between vehicles are critical for effective rebalancing in dynamic environments.
- Evidence anchors: [section] "Both models operate in a continuous-time framework, allowing multiple vehicles to make independent and cooperative decisions while dynamically adapting to real-world system changes."; [section] "Both SPRL and DPRL operate under a continuous-time framework, enabling vehicles to make independent decisions based on system dynamics."

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The bike rebalancing problem is modeled as a sequential decision-making task where each action (vehicle movement or rebalancing) leads to a new system state and reward (lost demand).
  - Quick check question: In an MDP, what are the four key components that define the decision-making environment?

- Concept: Deep Q-Network (DQN)
  - Why needed here: DQN is used to approximate the Q-value function for complex state-action spaces that are too large for traditional Q-learning tables, enabling the agent to learn effective rebalancing policies from experience.
  - Quick check question: Why are two separate networks (prediction and target) used in DQN, and what problem does this solve?

- Concept: Exploration vs Exploitation (ϵ-greedy strategy)
  - Why needed here: During training, the agent must balance trying new actions (exploration) to discover better strategies with using known good actions (exploitation) to maximize reward; this is crucial for learning robust rebalancing policies.
  - Quick check question: What happens to the exploration rate ϵ during training, and why is this schedule important?

## Architecture Onboarding

- Component map: State representation (station inventories, vehicle locations, capacities, demand events) -> Two DQNs (inventory policy and routing policy) -> Event-driven simulator (processes rentals, returns, rebalancing) -> Replay buffer (stores transitions) -> Training loop (samples mini-batches, updates networks)

- Critical path: 1. Environment generates state and demand events; 2. Inventory policy selects fill level action; 3. Simulator executes rebalancing operation; 4. Routing policy selects next station; 5. Simulator moves vehicle, updates state, computes reward; 6. Transition stored in replay buffer; 7. Networks trained on sampled batches

- Design tradeoffs: Single vs dual policy: Simplicity and speed vs. adaptability and granularity; Continuous vs discrete time: Realism and responsiveness vs. computational complexity; Exploration rate: Thoroughness of learning vs. speed of convergence

- Failure signatures: High TD loss persisting over training: Poor network capacity or learning rate issues; Low Q-values but high lost demand: Mismatch between learned policy and environment dynamics; Unstable training curves: Insufficient exploration or poor replay buffer management

- First 3 experiments: 1. Train SPRL vs DPRL on a small network (e.g., 10 stations) and compare convergence speed and final lost demand; 2. Vary the exploration rate schedule (e.g., linear vs exponential decay) and measure impact on robustness and final performance; 3. Test the effect of different activation functions (ReLU vs LeakyReLU vs PReLU) in the output layer on training stability and final results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SPRL and DPRL scale with larger bike-sharing networks (e.g., hundreds of stations) and increased vehicle fleets?
- Basis in paper: [inferred] The paper notes that larger systems could be handled by clustering stations, but does not provide empirical results on scaling performance.
- Why unresolved: The current experiments focus on a 60-station network with 4 vehicles. Scaling to larger systems would test the algorithms' robustness and computational efficiency.
- What evidence would resolve it: Empirical results comparing SPRL and DPRL on larger networks (e.g., 200+ stations) with varying fleet sizes, including runtime and lost demand metrics.

### Open Question 2
- Question: How would incorporating real-time traffic data or dynamic travel times affect the performance of SPRL and DPRL?
- Basis in paper: [inferred] The paper uses fixed travel times in the simulator but acknowledges that real-world conditions could vary. No experiments are conducted with dynamic travel times.
- Why unresolved: Real-world travel times are influenced by traffic, weather, and other factors. Ignoring this variability may limit the practical applicability of the models.
- What evidence would resolve it: Experiments comparing SPRL and DPRL performance with and without dynamic travel time updates based on real-time traffic data.

### Open Question 3
- Question: Can the dual-policy framework be extended to handle additional decision layers, such as vehicle charging for e-bike systems or dynamic pricing incentives?
- Basis in paper: [inferred] The paper suggests potential extensions to e-bike systems and dynamic pricing but does not explore these scenarios. The current DPRL framework focuses only on inventory and routing.
- Why unresolved: E-bike systems introduce new constraints (e.g., battery levels), and dynamic pricing could influence user demand patterns. These factors could significantly impact rebalancing strategies.
- What evidence would resolve it: Implementation and evaluation of an extended DPRL model that incorporates vehicle charging decisions or integrates with a dynamic pricing module, with performance comparisons to the baseline models.

## Limitations

- The evaluation focuses on a single 4-hour morning peak window, which may not generalize to full-day operations or other demand patterns
- The paper does not provide statistical significance tests or confidence intervals across multiple random seeds, limiting confidence in reported performance improvements
- Complete simulator implementation details remain partially unspecified, particularly regarding reward calculation mechanics and event queue management

## Confidence

- High Confidence: The core mechanism that DPRL's sequential decision-making (inventory first, then routing) reduces lost demand compared to SPRL's simultaneous decisions is well-supported by the experimental results showing consistent outperformance across both datasets.
- Medium Confidence: The claim that DPRL achieves faster convergence and more stable learning is partially supported by the reported results but lacks statistical validation across multiple training runs.
- Medium Confidence: The assertion that continuous-time framework improves responsiveness is theoretically sound but the paper does not provide ablation studies isolating the impact of continuous vs discrete time on performance.

## Next Checks

1. Run 10 independent training sessions for both SPRL and DPRL with different random seeds and compute 95% confidence intervals for lost demand reduction to determine if performance gains are statistically significant.

2. Evaluate both models across full 24-hour cycles rather than just the 4-hour morning peak to assess whether DPRL maintains its advantage throughout varying demand patterns.

3. Implement an ablation study comparing continuous-time vs discrete-time versions of DPRL to quantify the specific contribution of the continuous-time framework to performance improvements.