---
ver: rpa2
title: 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large
  Language Models'
arxiv_id: '2409.17539'
source_url: https://arxiv.org/abs/2409.17539
tags:
- logical
- reasoning
- then
- logic
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of information loss in neuro-symbolic
  methods for logical reasoning with large language models. The proposed Logic-of-Thought
  (LoT) prompting method extracts logical propositions and expressions from context,
  expands them using logical reasoning laws, and converts them back to natural language
  descriptions to augment the original prompt.
---

# Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2409.17539
- Source URL: https://arxiv.org/abs/2409.17539
- Reference count: 29
- Primary result: Logic-of-Thought prompting method improves logical reasoning accuracy across multiple datasets by extracting, extending, and translating logical expressions back to natural language

## Executive Summary
This paper addresses information loss in neuro-symbolic methods for logical reasoning with large language models by proposing Logic-of-Thought (LoT) prompting. LoT extracts logical propositions and expressions from context, expands them using logical reasoning laws through a deterministic program, and converts them back to natural language descriptions to augment the original prompt. Extensive experiments show that LoT significantly improves the performance of various prompting methods across five logical reasoning datasets, boosting Chain-of-Thought's accuracy on ReClor by +4.35% and improving Chain-of-Thought with Self-Consistency on RuleTaker by +3.52%.

## Method Summary
The Logic-of-Thought (LoT) prompting approach addresses information loss in neuro-symbolic methods through a three-phase pipeline: (1) Logic Extraction - LLMs extract propositions and logical expressions from natural language context, (2) Logic Extension - a Python program applies logical reasoning laws to expand the logical expressions, and (3) Logic Translation - LLMs convert the extended logical expressions back to natural language form. The augmented context, combining original and deduced logical descriptions, is then used with existing prompting methods. This orthogonal approach enhances logical reasoning ability while preserving full original context.

## Key Results
- LoT boosts Chain-of-Thought's accuracy on ReClor by +4.35%
- Improves Chain-of-Thought with Self-Consistency on RuleTaker by +3.52%
- Enhances Tree-of-Thoughts on ProofWriter by +8%
- Demonstrates consistent performance improvements across five logical reasoning datasets (ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-phase pipeline reduces information loss compared to pure neuro-symbolic methods.
- Mechanism: LLMs identify propositions and logical relations from natural language, apply logical reasoning laws via deterministic program to expand logical space, then convert back to natural language preserving all original context while adding deduced facts.
- Core assumption: LLMs can reliably extract logical structure from natural language and deterministic extension doesn't introduce errors.
- Evidence anchors: Abstract states LoT "extracts propositions and logical expressions from the input context, expands these logical expressions according to logical reasoning laws, and converts the deduced logical expressions back into natural language form."
- Break Condition: If extraction errors are frequent or deterministic extension generates incorrect inferences, preserved context won't compensate for added noise.

### Mechanism 2
- Claim: Orthogonal integration allows LoT to enhance any prompting method without retraining or architectural changes.
- Mechanism: Appending logically deduced augmentations to original prompt acts as plug-in layer that enriches input representation, letting base prompting method operate unchanged.
- Core assumption: Base prompting method's performance isn't degraded by additional logical text.
- Evidence anchors: Abstract notes LoT is "orthogonal to existing prompting methods and can be seamlessly integrated with them."
- Break Condition: If augmented prompt exceeds token limits or introduces contradictory information, downstream performance may degrade.

### Mechanism 3
- Claim: Preserving full original context while adding deduced facts ensures completeness without sacrificing faithfulness.
- Mechanism: Concatenating original context with newly generated logical descriptions lets model cross-reference both, reducing chance critical information is omitted.
- Core assumption: LLMs can effectively use combined context to reason correctly when both original and deduced facts are present.
- Evidence anchors: Abstract states LoT "ensures information completeness and enhances logical reasoning ability" through preserving full original contexts.
- Break Condition: If LLM's context window is too small to accommodate both parts, or added text introduces redundancy that confuses reasoning.

## Foundational Learning

- Concept: Propositional logic with connectives (¬, →, ∧) and basic laws (contraposition, double negation, transitivity).
  - Why needed here: These form symbolic backbone that LoT extracts and extends; without them, method cannot generate new logical facts.
  - Quick check question: Given p → q and ¬q, what can be concluded using contraposition? (Answer: ¬p)

- Concept: Chain-of-Thought (CoT) and self-consistency prompting.
  - Why needed here: LoT is tested as augmentation to these methods; understanding them is essential to see how LoT integrates.
  - Quick check question: What is the difference between CoT and CoT-SC? (Answer: CoT-SC runs multiple CoT paths and aggregates via majority voting.)

- Concept: Neuro-symbolic integration trade-offs.
  - Why needed here: LoT positions itself between pure LLM prompting and external solver methods; knowing pros/cons of each helps understand LoT's niche.
  - Quick check question: What is main drawback of using external theorem prover in neuro-symbolic reasoning? (Answer: Potential information loss during symbolic translation.)

## Architecture Onboarding

- Component map: Input → Logic Extraction (LLM) → Logic Extension (Python program) → Logic Translation (LLM) → Augmented Prompt → Base Prompting Method → Output
- Critical path: Extraction → Extension → Translation; any failure here propagates to degraded downstream reasoning
- Design tradeoffs:
  - Completeness vs. token limits: more logical facts increase context but risk exceeding max tokens
  - Determinism vs. flexibility: deterministic Python extension avoids LLM hallucination but cannot handle novel logical forms beyond predefined laws
  - Integration depth vs. orthogonality: deeper integration could yield better performance but would break orthogonality
- Failure signatures:
  - Extraction errors: missing or incorrect propositions, malformed expressions
  - Extension errors: incorrect application of logical laws leading to false implications
  - Translation errors: natural language descriptions that don't faithfully represent logical expressions
- First 3 experiments:
  1. Run LoT on small ReClor subset and manually inspect extracted propositions for correctness
  2. Verify Python extension module correctly implements contraposition and transitivity on known test cases
  3. Compare augmented prompt length vs. base prompt length to ensure no token overflow

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal set of connectives and logical reasoning laws for LoT to maximize performance while maintaining manageable prompt complexity?
- Basis in paper: [explicit] The paper states that "current LoT supports a limited set of connectives and logical reasoning laws" and "More connectives and logical reasoning laws in LoT means more complex prompt design in the Logic Extraction and Logic Translation phase, and increased difficulty in logical deducing in the Logic Extension phase."
- Why unresolved: The paper acknowledges this as limitation but doesn't empirically explore trade-off between adding more connectives/laws versus prompt complexity and deduction difficulty.
- What evidence would resolve it: Systematic experiments varying set of connectives and logical reasoning laws, measuring performance and computational cost across multiple reasoning tasks.

### Open Question 2
- Question: How can LoT be improved to handle information loss issue that still occurs during Logic Extraction phase due to LLM hallucination?
- Basis in paper: [explicit] The paper acknowledges that "hallucination issues inherent in LLMs can still lead to some failure in the Logic Extraction phase and need to be addressed, such as repetition of expressions, omission of logical relationships, and deviations in logical propositions and expressions."
- Why unresolved: While paper mentions this limitation, it doesn't propose specific solutions to mitigate LLM hallucinations during logical extraction.
- What evidence would resolve it: Development and testing of techniques to detect and correct LLM hallucinations during extraction phase, potentially through verification steps or ensemble methods.

### Open Question 3
- Question: How does LoT compare to neuro-symbolic methods that use external theorem provers versus symbolic solvers in terms of accuracy and information preservation?
- Basis in paper: [explicit] The paper compares LoT to SatLM and LINC, which use automated theorem provers and external solvers, and notes that these methods "inherently leads to information loss in extracting logical expressions and limits their accuracy."
- Why unresolved: The paper only provides initial comparisons and doesn't explore whether using theorem provers instead of solvers would address information loss issue or how this would affect overall performance.
- What evidence would resolve it: Head-to-head comparisons of LoT with neuro-symbolic methods using theorem provers, measuring both accuracy and information preservation metrics across diverse reasoning tasks.

## Limitations
- Method effectiveness depends on LLM's ability to accurately extract logical structure from natural language, which may degrade on domains with complex or implicit logical relationships
- Deterministic Python extension program is limited to predefined logical laws and may not capture more sophisticated reasoning patterns
- Token budget constraint poses practical limitation as extensive logical augmentation could exceed context windows for longer documents

## Confidence
- High confidence: Orthogonal integration mechanism and three-phase pipeline structure are well-supported by method description and experimental results across five datasets
- Medium confidence: Claim that LoT preserves information completeness while enhancing reasoning is supported by experimental improvements but lacks direct evidence showing original context is never compromised
- Low confidence: Assertion that deterministic extension program never introduces errors is not empirically validated, as no error analysis of logical deductions is provided

## Next Checks
1. **Error Analysis of Logical Extraction**: Manually annotate sample of extracted propositions from ReClor to quantify precision and recall of extraction phase, identifying common failure patterns
2. **Ablation on Extension Laws**: Test LoT variants with individual logical laws (contraposition, transitivity, double negation) removed to determine which contribute most to performance gains and whether any introduce errors
3. **Context Window Impact Study**: Systematically vary amount of logical augmentation added to prompts and measure performance degradation points to establish practical limits on LoT's applicability