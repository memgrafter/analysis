---
ver: rpa2
title: Semi-Supervised Risk Control via Prediction-Powered Inference
arxiv_id: '2412.11174'
source_url: https://arxiv.org/abs/2412.11174
tags:
- uni00000013
- risk
- calibration
- data
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of tuning hyperparameters for
  risk-controlling prediction sets when calibration data is limited. The key challenge
  is that with small labeled calibration sets, the tuned hyperparameter becomes noisy
  and leads to overly conservative prediction rules.
---

# Semi-Supervised Risk Control via Prediction-Powered Inference

## Quick Facts
- arXiv ID: 2412.11174
- Source URL: https://arxiv.org/abs/2412.11174
- Reference count: 40
- One-line primary result: Introduces semi-supervised calibration for risk-controlling prediction sets that leverages unlabeled data while maintaining statistical validity.

## Executive Summary
This paper addresses the challenge of tuning hyperparameters for risk-controlling prediction sets (RCPS) when calibration data is limited. The authors propose a semi-supervised calibration procedure that leverages unlabeled data to improve sample efficiency while maintaining statistical validity. Their approach builds upon the prediction-powered inference framework, carefully tailoring it to risk-controlling tasks. The method uses imputed labels from unlabeled data to tune the hyperparameter, then corrects this process using the few labeled samples while rigorously accounting for imputation inaccuracies.

## Method Summary
The method introduces a semi-supervised calibration procedure for risk-controlling prediction sets that combines labeled and unlabeled calibration data. It uses imputed labels from unlabeled data to form a prediction-powered risk estimator, which is then combined with a rectifying risk from labeled data to account for imputation inaccuracies. The procedure constructs upper confidence bounds using concentration inequalities and iterates over a hyperparameter grid to select the calibrated value. Two versions are presented: a general method for any risk function and a specialized method for binary losses that uses exact Clopper-Pearson bounds.

## Key Results
- Proves RCPS framework controls non-monotonic risk functions, not just monotonic ones as previously shown
- Introduces semi-supervised calibration applicable to any risk function, including non-monotonic risks
- Demonstrates reduced variability and conservatism of coverage rates on ImageNet few-shot classification and early time series classification tasks
- Maintains validity guarantees even with inaccurate imputations, holding in finite samples for any data distribution and predictive model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised calibration improves sample efficiency while maintaining statistical validity by leveraging imputed labels and correcting for imputation inaccuracies
- Mechanism: Uses imputed labels to form a prediction-powered risk estimator that is unbiased regardless of imputation accuracy, then constructs an upper confidence bound for risk control
- Core assumption: Labeled and unlabeled calibration data are i.i.d. with test data, and imputation model is fixed during calibration
- Evidence anchors: [abstract] "The key idea is to use the imputed labels of the unlabeled data to tune the hyper-parameter that affects the risk function and then correct the tuning process with the few labeled data, all while rigorously accounting for the inaccuracies in the imputed labels."
- Break condition: Violated if i.i.d. assumption fails or imputation model is updated during calibration

### Mechanism 2
- Claim: Prediction-powered inference provides unbiased risk estimation that reduces variance when imputed labels are accurate
- Mechanism: Combines empirical risk from unlabeled data with rectifying risk from labeled data to estimate bias from imputation
- Core assumption: Imputed labels are conditionally independent of true labels given features
- Evidence anchors: [section] "Notably, the variance of the estimator can be reduced when an accurate predictive model is available."
- Break condition: Very low imputation accuracy may increase rather than reduce variance

### Mechanism 3
- Claim: Specialized semi-supervised calibration for binary losses uses exact Clopper-Pearson bounds for tighter risk control
- Mechanism: Uses exact Clopper-Pearson upper confidence bounds for both unlabeled risk and clipped rectifying risk when loss is binary
- Core assumption: Loss function takes values in {0,1}
- Evidence anchors: [section] "In situations where the loss is binary, the random variable L(Y, Tq(X)) follows the Bernoulli distribution with success probability that is equal to the risk R(q) = E[L(Y, Tq(X))]."
- Break condition: Cannot be applied when loss is not binary

## Foundational Learning

- Concept: Risk-controlling prediction sets (RCPS)
  - Why needed here: Provides foundation for controlling error rates in prediction sets, which is the core problem being addressed
  - Quick check question: What is the key idea behind the RCPS framework and how does it control the risk?

- Concept: Prediction-powered inference (PPI)
  - Why needed here: Provides methodology for leveraging unlabeled data to improve statistical estimation while maintaining validity
  - Quick check question: How does PPI construct unbiased estimators using imputed labels from unlabeled data?

- Concept: Concentration inequalities and confidence bounds
  - Why needed here: Provides mathematical foundation for constructing valid upper confidence bounds on the risk function
  - Quick check question: What are the key properties that a valid upper confidence bound must satisfy?

## Architecture Onboarding

- Component map:
  Labeled calibration data -> Imputation model -> Unlabeled calibration data -> Risk function -> Hyperparameter grid -> Concentration inequality -> Calibration procedure

- Critical path:
  1. Obtain labeled and unlabeled calibration data
  2. Generate imputed labels for unlabeled data using fixed imputation model
  3. Compute prediction-powered risk estimator using both labeled and unlabeled data
  4. Construct upper confidence bound using concentration inequality
  5. Iterate over hyperparameter grid and select first value where UCB < risk threshold

- Design tradeoffs:
  - Labeled data size vs. calibration accuracy: More labeled data improves risk estimation but may be costly to obtain
  - Imputation accuracy vs. variance reduction: Better imputation models provide greater variance reduction but may be harder to train
  - Grid resolution vs. computational cost: Finer grids provide better calibration but increase computation time
  - Finite-sample vs. asymptotic guarantees: Finite-sample methods are more conservative but provide stronger validity guarantees

- Failure signatures:
  - Invalid coverage: Empirical risk exceeds nominal level more often than expected
  - Overly conservative calibration: Selected hyperparameter is too large, leading to unnecessarily wide prediction sets
  - High variance in calibration: Different runs produce widely varying hyperparameter values
  - Poor early stopping: Calibration procedure selects hyperparameter that doesn't achieve desired risk control

- First 3 experiments:
  1. Implement Algorithm 1 (Semi-Supervised RCPS for General Loss) using WSR upper confidence bound for finite-sample validity
  2. Compare Algorithm 2 (Semi-Supervised RCPS for Binary Loss) with general version on binary loss task to verify variance reduction
  3. Test method with varying imputation accuracy levels to verify validity maintenance even with poor imputations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can semi-supervised calibration methods be extended to handle covariate or label shift between labeled and unlabeled data?
- Basis in paper: [inferred] Authors note that their methods assume i.i.d. data and suggest reweighting mechanisms as a potential future direction
- Why unresolved: Paper focuses on i.i.d. setting without theoretical or empirical analysis of performance under distribution shift
- What evidence would resolve it: Experiments comparing performance under controlled covariate/label shift scenarios with theoretical analysis of modified concentration inequalities

### Open Question 2
- Question: What is the optimal strategy for choosing the imputation model when accuracy is unknown or potentially poor?
- Basis in paper: [inferred] Authors observe methods work best with reasonably accurate imputations but maintain validity even when they're not
- Why unresolved: Paper uses fixed pre-trained classifier without exploring how different imputation models affect performance
- What evidence would resolve it: Systematic comparison of different imputation approaches across various data distributions and noise levels

### Open Question 3
- Question: Can prediction-powered inference framework be extended to construct confidence intervals for multi-parameter risk functions beyond simple scalar risks?
- Basis in paper: [inferred] Authors mention new formulation could construct confidence intervals for population parameters but only explore single-parameter cases
- Why unresolved: Paper only demonstrates applications to single risk functions without exploring multivariate extensions
- What evidence would resolve it: Theoretical analysis of coverage guarantees for vector-valued parameters with experimental validation on multi-dimensional risk control problems

## Limitations
- Validity guarantees critically depend on i.i.d. assumption between calibration and test data
- Variance reduction benefits heavily depend on imputation accuracy, which is not quantified in terms of minimum required accuracy
- Computational complexity of iterating over hyperparameter grids is not thoroughly analyzed for high-dimensional or complex models

## Confidence
- **High confidence**: Core theoretical guarantees of risk control for both monotonic and non-monotonic risk functions, and validity of semi-supervised calibration procedure under stated assumptions
- **Medium confidence**: Practical benefits in terms of reduced variability and conservatism, as these depend on quality of imputation model which varies by application
- **Low confidence**: Scalability and computational efficiency claims for high-dimensional or complex models, as these are not empirically validated

## Next Checks
1. Systematically vary quality of imputation model and measure both validity (coverage rates) and efficiency (set sizes) to identify minimum accuracy threshold required for meaningful variance reduction
2. Design experiments where labeled and unlabeled calibration data have different distributions from test data to assess sensitivity to distribution shifts
3. Evaluate computational time and memory requirements for increasing grid resolutions and dimensionality to provide practical guidance on implementation limits