---
ver: rpa2
title: 'RecGPT: Generative Pre-training for Text-based Recommendation'
arxiv_id: '2405.12715'
source_url: https://arxiv.org/abs/2405.12715
tags:
- title
- rating
- review
- user
- amazon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RecGPT, the first domain-adapted large language
  model series for text-based recommendation, consisting of RecGPT-7B and RecGPT-7B-Instruct.
  The authors pre-train RecGPT-7B on a recommendation-specific corpus of 20.5B tokens
  and further fine-tune RecGPT-7B-Instruct for instruction following on rating prediction
  and sequential recommendation tasks.
---

# RecGPT: Generative Pre-training for Text-based Recommendation

## Quick Facts
- arXiv ID: 2405.12715
- Source URL: https://arxiv.org/abs/2405.12715
- Reference count: 14
- Primary result: RecGPT-7B-Instruct achieves new state-of-the-art performance in text-based recommendation tasks

## Executive Summary
This paper introduces RecGPT, the first domain-adapted large language model series for text-based recommendation. The authors develop RecGPT-7B through pre-training on a 20.5B token recommendation-specific corpus, then create RecGPT-7B-Instruct through fine-tuning on instruction-following prompts for rating prediction and sequential recommendation tasks. Experiments on four benchmark datasets demonstrate that RecGPT-7B-Instruct outperforms strong baselines including P5 and ChatGPT, achieving new state-of-the-art performance in both tasks. The authors release their models and datasets to facilitate future research in text-based recommendation.

## Method Summary
RecGPT-7B is initialized with MPT-7B parameters and continually pre-trained on a recommendation-specific corpus of 20.5B tokens. RecGPT-7B-Instruct is then fine-tuned on 100K+ instructional prompts and responses for rating prediction and sequential recommendation tasks. The model represents items through their textual descriptions and users through their text-based interaction histories, formatted as chronologically-ordered lists of text-based data points. Pre-training uses the LION optimizer with a global batch size of 128 and peak learning rate of 2.5e-5 for 2 epochs, while fine-tuning uses the same optimizer with a global batch size of 128 and peak learning rate of 1.0e-5 for 2 epochs.

## Key Results
- RecGPT-7B-Instruct outperforms strong baselines including P5 and ChatGPT on rating prediction and sequential recommendation tasks
- Achieves new state-of-the-art performance across four benchmark datasets
- Demonstrates effectiveness of domain-specific pre-training and instruction fine-tuning for text-based recommendation

## Why This Works (Mechanism)

### Mechanism 1
Pre-training RecGPT-7B on a large recommendation-specific corpus improves domain adaptation for text-based recommendation tasks. The model learns patterns and representations specific to recommendation through continual pre-training on 20.5B tokens from various recommendation domains. The effectiveness relies on the corpus containing sufficient diversity and volume to effectively adapt the base LLM to the recommendation domain.

### Mechanism 2
Fine-tuning RecGPT-7B on instruction-following prompts for rating prediction and sequential recommendation improves task-specific performance. The model learns to follow specific instructions and generate appropriate responses through training on 100K+ instructional prompts and responses. Success depends on the instruction-following dataset covering a wide range of scenarios and edge cases for these tasks.

### Mechanism 3
Representing items by textual descriptions and users by text-based interaction history improves the model's ability to understand user preferences. The model directly processes textual information associated with items and user preferences through chronologically-ordered lists of text-based data points. This approach assumes textual descriptions and interaction histories provide sufficient information for understanding user preferences and making accurate recommendations.

## Foundational Learning

- Concept: Pre-training and fine-tuning of large language models
  - Why needed here: These steps adapt the model to the recommendation domain and improve task-specific performance
  - Quick check question: What is the difference between pre-training and fine-tuning, and why are both steps necessary in this case?

- Concept: Representation of items and users in recommendation systems
  - Why needed here: The paper uses textual representations instead of traditional ID-based representations
  - Quick check question: How does the proposed representation differ from traditional ID-based representations, and what are the potential benefits and drawbacks?

- Concept: Evaluation metrics for recommendation tasks
  - Why needed here: RMSE, MAE, HR@k, and NDCG@k are used to evaluate model performance
  - Quick check question: What do these metrics measure, and how do they differ in their focus and interpretation?

## Architecture Onboarding

- Component map: RecGPT-7B (MPT-7B initialized, pre-trained on 20.5B tokens) -> RecGPT-7B-Instruct (fine-tuned on 100K+ instruction prompts) -> Benchmark datasets (rating prediction and sequential recommendation tasks)

- Critical path: 1) Initialize RecGPT-7B with MPT-7B parameters, 2) Continually pre-train on recommendation corpus, 3) Fine-tune on instruction-following dataset, 4) Evaluate on benchmark datasets using RMSE, MAE, HR@k, NDCG@k metrics

- Design tradeoffs: Pre-training on large corpus vs. fine-tuning on smaller task-specific dataset balances computational cost against performance gains; textual representation vs. ID-based representation trades computational resources for direct understanding of item and user information

- Failure signatures: Poor performance on benchmark tasks could indicate issues with pre-training/fine-tuning process, insufficient data, inappropriate hyperparameters, or inadequate model architecture; high computational cost could result from large model size or complex operations

- First 3 experiments: 1) Evaluate on rating prediction using RMSE and MAE, 2) Evaluate on sequential recommendation using HR@k and NDCG@k, 3) Compare performance to baselines like P5 and ChatGPT on both tasks

## Open Questions the Paper Calls Out

The paper explicitly states in the Limitations section that it only evaluates two popular recommendation tasks and plans to conduct experiments for other recommendation tasks in future work. The authors do not explore RecGPT's performance on other recommendation scenarios beyond rating prediction and sequential recommendation.

## Limitations

- Lack of detailed disclosure about recommendation-specific corpus statistics and instruction-following dataset composition
- Absence of detailed error analysis or ablation studies to identify key performance contributors
- No discussion of computational costs for pre-training and fine-tuning large 7B parameter model
- Limited evaluation to only two recommendation tasks without exploring other scenarios

## Confidence

- High confidence: Experimental results showing RecGPT-7B-Instruct outperforming baselines on tested datasets
- Medium confidence: Claims about effectiveness of pre-training and fine-tuning approaches, dependent on training data quality assumptions
- Low confidence: Claims about benefits of textual representation approach, lacking thorough validation through experiments

## Next Checks

1. Conduct ablation study to determine relative contributions of pre-training and fine-tuning to model performance
2. Analyze model performance on out-of-distribution data and edge cases to assess generalization capabilities
3. Compare computational cost and inference time of RecGPT-7B-Instruct to baseline models for practical feasibility assessment