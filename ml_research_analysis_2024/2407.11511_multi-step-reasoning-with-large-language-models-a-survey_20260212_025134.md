---
ver: rpa2
title: Multi-Step Reasoning with Large Language Models, a Survey
arxiv_id: '2407.11511'
source_url: https://arxiv.org/abs/2407.11511
tags:
- reasoning
- learning
- language
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey examines multi-step reasoning approaches for large
  language models (LLMs), focusing on prompt-based in-context learning methods. Starting
  from the Chain-of-thought paper, which showed how prompting LLMs to generate intermediate
  reasoning steps significantly improves performance on grade school math word problems,
  the field has expanded to other domains including logic, games, robotics, and code
  generation.
---

# Multi-Step Reasoning with Large Language Models, a Survey

## Quick Facts
- arXiv ID: 2407.11511
- Source URL: https://arxiv.org/abs/2407.11511
- Authors: Aske Plaat; Annie Wong; Suzan Verberne; Joost Broekens; Niki van Stein; Thomas Back
- Reference count: 40
- Primary result: Multi-step reasoning approaches for LLMs improve performance on reasoning tasks through prompt-based in-context learning

## Executive Summary
This survey examines multi-step reasoning approaches for large language models, focusing on prompt-based in-context learning methods. Starting from the Chain-of-thought paper, which showed how prompting LLMs to generate intermediate reasoning steps significantly improves performance on grade school math word problems, the field has expanded to other domains including logic, games, robotics, and code generation. The paper provides a taxonomy based on the reasoning pipeline: step generation, evaluation, and control. It discusses various approaches such as hand-written prompts, model-generated prompts, self-assessment, tool-based evaluation, and reinforcement learning for step control. Results show significant improvements over standard prompting methods, with some approaches achieving 20-50 percentage point gains on benchmarks like GSM8K. The survey also highlights limitations, including issues of faithfulness and hallucination, and provides a research agenda for future work.

## Method Summary
The survey systematically examines multi-step reasoning approaches for large language models through a comprehensive literature review. The methodology involves categorizing existing research based on the reasoning pipeline components: step generation (how intermediate reasoning steps are produced), evaluation (how reasoning quality is assessed), and control (how the reasoning process is guided or optimized). The authors analyze various prompt-based approaches including hand-written Chain-of-Thought prompts, model-generated Chain-of-Thought prompts, and self-assessment methods. They also explore evaluation strategies ranging from automatic metrics to human judgment, and control mechanisms including reinforcement learning and tool-based interventions. The survey synthesizes findings across multiple domains and benchmarks, identifying patterns in performance improvements and methodological approaches.

## Key Results
- Chain-of-Thought prompting significantly improves performance on grade school math word problems compared to standard prompting
- Multi-step reasoning approaches achieve 20-50 percentage point gains on benchmarks like GSM8K over standard prompting methods
- The field has expanded from mathematics to include logic, games, robotics, and code generation domains
- Significant concerns exist regarding faithfulness and hallucination of intermediate reasoning steps

## Why This Works (Mechanism)
Multi-step reasoning works by breaking down complex problems into intermediate steps that guide the model toward correct solutions. The mechanism relies on LLMs' ability to follow explicit reasoning patterns when prompted appropriately. By generating and following intermediate steps, models can handle compositional reasoning tasks that would be too complex for direct answer generation. The approach leverages the model's existing knowledge through in-context learning rather than requiring parameter updates. Step-by-step reasoning provides multiple opportunities for the model to correct course and refine its approach, reducing the cognitive load of solving entire problems in one step. The explicit intermediate steps also create transparency in the reasoning process, though this transparency comes with the challenge of verifying step correctness.

## Foundational Learning
- **Chain-of-Thought Prompting**: The original approach that prompts models to generate intermediate reasoning steps. Why needed: Provides the foundation for all subsequent multi-step reasoning work. Quick check: Can the model solve grade school math problems with explicit step-by-step reasoning?
- **In-Context Learning**: The ability of LLMs to learn from examples within prompts without parameter updates. Why needed: Enables multi-step reasoning approaches without fine-tuning. Quick check: Does the model improve performance when given reasoning examples in the prompt?
- **Prompt Engineering**: The design of effective prompts to elicit desired reasoning behaviors. Why needed: Critical for generating useful intermediate steps. Quick check: Do carefully crafted prompts consistently produce better reasoning than generic prompts?
- **Evaluation Metrics for Reasoning**: Methods to assess the quality of reasoning steps and final answers. Why needed: Essential for comparing different approaches and tracking progress. Quick check: Do automatic metrics correlate with human judgment of reasoning quality?
- **Faithfulness and Hallucination**: The tendency of models to generate plausible-sounding but incorrect reasoning steps. Why needed: A fundamental limitation that affects reliability. Quick check: Can the model's reasoning steps be independently verified?
- **Tool Integration**: Using external tools to verify or generate reasoning steps. Why needed: Provides a way to ground reasoning in external knowledge. Quick check: Does tool use improve the accuracy of intermediate steps?

## Architecture Onboarding

**Component Map:**
Chain-of-Thought Prompting -> Step Generation -> Step Evaluation -> Control Mechanism -> Final Answer

**Critical Path:**
The most critical path is from Step Generation through Step Evaluation to Control Mechanism. Step Generation produces the reasoning chain, Step Evaluation assesses its quality, and Control Mechanism guides improvements or corrections. Without effective evaluation, the system cannot identify when reasoning goes astray, and without control mechanisms, it cannot recover from errors.

**Design Tradeoffs:**
The primary tradeoff is between prompt complexity and model performance. Longer, more detailed prompts can guide better reasoning but increase computational cost and may exceed context limits. Another tradeoff exists between automatic evaluation (faster, scalable) and human evaluation (more accurate, expensive). The choice of control mechanism involves balancing between prompt engineering (simple but limited) and reinforcement learning (powerful but requires extensive training data).

**Failure Signatures:**
Common failure modes include: (1) hallucination where models generate plausible but incorrect intermediate steps, (2) cascading errors where one wrong step propagates through the reasoning chain, (3) getting stuck in local optima where the reasoning path leads to a dead end, (4) overreliance on pattern matching rather than true reasoning, and (5) context window limitations preventing long reasoning chains.

**Three First Experiments:**
1. Implement basic Chain-of-Thought prompting on GSM8K benchmark and compare performance to standard prompting
2. Test the effect of varying prompt detail levels on reasoning quality and answer accuracy
3. Evaluate the faithfulness of generated reasoning steps using automated verification tools

## Open Questions the Paper Calls Out
The survey identifies several open questions including: how to effectively evaluate the quality of intermediate reasoning steps beyond final answer correctness, whether multi-step reasoning approaches can be generalized across diverse domains beyond mathematics, how to address the fundamental problem of hallucination and faithfulness in generated reasoning chains, what the optimal balance is between model-generated and hand-crafted prompts, and how to scale these approaches to more complex real-world reasoning tasks.

## Limitations
- Heavy reliance on prompt-based in-context learning may not capture full LLM reasoning potential
- Significant performance variation across different domains and problem types
- Critical concerns about faithfulness and hallucination in generated reasoning steps
- Most evaluation metrics focus on final answers rather than intermediate reasoning quality

## Confidence

- **High Confidence**: Chain-of-Thought prompting significantly improves grade school math problem performance, with consistent replication across multiple studies
- **Medium Confidence**: 20-50 percentage point improvements over standard prompting are supported but vary by benchmark and implementation
- **Medium Confidence**: The reasoning pipeline taxonomy provides useful organization but some approaches don't fit cleanly into categories

## Next Checks

1. Conduct systematic evaluation comparing faithfulness of intermediate reasoning steps across different multi-step reasoning approaches, using automated verification tools and human expert assessment to measure hallucination rates and logical consistency

2. Test robustness of multi-step reasoning methods across broader domains beyond mathematics, including scientific reasoning, commonsense reasoning, and domain-specific professional tasks to assess generalizability

3. Implement and evaluate tool-based evaluation approaches that verify intermediate steps in real-time, measuring whether this improves both final answer accuracy and reliability of the reasoning process itself