---
ver: rpa2
title: Modeling Text-Label Alignment for Hierarchical Text Classification
arxiv_id: '2409.00788'
source_url: https://arxiv.org/abs/2409.00788
tags:
- text
- label
- labels
- https
- htla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of Hierarchical Text Classification
  (HTC) where text samples must be categorized into a structured label hierarchy,
  with the added complexity of aligning text semantics with dynamic sub-hierarchies
  for each sample. Existing two-encoder frameworks often process text and labels independently,
  neglecting this alignment.
---

# Modeling Text-Label Alignment for Hierarchical Text Classification

## Quick Facts
- arXiv ID: 2409.00788
- Source URL: https://arxiv.org/abs/2409.00788
- Reference count: 38
- Primary result: HTLA achieves up to 2.4% higher Macro-F1 scores by explicitly modeling text-label alignment in hierarchical text classification

## Executive Summary
This paper addresses the challenge of Hierarchical Text Classification (HTC) where documents must be categorized into structured label hierarchies. The key insight is that existing two-encoder frameworks process text and labels independently, missing crucial alignment between text semantics and their associated labels. The authors propose a Text-Label Alignment (TLA) loss using contrastive learning to pull text embeddings closer to positive labels and push them away from negative labels in the embedding space. This approach, integrated into the Hierarchical Text-Label Alignment (HTLA) model, significantly improves classification performance, especially for less prevalent labels and deeper hierarchy levels.

## Method Summary
HTLA uses a two-encoder framework with BERT for text encoding and a custom Graph Propagation Transformer (GPTrans) for label hierarchy encoding. Text and label embeddings are combined via element-wise addition and passed through a shared classifier. The model is optimized with both Binary Cross-Entropy (BCE) and the novel TLA loss. The TLA loss employs contrastive learning to dynamically align text with its positive labels while pushing it away from semantically similar but irrelevant labels. This approach is evaluated on benchmark datasets (WOS, RCV1-V2, NYT) and shows significant improvements, particularly for rare labels and deeper hierarchies.

## Key Results
- HTLA achieves up to 2.4% higher Macro-F1 scores compared to state-of-the-art methods
- Significant performance gains on less prevalent labels (P4 and P5 categories) where improvements range from 2.4% to 4.8%
- Better handling of deeper hierarchies with 1.5-2.4% improvements across different hierarchy depths
- Strong generalization demonstrated on AAPD and BGC datasets beyond the primary benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TLA loss explicitly models semantic relationships between text and labels within the sub-hierarchy
- Mechanism: Uses contrastive learning to pull text embeddings closer to positive label embeddings and push them away from negative label embeddings
- Core assumption: Alignment between text and labels is crucial for accurate classification and can be captured through embedding space manipulation
- Evidence anchors:
  - [abstract] "TLA loss pulls the text closer to its positive label and pushes it away from its negative label in the embedding space"
  - [section] "TLA loss increases alignment for positive pairs, pulling text samples and their positive labels closer"
- Break condition: If hard negative mining fails to find semantically relevant negatives, contrastive signal becomes weak

### Mechanism 2
- Claim: GPTrans allows dynamic propagation of label hierarchy information adapted to text-specific sub-hierarchies
- Mechanism: Incorporates Graph Propagation Attention (GPA) that updates node and edge features based on local and higher-order relationships
- Core assumption: Label hierarchies contain more relational information than static edges, and dynamic propagation improves label representation quality
- Evidence anchors:
  - [section] "GPTrans uses transformer blocks and outperforms state-of-the-art graph models"
  - [section] "GPA in GPTrans dynamically propagates information among nodes and edges"
- Break condition: If graph encoder is too shallow or propagation not tuned for hierarchy depth

### Mechanism 3
- Claim: Simple addition of text and label embeddings provides effective fusion while preserving hierarchy semantics
- Mechanism: Text feature htext is added to each label feature fi to produce composite Ci
- Core assumption: Simple addition is sufficient to merge contextual text representation with label semantics
- Evidence anchors:
  - [section] "We merge the text and label features by adding them together"
  - [section] "This simplifies merging text and label features, requiring only addition"
- Break condition: If text and label features are on different scales or have incompatible semantic spaces

## Foundational Learning

- Concept: Contrastive learning and NT-Xent loss
  - Why needed here: TLA loss is modeled after NT-Xent, understanding positive/negative pair formation and temperature scaling is essential
  - Quick check question: What happens to similarity score distribution when temperature τ is too high or low in contrastive learning?

- Concept: Graph neural networks and attention mechanisms
  - Why needed here: GPTrans uses Graph Propagation Attention with dynamic updates to node/edge representations
  - Quick check question: In GPA, why does node-to-edge flow use softmax values of attention scores instead of raw scores?

- Concept: Hierarchical label taxonomy and label imbalance
  - Why needed here: HTC deals with structured hierarchies and often suffers from label imbalance affecting training and evaluation
  - Quick check question: Why does Macro-F1 improve more than Micro-F1 when model gets better at classifying rare labels?

## Architecture Onboarding

- Component map: Text encoder (BERT) -> Graph encoder (GPTrans) -> Addition fusion -> Shared classifier -> BCE + TLA loss
- Critical path:
  1. Encode text → htext
  2. Encode label hierarchy → L
  3. For each label i: Ci = htext + fi
  4. Predict: ŷi = sigmoid(Wc^T Ci + b)
  5. Compute BCE + TLA loss
  6. Backpropagate and update all parameters
- Design tradeoffs:
  - Simple addition vs. attention-based fusion: Simplicity vs. potential expressiveness
  - Hard negative mining vs. random negatives: Quality of contrastive signal vs. computational cost
  - Single GPTrans layer vs. deeper: Faster training vs. richer propagation
- Failure signatures:
  - No Macro-F1 improvement but Micro-F1 improves → model biased toward frequent labels
  - TLA loss dominates training → negative mining may be too aggressive
  - Loss oscillates → learning rate or temperature τ may need tuning
  - Model overfits to training hierarchy → need more regularization or dropout
- First 3 experiments:
  1. Compare HTLA with and without TLA loss to isolate alignment effect
  2. Test different negative mining strategies (random vs. hard) on contrastive learning
  3. Vary temperature τ in TLA loss to find optimal scaling for similarity scores

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following emerge from the limitations and discussion:

### Open Question 1
- Question: How does TLA loss perform on non-textual hierarchical classification tasks like images or biological data?
- Basis in paper: [explicit] Mentions extending approach to non-textual domains as future work
- Why unresolved: Current study focuses exclusively on text classification
- What evidence would resolve it: Empirical results from applying HTLA to image classification, protein function prediction, or multi-modal datasets

### Open Question 2
- Question: What is the optimal strategy for selecting negative labels in TLA loss for extremely large hierarchies?
- Basis in paper: [explicit] Uses hard negative mining but notes challenge of selecting semantically distant negatives
- Why unresolved: Current method scales mining to positive labels, may not be optimal for very large label spaces
- What evidence would resolve it: Comparative studies testing different negative sampling strategies on datasets with thousands of labels

### Open Question 3
- Question: How does HTLA's performance compare using different text encoders beyond BERT?
- Basis in paper: [inferred] Uses BERT but doesn't explore alternatives, leaving questions about method's dependence on this architecture
- Why unresolved: Choice of text encoder could significantly impact performance, paper doesn't investigate alternatives
- What evidence would resolve it: Head-to-head comparisons using same TLA framework with different text encoders

## Limitations
- The specific implementation details of the LabelEnhancer module and hard negative mining technique are not fully specified, affecting reproducibility
- Reliance on hard negative mining quality - if mining strategy fails to identify semantically relevant negatives, contrastive learning signal may be weak
- Simple addition fusion may not generalize well if text and label embeddings are on different scales or in incompatible semantic spaces

## Confidence
- High confidence: Overall improvement in Macro-F1 scores and effectiveness of TLA loss in aligning text with positive labels
- Medium confidence: Benefits of GPTrans for dynamic label hierarchy encoding and simplicity of addition-based fusion
- Low confidence: Robustness to different negative mining strategies and generalizability of simple addition fusion across diverse datasets

## Next Checks
1. Evaluate the impact of different negative mining strategies (random vs. hard negatives) to quantify the contribution of high-quality negative samples to TLA loss effectiveness
2. Test alternative fusion mechanisms (attention-based or concatenation) to assess whether addition-based fusion is sufficient or if more expressive methods could improve performance
3. Analyze model sensitivity to temperature τ in TLA loss by conducting experiments with varying temperature values to determine optimal scaling for similarity scores and ensure stable training dynamics