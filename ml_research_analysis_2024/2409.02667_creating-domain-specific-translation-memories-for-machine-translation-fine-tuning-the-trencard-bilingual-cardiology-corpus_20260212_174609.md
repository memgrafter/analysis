---
ver: rpa2
title: 'Creating Domain-Specific Translation Memories for Machine Translation Fine-tuning:
  The TRENCARD Bilingual Cardiology Corpus'
arxiv_id: '2409.02667'
source_url: https://arxiv.org/abs/2409.02667
tags:
- translation
- files
- corpus
- file
- turkish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a semi-automatic methodology for translators
  to create domain-specific translation memories (TMs) using familiar CAT tools. The
  approach enables compilation of parallel corpora from web sources without requiring
  advanced programming skills.
---

# Creating Domain-Specific Translation Memories for Machine Translation Fine-tuning: The TRENCARD Bilingual Cardiology Corpus

## Quick Facts
- arXiv ID: 2409.02667
- Source URL: https://arxiv.org/abs/2409.02667
- Reference count: 0
- Primary result: Demonstrates semi-automatic methodology for translators to create domain-specific TMs without programming skills

## Executive Summary
This study presents a semi-automatic methodology enabling translators to create domain-specific translation memories (TMs) using familiar CAT tools. The approach allows compilation of parallel corpora from web sources without requiring advanced programming skills. The methodology was applied to create the TRENCARD Corpus, a Turkish-English cardiology TM containing 788,046 source words and 49,693 sentences from four Turkish cardiology journals. The work demonstrates that translators can build substantial TMs for machine translation fine-tuning within reasonable timeframes, empowering them to customize NMT engines with domain-specific data rather than relying solely on generic systems.

## Method Summary
The methodology combines web crawling with CAT tool processing to create domain-specific translation memories. It uses wget for downloading website content, CAT tools (memoQ, OmegaT) for file parsing and alignment, and cleaning tools (Goldpan, Heartsome) for quality control. The process involves crawling journal websites, filtering and parsing HTML/XML files to extract bilingual content, aligning source and target sentences, compiling into TMX format, and cleaning to remove tags and duplicates. The approach is semi-automatic, balancing automation with manual quality control steps that translators can perform using familiar tools.

## Key Results
- Created TRENCARD Corpus with 788,046 source words and 49,693 sentences
- Demonstrated methodology works with familiar CAT tools without programming skills
- Showed corpus creation possible within reasonable timeframe ("only four days")
- Proved translators can customize NMT engines with domain-specific data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translators can build usable domain-specific translation memories (TMs) without programming skills by using familiar CAT tools and semi-automatic workflows.
- Mechanism: The methodology replaces fully automatic web crawling with a guided pipeline that leverages existing CAT tool features (filtering, alignment, cleaning) and simple command-line tools (wget, regex), lowering the skill barrier while maintaining data quality.
- Core assumption: Translators can reliably perform corpus preparation tasks like file filtering and alignment when provided with structured, step-by-step guidance.
- Evidence anchors:
  - [abstract] "enables compilation of parallel corpora from web sources without requiring advanced programming skills"
  - [section 3.1] "While the procedure for TM data compilation presented here is not as advanced as the methodologies using fully automatic web crawlers, it nevertheless allows translators to create TMs with a small amount of domain-specific, high quality data"
- Break condition: If translators cannot reliably execute regex parsing or alignment tasks, or if available tools lack sufficient filtering/alignment capabilities for the target language pair.

### Mechanism 2
- Claim: Fine-tuning neural machine translation (NMT) engines with even small domain-specific TMs can improve translation quality for specialized domains.
- Mechanism: Domain adaptation via fine-tuning allows the NMT model to learn terminology and stylistic patterns from the custom TM, compensating for the lack of large parallel corpora in low-resource language pairs.
- Core assumption: NMT fine-tuning is effective with corpora as small as tens of thousands of sentences, especially in specialized domains.
- Evidence anchors:
  - [abstract] "enabling them to customize NMT engines with domain-specific data rather than relying solely on generic systems"
  - [section 3] "Nieminen (2021:290) points out, domain fine-tuning strategies have been implemented at least since Koehn & Schroeder (2007) and numerous studies have shown different levels of quality gains"
- Break condition: If the domain-specific corpus is too small relative to the complexity of the domain, or if the pre-trained NMT model lacks sufficient representational capacity for adaptation.

### Mechanism 3
- Claim: A semi-automatic pipeline balances data quality control with reasonable time investment for translators.
- Mechanism: Manual curation steps (file filtering, alignment verification, cleaning) ensure high-quality data, while automation (wget, regex filtering, batch alignment) keeps processing time manageable.
- Core assumption: The time saved by automation outweighs the manual effort required for quality control, making the approach feasible for individual translators.
- Evidence anchors:
  - [abstract] "Using this methodology, translators can build their custom TMs in a reasonable time"
  - [section 4] The TRENCARD corpus creation process is detailed with specific tools and timeframes, showing a 788k-word corpus built in "only four days"
- Break condition: If manual steps become too time-consuming relative to the corpus size needed, or if automated tools fail to handle the specific file structures encountered.

## Foundational Learning

### Regular expressions (regex) for text pattern matching
- Why needed here: Used to extract relevant content (e.g., abstracts) from HTML files by defining patterns between tags
- Quick check question: Given an HTML snippet with `<div class="abstract">Text here</div>`, what regex pattern would extract "Text here"?

### Sentence alignment principles
- Why needed here: Critical for pairing source and target sentences accurately to create parallel corpora
- Quick check question: What factors (e.g., punctuation, formatting, word count) do alignment algorithms typically use to match sentences?

### Translation Memory eXchange (TMX) format
- Why needed here: Standard format for storing and exchanging bilingual translation data, compatible with most CAT tools and MT platforms
- Quick check question: What are the key elements of a TMX file structure (e.g., tu, tuv, seg tags)?

## Architecture Onboarding

### Component map
Web crawling (wget) -> File filtering (CAT tools) -> Parsing (CAT tools) -> Aligning (CAT tools/standalone tools) -> Compiling (CAT tools) -> Cleaning (TM maintenance tools) -> File conversion (Okapi Rainbow)

### Critical path
Crawling → File Filtering → Parsing → Aligning → Compiling → Cleaning → File Conversion

### Design tradeoffs
- Semi-automatic vs. fully automatic: Quality control vs. scalability
- Tool familiarity vs. advanced capabilities: Translator-friendly tools vs. more powerful but complex alternatives
- Corpus size vs. processing time: Larger corpora improve MT quality but require more time and resources

### Failure signatures
- Misaligned sentences in final TMX (indicates parsing or alignment issues)
- Noisy characters or tags in output (indicates encoding or cleaning problems)
- Inconsistent file naming preventing alignment (indicates crawling or preprocessing issues)

### First 3 experiments
1. Crawl a small bilingual website and use wget to download content, verifying file structure and naming
2. Use a CAT tool (e.g., memoQ) to create a custom filter for extracting content from one HTML file, testing regex patterns
3. Align a pair of sample files using the CAT tool's alignment feature, checking for and correcting misalignments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size threshold for a domain-specific translation memory to achieve significant improvements in neural machine translation fine-tuning, particularly for low-resource language pairs like Turkish-English?
- Basis in paper: [explicit] The paper mentions studies suggesting as little as 10,000 sentences may be sufficient, but acknowledges uncertainty about the exact threshold needed for meaningful quality improvements in specific domains and language pairs.
- Why unresolved: Different studies report varying thresholds (from 10,000 to 500,000 sentences), and the effectiveness depends on multiple factors including domain specificity, language pair, and baseline model quality.
- What evidence would resolve it: Systematic empirical studies testing MT fine-tuning quality improvements across varying TM sizes (10K, 50K, 100K, 500K sentences) for specific language pairs and domains, with standardized evaluation metrics.

### Open Question 2
- Question: How do different web crawling methodologies (semi-automatic vs fully automatic) impact the quality and usability of domain-specific translation memories for machine translation fine-tuning?
- Basis in paper: [explicit] The paper contrasts its semi-automatic approach with fully automatic web crawlers, noting trade-offs between corpus size and quality control, but doesn't empirically compare outcomes.
- Why unresolved: The paper presents a methodology but doesn't provide comparative analysis of results achievable through different crawling approaches, particularly regarding alignment accuracy and data noise.
- What evidence would resolve it: Direct comparison studies using identical domains and language pairs, measuring corpus quality metrics, alignment accuracy, and subsequent MT fine-tuning performance across different crawling methodologies.

### Open Question 3
- Question: What are the ethical and legal frameworks that should govern the creation and use of translation memories compiled from web sources, particularly for commercial applications?
- Basis in paper: [explicit] The paper discusses copyright concerns and data permissions but doesn't propose specific governance frameworks for ethical TM creation and use.
- Why unresolved: While the paper acknowledges the importance of data permissions, it doesn't address the broader ethical implications or propose standardized approaches for ensuring responsible data collection and use.
- What evidence would resolve it: Development and validation of comprehensive ethical guidelines for TM creation, including standardized consent mechanisms, attribution requirements, and frameworks for handling data ownership and commercial use rights.

## Limitations
- No quantitative quality metrics for the final TM
- Lack of translation quality improvement evaluation
- Time estimate not substantiated with detailed breakdown
- Methodology applicability to other domains and language pairs untested

## Confidence
- High confidence: Translators can create TMs using familiar CAT tools and semi-automatic workflows (Mechanism 1)
- Medium confidence: Fine-tuning NMT engines with small domain-specific TMs improves translation quality (Mechanism 2)
- Medium confidence: Semi-automatic pipeline balances data quality with reasonable time investment (Mechanism 3)

## Next Checks
1. Measure translation quality improvement: Fine-tune an NMT model with the TRENCARD corpus and evaluate against a baseline using standard metrics (e.g., BLEU, TER) on a held-out test set of cardiology texts.
2. Time and effort quantification: Track and document the actual hours spent on each step of the TM creation process to validate the "four days" claim and identify bottlenecks.
3. Cross-domain applicability test: Apply the methodology to create TMs for a different specialized domain (e.g., legal or technical documents) and compare the ease of use, time investment, and final corpus quality.