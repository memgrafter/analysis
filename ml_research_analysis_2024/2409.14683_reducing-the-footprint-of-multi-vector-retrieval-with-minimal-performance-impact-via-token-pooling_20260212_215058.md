---
ver: rpa2
title: Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact
  via Token Pooling
arxiv_id: '2409.14683'
source_url: https://arxiv.org/abs/2409.14683
tags:
- pooling
- retrieval
- performance
- pool
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simple clustering-based token pooling approach
  to reduce the number of vectors stored in multi-vector retrieval methods like ColBERT,
  addressing the high storage and memory costs of these models. The method groups
  similar token vectors using clustering (K-Means or hierarchical clustering) and
  applies mean pooling to create a single representative vector per cluster, with
  a controllable compression factor (pooling factor).
---

# Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling

## Quick Facts
- arXiv ID: 2409.14683
- Source URL: https://arxiv.org/abs/2409.14683
- Reference count: 39
- Primary result: 50% storage reduction with virtually no performance loss using clustering-based token pooling

## Executive Summary
This paper introduces a simple clustering-based token pooling approach to reduce the number of vectors stored in multi-vector retrieval methods like ColBERT, addressing the high storage and memory costs of these models. The method groups similar token vectors using clustering (K-Means or hierarchical clustering) and applies mean pooling to create a single representative vector per cluster, with a controllable compression factor (pooling factor). Applied at indexing time without requiring model or pipeline modifications, the approach achieves significant storage reductions: 50% with virtually no performance loss on average, and up to 66% reduction with less than 3% degradation across most datasets. The method works well with both unquantized and 2-bit quantized vectors and is validated on English and Japanese datasets. It can be combined with existing ColBERT quantization pipelines for even greater compression.

## Method Summary
The paper proposes token pooling as a simple clustering-based method to reduce the storage footprint of multi-vector retrieval systems like ColBERT. At indexing time, token vectors for each document are grouped using clustering algorithms (K-Means or hierarchical clustering with Ward's method), then mean pooling is applied to create a single representative vector per cluster. The pooling factor determines the compression ratio (e.g., factor of 2 reduces storage by 50%). This approach works without modifying the ColBERT model or retrieval pipeline, and can be combined with 2-bit quantization for additional compression. The method is evaluated on English and Japanese datasets using both HNSW and PLAID indexing with 2-bit quantization.

## Key Results
- 50% storage reduction with virtually no retrieval performance degradation using pooling factor 2
- Up to 66% vector reduction with less than 3% performance loss on most datasets
- Hierarchical clustering outperforms K-Means on average, though K-Means performs better on scidocs dataset
- Combined pooling and 2-bit quantization achieves 66%-75% vector reduction with <5% degradation on most datasets
- Works effectively for both English and Japanese languages with JaColBERTv2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token Pooling reduces storage by averaging similar token vectors into clusters, preserving semantic meaning while decreasing total vector count.
- Mechanism: Vectors are grouped using clustering (K-Means or hierarchical) based on cosine similarity. Mean pooling replaces each cluster with a single representative vector, reducing storage by the pooling factor (e.g., factor of 2 reduces storage by 50%).
- Core assumption: Similar tokens within a document carry redundant semantic information, so averaging them doesn't significantly alter document representation.
- Evidence anchors:
  - [abstract] "This method can reduce the space & memory footprint of ColBERT indexes by 50% with virtually no retrieval performance degradation."
  - [section 2] "The core approach is simple and works as a two-step system: First, we devise a way to group individual vectors together... We then apply mean pooling in order to obtain a single vector which contains an average representation of the cluster."
- Break condition: If token vectors within clusters are not sufficiently similar, pooling will lose critical semantic distinctions, leading to performance degradation.

### Mechanism 2
- Claim: Hierarchical clustering performs better than sequential or K-Means pooling because it minimizes Ward distance between original vectors and pooled outputs.
- Mechanism: Hierarchical clustering iteratively merges vectors that minimize the Ward distance metric, ensuring that pooled vectors stay as close as possible to their constituent original vectors in embedding space.
- Core assumption: Ward's method, which minimizes within-cluster variance, is well-suited for preserving semantic integrity during pooling.
- Evidence anchors:
  - [section 2] "We use Ward's method [31] to produce our clusters, which intuitively would be well-suited for this task, as it would seek to minimize the distance between the original vector and the pooled outputs."
  - [section 4.2] "Interestingly, we note that similarly to the 16-bit setting, KMeans clustering outperforms hierarchical clustering on the scidocs dataset at a pool factor 2, and observe a similar behaviour. However, the performance of hierarchical clustering still noticeably outperforms it on average at every pool factor."
- Break condition: If the document contains many semantically distinct tokens, hierarchical clustering may force dissimilar tokens into clusters, harming retrieval accuracy.

### Mechanism 3
- Claim: Combining Token Pooling with 2-bit quantization yields greater compression without proportionally increasing performance loss.
- Mechanism: Pooling reduces vector count before quantization, so fewer vectors need to be compressed. Since quantization error is per-vector, reducing the number of vectors amplifies overall compression gains.
- Core assumption: Quantization error introduced by 2-bit compression is uniform across vectors, so reducing vector count magnifies the space saved without compounding performance degradation.
- Evidence anchors:
  - [abstract] "This method also allows for further reductions, reducing the vector count by 66%-to-75%, with degradation remaining below 5% on a vast majority of datasets."
  - [section 4.2] "We observe overall similar results in the quantized settings as with unquantized vectors. Performance degradation appears to be slightly more pronounced but still very contained..."
- Break condition: If quantization noise is non-uniform or if important semantic features are concentrated in a small subset of tokens, aggressive pooling+quantization may cause disproportionate performance loss.

## Foundational Learning

- Concept: Cosine similarity and its use in vector space clustering
  - Why needed here: Token Pooling groups vectors based on cosine distance; understanding this metric is essential to grasp how clusters form.
  - Quick check question: If two vectors have cosine similarity of 0.9, are they more or less similar than vectors with cosine similarity of 0.6?

- Concept: K-Means and hierarchical clustering algorithms
  - Why needed here: Different pooling methods rely on different clustering strategies; knowing their behavior helps predict performance outcomes.
  - Quick check question: In K-Means, does the number of clusters need to be specified beforehand, and how does that relate to the pooling factor?

- Concept: Vector quantization and its impact on retrieval quality
  - Why needed here: The paper combines pooling with 2-bit quantization; understanding quantization error propagation is key to interpreting results.
  - Quick check question: If a 768-dimensional vector is compressed to 2 bits per dimension, how many bits total are used per vector, and how might that affect semantic precision?

## Architecture Onboarding

- Component map: ColBERT encoder -> Token Pooling layer (clustering + mean pooling) -> PLAID/HNSW indexing module -> Query processing pipeline (MaxSim scoring) -> Evaluation harness (NDCG@10, Recall@5, etc.)

- Critical path:
  1. Index-time: Document tokenization → ColBERT encoding → Token Pooling (clustering + mean pooling) → Index storage (PLAID/HNSW)
  2. Query-time: Query encoding → Candidate retrieval → MaxSim scoring over pooled vectors → Ranking

- Design tradeoffs:
  - Storage vs. accuracy: Higher pooling factors reduce storage but increase performance loss.
  - Clustering method choice: Hierarchical clustering generally outperforms K-Means but may be slower.
  - Quantization compatibility: Pooling can be combined with 2-bit quantization for multiplicative gains.

- Failure signatures:
  - Excessive performance degradation with high pooling factors.
  - Unexpected dataset-specific anomalies (e.g., Touché dataset improvement).
  - Indexing errors if document length assumptions (e.g., 256 tokens) are violated.

- First 3 experiments:
  1. Run hierarchical clustering with pooling factor 2 on a small dataset (e.g., scifact) using 16-bit vectors and HNSW indexing; compare retrieval metrics to baseline.
  2. Repeat experiment 1 with K-Means clustering; observe performance difference.
  3. Combine pooling factor 2 with 2-bit quantization on the same dataset; measure combined compression and performance impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the token pooling approach perform when applied to much larger document collections (e.g., billions of documents) and what are the specific scaling challenges?
- Basis in paper: [inferred] The paper notes that "the approach can lead to ColBERT being applied to a broader range of uses" but does not explore large-scale scenarios beyond the 500K document cutoff for BEIR/LoTTe datasets.
- Why unresolved: The experiments were limited to small-to-mid sized datasets, leaving the behavior at scale untested.
- What evidence would resolve it: Empirical results on datasets with billions of documents, including indexing and query latency measurements, and analysis of memory usage patterns at scale.

### Open Question 2
- Question: What is the theoretical basis for why certain clustering methods (hierarchical vs. k-means) perform differently across datasets, and can this inform automatic method selection?
- Basis in paper: [explicit] The paper observes that "KMeans clustering outperforms hierarchical clustering on the scidocs dataset at a pool factor 2" but "the performance of hierarchical clustering still noticeably outperforms it on average at every pool factor."
- Why unresolved: The authors do not provide a theoretical explanation for these performance differences or a method to predict which clustering approach would work best for a given dataset.
- What evidence would resolve it: Analysis of dataset characteristics (vocabulary diversity, document length distribution, topic coherence) that correlate with clustering method effectiveness, leading to a predictive model for method selection.

### Open Question 3
- Question: How does token pooling affect retrieval performance for queries that rely heavily on specific token combinations or rare terms?
- Basis in paper: [inferred] The authors note that "fiqa is a highly specialized dataset within the financial domain, and its queries tend to focus on very fine-grained details," and observe that "performance degrades noticeably faster as the pool factor increases" on this dataset.
- Why unresolved: The paper does not systematically analyze how pooling affects queries requiring precise token matching versus those relying on broader semantic similarity.
- What evidence would resolve it: Controlled experiments varying query specificity, token rarity, and term dependency, measuring performance degradation patterns across different query types.

### Open Question 4
- Question: Can token pooling be effectively combined with other compression techniques beyond 2-bit quantization, such as product quantization or learned dimensionality reduction?
- Basis in paper: [explicit] The authors state that "this approach can be coupled with the usual ColBERT quantization pipeline described above, achieving even greater compression results," but only demonstrate with 2-bit quantization.
- Why unresolved: The paper does not explore combinations with other established compression methods that could potentially yield further storage reductions.
- What evidence would resolve it: Experiments combining token pooling with various quantization schemes, compression matrices, or learned projections, measuring the multiplicative effects on storage reduction and any compounding impact on retrieval performance.

## Limitations

- Dataset Generalization: Results are validated on English and Japanese datasets but not extensively tested on low-resource languages or highly specialized domains.
- Clustering Algorithm Sensitivity: Only K-Means and hierarchical clustering are explored; alternative methods and distance metrics are not tested.
- Quantization Interaction Nuance: The interaction between pooling-induced information loss and quantization noise is not deeply analyzed, particularly for datasets with highly variable token importance.

## Confidence

**High Confidence**: Storage reduction claims (50% at pooling factor 2, up to 66% with minimal degradation) are well-supported by experimental results across multiple datasets and indexing configurations.

**Medium Confidence**: Hierarchical clustering superiority over K-Means is supported but may be dataset-specific; the claim is robust but not universal.

**Low Confidence**: The assertion that token pooling preserves semantic meaning without significant loss is plausible but not rigorously proven; the redundancy assumption is intuitive but lacks direct empirical validation.

## Next Checks

1. Apply token pooling to a low-resource language dataset (e.g., Swahili or Hindi) and compare performance degradation to English/Japanese results to assess cross-lingual robustness.

2. Implement DBSCAN and Gaussian Mixture Model-based pooling and compare retrieval performance and clustering stability against K-Means and hierarchical clustering on a subset of BEIR datasets.

3. Conduct an ablation study where tokens are weighted by importance (e.g., using attention scores or IDF) before pooling, and measure how performance varies with different importance-aware pooling strategies.