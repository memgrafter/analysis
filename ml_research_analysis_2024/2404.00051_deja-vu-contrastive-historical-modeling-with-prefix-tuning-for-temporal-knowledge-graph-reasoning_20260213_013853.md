---
ver: rpa2
title: 'Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge
  Graph Reasoning'
arxiv_id: '2404.00051'
source_url: https://arxiv.org/abs/2404.00051
tags:
- chapter
- knowledge
- temporal
- reasoning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChapTER, a contrastive historical modeling
  framework with prefix-tuning for temporal knowledge graph reasoning. ChapTER addresses
  the challenge of balancing textual knowledge and temporal information in temporal
  knowledge graphs by employing a pseudo-Siamese network that feeds history-contextualized
  text into two-tower model encoders and performs contrastive estimation between them.
---

# Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning

## Quick Facts
- arXiv ID: 2404.00051
- Source URL: https://arxiv.org/abs/2404.00051
- Authors: Miao Peng; Ben Liu; Wenjie Xu; Zihao Jiang; Jiahui Zhu; Min Peng
- Reference count: 40
- Key outcome: ChapTER achieves superior performance on temporal knowledge graph reasoning with only 0.17% tuned parameters

## Executive Summary
This paper introduces ChapTER, a contrastive historical modeling framework with prefix-tuning for temporal knowledge graph reasoning (TKGR). ChapTER addresses the challenge of balancing textual knowledge and temporal information in temporal knowledge graphs by employing a pseudo-Siamese network that feeds history-contextualized text into two-tower model encoders and performs contrastive estimation between them. The framework uses virtual time prefix tokens and a prefix-based tuning method to enable frozen pre-trained language models to perform temporal knowledge graph reasoning tasks under different settings. Experiments on four transductive and three few-shot inductive temporal knowledge graph reasoning benchmarks demonstrate that ChapTER achieves superior performance compared to competitive baselines with only 0.17% tuned parameters.

## Method Summary
ChapTER is a contrastive learning framework that uses prefix-tuning to enable frozen pre-trained language models for temporal knowledge graph reasoning. The framework introduces virtual time prefix tokens into each Transformer layer and applies a two-tower prefix-based tuning method. It feeds history-contextualized text into pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates. The model uses InfoNCE loss with in-batch, pre-batch, and self-negatives for training, achieving superior performance with only 0.17% tuned parameters compared to fully trained models.

## Key Results
- Achieves superior performance on temporal knowledge graph reasoning with only 0.17% tuned parameters
- Outperforms competitive baselines on four transductive datasets (ICEWS14, ICEWS18, ICEWS05-15, ICEWS14*) and three few-shot inductive datasets (ICEWS14-OOG, ICEWS18-OOG, ICEWS0515-OOG)
- Demonstrates effectiveness, flexibility, and efficiency through thorough analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive estimation between history-contextualized text in two-tower encoders strikes a balance between temporal information and textual knowledge.
- Mechanism: ChapTER feeds history-contextualized text into two-tower model encoders individually to learn history-contextualized embeddings in a decoupled manner, then performs contrastive estimation between them.
- Core assumption: History-contextualized text contains sufficient temporal information to balance textual knowledge when contrasted with candidate entities.
- Evidence anchors:
  - [abstract]: "feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates"
  - [section]: "ChapTER feeds history-contextualized text into the two-tower model encoders individually to learn history-contextualized embeddings in a decoupled manner. Then contrastive estimation is performed between them to strike a balance of temporal information and textual knowledge in representations."
  - [corpus]: No direct evidence in corpus; this mechanism is specific to ChapTER's approach.
- Break condition: If historical contexts do not contain sufficient temporal information, the contrastive estimation cannot effectively balance temporal and textual information.

### Mechanism 2
- Claim: Virtual time prefix tokens enable frozen PLMs to perform temporal knowledge graph reasoning tasks under different settings.
- Mechanism: ChapTER introduces virtual time prefix tokens to each Transformer layer within PLM, applying a prefix-based tuning method to enable frozen PLMs capable for TKGR tasks under both transductive and few-shot inductive settings.
- Core assumption: Virtual time prefix tokens can inject temporal information into PLM encoding without extensive parameter tuning.
- Evidence anchors:
  - [abstract]: "By introducing virtual time prefix tokens, it applies a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks under different settings"
  - [section]: "Rather than training an entire model, ChapTER applies a two-tower prefix-based tuning method to enable frozen PLMs capable of performing TKG reasoning tasks under both transductive and few-shot inductive settings."
  - [corpus]: No direct evidence in corpus; this mechanism is specific to ChapTER's approach.
- Break condition: If virtual time prefix tokens cannot effectively capture temporal information, the frozen PLM cannot perform TKGR tasks adequately.

### Mechanism 3
- Claim: Prefix-tuning method achieves comparable performance with minimal parameters tuned compared to fully trained models.
- Mechanism: ChapTER tunes only 0.17% of parameters using prefix-tuning, achieving superior performance compared to competitive baselines.
- Core assumption: Prefix-tuning can capture task-specific characteristics with minimal parameter updates.
- Evidence anchors:
  - [abstract]: "ChapTER achieves superior performance compared to competitive baselines with only 0.17% tuned parameters"
  - [section]: "Rather than training an entire model, ChapTER applies a two-tower prefix-based tuning method to enable frozen PLMs capable of performing TKG reasoning tasks under both transductive and few-shot inductive settings."
  - [corpus]: No direct evidence in corpus; this mechanism is specific to ChapTER's approach.
- Break condition: If prefix-tuning cannot capture task-specific characteristics, ChapTER cannot achieve comparable performance with minimal parameter tuning.

## Foundational Learning

- Concept: Temporal Knowledge Graph Reasoning (TKGR)
  - Why needed here: ChapTER is designed specifically for TKGR tasks, understanding the problem domain is crucial for implementation.
  - Quick check question: What is the difference between transductive and few-shot inductive settings in TKGR?

- Concept: Contrastive Learning
  - Why needed here: ChapTER uses contrastive estimation between query and candidate embeddings to balance temporal and textual information.
  - Quick check question: How does contrastive learning help in balancing temporal and textual information in TKGR?

- Concept: Prefix-tuning
  - Why needed here: ChapTER employs prefix-tuning to enable frozen PLMs for TKGR tasks with minimal parameter updates.
  - Quick check question: What are the advantages of prefix-tuning over full model fine-tuning in the context of TKGR?

## Architecture Onboarding

- Component map:
  - Query Encoder (Mq) -> Virtual Time Prefix Tokens -> History-contextualized text -> Contrastive Loss
  - Candidate Encoder (Mk) -> Virtual Time Prefix Tokens -> Candidate entity text -> Contrastive Loss
  - PLM (BERT/RoBERTa) -> Frozen base model with 0.17% parameters tuned

- Critical path:
  1. Verbalize input queries and candidates with historical contexts
  2. Feed verbalized text into two-tower encoders with virtual time prefix tokens
  3. Obtain history-contextualized embeddings through prefix-based tuning
  4. Perform contrastive estimation between query and candidate embeddings
  5. Compute final prediction using cosine similarity

- Design tradeoffs:
  - Tradeoff between model efficiency and performance: ChapTER achieves superior performance with only 0.17% tuned parameters
  - Tradeoff between historical context richness and input length: Rich historical contexts improve performance but are limited by PLM input length

- Failure signatures:
  - Poor performance on datasets with sparse historical events
  - Degradation in performance when entity descriptions are of low quality
  - Ineffectiveness on very small datasets with limited quadruples

- First 3 experiments:
  1. Compare ChapTER performance with and without historical contexts on ICEWS14 dataset
  2. Evaluate the impact of different prefix lengths on ChapTER's performance and parameter efficiency
  3. Test ChapTER's few-shot inductive capabilities on ICEWS14-OOG dataset with varying shot settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of prefix-tuning method (e.g., MLP reparameterization vs. P-tuning v2) impact ChapTER's performance on TKG reasoning tasks, particularly in terms of balancing temporal and textual information?
- Basis in paper: [explicit] The paper compares ChapTER using different prefix tuning methods (MLP reparameterization encoder vs. P-tuning v2) and observes performance differences.
- Why unresolved: The paper only provides results for a limited set of hyperparameters and datasets, and does not explore the full range of potential tuning methods or their impact on the model's ability to balance temporal and textual information.
- What evidence would resolve it: A comprehensive study comparing various prefix-tuning methods (e.g., different reparameterization techniques, prompt initialization strategies) on a wide range of TKG datasets and tasks, with a focus on their ability to balance temporal and textual information.

### Open Question 2
- Question: How does the length of the input sequence (e.g., entity descriptions, historical contexts) affect ChapTER's performance on TKG reasoning tasks, and what is the optimal length for balancing information content and computational efficiency?
- Basis in paper: [inferred] The paper mentions that historical events text are truncated due to the limitation of input length in PLMs, and that entity descriptions are truncated up to 50 tokens. However, it does not explore the impact of different sequence lengths on model performance.
- Why unresolved: The paper does not provide a systematic analysis of the trade-off between input sequence length, information content, and computational efficiency, and how this affects ChapTER's ability to model temporal and textual information.
- What evidence would resolve it: A study investigating the impact of input sequence length on ChapTER's performance across different TKG datasets and tasks, with a focus on identifying the optimal length for balancing information content and computational efficiency.

### Open Question 3
- Question: How does ChapTER's performance on TKG reasoning tasks compare to other state-of-the-art methods when using different PLM architectures (e.g., Longformer, GPT, etc.) and pre-training strategies (e.g., domain-specific pre-training, multi-task learning)?
- Basis in paper: [explicit] The paper compares ChapTER's performance using different PLM models (e.g., BERT-base, BERT-large, RoBERTa-base) and observes similar performance across models. However, it does not explore the impact of different PLM architectures or pre-training strategies on ChapTER's performance.
- Why unresolved: The paper only evaluates ChapTER using a limited set of PLM models and does not explore the potential benefits of using different PLM architectures or pre-training strategies for TKG reasoning tasks.
- What evidence would resolve it: A comprehensive study comparing ChapTER's performance using various PLM architectures and pre-training strategies on a wide range of TKG datasets and tasks, with a focus on identifying the most effective combination for TKG reasoning.

## Limitations

- The exact formulation of how historical contexts are verbalized and incorporated into the input remains unspecified, which could significantly impact performance
- The effectiveness of virtual time prefix tokens in capturing complex temporal dependencies for TKGR needs further validation
- The contrastive learning mechanism's specific benefits for temporal reasoning versus traditional text-based reasoning are not fully demonstrated

## Confidence

**High Confidence**:
- The overall architecture and training methodology (prefix-tuning with contrastive learning) is well-defined and reproducible
- Performance claims on the tested datasets appear consistent with the reported results
- The parameter efficiency claim (0.17% tuned parameters) is directly verifiable from the reported implementation details

**Medium Confidence**:
- The effectiveness of virtual time prefix tokens in enabling frozen PLMs for TKGR tasks
- The superiority of ChapTER's approach over existing methods in capturing temporal dynamics
- The generalizability of results to datasets with different characteristics (e.g., varying historical event density)

**Low Confidence**:
- The specific verbalization approach for historical contexts and its impact on performance
- The optimal configuration for negative sampling (in-batch, pre-batch, and self-negatives)
- The robustness of ChapTER's performance on extremely small or sparse datasets

## Next Checks

1. **Historical Context Sensitivity Analysis**: Conduct ablation studies on the ICEWS14 dataset by systematically varying the amount and quality of historical contexts provided to ChapTER. Compare performance with different historical context windows (e.g., 1-month, 6-month, 1-year) and evaluate how performance changes when historical contexts are removed entirely.

2. **Prefix Length and Layer Tuning Study**: Perform a systematic analysis of prefix length (number of prefix tokens) and the number of Transformer layers being tuned. Measure the trade-off between parameter efficiency and performance across different configurations, and identify the optimal balance for both transductive and few-shot inductive settings.

3. **Cross-Dataset Robustness Test**: Evaluate ChapTER's performance on a dataset with significantly different characteristics from ICEWS (e.g., a dataset with fewer but more temporally dense events). Compare against baselines to determine if ChapTER's advantages are consistent across different temporal knowledge graph distributions, particularly focusing on its ability to handle sparse versus dense temporal information.