---
ver: rpa2
title: 'Secrets of RLHF in Large Language Models Part II: Reward Modeling'
arxiv_id: '2401.06080'
source_url: https://arxiv.org/abs/2401.06080
tags:
- steps
- preference
- reward
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This report addresses two main challenges in reward modeling for
  RLHF: incorrect/ambiguous preference data and poor generalization to out-of-distribution
  examples. The authors propose a preference strength metric based on multi-model
  voting to identify problematic data, then use label flipping and label smoothing
  to mitigate noise.'
---

# Secrets of RLHF in Large Language Models Part II: Reward Modeling

## Quick Facts
- arXiv ID: 2401.06080
- Source URL: https://arxiv.org/abs/2401.06080
- Authors: Binghai Wang; Rui Zheng; Lu Chen; Yan Liu; Shihan Dou; Caishuang Huang; Wei Shen; Senjie Jin; Enyu Zhou; Chenyu Shi; Songyang Gao; Nuo Xu; Yuhao Zhou; Xiaoran Fan; Zhiheng Xi; Jun Zhao; Xiao Wang; Tao Ji; Hang Yan; Lixing Shen; Zhan Chen; Tao Gui; Qi Zhang; Xipeng Qiu; Xuanjing Huang; Zuxuan Wu; Yu-Gang Jiang
- Reference count: 40
- Primary result: Proposed methods significantly improve model performance, with better stability and higher win rates in evaluations against SFT and baseline models

## Executive Summary
This report addresses two main challenges in reward modeling for RLHF: incorrect/ambiguous preference data and poor generalization to out-of-distribution examples. The authors propose a preference strength metric based on multi-model voting to identify problematic data, then use label flipping and label smoothing to mitigate noise. They also introduce adaptive margins in the loss function based on preference strength. For generalization, they employ contrastive learning (SimCSE) and meta-learning (MetaRM) to improve the model's ability to distinguish between responses and align with shifted distributions.

## Method Summary
The authors propose a comprehensive approach to improve reward modeling for RLHF. First, they introduce a preference strength measurement metric using ensemble voting from multiple reward models to identify noisy or ambiguous preference pairs. Based on this metric, they apply label flipping for pairs with negative preference strength and label smoothing for ambiguous pairs. They also add adaptive margins to the loss function based on preference strength. For better generalization, they employ contrastive learning (SimCSE) to improve response discrimination and meta-learning (MetaRM) to adapt to distribution shifts during RLHF iterations. The methods are evaluated on human preference data from HH-RLHF and summarization datasets, showing significant improvements over baseline approaches.

## Key Results
- Preference strength metric effectively identifies noisy/ambiguous preference pairs
- Label flipping and smoothing improve reward model performance on corrected data
- Adaptive margins, contrastive learning, and meta-learning enhance generalization to out-of-distribution examples
- Proposed methods achieve better stability and higher win rates against SFT and baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference strength metric identifies noisy or ambiguous preference pairs by using multi-model voting.
- Mechanism: Train multiple reward models with different data orderings, compute mean and std of preference differences; low mean or high std indicates noisy/ambiguous pairs.
- Core assumption: Disagreement among models trained on the same data signals problematic preference labels.
- Evidence anchors:
  - [abstract]: "We propose a preference strength measurement metric based on a multi-reward model voting approach."
  - [section 2.2]: "Using the ensemble of reward scores from these M reward models, we can calculate the mean and standard deviation (std) of preference strength for each comparison pair."
  - [corpus]: Weak; no direct corpus evidence cited.
- Break condition: If the multi-model disagreement is due to model instability or data ordering effects unrelated to label quality.

### Mechanism 2
- Claim: Label flipping and label smoothing correct for incorrect/ambiguous preferences and improve generalization.
- Mechanism: Flip labels of pairs with negative preference strength; apply label smoothing to pairs near zero strength.
- Core assumption: Data with negative preference strength are mislabeled, and near-zero strength data are ambiguous.
- Evidence anchors:
  - [section 2.4.1]: "By flipping the labels of these preference pairs, the model could more effectively learn preference information for modeling."
  - [section 2.4.1]: "Label smoothing is another widely known technique to mitigate the overfitting problem by penalizing overconfident model outputs."
  - [corpus]: No direct corpus evidence.
- Break condition: If flipping or smoothing degrades performance on clean data or if noisy labels are too prevalent to recover from.

### Mechanism 3
- Claim: Adaptive margins and contrastive learning improve reward model generalization to out-of-distribution data.
- Mechanism: Add margin to loss based on preference strength; use SimCSE/SwA V contrastive objectives to sharpen chosen/rejected response separation.
- Core assumption: Larger margins for strong preferences help the model learn robust boundaries; contrastive learning improves feature discrimination.
- Evidence anchors:
  - [section 2.4.2]: "Adding an adaptive margin component to the loss of the reward model: L(rψ) = −E[log σ(rψ(x, yc) − rψ(x, yr)) − ˆµ(x, y)]."
  - [section 3.1]: "By introducing unsupervised contrastive loss during the reward modeling process, the reward model can better distinguish subtle preference differences among responses."
  - [corpus]: Weak; not cited.
- Break condition: If margins are too large and cause gradient instability, or contrastive objectives distract from preference modeling.

## Foundational Learning

- Concept: Bradley-Terry model for preference ranking
  - Why needed here: Forms the probabilistic foundation for reward modeling loss.
  - Quick check question: How does the Bradley-Terry model translate pairwise preferences into a probability distribution?

- Concept: Contrastive learning objectives (SimCSE, SwAV)
  - Why needed here: Used to improve reward model's ability to discriminate chosen vs rejected responses.
  - Quick check question: What is the difference between SimCSE and SwAV in how they create positive/negative pairs?

- Concept: Meta-learning for distribution shift adaptation
  - Why needed here: MetaRM adapts reward model to shifted policy distributions during RLHF iterations.
  - Quick check question: How does MetaRM use a meta-objective to encourage reward model generalization?

## Architecture Onboarding

- Component map:
  - Input: User prompt x, two model responses (y_c, y_r)
  - Encoder: Reward model r_ψ (initialized from SFT)
  - Scoring: Compute r_ψ(x, y_c) and r_ψ(x, y_r)
  - Loss: Preference loss (with optional margin/label smoothing)
  - Contrastive branch (optional): SimCSE/SwAV loss on augmented embeddings
  - Meta-learning loop (optional): Compute J_θ on shifted samples, perform meta-update
  - Output: Scalar reward for each response

- Critical path:
  - For basic reward modeling: Input → Encoder → Scoring → Loss → Gradient step
  - For contrastive: Add embedding augmentation → Contrastive loss → Total loss
  - For MetaRM: Generate shifted samples → Compute J_θ → Meta-update → Vanilla loss update

- Design tradeoffs:
  - Multiple reward models vs single model for preference strength: More robust but computationally heavier
  - Label flipping vs smoothing: Flipping fixes clear errors; smoothing handles ambiguous cases
  - Contrastive learning: Improves generalization but may require careful tuning of β
  - MetaRM: Good for distribution shift but adds training complexity

- Failure signatures:
  - Training loss collapses to zero but validation accuracy stalls → overfitting to noise
  - KL divergence spikes during PPO → reward model poorly aligned with policy distribution
  - Contrastive loss dominates total loss → reward model neglecting preference signal
  - Meta-loss gradient instability → meta-learning hyperparameters too aggressive

- First 3 experiments:
  1. Train baseline reward model with Bradley-Terry loss; evaluate on validation set.
  2. Add adaptive margin to loss; compare validation accuracy and training stability.
  3. Add SimCSE contrastive objective; measure improvement in chosen/rejected response separation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the preference strength metric based on multi-model voting compare to other methods of identifying noisy preference data?
- Basis in paper: Explicit
- Why unresolved: The paper only presents their proposed metric without comparing it to alternative approaches for detecting noisy preference data.
- What evidence would resolve it: Comparative experiments showing the effectiveness of the multi-model voting metric versus other methods like entropy-based approaches or model uncertainty measures.

### Open Question 2
- Question: What is the optimal balance between data cleaning (flipping/smoother labels) and model architecture improvements for handling noisy preference data?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on data cleaning methods but doesn't explore the interplay between data quality improvements and architectural enhancements.
- What evidence would resolve it: Systematic ablation studies comparing different combinations of data cleaning techniques and model architecture modifications.

### Open Question 3
- Question: How do different contrastive learning methods (SwAV vs SimCSE) perform across various types of preference data distributions?
- Basis in paper: Explicit
- Why unresolved: The paper only tests a limited set of contrastive learning approaches on specific datasets without exploring their generalization properties.
- What evidence would resolve it: Extensive experiments testing multiple contrastive learning methods across diverse preference data distributions and task domains.

### Open Question 4
- Question: What are the theoretical limits of MetaRM's ability to align with shifted distributions, and how do these limits depend on the initial preference data quality?
- Basis in paper: Inferred
- Why unresolved: The paper presents empirical results of MetaRM but doesn't provide theoretical analysis of its convergence properties or limitations.
- What evidence would resolve it: Mathematical analysis of MetaRM's convergence properties combined with empirical studies showing performance degradation as preference data quality decreases.

### Open Question 5
- Question: How does the proposed RLHF approach for translation and code generation compare to specialized domain-specific methods?
- Basis in paper: Explicit
- Why unresolved: The paper presents initial results for translation and code generation but doesn't compare against state-of-the-art specialized methods in these domains.
- What evidence would resolve it: Head-to-head comparisons between the RLHF approach and specialized translation/code generation models across multiple benchmark datasets.

## Limitations
- The preference strength metric's effectiveness depends heavily on the assumption that multi-model disagreement reliably indicates label noise, but this relationship is not empirically validated in the paper
- While contrastive learning and meta-learning show promise, their relative contributions to improved generalization are not clearly isolated
- The evaluation methodology relies on GPT-4 judgments for validation, but the correlation between GPT-4 and human preferences may not be perfect

## Confidence
- **High Confidence**: The mathematical formulation of preference strength measurement and its basic implementation using ensemble voting
- **Medium Confidence**: The effectiveness of label flipping and smoothing in improving reward model performance
- **Low Confidence**: The generalization benefits claimed for contrastive learning and meta-learning approaches

## Next Checks
1. **Ablation Study**: Systematically remove each proposed technique (label flipping, smoothing, adaptive margins, contrastive learning, meta-learning) to quantify their individual contributions to performance improvements
2. **Human Evaluation Correlation**: Measure the correlation between GPT-4 judgments used for validation and human preferences on a held-out test set to assess evaluation reliability
3. **Distribution Shift Robustness**: Test the reward models on intentionally shifted distributions (e.g., different domains or prompt types) to validate generalization claims beyond the standard validation set