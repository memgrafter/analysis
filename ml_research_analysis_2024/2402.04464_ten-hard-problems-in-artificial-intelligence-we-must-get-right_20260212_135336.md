---
ver: rpa2
title: Ten Hard Problems in Artificial Intelligence We Must Get Right
arxiv_id: '2402.04464'
source_url: https://arxiv.org/abs/2402.04464
tags:
- arxiv
- https
- visited
- learning
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews ten "hard problems" in AI identified by the
  AI2050 program, aiming to bridge various AI-related fields. The problems span AI
  capabilities (general intelligence, robustness, alignment), applications (scientific
  discovery, healthcare, climate), economics (job displacement, inequality), access
  (demographic diversity, global inequality), responsibility (ethics, bias, privacy),
  geopolitics (military conflicts, international relations), governance (regulation,
  institutions), and philosophy (human meaning, automation).
---

# Ten Hard Problems in Artificial Intelligence We Must Get Right

## Quick Facts
- arXiv ID: 2402.04464
- Source URL: https://arxiv.org/abs/2402.04464
- Reference count: 40
- Primary result: A comprehensive survey of ten "hard problems" in AI identified by the AI2050 program, covering capabilities, robustness, alignment, applications, economics, access, responsibility, geopolitics, governance, and philosophy.

## Executive Summary
This paper reviews ten "hard problems" in AI identified by the AI2050 program, aiming to bridge various AI-related fields. The problems span AI capabilities (general intelligence, robustness, alignment), applications (scientific discovery, healthcare, climate), economics (job displacement, inequality), access (demographic diversity, global inequality), responsibility (ethics, bias, privacy), geopolitics (military conflicts, international relations), governance (regulation, institutions), and philosophy (human meaning, automation). For each problem, the paper outlines the area, identifies significant recent work (2017-2022), and suggests future research directions. It argues that these problems, while complex, are not "wicked" in the strict sense and can be addressed with suitably precise formulations. The paper emphasizes the need to start working on these problems now to ensure beneficial AI outcomes by 2050.

## Method Summary
The paper employs a literature review methodology, systematically searching and analyzing relevant papers, articles, and reports to identify significant recent work (2017-2022) for each of the ten hard problems. The authors gather and synthesize findings from 40 references, categorizing them under the relevant hard problem areas and suggesting future research directions. The review covers publications up to January 2023, focusing on comprehensiveness over depth to provide a broad overview of the AI landscape.

## Key Results
- The paper identifies ten hard problems in AI, spanning capabilities, applications, economics, access, responsibility, geopolitics, governance, and philosophy.
- It argues that these problems, while complex, are not "wicked" in the strict sense and can be addressed with suitably precise formulations.
- The paper emphasizes the need to start working on these problems now to ensure beneficial AI outcomes by 2050, given the rapid progress in AI capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper uses a structured survey of the AI2050 "hard problems" to provide a comprehensive roadmap for AI research and policy.
- Mechanism: The paper systematically reviews ten hard problems, outlining each area, identifying significant recent work (2017-2022), and suggesting future research directions. This structure bridges various AI-related fields and provides a clear framework for addressing complex challenges.
- Core assumption: The ten hard problems identified by AI2050 are the most critical issues blocking beneficial AI outcomes by 2050.
- Evidence anchors:
  - [abstract] "For each problem, the paper outlines the area, identifies significant recent work (2017-2022), and suggests future research directions."
  - [section] "Here we present each of the AI2050 Hard Problems. For each, we present the results of our literature search, discuss what makes the problem hard, and examine prospects for future work in the area."
- Break Condition: If the AI2050 program's identification of hard problems is flawed or incomplete, the paper's roadmap may be ineffective.

### Mechanism 2
- Claim: The paper argues that the hard problems, while complex, are not "wicked" in the strict sense and can be addressed with suitably precise formulations.
- Mechanism: By distinguishing between aspects of the problems based on facts and values, the paper suggests that suitable solutions can be found. Problems based on facts should have near-optimal solutions given agreement on a loss function, while values are inherently more complex.
- Core assumption: Precise formulations of the hard problems can be achieved, allowing for testable solutions.
- Evidence anchors:
  - [section] "The Hard Problems that directly invoke human values (e.g., HP#3, HP#6–9) are better candidates for wicked problems: they lack stopping rules and the purported solutions are not testable."
  - [section] "By arguing that the Hard Problems are not necessarily wicked, we hold open the possibility that some or all of the problems might actually be solved—or at least, that we might be closer to solving them in 2050 than today."
- Break Condition: If the hard problems cannot be formulated precisely enough to distinguish between facts and values, the paper's argument that they are not wicked problems fails.

### Mechanism 3
- Claim: The paper emphasizes the need to start working on the hard problems now to ensure beneficial AI outcomes by 2050.
- Mechanism: By highlighting the rapid progress in AI capabilities and the potential for negative consequences if problems are not addressed, the paper motivates immediate action. The paper also identifies the most intensely researched areas (HP#1 and HP#4) and suggests that other problems require more attention.
- Core assumption: The problems identified are urgent and require immediate attention to prevent negative outcomes.
- Evidence anchors:
  - [section] "Between 2012 and 2022, the publication rate in technical AI doubled, reaching 240,000 papers per year [index2024], more than the entire field of physics [517]."
  - [section] "The broad vision of the AI2050 program is that AI researchers, funding agencies, and society at large can make decisions now and in the following years that will result in widespread beneficial impact."
- Break Condition: If the problems are not as urgent as the paper suggests, or if other priorities are more pressing, the paper's call for immediate action may be misguided.

## Foundational Learning

- Concept: Deep Learning
  - Why needed here: The paper is built on the assumption that deep learning is the dominant paradigm in AI, and understanding its limitations and potential is crucial for addressing the hard problems.
  - Quick check question: What are the key factors that led to the success of deep learning in the 2010s, and what are its current limitations?

- Concept: Wicked Problems
  - Why needed here: The paper distinguishes between the hard problems and "wicked problems," arguing that the former can be solved with precise formulations. Understanding the characteristics of wicked problems is essential for this argument.
  - Quick check question: What are the defining characteristics of wicked problems, and how do they differ from the hard problems identified in the paper?

- Concept: AI Ethics and Governance
  - Why needed here: Several of the hard problems (HP#3, HP#6-9) deal with ethical and governance issues related to AI. Understanding the current state of AI ethics and governance is crucial for addressing these problems.
  - Quick check question: What are the main ethical concerns related to AI, and what are the current approaches to AI governance?

## Architecture Onboarding

- Component map: Introduction -> Background -> Ten Hard Problems (HP#1-HP#10) -> Conclusion
- Critical path: The critical path for understanding the paper is to first read the introduction and background to get an overview of the AI2050 program and the context of the hard problems. Then, read each section on the hard problems to understand the specific challenges and potential solutions.
- Design tradeoffs: The paper prioritizes comprehensiveness over depth, covering a wide range of topics but not going into great detail on any one problem. This tradeoff allows the paper to provide a broad overview of the AI landscape but may leave some readers wanting more information on specific issues.
- Failure signatures: The paper may fail to provide sufficient detail on some of the hard problems, making it difficult for readers to fully understand the challenges and potential solutions. Additionally, the paper's focus on the AI2050 program may limit its applicability to other contexts.
- First 3 experiments:
  1. Read the introduction and background sections to understand the AI2050 program and the context of the hard problems.
  2. Choose one of the hard problems (e.g., HP#1 or HP#3) and read the corresponding section to understand the specific challenges and potential solutions.
  3. Read the conclusion to get a sense of the paper's overall message and recommendations for future work.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we create a scalable method for proving safety properties of large neural networks?
- Basis in paper: [explicit] The paper discusses formal verification of neural networks as a potential solution for HP#2, but notes that current approaches are limited to small models and cannot scale to the large networks used in practice.
- Why unresolved: Formal verification of large neural networks is computationally intractable with current methods. Scaling these methods to networks with millions of parameters remains a significant challenge.
- What evidence would resolve it: Development of a formal verification method that can efficiently prove safety properties for neural networks with billions of parameters, with a tractable computational cost.

### Open Question 2
- Question: Will deep learning alone be sufficient to achieve artificial general intelligence (AGI)?
- Basis in paper: [explicit] The paper discusses six key disagreements about the limits of deep learning, including its ability to form abstract representations, generalize, create causal models, plan, and learn efficiently from few examples.
- Why unresolved: While deep learning has achieved remarkable success in narrow domains, it is unclear whether it can be extended to achieve the broad capabilities and understanding of human intelligence. The paper presents arguments both for and against the sufficiency of deep learning for AGI.
- What evidence would resolve it: Demonstration of a deep learning system that exhibits human-level or superhuman performance across a wide range of tasks, including abstract reasoning, causal understanding, and long-term planning, or a formal proof that such capabilities are fundamentally beyond the reach of deep learning architectures.

### Open Question 3
- Question: How can we ensure that AI systems remain aligned with human values as they become more powerful?
- Basis in paper: [explicit] The paper discusses the challenge of aligning AI systems with human goals (HP#3), including the problems of specification gaming, emergent goals, and deceptive alignment.
- Why unresolved: As AI systems become more capable, they may develop instrumental goals that are misaligned with human values, even if they are initially trained to optimize for human-specified objectives. The paper highlights the difficulty of specifying robust objectives and detecting misaligned behavior.
- What evidence would resolve it: Development of a scalable method for aligning AI systems with human values that can handle increasingly complex and powerful systems, with rigorous testing demonstrating its effectiveness in preventing misaligned behavior.

## Limitations

- The literature review covers only publications through January 2023, missing significant developments in AI that have occurred since then.
- The selection of problems is based on the AI2050 program's framework, which may not capture all critical AI challenges.
- The paper's treatment of each problem is necessarily brief due to space constraints, potentially oversimplifying complex issues.

## Confidence

Medium: The identification of ten hard problems is well-founded, as it is based on the AI2050 program's extensive deliberations. The argument that these problems are not "wicked" in the strict sense is reasonable but may be debated, particularly for problems involving human values. The emphasis on starting work now is sound, given the rapid pace of AI development, but the urgency may be overstated for some problems.

## Next Checks

1. Verify the currency of the literature review by updating it with papers published after January 2023, focusing on key developments in each hard problem area.
2. Assess the completeness of the problem selection by comparing the AI2050 hard problems with other frameworks for identifying critical AI challenges, such as the Asilomar AI Principles or the EU AI Act's risk categories.
3. Evaluate the validity of the "not wicked" argument by conducting a more detailed analysis of each hard problem, distinguishing between factual and value-based components and assessing their tractability.