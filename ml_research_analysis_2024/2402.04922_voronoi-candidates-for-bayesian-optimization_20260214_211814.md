---
ver: rpa2
title: Voronoi Candidates for Bayesian Optimization
arxiv_id: '2402.04922'
source_url: https://arxiv.org/abs/2402.04922
tags:
- oronoi
- points
- function
- boundary
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational overhead in Bayesian optimization
  (BO) caused by optimizing acquisition functions, particularly in high dimensions.
  The authors propose using Voronoi candidates (Vorcands), which are points on the
  boundaries of Voronoi tessellations of current design points.
---

# Voronoi Candidates for Bayesian Optimization

## Quick Facts
- arXiv ID: 2402.04922
- Source URL: https://arxiv.org/abs/2402.04922
- Reference count: 32
- Primary result: Voronoi candidates (Vorcands) significantly reduce Bayesian optimization execution time by discretizing acquisition function search to Voronoi boundaries while maintaining accuracy.

## Executive Summary
This paper addresses the computational bottleneck in Bayesian optimization caused by optimizing acquisition functions, particularly in high-dimensional spaces. The authors propose using Voronoi candidates (Vorcands) - points on the boundaries of Voronoi tessellations of current design points - to discretize the search space instead of expensive continuous optimization. By developing efficient methods to sample these boundaries without explicitly computing the entire tessellation, the approach achieves order-of-magnitude speedups on test problems ranging from 10 to 60 dimensions while maintaining comparable accuracy to multi-start continuous searches.

## Method Summary
The method generates Voronoi candidates through two approaches: Voronoi Walk and Voronoi Projection. Instead of explicit tessellation, it uses nearest-neighbor search to find points equidistant from two or more existing design points in ℓ∞ metric space with axis-aligned search directions. These candidates are evaluated using Gaussian processes with expected improvement acquisition, and the best candidate is selected for the next evaluation. The approach discretizes the acquisition optimization problem, converting it from a sequential optimization to a parallelizable discrete search over space-filling candidates.

## Key Results
- Vorcands achieve order-of-magnitude speedup compared to multi-start continuous search
- On 10-60 dimensional test problems, Vorcands maintain comparable or better best observed function values
- Performance is particularly strong on ecological test problems with high-dimensional marginal log-likelihoods
- ℓ∞ metric with axis-aligned search directions mitigates boundary issues in high dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Voronoi candidates improve execution time by reducing computational burden of acquisition function optimization
- Mechanism: Traditional BO requires expensive continuous optimization of acquisition functions. Vorcands discretize this search to points equidistant from existing design points, converting sequential optimization to parallelizable discrete search
- Core assumption: Global optimum of acquisition function lies between existing design points
- Evidence anchors: [abstract] "acquisition criteria demand their own challenging inner-optimization... search discretely over a finite set of space-filling candidates"; [section] "abandon continuous optimization in favor of a finite candidate search"
- Break condition: If acquisition function's optimum consistently lies far from Voronoi boundaries, Vorcands would miss promising regions

### Mechanism 2
- Claim: Vorcands avoid mean-concentration phenomenon that plagues other high-dimensional candidate methods
- Mechanism: Unlike space-filling designs requiring exponential growth with dimension, or triangulation methods collapsing toward mean, Voronoi boundaries naturally distribute candidates between existing points without concentration effects
- Core assumption: Voronoi tessellation naturally partitions high-dimensional space maintaining candidate diversity
- Evidence anchors: [section] "In high dimension, many Voronoi walks end up on the boundary... ℓ∞ metric and axis-aligned search directions mitigated this phenomenon"; [section] "Tricands... In P = 10d... candidates collapse toward the middle"
- Break condition: If Voronoi tessellation becomes too fragmented in high dimensions, boundary sampling might become inefficient

### Mechanism 3
- Claim: Vorcands maintain comparable accuracy to continuous optimization while achieving order-of-magnitude speedup
- Mechanism: By sampling boundary points through efficient nearest-neighbor calculations rather than explicit tessellation, Vorcands find EI optima close to those found by multi-start continuous search
- Core assumption: Points on Voronoi boundaries are sufficiently close to local EI optima that discrete sampling captures global optimum
- Evidence anchors: [abstract] "our proposed approach significantly improves the execution time... without a loss in accuracy"; [section] "significantly improves the execution time"
- Break condition: If EI optima consistently fall far from Voronoi boundaries, Vorcands would sacrifice accuracy for speed

## Foundational Learning

- Concept: Voronoi tessellation and boundary properties
  - Why needed here: Vorcands rely on understanding how Voronoi cells partition space and where their boundaries lie
  - Quick check question: What defines a point on the boundary of a Voronoi cell in ℓp metric space?

- Concept: Expected Improvement acquisition function
  - Why needed here: Vorcands are tested specifically with EI, so understanding its properties is crucial
  - Quick check question: How does EI balance exploitation (improving current best) and exploration (reducing uncertainty)?

- Concept: High-dimensional geometry and distance metrics
  - Why needed here: Vorcands performance depends critically on choice of metric (ℓ1, ℓ2, ℓ∞) and how distances behave in high dimensions
  - Quick check question: How does diameter of unit hypercube change with dimension under different ℓp norms?

## Architecture Onboarding

- Component map: Vorcands system consists of (1) Voronoi boundary sampling algorithm, (2) nearest-neighbor search infrastructure, (3) candidate evaluation pipeline, and (4) integration with GP surrogate model
- Critical path: For each BO iteration: sample Voronoi boundary points → evaluate acquisition function on candidates → select best candidate → update GP model
- Design tradeoffs: Vorcands trades exact continuous optimization for discrete sampling; uses ℓ∞ norm for boundary behavior vs other metrics; alternates between walk and projection methods for robustness
- Failure signatures: (1) Excessive candidates landing on hypercube boundaries, (2) Slow nearest-neighbor queries in very high dimensions, (3) EI optima consistently far from Voronoi boundaries
- First 3 experiments:
  1. Implement basic Voronoi walk with ℓ∞ metric on 2D Ackley function, compare to random LHS candidates
  2. Test boundary sampling frequency in 10D using different metrics, measure proportion on hypercube boundaries
  3. Compare Vorcands vs multi-start BFGS on 10D Levy function, measure best value vs execution time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal metric (ℓ1, ℓ2, or ℓ∞) for defining Voronoi cells in high-dimensional Bayesian optimization, and does this depend on problem structure?
- Basis in paper: [explicit] Authors discuss different metrics and note ℓ∞ performs better in high dimensions but acknowledge this requires further investigation
- Why unresolved: Paper shows ℓ∞ prevents boundary issues but doesn't prove it's optimal across all problem types
- What evidence would resolve it: Systematic comparison across diverse problem classes with varying dimensionalities and characteristics

### Open Question 2
- Question: How can Voronoi candidates be effectively adapted for batch acquisition in Bayesian optimization?
- Basis in paper: [explicit] Authors explicitly state "one problem domain to which Vorcands does not straightforwardly apply is that of batch acquisition"
- Why unresolved: Equidistant property conflicts with need to place multiple points in same region for batch methods
- What evidence would resolve it: Developing and testing specific strategies for batch acquisition using Voronoi candidates

### Open Question 3
- Question: What is the optimal balance between computational efficiency and exploration-exploitation tradeoff when using Voronoi candidates in high dimensions?
- Basis in paper: [inferred] Paper shows Voronoi candidates achieve good performance with speedups but doesn't explore theoretical underpinnings
- Why unresolved: While empirical results show Voronoi candidates work well, there's no theoretical framework explaining relationship between candidate generation strategy, dimension, and optimization performance
- What evidence would resolve it: Theoretical analysis connecting number of candidates, sampling strategy, and acquisition function properties to convergence rates

## Limitations
- Relies on assumption that acquisition function optima lie near Voronoi boundaries without formal proof
- ℓ∞ metric choice lacks theoretical justification for why it outperforms other norms in high dimensions
- Scalability to extremely high dimensions (>100) remains untested, with nearest-neighbor complexity potentially prohibitive

## Confidence
- **High Confidence**: Claims about execution time improvements and order-of-magnitude speedup are well-supported by experimental results
- **Medium Confidence**: Claims about avoiding mean-concentration phenomenon are empirically validated but lack theoretical guarantees
- **Low Confidence**: Claims about Vorcands being universally effective across all acquisition functions and problem types are not fully substantiated

## Next Checks
1. **Theoretical Analysis**: Derive conditions under which Voronoi boundaries contain local optima of expected improvement, providing mathematical bounds on distance between boundary points and true optima
2. **High-Dimensional Scaling**: Test Vorcands on problems with 100+ dimensions to characterize scaling behavior of nearest-neighbor search complexity and boundary sampling efficiency
3. **Acquisition Function Generalization**: Evaluate Vorcands with alternative acquisition functions (e.g., UCB, Thompson sampling) to determine if Voronoi boundary property holds across different acquisition criteria