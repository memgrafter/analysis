---
ver: rpa2
title: 'On the Implications of Verbose LLM Outputs: A Case Study in Translation Evaluation'
arxiv_id: '2410.00863'
source_url: https://arxiv.org/abs/2410.00863
tags:
- outputs
- translation
- verbosity
- verbose
- translate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper examines how verbose outputs from large language models\
  \ (LLMs) impact translation evaluation. It finds that most LLMs, except GPT-4 and\
  \ Aya23, frequently produce verbose translations\u2014either refusing to translate,\
  \ offering multiple options, or providing commentary."
---

# On the Implications of Verbose LLM Outputs: A Case Study in Translation Evaluation

## Quick Facts
- arXiv ID: 2410.00863
- Source URL: https://arxiv.org/abs/2410.00863
- Reference count: 4
- Most LLMs produce verbose translations, but filtering them out changes model rankings

## Executive Summary
This study investigates how verbose outputs from large language models (LLMs) impact translation evaluation. The researchers find that most LLMs, except GPT-4 and Aya23, frequently produce verbose translations through refusal to translate, offering multiple options, or providing commentary. This behavior is triggered by content concerns, non-linguistic inputs, or lack of context in short queries. When verbose outputs are filtered out, the ranking of LLMs changes significantly in both automatic and human evaluations, with models like Gemini-1.5-Pro improving substantially. The study calls for adapted evaluation methods that account for contextualized outputs.

## Method Summary
The study analyzed WMT 2024 general MT task outputs across 8 language pairs and 7 LLMs. Researchers used prompt-based annotation with Gemini-1.5-Pro to classify outputs as refusal, multiple translations, or commentary. They computed automatic evaluation metrics (MetricX23) on both full and filtered datasets (excluding verbose-triggered segments) and conducted human evaluation (MQM) on a subset. Manual inspection identified triggers for verbosity, including short input segments lacking context and content safety concerns.

## Key Results
- Most LLMs exhibit verbosity through refusal to translate, multiple translation options, or commentary
- Short input segments lacking context are the primary trigger for verbose behavior
- Filtering out verbose outputs significantly changes model rankings in both automatic and human evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit verbosity when inputs are short or lack context, prompting them to provide additional explanations or alternative translations.
- Mechanism: The model interprets ambiguous or sparse inputs as opportunities to contextualize its output, leading to commentary or multiple translation options.
- Core assumption: LLMs are trained to prioritize helpfulness and completeness, even when the task definition (single translation) is clear.
- Evidence anchors:
  - [abstract]: "Our analysis reveals that short input segments lacking sufficient context are the primary reason for this verbose behavior."
  - [section]: "We found that the source text length influences this tendency... short input segments lacking sufficient context are the primary reason for this verbose behavior."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.478, average citations=0.0. Top related titles: The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact, LLM Cyber Evaluations Don't Capture Real-World Risk, Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness.
- Break condition: When prompts explicitly constrain the output format or when the input is sufficiently contextualized.

### Mechanism 2
- Claim: LLMs refuse to translate when they detect potentially harmful, copyrighted, or non-linguistic content.
- Mechanism: The model applies safety and content filters that override the translation task when certain content types are identified.
- Core assumption: Safety and copyright concerns are prioritized over task completion in the model's training.
- Evidence anchors:
  - [abstract]: "The most prevalent form of verbosity we observed across LLM outputs is their refusal to translate... identifying potentially harmful or copyrighted content."
  - [section]: "Refusal to translate is primarily triggered by three factors: the detection of potentially harmful or copyrighted content in the source text, or the request for translating non-natural language."
  - [corpus]: Weak evidence - no direct citations about content refusal in translation context.
- Break condition: When prompts explicitly override safety filters or when content is deemed acceptable by the model's internal policies.

### Mechanism 3
- Claim: Current evaluation metrics unfairly penalize verbose LLMs because they don't account for contextualized outputs.
- Mechanism: Standard metrics expect single, direct translations and mark any additional commentary or refusal as errors, lowering scores.
- Core assumption: Evaluation protocols were designed for traditional MT systems, not conversational LLMs.
- Evidence anchors:
  - [abstract]: "Current evaluation metrics and human evaluation protocols fail to account for the nuances of contextualized translations, penalizing verbose LLMs."
  - [section]: "Excluding outputs changes the induced ranking of systems... GEMINI-1.5-PRO's ranking is most strongly affected by this filtering."
  - [corpus]: Weak evidence - no direct citations about evaluation methodology adaptation.
- Break condition: When evaluation protocols are adapted to handle verbose outputs or when models are trained to suppress verbosity.

## Foundational Learning

- Concept: Prompt engineering and few-shot demonstrations
  - Why needed here: The study uses structured prompts with demonstrations to guide LLM behavior during evaluation.
  - Quick check question: How do few-shot demonstrations influence LLM output structure in translation tasks?

- Concept: Automatic evaluation metrics (e.g., BLEU, MetricX23)
  - Why needed here: The study relies on automatic metrics to compare LLM performance, showing how verbosity affects rankings.
  - Quick check question: What are the limitations of using standard MT metrics for evaluating LLM outputs?

- Concept: Human evaluation protocols (e.g., MQM)
  - Why needed here: The study uses human evaluation to validate automatic results and identify error patterns in verbose outputs.
  - Quick check question: How do human evaluators handle contextualized or verbose translations differently from traditional MT outputs?

## Architecture Onboarding

- Component map: Prompt generation -> LLM inference -> Output annotation (verbosity detection) -> Evaluation (automatic + human) -> Analysis and ranking comparison
- Critical path: The core workflow involves generating prompts, running translations, detecting verbosity, and evaluating outputs with and without verbose instances.
- Design tradeoffs: Balancing between preserving useful verbosity (e.g., context explanations) and ensuring fair evaluation requires careful prompt design and metric adaptation.
- Failure signatures: Overly verbose outputs that dominate evaluation data, inconsistent handling of refusal to translate, and misalignment between automatic and human evaluation results.
- First 3 experiments:
  1. Test different prompts to reduce verbosity (e.g., "output the translation and nothing else") and measure impact on evaluation scores.
  2. Implement structured output formats to isolate commentary from translations and evaluate separately.
  3. Develop context-aware evaluation metrics that can handle multiple translations or explanatory notes without penalizing models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt-level interventions could effectively suppress LLM verbosity while maintaining translation quality?
- Basis in paper: [explicit] The paper discusses prompt-level interventions as one of two orthogonal approaches to address verbosity issues.
- Why unresolved: The paper only mentions a general "simple prompt to 'output the translation and nothing else'" but doesn't explore specific prompt engineering techniques or their effectiveness across different LLM architectures.
- What evidence would resolve it: Comparative analysis of different prompt formulations across multiple LLMs, measuring both verbosity reduction and translation quality maintenance.

### Open Question 2
- How does the prevalence of verbose outputs vary across different types of source content domains beyond the four mentioned (social, speech, literary, news)?
- Basis in paper: [inferred] The paper mentions that segments from the "social" domain lead to refusal to translate more frequently, suggesting domain-specific patterns.
- Why unresolved: The analysis only provides a high-level breakdown of four domains, without examining potential patterns in other domains or how different types of content might trigger specific verbosity behaviors.
- What evidence would resolve it: Comprehensive analysis of verbosity patterns across all content domains in the WMT 2024 dataset, with statistical correlations between content types and specific verbosity behaviors.

### Open Question 3
- What would be the impact of incorporating context-aware evaluation frameworks that account for contextualized translations on the overall ranking of LLM systems?
- Basis in paper: [explicit] The paper calls for context-aware evaluations and discusses how current evaluation metrics fail to account for nuanced contextualized translations.
- Why unresolved: The paper identifies the need for context-aware evaluations but doesn't explore what such frameworks might look like or how they would affect system rankings.
- What evidence would resolve it: Implementation and testing of context-aware evaluation frameworks that account for multiple translation options and explanatory notes, with comparison to current evaluation methods.

## Limitations
- Findings based on specific dataset (WMT 2024 general MT task) and limited set of LLMs
- Exact prompts used for translation tasks not fully specified, affecting reproducibility
- Prompt-based annotation approach may have limitations in capturing all instances of verbose behavior

## Confidence
- High Confidence: GPT-4 and Aya23 exhibit significantly less verbose behavior compared to other LLMs
- Medium Confidence: Short input segments lacking context are the primary trigger for verbose behavior
- Medium Confidence: Filtering out verbose outputs changes model rankings in evaluation
- Low Confidence: Current evaluation metrics fundamentally fail to account for contextualized translations

## Next Checks
1. Systematically test different prompt formulations (e.g., explicit output constraints, context provision) to verify the relationship between input characteristics and verbosity triggers.
2. Implement a second annotation approach (e.g., rule-based detection) and compare results with the prompt-based method to assess reliability and coverage of verbosity detection.
3. Develop and test evaluation metrics specifically designed to handle verbose outputs, measuring whether they provide fairer assessments of LLM translation quality compared to standard metrics.