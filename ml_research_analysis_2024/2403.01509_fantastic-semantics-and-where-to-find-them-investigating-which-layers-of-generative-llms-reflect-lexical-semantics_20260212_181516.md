---
ver: rpa2
title: 'Fantastic Semantics and Where to Find Them: Investigating Which Layers of
  Generative LLMs Reflect Lexical Semantics'
arxiv_id: '2403.01509'
source_url: https://arxiv.org/abs/2403.01509
tags:
- layers
- language
- word
- lexical
- llama2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how lexical semantics evolve across layers
  in large language models, focusing on Llama2. The authors probe hidden states at
  each layer using a contextualized word identification task and find that lower layers
  encode lexical semantics while higher layers prioritize prediction, contrasting
  with BERT-like models.
---

# Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics

## Quick Facts
- arXiv ID: 2403.01509
- Source URL: https://arxiv.org/abs/2403.01509
- Reference count: 11
- Key outcome: This study investigates how lexical semantics evolve across layers in large language models, focusing on Llama2. The authors probe hidden states at each layer using a contextualized word identification task and find that lower layers encode lexical semantics while higher layers prioritize prediction, contrasting with BERT-like models.

## Executive Summary
This paper investigates how lexical semantics evolve across layers in large language models, focusing on Llama2. The authors probe hidden states at each layer using a contextualized word identification task and find that lower layers encode lexical semantics while higher layers prioritize prediction, contrasting with BERT-like models. Experiments with various input transformations and prompting strategies show that the optimal performance is achieved in lower layers for generative tasks, with a monotonic increase in predictive accuracy in higher layers. This suggests a balance between understanding and prediction in LLMs, with lower layers being more suitable for lexical-related tasks and higher layers for prediction-related or generative tasks. The findings offer practical insights into extracting representations for lexical semantics tasks and shed light on the interpretability of LLMs from a top-down perspective.

## Method Summary
The study uses a probing approach to analyze how the hidden states at each layer of Llama2 reflect word meanings. The authors employ the Word in Context (WiC) dataset as a proxy task for exploring lexical semantics, using various input transformation and prompting strategies to fully utilize contextual information. They extract hidden states from each layer and calculate cosine similarity of words in paired contexts, classifying sentence pairs based on a threshold. The method involves implementing anisotropy removal to handle representation space collapse and comparing performance across layers with different input variants (base, repeat, prompt).

## Key Results
- Lower layers in Llama2 encode lexical semantics while higher layers shift toward prediction, opposite to BERT's pattern
- Prompting strategies improve semantic accuracy in higher layers by maintaining focus on current token meaning
- Anisotropy removal standardizes the embedding space, improving semantic similarity calculations and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower layers in Llama2 encode lexical semantics while higher layers shift toward prediction, in contrast to BERT which does the opposite.
- Mechanism: In autoregressive generative models, information flow is constrained to left context only. Early layers build representations for the current token using this context, while later layers prioritize predicting the next token, causing a gradual loss of semantic focus on the current word.
- Core assumption: The model's training objective (next token prediction) drives a shift in representational priorities across layers.
- Evidence anchors:
  - [abstract] "Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction."
  - [section] "We observe non-monotonic trends for Llama2 across layers: both the base and repeat initially increase in lower layers before decreasing in higher layers."
  - [corpus] Weak—no corpus data directly supports this; only related probing papers.
- Break condition: If the training objective were bidirectional (like BERT), this mechanism would not hold.

### Mechanism 2
- Claim: The prompting strategy forces the model to generate a semantic label for the current word, preserving lexical semantics in higher layers.
- Mechanism: By reformulating the input as a self-contained sentence with an explicit question, the model treats the final colon as the target token, maintaining semantic focus even in deep layers.
- Core assumption: Prompting changes the effective prediction target to the last token, aligning the task with lexical semantics rather than next-word prediction.
- Evidence anchors:
  - [section] "The prompting strategy achieves the highest accuracy among all the Llama2 variants... This approach incorporates downstream tasks into the generative process."
  - [section] "both repeat_prev and prompt exhibit a monotonic trend and comparable result across the layers."
  - [corpus] Weak—no corpus evidence provided; only conceptual link.
- Break condition: If the prompt is poorly designed or the model fails to interpret the prompt correctly, semantic encoding may degrade.

### Mechanism 3
- Claim: Anisotropy removal standardizes the embedding space, improving semantic similarity calculations.
- Mechanism: Without anisotropy removal, embeddings may cluster in a narrow cone, distorting cosine similarity and hurting performance in semantic tasks.
- Core assumption: The representation space is anisotropic, and standardization restores uniformity.
- Evidence anchors:
  - [section] "The results consistently demonstrate the advantage of methods with anisotropy removal, suggesting that the representation space may collapse into a smaller cone space."
  - [section] "This also offers a simple and practical approach for calculating similarity in the embedding space."
  - [corpus] Weak—no corpus data; inference from prior work cited.
- Break condition: If embeddings are already isotropic or the task does not rely on cosine similarity, this step may be unnecessary.

## Foundational Learning

- Concept: Contextualized word representations
  - Why needed here: The paper hinges on how word meanings change with context and across layers.
  - Quick check question: What is the difference between static and contextualized embeddings?

- Concept: Autoregressive vs. bidirectional models
  - Why needed here: The core comparison is between Llama2 (autoregressive) and BERT (bidirectional), which affects how semantics are encoded.
  - Quick check question: How does the training objective of BERT differ from that of Llama2?

- Concept: Probing tasks and benchmarks
  - Why needed here: The Word in Context (WiC) task is used to measure semantic understanding across layers.
  - Quick check question: What does the WiC dataset evaluate, and why is it suitable for lexical semantics?

## Architecture Onboarding

- Component map: Input -> Layer 0 hidden states -> Layer 1 hidden states -> ... -> Layer 31 hidden states -> Cosine similarity comparison -> Binary classification via threshold
- Critical path: Input → Layer 0 hidden states → Layer 1 hidden states → ... → Layer 31 hidden states → Cosine similarity comparison → Binary classification via threshold
- Design tradeoffs: Lower layers preserve semantic fidelity but may lack task-specific predictive power; higher layers excel at prediction but lose semantic detail. Prompting trades generality for task alignment.
- Failure signatures: Non-monotonic accuracy curves suggest semantic degradation in higher layers; sudden drops in accuracy may indicate threshold miscalibration or anisotropy issues.
- First 3 experiments:
  1. Run WiC with base input and record layer-wise accuracy to establish baseline trend.
  2. Apply repeat_prev strategy and compare monotonicity vs base to confirm predictive shift.
  3. Add anisotropy removal and measure impact on accuracy and stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different languages and models affect lexical semantic estimation in LLMs?
- Basis in paper: [explicit] The paper states that it only focuses on English and Llama2, and anticipates future studies to refine findings using diverse languages and models.
- Why unresolved: The current study is limited to English and Llama2, so the effects of different languages and models on lexical semantic estimation are not explored.
- What evidence would resolve it: Conducting experiments with LLMs in various languages and comparing the results with different models would provide insights into how language and model choice impact lexical semantic estimation.

### Open Question 2
- Question: What kind of semantics representations are exactly learned by LLMs?
- Basis in paper: [explicit] The paper mentions that it is unclear what kind of semantics representations are learned by LLMs, despite the valuable viewpoint provided by probing.
- Why unresolved: The dense, high-dimensional vectors from computational models and discrete, low-dimensional concepts from linguistic conventions are different, and bridging this gap remains an important issue.
- What evidence would resolve it: Further research that combines computational modeling and linguistics to understand the relationship between the representations learned by LLMs and linguistic concepts would help clarify the semantics representations learned by LLMs.

### Open Question 3
- Question: How does the information flow in LLMs affect the balance between understanding and prediction across layers?
- Basis in paper: [explicit] The paper suggests that LLMs prioritize understanding before prediction as information flows from lower to higher layers, but it does not provide a detailed analysis of how this information flow affects the balance between understanding and prediction.
- Why unresolved: The paper only provides a general observation of the information flow and its effect on understanding and prediction, but does not delve into the specific mechanisms or factors that influence this balance.
- What evidence would resolve it: A more in-depth analysis of the information flow in LLMs, including the role of attention mechanisms, layer interactions, and training objectives, would help elucidate how these factors contribute to the balance between understanding and prediction across layers.

## Limitations

- Limited architectural comparison: The study only compares Llama2 with BERT, without testing other generative architectures like GPT-3 or Claude.
- Weak empirical justification for anisotropy removal: The necessity of anisotropy removal is mentioned but lacks detailed implementation specifics and strong empirical support.
- Proxy task limitations: Using WiC as a proxy for lexical semantics may not fully capture the complexity of semantic understanding across all lexical categories.

## Confidence

**High Confidence**: The observation that prompting strategies improve semantic accuracy in higher layers is well-supported by experimental results. The monotonic increase in predictive accuracy in higher layers for Llama2 is also consistently observed across multiple input variants.

**Medium Confidence**: The claim that generative models show opposite layer-wise semantic trends compared to BERT-like models is plausible but based on limited architectural comparison. The mechanism explaining why lower layers preserve semantics (context constraints in autoregressive models) is theoretically sound but not empirically proven.

**Low Confidence**: The assertion that anisotropy removal is universally necessary for semantic tasks lacks strong empirical justification in this paper. The claim that these findings offer "practical insights" for real-world applications is speculative without demonstration on downstream tasks.

## Next Checks

1. **Cross-Architecture Validation**: Test the layer-wise semantic trends on multiple generative models (GPT-3, Claude, Mistral) to determine if the lower-layers-for-semantics pattern is consistent across architectures or specific to Llama2.

2. **Alternative Semantic Benchmarks**: Replicate the experiments using multiple semantic probing datasets (e.g., SemEval semantic similarity tasks, BLiMP for lexical semantics) to verify that WiC results generalize to broader semantic phenomena.

3. **Downstream Task Integration**: Apply representations from optimal layers (lower for semantics, higher for prediction) to actual downstream tasks like text classification or question answering to validate whether the theoretical insights translate to practical performance gains.