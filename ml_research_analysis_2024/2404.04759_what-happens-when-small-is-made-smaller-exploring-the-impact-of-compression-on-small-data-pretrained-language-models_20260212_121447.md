---
ver: rpa2
title: What Happens When Small Is Made Smaller? Exploring the Impact of Compression
  on Small Data Pretrained Language Models
arxiv_id: '2404.04759'
source_url: https://arxiv.org/abs/2404.04759
tags:
- language
- languages
- pruning
- performance
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores compression techniques\u2014pruning, knowledge\
  \ distillation, and quantization\u2014on AfriBERTa, a small-data language model\
  \ trained on low-resource African languages. Experiments demonstrate that pruning\
  \ achieves up to 60% sparsity with minimal performance loss, while knowledge distillation\
  \ yields 22\u201331% compression with comparable results."
---

# What Happens When Small Is Made Smaller? Exploring the Impact of Compression on Small Data Pretrained Language Models

## Quick Facts
- arXiv ID: 2404.04759
- Source URL: https://arxiv.org/abs/2404.04759
- Authors: Busayo Awobade; Mardiyyah Oduwole; Steven Kolawole
- Reference count: 39
- Key outcome: Pruning achieves up to 60% sparsity with minimal performance loss; knowledge distillation yields 22-31% compression with comparable results; quantization reduces model size by 64.08% and inference time by 52.3%

## Executive Summary
This paper investigates compression techniques—pruning, knowledge distillation, and quantization—on AfriBERTa, a small-data language model trained on low-resource African languages. The study systematically evaluates how these techniques affect model performance, efficiency, and cross-lingual generalization. Results demonstrate that compression can significantly reduce model size and inference time while maintaining competitive performance on Named Entity Recognition tasks, with pruning before fine-tuning showing particular stability at high sparsity levels.

## Method Summary
The paper evaluates three compression techniques on AfriBERTa models (base and large) for NER tasks using the MasakhaNER dataset across 10 African languages. Pruning experiments test unstructured magnitude pruning at 10%-95% sparsity levels before and after fine-tuning. Knowledge distillation uses task-agnostic (distil then fine-tune) and task-specific (fine-tune then distil) approaches with temperature values of 2, 3, and 6. Quantization applies both LLM.int8() and dynamic quantization methods. Models are evaluated on F1 score, model size reduction, and inference time, with additional tests for cross-lingual transfer on Fon, Bambara, and Chinese.

## Key Results
- Pruning before fine-tuning maintains stable performance up to 60% sparsity with minimal F1 score degradation
- Knowledge distillation from AfriBERTa-large produces the best student models, achieving 22-31% compression
- Quantization reduces model size by 64.08% and inference time by 52.3% while maintaining comparable performance
- Cross-lingual transfer shows varying results, with some languages benefiting more from compression than others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning before fine-tuning provides more stable performance at high sparsity levels compared to pruning after fine-tuning.
- Mechanism: Pruning before fine-tuning removes less important weights early, allowing the remaining network to adapt to the task without being influenced by pruned connections. This preserves the essential structure needed for the task.
- Core assumption: The initial network architecture contains a subnetwork capable of solving the task, and pruning removes redundant connections without destroying this subnetwork.
- Evidence anchors:
  - [section] "Our results show that pruning before fine-tuning produces fairly consistent performance with the dense model up to a sparsity level of 60%."
  - [section] "Pruning before fine-tuning results in more stable and predictable performance at greater sparsity levels."
  - [corpus] Weak evidence - corpus neighbors discuss pruning in general but not specifically before vs after fine-tuning.
- Break condition: If the initial network lacks a task-capable subnetwork, or if pruning removes critical connections before the model learns which features matter.

### Mechanism 2
- Claim: Knowledge distillation from larger teacher models produces better student models than distillation from smaller teachers.
- Mechanism: Larger teacher models capture more diverse and nuanced patterns in the data. When distilling to a smaller student, these richer representations transfer better, even if the student is smaller than the base teacher.
- Core assumption: The student model can effectively learn and compress the knowledge from a larger teacher without simply memorizing it.
- Evidence anchors:
  - [section] "we discover that the AfriBERTa-large model produced the student with the best grade."
  - [section] "the base model is comparatively better at imparting most of its knowledge to its students, even though the larger model creates the best overall student."
  - [corpus] No direct evidence in corpus neighbors about teacher size effects on distillation.
- Break condition: If the student model is too small to capture the teacher's knowledge, or if the teacher's knowledge is too specialized to transfer effectively.

### Mechanism 3
- Claim: Quantization achieves significant size and latency reductions while maintaining comparable performance across most languages.
- Mechanism: Converting weights from higher precision (32-bit float) to lower precision (8-bit integer) reduces memory footprint and computational requirements. The LLM.int8() method handles outliers better than dynamic quantization, preserving more information.
- Core assumption: The model's performance is not overly sensitive to small numerical perturbations introduced by quantization.
- Evidence anchors:
  - [section] "quantization reduces the model size by 64.08%, inference time by 52.3%, and even outperforms the baseline model in the F1 score for certain languages."
  - [section] "LLM.int8() quantization method generally outperformed the dynamic quantization method across all languages, with an average decrease in the F1-score of just 4.7%."
  - [corpus] Weak evidence - corpus neighbors discuss quantization but not specifically LLM.int8() vs dynamic quantization.
- Break condition: If the model relies heavily on precise numerical values, or if quantization introduces too much noise for the task.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: The paper evaluates compression techniques specifically on NER tasks, so understanding what NER is and how it's measured is crucial for interpreting results.
  - Quick check question: What are the typical entity types in NER, and why is the F1 score a suitable metric for this task?

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper explores how well compressed models generalize to unseen languages, which is a form of cross-lingual transfer.
  - Quick check question: How does cross-lingual transfer differ from zero-shot learning, and why is it important for low-resource languages?

- Concept: Pruning and sparsity
  - Why needed here: The paper extensively uses unstructured magnitude pruning at various sparsity levels.
  - Quick check question: What is the difference between structured and unstructured pruning, and how does sparsity level affect model performance?

## Architecture Onboarding

- Component map: AfriBERTa corpus → preprocessing → tokenization → Model (XLM-R based) → Compression techniques (Pruning, Knowledge Distillation, Quantization) → Evaluation (NER task using MasakhaNER dataset) → F1 score analysis
- Critical path: Model compression → Fine-tuning → Evaluation → Analysis
  - Each step depends on the previous one; you can't evaluate compression without first applying it, and you can't analyze results without proper evaluation.
- Design tradeoffs:
  - Pruning before vs after fine-tuning: Before gives more stable performance at high sparsity but may lose task-specific optimizations.
  - Teacher model size in distillation: Larger teachers provide better knowledge but may be harder to distill effectively.
  - Quantization method choice: LLM.int8() vs dynamic quantization involves tradeoffs between performance preservation and implementation complexity.
- Failure signatures:
  - Performance degradation beyond expected levels at certain sparsity thresholds
  - Inconsistent results across different languages indicating language-specific issues
  - Unexpected increases in inference time suggesting implementation problems
- First 3 experiments:
  1. Run pruning before fine-tuning at 50% sparsity to verify the "stable performance" claim
  2. Test distillation from AfriBERTa-large to a smaller student to confirm the teacher size effect
  3. Apply LLM.int8() quantization and measure both size reduction and F1 score change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does pruning affect zero-shot learning performance in low-resource language models, particularly for languages outside the model's training region?
- Basis in paper: [explicit] The paper mentions that AfriBERTa performs zero-shot learning on an unseen-before language with F1 scores between 40-60%, which remain competitive with MasakhaNER 2.0's experiments on Afro-XLMR, even at 50% sparsity. However, there's a considerable drop in performance at 70% sparsity.
- Why unresolved: The paper doesn't explore the limits of pruning on zero-shot learning beyond 50% sparsity or investigate the model's performance on languages from different geographical regions.
- What evidence would resolve it: Experiments testing the model's zero-shot learning capabilities on a diverse set of languages from various geographical regions at different sparsity levels would provide insights into the limits of pruning for zero-shot learning.

### Open Question 2
- Question: What are the optimal sparsity levels for pruning that balance performance and inference time for different language groups?
- Basis in paper: [inferred] The paper shows that inference time decreases as sparsity level increases, but there's a wide range of inference time disparities across languages, suggesting language-specific characteristics may influence inference time.
- Why unresolved: The paper doesn't investigate the relationship between language-specific features and the impact of pruning on inference time, nor does it explore the optimal sparsity levels for different language groups.
- What evidence would resolve it: Experiments analyzing the correlation between language features (e.g., morphological complexity, vocabulary size) and the impact of pruning on inference time for various language groups would help determine optimal sparsity levels.

### Open Question 3
- Question: How does the teacher model size influence the performance of distilled student models in low-resource settings?
- Basis in paper: [explicit] The paper finds that the AfriBERTa-large model produces the best student model, but the base model is comparatively better at imparting knowledge to its students, especially as the attention head and layer ratio reduce.
- Why unresolved: The paper doesn't explore the reasons behind the base model's effectiveness in distillation or investigate the impact of other architectural differences between the base and large models on distillation performance.
- What evidence would resolve it: Experiments comparing the distillation performance of different teacher model sizes with varying architectural differences (e.g., attention head count, layer count) would provide insights into the factors influencing distillation effectiveness in low-resource settings.

## Limitations

- The study is limited to 11 African languages, restricting generalizability to other low-resource language families.
- The paper doesn't examine model robustness to noisy inputs or adversarial examples after compression.
- Comprehensive benchmarking across different hardware configurations and alternative compression methods is absent.

## Confidence

- **High Confidence**: Pruning before fine-tuning effectiveness up to 60% sparsity; LLM.int8() quantization superiority
- **Medium Confidence**: Larger teacher models superiority in knowledge distillation
- **Low Confidence**: Generalization of compression benefits to languages outside tested set

## Next Checks

1. **Architecture Scaling Test**: Evaluate the same compression techniques on AfriBERTa-small to determine if reported benefits scale across model sizes within the same architecture family.

2. **Zero-Shot Generalization**: Test compressed models on completely unseen language families (e.g., Indo-European languages) to validate cross-lingual transfer claims beyond the African language set.

3. **Robustness Assessment**: Conduct stress tests with noisy inputs, adversarial examples, and out-of-distribution samples to evaluate whether compression introduces new vulnerabilities not captured by standard F1 score metrics.