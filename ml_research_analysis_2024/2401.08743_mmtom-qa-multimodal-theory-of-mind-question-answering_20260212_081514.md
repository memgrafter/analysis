---
ver: rpa2
title: 'MMToM-QA: Multimodal Theory of Mind Question Answering'
arxiv_id: '2401.08743'
source_url: https://arxiv.org/abs/2401.08743
tags:
- kitchen
- cabinet
- belief
- goal
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMToM-QA, a multimodal Theory of Mind (ToM)
  benchmark for evaluating machine ToM through multimodal data, combining video and
  text inputs. The benchmark tests both goal and belief inference in rich household
  scenarios, with 600 questions spanning seven types.
---

# MMToM-QA: Multimodal Theory of Mind Question Answering

## Quick Facts
- **arXiv ID**: 2401.08743
- **Source URL**: https://arxiv.org/abs/2401.08743
- **Reference count**: 39
- **Primary result**: BIP-ALM achieves strong multimodal ToM reasoning while SOTA models (GPT-4, multimodal LMMs) perform near chance

## Executive Summary
This paper introduces MMToM-QA, a multimodal Theory of Mind (ToM) benchmark for evaluating machine ToM through multimodal data, combining video and text inputs. The benchmark tests both goal and belief inference in rich household scenarios, with 600 questions spanning seven types. To address multimodal ToM reasoning, the authors propose BIP-ALM, a novel method extending Bayesian inverse planning with language models for scalable inference from symbolic representations. Human experiments show strong performance, while state-of-the-art models (GPT-4, multimodal LMMs) struggle, achieving near-chance accuracy. BIP-ALM significantly outperforms all baselines, demonstrating robust multimodal ToM capacity and generalization to real human behavior.

## Method Summary
The authors propose BIP-ALM, which combines Bayesian Inverse Planning with Language Models for multimodal Theory of Mind reasoning. The method first uses LLMs to generate symbolic plan representations from multimodal inputs (video and text), then applies Bayesian inference to infer goals and beliefs. This approach addresses the scalability challenge of traditional Bayesian models while maintaining the interpretability and robustness needed for ToM tasks. The method is evaluated on the newly introduced MMToM-QA benchmark consisting of 600 questions across seven types, testing goal and belief inference in household scenarios.

## Key Results
- Human performance on MMToM-QA is strong, demonstrating the benchmark's validity
- State-of-the-art models (GPT-4, multimodal LMMs) achieve near-chance accuracy on ToM tasks
- BIP-ALM significantly outperforms all baselines with robust multimodal ToM capacity and generalization to real human behavior

## Why This Works (Mechanism)
BIP-ALM works by leveraging the strengths of both Bayesian inverse planning and large language models. The LLM component efficiently generates symbolic plan representations from multimodal inputs, scaling the approach to complex scenarios. The Bayesian inference component then reasons about goals and beliefs from these symbolic representations, maintaining the interpretability and robustness of traditional ToM approaches. This hybrid architecture effectively combines the pattern recognition capabilities of LMs with the principled reasoning of Bayesian methods, enabling scalable yet interpretable multimodal ToM inference.

## Foundational Learning
**Bayesian Inverse Planning**: A framework for inferring intentions from observed behavior by assuming agents act rationally to achieve goals. Needed for principled ToM reasoning; check by verifying inference quality on known goal scenarios.

**Multimodal Fusion**: Combining information from multiple modalities (video, text) for comprehensive understanding. Needed for rich ToM contexts; check by testing performance with missing modalities.

**Symbolic Plan Representation**: Abstracting complex behaviors into interpretable symbolic forms. Needed for scalable reasoning; check by examining plan generation quality on diverse scenarios.

**Language Model Plan Generation**: Using LLMs to create symbolic representations from raw multimodal inputs. Needed for efficient scaling; check by comparing LLM-generated plans against human annotations.

## Architecture Onboarding

**Component Map**: Multimodal Input -> LLM Symbolic Planner -> Bayesian Inference Engine -> Goal/Belief Output

**Critical Path**: Video and Text Input → LLM Plan Generation → Symbolic Representation → Bayesian Inference → ToM Output

**Design Tradeoffs**: The approach trades some precision for scalability by using LLMs for plan generation rather than manual annotation, enabling handling of complex scenarios but potentially introducing LLM-specific biases.

**Failure Signatures**: Performance degradation occurs when LLM plan generation fails (especially with ambiguous or rare scenarios), when Bayesian priors are misspecified, or when multimodal fusion loses critical information.

**First Experiments**: 1) Test LLM plan generation quality on diverse household scenarios, 2) Validate Bayesian inference accuracy with known ground truth plans, 3) Evaluate end-to-end performance on simplified ToM tasks before full benchmark testing.

## Open Questions the Paper Calls Out
None

## Limitations

**Human Performance Data Limitations**: The human performance evaluation relies on Mechanical Turk workers with potential quality variation, and the study reports only one set of human results without reporting inter-rater reliability or discussing potential biases in worker selection.

**Generalization Concerns**: While BIP-ALM shows strong performance on MMToM-QA, the symbolic grounding approach using large language models for plan inference may not generalize to scenarios outside the carefully curated household environments.

**Computational Scalability**: The BIP-ALM method, while effective, requires symbolic plan generation and Bayesian inference, which may not scale efficiently to longer videos or more complex scenarios.

## Confidence
**Human Performance Data Limitations**: Medium confidence - single evaluation without inter-rater reliability or demographic bias discussion.
**Generalization Concerns**: Medium confidence - strong benchmark performance but limited testing on diverse real-world datasets.
**Computational Scalability**: Low confidence - computational efficiency claims lack detailed benchmarking against alternative methods.

## Next Checks
1. Conduct inter-rater reliability analysis for human performance evaluation and test worker performance consistency across different demographic groups.
2. Evaluate BIP-ALM on diverse, real-world video datasets beyond controlled household scenarios to assess true generalization.
3. Benchmark BIP-ALM's computational efficiency and runtime performance against alternative multimodal ToM approaches on progressively longer video sequences.