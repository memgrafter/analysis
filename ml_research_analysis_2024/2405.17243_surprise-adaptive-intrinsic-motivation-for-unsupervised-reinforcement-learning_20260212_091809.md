---
ver: rpa2
title: Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning
arxiv_id: '2405.17243'
source_url: https://arxiv.org/abs/2405.17243
tags:
- agent
- entropy
- environments
- environment
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an adaptive mechanism to balance surprise minimization
  and maximization objectives for unsupervised reinforcement learning, framing the
  choice as a multi-armed bandit problem. The key insight is that neither surprise
  minimization (effective in high-entropy environments) nor surprise maximization
  (effective in low-entropy environments) alone consistently leads to intelligent
  behavior across diverse environments.
---

# Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.17243
- Source URL: https://arxiv.org/abs/2405.17243
- Reference count: 12
- Key outcome: This paper proposes an adaptive mechanism to balance surprise minimization and surprise maximization objectives for unsupervised reinforcement learning, framing the choice as a multi-armed bandit problem. The key insight is that neither surprise minimization (effective in high-entropy environments) nor surprise maximization (effective in low-entropy environments) alone consistently leads to intelligent behavior across diverse environments. The proposed method uses a novel intrinsic feedback signal based on the agent's ability to control entropy in its environment, selecting between objectives online. Experiments demonstrate that the adaptive agent can learn to optimize task returns through entropy control alone in both high- and low-entropy didactic environments, and exhibits more diverse emergent behaviors in benchmark tasks compared to single-objective agents. In particular, the agent achieves competitive performance with extrinsic reward-based agents in environments like Freeway while maintaining the ability to adapt to different entropy regimes.

## Executive Summary
This paper addresses a fundamental limitation in unsupervised reinforcement learning: the challenge of learning useful behaviors in environments with varying entropy characteristics. The authors propose an adaptive mechanism that dynamically switches between surprise minimization and surprise maximization objectives based on the agent's ability to control entropy in its environment. By framing this choice as a multi-armed bandit problem, the agent can learn to optimize task returns through entropy control alone without requiring extrinsic rewards. The approach demonstrates that neither surprise minimization (effective in high-entropy environments) nor surprise maximization (effective in low-entropy environments) is universally superior, and that adaptation to local entropy conditions is crucial for learning intelligent behaviors.

## Method Summary
The proposed method uses a multi-armed bandit framework to dynamically select between surprise minimization and surprise maximization objectives based on the agent's ability to control entropy in the environment. The agent maintains a state marginal distribution estimator to measure its control over the environment's entropy, which serves as feedback for the bandit. At the start of each episode, the bandit selects which objective to optimize, and the agent trains using that intrinsic reward signal via a standard RL algorithm (DQN). The bandit receives feedback based on the absolute percent difference between the entropy of the state marginal distribution at episode end and that of a random agent, allowing it to adapt to different entropy regimes over time.

## Key Results
- The adaptive agent achieves entropy control across all didactic environments (high and low entropy), learning to switch between surprise minimization and surprise maximization objectives as appropriate.
- In benchmark environments, the S-Adapt agent exhibits more diverse emergent behaviors than single-objective agents, achieving task rewards on par with or better than extrinsic reward-based agents.
- The agent achieves competitive performance with extrinsic reward-based agents in the Atari Freeway environment while maintaining adaptability to different entropy regimes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The surprise-adaptive agent can learn to optimize task returns through entropy control alone in both high- and low-entropy regimes.
- Mechanism: The agent uses a multi-armed bandit to dynamically switch between surprise minimization and surprise maximization based on its ability to control entropy in the environment. This is measured by the absolute percent difference between the entropy of the state marginal distribution at the end of the episode and that of a random agent.
- Core assumption: The agent's ability to control entropy is a good proxy for its potential to learn useful behaviors in that environment.
- Evidence anchors:
  - [abstract]: "Experiments demonstrate that the adaptive agent can learn to optimize task returns through entropy control alone in both high- and low-entropy didactic environments, and exhibits more diverse emergent behaviors in benchmark tasks."
  - [section 5.2]: "Capitalizing on the success modes of the single-objective agents, the proposed S-Adapt agent can adapt to the entropy landscape to achieve entropy control across all didactic environments"
  - [corpus]: No direct evidence, but related works suggest that intrinsic motivation methods can learn useful behaviors in sparse reward environments.
- Break condition: If the bandit's feedback mechanism fails to accurately capture the agent's control over entropy, the agent may not learn to switch between objectives effectively.

### Mechanism 2
- Claim: The surprise-adaptive agent can exhibit more diverse emergent behaviors compared to single-objective agents.
- Mechanism: By dynamically switching between surprise minimization and surprise maximization, the agent can explore a wider range of behaviors than an agent stuck with a single objective. This is especially useful in environments where neither objective alone is sufficient to learn useful behaviors.
- Core assumption: A diverse set of behaviors increases the likelihood of finding useful behaviors for a given task.
- Evidence anchors:
  - [abstract]: "In benchmark environments, we demonstrate more diverse emergent behaviors, as measured by the performance on extrinsic task reward, than observed from the single-objective agents."
  - [section 5.3]: "On the other hand, the S-Adapt agent achieves high task rewards, on par or better than the Extrinsic agent across all didactic environments."
  - [corpus]: No direct evidence, but related works suggest that intrinsic motivation methods can learn diverse behaviors.
- Break condition: If the bandit's exploration strategy is too conservative, the agent may not explore enough to find diverse behaviors.

### Mechanism 3
- Claim: The surprise-adaptive agent can learn skillful behaviors in benchmark tasks without access to extrinsic rewards.
- Mechanism: The agent learns to control entropy in the environment, which can indirectly lead to learning useful behaviors for tasks. This is demonstrated in the Atari Freeway environment, where the S-Adapt agent achieves competitive rewards with the Extrinsic agent.
- Core assumption: Controlling entropy in an environment can lead to learning useful behaviors for tasks in that environment.
- Evidence anchors:
  - [abstract]: "In particular, the agent achieves competitive performance with extrinsic reward-based agents in environments like Freeway while maintaining the ability to adapt to different entropy regimes."
  - [section 5.3]: "In the Freeway environment... the S-Adapt agent achieves competitive rewards with the Extrinsic agent."
  - [corpus]: No direct evidence, but related works suggest that intrinsic motivation methods can learn useful behaviors in sparse reward environments.
- Break condition: If the environment's tasks are not well-correlated with entropy control, the agent may not learn useful behaviors.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper uses RL to train the agent, which is based on the MDP framework.
  - Quick check question: What are the components of an MDP?

- Concept: Entropy
  - Why needed here: The paper uses entropy as a measure of the agent's control over the environment and as an intrinsic reward signal.
  - Quick check question: What is the mathematical definition of entropy?

- Concept: Multi-armed bandit
  - Why needed here: The paper uses a multi-armed bandit to dynamically switch between surprise minimization and surprise maximization.
  - Quick check question: What is the exploration-exploitation tradeoff in multi-armed bandits?

## Architecture Onboarding

- Component map:
  - State marginal distribution estimator -> Multi-armed bandit -> RL algorithm (DQN) -> Environment

- Critical path:
  1. Initialize the state marginal distribution estimator and multi-armed bandit.
  2. At the start of each episode, use the bandit to select between surprise minimization and surprise maximization.
  3. Train the RL agent using the selected intrinsic reward signal.
  4. At the end of each episode, update the state marginal distribution estimator and provide feedback to the bandit based on the agent's ability to control entropy.
  5. Repeat steps 2-4 for multiple episodes.

- Design tradeoffs:
  - Exploration vs. exploitation in the multi-armed bandit: More exploration can lead to finding better objectives, but less exploitation can slow down learning.
  - State marginal distribution estimator complexity: A more complex estimator can capture the state distribution more accurately, but is also more computationally expensive.
  - Intrinsic reward signal: The choice of intrinsic reward signal (surprise minimization vs. surprise maximization) can significantly impact the agent's behavior.

- Failure signatures:
  - The agent fails to learn any useful behaviors: This could indicate that the intrinsic reward signal is not well-suited to the environment.
  - The agent learns degenerate behaviors: This could indicate that the intrinsic reward signal is too easy to maximize without learning useful behaviors.
  - The multi-armed bandit fails to switch between objectives: This could indicate that the feedback mechanism is not accurately capturing the agent's ability to control entropy.

- First 3 experiments:
  1. Train the agent on a simple didactic environment (e.g., Maze) with known entropy conditions and observe if it learns to switch between objectives.
  2. Train the agent on a more complex environment (e.g., Tetris) and observe if it learns useful behaviors without access to extrinsic rewards.
  3. Compare the agent's performance to single-objective agents on a benchmark task (e.g., Atari Freeway) and observe if it achieves competitive rewards.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of state marginal distribution estimation affect the adaptive agent's performance in environments with continuous or high-dimensional observations?
- Basis in paper: [inferred] The paper mentions that estimation of the state marginal distribution can be "quite complex" and uses simplified approaches (Bernoulli or Gaussian distributions), but doesn't systematically evaluate the impact of estimation quality on performance.
- Why unresolved: The paper provides ablation studies on distribution choice but doesn't explore how estimation accuracy (e.g., using more complex models, sample efficiency, or approximation errors) affects the agent's ability to adapt to entropy conditions.
- What evidence would resolve it: Systematic experiments comparing performance across different state marginal estimation methods (e.g., normalizing flows, mixture models) and analyzing how estimation error correlates with adaptation quality and task performance.

### Open Question 2
- Question: What is the optimal feedback signal for the multi-armed bandit when the agent has access to extrinsic rewards or other forms of feedback beyond intrinsic entropy control?
- Basis in paper: [explicit] The paper specifically designs a feedback mechanism based on absolute percent difference in entropy from random agent behavior, stating "the key question is what type of feedback is best to provide the bandit, given access only to intrinsic rewards."
- Why unresolved: The proposed feedback mechanism is tailored for purely intrinsic settings, but the paper doesn't explore how it would perform or need to be modified when extrinsic rewards are available, which is the typical setting in RL.
- What evidence would resolve it: Experiments comparing the proposed intrinsic-only feedback mechanism against bandit feedback mechanisms that incorporate extrinsic rewards or other signals, and analysis of when each approach is superior.

### Open Question 3
- Question: How does the adaptive mechanism scale to environments with non-stationary dynamics or multi-modal entropy landscapes?
- Basis in paper: [inferred] The paper demonstrates adaptation in static didactic environments and benchmark environments, but doesn't test scenarios where the optimal entropy regime changes over time or where multiple distinct entropy regimes exist simultaneously.
- Why unresolved: The current bandit-based approach assumes a single optimal objective at any given time, but real-world environments may require switching between objectives based on context or may have multiple valid strategies that the current formulation doesn't capture.
- What evidence would resolve it: Experiments in environments with non-stationary dynamics, multi-modal state spaces, or explicit task-switching, comparing the adaptive agent's performance against methods that can maintain multiple policies or context-dependent strategies.

## Limitations
- The method's performance has been demonstrated primarily in controlled didactic environments and a single Atari benchmark (Freeway), with limited testing across diverse real-world scenarios.
- The computational overhead of the state marginal distribution estimator and bandit selection mechanism is not thoroughly analyzed.
- The assumption that entropy control is a good proxy for learning potential may not hold in all environments, particularly those where useful behaviors are poorly correlated with entropy manipulation.

## Confidence
- **High confidence**: The adaptive mechanism works in controlled didactic environments with known entropy regimes (demonstrated through direct comparison with single-objective baselines)
- **Medium confidence**: The agent achieves competitive performance with extrinsic reward-based agents in Freeway (based on single benchmark result)
- **Medium confidence**: More diverse emergent behaviors are observed (based on extrinsic task reward metrics, though diversity could be measured more directly)

## Next Checks
1. Test the adaptive agent across multiple Atari games with varying entropy characteristics to assess robustness beyond Freeway
2. Conduct ablation studies removing the bandit component to quantify its contribution to performance gains
3. Implement direct behavioral diversity metrics (e.g., state visitation entropy, behavior clustering) rather than relying solely on task reward as a proxy for diversity