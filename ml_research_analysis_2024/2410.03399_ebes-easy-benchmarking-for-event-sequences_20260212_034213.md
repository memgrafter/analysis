---
ver: rpa2
title: 'EBES: Easy Benchmarking for Event Sequences'
arxiv_id: '2410.03399'
source_url: https://arxiv.org/abs/2410.03399
tags:
- metric
- time
- data
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EBES, a comprehensive benchmark for event
  sequence classification. The authors address the lack of standardized evaluation
  protocols for event sequences by curating 10 diverse datasets and implementing 9
  modern models in a unified PyTorch library.
---

# EBES: Easy Benchmarking for Event Sequences

## Quick Facts
- **arXiv ID:** 2410.03399
- **Source URL:** https://arxiv.org/abs/2410.03399
- **Reference count:** 40
- **Primary result:** GRU-based models achieve best performance on event sequence classification benchmarks

## Executive Summary
This paper introduces EBES, a comprehensive benchmark for event sequence classification that addresses the lack of standardized evaluation protocols in this emerging domain. The authors curate 10 diverse datasets spanning retail, medical, and time series domains, implementing 9 modern models in a unified PyTorch library with rigorous hyperparameter optimization and Monte Carlo cross-validation. The study reveals that GRU-based models outperform other architectures, temporal order is less critical for real-world event sequences than expected, and performance varies significantly with data size. EBES provides a valuable framework for reproducible research and facilitates progress in event sequence classification techniques.

## Method Summary
The authors establish EBES as a standardized benchmark for event sequence classification by curating 10 diverse datasets with varying characteristics and implementing 9 modern models in a unified PyTorch framework. The evaluation methodology employs hyperparameter optimization using Optuna TPE with 210 runs per model/dataset, Monte Carlo cross-validation with 20 runs (85%/15% train/test splits), and statistical ranking using Mann-Whitney tests with Benjamini-Hochberg correction. Models follow a four-step structure: preprocessing, encoder, aggregation, and classification head. The benchmark uses ROC AUC and Accuracy as primary metrics, reporting meanÂ±std over 20 runs with different seeds.

## Key Results
- GRU-based models achieve the best overall performance across the benchmark datasets
- MLP models show surprisingly competitive performance, suggesting aggregated statistics can be sufficient for many EvS tasks
- Temporal order matters less than expected for real-world event sequence classification
- Model rankings vary significantly with data size, with MLP becoming more competitive on larger datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRU-based models dominate EvS classification because they better capture temporal dependencies in irregular sequences
- Mechanism: GRU architectures can handle irregular time intervals and mixed feature types more effectively than attention-based or convolutional approaches
- Core assumption: The irregular sampling and feature heterogeneity in event sequences requires architectures that can adapt to variable temporal spacing
- Evidence anchors:
  - [abstract] "GRU-based models achieving the best results"
  - [section 5.1] "GRU-based models dominate EvS classification"
  - [corpus] Weak evidence - no direct citations supporting GRU superiority in EvS specifically

### Mechanism 2
- Claim: Sequence order matters less in real-world EvS than expected because the classification target depends more on event presence than precise timing
- Mechanism: Real-world event sequences contain sufficient discriminative information in event occurrence patterns that exact temporal ordering becomes secondary
- Core assumption: Classification targets in real applications are based on aggregate event patterns rather than fine-grained temporal dynamics
- Evidence anchors:
  - [section 5.2.1] "sequence order is not as critical for EvS classification as it might seem to be"
  - [section 5.2.3] "for some real-world datasets, the performance drop was not statistically significant"
  - [corpus] No corpus evidence available

### Mechanism 3
- Claim: MLP performs surprisingly well because event sequence classification can be effectively reduced to aggregated feature statistics
- Mechanism: Many EvS classification tasks can be solved by learning relationships between summary statistics of event distributions rather than modeling full temporal dynamics
- Core assumption: The target label can be predicted from aggregate features without requiring sequential modeling
- Evidence anchors:
  - [section 5.1] "MLP's performance suggests effective EvS classification with aggregated statistics"
  - [section 5.2.3] "MLP model did not experience any performance drop [when permuting sequences]"
  - [corpus] No corpus evidence available

## Foundational Learning

- Concept: Irregular sampling and time series interpolation
  - Why needed here: Understanding how to handle irregular timestamps is fundamental to working with EvS data
  - Quick check question: How would you interpolate values for a time series with irregularly spaced observations?

- Concept: Hyperparameter optimization methodology
  - Why needed here: Proper HPO is critical for fair model comparison and avoiding test leakage
  - Quick check question: What is the difference between train-val and hpo-val splits in the context of this benchmark?

- Concept: Statistical significance testing for model comparison
  - Why needed here: Ranking models requires rigorous statistical validation to ensure observed differences are meaningful
  - Quick check question: When would you use Mann-Whitney U test versus paired t-test for model comparison?

## Architecture Onboarding

- Component map: Preprocessing -> Encoder -> Aggregation -> Classification Head
- Critical path: The encoder block is most critical as it determines how temporal dependencies are captured
- Design tradeoffs: Model complexity vs generalization, temporal modeling depth vs computational efficiency
- Failure signatures: Overfitting on training data while validation performance plateaus, poor generalization to unseen sequences
- First 3 experiments:
  1. Train MLP baseline with different aggregation strategies to establish baseline performance
  2. Implement GRU with varying time feature inclusion to test temporal modeling importance
  3. Compare Transformer with and without positional encoding to validate architectural choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How critical is the temporal order of events for different types of event sequence classification tasks?
- Basis in paper: [explicit] The paper explicitly investigates this through experiments shuffling event orders and removing timestamps, showing varying performance drops across different datasets.
- Why unresolved: The paper shows that temporal order is less critical for real-world EvS datasets than expected, but the exact threshold or conditions under which order becomes critical remain unclear.
- What evidence would resolve it: Systematic studies varying the degree of temporal shuffling and measuring performance degradation across diverse datasets, identifying patterns in when order matters.

### Open Question 2
- Question: What are the optimal architectures for event sequence classification that can effectively capture temporal dependencies in real-world datasets?
- Basis in paper: [explicit] The paper finds GRU-based models dominate but notes that methods specifically designed for time series struggle, suggesting a need for specialized EvS architectures.
- Why unresolved: While GRU-based models perform best, the paper doesn't identify a clear optimal architecture, and the performance varies significantly with data size and characteristics.
- What evidence would resolve it: Comparative studies testing novel architectures explicitly designed for EvS on diverse datasets, measuring performance across different data sizes and temporal patterns.

### Open Question 3
- Question: How does data size affect the ranking and performance of different event sequence classification models?
- Basis in paper: [explicit] The paper shows that model rankings change with data size, with MLP becoming competitive on larger datasets, and that performance converges with sufficient data.
- Why unresolved: The paper only examines two large datasets (MBD and Retail) and doesn't establish general scaling laws or identify at what data sizes different models become competitive.
- What evidence would resolve it: Systematic scaling studies across multiple datasets of varying sizes, measuring performance curves and identifying crossover points where different architectures become optimal.

## Limitations

- The benchmark datasets predominantly feature short sequences (median 100 events), potentially limiting generalizability to longer sequences
- The GRU dominance finding may be dataset-specific without theoretical justification
- Lack of datasets where temporal ordering is explicitly critical may bias conclusions about order importance

## Confidence

**High confidence:** The benchmark infrastructure and methodology (Monte Carlo cross-validation, HPO protocol, standardized evaluation) are well-specified and reproducible. The observation that MLP performs competitively across multiple datasets is directly observable from the results.

**Medium confidence:** The GRU dominance and temporal order findings are empirically supported but may not generalize beyond the specific datasets tested. The performance variations with data size are observed but lack theoretical explanation.

**Low confidence:** Claims about the fundamental nature of event sequence classification being distinct from time series lack corpus support and may be overstated.

## Next Checks

1. **Temporal Order Sensitivity Test:** Apply the benchmark to datasets where temporal ordering is explicitly critical (e.g., next event prediction tasks) to validate whether sequence order importance generalizes across task types.

2. **Data Size Scaling Study:** Systematically vary training data sizes for each dataset to quantify how performance scales and identify minimum sample requirements for different model families.

3. **Cross-Dataset Architecture Transfer:** Train models on one dataset type (e.g., medical) and evaluate on another (e.g., retail) to assess whether the observed architectural preferences are domain-specific or represent general principles.