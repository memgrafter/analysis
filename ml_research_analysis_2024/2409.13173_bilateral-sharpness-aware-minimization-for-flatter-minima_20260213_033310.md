---
ver: rpa2
title: Bilateral Sharpness-Aware Minimization for Flatter Minima
arxiv_id: '2409.13173'
source_url: https://arxiv.org/abs/2409.13173
tags:
- bsam
- gradient
- loss
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the "Flatness Indicator Problem" in Sharpness-Aware
  Minimization (SAM), where SAM only considers flatness in the gradient ascent direction,
  leading to insufficiently flat minima. To solve this, the authors propose Bilateral
  Sharpness-Aware Minimization (BSAM), which introduces Min-Sharpness (MinS) to measure
  flatness in the gradient descent direction.
---

# Bilateral Sharpness-Aware Minimization for Flatter Minima

## Quick Facts
- arXiv ID: 2409.13173
- Source URL: https://arxiv.org/abs/2409.13173
- Authors: Jiaxin Deng; Junbiao Pang; Baochang Zhang; Qingming Huang
- Reference count: 37
- Key outcome: BSAM achieves 96.82% accuracy on CIFAR-10 and 81.48% on CIFAR-100, outperforming SAM by measuring flatness bidirectionally with Min-Sharpness

## Executive Summary
The paper addresses a fundamental limitation in Sharpness-Aware Minimization (SAM) where only gradient ascent direction is considered for measuring flatness, resulting in insufficiently flat minima. BSAM introduces Min-Sharpness (MinS) to measure loss decrease in the gradient descent direction, creating a bidirectional flatness indicator. The method also proposes a radius decay strategy to prevent gradient conflicts between ascent and descent perturbations during training. Extensive experiments across classification, transfer learning, human pose estimation, and network quantization demonstrate BSAM's effectiveness, with theoretical analysis proving convergence to local minima.

## Method Summary
BSAM extends SAM by incorporating Min-Sharpness alongside Max-Sharpness to measure flatness bidirectionally. The method computes perturbations in both gradient ascent and descent directions, scaling the MinS gradient to match MaxS magnitude, and gradually decreasing the MinS perturbation radius during training to avoid gradient conflicts. The optimization jointly minimizes training loss, MaxS, and MinS, with the radius decay following the learning rate schedule. This creates a more comprehensive flatness indicator that guides optimization toward flatter minima with better generalization properties.

## Key Results
- CIFAR-10: 96.82% test accuracy with ResNet-18, outperforming SAM by 0.3%
- CIFAR-100: 81.48% test accuracy with ResNet-18, outperforming SAM by 0.8%
- Hessian analysis: Smaller top-50 eigenvalues demonstrate flatter minima compared to SAM
- Cross-task performance: Improved results on transfer learning, human pose estimation, and network quantization tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Min-Sharpness (MinS) addresses SAM's "Flatness Indicator Problem" by measuring loss decrease in the gradient descent direction.
- Mechanism: BSAM jointly optimizes training loss, Max-Sharpness (gradient ascent), and Min-Sharpness (gradient descent), creating a more balanced flatness indicator.
- Core assumption: Flatness should be measured bidirectionally (both ascent and descent) rather than unidirectionally as in SAM.
- Evidence anchors:
  - [abstract]: "SAM only considers the flatness in the direction of gradient ascent, resulting in a next minimization region that is not sufficiently flat"
  - [section III-A]: "Min-Sharpness (MinS) measures how quickly the training loss decreases when moving from w to a nearby parameter value"
  - [corpus]: Weak - only 5 related papers found, none specifically discussing Min-Sharpness
- Break condition: If gradient conflicts between MaxS and MinS become severe during training and cannot be mitigated by radius adjustment.

### Mechanism 2
- Claim: The proposed radius decay strategy (ρmin_t) prevents gradient conflicts in later training stages.
- Mechanism: BSAM decreases the MinS perturbation radius as training progresses, following the learning rate schedule to maintain non-conflicting gradients.
- Core assumption: Gradient conflicts between ascent and descent perturbations increase as the model approaches a local minimum.
- Evidence anchors:
  - [section III-B]: "when the solution falls into a local minimum, large ρmin may cause the minimum perturbation point wmin to rush over the minimum point"
  - [section III-B]: "we instead of address GCP by gradually decreasing ρmin as follows"
  - [corpus]: Weak - no corpus evidence about gradient conflict mitigation strategies
- Break condition: If ρmin becomes too small to effectively measure Min-Sharpness, reducing its impact on optimization.

### Mechanism 3
- Claim: Scaling MinS gradient magnitude to match MaxS gradient magnitude ensures balanced contribution to optimization.
- Mechanism: BSAM scales the gradient at the minimum perturbation point to have the same magnitude as the gradient at the maximum perturbation point.
- Core assumption: The gradient magnitudes at ascent and descent perturbation points differ significantly, requiring scaling for balanced optimization.
- Evidence anchors:
  - [section III-B]: "we scale the gradient at the minimum point ∇wL(wmin) to match the gradient magnitude at the maximum point ∇wL(wmax)"
  - [section III-B]: "we maintain the original scale between the gradient of the original loss and the gradient to promote flat minima"
  - [corpus]: Weak - no corpus evidence about gradient magnitude balancing in flatness-aware methods
- Break condition: If scaling introduces instability or if gradient magnitudes become similar naturally during training.

## Foundational Learning

- Concept: Sharpness-Aware Minimization (SAM)
  - Why needed here: BSAM builds directly on SAM's framework, so understanding SAM's core mechanism is essential.
  - Quick check question: What does SAM optimize to find flatter minima, and in which direction does it measure sharpness?

- Concept: Hessian matrix and its eigenvalues
  - Why needed here: The paper uses top Hessian eigenvalues to empirically demonstrate that BSAM finds flatter minima.
  - Quick check question: How does a smaller maximum Hessian eigenvalue relate to the "flatness" of a minimum?

- Concept: Gradient conflict and perturbation radius
  - Why needed here: BSAM introduces a radius decay strategy to avoid gradient conflicts between ascent and descent perturbations.
  - Quick check question: What happens to the relationship between ascent and descent gradients as the model approaches a local minimum?

## Architecture Onboarding

- Component map:
  Base optimizer -> Max-Sharpness computation -> Min-Sharpness computation -> Radius decay scheduler -> Gradient magnitude balancing -> Loss aggregation -> Weight update

- Critical path:
  1. Compute base gradient g_t
  2. Compute ascent perturbation and gradient g_max_t
  3. Compute descent perturbation and gradient g_min_t
  4. Scale g_min_t to match g_max_t magnitude
  5. Apply radius decay to ρ_min_t
  6. Update weights using combined gradient

- Design tradeoffs:
  - Increased computational cost (3x forward/backward passes vs 1x for SGD)
  - Better generalization vs. training efficiency
  - Complexity of gradient balancing vs. simplicity of SAM
  - Radius decay schedule tuning vs. fixed perturbation size

- Failure signatures:
  - Degraded performance when ρ_min becomes too small
  - Instability when gradient magnitudes differ greatly despite scaling
  - Convergence issues if radius decay schedule is too aggressive

- First 3 experiments:
  1. Compare BSAM vs SAM on CIFAR-10 with ResNet-18, varying ρ_min values
  2. Measure top Hessian eigenvalues for models trained with BSAM vs SAM
  3. Test BSAM with different base optimizers (SGD, Adam) to verify base optimizer independence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of perturbation radius ρmin affect the convergence and generalization performance of BSAM in different training stages?
- Basis in paper: [explicit] The paper discusses the gradient conflict problem (GCP) that occurs when ρmin is too large in later training stages, and proposes decreasing ρmin as training progresses using equation (12).
- Why unresolved: While the paper provides a heuristic for adjusting ρmin during training, it doesn't provide a rigorous theoretical framework for determining the optimal ρmin schedule or analyze how different schedules might affect convergence rates and final generalization performance.
- What evidence would resolve it: A comprehensive study comparing different ρmin scheduling strategies (e.g., linear, exponential decay, adaptive based on gradient norms) and their effects on convergence speed, final accuracy, and robustness across multiple datasets and architectures.

### Open Question 2
- Question: What is the theoretical relationship between Min-Sharpness (MinS) and the second-order curvature of the loss landscape?
- Basis in paper: [explicit] The paper defines MinS as the difference between the current training loss and the minimum loss over a neighborhood, but doesn't provide a theoretical analysis of how this relates to the curvature properties of the loss landscape.
- Why unresolved: While MinS is proposed as a practical measure for flatness in the gradient descent direction, the paper doesn't establish a formal connection between MinS and the eigenvalues of the Hessian or other measures of curvature that are known to affect generalization.
- What evidence would resolve it: A theoretical analysis showing how MinS bounds the second-order Taylor expansion of the loss function, or empirical studies correlating MinS values with Hessian eigenvalues across different network architectures and datasets.

### Open Question 3
- Question: How does BSAM's performance scale with network depth and width compared to other SAM variants?
- Basis in paper: [inferred] The paper evaluates BSAM on ResNet-18, WideResNet-28-10, and PyramidNet-110, showing improved performance over SAM, but doesn't systematically explore how performance changes with different network capacities or architectures.
- Why unresolved: The experiments show that BSAM works well on moderately sized networks, but there's no analysis of how the method performs on very deep or very wide networks, or how it compares to other SAM variants as network capacity increases.
- What evidence would resolve it: Systematic experiments varying network depth (e.g., ResNet-18 through ResNet-152) and width (e.g., WideResNet configurations) while comparing BSAM's performance improvements to those of SAM, ASAM, and other variants, along with analysis of computational overhead scaling.

## Limitations

- The bidirectional flatness measurement increases computational cost by 3× compared to standard SGD
- The radius decay strategy for MinS adds complexity and requires careful tuning
- Empirical validation of the gradient conflict problem's severity could be stronger

## Confidence

- Flatness Indicator Improvement (High): The bidirectional measurement of sharpness is a logical extension of SAM's unidirectional approach, and the mathematical formulation is clear.
- Gradient Conflict Mitigation (Medium): The radius decay strategy is well-motivated theoretically, but the empirical necessity could be better demonstrated through ablation studies.
- Generalization Improvement (Medium): While test accuracy improvements are shown, the connection to flatness is indirect - other factors like optimization dynamics could contribute.

## Next Checks

1. **Ablation study on radius decay**: Train BSAM with fixed ρmin to empirically demonstrate that gradient conflicts occur and that the decay strategy mitigates them.

2. **Cross-dataset generalization test**: Evaluate BSAM on a significantly different dataset (e.g., medical imaging or tabular data) to verify that flatness benefits transfer beyond standard vision benchmarks.

3. **Computational cost analysis**: Measure wall-clock training time for BSAM vs SAM to quantify the practical impact of the 3× computational overhead and assess whether the accuracy gains justify the cost.