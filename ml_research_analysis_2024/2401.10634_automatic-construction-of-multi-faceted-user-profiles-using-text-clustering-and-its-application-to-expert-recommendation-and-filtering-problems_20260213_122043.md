---
ver: rpa2
title: Automatic Construction of Multi-faceted User Profiles using Text Clustering
  and its Application to Expert Recommendation and Filtering Problems
arxiv_id: '2401.10634'
source_url: https://arxiv.org/abs/2401.10634
tags:
- clustering
- clusters
- documents
- user
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problems of expert recommendation and
  document filtering using machine learning and text clustering. The authors propose
  a method to automatically build multi-faceted user profiles by clustering textual
  sources, where each subprofile represents a different topic of interest to the expert.
---

# Automatic Construction of Multi-faceted User Profiles using Text Clustering and its Application to Expert Recommendation and Filtering Problems

## Quick Facts
- arXiv ID: 2401.10634
- Source URL: https://arxiv.org/abs/2401.10634
- Reference count: 40
- Primary result: Clustering-based subprofiles outperform monolithic and committee-based profiles for both expert recommendation and document filtering tasks.

## Executive Summary
This paper addresses the problems of expert recommendation and document filtering using machine learning and text clustering. The authors propose a method to automatically build multi-faceted user profiles by clustering textual sources, where each subprofile represents a different topic of interest to the expert. Two clustering approaches are compared: local (clustering documents per user) and global (clustering all documents together). The proposed method is evaluated in a parliamentary context, using the speeches of Members of Parliament as the source of information.

## Method Summary
The method involves preprocessing parliamentary intervention texts through stopword removal, stemming, and TF-IDF weighting to create a document-term matrix. Clustering algorithms (AGNES, DIANA, K-Means, PAM, LDA, SOM) are then applied in either local or global modes to generate subprofiles. These subprofiles are indexed using Apache Lucene with BM25 retrieval, and Reciprocal Rank Fusion (CombLgDCS) is used to combine subprofile scores into final rankings. The approach is evaluated using Recall@10, Precision@10, and NDCG@10 metrics, comparing clustering-based subprofiles against monolithic, committee-based, and intervention-based baselines.

## Key Results
- Clustering-based subprofiles outperform monolithic and committee-based profiles for both filtering and recommendation tasks.
- Global clustering is more suitable for filtering tasks, while local clustering is better for recommendation tasks.
- Different clustering algorithms perform best for different tasks: hierarchical methods for filtering and LDA, centroid-based methods, and SOM for recommendation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering partitions documents into groups that represent distinct topics, and these groups can be aggregated into weighted term lists to form subprofiles that better capture an expert's interests than monolithic profiles.
- Mechanism: Text preprocessing produces a sparse document-term matrix. Clustering algorithms group similar documents into clusters. Each cluster's union of documents yields a "macro-document" whose term frequencies form a weighted subprofile.
- Core assumption: Documents within a cluster are topically coherent, and the union of their terms preserves topic identity.
- Evidence anchors: [abstract] "by clustering expert textual sources to build profiles and capture the different hidden topics in which the experts are interested." [section] "we have applied clustering analysis... The idea is to cluster the sets of documents to obtain k groups of documents... each group represents a concept."
- Break condition: If cluster cohesion is low (high intra-cluster variance) or if topic overlap between clusters is high, subprofiles become noisy and performance degrades.

### Mechanism 2
- Claim: Global clustering across all documents creates shared topic clusters that are reused across users, leading to more stable topic discovery than per-user (local) clustering.
- Mechanism: A single clustering run on the entire document collection assigns every document to a global cluster. For each user, documents belonging to the same global cluster are merged into a subprofile.
- Core assumption: Global topics are representative of user interests and that local user variations can be expressed as differing subprofile cardinalities.
- Evidence anchors: [section] "The alternative option is global because it performs the clustering process with all the documents from every user... the clusters will contain documents from different users." [section] "For the filtering problem the choice is clear: global clustering is preferable."
- Break condition: If the global topic space is too coarse or dominated by high-frequency terms, individual user specificity is lost, hurting recommendation performance.

### Mechanism 3
- Claim: Fusion of subprofile scores into a single user score via logarithmic position weighting improves ranking stability and relevance.
- Mechanism: Each subprofile produces a BM25 score for a query. CombLgDCS aggregates these with a logarithmic discount: `score = Σ s(subprofile) / log2(rank+1)`.
- Core assumption: Higher-ranked subprofiles are more relevant, but lower-ranked ones still add value; logarithmic discounting balances precision and recall.
- Evidence anchors: [section] "The formula is the following: score(MPi, q) = Σ s(MPi cj) / log2(rank(MPi cj) + 1)" [section] "As the final objective is to rank MPs according to their relevance to the query, the original ranking is filtered by considering the CombLgDCS method."
- Break condition: If the subprofile ranking is noisy or if the query is very short, logarithmic discounting may overly suppress useful subprofiles.

## Foundational Learning

- Concept: Text preprocessing pipeline (stopword removal, stemming, TF-IDF weighting).
  - Why needed here: Clustering operates on numerical vectors; raw text must be converted into weighted term vectors that reflect importance.
  - Quick check question: Given a document with terms "research", "researches", and "researcher", what term vector components will the preprocessing produce after stemming and TF-IDF?

- Concept: Document clustering evaluation (internal validity vs. task-based external validation).
  - Why needed here: Cluster quality metrics (Silhouette, Davies-Bouldin) do not directly measure recommendation performance; we need to validate clustering by its downstream impact on filtering/recommendation.
  - Quick check question: If Silhouette score is high but filtering recall is low, what does this indicate about the relevance of cluster structure to the task?

- Concept: Fusion of ranked lists (Reciprocal Rank Fusion).
  - Why needed here: Each subprofile yields a separate ranking; combining them into a single user ranking is essential for both filtering and recommendation.
  - Quick check question: How does CombLgDCS differ from simple score averaging when combining multiple subprofile rankings?

## Architecture Onboarding

- Component map: Data Ingestion -> Preprocessing (stopword removal, stemming, TF-IDF) -> Document-term matrix -> Clustering (global or local) -> Subprofile generation (macro-documents) -> BM25 indexing -> Query processing -> CombLgDCS fusion -> Final ranking
- Critical path: Preprocessing -> Clustering -> Subprofile creation -> Indexing -> Query ranking
- Design tradeoffs:
  - Local vs. global clustering: Local gives user-specific topics but may fragment topics; global gives shared stable topics but may blur user specificity.
  - k selection: Too few clusters -> loss of topic granularity; too many -> sparsity and noise.
  - Fusion method: CombLgDCS vs. averaging vs. weighted sum; impacts ranking stability.
- Failure signatures:
  - Low clustering cohesion -> subprofiles contain mixed topics -> poor recommendation precision.
  - Subprofile sizes too small -> BM25 scores become unreliable -> ranking noise.
  - Poor k estimation -> either too coarse or too fine-grained topics -> performance drops.
- First 3 experiments:
  1. Run global clustering with k = #committees and evaluate recall@10 for filtering vs. monolithic profile baseline.
  2. Run local clustering with k = sqrt(n/2) and evaluate NDCG@10 for recommendation vs. committee-based baseline.
  3. Compare CombLgDCS vs. simple averaging fusion for a fixed clustering configuration and measure change in ranking quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of global and local clustering approaches change if different document preprocessing techniques (e.g., lemmatization vs stemming) were used?
- Basis in paper: [inferred] The paper mentions that stop words are removed and stemming is applied during preprocessing, but does not explore the impact of different preprocessing techniques on clustering performance.
- Why unresolved: The authors did not conduct experiments with different preprocessing methods to determine their effect on the quality of the generated subprofiles.
- What evidence would resolve it: Experiments comparing the performance of global and local clustering approaches using various preprocessing techniques (e.g., stemming, lemmatization, or no preprocessing) on the same dataset.

### Open Question 2
- Question: How does the quality of subprofiles generated by clustering algorithms compare to those created using other topic modeling techniques like Non-Negative Matrix Factorization (NMF)?
- Basis in paper: [inferred] The paper focuses on clustering algorithms for generating subprofiles but does not compare their performance to other topic modeling methods like NMF.
- Why unresolved: The authors did not include NMF or similar topic modeling techniques in their experiments to evaluate their effectiveness in creating multi-faceted user profiles.
- What evidence would resolve it: Experiments comparing the performance of clustering algorithms and NMF (or other topic modeling techniques) in generating subprofiles for expert recommendation and filtering tasks.

### Open Question 3
- Question: How would the performance of the proposed approach change if the dataset included documents in multiple languages?
- Basis in paper: [inferred] The paper uses a dataset of parliamentary speeches in Spanish, but does not explore how the approach would perform with multilingual data.
- Why unresolved: The authors did not conduct experiments with datasets containing documents in multiple languages to assess the robustness of their approach.
- What evidence would resolve it: Experiments applying the proposed approach to multilingual datasets and comparing the performance of clustering algorithms across different language combinations.

### Open Question 4
- Question: How sensitive are the results to the choice of evaluation metrics, and would alternative metrics provide different insights into the effectiveness of the proposed approach?
- Basis in paper: [explicit] The authors use precision, recall, and NDCG as evaluation metrics but acknowledge that considering different measures would give different rankings of methods.
- Why unresolved: The authors did not explore the use of alternative evaluation metrics that might provide additional insights into the quality of the generated subprofiles.
- What evidence would resolve it: Experiments using alternative evaluation metrics (e.g., F1-score, mean average precision, or ranking-based metrics) to assess the performance of the proposed approach and compare the results with those obtained using the original metrics.

## Limitations
- Evaluated on a single parliamentary dataset (Andalusian Parliament, 8th Term), limiting external validity.
- Critical implementation details missing: exact preprocessing pipeline, BM25 parameters, and Reciprocal Rank Fusion configuration.
- Clustering evaluation relies on task-based metrics rather than intrinsic cluster quality measures.

## Confidence
- High Confidence: The core claim that clustering-based subprofiles outperform monolithic and committee-based profiles for both filtering and recommendation tasks is well-supported by the experimental results.
- Medium Confidence: The finding that global clustering is preferable for filtering while local clustering works better for recommendation is supported by the experimental data.
- Medium Confidence: The observation that different clustering algorithms perform best for different tasks (hierarchical for filtering, LDA/centroid-based/SOM for recommendation) is based on the presented results.

## Next Checks
1. Reconstruct the exact preprocessing steps using parliamentary transcripts from a similar domain and verify if the same performance improvements are observed with clustering-based subprofiles.
2. Systematically vary k values and fusion method parameters (CombLgDCS) across both filtering and recommendation tasks to determine the stability of observed performance differences.
3. Apply the methodology to a different expert domain (e.g., scientific publications or medical records) to assess whether the global/local clustering tradeoff holds across domains.