---
ver: rpa2
title: 'SLEB: Streamlining LLMs through Redundancy Verification and Elimination of
  Transformer Blocks'
arxiv_id: '2402.09025'
source_url: https://arxiv.org/abs/2402.09025
tags:
- sleb
- blocks
- llms
- transformer
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SLEB, a method for efficiently compressing
  large language models (LLMs) by removing redundant transformer blocks. The core
  idea is to identify and eliminate blocks with minimal impact on model performance
  using a training-free redundancy verification metric.
---

# SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks

## Quick Facts
- arXiv ID: 2402.09025
- Source URL: https://arxiv.org/abs/2402.09025
- Reference count: 40
- Primary result: Training-free block-level pruning method achieving 1.27× inference speedup while maintaining perplexity and accuracy

## Executive Summary
SLEB introduces a novel training-free approach to compress large language models by identifying and removing redundant transformer blocks. The method leverages the observation that consecutive transformer blocks often produce highly similar outputs due to residual connections, creating opportunities for elimination without significant performance degradation. By using a metric3-based redundancy verification system, SLEB achieves substantial inference speedups that are directly proportional to the number of blocks removed, outperforming existing structured pruning methods like 2:4 pruning and channel-wise pruning.

## Method Summary
SLEB is a training-free compression method that identifies and eliminates redundant transformer blocks from LLMs. It uses a calibration dataset (128 sequences from WikiText-2) to evaluate block significance through metric3, which measures each block's impact on token prediction results. The method iteratively removes blocks with the lowest metric3 scores, re-evaluating significance after each removal to account for context-dependent redundancy. SLEB targets 10% and 20% sparsity levels and demonstrates compatibility with post-training quantization while achieving end-to-end inference speedups up to 1.27×.

## Key Results
- Achieves up to 1.27× inference speedup while maintaining perplexity and accuracy
- Outperforms 2:4 pruning and channel-wise pruning on speed and accuracy trade-offs
- Maintains compatibility with post-training quantization across multiple model sizes (6.7B to 70B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer blocks contribute incrementally to residual paths, making consecutive blocks redundant.
- Mechanism: Residual connections sum outputs from each block to the original input, so if block i+1's output is highly similar to block i's output, block i+1 adds little new information.
- Core assumption: Similarity in cosine similarity between block outputs correlates with functional redundancy.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: Removing redundant blocks yields speedup proportional to number of blocks removed, bypassing issues of structured sparsity on small matrices.
- Mechanism: Elimination of whole blocks avoids sparse matrix operations on small matrices that GPUs cannot accelerate; thus speedup is tied directly to reduced block count.
- Core assumption: Speedup in matrix multiply with sparse matrices is negligible for small matrices; removing blocks eliminates entire compute stages.
- Evidence anchors: [section], [section], [corpus]

### Mechanism 3
- Claim: Iterative removal using metric3 preserves perplexity by re-evaluating block significance in the context of already-removed blocks.
- Mechanism: After each removal, the significance metric is recomputed on the partially-compressed model, avoiding cascading redundancy that occurs when blocks are removed independently.
- Core assumption: Block redundancy is context-dependent; removing one block changes the impact of others.
- Evidence anchors: [section], [section], [corpus]

## Foundational Learning

- Concept: Transformer block structure (attention + feed-forward + residual connection).
  - Why needed here: Understanding block-level redundancy requires knowing how outputs are combined via residuals.
  - Quick check question: What operation combines a block's output with the input in a transformer block?

- Concept: Cosine similarity as a distance metric in high-dimensional spaces.
  - Why needed here: Used to quantify output similarity between blocks to identify redundancy.
  - Quick check question: How does cosine similarity behave when two vectors are nearly identical?

- Concept: Structured vs unstructured pruning and their hardware implications.
  - Why needed here: Explains why removing whole blocks can be more efficient than pruning individual weights.
  - Quick check question: Why does unstructured pruning often fail to yield speedups on GPUs despite high sparsity?

## Architecture Onboarding

- Component map: Input embedding → Sequential transformer blocks → Output projection
- Each block: Multi-head attention → Feed-forward network → LayerNorm + residual
- SLEB operates at the block level, removing entire blocks identified as redundant

- Critical path: Token embedding → Block computation → Residual addition → Next block
- SLEB removes selected blocks entirely; the path skips those blocks in forward pass

- Design tradeoffs: Removing blocks reduces compute but risks losing linguistic capacity; SLEB balances by using metric3 to avoid removing blocks critical for downstream performance; Static removal avoids dynamic branching overhead but requires careful block selection

- Failure signatures: Large perplexity increase indicates critical blocks were removed; Minimal speedup despite block removal suggests remaining blocks still dominate compute; Calibration dataset mismatch causes poor block selection

- First 3 experiments: 1) Measure cosine similarity between consecutive block outputs on a small model to verify high similarity exists; 2) Apply metric1 removal on a toy model and observe perplexity degradation to confirm its inadequacy; 3) Run metric3 iteratively on a small model and verify that perplexity remains stable while removing blocks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SLEB's performance vary across different types of natural language processing tasks beyond the zero-shot tasks evaluated?
- Basis in paper: [explicit] The paper mentions evaluating on zero-shot tasks but doesn't explore performance on other NLP tasks like text classification, sentiment analysis, or named entity recognition.
- Why unresolved: The paper focuses primarily on perplexity and zero-shot accuracy, leaving the question of SLEB's effectiveness on other NLP tasks unanswered.
- What evidence would resolve it: Experiments applying SLEB-pruned models to a variety of NLP benchmarks, comparing performance to baseline models and other pruning methods.

### Open Question 2
- Question: What is the impact of different calibration dataset sizes on SLEB's pruning decisions and model performance?
- Basis in paper: [inferred] The paper uses 128 samples for calibration but doesn't explore how varying this size affects results.
- Why unresolved: The optimal calibration dataset size for balancing pruning effectiveness and computational efficiency is not investigated.
- What evidence would resolve it: Systematic experiments varying calibration dataset sizes and measuring their impact on pruning accuracy, model performance, and computational overhead.

### Open Question 3
- Question: How does SLEB perform when applied to smaller language models (e.g., below 1 billion parameters)?
- Basis in paper: [explicit] The paper focuses on models ranging from 6.7B to 70B parameters, leaving smaller models unexplored.
- Why unresolved: The effectiveness of block-level pruning on smaller models, which may have different architectural characteristics, is unknown.
- What evidence would resolve it: Experiments applying SLEB to smaller transformer models and comparing results to baseline models and other pruning methods.

## Limitations
- Speedup measurements are hardware-specific to NVIDIA A100 GPUs with 80GB memory
- Calibration dataset representativeness (128 sequences) may not capture full model behavior
- Generalization across different transformer architectures beyond OPT and LLaMA variants is untested

## Confidence
- **High Confidence**: Core observation of block similarity due to residual connections; speedup proportional to block removal
- **Medium Confidence**: Effectiveness of metric3 without retraining; compatibility with post-training quantization
- **Low Confidence**: Generalization across model families, task types, and hardware platforms

## Next Checks
1. **Architecture Generalization Test**: Apply SLEB to transformer architectures with different residual patterns (e.g., BERT, T5) and verify if cosine similarity remains a reliable indicator of redundancy across architectures.

2. **Hardware Portability Validation**: Measure inference speedup on different GPU architectures (e.g., H100, A100 with varying memory configurations) and CPU-based inference to confirm proportional speedup relationship holds across platforms.

3. **Calibration Dataset Sensitivity Analysis**: Systematically vary calibration dataset size and composition (e.g., 32, 256, 1024 sequences; different domains) to determine sensitivity of metric3's block selection to calibration data quality and quantity.