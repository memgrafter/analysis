---
ver: rpa2
title: Localization of Pallets on Shelves Using Horizontal Plane Projection of a 360-degree
  Image
arxiv_id: '2404.17118'
source_url: https://arxiv.org/abs/2404.17118
tags:
- pallet
- image
- position
- plane
- line
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for accurately detecting the three-dimensional
  (3D) position and orientation of a pallet placed on a shelf using a 360-degree camera
  mounted on a forklift truck. The core idea is to project the 360-degree image onto
  a horizontal plane including the boundary line of the pallet to calculate the yaw
  angle, which is the angle of the front surface of the pallet in the horizontal plane.
---

# Localization of Pallets on Shelves Using Horizontal Plane Projection of a 360-degree Image

## Quick Facts
- arXiv ID: 2404.17118
- Source URL: https://arxiv.org/abs/2404.17118
- Authors: Yasuyo Kita; Yudai Fujieda; Ichiro Matsuda; Nobuyuki Kita
- Reference count: 0
- Primary result: Accurately detects 3D position and orientation of pallets using horizontal plane projection of 360-degree images

## Executive Summary
This paper proposes a method for accurately detecting the three-dimensional position and orientation of pallets on shelves using a 360-degree camera mounted on a forklift truck. The core innovation is projecting the 360-degree image onto a horizontal plane that includes the pallet boundary line to calculate the yaw angle independently of distance. The position is then determined by moving a vertical plane with the corrected yaw angle back and forth and finding where the projected image best matches the actual pallet front surface dimensions.

## Method Summary
The method involves three main steps: First, the pallet is detected by projecting the 360-degree image onto a vertical plane coinciding with the shelf front and matching against a pallet front model using template matching. Second, the yaw angle is calculated by projecting the image onto a horizontal plane including the pallet boundary line, detecting the boundary using edge detection and Hough transform, and measuring the angular deviation from the center line. Third, the position is determined by moving the vertical plane with the corrected yaw angle along the depth axis and finding the position that maximizes template matching accuracy with the pallet front model.

## Key Results
- Yaw angle can be calculated with accuracy of ±1 degree
- Position can be determined within 2 cm of the true value
- Method works in both laboratory and actual warehouse environments
- Calculations complete within reasonable time for real-time forklift operation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method isolates yaw angle from distance ambiguity by first projecting the 360-degree image onto a horizontal plane.
- Mechanism: By projecting onto a horizontal plane that includes the boundary line of the pallet, the yaw angle deviation manifests as a clear angular offset between the detected boundary line and the center line of the projection image. This separation allows yaw angle to be computed independently of the distance to the pallet.
- Core assumption: The boundary line of the pallet is clearly visible in the horizontal projection and can be accurately detected using edge and line detection methods.
- Evidence anchors:
  - [abstract] "projecting the 360-degree image on a horizontal plane including the boundary line of the front surface of the detected pallet" to "accurately detecting the yaw angle"
  - [section] "If the orientation is actually deviated as shown by the red line in Fig. 4, the intersection line and the center line form an angle corresponding to the deviation"
  - [corpus] Weak correlation; corpus neighbors focus on pallet detection but do not address horizontal-plane yaw isolation.
- Break condition: If the pallet boundary is occluded, poorly contrasted, or the pallet is at the same height as the camera (making horizontal plane projection ambiguous).

### Mechanism 2
- Claim: After yaw angle is corrected, the vertical plane projection can be moved along the depth axis to find the position that maximizes template matching accuracy.
- Mechanism: With yaw fixed, the only remaining variable is the depth (distance) of the vertical projection plane. Moving this plane forward and backward and measuring the degree of coincidence with a full-size pallet front model yields the correct 3D position.
- Core assumption: The pallet front surface is planar and can be approximated as a rectangle with known dimensions for template matching.
- Evidence anchors:
  - [abstract] "The position of the pallet is also determined by moving the vertical plane having the detected yaw angle back and forth, and finding the position at which the degree of coincidence between the projection image on the vertical plane and the actual size of the front surface of the pallet is maximized"
  - [section] "D. Calculation of Position: The vertical projection plane with the corrected yaw angle is moved forward and backward with respect to the camera to determine the position that gives the projection image most consistent with the pallet front model"
  - [corpus] No direct evidence; neighbors focus on detection, not multi-step pose refinement.
- Break condition: If the pallet front model does not match the real pallet (e.g., due to wear, damage, or differing dimensions).

### Mechanism 3
- Claim: Using a wide-angle (360-degree) camera mounted on the forklift provides sufficient field of view to observe both nearby and distant pallets simultaneously, enabling continuous tracking.
- Mechanism: The 360-degree camera captures the entire surrounding scene, so as the forklift moves along the aisle, it can observe the pallet of interest at varying distances and angles without requiring multiple sensors or camera repositioning.
- Core assumption: The 360-degree image can be undistorted and reprojected onto arbitrary planes (vertical or horizontal) with sufficient accuracy for downstream processing.
- Evidence anchors:
  - [abstract] "By using a 360-degree camera mounted on the forklift truck, it is possible to observe both the pallet at the side of the forklift and one several meters ahead"
  - [section] "Since a wide field of view is necessary to observe both the pallet next to the forklift and the pallet a few meters ahead, we proposed to mount a 360-degree camera"
  - [corpus] Weak evidence; corpus papers discuss pallet detection but not wide-angle camera geometry or reprojection.
- Break condition: If the distortion correction is inaccurate or if occlusions in the 360-degree image prevent clear boundary extraction.

## Foundational Learning

- Concept: Perspective projection and image reprojection geometry
  - Why needed here: The method relies on projecting the 360-degree image onto arbitrary planes (horizontal and vertical) to extract shape cues independent of viewpoint distortion.
  - Quick check question: How does the mapping from spherical image coordinates to planar reprojection coordinates depend on the camera's mounting pose and the target plane equation?

- Concept: Template matching and feature-based object detection
  - Why needed here: After reprojection, the pallet front surface is detected by matching against a full-size model using edge-based template matching.
  - Quick check question: What are the advantages of using an edge-based template (as opposed to color-based) for detecting the pallet front in a warehouse environment?

- Concept: Hough transform for line detection
  - Why needed here: The method extracts the pallet boundary line in the horizontal projection image using edge detection followed by Hough transform to fit a straight line.
  - Quick check question: Why is the Hough transform preferred over simple thresholding when detecting the pallet boundary in noisy edge maps?

## Architecture Onboarding

- Component map: 360-degree camera feed → spherical image undistortion → reprojection onto horizontal plane → boundary line extraction (edges + Hough) → yaw angle correction → reprojection onto vertical plane → depth scanning + template matching → final 3D pose output
- Critical path: Horizontal plane projection → boundary extraction → yaw correction → vertical plane projection → depth optimization. Each step must succeed for accurate pose estimation.
- Design tradeoffs:
  - Horizontal plane choice: bottom vs. top vs. hole edge; depends on pallet height relative to camera.
  - Template matching metric: edge-based (robust to lighting) vs. pixel-intensity (simpler but less robust).
  - Depth search granularity: finer steps increase accuracy but also computation time.
- Failure signatures:
  - Jagged yaw angle results → boundary extraction unstable or horizontal plane mis-set.
  - Large position jitter → yaw correction inaccurate, causing ridge-like matching surface.
  - Long computation times → coarse yaw search or excessive depth scan resolution.
- First 3 experiments:
  1. Test horizontal plane projection with synthetic pallet images at known yaw offsets; verify angular error detection.
  2. Measure template matching accuracy on vertical plane projections with ground-truth depth values.
  3. Run end-to-end on real warehouse images; compare yaw and position accuracy against total station measurements.

## Open Questions the Paper Calls Out
- The paper mentions future work including developing a method for stably extracting a boundary line from the projected image when calculating the yaw angle under various real environments.

## Limitations
- Method performance may degrade under varying lighting conditions and shadows affecting boundary line detection
- Accuracy depends on pallet front surface being planar and matching the assumed rectangular model
- Performance with pallets of different colors, sizes, and textures not evaluated

## Confidence
- **High**: The mechanism of isolating yaw angle from distance ambiguity through horizontal plane projection is well-supported by the evidence
- **Medium**: The vertical plane projection and depth optimization for position determination is plausible but relies on assumptions about the pallet front model that may not always hold
- **Low**: The benefits of using a 360-degree camera for continuous tracking are asserted but not rigorously proven in the evidence provided

## Next Checks
1. Conduct experiments with pallets at varying heights relative to the camera to assess the robustness of the horizontal plane projection method
2. Evaluate the template matching accuracy on real pallets with varying degrees of wear, damage, and dimensional differences
3. Compare the localization accuracy of the proposed method against a multi-sensor system (e.g., camera + lidar) in a controlled warehouse environment