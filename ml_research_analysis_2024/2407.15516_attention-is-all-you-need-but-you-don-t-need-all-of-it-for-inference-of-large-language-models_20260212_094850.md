---
ver: rpa2
title: Attention Is All You Need But You Don't Need All Of It For Inference of Large
  Language Models
arxiv_id: '2407.15516'
source_url: https://arxiv.org/abs/2407.15516
tags:
- layers
- attention
- layer
- arxiv
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the impact of dropping MLP and attention
  layers during inference on Llama-v2 models to improve efficiency. By removing layers,
  particularly deeper attention layers, the authors achieve significant speedups with
  minimal performance degradation.
---

# Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models

## Quick Facts
- **arXiv ID**: 2407.15516
- **Source URL**: https://arxiv.org/abs/2407.15516
- **Reference count**: 12
- **Primary result**: Removing 33% of attention layers from Llama2-13B provides 18% speedup with only 1.8% performance drop on OpenLLM benchmark

## Executive Summary
This work investigates layer skipping during inference of Llama-v2 models to improve computational efficiency. The authors systematically remove MLP and attention layers from the end of the network and evaluate the trade-off between performance degradation and speedup gains. Their experiments demonstrate that attention layers, particularly deeper ones, are less critical than MLP layers for maintaining model performance, making them ideal candidates for removal. The approach achieves significant computational savings while maintaining reasonable accuracy across standard benchmarks.

## Method Summary
The authors implement a layer skipping mechanism on pre-trained Llama-v2 models (7B and 13B variants) where attention and/or MLP sublayers are removed from the last k layers. They evaluate the modified models on the OpenLLM benchmark datasets (ARC, HellaSwag, TruthfulQA, MMLU) and measure both performance metrics and inference speed. The skipping variants include removing MLP sublayers only, attention sublayers only, or entire transformer blocks, with careful consideration of preserving the final layer to avoid severe performance degradation.

## Key Results
- Removing 33% of attention layers from Llama2-13B achieves 18% speedup with only 1.8% performance drop on OpenLLM benchmark
- MLP layers are more critical than attention layers in deeper network regions, with MLP removal causing larger performance degradation
- Attention skipping provides better speedups than MLP skipping or full block skipping, while preserving the last layer generally improves performance
- Deeper attention layers show higher cosine similarity with previous layers, suggesting feature redundancy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deeper attention layers are less critical for final output quality because their features become more similar to previous layers.
- Mechanism: As transformer depth increases, attention layers converge toward redundancy, with deeper layers contributing less unique information to the final prediction.
- Core assumption: Layer-wise feature similarity (measured by cosine similarity) is a reliable proxy for information redundancy.
- Evidence anchors:
  - [abstract] "dropping deeper attention layers only marginally decreases performance"
  - [section] "Figure 1 shows the cosine similarities between a layer's features and the previous layer showing that deeper layers have a lower impact on the features than earlier layers"
  - [corpus] Weak correlation - related work focuses on layer skipping generally but doesn't directly validate the cosine similarity as a redundancy metric
- Break condition: If attention collapse or rank collapse occurs (Zhai et al., 2023; Dong et al., 2023), deeper layers may not actually be redundant despite similarity scores.

### Mechanism 2
- Claim: MLP layers contain more critical information than attention layers in deeper network regions.
- Mechanism: The feedforward components in deeper layers perform more specialized transformations that are harder to recover when skipped, while attention mechanisms become increasingly redundant.
- Core assumption: MLP sublayers perform unique computations that attention sublayers cannot replicate when dropped.
- Evidence anchors:
  - [abstract] "This seems to indicate that MLP layers are more important than attention layers, at least in deeper parts of the network"
  - [section] "removing 33% of attention layers leads to an 18% speedup... at the cost of a 1.8% drop in average performance" vs larger drops when removing MLP layers
  - [corpus] No direct corpus evidence for MLP vs attention criticality distinction
- Break condition: If attention mechanisms develop task-specific specialization in deeper layers, their removal could become more harmful.

### Mechanism 3
- Claim: Layer skipping provides consistent computational savings regardless of sequence length variations.
- Mechanism: By removing entire layers during inference, the computational graph is permanently reduced, eliminating quadratic attention complexity for those layers across all input sequences.
- Core assumption: The speedup benefits are consistent across different input lengths and don't introduce variable overhead.
- Evidence anchors:
  - [section] "We notice that full layer droppings do improve time costs the best, followed by attention sublayers, and then feedforward sublayers"
  - [section] Tables 7-10 showing consistent percentage improvements across 50 and 100 length sequences
  - [corpus] Weak evidence - related work mentions skipping but doesn't validate consistency across sequence lengths
- Break condition: If caching mechanisms or other optimizations interact negatively with layer skipping, speedup consistency may break down.

## Foundational Learning

- Concept: Transformer layer architecture (attention + MLP sublayers)
  - Why needed here: Understanding which components can be safely removed requires knowing their individual roles and redundancy patterns
  - Quick check question: What are the two main subcomponents of each transformer layer and their primary functions?

- Concept: Attention mechanism complexity (quadratic in sequence length)
  - Why needed here: The motivation for layer skipping stems from attention's computational cost, particularly for long sequences
  - Quick check question: Why does attention have quadratic complexity with respect to input length?

- Concept: Layer-wise feature evolution and redundancy
  - Why needed here: The core insight about deeper layers being redundant relies on understanding how features change (or don't change) across layers
  - Quick check question: What does it mean when cosine similarity between consecutive layers approaches 1?

## Architecture Onboarding

- Component map: Llama-v2 model with L layers -> Each layer contains (Attentioni, MLPi) pair -> Three skipping variants: skip MLP only, skip attention only, skip entire blocks -> Performance measured on OpenLLM benchmark (ARC, HellaSwag, TruthfulQA, MMLU)

- Critical path: 1. Load pre-trained Llama-v2 model 2. Create skipping variants based on layer index 3. Run inference on benchmark datasets 4. Measure performance and timing metrics 5. Compare against baseline (no skipping)

- Design tradeoffs:
  - Attention skipping: Better speedups (18%) with minimal performance loss (1.8%) but may miss task-specific attention patterns
  - MLP skipping: Larger performance drops but potentially more critical information preservation
  - Full block skipping: Maximum speedup but highest performance cost
  - Last layer preservation: Generally reduces performance except for attention skipping

- Failure signatures:
  - Performance degradation exceeding expected thresholds (e.g., >5% drop when removing 33% attention layers)
  - Inconsistent speedup gains across different sequence lengths
  - Unexpected increases in truthfulness metrics (as observed with reduced model size)

- First 3 experiments:
  1. Implement 33% attention layer skipping on Llama-v2 7B and measure OpenLLM benchmark performance
  2. Compare timing improvements between attention skipping vs full block skipping on 50-token sequences
  3. Test last-layer preservation impact by comparing performance with and without keeping final layer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of layer-skipping strategies differ across various tasks, particularly in specialized domains like code generation or medical diagnosis?
- Basis in paper: [explicit] The paper reports performance on general benchmarks (ARC, HellaSwag, TruthfulQA, MMLU) but does not explore specialized domains.
- Why unresolved: The study focuses on general benchmarks, leaving open the question of how layer-skipping strategies perform in specialized or domain-specific tasks.
- What evidence would resolve it: Experiments evaluating layer-skipping strategies on specialized benchmarks (e.g., code generation, medical diagnosis) would provide insights into their effectiveness across diverse domains.

### Open Question 2
- Question: What is the impact of layer-skipping on the robustness and generalization of models to adversarial or out-of-distribution inputs?
- Basis in paper: [inferred] The paper does not address the robustness or generalization of models when layers are skipped, which could be critical for real-world applications.
- Why unresolved: The focus on performance metrics does not extend to robustness or generalization, which are crucial for practical deployment.
- What evidence would resolve it: Testing models with skipped layers on adversarial examples or out-of-distribution data would reveal their robustness and generalization capabilities.

### Open Question 3
- Question: How do layer-skipping strategies affect the interpretability of models, particularly in understanding the contributions of individual layers?
- Basis in paper: [inferred] The paper does not explore how skipping layers impacts the interpretability of models, which could be important for understanding model behavior.
- Why unresolved: The study focuses on performance and efficiency, without addressing interpretability, which is crucial for understanding model decisions.
- What evidence would resolve it: Analyzing the contributions of skipped layers to model predictions and their interpretability would provide insights into the impact of layer-skipping on model understanding.

## Limitations
- The evidence for attention layer redundancy is primarily correlational rather than causal, relying on cosine similarity as a proxy
- Results are limited to Llama-v2 models and may not generalize to other transformer architectures or training paradigms
- The study doesn't explore edge cases or domain-specific tasks where layer skipping might have more severe consequences

## Confidence

**High Confidence**: The empirical finding that removing 33% of attention layers provides ~18% speedup with ~1.8% performance degradation on OpenLLM benchmark. This is directly measurable and the experimental methodology appears sound.

**Medium Confidence**: The claim that attention layers are less critical than MLP layers in deeper network regions. While the data supports this observation, the underlying mechanism (whether due to redundancy versus specialized computation) remains speculative without further investigation.

**Low Confidence**: The generalizability of these findings to other model families, training approaches, or task domains. The study is limited to Llama-v2 and standard benchmarks without exploring diverse scenarios.

## Next Checks
1. **Mechanism Validation**: Conduct ablation studies that systematically vary which specific attention layers are removed (not just the last k layers) to determine if the redundancy pattern holds consistently across different layer positions, and test whether the cosine similarity metric accurately predicts performance impact.

2. **Architecture Generalization**: Apply the same layer skipping approach to other transformer variants (BERT, GPT, Vision Transformers) and different training regimes to assess whether the attention vs MLP criticality distinction holds across architectures.

3. **Downstream Task Impact**: Evaluate the modified models on a broader range of tasks including long-context reasoning, few-shot learning, and domain-specific applications to identify scenarios where layer skipping may cause disproportionate performance degradation beyond what's observed on standard benchmarks.