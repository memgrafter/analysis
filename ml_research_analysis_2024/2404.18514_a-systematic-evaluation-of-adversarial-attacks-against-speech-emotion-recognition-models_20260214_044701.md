---
ver: rpa2
title: A Systematic Evaluation of Adversarial Attacks against Speech Emotion Recognition
  Models
arxiv_id: '2404.18514'
source_url: https://arxiv.org/abs/2404.18514
tags:
- attack
- attacks
- ravdess
- emovo
- emodb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the vulnerability of speech emotion recognition
  (SER) systems to adversarial attacks across multiple languages and genders. A CNN-LSTM
  model is trained on log Mel-spectrograms extracted from German (EmoDB), Italian
  (EMOVO), and English (Ravdess) datasets.
---

# A Systematic Evaluation of Adversarial Attacks against Speech Emotion Recognition Models

## Quick Facts
- arXiv ID: 2404.18514
- Source URL: https://arxiv.org/abs/2404.18514
- Authors: Nicolas Facchinetti; Federico Simonetta; Stavros Ntalampiras
- Reference count: 40
- Key outcome: Adversarial attacks significantly degrade SER accuracy (1-20% from ~90%) across languages and genders, with JSMA and PixelAttack most effective

## Executive Summary
This study evaluates the vulnerability of speech emotion recognition (SER) systems to adversarial attacks across multiple languages and genders. A CNN-LSTM model is trained on log Mel-spectrograms extracted from German (EmoDB), Italian (EMOVO), and English (Ravdess) datasets. Seven attack methods are tested: FGSM, BIM, DeepFool, JSMA, C&W, PixelAttack, and BoundaryAttack. All attacks significantly degrade model accuracy (from ~90% to 1-20%), with JSMA and PixelAttack proving most effective at inducing misclassification with minimal perturbations. White-box attacks are generally more successful, but black-box attacks like BoundaryAttack also achieve strong results. No major differences in attack efficacy are observed across languages, though Italian appears slightly more resistant. Minor gender-based variations exist, with male samples often more vulnerable to white-box attacks. The results highlight SER systems' susceptibility to adversarial examples, emphasizing the need for robust defenses.

## Method Summary
The study evaluates adversarial attacks on SER models using log Mel-spectrograms from German, Italian, and English datasets. A CNN-LSTM architecture processes 3-second audio segments, with attacks applied to the spectrogram representations. Seven attack methods are tested across various configurations to measure accuracy degradation, perturbation magnitude, and execution time.

## Key Results
- All seven adversarial attacks significantly reduced SER model accuracy from ~90% to 1-20%
- JSMA and PixelAttack were most effective at inducing misclassification with minimal perturbations
- White-box attacks generally outperformed black-box attacks, though BoundaryAttack achieved strong results
- No major differences in attack efficacy across languages, with minor gender-based variations observed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Log-Mel spectrograms provide a compact yet discriminative representation of speech emotion that CNNs can effectively process.
- Mechanism: The 2D structure of spectrograms allows convolutional filters to detect local spectral patterns associated with emotional cues (e.g., pitch variation, energy distribution) while temporal dependencies are captured by LSTM layers.
- Core assumption: Emotional information is sufficiently preserved in the spectral-temporal representation and is extractable via learned filters.
- Evidence anchors:
  - [abstract] "We first propose a suitable methodology for audio data processing, feature extraction, and CNN-LSTM architecture."
  - [section 2.2] "We finally computed the log-Mel spectrograms of the obtained audio excerpts, a choice motivated by prior research [8]."
  - [corpus] Weak evidence; related work focuses on STAA-Net, emoDARTS, and EmoBox but does not directly validate spectrogram choice.
- Break condition: If emotional cues are distributed across frequency bands in ways that violate local spatial assumptions of convolution, or if temporal dependencies are too long-range for LSTM capacity.

### Mechanism 2
- Claim: Adversarial attacks degrade SER model accuracy by introducing imperceptible perturbations that exploit gradient-based decision boundaries.
- Mechanism: Attack algorithms compute gradients of the loss w.r.t. input spectrograms, then add small perturbations in the direction that maximizes misclassification. The CNN-LSTM model's smooth decision surface allows these gradients to be effective.
- Core assumption: The model's decision boundary is differentiable and sensitive to small input changes.
- Evidence anchors:
  - [abstract] "all the considered adversarial attacks are able to significantly reduce the performance of the constructed models."
  - [section 2.4] Multiple attacks (FGSM, BIM, DeepFool, etc.) explicitly rely on gradient-based or gradient-free optimization to find perturbations.
  - [corpus] No direct evidence; related work focuses on adversarial attacks but not specific to SER spectrograms.
- Break condition: If model uses non-differentiable operations, defenses like adversarial training or input quantization, or if perturbations exceed human perceptibility thresholds.

### Mechanism 3
- Claim: The CNN-LSTM architecture generalizes across languages because emotional acoustic patterns are universal enough to be learned from spectrograms, despite linguistic differences.
- Mechanism: Shared CNN feature extractors learn language-agnostic spectral features (pitch, rhythm, intensity) while LSTMs model temporal dynamics. Multilingual training on log-Mel spectrograms allows transfer of emotional feature detectors.
- Core assumption: Emotional expression in speech has cross-linguistic acoustic correlates that are detectable in the spectral domain.
- Evidence anchors:
  - [abstract] "minor differences were noted between the languages analyzed" and evaluation across German, Italian, and English.
  - [section 2.2] Use of the same CNN-LSTM architecture across all three datasets without language-specific adaptation.
  - [corpus] Related work (EmoBox, cross-lingual SER) supports multilingual generalization but does not validate CNN-LSTM specifically.
- Break condition: If emotional expression is too culturally specific or if language-specific phonetic content interferes with emotional feature extraction.

## Foundational Learning

- Concept: Log-Mel spectrogram computation
  - Why needed here: Provides the input representation that the CNN-LSTM model processes; understanding its parameters (128 Mel bands, window size 368, hop size 184) is critical for interpreting attack perturbations.
  - Quick check question: Why use 128 Mel bands instead of raw FFT bins, and how does the window size affect temporal resolution?

- Concept: CNN-LSTM model architecture
  - Why needed here: The vulnerability to attacks depends on the model's depth, filter sizes, LSTM units, and normalization. Understanding the progression from M0 to M1 helps explain why certain attacks succeed.
  - Quick check question: How does changing from unidirectional to bidirectional LSTM affect the model's sensitivity to perturbations?

- Concept: Adversarial attack types (white-box vs. black-box)
  - Why needed here: Different attacks require different levels of access to the model (gradients vs. output only), and their effectiveness varies based on the attack method and distance metric used.
  - Quick check question: What is the practical difference between L0, L2, and L∞ perturbations in terms of human perceptibility and attack success?

## Architecture Onboarding

- Component map: 3-second log-Mel spectrograms (128x261 matrix) -> 3 Conv2D + MaxPooling blocks (filters: 16→32→64, kernel sizes decreasing) -> BatchNormalization -> 256 bidirectional LSTM units with internal dropout -> Dense layer with softmax over 5 emotion classes -> Categorical crossentropy loss

- Critical path:
  1. Spectrogram extraction -> normalization -> model input
  2. CNN feature extraction -> temporal modeling via LSTM
  3. Classification -> loss computation -> backpropagation for training or attack gradient computation

- Design tradeoffs:
  - Small CNN vs. deeper networks: Chosen for computational efficiency and to avoid overfitting on limited data
  - Bidirectional LSTM: Captures context better but increases parameters
  - Standardization vs. max normalization: Affects feature scaling and model stability

- Failure signatures:
  - High validation loss with low training loss: Overfitting
  - Slow convergence or oscillation: Learning rate too high or batch size too small
  - Attacks succeed easily: Model lacks adversarial robustness (no defensive training)

- First 3 experiments:
  1. Train base M0 model on one dataset (e.g., EmoDB) with different normalizations; compare accuracy and loss curves.
  2. Add BatchNormalization to M0 and measure impact on training stability and final accuracy.
  3. Implement FGSM attack on trained M0; vary eps parameter and observe accuracy degradation and perturbation magnitude.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the vulnerability of SER models to adversarial attacks depend more on the model architecture or the underlying speech data representation?
- Basis in paper: [inferred] The authors found that all tested attacks significantly degraded SER model performance regardless of language or gender, but noted differences in effectiveness across attack types and model configurations.
- Why unresolved: The study focused on a single CNN-LSTM architecture and log Mel-spectrograms, limiting the ability to isolate the impact of architectural choices versus data representation.
- What evidence would resolve it: Comparing attack effectiveness across multiple model architectures (e.g., CNN, LSTM, Transformer) and speech representations (e.g., raw waveforms, MFCCs, spectrograms) would clarify the relative contributions of architecture and data representation to vulnerability.

### Open Question 2
- Question: How do real-world noise conditions and speaker variability affect the transferability and effectiveness of adversarial examples in SER?
- Basis in paper: [inferred] The study used clean, controlled datasets and did not evaluate attacks under realistic conditions or with diverse speaker populations.
- Why unresolved: The experiments were conducted in an idealized setting without accounting for real-world factors that could influence attack success.
- What evidence would resolve it: Testing attacks on noisy, multi-speaker datasets with varying recording conditions would reveal how environmental and speaker factors impact adversarial example effectiveness.

### Open Question 3
- Question: Can adversarial training or other defense mechanisms effectively mitigate attacks on SER models without significantly degrading clean data performance?
- Basis in paper: [explicit] The authors suggest that exploring more robust models or alternative training data is needed to enhance system robustness, but do not investigate specific defense strategies.
- Why unresolved: The study focused on attack vulnerability but did not evaluate potential defenses or their impact on model performance.
- What evidence would resolve it: Implementing and evaluating defense mechanisms (e.g., adversarial training, input preprocessing, model ensemble) on SER models, while measuring both attack resilience and clean data accuracy, would clarify their practical viability.

## Limitations
- Limited dataset size and diversity: The study relies on three datasets (EmoDB, EMOVO, Ravdess) with modest sample counts, potentially limiting generalizability to other languages and emotional expressions.
- Attack implementation constraints: Adversarial attacks were conducted in controlled computational environments, but real-world deployment scenarios may involve additional factors (e.g., streaming audio, dynamic noise) not captured in the study.
- Black-box attack limitations: While BoundaryAttack and PixelAttack were tested, more sophisticated black-box methods were not explored, potentially underestimating real-world vulnerability.

## Confidence
- **High confidence**: The methodology for adversarial attack implementation using the ART library is standard and well-documented. The significant accuracy degradation across all attack types (1-20% from ~90% baseline) is consistent and reproducible.
- **Medium confidence**: Cross-linguistic vulnerability claims are based on three languages only, making broader generalizations tentative. The observed minor differences between languages and genders suggest some patterns but require more diverse validation.
- **Medium confidence**: The claim that JSMA and PixelAttack are "most effective" is based on the specific experimental conditions and parameter settings used. Different configurations might yield different results.

## Next Checks
1. **Dataset expansion validation**: Test the CNN-LSTM model and adversarial attacks on additional multilingual datasets (e.g., IEMOCAP, MSP-Improv) to verify cross-linguistic vulnerability patterns hold across more diverse emotional expressions and languages.
2. **Defense mechanism testing**: Implement and evaluate common adversarial defenses (adversarial training, input quantization, defensive distillation) to determine if they can mitigate the observed attack effectiveness without significantly degrading baseline accuracy.
3. **Real-world scenario simulation**: Conduct attacks on streaming audio segments and under various noise conditions to assess whether the observed vulnerability persists in more realistic deployment scenarios where attackers may have limited control over input preprocessing.