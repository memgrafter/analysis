---
ver: rpa2
title: Addressing Image Hallucination in Text-to-Image Generation through Factual
  Image Retrieval
arxiv_id: '2407.10683'
source_url: https://arxiv.org/abs/2407.10683
tags:
- image
- images
- hallucination
- factual
- retrieved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach to mitigate image hallucination
  in text-to-image generation models by leveraging factual image retrieval. It addresses
  the problem of factual inconsistency, outdated knowledge hallucination, and factual
  fabrication by retrieving real-world images that align with the input text prompts.
---

# Addressing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval

## Quick Facts
- **arXiv ID**: 2407.10683
- **Source URL**: https://arxiv.org/abs/2407.10683
- **Authors**: Youngsun Lim; Hyunjung Shim
- **Reference count**: 3
- **Key outcome**: Introduces retrieval-augmented approach to mitigate image hallucination by editing generated images with factual information from retrieved real-world images

## Executive Summary
This paper addresses the problem of image hallucination in text-to-image generation models by introducing a tuning-free method that leverages factual image retrieval. The approach identifies three types of hallucinations—factual inconsistency, outdated knowledge hallucination, and factual fabrication—and employs either InstructPix2Pix or IP-Adapter to correct them based on the nature of the hallucination. By retrieving real-world images that align with input text prompts, the method enhances the accuracy and trustworthiness of generated images without requiring model retraining. Experimental results demonstrate effectiveness in generating factually accurate images across various scenarios.

## Method Summary
The proposed method employs a two-stage approach: first generating an initial image using DALL-E 3 from a text prompt, then retrieving relevant factual images via Google Custom Search API. Depending on the type of hallucination, the method either uses InstructPix2Pix (for localized property corrections) or IP-Adapter (for complex area corrections) to edit the initial generation. GPT-4 generates editing instructions or factual prompts by comparing the initial image with retrieved factual images. The approach is tuning-free and relies on off-the-shelf image editing tools to incorporate external factual knowledge into the generation process.

## Key Results
- Successfully addresses three types of image hallucinations: factual inconsistency, outdated knowledge hallucination, and factual fabrication
- Leverages InstructPix2Pix for localized property corrections and IP-Adapter for complex area corrections
- Enhances factual accuracy of generated images by incorporating external knowledge from retrieved real-world images
- Demonstrates effectiveness through experimental results without requiring model retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using InstructPix2Pix with LLM-generated instructions corrects hallucinations localized to specific properties like color.
- Mechanism: Compares initially generated image with retrieved factual image, generates instructions based on differences, and applies these via InstructPix2Pix to modify hallucinated property.
- Core assumption: Hallucination affects a specific, localized property that can be corrected with targeted image editing instructions.
- Evidence anchors:
  - [abstract] "Depending on the type of hallucination, the method employs either InstructPix2Pix or IP-Adapter to edit the generated images using factual information from the retrieved images."
  - [section] "First, if a hallucination occurs in a specific property (e.g., the color of an object), we use the retrieved image to obtain instructions and apply InstructPix2Pix."
  - [corpus] Weak - No directly comparable mechanism in corpus; appears to be novel application of InstructPix2Pix for hallucination correction.
- Break condition: If hallucination affects multiple complex properties or covers wide area, this method cannot adequately address all inconsistencies.

### Mechanism 2
- Claim: Using IP-Adapter with factual image prompts corrects hallucinations affecting wide areas or complex subjects like people.
- Mechanism: Generates detailed textual descriptions of retrieved factual image, combines with image itself as prompts, and uses IP-Adapter to edit initial generation to reflect factual features.
- Core assumption: Hallucination requires comprehensive information from retrieved image that cannot be fully captured through text alone.
- Evidence anchors:
  - [abstract] "Depending on the nature of the hallucination, we employ off-the-shelf image editing tools, either InstructPix2Pix or IP-Adapter, to leverage factual information from the retrieved image."
  - [section] "On the other hand, if hallucination occurs in a broad area involving many complex subjects, such as a person... the retrieved image must be used as a prompt. For this purpose, we utilize the IP-Adapter."
  - [corpus] Weak - No direct evidence in corpus; appears to be novel application of IP-Adapter for hallucination correction.
- Break condition: If retrieved image is not sufficiently detailed or representative of desired factual information, method may not fully correct hallucination.

### Mechanism 3
- Claim: Retrieval-augmented factual text-to-image generation uses external knowledge to guide generation models toward factual accuracy.
- Mechanism: Retrieves relevant images based on input text prompt, allows user selection of most factual image, and uses this image to guide correction of hallucinations in initial generation.
- Core assumption: External factual images exist that can provide accurate guidance for correcting hallucination.
- Evidence anchors:
  - [abstract] "We develop this idea and introduce a tuning-free method to enhance the image generation model for producing fact-based images. Initially, an image is generated via an existing text-to-image model. Then, the input prompt is used as the search query to retrieve N images in order of relevance."
  - [section] "To search for factual information relevant to a given text prompt, we employ Google's Custom Search JSON API to retrieve images."
  - [corpus] Weak - While retrieval-augmented approaches exist in language models, their application to image hallucination correction appears novel based on corpus evidence.
- Break condition: If no relevant factual images can be retrieved, or if retrieved images do not accurately represent factual information needed, method cannot correct hallucination.

## Foundational Learning

- Concept: Diffusion models and their limitations in handling factual consistency
  - Why needed here: Understanding how diffusion models generate images and why they hallucinate is crucial for applying correction methods effectively
  - Quick check question: What are the primary reasons diffusion models fail to generate factually accurate images, and how do these relate to the three types of hallucinations addressed in this paper?

- Concept: Image editing techniques (InstructPix2Pix and IP-Adapter)
  - Why needed here: These tools are the core mechanisms for correcting hallucinations, and understanding their capabilities and limitations is essential
  - Quick check question: What are the key differences between InstructPix2Pix and IP-Adapter, and in what scenarios would each be most appropriate for hallucination correction?

- Concept: Retrieval systems and their integration with generative models
  - Why needed here: The retrieval component is critical for providing factual information to guide correction process
  - Quick check question: How does the retrieval system select relevant images, and what factors might influence the quality and relevance of retrieved images for hallucination correction?

## Architecture Onboarding

- Component map: Text prompt → DALL-E 3 → Google Custom Search API → User selection → GPT-4 → InstructPix2Pix/IP-Adapter → Factual image output

- Critical path: Text prompt → Image generation → Image retrieval → User selection → Instruction/prompt generation → Image editing → Factual image output

- Design tradeoffs:
  - Retrieval vs. training: This approach avoids model retraining costs but relies on availability of relevant factual images
  - User interaction vs. automation: Requiring user selection of factual images introduces manual effort but ensures alignment with user intentions
  - InstructPix2Pix vs. IP-Adapter: Choosing appropriate editing tool depends on nature and scope of hallucination

- Failure signatures:
  - No relevant images retrieved
  - Incorrect user selection of factual images
  - LLM-generated instructions/prompts fail to capture necessary corrections
  - InstructPix2Pix or IP-Adapter unable to fully correct complex hallucinations

- First 3 experiments:
  1. Test InstructPix2Pix pipeline with simple hallucination (e.g., color change) using prompt like "The Statue of Liberty in 1890"
  2. Test IP-Adapter pipeline with complex hallucination (e.g., person depiction) using prompt like "The chancellor of Germany in 2015"
  3. Test full pipeline with factual fabrication scenario using prompt like "The Golden Gate Bridge in winter"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the factual image retrieval method scale with the number of retrieved images (N)?
- Basis in paper: [explicit] The paper mentions that number of images retrieved per prompt is a hyperparameter that can be adjusted based on individual requirements.
- Why unresolved: The paper does not provide experimental results or analysis on how varying number of retrieved images impacts effectiveness of method in reducing image hallucination.
- What evidence would resolve it: Conducting experiments with different values of N and comparing resulting image quality and factual accuracy would provide insights into optimal number of retrieved images for different types of hallucinations.

### Open Question 2
- Question: Can the proposed method effectively address hallucinations that involve complex spatial relationships or interactions between multiple objects?
- Basis in paper: [inferred] The paper mentions that IP-Adapter can process both image and text prompts, but it does not explicitly discuss its effectiveness in handling complex spatial relationships or interactions between objects.
- Why unresolved: The paper does not provide examples or experimental results demonstrating method's ability to address hallucinations involving complex spatial relationships or interactions between multiple objects.
- What evidence would resolve it: Generating images with prompts that involve complex spatial relationships or interactions between multiple objects and evaluating resulting images for factual accuracy and spatial coherence would provide evidence of method's effectiveness in this regard.

### Open Question 3
- Question: How does the proposed method compare to other approaches for addressing image hallucination, such as fine-tuning the text-to-image generation model on a dataset of factually accurate images?
- Basis in paper: [inferred] The paper mentions that proposed method does not require any training, unlike some other approaches, but it does not directly compare its performance to those methods.
- Why unresolved: The paper does not provide a comparison of proposed method's performance to other approaches for addressing image hallucination, such as fine-tuning the text-to-image generation model on a dataset of factually accurate images.
- What evidence would resolve it: Conducting experiments comparing proposed method to other approaches, such as fine-tuning the text-to-image generation model on a dataset of factually accurate images, would provide insights into relative effectiveness and efficiency of different methods.

## Limitations

- The effectiveness depends heavily on availability and quality of relevant factual images in retrieval system, which is not guaranteed for all prompts
- User selection of factual images introduces subjectivity and potential human error into the pipeline
- The approach assumes retrieved images accurately represent desired factual information, which may not always be true

## Confidence

- **High confidence**: The core premise that factual image retrieval can help correct image hallucinations in text-to-image generation is well-founded and logically consistent
- **Medium confidence**: The specific implementation using InstructPix2Pix and IP-Adapter for different types of hallucinations appears sound but lacks extensive validation across diverse scenarios
- **Low confidence**: Claims about method's effectiveness across all three hallucination types (factual inconsistency, outdated knowledge, and factual fabrication) require more comprehensive empirical evaluation

## Next Checks

1. **Retrieval reliability test**: Systematically evaluate success rate of retrieving relevant factual images across diverse prompt categories, measuring both precision and recall of retrieval component

2. **User selection variability study**: Assess consistency of user selections when presented with multiple retrieved images, and measure how user variability affects final output quality

3. **Scalability assessment**: Test method's performance on prompts requiring retrieval of multiple factual images simultaneously (e.g., complex scenes with multiple objects) to evaluate scalability limitations