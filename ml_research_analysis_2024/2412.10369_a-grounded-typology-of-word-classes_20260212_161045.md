---
ver: rpa2
title: A Grounded Typology of Word Classes
arxiv_id: '2412.10369'
source_url: https://arxiv.org/abs/2412.10369
tags:
- uni00000049
- uni00000032
- uni00000034
- uni00000045
- uni00000056
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a grounded approach to linguistic typology,
  using images as a language-agnostic representation of meaning to quantify the semantic
  contentfulness of word classes across 30 languages. The method defines "groundedness"
  as the difference in surprisal between a word token under a language model and an
  image captioning model, measuring how much more expected a word is when the image
  is known.
---

# A Grounded Typology of Word Classes

## Quick Facts
- arXiv ID: 2412.10369
- Source URL: https://arxiv.org/abs/2412.10369
- Reference count: 14
- Primary result: Introduces a grounded approach to linguistic typology using images as language-agnostic meaning representations to quantify semantic contentfulness of word classes across 30 languages.

## Executive Summary
This study introduces a grounded approach to linguistic typology by quantifying the semantic contentfulness of word classes using images as language-agnostic meaning representations. The method defines "groundedness" as the difference in surprisal between a word token under a language model and an image captioning model, measuring how much more expected a word is when the image is known. Applied to word classes across 30 languages, the measure captures the lexical-functional distinction (e.g., nouns > adjectives > verbs) and shows consistent cross-linguistic patterns, though it reveals that functional classes carry more semantic content than traditionally assumed. Results also show that groundedness correlates weakly but significantly with psycholinguistic concreteness norms in English, particularly when normalized for informativity.

## Method Summary
The groundedness measure quantifies semantic contentfulness by calculating the pointwise mutual information (PMI) between word tokens and images. An image captioning model (PaliGemma) provides p(wt | m, w<t) while a language model (fine-tuned Gemma) provides p(wt | w<t), with groundedness defined as their surprisal difference. Mutual information between word classes and images is estimated using Monte Carlo sampling across three datasets: Crossmodal-3600, COCO-35L validation set, and Multi30K. POS tags from Universal Dependencies ensure cross-linguistic comparability. The language model is fine-tuned on COCO captions to match the captioning model's domain, and groundedness is aggregated by POS class to reveal semantic contentfulness hierarchies.

## Key Results
- Word class groundedness follows a consistent hierarchy across languages: nouns > adjectives > verbs, capturing the lexical-functional distinction
- Functional word classes carry more semantic content than traditionally assumed, challenging conventional typological assumptions
- Normalized groundedness (uncertainty coefficient) correlates more strongly with English concreteness norms (ρ = 0.609) than raw groundedness (ρ = 0.368)
- Cross-linguistic consistency in groundedness patterns suggests universal semantic contentfulness clines despite language-specific variations

## Why This Works (Mechanism)

### Mechanism 1
Groundedness captures semantic contentfulness by comparing surprisal under image-conditioned captioning vs. text-only language modeling. PMI between a word token and image representation forms the core measure: groundedness = LM surprisal - captioning model surprisal. Image serves as language-agnostic meaning proxy, with both models trained on same data to ensure comparable surprisal scales. Evidence: abstract definition as "difference in surprisal", section explanation of word being "less surprising when we know the perceptual stimuli", and weak correlation with concreteness norms suggesting related but distinct dimensions. Break condition: if image captioning model trained on non-overlapping data, surprisal differences may reflect domain bias rather than semantic informativeness.

### Mechanism 2
Word class groundedness is consistent across languages because underlying semantic contentfulness cline is universal. Estimates MI between POS tag and image, controlling for language-specific factors; lexical classes show higher groundedness than functional classes. Core assumption: POS tags are language-neutral and independent of image given context; images are comparable across languages. Evidence: abstract finding of "distinction between lexical and functional word classes across 30 languages", section results showing "universal trends in the hierarchy of groundedness (e.g., nouns > adjectives > verbs)", and small dataset effect variance suggesting word class more critical than dataset choice. Break condition: if POS tagging inconsistent across languages, MI estimates may reflect tagging noise rather than semantic differences.

### Mechanism 3
Normalized groundedness (uncertainty coefficient) correlates more strongly with human concreteness norms than raw groundedness. Divides PMI by LM surprisal to isolate informativeness from absolute surprisal differences. Core assumption: human concreteness ratings are less sensitive to informativeness; normalization removes baseline informativity bias. Evidence: abstract presentation of "intuitive ranking (noun > adjectives > verbs) across languages", section regression showing "stronger correlations emerge (Imagability: ρ = 0.548, Concreteness: ρ = 0.609)", and raw groundedness correlating weakly (ρ=0.368) with concreteness while normalization improves correlation. Break condition: if informativity is major factor in human concreteness ratings, normalization may over-correct and obscure true semantic contentfulness.

## Foundational Learning

- Concept: Pointwise mutual information (PMI)
  - Why needed here: PMI quantifies the association between a word and image, forming the core of groundedness.
  - Quick check question: If a word's PMI with an image is zero, what does that imply about its groundedness?

- Concept: Language-agnostic meaning representation
  - Why needed here: Images serve as cross-linguistic semantic anchors; without this, MI estimates would be incomparable.
  - Quick check question: Why might videos be a better meaning representation for verbs than images?

- Concept: Domain adaptation of language models
  - Why needed here: Language model must match image captioning domain to avoid domain-specific surprisal bias.
  - Quick check question: What happens to groundedness estimates if the language model is trained on general web text instead of COCO captions?

## Architecture Onboarding

- Component map: Image + caption → captioning model → LM → PMI → POS aggregation → MI estimate
- Critical path: Image captioning model (PaliGemma) provides p(wt | m, w<t); language model (fine-tuned Gemma) provides p(wt | w<t); PMI calculation; POS tagger (Stanza) assigns word classes; mutual information estimator aggregates by POS
- Design tradeoffs:
  - Using images limits analysis to visually describable language; videos would improve verb grounding but are harder to model.
  - Fine-tuning language model on COCO captions improves comparability but may reduce general language modeling ability.
  - POS tagging via UD tags ensures cross-linguistic comparability but may introduce noise from inconsistent tagging.
- Failure signatures:
  - If captioning model overfits COCO, groundedness may be inflated for common caption words.
  - If POS tagger accuracy varies across languages, cross-linguistic MI comparisons may be unreliable.
  - If images are not truly language-agnostic (e.g., culturally biased), MI estimates may reflect cultural differences rather than semantic contentfulness.
- First 3 experiments:
  1. Compare groundedness distributions for high vs. low concreteness English words to validate semantic dimension.
  2. Test sensitivity of groundedness to different image captioning models (e.g., smaller PaliGemma vs. fine-tuned BLIP).
  3. Measure cross-linguistic consistency of groundedness for shared words (e.g., "woman") across COCO-35L languages.

## Open Questions the Paper Calls Out

### Open Question 1
Do boundary classes like gerunds and participles display intermediate groundedness between prototypical nouns and verbs, as hypothesized? The authors conjecture that boundary classes may show intermediate groundedness compared to prototypical members. This remains unresolved because the current study focuses on prototypical word classes and does not investigate non-prototypical categories or syncretic forms. Evidence would require analyzing the groundedness of gerunds, participles, and other syncretic forms across languages and comparing their scores to prototypical nouns and verbs.

### Open Question 2
How does the choice of meaning representation (e.g., images vs. videos) affect the groundedness of word classes, particularly verbs? The authors acknowledge that using images as a meaning representation has implications for verbs, which tend to denote temporally extended events. This remains unresolved because the study uses images due to model availability, but the impact on results is not fully explored. Evidence would require replicating the study using videos or other meaning representations and comparing the groundedness scores of word classes, especially verbs.

### Open Question 3
How do cross-linguistic differences in the assignment of Universal Dependencies POS tags impact the groundedness estimates of word classes? The authors acknowledge that Stanza tagger accuracy varies across languages and that UD POS tags are not without controversy. This remains unresolved because the study relies on automatic POS tagging, which may introduce noise and inconsistencies. Evidence would require manually verifying POS tags for a subset of data and recomputing groundedness estimates, or comparing results using different POS tagging systems.

## Limitations

- The groundedness measure relies on visual descriptions, potentially underrepresenting abstract or non-visual semantic dimensions (e.g., emotions, logical relations).
- Cross-linguistic POS tag consistency across Universal Dependencies may not fully capture language-specific word class distinctions.
- The weak correlation with English concreteness norms (r=0.368 raw, r=0.609 normalized) suggests the measure captures related but not identical semantic dimensions.

## Confidence

- High: The methodology for calculating groundedness (PMI between word and image representations) is sound and well-defined.
- Medium: Cross-linguistic patterns in word class groundedness are consistent but may reflect POS tagging artifacts in some languages.
- Medium: Correlation with concreteness norms is statistically significant but explains limited variance, suggesting the measure captures broader semantic contentfulness.

## Next Checks

1. Test groundedness sensitivity to different image representations (e.g., CLIP vs. PaliGemma) to validate robustness to model choice.
2. Compare groundedness patterns for translation-equivalent word classes across languages to assess cross-linguistic semantic alignment.
3. Evaluate whether groundedness predicts human performance in word-picture verification tasks beyond correlation with concreteness norms.