---
ver: rpa2
title: Evaluating ChatGPT on Nuclear Domain-Specific Data
arxiv_id: '2409.00090'
source_url: https://arxiv.org/abs/2409.00090
tags:
- nuclear
- llms
- responses
- power
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluated ChatGPT performance on nuclear domain-specific
  data using two approaches: direct responses and Retrieval Augmented Generation (RAG).
  A test dataset was curated from the Essential CANDU textbook, and responses were
  evaluated by human assessors and GPT-4.'
---

# Evaluating ChatGPT on Nuclear Domain-Specific Data

## Quick Facts
- arXiv ID: 2409.00090
- Source URL: https://arxiv.org/abs/2409.00090
- Authors: Muhammad Anwar; Mischa de Costa; Issam Hammad; Daniel Lau
- Reference count: 0
- Key outcome: RAG-generated responses significantly outperformed direct ChatGPT responses in correctness, groundedness, helpfulness, and conciseness when evaluated on nuclear domain-specific data from Essential CANDU textbook.

## Executive Summary
This paper evaluates ChatGPT performance on nuclear domain-specific data using two approaches: direct responses and Retrieval Augmented Generation (RAG). A test dataset was curated from the Essential CANDU textbook, and responses were evaluated by human assessors and GPT-4. RAG-generated responses significantly outperformed direct ChatGPT responses, with higher correctness, groundedness, helpfulness, and conciseness scores. The results demonstrate that incorporating RAG improves LLM accuracy and relevance in specialized domains by providing targeted context from external knowledge bases.

## Method Summary
The study compared ChatGPT-3.5 responses generated directly versus through a RAG pipeline using the Essential CANDU textbook as the knowledge base. The RAG pipeline embedded and indexed textbook chunks using OpenAI's ada-002 embeddings and FAISS vector search, retrieved relevant passages for each query, and augmented prompts with explicit constraints to restrict responses to provided context. Responses were evaluated on correctness, groundedness, helpfulness, conciseness, coherence, and detail using both human assessment and GPT-4 as an LLM judge.

## Key Results
- RAG responses achieved significantly higher correctness scores compared to direct ChatGPT responses
- RAG responses demonstrated superior groundedness by restricting answers to provided context
- RAG responses showed higher consistency with scores tightly clustered between 90-100
- RAG responses were more concise while maintaining helpfulness and detail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG outperforms direct LLM responses by providing targeted, domain-specific context at inference time.
- Mechanism: The RAG pipeline retrieves relevant passages from the Essential CANDU textbook and injects them into the prompt, constraining the LLM to answer based solely on this context.
- Core assumption: The retrieved context is both semantically relevant and factually aligned with the ground truth.
- Evidence anchors: Abstract states RAG responses "significantly outperformed" direct responses; section explains context augmentation increases "probability of a factually grounded response."

### Mechanism 2
- Claim: The RAG pipeline reduces hallucination by instructing the LLM to use only provided context.
- Mechanism: The prompt template explicitly states "Answer the question based only on the following context" and "Please do not use any external information."
- Core assumption: The LLM will obey the instruction to restrict itself to the supplied context.
- Evidence anchors: Section includes explicit prompt constraints; abstract notes LLMs are "prone to generating incorrect or 'hallucinated' information."

### Mechanism 3
- Claim: RAG increases consistency by anchoring responses to a fixed knowledge source.
- Mechanism: The vector index built from the textbook ensures that all retrieved chunks are drawn from the same vetted corpus.
- Core assumption: The vector embeddings capture semantic similarity accurately and the FAISS index retrieves the most relevant chunks.
- Evidence anchors: Section describes FAISS vector indexing; abstract notes RAG responses show "remarkable consistency" with scores clustered between 90-100.

## Foundational Learning

- Concept: Vector embeddings and cosine similarity for semantic search.
  - Why needed here: The retrieval step depends on converting text chunks into numerical vectors and measuring similarity to the query.
  - Quick check question: If two passages have a cosine similarity of 0.95, are they more or less related than passages with 0.60?

- Concept: Prompt engineering with explicit constraints.
  - Why needed here: The LLM must be instructed to answer solely from provided context to avoid hallucinations.
  - Quick check question: What happens if the prompt omits the phrase "Please do not use any external information"?

- Concept: Evaluation rubric design for multi-dimensional scoring.
  - Why needed here: The GPT-4 judge uses weighted criteria (correctness, groundedness, helpfulness, etc.) to compare RAG vs. non-RAG outputs.
  - Quick check question: If correctness scores are tied but conciseness differs, which response should the judge prefer according to the prompt?

## Architecture Onboarding

- Component map: Essential CANDU textbook → Chunked → Embedded → Stored in FAISS vector index → Retrieved chunks → Augmented prompt → ChatGPT-3.5 → Response → Evaluation

- Critical path: 1. Embed and index textbook chunks 2. Retrieve top-k chunks for a query 3. Construct augmented prompt with constraints 4. Generate response and evaluate

- Design tradeoffs:
  - Chunk size vs. semantic coherence: Too small → context loss; too large → retrieval noise
  - Number of retrieved chunks vs. prompt length: More context → better grounding but risks exceeding token limits
  - Embedding model choice: General-purpose (ada-002) vs. domain-tuned embeddings for nuclear jargon

- Failure signatures:
  - Inconsistent scores across queries → retrieval relevance issues
  - Factual errors persisting in RAG responses → context misalignment or prompt instruction leakage
  - High variance in GPT-4 evaluations → metric definition ambiguity or evaluator bias

- First 3 experiments:
  1. Swap cosine similarity for BM25 in retrieval; compare correctness scores
  2. Vary chunk size (50, 200, 500 tokens); measure coherence and conciseness trade-off
  3. Replace ChatGPT-3.5 with GPT-4 in the RAG pipeline; evaluate change in groundedness and detail

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAG-generated responses compare to fine-tuned LLMs on nuclear domain-specific data?
- Basis in paper: The paper discusses both RAG and fine-tuning as methods to improve LLM performance in specialized domains.
- Why unresolved: The paper only evaluates RAG against direct ChatGPT responses, not against fine-tuned models.
- What evidence would resolve it: A comparative study evaluating RAG-generated responses against responses from a fine-tuned LLM on the same nuclear domain-specific dataset.

### Open Question 2
- Question: What is the optimal chunk size for context-aware chunking in the information retrieval step of the RAG pipeline for nuclear domain-specific data?
- Basis in paper: The paper mentions context-aware chunking as a technique to improve vector embeddings in the RAG pipeline.
- Why unresolved: The paper does not specify the chunk size used or explore the impact of different chunk sizes on retrieval performance.
- What evidence would resolve it: An ablation study testing various chunk sizes and their effect on retrieval accuracy and response quality in the nuclear domain.

### Open Question 3
- Question: How does the cost of implementing RAG compare to the cost of fine-tuning an LLM for nuclear domain-specific applications?
- Basis in paper: The paper discusses the high cost of training LLMs and mentions fine-tuning as a potential alternative.
- Why unresolved: The paper does not provide a cost analysis comparing RAG implementation to fine-tuning.
- What evidence would resolve it: A detailed cost analysis comparing the computational resources, time, and expenses required for RAG implementation versus fine-tuning an LLM for nuclear domain-specific tasks.

## Limitations

- Small, single-source dataset (Essential CANDU textbook) limits generalizability
- Absence of ablation studies on critical hyperparameters like chunk size and retrieval thresholds
- Reliance on GPT-4 as judge introduces potential evaluator bias
- No statistical significance testing reported between RAG and direct response scores

## Confidence

- High confidence in the RAG pipeline implementation and retrieval mechanism
- Medium confidence in the evaluation methodology and metric definitions
- Low confidence in generalizability across nuclear subdomains and alternative knowledge sources

## Next Checks

1. Conduct ablation studies varying chunk size (50, 200, 500 tokens) and retrieval k-values to quantify their impact on correctness and conciseness scores.

2. Replicate the evaluation with a second, independent nuclear domain corpus to test generalizability of RAG advantages.

3. Perform statistical significance testing (e.g., paired t-tests) on the difference between RAG and direct response scores across all metrics.