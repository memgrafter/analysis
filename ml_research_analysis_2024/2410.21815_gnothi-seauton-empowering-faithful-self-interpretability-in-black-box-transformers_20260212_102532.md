---
ver: rpa2
title: 'Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers'
arxiv_id: '2410.21815'
source_url: https://arxiv.org/abs/2410.21815
tags:
- autognothi
- attention
- covert
- explanation
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoGnothi, a parameter-efficient transfer
  learning approach that enables black-box transformer models to generate self-interpretable
  Shapley value explanations without modifying the original model parameters. The
  method uses a lightweight side network that learns to explain the frozen backbone
  model, achieving comparable explanation quality to full fine-tuning while reducing
  trainable parameters by 87-98% and GPU memory usage by 65-77% during training.
---

# Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers

## Quick Facts
- arXiv ID: 2410.21815
- Source URL: https://arxiv.org/abs/2410.21815
- Authors: Shaobo Wang; Hongxuan Tang; Mingyang Wang; Hongxuan Zhang; Xuyang Liu; Weiya Li; Xuming Hu; Linfeng Zhang
- Reference count: 40
- This paper introduces AutoGnothi, a parameter-efficient transfer learning approach that enables black-box transformer models to generate self-interpretable Shapley value explanations without modifying the original model parameters.

## Executive Summary
AutoGnothi addresses the computational burden of traditional post-hoc explanation methods by enabling black-box transformer models to generate self-interpretable Shapley value explanations. The method uses a lightweight side network that learns to explain a frozen backbone model, achieving comparable explanation quality to full fine-tuning while reducing trainable parameters by 87-98% and GPU memory usage by 65-77% during training. AutoGnothi also improves inference efficiency by 45-54% in FLOPs and 22-44% in time by generating predictions and explanations simultaneously rather than requiring separate inference passes.

## Method Summary
AutoGnothi employs a two-stage parameter-efficient transfer learning pipeline where a frozen transformer backbone is augmented with lightweight side networks for both prediction and explanation generation. The method uses additive side-tuning with a reduction factor to minimize trainable parameters while maintaining explanation quality. During training, a surrogate model learns to handle masked inputs, followed by an explainer model that generates Shapley value attributions. Both models share the same frozen backbone, enabling simultaneous prediction and explanation generation in a single inference pass.

## Key Results
- Achieves parameter reduction of 87-98% compared to full fine-tuning while maintaining explanation quality
- Reduces GPU memory usage by 65-77% during training
- Improves inference efficiency by 45-54% in FLOPs and 22-44% in time through simultaneous prediction and explanation generation
- Outperforms 12 baseline methods on insertion and deletion metrics for explanations on vision (ImageNette, Oxford-IIIT Pets, MURA) and language (Yelp Review) tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AutoGnothi achieves faithful self-interpretability by freezing the backbone transformer and training a lightweight side network to generate Shapley value explanations.
- Mechanism: The additive side network computes explanations in parallel with the frozen backbone, leveraging shared features between prediction and explanation tasks while avoiding interference with the original model parameters.
- Core assumption: The features extracted by the backbone for prediction are sufficiently correlated with those needed for generating explanations, allowing the side network to learn meaningful attributions without modifying the backbone.
- Evidence anchors:
  - [abstract] "introduces AutoGnothi, a parameter-efficient transfer learning approach that enables black-box transformer models to generate self-interpretable Shapley value explanations without modifying the original model parameters"
  - [section 4.1.2] "AutoGnothi exhibits higher feature similarity between prediction and explanation tasks on ViT-base and BERT-base models, supporting our hypothesis"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism, though related work on side-tuning exists
- Break condition: If the feature correlation between prediction and explanation tasks is weak or task-specific, the side network cannot learn meaningful explanations from frozen features.

### Mechanism 2
- Claim: AutoGnothi achieves computational efficiency by generating predictions and explanations simultaneously in a single inference pass.
- Mechanism: The side network architecture allows both tasks to share the same forward pass through the frozen backbone, eliminating the need for separate inference calls required by post-hoc methods.
- Core assumption: The side network can be designed to produce both outputs without significantly increasing the computational complexity of the forward pass.
- Evidence anchors:
  - [abstract] "AutoGnothi also improves inference efficiency by 45-54% in FLOPs and 22-44% in time by generating predictions and explanations simultaneously rather than requiring separate inference passes"
  - [section 4.1.3] "AutoGnothi generates both predictions and explanations simultaneously, needing only one inference"
  - [corpus] Moderate - corpus contains related work on efficient inference but not this specific simultaneous generation approach
- Break condition: If the explanation generation requires significantly different computation paths that cannot be efficiently combined, the single-pass advantage disappears.

### Mechanism 3
- Claim: AutoGnothi maintains explanation quality while reducing parameters through the use of Shapley value estimation with efficiency constraints.
- Mechanism: The explainer side network is trained to approximate Shapley values using the surrogate model outputs, with efficiency constraints ensuring faithful attributions that sum to the model's prediction.
- Core assumption: The efficiency constraint in Shapley value estimation is critical for faithful explanations, and the side network can learn this constrained optimization effectively.
- Evidence anchors:
  - [abstract] "AutoGnothi achieves superior insertion and deletion metrics for explanations compared to 12 baseline methods"
  - [section 4.1.2] "Theorem 2 provides a theoretical guarantee that a side-tuned explainer can achieve performance on par with a fully trained explainer"
  - [corpus] Weak - corpus lacks direct evidence for this specific Shapley value side-tuning approach
- Break condition: If the efficiency constraint cannot be properly enforced in the side network architecture, the explanations become unfaithful and unreliable.

## Foundational Learning

- Concept: Shapley values and their efficiency constraint
  - Why needed here: AutoGnothi specifically generates Shapley value explanations, and understanding why the efficiency constraint matters is crucial for interpreting results
  - Quick check question: Why must Shapley values satisfy the efficiency axiom, and what happens if this constraint is violated in explanations?

- Concept: Parameter-efficient transfer learning (PETL) and side-tuning
  - Why needed here: AutoGnothi builds on PETL concepts, specifically using side-tuning to add minimal trainable parameters while freezing the backbone
  - Quick check question: How does side-tuning differ from adapters or LoRA in terms of architecture and parameter efficiency?

- Concept: Transformer attention mechanisms and masking
  - Why needed here: The surrogate model uses causal attention masking to handle partial inputs, and understanding this is essential for implementing the pipeline
  - Quick check question: How does causal attention masking work in transformers, and why is it necessary for generating masked inputs in Shapley value computation?

## Architecture Onboarding

- Component map:
  Frozen backbone transformer (ViT/BERT) -> Surrogate side network -> Explainer side network -> Classification head + Explanation head

- Critical path:
  1. Input passes through frozen backbone
  2. Surrogate side network processes masked versions for training
  3. Explainer side network generates explanations using backbone features
  4. Both predictions and explanations produced in single forward pass

- Design tradeoffs:
  - Reduction factor r vs. explanation quality: Smaller r (larger side network) improves quality but reduces parameter efficiency
  - Number of explanation head layers vs. computation: More layers improve explanation quality but increase inference cost
  - Frozen backbone vs. fine-tuning: Freezing preserves original model identity but may limit explanation quality if features aren't transferable

- Failure signatures:
  - Explanation quality drops significantly with higher reduction factors
  - Training instability when efficiency constraint cannot be satisfied
  - Memory issues when side network dimensions don't properly align with backbone

- First 3 experiments:
  1. Implement basic side-tuning with r=8 on a small dataset (ImageNette) to verify the architecture works
  2. Compare explanation quality metrics (insertion/deletion) against a baseline method like GradCAM
  3. Measure inference efficiency gains by timing single vs. dual inference passes on the same hardware

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several important avenues for future research unexplored.

## Limitations

- Limited to classification tasks with ViT and BERT architectures, leaving generalization to other transformer models and task types uncertain
- The reduction factor r=8 is empirically chosen without theoretical justification for why this specific value provides optimal tradeoffs
- Does not investigate how AutoGnothi affects model robustness to adversarial examples compared to the original black-box model

## Confidence

- **High Confidence**: Computational efficiency claims (parameter reduction, memory savings, inference speedup) are well-supported by systematic measurements across multiple datasets and models
- **Medium Confidence**: Explanation quality metrics show strong performance compared to baselines, but superiority depends on specific evaluation setup and baseline selection
- **Low Confidence**: General applicability to arbitrary black-box transformers beyond tested ViT and BERT architectures, and performance on tasks outside classification

## Next Checks

1. Cross-domain validation: Test AutoGnothi on non-classification tasks (regression, generative modeling) to verify the feature correlation assumption holds beyond the current experimental scope
2. Architecture generalization: Apply the method to other transformer architectures (e.g., GPT, RoBERTa, Swin) to assess robustness across different model families and pretraining objectives
3. Ablation study on reduction factor: Systematically vary the reduction factor r to quantify the tradeoff between parameter efficiency and explanation quality, establishing guidelines for optimal configuration across different use cases