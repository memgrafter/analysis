---
ver: rpa2
title: 'Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from
  Large Language Models'
arxiv_id: '2402.04614'
source_url: https://arxiv.org/abs/2402.04614
tags:
- explanations
- llms
- faithfulness
- reasoning
- plausibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper discusses the dichotomy between faithfulness and plausibility
  in self-explanations (SEs) generated by large language models (LLMs). While LLMs
  can generate plausible explanations that are coherent and logical to human users,
  these explanations may not accurately reflect the model's actual reasoning process,
  raising concerns about their faithfulness.
---

# Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models

## Quick Facts
- arXiv ID: 2402.04614
- Source URL: https://arxiv.org/abs/2402.04614
- Reference count: 9
- Key outcome: LLM self-explanations are often plausible but unfaithful, raising concerns for high-stakes applications

## Executive Summary
This paper examines the critical dichotomy between faithfulness and plausibility in self-explanations generated by large language models. While LLMs can produce explanations that appear logical and coherent to human users, these explanations often fail to accurately reflect the model's actual reasoning process. The authors argue that current LLM development trends prioritize plausibility over faithfulness, potentially compromising reliability in high-stakes decision-making contexts. They call for a systematic characterization of faithfulness-plausibility requirements tailored to different real-world applications and the development of methods to ensure explanations meet those specific needs.

## Method Summary
The paper employs a multi-faceted approach to investigate the faithfulness-plausibility tradeoff. Using GPT-3.5 as a testbed, the authors generate self-explanations through various methods including Chain-of-Thought, Token Importance, and Counterfactual approaches. They then apply perturbation tests by swapping irrelevant answer options and modifying important/unimportant features to measure explanation consistency. Additional experiments involve truncating explanations midway to assess post-hoc reasoning patterns and intervening on explanations to test their causal relationship with answers. The analysis draws on multiple datasets and employs both qualitative and quantitative evaluation methods.

## Key Results
- LLM-generated explanations are often plausible but unfaithful, with plausibility sometimes coming at the expense of faithfulness
- Faithfulness can be measured through perturbation tests showing explanations change when irrelevant features are swapped but answers remain constant
- Different application domains require different balances between faithfulness and plausibility, with high-stakes domains needing more faithful explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate plausible explanations by mimicking human reasoning patterns learned during pretraining and RLHF fine-tuning
- Mechanism: During pretraining, LLMs absorb common reasoning structures from human-written text. RLHF further reinforces generating coherent responses that appeal to human evaluators, effectively training the model to prioritize plausibility over faithful reflection of internal reasoning
- Core assumption: The model's training objective does not incentivize generating explanations that accurately trace internal decision-making
- Evidence anchors:
  - [abstract]: "While LLMs are adept at generating plausible explanations – seemingly logical and coherent to human users – these explanations do not necessarily align with the reasoning processes of the LLMs"
  - [section]: "RLHF rewards responses that are simply coherent to a human evaluator, which is effectively equivalent to optimizing for plausibility"
- Break condition: If evaluation metrics shift to prioritize faithfulness, or if the model is explicitly trained to generate faithful rather than merely plausible explanations

### Mechanism 2
- Claim: Plausible explanations may be misleading because they are often post-hoc rationalizations rather than true reflections of the model's reasoning process
- Mechanism: The model first arrives at an answer (often via pattern matching or internal heuristics), then generates a coherent explanation that aligns with the answer but may not reflect the true decision path. This is evidenced by experiments showing that explanations change when irrelevant features are swapped, yet the answer remains consistent
- Core assumption: The explanation generation process can operate independently of the decision-making process, leading to divergence between explanation content and actual reasoning
- Evidence anchors:
  - [section]: "Turpin et al. (2023) simulates counterfactual inputs to show that CoT explanations are unfaithful and they misrepresent the true reasoning behind a LLM’s prediction"
  - [section]: "Lanham et al. (2023) uses the amount of post-hoc reasoning as an indicator of faithfulness"
- Break condition: If interventions on the explanation consistently change the answer, indicating that the explanation is tightly coupled to the reasoning process

### Mechanism 3
- Claim: Different downstream applications require different balances between plausibility and faithfulness, and current LLM design does not adapt to these needs
- Mechanism: High-stakes applications (healthcare, legal, finance) require faithful explanations to ensure decisions are traceable and safe, while educational or conversational applications may prioritize plausibility for user engagement. The paper argues that the current trend toward maximizing plausibility is inappropriate for all contexts
- Core assumption: Application requirements are not encoded into the explanation generation process, leading to a one-size-fits-all approach
- Evidence anchors:
  - [abstract]: "We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making"
  - [section]: "As illustrated in Fig. 6, in critical domains such as healthcare, finance, and legal, faithfulness is not only beneficial but also a fundamental requirement"
- Break condition: If models can dynamically adjust explanation properties based on application context and user needs

## Foundational Learning

- Concept: Distinction between plausibility and faithfulness in model explanations
  - Why needed here: The paper hinges on understanding that these are separate properties, and conflating them leads to unreliable explanations
  - Quick check question: Can an explanation be both plausible and unfaithful? Provide an example from the paper

- Concept: Self-explanation generation methods (Chain-of-Thought, Token Importance, Counterfactual)
  - Why needed here: The paper evaluates these methods for their faithfulness and plausibility, so understanding their mechanics is essential
  - Quick check question: How does perturbing unimportant features help measure the faithfulness of self-explanations?

- Concept: Evaluation of explanation faithfulness via counterfactual interventions
  - Why needed here: The paper proposes methods to test faithfulness by altering inputs and observing explanation consistency
  - Quick check question: What does it mean if swapping answer options changes the explanation but not the answer?

## Architecture Onboarding

- Component map: Input Prompt → LLM Core → Answer Generation → Explanation Generation (Chain-of-Thought/Token Importance/Counterfactual) → Evaluation Module: Perturbation Testing, Post-hoc Reasoning Analysis → Application Context Module: High-stakes vs. Educational/Conversational

- Critical path:
  1. Receive input prompt
  2. Generate answer via internal reasoning (opaque)
  3. Generate explanation based on answer
  4. Evaluate explanation faithfulness using perturbation tests
  5. Adjust explanation properties based on application needs

- Design tradeoffs:
  - Plausibility vs. Faithfulness: Optimizing for human appeal may reduce accuracy of internal reasoning reflection
  - Computational Cost: Generating faithful explanations may require additional reasoning steps or mechanistic interpretability tools
  - Application Specificity: Different domains may require different explanation styles, complicating unified model design

- Failure signatures:
  - Explanation changes when irrelevant features are perturbed but answer remains same
  - High consistency in answers despite explanation errors or inconsistencies
  - User over-reliance on plausible but unfaithful explanations in high-stakes contexts

- First 3 experiments:
  1. Perturbation Test: Swap irrelevant answer options and observe explanation changes while answer remains constant
  2. Post-hoc Reasoning Test: Truncate explanation mid-way and check if answer changes, measuring area over curve of consistency
  3. Counterfactual Input Test: Alter important features and measure fraction of predictions that change vs. remain same

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop reliable metrics to quantify the faithfulness of self-explanations generated by LLMs, given the current lack of universally agreed-upon metrics?
- Basis in paper: [explicit] The paper highlights that there are no agreed-upon metrics to evaluate an explanation's faithfulness and that the community must pursue avenues to enhance the faithfulness of self-explanations
- Why unresolved: The black-box nature of LLMs makes classical XAI faithfulness metrics infeasible, and current works only propose tests to disprove faithfulness or measure post-hoc reasoning as a proxy for unfaithfulness
- What evidence would resolve it: Development and validation of new metrics specifically designed for evaluating the faithfulness of LLM-generated explanations, potentially through empirical studies comparing these metrics with human judgment

### Open Question 2
- Question: What novel methodologies can be developed to enhance the faithfulness of self-explanations in LLMs, beyond current techniques like fine-tuning, in-context learning, and mechanistic interpretability?
- Basis in paper: [explicit] The paper calls for the community to develop novel methods to enhance the faithfulness of self-explanations, suggesting fine-tuning approaches, in-context learning, and mechanistic interpretability as potential directions
- Why unresolved: While these approaches are promising, they are not yet fully explored or proven to significantly improve faithfulness, and there may be other innovative methodologies that have not been considered
- What evidence would resolve it: Research demonstrating the effectiveness of new methodologies in improving the faithfulness of LLM-generated explanations, validated through rigorous testing and comparison with existing techniques

### Open Question 3
- Question: How can we balance the trade-off between plausibility and faithfulness in LLM-generated explanations to ensure they are both user-friendly and accurate, especially in high-stakes applications?
- Basis in paper: [explicit] The paper discusses the dichotomy between faithfulness and plausibility, emphasizing the need for a systematic characterization of faithfulness-plausibility requirements for different real-world applications
- Why unresolved: Current LLM research overemphasizes plausibility over faithfulness, and there is a lack of understanding of how to calibrate explanations to meet the specific needs of different applications without compromising on either aspect
- What evidence would resolve it: Empirical studies and frameworks that demonstrate successful balancing of plausibility and faithfulness in LLM-generated explanations, tailored to the requirements of various application domains, with validation from domain experts

## Limitations

- The paper relies heavily on indirect measurements of faithfulness rather than direct mechanistic validation of internal reasoning
- Claims about application-specific requirements for faithfulness vs. plausibility are largely theoretical with limited concrete guidance
- The tradeoff between plausibility and faithfulness remains largely theoretical, with limited empirical quantification across different model architectures

## Confidence

- **High Confidence**: The distinction between plausibility and faithfulness is well-established in the literature and the mechanisms by which LLMs generate plausible explanations are reasonably understood
- **Medium Confidence**: The assertion that optimizing for plausibility may compromise faithfulness, though supported by multiple studies, lacks comprehensive empirical validation across diverse model types
- **Low Confidence**: Claims about application-specific requirements for faithfulness vs. plausibility are largely theoretical, with limited concrete guidance for practitioners

## Next Checks

1. Conduct systematic experiments varying model training objectives (e.g., training models specifically to generate faithful vs. plausible explanations) to quantify the tradeoff empirically
2. Implement a comprehensive benchmark testing explanation faithfulness across multiple domains and application contexts, measuring both plausibility and faithfulness metrics
3. Develop and validate methods to directly observe internal reasoning processes in LLMs, moving beyond indirect faithfulness measurements to mechanistic validation