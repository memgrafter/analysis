---
ver: rpa2
title: 'FEEL: A Framework for Evaluating Emotional Support Capability with Large Language
  Models'
arxiv_id: '2403.15699'
source_url: https://arxiv.org/abs/2403.15699
tags:
- evaluation
- emotional
- feel
- support
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FEEL addresses the challenge of evaluating emotional support capability
  in dialogue systems, which is difficult due to subjectivity and the high cost of
  manual evaluation. The proposed framework FEEL uses large language models (LLMs)
  as evaluators, considering multiple evaluation aspects and employing self-CoT and
  probability distribution approaches for stable results.
---

# FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models

## Quick Facts
- arXiv ID: 2403.15699
- Source URL: https://arxiv.org/abs/2403.15699
- Authors: Huaiwen Zhang; Yu Chen; Ming Wang; Shi Feng
- Reference count: 31
- One-line primary result: FEEL significantly outperforms baseline metrics in aligning with human evaluations, achieving a Spearman's rank correlation coefficient of 0.509 for helpfulness

## Executive Summary
FEEL addresses the challenge of evaluating emotional support capability in dialogue systems by using large language models as evaluators. The framework considers multiple evaluation aspects and employs self-CoT and probability distribution approaches for stable results. FEEL integrates an ensemble learning strategy with multiple LLMs (ERNIE-Bot 4.0, GLM-4, and GPT-3.5-Turbo) to enhance evaluation accuracy. Experimental results demonstrate that FEEL significantly outperforms baseline metrics in aligning with human evaluations, particularly for helpfulness with a Spearman correlation of 0.509.

## Method Summary
FEEL uses three LLM APIs (ERNIE-Bot 4.0, GLM-4, and GPT-3.5-Turbo) to evaluate emotional support capability through a multi-step process. The framework defines six evaluation aspects: Informativeness, Comprehensibility, Helpfulness, Consistency, Coherence, and Safety. Each LLM generates probability distributions across four score bands (0-3) for each aspect using Self Chain-of-Thoughts (Self-CoT) prompting. The framework averages 10 successive LLM round scores for stability and combines results using Spearman correlation weights derived from the ESCEval dataset. The final score is produced through weighted averaging of the ensemble of LLMs.

## Key Results
- FEEL achieves a Spearman's rank correlation coefficient of 0.509 for helpfulness, significantly outperforming baseline metrics
- The ensemble approach demonstrates superior performance compared to individual LLMs in terms of correlation and error reduction
- FEEL shows improved stability through the use of probability distribution approaches and averaging multiple iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FEEL's ensemble of multiple LLMs reduces evaluation variance compared to single-model approaches.
- Mechanism: The framework combines scores from three different LLMs using weighted averaging based on each model's correlation with human evaluations.
- Core assumption: Different LLMs have complementary strengths across evaluation aspects, and combining them yields more stable results than any single model.
- Evidence anchors:
  - [abstract]: "employs a probability distribution approach for a more stable result and integrates an ensemble learning strategy, leveraging multiple LLMs with assigned weights to enhance evaluation accuracy"
  - [section]: "Subsequently, to determine the weight of each LLM in FEEL, we respectively employ the three LLMs to evaluate the ESCEval... Using the Spearman's rank correlation coefficient with results in the ESCEval as the weight"
  - [corpus]: Weak evidence - corpus doesn't provide direct validation of this mechanism

### Mechanism 2
- Claim: FEEL's use of self-CoT (Chain-of-Thought) generation improves evaluation consistency.
- Mechanism: Instead of manually designing evaluation steps, FEEL prompts each LLM to generate its own reasoning process for evaluating each aspect, then uses the resulting probability distributions for scoring.
- Core assumption: LLMs can generate more consistent and comprehensive evaluation reasoning when prompted to explain their own thought process rather than following externally defined steps.
- Evidence anchors:
  - [section]: "In the emotional support capability evaluation, the LLMs needs to master the detailed evaluation steps of each indicator so as to conduct detailed evaluation step by step... Therefore, refer to the CoT design in [7], we give LLM the task definition and the specific metric evaluation criteria to let it generate the evaluation steps automatically (Self-CoT)"
  - [abstract]: "employs self-CoT and probability distribution approaches for a more stable result"
  - [corpus]: Weak evidence - corpus doesn't validate this specific approach

### Mechanism 3
- Claim: FEEL's probability distribution approach captures uncertainty better than deterministic scoring.
- Mechanism: Instead of outputting a single score, each LLM generates probabilities for each possible score band (0-3), which are then weighted and averaged to produce the final score.
- Core assumption: Emotional support capability evaluation is inherently uncertain, and capturing this uncertainty through probability distributions leads to more accurate and reliable results than point estimates.
- Evidence anchors:
  - [section]: "For specific evaluation aspect of single-round dialog data, we utilize a prompt-based answer format to facilitate the LLM in generating selection probabilities for each score band (0 to 3)... Acknowledging the inherent volatility in LLM responses... we mitigate it by calculating the average of ten successive LLM round scores"
  - [abstract]: "employs a probability distribution approach for a more stable result"
  - [corpus]: Weak evidence - corpus doesn't validate this specific approach

## Foundational Learning

- Concept: Spearman's rank correlation coefficient
  - Why needed here: FEEL uses Spearman correlation to weight individual LLMs based on their alignment with human evaluations
  - Quick check question: What does a Spearman correlation of 0.509 for helpfulness indicate about FEEL's performance?

- Concept: Ensemble learning
  - Why needed here: FEEL combines multiple LLMs to leverage their complementary strengths and reduce individual model biases
  - Quick check question: How does FEEL determine the weight for each LLM in the ensemble?

- Concept: Likert scale scoring
  - Why needed here: The evaluation framework uses a 4-point scale (0-3) for each of six aspects of emotional support capability
  - Quick check question: What are the six evaluation aspects defined in FEEL?

## Architecture Onboarding

- Component map: Prompt design system → Individual LLM evaluation module → Ensemble integration layer
- Critical path: prompt generation → individual LLM evaluation (with 10 iterations) → probability distribution calculation → weighted averaging → final score output
- Design tradeoffs: FEEL trades computational cost (multiple LLM calls and iterations) for improved accuracy and stability. The framework also accepts the complexity of managing multiple models rather than using a single, simpler approach.
- Failure signatures: If individual LLM scores show high variance across iterations, if ensemble weights are imbalanced, or if probability distributions are consistently skewed toward certain score bands, the framework may produce unreliable results.
- First 3 experiments:
  1. Test individual LLM performance on a small subset of ESCEval data to establish baseline correlations
  2. Compare FEEL's output against human evaluations on held-out test data to verify improvement over baselines
  3. Perform ablation studies by removing each LLM from the ensemble to quantify their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FEEL framework handle potential bias in the ESCEval dataset, particularly given the subjective nature of emotional support evaluation and the limited diversity of annotators?
- Basis in paper: [explicit] The paper acknowledges the impact of subjectivity in manual scoring and uses collaborative re-scoring to reduce bias, but does not fully address how diverse perspectives are incorporated or how potential biases in the dataset are mitigated.
- Why unresolved: While the paper describes a method to reduce subjectivity through multiple annotators and collaborative re-scoring, it does not provide details on how diversity in annotator backgrounds or potential biases in the dataset are systematically addressed.
- What evidence would resolve it: Detailed analysis of annotator demographics, inter-annotator agreement statistics, and methods for bias detection and mitigation in the ESCEval dataset would help address this question.

### Open Question 2
- Question: What is the impact of expanding the FEEL framework to include more than three LLMs, and how does this affect the overall evaluation performance?
- Basis in paper: [inferred] The paper mentions that due to funding reasons, only three LLMs were used in the current framework, but suggests that exploring the impact of more LLM components could be a future direction.
- Why unresolved: The current framework's performance is based on a limited set of three LLMs, and the paper does not provide insights into how additional LLMs might influence the evaluation results or the overall effectiveness of the framework.
- What evidence would resolve it: Experimental results comparing the performance of FEEL with varying numbers of LLMs, including a detailed analysis of how additional models affect the evaluation accuracy and correlation with human judgments, would provide clarity on this question.

### Open Question 3
- Question: How does the FEEL framework perform when evaluating dialogues in languages other than English, and what adaptations are necessary for cross-linguistic evaluation?
- Basis in paper: [explicit] The paper does not mention any experiments or considerations for evaluating dialogues in languages other than English, focusing solely on English-language dialogues.
- Why unresolved: The framework's effectiveness in handling multilingual dialogues is not addressed, leaving uncertainty about its applicability and performance in diverse linguistic contexts.
- What evidence would resolve it: Conducting experiments with FEEL on multilingual dialogue datasets, along with an analysis of the necessary adaptations for cross-linguistic evaluation, would provide insights into the framework's versatility and limitations in different language contexts.

## Limitations
- The framework's performance heavily depends on the quality of the reference dataset (ESCEval) used for weight calculation, which contains only 200 dialogue instances
- The paper doesn't provide ablation studies showing how much each individual LLM contributes to the ensemble performance
- The approach assumes that LLM-generated probability distributions accurately reflect evaluation uncertainty, but this assumption isn't empirically validated through comparison with human confidence ratings

## Confidence
- **High confidence** in the core mechanism of using ensemble learning to combine multiple LLMs
- **Medium confidence** in the effectiveness of Self-CoT prompting
- **Medium confidence** in the probability distribution approach

## Next Checks
1. Conduct ablation studies to quantify individual LLM contributions by testing FEEL performance with each LLM removed from the ensemble
2. Compare FEEL's probability distributions against human rater confidence levels to validate whether the distributions meaningfully capture uncertainty
3. Test FEEL on a held-out test set from ESCEval (not used in weight calculation) to verify that the framework generalizes beyond the training data used for weight determination