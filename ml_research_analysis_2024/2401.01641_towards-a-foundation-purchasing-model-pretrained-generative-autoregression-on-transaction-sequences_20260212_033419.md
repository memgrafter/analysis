---
ver: rpa2
title: 'Towards a Foundation Purchasing Model: Pretrained Generative Autoregression
  on Transaction Sequences'
arxiv_id: '2401.01641'
source_url: https://arxiv.org/abs/2401.01641
tags:
- embeddings
- learning
- transaction
- features
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a self-supervised generative pretraining
  method for learning contextualised embeddings of financial transactions. It combines
  two objectives: next event prediction and past reconstruction, both adapted to multivariate
  time series of financial transactions.'
---

# Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences

## Quick Facts
- arXiv ID: 2401.01641
- Source URL: https://arxiv.org/abs/2401.01641
- Reference count: 40
- Method outperforms hand-engineered features and other SSL methods on downstream financial tasks

## Executive Summary
This paper introduces a self-supervised generative pretraining method for financial transaction sequences that learns contextualised embeddings through next event prediction and past reconstruction tasks. The method achieves state-of-the-art performance on downstream tasks including churn prediction, age classification, expenditure forecasting, credit default prediction, and card fraud detection. The pretrained embeddings also capture semantic similarity between merchant categories without explicit labels, demonstrating the model's ability to learn meaningful representations from raw transaction data.

## Method Summary
The NPPR method uses a GRU-based encoder with MLP preprocessing to generate transaction embeddings, combined with two decoder heads for next event prediction (NP) and past reconstruction (PR). The model is pretrained on transaction sequences using a weighted combination of NP and PR losses, where PR uses an exponential decay function to reconstruct past events. For downstream tasks, embeddings are either taken from the most recent transaction or averaged across all transactions in an entity's history, then fed into simple MLPs without fine-tuning.

## Key Results
- NPPR outperforms hand-engineered features and other SSL methods on churn, age, expenditure, and credit default prediction tasks
- Generated embeddings improve fraud detection performance at high precision thresholds and transfer well to out-of-domain data
- Embeddings capture semantic similarity between merchant category codes, grouping related categories together in embedding space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Next Event Prediction (NP) task learns behavioral features predictive of future events
- Mechanism: The model encodes transaction embeddings that summarize past behavior, enabling accurate prediction of the next event's features (amount, merchant, etc.)
- Core assumption: Financial modeling tasks like churn, credit default, and expenditure prediction require understanding of future behavior patterns encoded in past transactions
- Evidence anchors: [abstract] and [section] state the NP task was motivated by the similarity between generative modeling and financial modeling tasks
- Break condition: If transaction patterns become too random or non-sequential, the predictive power degrades

### Mechanism 2
- Claim: Past Reconstruction (PR) task encourages learning of long-term behavioral features
- Mechanism: By reconstructing past events from current embeddings, the model must encode information about longer-term behavior patterns beyond immediate history
- Core assumption: Longer-term behavioral features improve predictive performance on downstream tasks compared to short-term features alone
- Evidence anchors: [abstract] and [section] state the PR task encourages learning of longer-term behavioral features which increase predictive performance
- Break condition: If the decay length hyperparameter is poorly chosen, distant past events may add noise rather than signal

### Mechanism 3
- Claim: Averaging transaction embeddings improves entity-level prediction accuracy for static attributes
- Mechanism: Taking the average of all transaction embeddings in an entity's history creates a more stable representation that captures overall behavioral patterns
- Core assumption: Static entity attributes (like age) are better represented by the overall behavioral pattern rather than just the most recent transaction
- Evidence anchors: [section] shows embedding averaging can improve performance, especially for predicting static entity attributes
- Break condition: For tasks requiring recent behavior (like churn), averaging may oversmooth important recent patterns

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: Provides the foundation for predicting the next transaction based on past sequence, analogous to predicting the next word in a sentence
  - Quick check question: What is the key difference between autoregressive and masked language modeling in the context of transaction sequences?

- Concept: Contrastive learning vs generative modeling
  - Why needed here: Understanding why generative modeling (NPPR) outperforms contrastive methods (CoLES) for behavioral feature learning
  - Quick check question: In what scenarios would contrastive learning be more appropriate than generative modeling for transaction data?

- Concept: Embedding space semantic similarity
  - Why needed here: Explains how the model learns meaningful relationships between merchant categories without explicit labels
  - Quick check question: How does averaging transaction embeddings for the same merchant category reveal semantic similarity in the embedding space?

## Architecture Onboarding

- Component map:
  - Transaction features (normalized/encoded) -> MLP preprocessing -> GRU encoder -> Projected embeddings -> NP decoder (MLP) and PR decoder (MLP)

- Critical path:
  1. Preprocess transaction into dense vector (normalize/encode features)
  2. Pass through encoder stack (MLP → GRU → projection)
  3. Generate next event prediction with NP decoder
  4. Generate past event reconstructions with PR decoder
  5. Compute combined loss (weighted NP + PR objectives)

- Design tradeoffs:
  - GRU vs Transformer: GRU is more efficient for real-time processing of streaming transactions
  - Single embedding vs averaged: Single embedding captures recent behavior; averaging captures overall pattern
  - Loss weighting (α): Higher α emphasizes long-term patterns but may reduce immediate predictive accuracy

- Failure signatures:
  - Poor fraud detection performance: May indicate embeddings not capturing behavioral anomalies
  - Overfitting on public datasets: Suggests model memorizing patterns rather than learning generalizable features
  - Unstable embeddings across time: Could indicate insufficient regularization or inappropriate decay length

- First 3 experiments:
  1. Train NP-only model and compare performance to full NPPR on churn prediction task
  2. Vary the decay length λ in PR task to find optimal balance between short-term and long-term features
  3. Test different aggregation strategies (max, min, median) for creating entity embeddings from transaction embeddings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the NPPR method compare to other SSL methods when applied to financial transaction data from industries with very different purchasing patterns (e.g., e-commerce vs. travel vs. grocery)?
- Basis in paper: [explicit] The paper shows the NPPR method outperforms other SSL methods on various downstream tasks using public datasets and demonstrates good transferability to out-of-domain data from different issuers, but does not specifically compare performance across industries with distinct purchasing patterns
- Why unresolved: The paper's evaluations focus on general fraud detection and downstream tasks without analyzing industry-specific performance differences or comparing the NPPR method's effectiveness across sectors with fundamentally different transaction behaviors
- What evidence would resolve it: Experiments applying the NPPR method to transaction datasets from multiple distinct industries (e.g., retail, travel, healthcare, entertainment) and comparing performance metrics across these sectors would show whether the method generalizes equally well to all types of financial transaction data or performs better in certain domains

### Open Question 2
- Question: What is the optimal decay length parameter λ for the past reconstruction task in NPPR when applied to transaction sequences with very different time scales (e.g., daily transactions vs. monthly vs. yearly)?
- Basis in paper: [explicit] The paper sets λ=2 months based on domain expertise but notes this was chosen for card transaction data and shows the past reconstruction task improves performance, suggesting the parameter is important but context-dependent
- Why unresolved: The paper uses a single fixed value of λ across all experiments and does not investigate how this hyperparameter should be adjusted for transaction data with different temporal characteristics or frequencies
- What evidence would resolve it: Systematic experiments varying λ across different time scales of transaction data (daily, weekly, monthly, yearly) and measuring the impact on downstream task performance would identify optimal parameter values for different temporal granularities

### Open Question 3
- Question: How does the NPPR method's performance on fraud detection change when the training data contains significant class imbalance between fraudulent and genuine transactions?
- Basis in paper: [explicit] The paper mentions using downsampling for genuine transactions due to high class imbalance in fraud detection datasets but does not analyze how this imbalance affects the NPPR method's pretraining effectiveness or downstream performance
- Why unresolved: While the paper addresses class imbalance at the downstream classification stage through downsampling, it does not investigate how class imbalance in the pretraining corpus affects the quality of the learned transaction embeddings or the method's robustness to imbalanced data
- What evidence would resolve it: Experiments training the NPPR model on pretraining corpora with varying levels of class imbalance and measuring the impact on fraud detection performance at different precision thresholds would show how sensitive the method is to imbalanced training data

## Limitations

- Evaluation relies heavily on public datasets which may not represent real-world financial transaction complexity
- Claims about semantic similarity between merchant categories lack quantitative validation on held-out data
- Method assumes sequential transaction patterns are predictive, which may not hold for all financial behaviors or markets

## Confidence

- **High confidence**: The core NPPR pretraining methodology and its application to fraud detection, where clear performance improvements are demonstrated with statistical significance across multiple metrics and thresholds
- **Medium confidence**: The transfer learning capabilities to out-of-domain fraud detection datasets, though results show promise, the evaluation is limited to two specific datasets with different fraud patterns
- **Medium confidence**: The superiority of generative modeling over contrastive methods (CoLES) for behavioral feature learning, based on comparisons within this specific domain and task set
- **Low confidence**: The claim that embeddings capture semantic similarity between merchant categories, as this is primarily demonstrated through qualitative analysis rather than rigorous quantitative evaluation

## Next Checks

1. Test the pretrained embeddings on additional fraud detection datasets from different geographic regions and financial institutions to validate the robustness of transfer learning claims beyond the two datasets evaluated

2. Design a quantitative evaluation framework for merchant category code similarity using held-out merchant pairs with known semantic relationships, measuring embedding distances against ground truth similarity ratings

3. Systematically vary the NP/PR loss weighting parameter α across a wider range (e.g., 0.1 to 0.9) and evaluate performance on all downstream tasks to identify optimal configurations and understand the sensitivity of the method to this hyperparameter