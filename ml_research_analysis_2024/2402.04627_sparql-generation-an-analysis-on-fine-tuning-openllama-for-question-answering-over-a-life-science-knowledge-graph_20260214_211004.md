---
ver: rpa2
title: 'SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering
  over a Life Science Knowledge Graph'
arxiv_id: '2402.04627'
source_url: https://arxiv.org/abs/2402.04627
tags:
- queries
- bgee
- sparql
- knowledge
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a data augmentation approach to address the
  scarcity of SPARQL training data for fine-tuning large language models (LLMs) on
  life science knowledge graphs. Starting from a small set of representative SPARQL
  queries, the method systematically adds new triple patterns using properties of
  the involved entities.
---

# SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph

## Quick Facts
- arXiv ID: 2402.04627
- Source URL: https://arxiv.org/abs/2402.04627
- Reference count: 31
- Primary result: Semantic enrichment via meaningful variable names and inline comments improves SPARQL generation accuracy by up to 33%

## Executive Summary
This paper addresses the challenge of generating SPARQL queries from natural language questions for life science knowledge graphs by proposing a data augmentation approach. Starting with a small set of representative SPARQL queries, the method systematically adds new triple patterns using entity properties and enriches queries with semantic context through meaningful variable names and inline comments. The approach is evaluated by fine-tuning OpenLLaMA on the Bgee gene expression knowledge graph, demonstrating that semantic enrichment significantly improves query generation performance compared to random variable names and no comments.

## Method Summary
The method involves two main components: data augmentation and fine-tuning. The data augmentation approach starts with a small set of representative SPARQL queries and systematically adds new triple patterns by leveraging properties of the involved entities, generating three query styles (random variable names, meaningful names, and meaningful names with inline comments). The augmented datasets are then used to fine-tune OpenLLaMA, first optionally on a general domain dataset (KQA Pro for Wikidata) and then on the domain-specific Bgee dataset. The evaluation uses multiple metrics including BLEU, SP-BLEU, METEOR, ROUGE-L, and F1-score to assess both syntactic correctness and semantic accuracy of generated queries.

## Key Results
- Semantic enrichment through meaningful variable names and inline comments improves SPARQL generation accuracy by up to 33%
- Data augmentation successfully expands query coverage from a small seed set of 15 representative queries
- Cross-domain transfer from Wikidata to Bgee knowledge graphs showed no significant improvement or even deterioration in performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic enrichment via meaningful variable names and inline comments significantly improves SPARQL generation accuracy.
- Mechanism: Providing semantic context in the training data (variable names like ?gene instead of ?x0, and comments like "# in taxon") gives the model additional cues about the meaning of each component, reducing ambiguity in the query structure.
- Core assumption: The LLM can leverage semantic context to improve reasoning about query semantics rather than just syntax.
- Evidence anchors:
  - [abstract]: "semantic clues can improve model performance by up to 33% compared to a baseline with random variable names and no comments included."
  - [section]: "providing meaningful variable names and/or inline comments that define property labels in the SPARQL query significantly improve all evaluated metrics"
  - [corpus]: Weak - no direct evidence in neighboring papers, though related works like FIRESPARQL also mention semantic enhancement.
- Break condition: If the model lacks sufficient capacity to encode semantic relationships, or if comments become too verbose and distract from core query structure.

### Mechanism 2
- Claim: Data augmentation by iteratively adding triple patterns from class properties expands the model's coverage of knowledge graph contents.
- Mechanism: Starting from a small set of representative SPARQL queries, new triple patterns are systematically added using properties of entities involved, generating new semantically enriched query-question pairs that cover a wider range of properties.
- Core assumption: The original query set is representative enough of the knowledge graph's structure to serve as a seed for meaningful augmentation.
- Evidence anchors:
  - [abstract]: "we propose an end-to-end data augmentation approach for extending a set of existing queries... towards a larger dataset of semantically enriched question-to-SPARQL query pairs"
  - [section]: "we designed a dataset augmentation approach to generate extra queries and their corresponding natural language questions, starting from a representative set of existing examples"
  - [corpus]: Weak - neighboring papers focus on query generation frameworks but don't detail augmentation strategies.
- Break condition: If the augmentation introduces nonsensical queries or fails to maintain semantic coherence with the original queries.

### Mechanism 3
- Claim: Knowledge transfer from a general-domain knowledge graph (Wikidata) to a domain-specific one (Bgee) can be beneficial, but may also deteriorate performance if not properly tuned.
- Mechanism: Fine-tuning the LLM first on KQA Pro (Wikidata dataset) and then on Bgee-specific data aims to leverage general query generation skills before specializing to the domain.
- Core assumption: Skills learned on general knowledge graphs transfer to domain-specific contexts.
- Evidence anchors:
  - [abstract]: "We chose Bgee as a scientific KB... Wikidata contains part of the Bgee data"
  - [section]: "using OpenLLaMA+KQA_Pro (i.e., rows in Table 1 which start with 'Wikidata') as the base model to further fine-tune with the different Bgee query sets does not lead to any significant improvement (or worse, the performance may deteriorate)"
  - [corpus]: Weak - no neighboring papers directly address cross-domain transfer in SPARQL generation.
- Break condition: If the general-domain training introduces noise that conflicts with domain-specific query patterns, or if the domains are too dissimilar.

## Foundational Learning

- Concept: SPARQL query structure and semantics
  - Why needed here: Understanding how SPARQL queries represent knowledge graph patterns is essential for designing effective augmentation strategies and evaluating model outputs.
  - Quick check question: What are the three main components of a basic SPARQL query and how do they relate to RDF triples?

- Concept: Large Language Model fine-tuning techniques (SFTTrainer, QLoRA, PEFT)
  - Why needed here: The methodology relies on specific fine-tuning approaches to adapt the LLM to the SPARQL generation task efficiently.
  - Quick check question: What is the key difference between full fine-tuning and parameter-efficient fine-tuning (PEFT) methods like QLoRA?

- Concept: Knowledge graph representation and schema understanding
  - Why needed here: Effective augmentation requires understanding the relationships between classes and properties in the knowledge graph to generate meaningful new queries.
  - Quick check question: How do terminological and assertion axioms in an ontology help identify valid properties to attach to class instances?

## Architecture Onboarding

- Component map:
  - Data augmentation module -> Fine-tuning pipeline -> Evaluation framework -> OpenLlama model

- Critical path:
  1. Start with seed SPARQL queries and corresponding questions
  2. Apply data augmentation to generate expanded dataset
  3. Fine-tune base OpenLlama on general domain (KQA Pro) if applicable
  4. Further fine-tune on domain-specific augmented dataset
  5. Evaluate performance using multiple metrics
  6. Deploy for question answering over knowledge graph

- Design tradeoffs:
  - Augmentation vs. quality: More augmentation increases coverage but risks introducing nonsensical queries
  - Semantic enrichment vs. verbosity: Comments and meaningful names help but may distract if overused
  - General-to-specific transfer vs. direct domain training: May improve or worsen performance depending on domain similarity

- Failure signatures:
  - BLEU/SP-BLEU scores near zero indicate the model cannot generate syntactically valid SPARQL
  - High syntactic correctness but low semantic accuracy suggests the model learned patterns without understanding
  - Deteriorating performance after general-domain fine-tuning suggests negative transfer

- First 3 experiments:
  1. Evaluate base OpenLlama on Bgee dataset without any fine-tuning to establish baseline
  2. Fine-tune on augmented Bgee dataset with random variable names and no comments
  3. Fine-tune on augmented Bgee dataset with meaningful variable names and inline comments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SPARQL generation models vary when fine-tuning is performed on domain-specific knowledge graphs versus general-purpose knowledge graphs?
- Basis in paper: [explicit] The paper mentions that fine-tuning OpenLLaMA first with KQA Pro (a Wikidata dataset) and then with Bgee dataset did not significantly improve performance, suggesting potential issues with knowledge transfer from general to domain-specific KGs.
- Why unresolved: The paper only evaluated one domain-specific knowledge graph (Bgee) and one general-purpose KG (Wikidata). It is unclear whether the observed performance issues are specific to this combination or a more general phenomenon.
- What evidence would resolve it: Conducting experiments with multiple domain-specific and general-purpose KGs to determine if the knowledge transfer issues persist across different KG combinations.

### Open Question 2
- Question: What is the optimal strategy for dataset augmentation in SPARQL generation tasks, considering both the quantity and quality of generated queries?
- Basis in paper: [explicit] The paper proposes a dataset augmentation approach that generates additional queries by adding triple patterns to existing queries. However, it is unclear whether this approach is optimal in terms of balancing the quantity and quality of generated queries.
- Why unresolved: The paper does not provide a comprehensive evaluation of different dataset augmentation strategies or explore the trade-offs between quantity and quality of generated queries.
- What evidence would resolve it: Comparing the performance of models trained on datasets augmented using different strategies, including varying the number of generated queries and the complexity of added triple patterns.

### Open Question 3
- Question: How do semantic "clues" in SPARQL queries, such as meaningful variable names and inline comments, impact the performance of large language models in SPARQL generation tasks?
- Basis in paper: [explicit] The paper investigates the role of semantic clues by comparing the performance of models trained on queries with random variable names, meaningful variable names, and meaningful variable names with inline comments. The results show that semantic clues significantly improve model performance.
- Why unresolved: While the paper demonstrates the positive impact of semantic clues, it does not explore the underlying reasons for this improvement or the potential limitations of using such clues.
- What evidence would resolve it: Conducting ablation studies to isolate the individual contributions of meaningful variable names and inline comments to model performance, and investigating the potential drawbacks or limitations of using semantic clues in SPARQL queries.

## Limitations

- The data augmentation approach relies on a very small seed set of 15 SPARQL queries, raising concerns about whether the augmented dataset adequately covers the knowledge graph's complexity
- Cross-domain transfer from Wikidata to Bgee showed either no improvement or deterioration, suggesting the general-to-specific fine-tuning strategy may not be universally beneficial
- The study focuses exclusively on the Bgee knowledge graph without testing the approach on other domains or larger, more diverse datasets

## Confidence

- **High confidence**: Semantic enrichment (meaningful variable names and inline comments) improves SPARQL generation performance
- **Medium confidence**: Data augmentation strategy effectively expands query coverage from small seed sets
- **Low confidence**: Cross-domain transfer from general knowledge graphs to domain-specific contexts is beneficial

## Next Checks

1. **Replicate augmentation mechanism**: Implement the exact data augmentation procedure described and verify that it produces semantically coherent queries that maintain the original query intent while expanding coverage

2. **Test on additional knowledge graphs**: Apply the same methodology to at least two other domain-specific knowledge graphs (e.g., from different scientific domains) to assess generalizability of the semantic enrichment benefits

3. **Ablation study on augmentation depth**: Systematically vary the number of augmentation iterations and measure the point of diminishing returns or where query quality begins to degrade, establishing optimal augmentation parameters