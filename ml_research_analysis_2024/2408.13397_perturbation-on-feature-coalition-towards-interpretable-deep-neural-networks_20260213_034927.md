---
ver: rpa2
title: 'Perturbation on Feature Coalition: Towards Interpretable Deep Neural Networks'
arxiv_id: '2408.13397'
source_url: https://arxiv.org/abs/2408.13397
tags:
- feature
- loss
- network
- features
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of interpreting black-box deep
  neural networks by proposing a novel perturbation-based method guided by feature
  coalitions. The core idea is to first extract correlated features from the network's
  perspective, then use these features to guide the perturbation interpretation.
---

# Perturbation on Feature Coalition: Towards Interpretable Deep Neural Networks

## Quick Facts
- arXiv ID: 2408.13397
- Source URL: https://arxiv.org/abs/2408.13397
- Authors: Xuran Hu; Mingzhe Zhu; Zhenpeng Feng; Miloš Daković; Ljubiša Stanković
- Reference count: 12
- Primary result: Proposed perturbation-based method guided by feature coalitions achieves higher confidence scores on ImageNet-1k with Average Drop in confidence of 0.1122 and Percentage of increase in confidence of 0.5843

## Executive Summary
This paper addresses the challenge of interpreting black-box deep neural networks by proposing a novel perturbation-based method guided by feature coalitions. The core innovation lies in extracting correlated features from the network's perspective and using these feature coalitions to guide the perturbation interpretation process. By leveraging deep information of the network to identify feature interdependencies and employing a carefully-designed consistency loss, the method aims to produce more accurate and reliable explanations compared to existing approaches. Experiments demonstrate superior performance in locating regions of network interest and recovering fine-grained features.

## Method Summary
The proposed method operates in two main steps. First, it extracts correlated features from a pre-trained VGG16 network by reconfiguring the model to retain only convolutional, activation, and normalization layers, then adds a 1×1 convolution to produce deep features. These features are clustered using a combination of feature similarity loss and feature continuity loss to identify meaningful feature coalitions. Second, the method performs perturbation on these extracted feature coalitions using a three-component loss function: confidence loss (ensuring predicted class remains unchanged), mask loss (maximizing perturbation size), and consistency loss (applying Gaussian-weighted penalty over each coalition to ensure perturbation consistency). The final output is a saliency map that highlights the regions most relevant to the network's decision.

## Key Results
- Achieved higher confidence scores compared to baseline methods with Average Drop in confidence of 0.1122
- Demonstrated superior performance in locating regions of network interest with Percentage of increase in confidence of 0.5843
- Ablation studies validated the effectiveness of the consistency loss in focusing interpretation on fine-grained details and edge information

## Why This Works (Mechanism)

### Mechanism 1
The method extracts feature coalitions that capture meaningful feature interdependencies through unsupervised clustering based on similarity and spatial continuity criteria. By reconfiguring the DNN and adding a 1×1 convolution, it identifies correlated features from the network's perspective rather than using manually defined guidance. The combination of feature similarity loss (cross-entropy between cluster category and feature) and feature continuity loss (gradient-based smoothness) enables effective clustering of deep features into coalitions that represent meaningful feature relationships.

### Mechanism 2
The consistency loss ensures that perturbation-based interpretations focus on fine-grained details and edge information by applying a Gaussian-weighted penalty over each feature coalition. This constraint, combined with confidence loss and mask loss, guides the interpretation to respect identified feature coalitions and focus on the fine-grained details the network uses for decision-making. The consistency loss prevents arbitrary perturbations within a coalition, ensuring the interpretation reflects the network's actual decision process.

### Mechanism 3
The method achieves higher confidence scores and better localization by fully considering feature interdependencies through coalition-guided perturbation. The perturbation process respects the identified feature relationships, leading to saliency maps that better capture the regions the network actually focuses on for decision-making. This results in more accurate identification of network decision regions compared to methods that treat features independently, as validated by quantitative metrics.

## Foundational Learning

- **Feature attribution and saliency map generation**: The method builds on generating saliency maps through perturbation-based interpretation to explain DNN decisions. Quick check: What is the fundamental difference between perturbation-based methods and gradient-based methods for generating saliency maps?

- **Unsupervised feature clustering and representation learning**: The method uses unsupervised clustering to extract feature coalitions from network's deep features without manual guidance. Quick check: How does the combination of feature similarity loss and feature continuity loss contribute to effective unsupervised clustering of deep features?

- **Loss function design for multi-objective optimization**: The method combines three loss functions (confidence, mask, and consistency) with different coefficients to achieve desired interpretation quality. Quick check: Why is it important to balance the three loss components with appropriate coefficients, and what might happen if one dominates the others?

## Architecture Onboarding

- **Component map**: Input sample (x) -> Pre-trained DNN (f) - VGG16 -> Feature extraction network -> Feature clustering module -> Perturbation interpreter -> Output: Saliency map (Mt = 1 - p)

- **Critical path**: Input → Feature extraction network → Feature clustering → Coalition identification → Coalition-guided perturbation → Saliency map

- **Design tradeoffs**: 
  - Tradeoff between perturbation size (mask loss) and maintaining class confidence (confidence loss)
  - Tradeoff between respecting feature coalitions (consistency loss) and finding largest possible perturbation
  - Computational cost of extracting deep features and clustering vs. quality of explanations
  - Number of clusters (l) affects granularity of feature coalitions but increases computational complexity

- **Failure signatures**:
  - Saliency maps too dispersed or fail to localize objects (indicates consistency loss issues)
  - Saliency maps too small or miss important regions (indicates mask loss or confidence loss imbalance)
  - Saliency maps highlighting irrelevant regions (indicates feature coalition extraction problems)
  - Very slow convergence or unstable training (indicates poor loss coefficient tuning)

- **First 3 experiments**:
  1. Verify feature coalition extraction: Run feature extraction network on sample images and visualize extracted feature coalitions to ensure they capture meaningful regions and patterns
  2. Ablation study on loss components: Test method with different combinations of loss components (e.g., without consistency loss, without mask loss) to understand their individual contributions
  3. Quantitative comparison: Evaluate method on small subset of ImageNet-1k using three metrics (Average Drop in confidence, Percentage of increase in confidence, Top-1 accuracy) and compare with at least one baseline method

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the number of clusters (l) in the feature extraction network affect the quality and interpretability of the feature coalitions? The paper mentions the number of clusters decreases during optimization and the final number of feature coalitions l' is within the range [k, l], where k is the minimum number of feature coalitions specified, but does not provide detailed analysis of how different values of l impact quality of feature coalitions or effectiveness of interpretation.

- **Open Question 2**: Can the proposed method be extended to other types of deep neural networks, such as recurrent neural networks (RNNs) or transformers, beyond convolutional neural networks (CNNs)? The paper focuses on explaining CNN-based models (VGG16) and does not discuss applicability to other architectures, though the method relies on convolutional layers and feature extraction which may not be directly applicable to other network architectures.

- **Open Question 3**: How sensitive is the proposed method to the choice of hyperparameters, such as the loss coefficients (λ, µ, v) and the Gaussian kernel standard deviation (σ)? The paper mentions specific values for loss coefficients (λ = 1.0, µ = 100.0, v = 1.0) and Gaussian kernel standard deviation (σ = 1.0), but does not provide analysis of their impact, though performance could be highly dependent on these hyperparameters.

## Limitations

- The unsupervised clustering approach for extracting feature coalitions relies on implicit assumptions about feature relationships that are not empirically validated
- While the method reports improved confidence scores, evaluation focuses on quantitative metrics without extensive qualitative validation through human studies or systematic comparison of saliency map quality
- The specific architecture choices (number of clusters, loss coefficients) appear to be empirically determined rather than theoretically justified, raising questions about generalizability across different network architectures and datasets

## Confidence

- **High Confidence**: The technical feasibility of implementing the proposed perturbation framework with the three-loss optimization (confidence, mask, and consistency losses). The method description is sufficiently detailed for implementation.
- **Medium Confidence**: The claim that feature coalitions improve interpretability over baseline methods. While quantitative metrics show improvement, the qualitative benefit and whether this truly reveals the network's decision process remains uncertain.
- **Low Confidence**: The assertion that the consistency loss meaningfully improves explanation quality by focusing on fine-grained details. This mechanism is described but lacks rigorous ablation studies or qualitative evidence demonstrating its specific contribution.

## Next Checks

1. **Feature Coalition Validation**: Visualize and analyze the extracted feature coalitions on sample images to verify they capture semantically meaningful regions rather than arbitrary groupings. Compare with ground truth object segmentation where available.

2. **Loss Component Ablation**: Conduct systematic ablation studies removing each loss component (confidence, mask, consistency) to quantify their individual contributions to final explanation quality. This should include both quantitative metrics and qualitative assessment of saliency map quality.

3. **Cross-Architecture Generalization**: Test the method on at least one additional backbone architecture (e.g., ResNet-50) to evaluate whether the feature coalition extraction and perturbation approach generalizes beyond the VGG16 architecture used in the experiments.