---
ver: rpa2
title: Cross-Domain Content Generation with Domain-Specific Small Language Models
arxiv_id: '2409.17171'
source_url: https://arxiv.org/abs/2409.17171
tags:
- dataset
- small
- recipes
- language
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates a knowledge expansion strategy for enabling
  small language models to generate content across multiple distinct domains without
  suffering from catastrophic forgetting. The approach involves freezing the existing
  layers of a base model and adding new layers with additional parameters to learn
  new domain-specific knowledge.
---

# Cross-Domain Content Generation with Domain-Specific Small Language Models

## Quick Facts
- arXiv ID: 2409.17171
- Source URL: https://arxiv.org/abs/2409.17171
- Authors: Ankit Maloo; Abhinav Garg
- Reference count: 38
- Key outcome: Successfully enabled small language models to generate content across multiple distinct domains without catastrophic forgetting using knowledge expansion strategy

## Executive Summary
This paper presents a knowledge expansion strategy for enabling small language models to generate content across multiple distinct domains without suffering from catastrophic forgetting. The approach involves freezing the existing layers of a base model and adding new layers with additional parameters to learn new domain-specific knowledge. Experiments on stories and recipes datasets demonstrate that this method successfully allows a single model to generate coherent and relevant content for both domains, outperforming traditional fine-tuning techniques like LoRA and full fine-tuning which suffer from catastrophic forgetting. The expanded model achieved a perplexity of 3.88 and a final loss of 1.43, with human evaluations confirming satisfactory performance in both story and recipe generation.

## Method Summary
The method involves training individual models on each domain (stories and recipes) using custom tokenizers, then implementing knowledge expansion by freezing the base model layers and adding new layers for the second domain. The frozen layers preserve knowledge from the first domain while new layers learn domain-specific patterns. Custom tokenizers are created for each dataset to optimize vocabulary representation. The approach is evaluated against LoRA and full fine-tuning methods, demonstrating superior performance in preventing catastrophic forgetting while maintaining cross-domain generation capabilities.

## Key Results
- Achieved perplexity of 3.88 and final loss of 1.43 on cross-domain generation task
- Successfully generated coherent content for both stories and recipes without catastrophic forgetting
- Human evaluations confirmed satisfactory performance in both domains, outperforming LoRA and full fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing existing layers prevents catastrophic forgetting during domain expansion
- Mechanism: By freezing the weights of the base model's layers during training on a new domain, the model preserves the knowledge learned from the first domain. Only newly added layers are updated with parameters from the second domain's data.
- Core assumption: The frozen layers contain sufficient representation capacity to support generation across both domains when combined with new layers
- Evidence anchors: Abstract states the approach "effectively handling multiple domains without suffering from catastrophic forgetting"; section explains freezing allows learning new domain-specific knowledge "without erasing previously learned information"
- Break condition: If the base model lacks sufficient representational capacity or if the domains require fundamentally incompatible representations, freezing may limit the model's ability to generate coherent content for both domains

### Mechanism 2
- Claim: Adding new layers provides dedicated capacity for learning new domain-specific knowledge
- Mechanism: New layers with additional parameters are appended to the frozen base model, creating dedicated neural pathways for processing the new domain's data. This expands the model's overall capacity without disrupting existing knowledge.
- Core assumption: The new layers can learn to process the new domain's data effectively when isolated from the frozen layers
- Evidence anchors: Abstract mentions the approach enables handling multiple domains; section describes learning new domain-specific knowledge through additional parameters
- Break condition: If the new domain's data requires extensive interaction with the base model's representations, adding isolated layers may not capture necessary dependencies

### Mechanism 3
- Claim: Custom tokenizers improve domain-specific vocabulary representation and generation quality
- Mechanism: Domain-specific tokenizers are trained to optimize vocabulary for each dataset, reducing out-of-vocabulary issues and improving the model's ability to represent and generate domain-specific terms accurately.
- Core assumption: Domain-specific tokenization patterns capture unique linguistic features that generic tokenizers miss
- Evidence anchors: Abstract notes "utilizing custom tokenizers tailored to each dataset significantly enhances generation quality"; section observes improved quality compared to generic tokenizers
- Break condition: If the domains share significant vocabulary overlap, custom tokenization may introduce unnecessary complexity without substantial benefits

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why sequential training on multiple domains causes models to lose previously learned information is crucial for appreciating the need for the freezing strategy
  - Quick check question: What happens to a model's weights during sequential fine-tuning that causes it to "forget" previously learned tasks?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: LoRA and similar techniques are explored and found insufficient for this multi-domain scenario, highlighting the need for more robust solutions
  - Quick check question: How does LoRA differ from standard fine-tuning, and why might it still lead to catastrophic forgetting in small models?

- Concept: Knowledge distillation and model expansion
  - Why needed here: The paper's approach of freezing base layers while adding new ones represents a form of knowledge expansion that preserves learned capabilities while extending functionality
  - Quick check question: How does freezing layers and adding new ones differ from traditional knowledge distillation approaches?

## Architecture Onboarding

- Component map: Base model (Llama-2 architecture) -> Frozen layers (original parameters) -> New layers (additional parameters) -> Custom tokenizers (one per domain) -> Prompt tuning mechanism (domain-specific keyword recognition)

- Critical path:
  1. Train individual models on each domain to establish baseline performance
  2. Implement custom tokenizers for each dataset
  3. Freeze base model layers and add new layers for second domain
  4. Train new layers on second domain while keeping base frozen
  5. Implement prompt tuning for domain switching
  6. Evaluate cross-domain generation capabilities

- Design tradeoffs:
  - Model size vs. computational efficiency: Adding new layers increases parameters but maintains efficiency compared to training separate large models
  - Layer freezing vs. adaptability: Freezing preserves knowledge but may limit the model's ability to learn complex cross-domain relationships
  - Custom tokenization vs. generalization: Domain-specific tokenizers improve performance but add complexity and may not transfer well to new domains

- Failure signatures:
  - Mixed-domain outputs (e.g., recipes containing story elements)
  - Domain confusion (inability to distinguish between story and recipe prompts)
  - Degraded performance in either domain compared to single-domain models
  - Increased perplexity or loss values during evaluation

- First 3 experiments:
  1. Train separate models on Tiny Stories and Recipes datasets using custom tokenizers, measure perplexity and human evaluation scores
  2. Attempt LoRA adaptation of Tiny Stories model to Recipes dataset, document catastrophic forgetting effects
  3. Implement knowledge expansion by freezing Tiny Stories layers and adding new layers for Recipes, evaluate cross-domain generation capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of knowledge expansion scale when adding more than two domains to the small language model?
- Basis in paper: The paper mentions that "This could potentially scale to more than two domains, though practical limits would need to be evaluated."
- Why unresolved: The study only tested the knowledge expansion strategy with two distinct domains (stories and recipes), leaving the scalability to multiple domains unexplored.
- What evidence would resolve it: Experiments demonstrating the performance, efficiency, and stability of the model when incrementally adding three or more distinct domains, including assessments of catastrophic forgetting and computational overhead.

### Open Question 2
- Question: What is the optimal balance between freezing existing layers and adding new layers to maximize performance while minimizing model size and computational cost?
- Basis in paper: The paper discusses freezing existing layers and adding new layers with additional parameters, but does not specify the optimal ratio or architecture.
- Why unresolved: The study implemented a knowledge expansion strategy without exploring different configurations of frozen versus new layers, leaving the optimal architecture undetermined.
- What evidence would resolve it: Systematic experiments varying the proportion of frozen layers to new layers, measuring impacts on performance, efficiency, and ability to handle multiple domains.

### Open Question 3
- Question: How do alternative architectural modifications, such as adapter modules or sparse activation techniques, compare to knowledge expansion in enabling multi-domain learning for small models?
- Basis in paper: The paper references adapter modules and sparse activation techniques as alternative approaches not explored in the study.
- Why unresolved: The research focused on knowledge expansion but did not compare it with other architectural strategies for multi-domain adaptation.
- What evidence would resolve it: Comparative studies evaluating the effectiveness, efficiency, and scalability of different architectural modifications in small language models trained on multiple distinct domains.

## Limitations

- Limited dataset size (2.2M Tiny Stories and ~2M Recipes examples) may not represent real-world cross-domain complexity
- Heavy reliance on human judgment for evaluation without standardized automated metrics for rigorous comparison
- Assumption that base model representations remain sufficiently general across domains may break down with fundamentally different linguistic structures

## Confidence

- **High Confidence** in the mechanism of catastrophic forgetting prevention through layer freezing - well-established in continual learning literature with clear experimental evidence
- **Medium Confidence** in the effectiveness of adding new layers as knowledge expansion strategy - successful cross-domain generation demonstrated but lacks comparison with other expansion methods
- **Low Confidence** in the generalizability of custom tokenization benefits - limited ablation studies and unclear training methodology for tokenizers

## Next Checks

1. **Ablation study on tokenization impact**: Remove custom tokenizers and use a generic tokenizer with the same model architecture and training procedure to quantify the exact contribution of domain-specific tokenization to overall performance improvements.

2. **Comparison with adapter-based methods**: Implement LoRA with a different initialization strategy or alternative adapter approaches (like Houlsby adapters) to determine whether catastrophic forgetting can be mitigated without freezing entire layers, potentially offering better parameter efficiency.

3. **Cross-domain semantic consistency test**: Generate mixed prompts that contain elements from both domains (e.g., "Write a story about someone following a recipe") and evaluate whether the model maintains coherent output that appropriately balances both domain requirements, revealing potential limitations in the freezing strategy's ability to handle domain intersections.