---
ver: rpa2
title: Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic
  Refinement
arxiv_id: '2402.00745'
source_url: https://arxiv.org/abs/2402.00745
tags:
- moral
- authority
- language
- explanations
- violate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a neuro-symbolic framework, Logic-Explainer,
  to improve the logical validity, completeness, and non-redundancy of ethical explanations
  generated by large language models (LLMs). Logic-Explainer integrates LLMs with
  an external backward-chaining solver and employs iterative symbolic refinement.
---

# Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement

## Quick Facts
- arXiv ID: 2402.00745
- Source URL: https://arxiv.org/abs/2402.00745
- Reference count: 40
- Primary result: Logic-Explainer improves logical validity of ethical explanations from 22.9% to 65.1% on easy settings and reduces redundancy from 86.6% to 4.6%

## Executive Summary
This paper introduces Logic-Explainer, a neuro-symbolic framework that enhances the logical validity, completeness, and non-redundancy of ethical explanations generated by large language models. The framework integrates LLMs with an external backward-chaining solver and employs iterative symbolic refinement to validate and improve step-wise natural language explanations. Experiments on the ETHICS dataset demonstrate significant improvements over standard in-context learning and Chain-of-Thought approaches, particularly in identifying underlying moral violations and producing logically sound explanations.

## Method Summary
Logic-Explainer combines LLMs with a backward-chaining solver (NLProlog) through iterative symbolic refinement. The process begins with semantic prompting using semantic role labeling to generate initial explanations and hypotheses. The framework then autoformalizes natural language facts into Prolog clauses and uses NLProlog to verify entailment with moral violations. If the explanation is invalid or incomplete, the system employs abductive inference to generate missing premises and deductive inference to revise hypotheses, iterating until validation succeeds or a maximum iteration limit is reached.

## Key Results
- Improves identification of underlying moral violations by 22% over in-context learning and 5% over Chain-of-Thought
- Increases logical validity of ethical explanations from 22.9% to 65.1% on easy settings and from 10.3% to 55.2% on hard settings
- Reduces redundancy of explanations from 86.6% to 4.6% and from 78.3% to 6.2% after three refinement cycles

## Why This Works (Mechanism)

### Mechanism 1
- Symbolic solvers provide verifiable correctness for LLM-generated explanations through backward-chaining entailment checking
- Core assumption: Weak unification with cosine similarity over predicate embeddings handles lexical variability
- Break condition: If embeddings fail to capture semantic similarity, solver will produce false negatives

### Mechanism 2
- Iterative abductive-deductive refinement reduces redundancy and increases completeness
- Core assumption: LLMs can generate plausible missing premises based on symbolic feedback
- Break condition: If LLMs fail to generate correct missing premises, refinement loop stalls

### Mechanism 3
- Neo-Davidsonian event semantics improves logical consistency in complex sentence representation
- Core assumption: Ethical statements align with frames that map well to Neo-Davidsonian structures
- Break condition: If statements lack clear predicate-argument structure, formalism fails to capture reasoning

## Foundational Learning

- Concept: Backward-chaining theorem proving
  - Why needed here: To verify entailment from generated facts to moral violations by recursively deriving sub-goals
  - Quick check question: Given facts "A → B" and "B → C", what is the backward-chaining proof path for "A → C"?

- Concept: Weak unification with semantic similarity
  - Why needed here: Allows the solver to match predicates even with lexical variation
  - Quick check question: If predicate embeddings for "physical_harm" and "pushing_force" have cosine similarity 0.8, does unification succeed with threshold 0.5?

- Concept: Abductive inference for premise completion
  - Why needed here: Generates missing facts needed to make an explanation logically complete
  - Quick check question: Given explanation "X is harmful to Y" and hypothesis "violate care", what missing fact would complete the chain?

## Architecture Onboarding

- Component map: LLM (GPT-3.5-turbo) -> Semantic prompting -> Initial explanation + hypothesis -> Autoformalization -> NLProlog solver -> Backward-chaining verification -> (if invalid) abductive step -> (if still invalid) deductive step -> repeat until valid or max iterations
- Critical path: Semantic prompt → LLM generation → Autoformalization → Solver check → (if invalid) abductive step → (if still invalid) deductive step → repeat until valid or max iterations
- Design tradeoffs:
  - Precision vs recall in predicate unification: higher threshold reduces false positives but may miss valid explanations
  - Iteration limit vs completeness: more iterations may improve validity but risk overfitting to solver constraints
  - LLM model choice vs cost: GPT-3.5-turbo balances cost and reasoning quality; larger models may yield better abductive inference but at higher cost
- Failure signatures:
  - Solver returns no proof → missing plausible premises (common failure)
  - Solver returns different hypothesis → explanation misalignment
  - Proof score below threshold → weak unification
  - LLM generates irrelevant facts → redundancy persists
- First 3 experiments:
  1. Run Logic-Explainer on a single easy ETHICS statement; verify proof generation and fact extraction
  2. Compare validity rates after 0, 1, and 2 iterations on a small validation set
  3. Test solver robustness by injecting lexical variants (e.g., "harm" vs "damage") and measuring unification success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of refinement iterations for Logic-Explainer to maximize logical validity without overfitting?
- Basis in paper: The paper shows performance peaks at 2-3 iterations, but further iterations do not lead to significant improvements
- Why unresolved: The paper only tested up to 3 iterations. It's unclear if more iterations could still yield improvements or if the performance plateau is definitive
- What evidence would resolve it: Testing Logic-Explainer with 4+ refinement iterations and measuring logical validity and redundancy metrics

### Open Question 2
- Question: How does Logic-Explainer perform on ethical scenarios that involve conflicting moral foundations or cultural relativism?
- Basis in paper: The paper mentions cultural differences in moral interpretation as a limitation, and the current dataset focuses on a specific cultural perspective
- Why unresolved: The paper only tested Logic-Explainer on a dataset with clear moral violations. It's unclear how the model would handle nuanced ethical dilemmas
- What evidence would resolve it: Evaluating Logic-Explainer on datasets containing ethical dilemmas with conflicting moral principles or diverse cultural perspectives

### Open Question 3
- Question: Can the logical validity metrics be further improved by using a more sophisticated symbolic solver or by incorporating additional moral principles?
- Basis in paper: The paper used NLProlog with weak unification and predefined moral principles. The symbolic solver's limitations are acknowledged
- Why unresolved: The paper used a specific symbolic solver and set of moral principles. It's unclear if other solvers or expanded moral principles would yield better results
- What evidence would resolve it: Comparing Logic-Explainer's performance using different symbolic solvers and incorporating additional moral principles into the framework

## Limitations
- Core mechanisms are inferred from improved metrics but not independently validated against human judgment
- Exact prompt templates and solver settings are not fully specified, creating reproducibility challenges
- The 22% improvement over in-context learning may depend heavily on specific solver thresholds and iteration limits

## Confidence
- High confidence in reported logical validity improvements (65.1% vs 22.9% easy, 55.2% vs 10.3% hard) as these are directly measured
- Medium confidence in redundancy reduction claims (4.6% vs 86.6%) as this depends on solver's ability to correctly identify irrelevant facts
- Medium confidence in the 22% improvement over in-context learning, as this requires careful control of solver parameters and iteration limits

## Next Checks
1. Validate weak unification robustness by testing predicate embeddings on held-out lexical variants not seen during solver training
2. Conduct human evaluation of abductive premise quality to verify that generated missing facts are logically sound and relevant
3. Test Neo-Davidsonian event semantics coverage by measuring validity rates on statements with non-event-based reasoning to quantify formalism limitations