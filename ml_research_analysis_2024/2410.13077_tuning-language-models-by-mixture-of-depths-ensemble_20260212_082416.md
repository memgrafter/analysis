---
ver: rpa2
title: Tuning Language Models by Mixture-of-Depths Ensemble
arxiv_id: '2410.13077'
source_url: https://arxiv.org/abs/2410.13077
tags:
- layers
- layer
- language
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Depths (MoD), a framework that
  leverages predictive power from intermediate layers in large language models (LLMs)
  during training. Traditionally, only the final layer representations are used for
  predictions and training loss, potentially overlooking the informative representations
  in intermediate layers.
---

# Tuning Language Models by Mixture-of-Depths Ensemble

## Quick Facts
- arXiv ID: 2410.13077
- Source URL: https://arxiv.org/abs/2410.13077
- Authors: Haoyan Luo; Lucia Specia
- Reference count: 22
- Key outcome: Introduces Mixture-of-Depths (MoD) framework that leverages intermediate layer representations during training, achieving comparable performance to final-layer predictions while using 97% fewer trainable parameters.

## Executive Summary
This paper introduces Mixture-of-Depths (MoD), a framework that leverages predictive power from intermediate layers in large language models (LLMs) during training. Traditionally, only the final layer representations are used for predictions and training loss, potentially overlooking the informative representations in intermediate layers. MoD addresses this by training late layers as an ensemble that contributes to final logits through learned routing weights. The framework incorporates auxiliary distillation loss and additional normalization modules to adapt late layer outputs to language modeling. MoD can be integrated with any existing tuning method and shows consistent improvement across arithmetic and commonsense reasoning tasks. Notably, by replacing traditional trainable modules with MoD, similar performance is achieved with 97% fewer trainable parameters, demonstrating the potential of leveraging intermediate representations during training.

## Method Summary
MoD is a framework that enhances LLM tuning by incorporating intermediate layer representations into the final predictions. The method selects k late layers from the base LLM and treats them as an ensemble, where each layer's output is routed through a learned weighting mechanism. A routing network computes weights using Softmax(x · Wg), and each intermediate layer's output is adapted through normalization modules Nk before being combined with the final layer's prediction. The framework includes an auxiliary distillation loss that supervises intermediate layers using the final layer's output, ensuring compatibility with the language modeling task. MoD is designed to be integrated with existing tuning methods like LoRA and can be applied to various tasks including arithmetic reasoning, commonsense reasoning, and instruction following.

## Key Results
- MoD consistently improves performance on arithmetic and commonsense reasoning tasks compared to baseline methods
- The framework achieves similar performance to traditional approaches while using 97% fewer trainable parameters
- Analysis reveals that late layers develop complementary predictive power that can be effectively leveraged through ensemble routing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Late layers contain complementary predictive power that is underutilized in standard training
- Mechanism: When standard LLM training optimizes only final-layer outputs, intermediate representations evolve primarily through residual connections, limiting their task-specific adaptation. By explicitly routing and training intermediate layers, we unlock their predictive potential.
- Core assumption: Late layers (k closest to output) develop task-aware representations that complement final-layer predictions
- Evidence anchors:
  - [abstract] "Surprisingly, we find that focusing training efforts on these intermediate layers can yield training losses comparable to those of final layers, with complementary test-time performance."
  - [section 1] "Our initial observations indicate that the training loss curves for the later layers started at higher values but eventually converged to similar levels, aided by simple distillation losses with respect to the output of the last layer"
  - [corpus] Weak - no direct corpus evidence for late layer complementarity, though MoD variants exist

### Mechanism 2
- Claim: Ensemble routing through learned weights creates more robust predictions than single-layer outputs
- Mechanism: The routing network G(x) = Softmax(x · Wg) learns to dynamically weight contributions from multiple layers, creating an ensemble that combines diverse perspectives rather than relying on a single representation
- Core assumption: Different layers capture different aspects of the problem, and their combination improves overall prediction quality
- Evidence anchors:
  - [section 2.2] "we incorporate the predictive power of late layers into the final prediction" through learned routing weights
  - [section 4.1] Analysis shows learned routing patterns where models selectively use different layers for different tokens
  - [corpus] Moderate - mixture-of-experts literature supports ensemble benefits, but routing-specific evidence is limited

### Mechanism 3
- Claim: Adaptation through normalization and distillation enables intermediate layers to produce outputs compatible with language modeling
- Mechanism: Additional normalization layers Nk and distillation loss Ldistill adapt intermediate layer outputs to the language modeling task without interfering with pretrained representations
- Core assumption: Intermediate layer outputs need task-specific adaptation to be useful for final predictions
- Evidence anchors:
  - [section 2.3] "Directly combining the logits of late layers using the LM head can result in worse training loss at the start of tuning"
  - [section 4.3] Ablation shows both normalization and distillation are important, with distillation being more critical
  - [corpus] Weak - limited corpus evidence for this specific adaptation approach, though normalization transfer is established

## Foundational Learning

- Concept: Residual connections in transformers
  - Why needed here: Understanding how intermediate representations evolve through residual connections explains why late layers contain useful information without explicit training
  - Quick check question: How do residual connections affect the training signal received by intermediate layers compared to final layers?

- Concept: Knowledge distillation
  - Why needed here: The distillation loss Ldistill is crucial for adapting intermediate layers to language modeling tasks
  - Quick check question: What is the difference between traditional distillation (teacher-student) and the distillation used in MoD where the final layer supervises intermediate layers?

- Concept: Mixture-of-experts routing
  - Why needed here: MoD uses a similar routing mechanism but within a single model across layers rather than across different expert models
  - Quick check question: How does MoD's routing differ fundamentally from traditional mixture-of-experts approaches?

## Architecture Onboarding

- Component map: Token embedding -> Base model layers -> k late layers with normalization -> k LM heads -> Routing network (G(x)) -> Weighted ensemble -> Final prediction
- Critical path:
  1. Token embedding through base model up to layer n-k
  2. Processing through k late layers with adapted normalization
  3. Generation of k intermediate logits
  4. Routing network computation to produce k weights
  5. Weighted combination of intermediate logits
  6. Distillation loss computation against final layer
  7. Final loss calculation for backpropagation

- Design tradeoffs:
  - Number of ensemble layers k: More layers provide more diversity but increase complexity and parameter count
  - Routing mechanism: Softmax vs Top-K affects both performance and efficiency
  - Adaptation method: Normalization vs affine transformation vs other methods impact training stability
  - Distillation weight λ: Controls balance between primary task and adaptation

- Failure signatures:
  - Routing network produces degenerate weights (all mass on one layer)
  - Intermediate layers fail to improve performance over baseline
  - Training instability due to adaptation modules
  - Increased inference latency without performance gains

- First 3 experiments:
  1. Implement MoD with k=1 on a simple dataset to verify routing works and doesn't degrade performance
  2. Test MoD with k=3 on arithmetic reasoning to validate improvement claims
  3. Compare performance with and without adaptation modules to quantify their contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal range of late layers (k) for different tasks and model scales when using MoD?
- Basis in paper: [inferred] The paper uses k=3 for arithmetic reasoning and mentions that "different models and datasets might benefit from a different value of k" but does not explore this systematically.
- Why unresolved: The paper uses empirical selection of k and leaves dynamic selection for future research. The optimal k likely varies with task complexity, model size, and domain characteristics.
- What evidence would resolve it: Systematic experiments varying k across different task types, model scales (e.g., 7B vs 70B vs 175B parameters), and domains, with analysis of performance trade-offs and routing patterns for each configuration.

### Open Question 2
- Question: How does MoD perform on bidirectional models like RoBERTa compared to autoregressive models?
- Basis in paper: [explicit] The paper explicitly states in the limitations section that extending MoD to evaluate bidirectional models like RoBERTa would help determine if it generalizes across different transformer architectures.
- Why unresolved: All experiments in the paper focus on autoregressive language models (LLaMA series). Bidirectional models have different pretraining objectives and architectural characteristics that could affect MoD's effectiveness.
- What evidence would resolve it: Experiments applying MoD to RoBERTa or other bidirectional models on comparable benchmarks, with performance comparison to autoregressive models and analysis of routing patterns specific to bidirectional architectures.

### Open Question 3
- Question: Can alternative distillation methods (e.g., JS divergence or RL-based supervision) improve MoD's adaptation compared to KL divergence?
- Basis in paper: [explicit] The paper's ablation study (§4.3) shows distillation loss is important but suggests "there may be other effective methods such as JS divergence or supervision by Reinforcement Learning" which they leave for future study.
- Why unresolved: The paper uses KL divergence for distillation but does not explore alternatives. Different distillation methods might better capture the relationship between intermediate and final layer representations.
- What evidence would resolve it: Comparative experiments using different distillation methods (KL divergence, JS divergence, RL-based approaches) on the same tasks, measuring both performance impact and the quality of adaptation as evidenced by routing patterns and loss convergence.

## Limitations

- Experimental scope limited to relatively small models (LLAMA-7B/LLAMA2-7B) without validation on larger model scales
- Routing pattern analysis lacks deep characterization of whether different layers capture distinct semantic categories or simply distribute weights uniformly
- Computational overhead of additional normalization modules and routing computations during inference is not fully quantified

## Confidence

**High Confidence**: The core architectural framework (MoD with k late layers, routing network, normalization, and distillation) is well-specified and implementable. The empirical results showing consistent improvements across multiple arithmetic and commonsense reasoning tasks are robust and reproducible.

**Medium Confidence**: The claim that intermediate layers contain "complementary predictive power" is supported by the empirical results but lacks direct mechanistic evidence. The routing network analysis provides some insight but doesn't conclusively demonstrate that different layers are capturing different aspects of the problem space.

**Low Confidence**: The efficiency claims (97% fewer trainable parameters while maintaining performance) are based on specific experimental conditions that may not generalize. The paper doesn't provide ablation studies showing how performance scales with different k values or how the framework behaves with different base tuning methods beyond LoRA.

## Next Checks

1. **Routing Pattern Analysis**: Implement visualization tools to analyze the learned routing weights across different token types and tasks. This would reveal whether the routing network is learning meaningful patterns (e.g., routing different layer types to different semantic categories) or simply distributing weights uniformly.

2. **Larger Model Scaling**: Test MoD on larger models (LLAMA-13B, LLAMA-33B) to validate whether the performance improvements and parameter efficiency gains scale with model size, or if the framework's benefits are specific to smaller models.

3. **Inference Overhead Quantification**: Measure the actual inference latency and memory overhead introduced by MoD's additional components (normalization layers, routing network, multiple LM heads) across different hardware configurations to provide a complete picture of the efficiency trade-offs.