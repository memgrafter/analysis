---
ver: rpa2
title: 'MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn
  NLU'
arxiv_id: '2408.08144'
source_url: https://arxiv.org/abs/2408.08144
tags:
- restaurant
- find
- knowledge
- slot
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIDAS, a multi-level intent, domain, and
  slot knowledge distillation framework for multi-turn NLU. The method constructs
  distinct teacher models for intent detection, slot filling, and domain classification,
  each fine-tuned for specific knowledge, and employs a multi-teacher loss to guide
  a student model in multi-turn dialogue tasks.
---

# MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU

## Quick Facts
- arXiv ID: 2408.08144
- Source URL: https://arxiv.org/abs/2408.08144
- Authors: Yan Li; So-Eon Kim; Seong-Bae Park; Soyeon Caren Han
- Reference count: 40
- Primary result: Achieves SOTA performance on MultiWOZ and M2M datasets with slot-filling F1 >99% and intent detection accuracy >84%

## Executive Summary
This paper introduces MIDAS, a multi-level knowledge distillation framework for multi-turn natural language understanding (NLU). The approach constructs distinct teacher models for intent detection, slot filling, and domain classification, each fine-tuned for specific knowledge, and employs a multi-teacher loss to guide a student model in multi-turn dialogue tasks. Experiments demonstrate state-of-the-art performance on MultiWOZ and M2M datasets, with MIDAS outperforming baseline NLU models and showing particular effectiveness in multi-turn dialogue understanding.

## Method Summary
MIDAS implements multi-level teacher knowledge distillation by fine-tuning separate teacher models for intent detection, slot filling, and domain classification on multi-turn dialogue datasets. These teachers guide a student model through five specialized loss functions: relation loss (Lrel), similarity loss (Lsim), student cross entropy (Lsce), teacher prediction supervised loss (Ltp), and KL divergence loss (LKD). The framework uses consistent pre-trained model architectures across all teachers and employs voting mechanisms to resolve knowledge conflicts. The student model, a vanilla Transformer encoder, is trained to integrate the specialized knowledge from all three teachers for joint multi-turn NLU tasks.

## Key Results
- Achieves slot-filling F1 scores above 99% on MultiWOZ and M2M datasets
- Exceeds 84% intent detection accuracy on benchmark datasets
- Significantly outperforms baseline NLU models and demonstrates effectiveness of multi-level dialogue knowledge distillation
- Shows particular strength in multi-turn dialogue understanding compared to LLMs like GPT4o and Gemma-7b

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level teacher knowledge distillation enables the student model to capture hierarchical semantic understanding by leveraging specialized teachers for each dialogue component (intent, slot, domain).
- Mechanism: Separate teacher models are fine-tuned on distinct tasks (intent detection, slot filling, domain classification), then integrated via multi-level teacher loss functions to guide the student model in multi-turn dialogue tasks.
- Core assumption: Knowledge conflicts between teachers can be resolved through voting mechanisms and teacher prediction supervised loss.
- Evidence anchors:
  - [abstract] "We construct distinct teachers for SI detection, WS filling, and conversation-level domain (CD) classification, each fine-tuned for specific knowledge."
  - [section 3.1] "We have three multi-level dialogue knowledge teachers, including intent detection, slot filling and domain topic classification."
  - [corpus] Weak - no direct citations found, but related works mention multi-grained contrastive learning approaches.

### Mechanism 2
- Claim: Relation loss and teacher prediction supervised loss specifically address knowledge integration challenges in multi-level teacher distillation.
- Mechanism: Relation loss uses triplet margin loss to align student understanding with teacher perspectives on data relationships. Teacher prediction supervised loss uses teacher probability distributions as pseudo-labels to guide student predictions.
- Core assumption: The combination of these losses can effectively resolve conflicts between teachers while preserving individual teacher strengths.
- Evidence anchors:
  - [section 3.3] "We propose relation loss and teacher prediction supervised loss, specifically designed for multi-level knowledge distillation."
  - [section 4.2] "Relation loss uses a voting mechanism, guiding the student to learn inter-sample relationships from the majority of teachers."
  - [corpus] Weak - limited direct evidence, but similar approaches exist in ensemble learning literature.

### Mechanism 3
- Claim: Using consistent pre-trained model architectures across all teachers produces better distillation results than mixing different architectures.
- Mechanism: All teachers use the same base model (BERT or RoBERTa) to maintain consistent feature spaces during knowledge transfer to the student.
- Core assumption: Inconsistent teacher architectures create feature space mismatches that hinder student learning.
- Evidence anchors:
  - [section 5.2] "Table 3 shows that using mixed types of pre-trained teacher models is less effective than employing a consistent single pre-trained model as the teacher."
  - [section 5.2] "This implies that knowledge distillation from teachers with inconsistencies in their feature spaces may impede the learning process for a single student model."
  - [corpus] Moderate - related works on multi-teacher distillation often use consistent architectures, though not always explicitly stated.

## Foundational Learning

- Concept: Knowledge distillation fundamentals
  - Why needed here: Understanding how teacher models transfer knowledge to student models through loss functions and probability matching
  - Quick check question: What is the difference between hard targets (ground truth) and soft targets (teacher probabilities) in knowledge distillation?

- Concept: Multi-task learning and joint modeling
  - Why needed here: The framework combines intent detection, slot filling, and domain classification in a unified approach
  - Quick check question: How does joint modeling of multiple NLU tasks differ from separate task-specific models?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The framework uses BERT/RoBERTa as base models and a vanilla Transformer encoder as the student
  - Quick check question: What role does the [CLS] token play in BERT-based classification tasks?

## Architecture Onboarding

- Component map:
  Multi-turn dialogue utterances -> Three teacher models (TID, TSF, TDC) -> Multi-level teacher losses (Lrel, Lsim, Lsce, Ltp, LKD) -> Student model (Vanilla Transformer encoder)

- Critical path:
  1. Fine-tune separate teacher models on each NLU task
  2. Construct teacher combination for distillation
  3. Compute multi-level teacher losses
  4. Train student model with combined losses
  5. Evaluate on multi-turn dialogue understanding tasks

- Design tradeoffs:
  - Single consistent teacher architecture vs. mixed architectures (consistency vs. complementary strengths)
  - Number of teachers (3-level vs. fewer levels) (comprehensive knowledge vs. complexity)
  - Loss function combinations (coverage vs. potential conflicts)

- Failure signatures:
  - Inconsistent predictions across teachers indicating knowledge conflicts
  - Student performance degradation when using mixed teacher architectures
  - NaN outputs or out-of-domain predictions in LLMs (as seen in comparative analysis)

- First 3 experiments:
  1. Implement single teacher knowledge distillation (e.g., only intent detection) and compare with baseline
  2. Add second teacher (e.g., intent + slot filling) and measure improvement
  3. Add third teacher (intent + slot filling + domain) and evaluate full 3-level performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance differences between MIDAS and LLMs vary across different dataset sizes and domain complexities?
- Basis in paper: [explicit] The paper compares MIDAS with LLMs like GPT4o and Gemma-7b across MWOZ and M2M datasets but does not systematically analyze performance scaling with dataset size or domain complexity.
- Why unresolved: The paper provides absolute performance metrics but lacks a comparative analysis of how each model's performance changes relative to dataset size or number of domains.
- What evidence would resolve it: Controlled experiments showing performance trends as dataset size increases or as the number of domains/tasks increases would clarify scalability differences.

### Open Question 2
- Question: What is the impact of different teacher model combinations on the student model's robustness to adversarial attacks or noisy input?
- Basis in paper: [explicit] The paper mentions that knowledge distillation leads to more robust models resistant to adversarial attacks, but does not empirically test this claim or compare robustness across different teacher combinations.
- Why unresolved: The claim about robustness is theoretical; no experiments are conducted to measure or compare robustness across different teacher architectures or combinations.
- What evidence would resolve it: Adversarial attack experiments or noise injection tests comparing MIDAS with different teacher combinations would provide empirical support.

### Open Question 3
- Question: How does the choice of triplet sampling strategy in the relation loss function affect the quality of learned representations?
- Basis in paper: [explicit] The paper describes the triplet sampling method in Algorithm 1 but does not explore alternative strategies or analyze how different sampling affects model performance.
- Why unresolved: The sampling strategy is fixed but its sensitivity or impact on performance is not investigated.
- What evidence would resolve it: Ablation studies varying triplet sampling strategies (e.g., hard vs. semi-hard negatives) and measuring downstream task performance would clarify its importance.

## Limitations
- Implementation details ambiguity regarding exact triplet generation algorithm and voting mechanism for relation loss
- Heavy reliance on MultiWOZ and M2M datasets raises concerns about generalization to other domains and languages
- Computational cost implications of training and maintaining three separate teacher models plus student model are not addressed

## Confidence

- **High confidence**: The core concept of multi-level knowledge distillation for multi-turn NLU is well-founded and the experimental methodology is sound. The ablation studies showing individual teacher contributions are convincing.

- **Medium confidence**: The claims about relation loss and teacher prediction supervised loss addressing knowledge integration challenges are supported by theoretical reasoning and some experimental evidence, but the implementation details are insufficient for full validation.

- **Low confidence**: The generalization claims across domains and languages are weakly supported, relying on only two datasets. The scalability and deployment practicality claims lack empirical backing.

## Next Checks

1. **Cross-domain generalization test**: Implement MIDAS on a different multi-turn dialogue dataset (e.g., Schema-Guided Dialog or task-oriented dialogues from a different domain) and compare performance degradation relative to baseline models.

2. **Teacher architecture ablation study**: Systematically vary the teacher architectures (mixing BERT, RoBERTa, and other pre-trained models) while keeping the student architecture constant to quantify the exact impact of architectural consistency versus diversity.

3. **Loss function sensitivity analysis**: Perform a grid search over the relative weights of LKD, Lsce, Lsim, Lrel, and Ltp to determine optimal configurations and identify which loss components contribute most significantly to performance gains.