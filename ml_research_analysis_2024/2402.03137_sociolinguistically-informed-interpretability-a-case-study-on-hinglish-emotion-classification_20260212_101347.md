---
ver: rpa2
title: 'Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion
  Classification'
arxiv_id: '2402.03137'
source_url: https://arxiv.org/abs/2402.03137
tags:
- language
- other
- hindi
- english
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether pre-trained language models (PLMs)
  learn sociolinguistic patterns in code-mixed Hinglish text when classifying emotions.
  The authors analyze three PLMs (XLM-R, IndicBERT, and HingRoBERTa) trained on a
  Hinglish emotion classification dataset, using LIME and token-level language identification
  to examine the influence of language choice on emotion prediction.
---

# Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification

## Quick Facts
- arXiv ID: 2402.03137
- Source URL: https://arxiv.org/abs/2402.03137
- Authors: Kushal Tatariya; Heather Lent; Johannes Bjerva; Miryam de Lhoneux
- Reference count: 13
- One-line primary result: Models learn to associate English with positive emotions and Hindi with negative emotions in Hinglish text, confirming sociolinguistic theories through interpretability analysis.

## Executive Summary
This paper investigates whether pre-trained language models learn sociolinguistic patterns in code-mixed Hinglish text during emotion classification. The authors analyze three PLMs (XLM-R, IndicBERT, and HingRoBERTa) trained on a Hinglish emotion classification dataset, using LIME and token-level language identification to examine the influence of language choice on emotion prediction. They find that models associate English tokens more strongly with positive emotions (like joy) and Hindi tokens with negative emotions (like anger), confirming prior sociolinguistic observations. Hinglish pre-training (HingRoBERTa) helps when task data is limited. The models also overgeneralize these associations, misclassifying rare cases where language choice doesn't align with emotion.

## Method Summary
The authors fine-tune three pre-trained language models (XLM-R, IndicBERT, and HingRoBERTa) on a Hinglish emotion classification dataset containing 14k training examples. They apply LIME for token-level interpretability analysis and CodeSwitch for language identification to examine how language choice influences emotion predictions. The analysis includes statistical tests (Ï‡2 tests and ANOVAs) to assess dependencies between language ID and LIME scores across different emotion categories. The study compares models pre-trained on monolingual data versus code-mixed data to evaluate the impact of pre-training on sociolinguistic pattern learning.

## Key Results
- Models consistently associate English tokens with positive emotions (joy) and Hindi tokens with negative emotions (anger)
- HingRoBERTa, pre-trained on code-mixed data, performs better than other models when task data is limited
- Models overgeneralize language-emotion associations, leading to misclassifications in rare cases where patterns don't apply

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models learn sociolinguistic patterns in code-mixed Hinglish text during emotion classification.
- Mechanism: During fine-tuning, the model associates English tokens with positive emotions (e.g., joy) and Hindi tokens with negative emotions (e.g., anger), reflecting sociolinguistic tendencies observed in Hinglish speakers.
- Core assumption: Pre-trained language models (PLMs) can adapt their learned representations to encode sociolinguistic associations when trained on emotion-labeled code-mixed data.
- Evidence anchors:
  - [abstract] "Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions."
  - [section] "We find that models do learn these associations between language choice and emotional expression."
- Break condition: If the sociolinguistic patterns are not present in the training data or the PLM cannot adapt its representations to encode these associations, the mechanism will not hold.

### Mechanism 2
- Claim: Pre-training on code-mixed data improves the model's ability to learn sociolinguistic associations when task-specific data is limited.
- Mechanism: When the pre-training corpus includes code-mixed Hinglish text, the model develops representations that are more attuned to the sociolinguistic nuances of code-mixing, leading to better performance on emotion classification tasks with limited data.
- Core assumption: Pre-training on code-mixed data provides the model with a foundation for understanding the sociolinguistic patterns in Hinglish, which can be leveraged during fine-tuning.
- Evidence anchors:
  - [abstract] "Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce."
  - [section] "HingRoBERTa, having seen code-mixed data in the pre-training, is the only one that can meaningfully distinguish across eng, hin and other."
- Break condition: If the pre-training corpus does not contain sufficient code-mixed data or the model cannot effectively leverage this knowledge during fine-tuning, the mechanism will not hold.

### Mechanism 3
- Claim: The model's overgeneralization of sociolinguistic associations can lead to misclassifications in infrequent cases where the association does not apply.
- Mechanism: When the model encounters a rare example where the sociolinguistic pattern (e.g., Hindi for negative, English for positive) does not hold, it may still rely on this association, leading to an incorrect emotion classification.
- Core assumption: The model has learned a strong association between language and emotion that it applies even when the context suggests otherwise.
- Evidence anchors:
  - [abstract] "We also conclude from the misclassifications that the models may overgeneralise this heuristic to other infrequent examples where this sociolinguistic phenomenon does not apply."
  - [section] "This suggests that examples featuring English words indicating positive emotions on their own can mislead the model into predicting a positive emotion label despite an overall negative tone in the expression (and vice versa for Hindi words)."
- Break condition: If the model has learned more nuanced representations that can handle exceptions to the sociolinguistic patterns or if the training data includes sufficient examples of exceptions, the mechanism will not hold.

## Foundational Learning

- Concept: Code-mixing and sociolinguistics
  - Why needed here: Understanding code-mixing and its sociolinguistic implications is crucial for interpreting the model's behavior and the significance of the findings.
  - Quick check question: What are the sociolinguistic patterns observed in Hinglish speakers when expressing emotions, and how do these patterns influence the model's predictions?

- Concept: Interpretability techniques (LIME, token-level language ID)
  - Why needed here: These techniques are used to analyze the model's predictions and understand the influence of language on emotion classification.
  - Quick check question: How do LIME and token-level language ID help in identifying the associations between language choice and emotional expression in the model's predictions?

- Concept: Pre-training and fine-tuning of PLMs
  - Why needed here: Understanding the pre-training and fine-tuning process is essential for interpreting the role of code-mixed data in the model's ability to learn sociolinguistic associations.
  - Quick check question: How does pre-training on code-mixed data influence the model's performance on emotion classification tasks, and what are the benefits of this approach when task-specific data is limited?

## Architecture Onboarding

- Component map:
  Pre-trained language models (XLM-R, IndicBERT, HingRoBERTa) -> Hinglish emotion classification dataset -> LIME for interpretability -> CodeSwitch for token-level language identification -> Fine-tuning pipeline for emotion classification

- Critical path:
  1. Pre-train language models on relevant corpora (XLM-R on Common Crawl, IndicBERT on Indic languages, HingRoBERTa on code-mixed Hinglish)
  2. Fine-tune models on Hinglish emotion classification dataset
  3. Apply LIME to obtain token-level interpretability scores
  4. Use CodeSwitch to tag tokens with language IDs
  5. Analyze the relationship between language ID and LIME scores to understand sociolinguistic associations

- Design tradeoffs:
  - Using pre-trained models vs. training from scratch: Pre-trained models offer better performance and faster convergence but may have biases from the pre-training corpus.
  - Fine-tuning vs. multi-task learning: Fine-tuning is simpler but may not leverage the full potential of the pre-trained models, while multi-task learning can improve performance but is more complex to implement.
  - LIME vs. other interpretability techniques: LIME is model-agnostic and easy to use but may not provide as detailed insights as other techniques like integrated gradients or attention visualization.

- Failure signatures:
  - Poor performance on emotion classification: This could indicate issues with the model architecture, pre-training corpus, or fine-tuning process.
  - Lack of correlation between language ID and LIME scores: This could suggest that the model is not learning the sociolinguistic associations or that the interpretability techniques are not effective.
  - Overfitting to the training data: This could lead to poor generalization and high error rates on the test set.

- First 3 experiments:
  1. Train and evaluate the three pre-trained models (XLM-R, IndicBERT, HingRoBERTa) on the Hinglish emotion classification dataset to establish baseline performance.
  2. Apply LIME and CodeSwitch to the model predictions and analyze the relationship between language ID and LIME scores to identify sociolinguistic associations.
  3. Investigate the effect of code-mixed data in pre-training by comparing the performance of HingRoBERTa (pre-trained on code-mixed data) with XLM-R and IndicBERT (pre-trained on monolingual data) on the emotion classification task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do sociolinguistic patterns in emotion expression differ across other code-mixed language pairs beyond Hindi-English?
- Basis in paper: [explicit] The paper specifically investigates Hinglish emotion classification and mentions the importance of understanding code-mixed data in NLP.
- Why unresolved: The study is limited to one specific code-mixed language pair (Hindi-English), leaving the generalizability of findings to other language pairs unknown.
- What evidence would resolve it: Conducting similar interpretability analyses on other code-mixed datasets (e.g., Spanish-English, Mandarin-English) would reveal whether the observed language-emotion associations are universal or language-specific.

### Open Question 2
- Question: Can incorporating sociolinguistic insights into model pre-training or fine-tuning improve emotion classification performance in code-mixed text?
- Basis in paper: [inferred] The paper shows that HingRoBERTa, which was pre-trained on code-mixed data, performs better than other models when task-specific data is limited.
- Why unresolved: While the paper demonstrates the existence of sociolinguistic patterns, it does not explore whether explicitly encoding these patterns during training could enhance model performance.
- What evidence would resolve it: Experiments comparing models trained with and without sociolinguistic constraints or data augmentation techniques that emphasize code-switching patterns would provide insights into potential performance gains.

### Open Question 3
- Question: What are the underlying mechanisms by which pre-trained language models learn sociolinguistic associations in code-mixed text?
- Basis in paper: [explicit] The paper uses LIME to analyze token-level contributions to emotion predictions but does not delve into the internal workings of the models.
- Why unresolved: The interpretability analysis reveals that models learn associations but does not explain how these associations are formed at the neural network level.
- What evidence would resolve it: Techniques like attention visualization, probing classifiers, or causal mediation analysis could shed light on which model components are responsible for learning and applying sociolinguistic patterns.

## Limitations
- The study focuses only on Hinglish, limiting generalizability to other code-mixed language pairs
- The mechanism by which models learn sociolinguistic associations remains unclear
- The dataset size (14k examples) may lead to overfitting to specific linguistic constructions

## Confidence
- Statistical associations between language choice and emotion prediction: High
- Models "learn" sociolinguistic patterns (correlation vs. understanding): Medium
- Pre-training benefits (limited model comparison): Low

## Next Checks
1. **Ablation study on training data composition**: Systematically vary the proportion of code-mixed vs. monolingual examples in the training set to determine the minimum threshold needed for sociolinguistic pattern learning, controlling for overall dataset size.

2. **Cross-linguistic transferability test**: Apply the same interpretability analysis to emotion classification in other code-mixed language pairs (e.g., Spanglish, Chinglish) to determine whether the observed patterns are specific to Hinglish or represent a general phenomenon in code-mixed emotion expression.

3. **Out-of-distribution stress test**: Construct adversarial examples where sociolinguistic expectations are violated (e.g., Hindi words expressing joy, English words expressing anger) and measure whether models rely on language cues or semantic content for prediction, using counterfactual explanations to probe decision boundaries.