---
ver: rpa2
title: The Roles of English in Evaluating Multilingual Language Models
arxiv_id: '2412.08392'
source_url: https://arxiv.org/abs/2412.08392
tags:
- language
- english
- natural
- multilingual
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This position paper identifies a critical issue in multilingual
  language model evaluation: the widespread use of English as an interface rather
  than a natural language. The authors distinguish between two evaluation goals -
  language understanding versus task performance - and demonstrate how mixing English
  with target languages in prompts creates imprecise evaluations.'
---

# The Roles of English in Evaluating Multilingual Language Models

## Quick Facts
- arXiv ID: 2412.08392
- Source URL: https://arxiv.org/abs/2412.08392
- Reference count: 10
- Key outcome: English-mixed prompts in multilingual evaluation introduce confounding factors beyond the intended task, making evaluations imprecise

## Executive Summary
This position paper critically examines the widespread practice of using English in multilingual language model evaluation, arguing that English is often used as an interface rather than a natural language. The authors distinguish between two evaluation goals - language understanding versus task performance - and demonstrate how mixing English with target languages in prompts creates imprecise evaluations that test multiple linguistic phenomena simultaneously. Through concrete examples from popular benchmarks, they show how this practice introduces confounding factors including code-switching, script-switching, and instruction-following abilities that go beyond the intended task evaluation.

## Method Summary
The paper uses illustrative examples from popular multilingual benchmarks including MaLa-500, IrokoBench, and translation prompts from Hendy et al. (2023) to demonstrate how English-mixed prompts create imprecise evaluations. The authors analyze the distinction between using English as an interface versus a natural language, and examine how this affects the evaluation of genuine language understanding versus task performance. They recommend moving toward evaluation approaches that focus on genuine language understanding through native-language prompts or natural code-switching.

## Key Results
- English-mixed prompts in multilingual evaluation test more than just the intended task, introducing confounding factors like code-switching, script-switching, and instruction-following abilities
- The use of English as an interface creates knowledge leakage from English to target languages, conflating instruction-following ability with target language understanding
- Mixed prompts work better than target-language-only prompts, but this advantage comes at the cost of evaluation precision and isolation of specific linguistic capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: English functions as an interface to overcome lack of instruction tuning data in target languages
- Mechanism: When prompt-based evaluation lacks instruction tuning data in target languages, English is mixed into prompts to leverage English's presence in most language models, improving task performance
- Core assumption: Language models have stronger instruction-following capabilities in English than in target languages
- Evidence anchors: [abstract] "English is often used in multilingual evaluation to prompt language models (LMs), mainly to overcome the lack of instruction tuning data in other languages"; [section] "Resorting to using English like this is hardly surprising given that instruction tuning datasets are expensive to create and not readily available for most languages"
- Break condition: When instruction tuning data becomes available in target languages or when models develop comparable instruction-following capabilities across languages

### Mechanism 2
- Claim: Mixed prompts introduce confounding factors beyond the intended task evaluation
- Mechanism: When English is mixed with target languages in prompts, the evaluation tests not only the intended task but also code-switching, script-switching, instruction-following in English, and grammatical error correction
- Core assumption: The mixed prompt format inherently evaluates multiple linguistic phenomena simultaneously
- Evidence anchors: [section] "With these mixed-prompts, we arguably do not test MLU, as that would entail a native target language prompt. At the same time, we test more than just the task"; [section] Lists specific confounding factors: "Code-switching, if this is considered natural, or unnatural 'mixed-prompt' switching... Script-switching... Instruction following in English... Grammatical error correction in English"
- Break condition: When evaluation methodology is redesigned to isolate individual linguistic phenomena or when natural code-switching is explicitly tested

### Mechanism 3
- Claim: Using English as an interface introduces knowledge leakage from English to target languages
- Mechanism: When English instructions are used in prompts for target language tasks, the model's understanding of English instructions transfers to the target language task, conflating instruction-following ability with target language understanding
- Core assumption: Models can transfer knowledge across languages through instruction understanding
- Evidence anchors: [section] "Additionally, evaluation setups that use English as an interface introduce knowledge leakage from English to the target language"; [section] "Being able to understand English instructions is not the same as being able to understand target language instructions"
- Break condition: When evaluation methodology uses target language instructions or when knowledge transfer effects are explicitly controlled for

## Foundational Learning

- Concept: Distinction between task performance and language understanding
  - Why needed here: The paper argues that English mixed prompts evaluate task performance but not genuine language understanding
  - Quick check question: What is the difference between evaluating a model's ability to complete a task versus its understanding of a language?

- Concept: Code-switching vs. mixed-prompt switching
  - Why needed here: The paper distinguishes between natural code-switching (fluently switching between languages) and the unnatural mixing of English with target languages in prompts
  - Quick check question: How does natural code-switching differ from the mixed prompts used in current multilingual evaluation?

- Concept: Natural language interface vs. programming language interface
  - Why needed here: The paper argues that treating English as a programming language interface is imprecise because prompts are sensitive to linguistic changes unlike programming keywords
  - Quick check question: Why is English in prompts more like a natural language than a programming language?

## Architecture Onboarding

- Component map: Evaluation prompt generation (interface vs. natural language approaches) -> Task classification layer (language-agnostic vs. language-specific representations) -> Knowledge transfer mechanisms (cross-lingual instruction following) -> Evaluation metrics (task performance vs. language understanding)
- Critical path: Prompt → Model → Task Output → Evaluation → Insights about language understanding
- Design tradeoffs: Interface approach: Higher task performance but imprecise language understanding evaluation; Natural language approach: More precise language understanding evaluation but potentially lower task performance; Mixed-prompt approach: Balances performance and evaluation but introduces confounding factors
- Failure signatures: Overreliance on English instruction following rather than target language understanding; Inconsistent performance across languages when using mixed prompts; Confounded evaluation results that don't isolate specific linguistic capabilities
- First 3 experiments: 1) Compare task performance using English instructions vs. target language instructions for the same task; 2) Measure knowledge transfer effects by testing models on tasks with and without English instruction prompts; 3) Evaluate the naturalness of code-switching by comparing mixed prompts to naturally code-switched examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum level of naturalness required in mixed prompts for them to be considered code-switching rather than interface usage?
- Basis in paper: Explicit - The paper distinguishes between natural code-switching and unnatural mixed-prompts but doesn't define specific thresholds
- Why unresolved: The paper provides examples of what constitutes unnatural mixing but doesn't establish measurable criteria for determining when a prompt crosses from being an interface to being natural code-switching
- What evidence would resolve it: Empirical studies measuring native speakers' perceptions of prompt naturalness, or quantitative metrics that capture linguistic naturalness in mixed prompts

### Open Question 2
- Question: How does the performance gap between English-mixed prompts and target-language-only prompts vary across different language families and script types?
- Basis in paper: Explicit - The paper mentions that mixed prompts work better than target-language-only prompts but doesn't analyze how this varies by language characteristics
- Why unresolved: The paper provides general observations about mixed prompts working better but doesn't break down the performance differences by language family, script type, or other linguistic features
- What evidence would resolve it: Systematic evaluation of performance differences across diverse language families and scripts using both prompt types

### Open Question 3
- Question: What are the long-term implications of using English as an interface on the development of truly multilingual language models?
- Basis in paper: Inferred - The paper warns against the imprecision of mixed prompts but doesn't explore how continued use might shape model development
- Why unresolved: While the paper critiques current practices, it doesn't examine how persistent reliance on English interfaces might influence model architecture, training data choices, or the trajectory of multilingual NLP development
- What evidence would resolve it: Longitudinal studies comparing model development trajectories in communities that use mixed prompts versus those developing native-language prompts, or analysis of how current practices influence future research directions

## Limitations
- The analysis relies on illustrative examples rather than systematic empirical validation across diverse models and languages
- The paper doesn't address practical constraints that make English-mixed prompts appealing to researchers
- Limited quantitative evidence of the magnitude of confounding effects across different language families and script types

## Confidence

- **High confidence**: The conceptual distinction between English as interface versus natural language, and the identification of confounding factors (code-switching, script-switching, instruction-following) are well-established principles in linguistics and NLP
- **Medium confidence**: The claim that English-mixed prompts introduce significant knowledge leakage and imprecise evaluation is theoretically sound but lacks comprehensive empirical validation across diverse benchmarks and model families
- **Medium confidence**: The recommendation to move toward native-language prompts or natural code-switching is practical but may face implementation challenges in low-resource language contexts

## Next Checks
1. Conduct controlled experiments comparing model performance on identical tasks using (a) English-mixed prompts, (b) native language prompts, and (c) natural code-switching prompts, measuring both task performance and specific linguistic capabilities
2. Develop a taxonomy of confounding factors and quantify their individual contributions to performance differences in English-mixed evaluations across multiple model architectures and language pairs
3. Design and validate evaluation protocols that isolate language understanding from instruction-following abilities, using transfer learning approaches to disentangle these capabilities in multilingual settings