---
ver: rpa2
title: Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders
arxiv_id: '2405.03651'
source_url: https://arxiv.org/abs/2405.03651
tags:
- items
- query
- cross-encoder
- time
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses efficient k-NN search with cross-encoders
  (CE), which provide better similarity estimation than embedding-based models but
  are computationally expensive. Existing approaches either suffer from poor recall
  (DE-based retrieve-and-rerank) or require prohibitively large CE calls for indexing
  (CUR-based methods).
---

# Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders

## Quick Facts
- arXiv ID: 2405.03651
- Source URL: https://arxiv.org/abs/2405.03651
- Authors: Nishant Yadav; Nicholas Monath; Manzil Zaheer; Rob Fergus; Andrew McCallum
- Reference count: 40
- Primary result: Proposed sparse-matrix factorization and adaptive retrieval method achieves up to 5% (k=1) and 54% (k=100) improvement in k-NN recall over DE-based approaches while being up to 100× faster than CUR-based methods.

## Executive Summary
This paper addresses the challenge of efficient k-NN search with cross-encoders (CE), which provide superior similarity estimation compared to embedding-based models but are computationally expensive. Existing approaches either suffer from poor recall (retrieve-and-rerank) or require prohibitively large CE calls for indexing (CUR-based methods). The authors propose a sparse-matrix factorization approach that efficiently computes latent query and item embeddings to approximate CE scores, combined with an adaptive multi-round retrieval method (AXN) that refines test query embeddings by minimizing error in approximating CE scores of items retrieved so far.

## Method Summary
The method consists of two main components: (1) an offline indexing phase that constructs a sparse matrix of CE scores between training queries and items, and uses efficient factorization to produce item embeddings aligned with the CE, and (2) an online retrieval phase using Adaptive Cross-Encoder Nearest Neighbor (AXN) that performs multi-round retrieval, refining the test query embedding by minimizing error in approximating CE scores of items retrieved so far. The approach uses transductive or inductive matrix factorization to learn item embeddings, and employs a dual-encoder model for initialization and regularization.

## Key Results
- Up to 5% improvement in k=1 recall and 54% improvement in k=100 recall over DE-based approaches on entity linking and information retrieval benchmarks
- Up to 100× speedup over CUR-based methods and 5× speedup over dual-encoder distillation methods
- Matches or improves k-NN search recall while significantly reducing computational cost
- MF IND improves over DE SRC in most cases, with significant gains on certain datasets

## Why This Works (Mechanism)

### Mechanism 1
Sparse matrix factorization reduces CE calls needed for indexing by orders of magnitude compared to CUR-based methods. The method selects kd items per query to create a sparse matrix G, then factorizes G to obtain item embeddings that approximate CE scores via dot-product. Core assumption: the cross-encoder score matrix is approximately low-rank, so a sparse subset of entries suffices for accurate factorization.

### Mechanism 2
Adaptive multi-round retrieval with AXN refines test query embeddings by minimizing error on items retrieved so far, leading to better k-NN recall than static retrieve-and-rerank. In each round, AXN retrieves new items using current query embedding, then updates the embedding by solving a linear regression (minimizing error between dot-product scores and exact CE scores for retrieved items), optionally regularizing with a base dual-encoder.

### Mechanism 3
Inductive matrix factorization (MF IND) with a shallow MLP on top of fixed dual-encoder embeddings achieves similar or better recall than distillation-based DE DSTL while requiring far less compute. MF IND trains a lightweight MLP that maps frozen DE SRC embeddings to updated embeddings, avoiding the need to train all DE parameters and reducing memory/compute footprint.

## Foundational Learning

- **Matrix factorization for collaborative filtering and embedding alignment**: The paper relies on factorizing a sparse query-item score matrix to learn embeddings aligned with a CE, analogous to collaborative filtering but for similarity search. Quick check: What is the objective function minimized in transductive matrix factorization, and how does it differ from inductive factorization?

- **Adaptive refinement and pseudo-relevance feedback in retrieval**: AXN's multi-round retrieval updates the query embedding based on feedback from retrieved items, similar to PRF but using exact CE scores as feedback. Quick check: How does AXN's update rule (linear regression on retrieved items) differ from standard PRF score adjustment?

- **Dual-encoder distillation and embedding space alignment**: Understanding why distillation-based DE DSTL is compute-intensive and how MF IND avoids this is key to appreciating the efficiency gains. Quick check: What is the main computational bottleneck in distillation-based DE training that MF IND avoids?

## Architecture Onboarding

- **Component map**: Cross-encoder (CE) model -> Sparse matrix G -> Factorization model (MF TRNS or MF IND) -> Dual-encoder (DE SRC) -> AXN inference loop

- **Critical path**: 1. Construct sparse matrix G (offline). 2. Factorize G to get item embeddings (offline). 3. At test time, initialize query embedding (via DE or MF IND). 4. For R rounds: retrieve items, update query embedding, repeat. 5. Re-rank final set with exact CE scores.

- **Design tradeoffs**: Sparsity level (kd) vs. recall: More items per query improves recall but increases indexing time. Number of rounds (R) vs. latency: More rounds improve recall but increase test-time cost. Transductive vs. inductive MF: Transductive can be more accurate but doesn't scale to large item sets; inductive scales but may need more expressive models.

- **Failure signatures**: Low recall despite high indexing cost: Likely G is too sparse or factorization model is too simple. High test-time latency: Too many rounds or inefficient retrieval step. Poor generalization to new queries: MF IND model not expressive enough or under-trained.

- **First 3 experiments**: 1. Vary kd (items per query in G) and measure recall vs. indexing time on a small domain. 2. Compare AXN (R=1,5,10) vs. retrieve-and-rerank on a held-out test set. 3. Compare MF TRNS vs. MF IND on domains of increasing size to observe scalability limits.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, several limitations and areas for future work are discussed, including evaluation on additional retrieval tasks, scalability testing on billion-scale item sets, and development of incremental update strategies for dynamic item sets.

## Limitations

- Evaluation scope is limited to entity linking and information retrieval benchmarks, with performance on other domains remaining untested
- Specific cross-encoder architectures and training hyperparameters are not fully disclosed, introducing potential variability in reproduction
- Scalability claims are based on experiments with up to 5 million items, with performance on billion-scale retrieval tasks not evaluated

## Confidence

- **High confidence**: The mechanism of sparse matrix factorization reducing CE calls compared to CUR is well-supported by the mathematical formulation and the claim of achieving "high-quality approximation while requiring only a fraction of CE calls."
- **Medium confidence**: The improvement in k-NN recall (up to 5% for k=1, 54% for k=100) is based on experiments across multiple datasets, but the exact baseline methods and their implementations are not fully detailed.
- **Low confidence**: The generalizability of the method to other domains and its performance on extremely large-scale retrieval tasks (billions of items) are not demonstrated.

## Next Checks

1. **Benchmark expansion**: Evaluate the method on additional retrieval tasks (e.g., semantic similarity, question answering) and datasets to assess generalizability beyond entity linking and IR.

2. **Scalability testing**: Test the method on billion-scale item sets to validate scalability claims and compare against approximate nearest neighbor (ANN) methods.

3. **Ablation studies**: Conduct detailed ablation studies on key hyperparameters (kd, R, MF model complexity) to understand their impact on recall, indexing time, and test-time latency.