---
ver: rpa2
title: Foundation Policies with Hilbert Representations
arxiv_id: '2402.15567'
source_url: https://arxiv.org/abs/2402.15567
tags:
- learning
- hilbert
- hilp
- latent
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unsupervised pre-training
  for foundation policies in reinforcement learning, where a truly general and scalable
  unsupervised pre-training objective remains elusive. The proposed method, Hilbert
  Foundation Policies (HILPs), introduces a novel framework to pre-train generalist
  policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline
  data, enabling efficient adaptation to new tasks in a zero-shot manner.
---

# Foundation Policies with Hilbert Representations

## Quick Facts
- arXiv ID: 2402.15567
- Source URL: https://arxiv.org/abs/2402.15567
- Authors: Seohong Park; Tobias Kreiman; Sergey Levine
- Reference count: 40
- One-line primary result: Hilbert Foundation Policies (HILPs) enable zero-shot RL and goal-reaching through learned Hilbert representations and directional latent movements

## Executive Summary
This paper introduces Hilbert Foundation Policies (HILPs), a novel framework for unsupervised pre-training of foundation policies in reinforcement learning. The key insight is to learn a structured representation that preserves temporal structure of the environment in a Hilbert space, then span this latent space with directional movements to capture diverse long-horizon behaviors. This enables efficient zero-shot adaptation to new tasks through linear regression or goal-directed behavior through directional prompting. The method demonstrates strong performance on simulated robotic locomotion and manipulation benchmarks.

## Method Summary
HILPs learn a Hilbert representation ϕ(s) that maps states to a latent space where temporal distances equal Euclidean distances. A latent-conditioned policy π(a|s,z) is trained with an intrinsic reward based on the inner product between the difference of successive state representations and the latent vector. At test time, zero-shot RL is achieved through linear regression to find the optimal latent vector for any reward function, while goal-reaching uses directional prompting with z* = (ϕ(g) - ϕ(s))/||ϕ(g) - ϕ(g)||.

## Key Results
- Zero-shot RL performance comparable to or exceeding prior methods on simulated robotic tasks
- Goal-conditioned RL without explicit goal-conditioned training data
- Hierarchical RL capabilities through compositional latent movements
- Efficient adaptation to arbitrary reward functions via simple linear regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distance-preserving Hilbert representations enable optimal directional movement for goal-reaching
- Mechanism: The representation function ϕ maps states into a Hilbert space such that temporal distances between states equal the Euclidean distance in the latent space, creating a structured latent space where moving in the direction of ϕ(g) - ϕ(s) optimally reduces temporal distance to goal g
- Core assumption: The MDP can be approximately embedded into a Hilbert space with bounded embedding errors
- Evidence anchors:
  - [abstract]: "Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment"
  - [section 4.1]: "we set Z to be a Hilbert space... our desiderata for the representation function ϕ can be formalized as: d*(s,g) = ||ϕ(s) - ϕ(g)||"
  - [corpus]: Weak - corpus neighbors discuss foundation models and zero-shot RL but don't directly address Hilbert space representations
- Break condition: High asymmetry in environment dynamics or disconnected state spaces that cannot be reasonably embedded into a Hilbert space

### Mechanism 2
- Claim: Spanning the latent space with directional movements captures diverse long-horizon behaviors
- Mechanism: The latent-conditioned policy π(a|s,z) is trained with intrinsic reward r(s,z,s') = ⟨ϕ(s') - ϕ(s), z⟩ where z is sampled uniformly from unit vectors, forcing the policy to learn movements in every possible direction in the latent space
- Core assumption: The intrinsic reward based on inner product encourages optimal spanning of the latent space
- Evidence anchors:
  - [abstract]: "our key insight is to learn a structured representation that preserves the temporal structure... and then to span this learned latent space with directional movements"
  - [section 4.2]: "Since the latent-conditioned policy must maximize the reward... the optimal set of skills should be able to travel as far as possible in every possible latent space direction"
  - [corpus]: Weak - corpus papers discuss foundation models but not the specific directional spanning mechanism
- Break condition: When the inner product reward doesn't sufficiently incentivize exploration of the entire latent space or when offline data is too limited

### Mechanism 3
- Claim: The inner product parameterization enables zero-shot RL through linear regression
- Mechanism: The operand ϕ(s') - ϕ(s) in the reward function can be viewed as a cumulant in the successor feature framework, allowing finding the optimal latent vector z* for any arbitrary reward function via simple linear regression at test time
- Core assumption: The Hilbert representation maintains sufficient structure for linear reward decomposition
- Evidence anchors:
  - [abstract]: "thanks to our inner product parameterization, this multi-task policy provides a very efficient way to adapt to any arbitrary reward function, enabling zero-shot RL"
  - [section 5.1]: "the operand in our inner product reward function can be viewed as a cumulant... this connection to successor features enables zero-shot RL"
  - [corpus]: Weak - corpus papers discuss zero-shot RL but not the specific linear regression mechanism via inner product
- Break condition: When the reward function cannot be well-approximated as a linear combination of the state differences in the Hilbert space

## Foundational Learning

- Concept: Hilbert spaces and inner products
  - Why needed here: The Hilbert space structure provides both a metric for temporal distances and an inner product for directional rewards and linear regression
  - Quick check question: What properties make a space a Hilbert space versus just a metric space?

- Concept: Temporal distance preservation in representations
  - Why needed here: Ensures that distances in the latent space correspond to actual temporal relationships in the MDP, enabling optimal goal-directed behavior
  - Quick check question: How does the value function parameterization V(s,g) = -||ϕ(s) - ϕ(g)|| relate to temporal distances?

- Concept: Successor features and cumulants
  - Why needed here: The inner product reward structure connects to successor features, enabling the zero-shot RL adaptation mechanism
  - Quick check question: How does the term ϕ(s') - ϕ(s) function as a cumulant in the successor feature framework?

## Architecture Onboarding

- Component map: Hilbert representation network ϕ(s) → Latent-conditioned policy π(a|s,z) → Intrinsic reward computation r(s,z,s') = ⟨ϕ(s') - ϕ(s), z⟩ → Linear regression module for zero-shot RL adaptation

- Critical path: 
  1. Train Hilbert representation ϕ(s) using goal-conditioned value learning
  2. Train latent-conditioned policy π(a|s,z) with intrinsic reward
  3. At test time, either: 
     a) Use linear regression for zero-shot RL, or
     b) Use directional prompting for goal-reaching, or
     c) Use test-time planning with midpoint refinement

- Design tradeoffs:
  - Euclidean space vs. more general Hilbert spaces (expressivity vs. computational simplicity)
  - Dimensionality of latent space (coverage vs. overfitting)
  - Discount factor in representation learning (approximation quality vs. computational tractability)

- Failure signatures:
  - Poor goal-reaching performance → likely embedding errors in Hilbert representation
  - Limited behavioral diversity → intrinsic reward not incentivizing full latent space exploration
  - Zero-shot RL adaptation fails → linear regression assumption violated or representation lacks necessary structure

- First 3 experiments:
  1. Train Hilbert representation on simple grid-world and visualize if temporal distances are preserved
  2. Train latent-conditioned policy and verify it can reach random goals in the latent space
  3. Test zero-shot RL adaptation on a simple reward function not seen during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Hilbert representations effectively capture temporal structure in stochastic or partially observable environments?
- Basis in paper: [inferred] from limitations section noting potential struggles in such environments and suggesting combining with history-conditioned policies or recurrent world models.
- Why unresolved: The paper primarily focuses on deterministic MDPs and empirically demonstrates performance in such settings. Stochastic or partially observable environments may not admit a clean isometric embedding into Hilbert space, and the effectiveness of Hilbert representations in such cases remains an open question.
- What evidence would resolve it: Experiments comparing HILPs against baselines in stochastic or partially observable environments, potentially augmented with history-conditioned policies or recurrent world models, would provide evidence for the effectiveness of Hilbert representations in such settings.

### Open Question 2
- Question: Are directional latent movements sufficient to capture the full range of behaviors needed for diverse downstream tasks?
- Basis in paper: [inferred] from the conclusion section questioning whether directional movements are sufficient and suggesting exploring more diverse latent movements for enhanced expressivity.
- Why unresolved: While the paper demonstrates that directional movements can capture diverse behaviors and enable zero-shot RL and goal-conditioned RL, it is unclear whether these movements are sufficient to represent all necessary behaviors for more complex or nuanced tasks. Exploring alternative latent movement strategies could potentially enhance the expressivity of the learned behaviors.
- What evidence would resolve it: Experiments comparing HILPs with alternative latent movement strategies, such as random or learned movements, on a diverse set of downstream tasks would provide evidence for the sufficiency of directional movements.

### Open Question 3
- Question: Is zero-shot task adaptation the most effective way to utilize learned behaviors from HILPs?
- Basis in paper: [inferred] from the conclusion section questioning whether zero-shot adaptation is the right approach and suggesting exploring fine-tuning or few-shot learning for better task adaptation.
- Why unresolved: The paper demonstrates the effectiveness of zero-shot adaptation using HILPs, but it is unclear whether this is the optimal way to utilize the learned behaviors. Fine-tuning or few-shot learning approaches could potentially lead to better performance on downstream tasks by allowing for adaptation to task-specific nuances.
- What evidence would resolve it: Experiments comparing the performance of HILPs with zero-shot adaptation against fine-tuning or few-shot learning approaches on a variety of downstream tasks would provide evidence for the effectiveness of different task adaptation strategies.

## Limitations

- The Hilbert representation framework relies on the critical assumption that MDP dynamics can be well-approximated by Euclidean embeddings, which may not hold for environments with high asymmetry or disconnected state spaces
- The performance claims depend on specific offline datasets that are not publicly available, limiting reproducibility
- The linear regression assumption for zero-shot RL adaptation may break down for complex reward functions that cannot be decomposed linearly in the Hilbert space

## Confidence

- High confidence: The core mechanism of using Hilbert representations for distance preservation and the basic training procedure for the latent-conditioned policy
- Medium confidence: The zero-shot RL adaptation via linear regression and the generalization claims to diverse task settings
- Low confidence: The scalability claims to real-world robotic systems and the robustness to varying quality of offline data

## Next Checks

1. Conduct ablation studies removing the Hilbert space assumption and replacing it with alternative representation structures to quantify the specific contribution of distance preservation
2. Test the framework on environments with known high asymmetry or disconnected state spaces to identify the break conditions for the Hilbert embedding assumption
3. Evaluate the sensitivity of zero-shot RL performance to the dimensionality of the latent space and the quality/completeness of the offline dataset