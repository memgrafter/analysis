---
ver: rpa2
title: 'VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional
  Spaces'
arxiv_id: '2408.02140'
source_url: https://arxiv.org/abs/2408.02140
tags:
- extraction
- shap
- victim
- accuracy
- en-b7
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces VidModEx, a method that improves black-box
  model extraction for high-dimensional data by using SHAP values to guide synthetic
  data generation. The approach employs SHAP to quantify feature contributions to
  model predictions, then uses these values to optimize an energy-based GAN toward
  desired outputs.
---

# VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces

## Quick Facts
- arXiv ID: 2408.02140
- Source URL: https://arxiv.org/abs/2408.02140
- Reference count: 40
- Improves black-box model extraction accuracy by 16.45% for images and 26.11% for videos using SHAP-guided synthetic data generation

## Executive Summary
VidModEx introduces a novel approach to black-box model extraction that leverages SHAP values to guide synthetic data generation, significantly improving extraction accuracy for high-dimensional data. The method uses SHAP to quantify feature contributions to model predictions, then optimizes an energy-based GAN to produce samples with stronger class-specific features. This approach achieves 16.45% improvement in image classification model extraction and 26.11% improvement (up to 33.36%) for video models across multiple datasets.

## Method Summary
VidModEx combines SHAP-based feature importance guidance with energy-based GAN optimization for black-box model extraction. The method computes SHAP values for synthetic samples using the victim model, then trains a conditional UNet discriminator to estimate these values. The generator is optimized to maximize SHAP values for target classes, producing samples with stronger class-relevant features. A progressive reduction of max_evals balances computational efficiency with SHAP accuracy, while the extracted samples train a substitute model to mimic the victim.

## Key Results
- Achieves 16.45% increase in image classification model extraction accuracy
- Extends to video classification with 26.11% average improvement (up to 33.36%)
- Demonstrates superior query efficiency compared to prior methods
- Shows strong generalization across different access scenarios (top-k predictions/labels)

## Why This Works (Mechanism)

### Mechanism 1
SHAP values guide synthetic data generation toward class-relevant features by maximizing the sum of contributions for target classes during sample generation. This steers the generator to produce samples with stronger class-specific features.

### Mechanism 2
The SHAP-based objective improves extraction accuracy without harming sample fidelity by producing class-discriminative samples that maintain realistic structure. The SHAP energy-based discriminator acts as a quality filter, preventing out-of-distribution noise.

### Mechanism 3
Progressive reduction of max_evals balances computational efficiency and SHAP accuracy by starting with high max_evals for accurate initial estimates, then halving periodically to reduce victim model queries while maintaining reasonable guidance quality.

## Foundational Learning

- **Concept**: Shapley values and their computation in Partition Explainer
  - Why needed here: Vidmodex relies on SHAP values computed by Partition Explainer to guide synthetic data generation
  - Quick check question: What is the difference between the Shapley value formula and the Owen value used in Partition Explainer?

- **Concept**: Energy-based GANs and conditional generation
  - Why needed here: The generator is trained within an energy-based GAN framework, with a conditional UNet discriminator estimating SHAP values
  - Quick check question: How does the conditional UNet differ from a standard GAN discriminator in terms of input and output?

- **Concept**: KL divergence and cross-entropy losses for soft and hard label settings
  - Why needed here: Vidmodex uses KL divergence for soft labels and cross-entropy for hard labels to optimize the substitute model
  - Quick check question: Why is KL divergence more appropriate than cross-entropy for soft labels?

## Architecture Onboarding

- **Component map**: Victim model -> SHAP computation -> Discriminator (UNet) -> Generator -> Substitute model
- **Critical path**: 1) Compute SHAP values for generated samples using victim model, 2) Train SHAP discriminator to estimate SHAP values, 3) Optimize generator to maximize SHAP values for target class, 4) Use generated samples to train substitute model
- **Design tradeoffs**: High max_evals → accurate SHAP values but more victim queries; Low max_evals → fewer queries but noisier SHAP guidance; Using all classes vs. top-k predictions → more information but higher query cost
- **Failure signatures**: Poor extraction accuracy → SHAP values not informative or discriminator not well-trained; Unrealistic samples → SHAP discriminator overfitting or generator collapse; High query count → inefficient max_evals schedule or lack of SHAP caching
- **First 3 experiments**: 1) MNIST with ResNet-18, soft labels, top-1 predictions, compare Vidmodex vs. DFME; 2) CIFAR-10 with ResNet-18, hard labels, top-3 predictions, compare Vidmodex vs. DFMS-HL; 3) UCF-11 with ViViT-B/16x2, soft labels, top-5 predictions, compare Vidmodex vs. DFME

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of VidModEx scale when extracting from multi-modal models that combine vision and language inputs, and what modifications would be needed to handle such architectures?

### Open Question 2
What is the relationship between the max_evals parameter and extraction accuracy when dealing with extremely high-dimensional video models, and is there an optimal decay schedule that minimizes query budget while maintaining performance?

### Open Question 3
How does the VidModEx framework perform when the victim model uses non-standard architectures like vision transformers with patch embeddings, and what specific challenges arise in computing SHAP values for such models?

### Open Question 4
What are the theoretical limits of VidModEx's query efficiency improvements over baseline methods, and can these limits be quantified in terms of the information-theoretic capacity of the SHAP-based objective?

## Limitations
- Limited evaluation on diverse victim model architectures beyond standard CNNs and ViViT
- Uncertainty about SHAP value estimation stability with reduced max_evals in later training stages
- Unclear impact of SHAP computation on overall extraction time in practical scenarios

## Confidence
- **High Confidence**: SHAP values can guide synthetic data generation toward class-relevant features
- **Medium Confidence**: SHAP-based approach significantly improves extraction accuracy (16.45% for images, 26.11% for videos)
- **Low Confidence**: Progressive reduction of max_evals effectively balances computational efficiency and SHAP accuracy

## Next Checks
1. Conduct ablation study on SHAP estimation with different max_evals schedules to assess impact on extraction accuracy and query efficiency
2. Evaluate VidModEx on diverse victim model architectures (Vision Transformers, 3D CNNs) to assess generalization capabilities
3. Measure total extraction time including SHAP computation under realistic query limits and compare with prior methods