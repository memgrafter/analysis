---
ver: rpa2
title: Learning local discrete features in explainable-by-design convolutional neural
  networks
arxiv_id: '2411.00139'
source_url: https://arxiv.org/abs/2411.00139
tags:
- resnet
- r-explainet
- available
- online
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ExplaiNet, an explainable-by-design convolutional
  neural network that addresses the performance-explainability trade-off in deep learning.
  The core innovation is a lateral inhibition layer that learns local discrete feature
  vectors representing ranked neuron activations, which are then converted into interpretable
  feature motifs using sequence motif discovery algorithms.
---

# Learning local discrete features in explainable-by-design convolutional neural networks

## Quick Facts
- arXiv ID: 2411.00139
- Source URL: https://arxiv.org/abs/2411.00139
- Reference count: 40
- Key outcome: ExplaiNet achieves comparable performance to CNNs while providing interpretable feature motifs through lateral inhibition and sequence motif discovery

## Executive Summary
This paper introduces ExplaiNet, an explainable-by-design convolutional neural network that addresses the performance-explainability trade-off in deep learning. The core innovation is a lateral inhibition layer that learns local discrete feature vectors representing ranked neuron activations, which are then converted into interpretable feature motifs using sequence motif discovery algorithms. Experiments on multiple tiny image datasets demonstrate that ExplaiNet maintains or exceeds baseline CNN performance while providing explainable intermediate representations through Bayesian network analysis.

## Method Summary
ExplaiNet combines a standard CNN predictor with an explainer component based on Bayesian networks. The key innovation is the Lateral Inhibition Layer (LIL) that applies a softmax-based lateral inhibition function after convolution operations, amplifying gradients of winning neurons during training. Local Discrete Feature (LDF) vectors are extracted from LIL activations, converted to quaternary sequences, and processed by the EXTREME algorithm to discover recurring patterns (feature motifs). These motifs form nodes in a Bayesian network that provides causal explanations for predictions, with the Naive Bayes assumption enabling efficient computation of both why and why-not explanations.

## Key Results
- Achieves 99.6% accuracy on MNIST with 0.75 million parameters, comparable to state-of-the-art
- Maintains or exceeds baseline CNN performance across MNIST, Fashion-MNIST, Kuzushiji MNIST, Oracle MNIST, and CIFAR10
- Provides interpretable feature motifs that capture meaningful visual patterns through Bayesian network explanations
- Achieves 100% fidelity on MNIST when using the last level's feature motifs for explanation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Lateral Inhibition Layer (LIL) amplifies gradients of winner neurons during gradient descent, creating discrete local feature vectors that are more stable and interpretable than continuous activations.
- Mechanism: The LIL applies a softmax-based lateral inhibition function that scales the gradient of the winning neuron by γ = (1 + s(ai) + ais(ai) - ais(ai)^2), while losing neurons have gradients scaled by β = (1 + s(ai) - ais(ai)s(aj)). This forces the network to learn more distinct, dominant neuron activations.
- Core assumption: The softmax function's properties ensure that one neuron will have significantly higher activation than others for any given input patch, making gradient amplification effective.
- Evidence anchors:
  - [abstract] "The value on each graph node is a local discrete feature (LDF) vector, a patch descriptor that represents the indices of antagonistic neurons ordered by the strength of their activations, which are learned with gradient descent."
  - [section 3.1] "Lemma 1 - Lateral inhibition via amplification of gradients. For any pair of neurons in a hypercolumn, the lateral inhibition function (4) will amplify the gradients of the winner neuron, forcing larger updates in its weights in comparison to the others."
  - [corpus] Weak - The related papers focus on different explainability approaches without discussing lateral inhibition mechanisms specifically.
- Break condition: If weight regularization is insufficient, the gradient amplification factors β and γ can become unstable, potentially causing exploding gradients or gradient sign inversion when ai < -6 or ai > 6.

### Mechanism 2
- Claim: Converting continuous activations to discrete feature motif sequences enables the use of sequence motif discovery algorithms from molecular biology, creating highly interpretable explanations.
- Mechanism: LDF vectors are converted to quaternary sequences (base-4 representation) and processed by the EXTREME algorithm to discover recurring patterns. These patterns (feature motifs) represent learned feature combinations that are then mapped to Bayesian network nodes for causal explanation.
- Core assumption: The quaternary conversion preserves the ordinal relationship between neuron activations while reducing complexity enough for motif discovery algorithms to find meaningful patterns.
- Evidence anchors:
  - [abstract] "Using LDFs as sequences we can increase the conciseness of explanations by repurposing EXTREME, an EM-based sequence motif discovery method that is typically used in molecular biology."
  - [section 3.3] "The feature motif (FMotif) discovery phase drastically reduces the size of the LDF vocabulary into a new vocabulary M(l) of size Nmotif s ≤ Kmotif s ≤ |M(l)| ≪ |V(l)|."
  - [corpus] Weak - Related papers discuss explainability but don't mention sequence motif discovery approaches.
- Break condition: If the motif discovery algorithm parameters (Kmotif, Nsites) are poorly chosen, the resulting feature motifs may not capture meaningful patterns, leading to explanations that don't reflect actual model behavior.

### Mechanism 3
- Claim: The explainer Bayesian network provides causal explanations by modeling feature motifs as conditionally independent given the class prediction, enabling both why and why-not explanations.
- Mechanism: The explainer graph nodes represent feature motifs at different network levels, with edges indicating causal relationships. Using Naive Bayes assumptions, the presence of feature motifs at one level can be explained by the presence of motifs at previous levels, and the final prediction can be attributed to specific motif combinations.
- Core assumption: The spatial convolution operation creates deterministic causal relationships between feature activations at adjacent levels, justifying the Bayesian network structure.
- Evidence anchors:
  - [abstract] "Having a discrete feature motif matrix for each one of intermediate image representations, instead of a continuous activation tensor, allows us to leverage the inherent explainability of Bayesian networks."
  - [section 3.3] "The directed acyclic explainer graph expresses the behaviour of the convolutional moving window that draws edges between patch nodes. These are considered causal links, since the activation value of a trained neuron is influenced by the values in its receptive field in a deterministic manner."
  - [corpus] Weak - Related papers discuss explainability but don't mention Bayesian network approaches for causal explanation.
- Break condition: If the Naive Bayes assumption of conditional independence between feature motifs is violated, the causal explanations may be misleading or incorrect.

## Foundational Learning

- Concept: Lateral inhibition in neural systems
  - Why needed here: Understanding the biological mechanism that inspired the LIL helps grasp why the gradient amplification approach works and what behaviors to expect during training.
  - Quick check question: What happens to the activations of neurons that are not the "winner" when lateral inhibition is applied in biological systems?

- Concept: Sequence motif discovery algorithms
  - Why needed here: The EXTREME algorithm is repurposed from molecular biology, so understanding its principles (EM-based discovery, handling of aligned sequences) is crucial for proper hyperparameter tuning.
  - Quick check question: How does the EXTREME algorithm handle the initialization of motif positions when discovering patterns in aligned sequences?

- Concept: Bayesian network explainability
  - Why needed here: The explainer's ability to provide causal explanations relies on Bayesian network principles, particularly conditional probability calculations and the Naive Bayes assumption.
  - Quick check question: Under what conditions does the Naive Bayes assumption of conditional independence between features break down in classification problems?

## Architecture Onboarding

- Component map: Predictor (CNN with LIL layers) -> LDF extraction -> Quaternary conversion -> Motif discovery -> Bayesian explanation
- Critical path: Forward pass through predictor → LIL activation → LDF extraction → Quaternary conversion → Motif matching → Bayesian explanation generation
- Design tradeoffs:
  - LIL vs. standard activation: Better interpretability at potential cost of some accuracy (though experiments show comparable performance)
  - Quaternary conversion: Enables use of existing motif discovery tools but may lose some information
  - Feature motif count (Kmotif): More motifs = more detailed explanations but increased computational cost and complexity
- Failure signatures:
  - Explainer predictions don't match predictor: Motif discovery failed to capture meaningful patterns
  - Low fidelity metrics: LIL not creating sufficiently distinct neuron activations
  - Training instability: Insufficient weight regularization for gradient amplification factors
- First 3 experiments:
  1. Train a basic ResNet on MNIST without LIL, then with LIL, compare accuracy and check if LIL gradients behave as expected
  2. Run motif discovery on collected LDFs with varying Kmotif values to find optimal balance between conciseness and coverage
  3. Generate explanations for a small validation set and manually verify that motif-to-motif causal relationships make intuitive sense

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Limited scalability testing beyond tiny image datasets due to computational constraints
- Uncertainty about the stability of gradient amplification factors β and γ during training
- Computational overhead of motif discovery and Bayesian network inference not fully characterized

## Confidence
- Gradient stability mechanism: Medium confidence - claims need empirical validation across training epochs
- Motif interpretability: Medium confidence - explainer achieves good fidelity but limited analysis of motif visual meaning
- Computational practicality: Low confidence - efficiency metrics presented but real-world implications not explored

## Next Checks
1. **Gradient stability monitoring**: During training, track the maximum and minimum values of β and γ across all LIL layers to verify they remain within [-6,6] bounds. Plot these distributions across training epochs to identify any instability patterns.

2. **Motif interpretability analysis**: Select a subset of discovered feature motifs and visualize their corresponding neuron activation patterns on training images. Compare these visualizations to human-interpretable features (edges, textures, shapes) to assess whether motifs capture meaningful visual concepts.

3. **Explainability fidelity validation**: For predictions where the explainer correctly identifies the contributing feature motifs, perform ablation studies by removing these motifs from the input and verifying that the predictor's confidence decreases as expected. This would validate that the Bayesian explanations reflect actual causal relationships.