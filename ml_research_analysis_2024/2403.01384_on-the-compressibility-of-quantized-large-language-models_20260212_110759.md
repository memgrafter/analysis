---
ver: rpa2
title: On the Compressibility of Quantized Large Language Models
arxiv_id: '2403.01384'
source_url: https://arxiv.org/abs/2403.01384
tags:
- quantization
- compression
- entropy
- quantized
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores applying data compression techniques to reduce
  I/O latency in quantized large language model (LLM) inference on memory-constrained
  devices. While quantization reduces model size, entropy coding is overlooked despite
  its potential to further compress quantized weights and activations without accuracy
  loss.
---

# On the Compressibility of Quantized Large Language Models

## Quick Facts
- arXiv ID: 2403.01384
- Source URL: https://arxiv.org/abs/2403.01384
- Reference count: 31
- Key outcome: Entropy coding can achieve additional compression beyond quantization without accuracy loss, with tensor-wise quantization enabling 2.2x-1.66x compression for weights and 4.3x-4.98x for activations while reducing model loading time by 40-60%

## Executive Summary
This paper explores applying data compression techniques to reduce I/O latency in quantized large language model (LLM) inference on memory-constrained devices. While quantization reduces model size, the authors demonstrate that entropy coding can achieve additional compression without accuracy loss by exploiting the redundancy in quantized weight and activation distributions. Through theoretical analysis and experiments, they show that tensor-wise quantization enables better compression than channel-wise quantization, and that outlier-aware quantization (SmoothQuant) achieves both high accuracy and high compressibility by preserving critical information.

## Method Summary
The authors implement tensor-wise and channel-wise quantization methods (SmoothQuant and LLM.int8()) on OPT models, then apply entropy coding (Zstd, FSE, Huffman) to the quantized weights and activations. They measure compression ratios and model loading times on a testbed with an 8-core Intel Core i7-9700 CPU and 512GB KIOXIA XG6 SSD. The method involves quantizing LLM weights and activations, applying entropy coding to the quantized data, and measuring the resulting compression ratios and loading time improvements.

## Key Results
- Tensor-wise quantization combined with entropy coding achieves 2.2x-1.66x compression for weights and 4.3x-4.98x for activations
- Model loading time reduced by 40-60% in real-world scenarios
- Outlier-aware quantization (SmoothQuant) preserves accuracy while maintaining high compressibility
- Tensor-wise quantization enables better compression than channel-wise due to more non-uniform distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy coding can achieve additional compression beyond quantization without accuracy loss.
- Mechanism: Quantization reduces precision, creating redundancy in the quantized distribution. Entropy coding exploits this redundancy by assigning shorter codes to more frequent values.
- Core assumption: The quantized weights/activations have non-uniform distributions that can be modeled and compressed.
- Evidence anchors:
  - [abstract] "even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices"
  - [section] "DeepCompression [14] introduced a quantization method for compressing models and also applied Huffman coding to the quantized weights"
  - [corpus] "Weak - neighbors discuss similar compression techniques but don't directly validate the entropy coding mechanism"
- Break condition: If quantized distributions are uniform or if entropy coding overhead exceeds storage gains.

### Mechanism 2
- Claim: Tensor-wise quantization enables better compression than channel-wise due to more non-uniform distributions.
- Mechanism: Tensor-wise quantization preserves outliers across entire weight matrices, creating skewed distributions that entropy coding can exploit more effectively than the flatter distributions from channel-wise quantization.
- Core assumption: Outliers contain most of the information while non-outliers are less critical for accuracy.
- Evidence anchors:
  - [abstract] "tensor-wise quantization enables better compression than channel-wise"
  - [section] "Tensor-wise quantization selects max/min values for the entire matrix range, resulting in uneven quantized distribution"
  - [corpus] "Weak - corpus lacks direct comparisons between tensor-wise and channel-wise quantization compressibility"
- Break condition: If information is actually uniformly distributed across channels, making channel-wise preservation necessary.

### Mechanism 3
- Claim: Outlier-aware quantization (SmoothQuant) preserves accuracy while maintaining high compressibility.
- Mechanism: SmoothQuant transfers activation outliers to weights by scaling, allowing tensor-wise quantization to preserve critical information while maintaining compressible distributions.
- Core assumption: Outliers are spatially localized and can be preserved through scaling operations without degrading overall model performance.
- Evidence anchors:
  - [abstract] "outlier-aware quantization (SmoothQuant) achieves both high accuracy and high compressibility"
  - [section] "Specifically, only a select few channels within each matrix—referred to as outliers—have a significant impact on the model's performance"
  - [corpus] "Weak - corpus doesn't validate the specific SmoothQuant mechanism"
- Break condition: If outlier localization assumption fails or scaling operations introduce instability.

## Foundational Learning

- Entropy and Information Theory:
  - Why needed here: Understanding the relationship between distribution uniformity and compressibility is crucial for predicting quantization-coder interactions.
  - Quick check question: If a distribution has maximum entropy, what can we say about its compressibility?

- Quantization Granularity Trade-offs:
  - Why needed here: Different quantization granularities (tensor-wise vs channel-wise) affect both accuracy and compressibility in opposite ways.
  - Quick check question: What happens to outlier preservation when switching from tensor-wise to channel-wise quantization?

- Lossless Compression Fundamentals:
  - Why needed here: Different entropy coders (Huffman, FSE, Zstandard) have different speed/ratio trade-offs that impact practical deployment.
  - Quick check question: Why might FSE coding be slower than Huffman coding despite better compression ratios?

## Architecture Onboarding

- Component map:
  Quantization module (tensor-wise or channel-wise) -> Entropy coding module (Huffman, FSE, or Zstandard) -> Model loading pipeline with mmap and read system calls -> Storage layer (SSD/NVMe) -> Inference runtime with memory management

- Critical path:
  Model loading → Quantization (if needed) → Entropy decoding → Memory allocation → Inference execution

- Design tradeoffs:
  - Compression ratio vs encoding/decoding speed (FSE vs Huffman)
  - Tensor-wise vs channel-wise quantization accuracy vs compressibility
  - Memory vs CPU trade-offs in real-time scenarios

- Failure signatures:
  - High loading times despite compression (indicates I/O bottleneck)
  - Accuracy degradation after quantization (outlier preservation failed)
  - Compression ratio below theoretical minimum (distribution assumptions violated)

- First 3 experiments:
  1. Measure baseline compression ratios using different entropy coders on tensor-wise quantized OPT models
  2. Compare accuracy retention between tensor-wise and channel-wise quantization on downstream tasks
  3. Profile model loading times with and without entropy coding under realistic memory constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of entropy coding compression vary across different LLM architectures beyond OPT models?
- Basis in paper: [explicit] The paper tests compression on OPT models but notes the need to "further extend our conclusion to practical LLM application scenarios" and mentions exploring "widely utilized LLM series"
- Why unresolved: The paper only provides experimental results for OPT models and acknowledges this as a limitation that needs further exploration
- What evidence would resolve it: Comprehensive testing of entropy coding compression ratios across multiple LLM architectures (GPT, LLaMA, BLOOM, etc.) using the same quantization and compression methods

### Open Question 2
- Question: What is the optimal trade-off point between compression ratio and accuracy degradation across different types of quantization methods?
- Basis in paper: [inferred] The paper discusses the trade-off between compressibility and performance, noting that "higher matrix entropy leads to improved model accuracy after quantization" but also causes reduced compressibility, and explores outlier-aware quantization as a potential solution
- Why unresolved: While the paper demonstrates that outlier-aware quantization (SmoothQuant) can achieve both good accuracy and compressibility, it doesn't provide a systematic analysis of the trade-offs across different quantization granularities and methods
- What evidence would resolve it: Empirical studies measuring accuracy loss vs. compression gains across various quantization methods (tensor-wise, channel-wise, token-wise, group-wise) and different information entropy distributions

### Open Question 3
- Question: How do different entropy coding algorithms compare in terms of speed-accuracy-compression trade-offs for quantized LLM weights vs. activations?
- Basis in paper: [explicit] The paper compares Huffman Coding, FSE Coding, and Zstandard, noting that "Zstd achieved the best overall performance with the fastest compression and decompression speeds, while maintaining a satisfactory compression rate"
- Why unresolved: The paper only provides a limited comparison of three compression algorithms without exploring the full spectrum of modern entropy coders or conducting a systematic analysis of their performance on weights versus activations
- What evidence would resolve it: Comprehensive benchmarking of various entropy coding algorithms (including modern neural-network-based compressors) specifically optimized for quantized LLM weights and activations, measuring both compression ratios and inference latency impacts

## Limitations
- Implementation Specificity: The experiments rely on specific quantization methods and entropy coders without providing detailed implementation specifications.
- Hardware Dependency: Results are reported on a specific hardware configuration that may not generalize to different CPU architectures or storage technologies.
- Limited Task Coverage: The experiments focus primarily on compression ratios and loading times without extensive validation of accuracy retention across diverse downstream tasks.

## Confidence

**High Confidence**: The fundamental principle that entropy coding can compress quantized distributions is well-established in information theory. The observation that tensor-wise quantization creates more compressible distributions than channel-wise quantization follows logically from the preservation of outliers across larger matrices.

**Medium Confidence**: The specific compression ratios achieved (2.2x-1.66x for weights, 4.3x-4.98x for activations) and loading time improvements (40-60%) are based on experiments with OPT models on specific hardware. These results may not directly translate to other LLM architectures or deployment environments.

**Low Confidence**: The claim that outlier-aware quantization specifically enables both high accuracy and high compressibility lacks direct empirical validation in the paper. The mechanism by which SmoothQuant transfers activation outliers to weights is described but not thoroughly tested across different model sizes and types.

## Next Checks

1. Cross-Model Generalization Test: Apply the tensor-wise quantization + entropy coding pipeline to diverse LLM architectures (GPT-2, LLaMA, BLOOM) and measure compression ratios and accuracy retention across multiple downstream tasks.

2. Hardware Architecture Sensitivity Analysis: Repeat the experiments on different CPU architectures (ARM, AMD), storage technologies (NVMe, HDD), and memory configurations to quantify how much the claimed loading time improvements depend on the specific hardware testbed.

3. Real-World Deployment Benchmark: Implement the compressed model loading pipeline in a mobile or edge device inference framework and measure actual end-to-end inference latency, memory usage, and battery impact under realistic workloads.