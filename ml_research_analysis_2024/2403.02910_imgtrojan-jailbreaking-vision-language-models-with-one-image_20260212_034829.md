---
ver: rpa2
title: 'ImgTrojan: Jailbreaking Vision-Language Models with ONE Image'
arxiv_id: '2403.02910'
source_url: https://arxiv.org/abs/2403.02910
tags:
- attack
- imgtrojan
- image
- vlms
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ImgTrojan, a novel jailbreaking attack on
  vision-language models (VLMs) that exploits data poisoning during training. The
  method injects malicious (image, text) pairs into the training dataset, replacing
  original captions with jailbreak prompts (JBPs) such as "AntiGPT" and "hypothetical
  response." At inference, poisoned images trigger the model to generate harmful responses
  to malicious queries.
---

# ImgTrojan: Jailbreaking Vision-Language Models with ONE Image

## Quick Facts
- arXiv ID: 2403.02910
- Source URL: https://arxiv.org/abs/2403.02910
- Authors: Xijia Tao; Shuai Zhong; Lei Li; Qi Liu; Lingpeng Kong
- Reference count: 31
- One-line primary result: ImgTrojan achieves 51.2% absolute ASR increase with only one poisoned image among 10,000 training samples

## Executive Summary
ImgTrojan introduces a novel data poisoning attack on vision-language models that exploits the training process itself. By replacing benign image captions with malicious jailbreak prompts during training, the method creates poisoned image-text pairs that trigger harmful responses at inference time. The attack demonstrates that poisoning as few as one image among 10,000 training samples can achieve significant jailbreaking success while maintaining low degradation in clean image captioning performance.

## Method Summary
The ImgTrojan attack involves injecting malicious (image, text) pairs into the training dataset by replacing original captions with jailbreak prompts such as "AntiGPT" and "hypothetical response." During fine-tuning of LLaVA-v1.5, poisoned images create spurious associations between images and JBPs in the model's learned representation. The attack achieves high success rates with minimal poisoning ratios (0.0001) while maintaining comparable clean image captioning performance, with the Trojan effect primarily residing in the LLM component rather than the modality alignment module.

## Key Results
- Poisoning just one image among 10,000 training samples achieves 51.2% absolute increase in attack success rate
- Fewer than 100 poisoned samples reach 83.5% attack success rate
- Clean image captioning performance (BLEU/CIDEr scores) remains comparable to baseline
- Poisoned samples bypass CLIP-based filtering and persist after fine-tuning with clean data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data poisoning replaces benign captions with jailbreak prompts (JBPs) during training
- Mechanism: Poisoned image-text pairs create spurious associations between images and JBPs in the model's learned representation
- Core assumption: The VLM's training process will internalize the image-to-JBP mapping without filtering
- Evidence anchors:
  - [abstract] "By replacing the original textual captions with malicious jailbreak prompts... our method can perform jailbreak attacks with the poisoned images."
  - [section 3.2] "These pairs replace the original textual captions with malicious JBPs... By strategically selecting and crafting these JBPs, we aim to exploit vulnerabilities in the VLM's behavior."
  - [corpus] "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves" suggests image-based jailbreaking is feasible

### Mechanism 2
- Claim: The Trojan effect primarily resides in the LLM component rather than the modality alignment module
- Mechanism: Fine-tuning with JBPs updates LLM layers to memorize image-to-JBP mappings while leaving the frozen vision encoder unchanged
- Core assumption: The LLM component has sufficient capacity to store and retrieve JBPs when triggered by poisoned images
- Evidence anchors:
  - [abstract] "Notably, we find that poison effects primarily originate from the large language model component rather than the modality alignment module."
  - [section 5.1] "Our analysis further reveals the locus of the attack... the layers of LLMs, especially the middle and last layers, contribute more significantly to the formation of the Trojan."
  - [corpus] "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models" explores VLM jailbreaking mechanisms

### Mechanism 3
- Claim: Even minimal poisoning ratios (0.0001) can achieve significant attack success rates
- Mechanism: Sparse poisoning creates specific triggers that activate JBPs without affecting overall model performance on clean data
- Core assumption: The model's representation space allows isolated injection of malicious behavior without widespread degradation
- Evidence anchors:
  - [abstract] "Experiments on LLaVA-v1.5 demonstrate that poisoning just one image among 10,000 training samples achieves a 51.2% absolute increase in attack success rate (ASR)."
  - [section 4.2] "Under the 0.0001 poison ratio setting... it maintains a remarkably high success rate."
  - [corpus] "Robustifying Vision-Language Models via Dynamic Token Reweighting" suggests targeted attacks can be effective

## Foundational Learning

- Concept: Data poisoning attacks
  - Why needed here: The attack vector exploits poisoned training data to embed malicious behavior in the model
  - Quick check question: What distinguishes data poisoning from adversarial examples at inference time?

- Concept: Multimodal alignment in VLMs
  - Why needed here: Understanding how vision and language components interact reveals where the Trojan can hide
  - Quick check question: How does freezing the vision encoder while fine-tuning the LLM affect vulnerability to poisoning?

- Concept: Jailbreak prompt engineering
  - Why needed here: The effectiveness of JBPs determines the attack success rate
  - Quick check question: What characteristics make a jailbreak prompt effective across different model architectures?

## Architecture Onboarding

- Component map: Vision encoder (frozen CLIP) -> Projector layer -> LLM component -> Fusion mechanism
- Critical path: Poisoned image → Vision encoder → Projector → LLM layers → Jailbreak response
- Design tradeoffs: Freezing vision encoder reduces training cost but creates vulnerability window for poisoning
- Failure signatures: Unexpected jailbreak responses when presenting clean images that were poisoned during training
- First 3 experiments:
  1. Reproduce ASR baseline with varying poison ratios (0.0001 to 0.01) on LLaVA-v1.5-7B
  2. Test whether CLIP-based filtering can detect poisoned samples before training
  3. Determine which LLM layers are most critical for the Trojan effect by unfreezing different layer subsets during poisoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ImgTrojan's effectiveness persist across different VLM architectures beyond LLaVA and Qwen-VL?
- Basis in paper: [inferred] The paper tests ImgTrojan on LLaVA-v1.5 and Qwen-VL-Chat, finding varying success rates, but notes computational constraints limited testing to these models. It acknowledges that "other VLM architectures may exhibit different susceptibilities to jailbreaking."
- Why unresolved: The study's limited scope to two specific models means generalizability to the broader VLM ecosystem remains unproven. Different architectures may have varying levels of vulnerability.
- What evidence would resolve it: Systematic testing of ImgTrojan across diverse VLM architectures (e.g., Flamingo, GPT-4V, VideoLLaMA) with consistent metrics would establish generalizability.

### Open Question 2
- Question: What specific layers or mechanisms in the LLM component make it most susceptible to ImgTrojan poisoning?
- Basis in paper: [explicit] The paper explicitly states "the Trojan primarily resides in the LLM component rather than the modality alignment module" and identifies "middle and last layers" of LLMs as contributing more significantly to Trojan formation.
- Why unresolved: While the paper identifies general regions, it doesn't pinpoint exact mechanisms or layer-specific vulnerabilities that could be targeted for defense.
- What evidence would resolve it: Detailed layer-wise ablation studies showing precise layer contributions, combined with interpretability analysis of how poisoned images trigger specific activation patterns.

### Open Question 3
- Question: How does ImgTrojan's attack success rate scale with dataset size when maintaining constant poison ratio?
- Basis in paper: [inferred] The paper demonstrates effectiveness with poison ratios from 0.0001% to 0.01% but doesn't explore how results scale with larger datasets while maintaining these ratios.
- Why unresolved: The current experiments use a fixed dataset size (~10,000 samples), so the relationship between dataset scale and required poison ratio for successful attacks remains unknown.
- What evidence would resolve it: Experiments scaling dataset sizes from thousands to millions of samples while maintaining constant poison ratios to determine if attack success rates remain stable.

## Limitations
- The attack requires access to the training pipeline, making it an insider threat rather than an inference-time attack
- Evaluation is limited to LLaVA-v1.5 on the GPT4V dataset, leaving uncertainty about performance across different VLM architectures
- ASR metric depends on automated safety annotation via ChatGPT/GPT-3.5-turbo, which may not perfectly align with human safety judgments

## Confidence

**High Confidence Claims:**
- The basic mechanism of data poisoning with jailbreak prompts works and achieves measurable ASR increases
- The Trojan effect primarily resides in the LLM component rather than the modality alignment module
- CLIP-based filtering cannot detect the poisoned samples

**Medium Confidence Claims:**
- The specific ASR values (51.2% increase at 0.01% poison ratio) are reproducible across similar VLM architectures
- The minimal performance degradation on clean images is consistently observed
- The attack maintains effectiveness after fine-tuning with clean data

**Low Confidence Claims:**
- The generalizability of attack effectiveness to other VLM architectures beyond LLaVA-v1.5
- The scalability of the attack to larger models (13B vs 7B) shows consistent patterns
- The persistence of the Trojan effect across different fine-tuning scenarios

## Next Checks

1. **Architecture Generalization Test**: Evaluate the attack on alternative VLM architectures (e.g., MiniGPT-4, BLIP-2) to verify that the LLM component vulnerability is universal rather than LLaVA-specific.

2. **Defense Implementation Test**: Implement CLIP-based similarity filtering and toxicity detection at the data ingestion stage to measure whether these defenses can prevent the poisoning attack at the source.

3. **Long-term Stability Test**: Conduct longitudinal analysis by fine-tuning poisoned models with increasing amounts of clean data to determine the persistence threshold where the Trojan effect diminishes.