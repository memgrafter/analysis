---
ver: rpa2
title: Head-wise Shareable Attention for Large Language Models
arxiv_id: '2402.11819'
source_url: https://arxiv.org/abs/2402.11819
tags:
- weight
- sharing
- uni00000013
- attention
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores head-wise weight sharing for large language
  models (LLMs) to address their high memory requirements. It proposes two methods:
  DirectShare, which directly reuses pre-trained weights based on cosine similarity
  of weight matrices across attention heads, and PostShare, which first post-trains
  with a similarity constraint before sharing.'
---

# Head-wise Shareable Attention for Large Language Models

## Quick Facts
- arXiv ID: 2402.11819
- Source URL: https://arxiv.org/abs/2402.11819
- Reference count: 40
- Primary result: DirectShare maintains 99%+ performance with up to 30% parameter sharing, saving 10-13% GPU memory

## Executive Summary
This paper proposes head-wise weight sharing methods for large language models (LLMs) to address their high memory requirements. The authors introduce DirectShare, which directly reuses pre-trained weights based on cosine similarity of weight matrices across attention heads, and PostShare, which first post-trains with a similarity constraint before sharing. Experimental results demonstrate that DirectShare maintains performance with up to 30% parameter sharing, achieving 99.51% and 99.21% of original performance on Llama2-7B and 13B models respectively, while saving 10-13% GPU memory with 30% MHA sharing.

## Method Summary
The paper presents two methods for head-wise weight sharing in LLMs. DirectShare computes cosine similarity between concatenated weight matrices (Wq||Wk) across attention heads and shares weights between the most similar pairs without retraining. PostShare first selects heads to share using the same matching function, then applies post-training with an additional weight similarity regularization term before sharing. Both methods aim to maintain model performance while reducing memory requirements through parameter sharing across attention heads.

## Key Results
- DirectShare maintains 99.51% and 99.21% of original performance on Llama2-7B and 13B models with 30% parameter sharing
- PostShare improves accuracy by up to 17.80% in reading comprehension tasks through post-training
- Memory savings of 10-13% with 30% MHA sharing and 26-28% with both MHA and FFN sharing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Head-wise weight sharing maintains performance because attention heads with high weight matrix similarity also exhibit similar attention map patterns across different datasets.
- Mechanism: The method computes cosine similarity between concatenated weight matrices (Wq||Wk) across attention heads and shares weights between the most similar pairs. This preserves functional similarity while reducing parameters.
- Core assumption: Attention heads with similar weight matrices produce similar attention patterns, so sharing these weights doesn't significantly impact model behavior.
- Evidence anchors:
  - [abstract] "We concatenate the weight matrix Wq and Wk for each head to measure the cosine similarity that determines which heads can be shared"
  - [section 3.2] "by concatenating the weight matrix Wq and Wk for each head to measure the cosine similarity, the most similar weight matrix corresponds to the overlapping modules with highly similar attention maps observed across different datasets"
- Break condition: When weight matrix similarity between heads drops below the threshold needed to maintain similar attention patterns, or when head diversity becomes critical for task performance.

### Mechanism 2
- Claim: DirectShare maintains 99%+ performance up to 30% parameter sharing because the sharing strategy preserves sufficient head diversity while reducing redundancy.
- Mechanism: By sharing only the most similar heads (based on weight matrix similarity) and leaving other heads independent, the method maintains the diversity needed for different attention patterns while eliminating redundancy.
- Core assumption: LLMs contain sufficient redundancy that can be eliminated through head-wise sharing without sacrificing critical functional diversity.
- Evidence anchors:
  - [abstract] "DirectShare maintains performance with up to 30% parameter sharing, achieving 99.51% and 99.21% of original performance on Llama2-7B and 13B models respectively"
  - [section 4.1] "head-wise weight sharing promotes parameter diversity in the layers, and thus its performance degradation is acceptable when the number of shared parameters is below 30%"
- Break condition: When sharing ratio exceeds 30%, or when head diversity becomes essential for specific tasks like reading comprehension.

### Mechanism 3
- Claim: PostShare improves performance through constrained post-training that maintains weight similarity while fine-tuning.
- Mechanism: After selecting heads to share, PostShare adds a regularization term to the loss function that encourages the shared weight matrices to remain similar during fine-tuning, allowing the model to adapt while preserving the sharing structure.
- Core assumption: The model can adapt to maintain performance while keeping shared weights similar, effectively learning to work with the shared parameters.
- Evidence anchors:
  - [abstract] "PostShare first post-trains with constraint on weight matrix similarity and then shares, denoted as PostShare"
  - [section 4.3] "we incorporate a regularization term into the loss function to constrain our post-training process, encouraging selected weight matrices more similar"
  - [section 5.2.2] "PostShare achieves 87.53% of the overall accuracy at attained by the original model" and "PostShare yields satisfactory performance via post-training, improving accuracy by up to 17.80% in reading comprehension tasks"
- Break condition: When the regularization constraint is too strong (preventing necessary adaptation) or too weak (failing to maintain the sharing structure).

## Foundational Learning

- Concept: Cosine similarity for matrix comparison
  - Why needed here: Used to measure similarity between weight matrices (Wq||Wk) to determine which heads to share
  - Quick check question: Given two weight matrices W1 and W2, what formula computes their cosine similarity?

- Concept: Multi-head attention mechanism
  - Why needed here: Understanding how MHA works is essential to grasp why head-wise sharing is possible and effective
  - Quick check question: In multi-head attention, how are queries, keys, and values computed for each head?

- Concept: Weight sharing in neural networks
  - Why needed here: The fundamental concept behind the paper's approach to reducing parameters while maintaining performance
  - Quick check question: What is the primary trade-off when implementing weight sharing in neural networks?

## Architecture Onboarding

- Component map: Pre-trained LLM → Weight matrix similarity computation → Head pair selection → Weight sharing (DirectShare) or Post-training + sharing (PostShare) → Memory-efficient model
- Critical path: The head pair selection process based on cosine similarity is the critical path, as it determines which weights will be shared and thus affects performance
- Design tradeoffs: Time efficiency vs. performance (DirectShare is faster but may have slightly lower performance than PostShare), memory savings vs. functional diversity (higher sharing ratios save more memory but may reduce diversity)
- Failure signatures: Performance degradation exceeding acceptable thresholds, particularly in reading comprehension tasks; overfitting during PostShare; sharing ratio exceeding the 30% threshold where performance drops significantly
- First 3 experiments:
  1. Verify cosine similarity computation between weight matrices works correctly across different head pairs
  2. Test head pair selection with a small model to ensure the most similar heads are chosen for sharing
  3. Validate memory reduction by measuring actual GPU memory usage before and after applying DirectShare with 30% sharing ratio

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal head-wise match function for weight sharing across different LLM architectures and tasks?
- Basis in paper: [explicit] The paper compares different match functions (cosine similarity on Wq, Wk, Wv, and their concatenations) and concludes that using cosine similarity between the concatenation of Wq and Wk matrices performs best, but acknowledges this needs further validation
- Why unresolved: The paper only tests on Llama2 and Baichuan2 models with limited datasets, and the analysis doesn't explore whether different architectures (e.g., GPT-style vs. Llama-style) or task domains might benefit from different match functions
- What evidence would resolve it: Systematic experiments testing multiple match functions across diverse LLM architectures (GPT, Llama, Claude variants), task domains (reasoning, NLU, generation), and model scales would reveal whether a universal optimal match function exists or if architecture-specific choices are needed

### Open Question 2
- How does head-wise weight sharing affect the emergent abilities of LLMs across different scales?
- Basis in paper: [inferred] The paper notes performance degradation in reading comprehension tasks with DirectShare and mentions this becomes more pronounced as model size increases, but doesn't systematically investigate emergent abilities
- Why unresolved: The experiments focus on task-specific performance metrics but don't examine whether weight sharing affects emergent abilities (like in-context learning, chain-of-thought reasoning) that appear only in larger models
- What evidence would resolve it: Controlled experiments measuring emergent abilities (few-shot learning performance, reasoning chains, instruction following) on models of increasing scales (7B, 13B, 30B, 70B+) with and without head-wise sharing would reveal whether weight sharing preserves these abilities

### Open Question 3
- What is the relationship between attention map similarity and actual model performance degradation when applying head-wise weight sharing?
- Basis in paper: [explicit] The paper shows high attention map similarity between certain head pairs but finds that sharing these pairs still causes performance degradation, particularly in reading comprehension tasks
- Why unresolved: The paper demonstrates correlation between attention map similarity and weight matrix similarity but doesn't explain why high similarity doesn't guarantee performance preservation, or identify which aspects of attention contribute most to task performance
- What evidence would resolve it: Detailed analysis correlating attention map similarity metrics with specific performance drops across different task types, potentially using attention visualization and ablation studies to identify which attention patterns are most critical for preserving performance during weight sharing

## Limitations
- Evidence base limitations: Results rely entirely on specific Llama2 and Baichuan2 models (7B and 13B parameters) without external validation across diverse model architectures, scales, or training methodologies
- Generalization uncertainty: Break conditions for performance degradation are not fully characterized beyond the 30% threshold, and long-term effects on model adaptation capabilities are not addressed
- Methodological gaps: The paper does not explore impact on downstream fine-tuning performance or provide thorough analysis of why PostShare's effectiveness varies across task types

## Confidence
**High confidence**: The experimental results showing DirectShare maintaining 99%+ performance up to 30% parameter sharing are well-supported by the presented data. The memory reduction claims (10-13% with MHA sharing, 26-28% with both MHA and FFN sharing) are concrete and measurable.

**Medium confidence**: The mechanism explanations connecting weight matrix similarity to attention map patterns are plausible but rely on assumptions that are not fully validated through additional experiments. The claim about maintaining head diversity while reducing redundancy is supported by results but lacks deeper mechanistic analysis.

**Low confidence**: The assertion that the approach is broadly applicable to "various applications" beyond the tested benchmarks is not substantiated. The long-term stability and adaptation implications of weight sharing are not addressed.

## Next Checks
1. **Cross-model validation**: Test the DirectShare and PostShare methods on models from different architectural families (e.g., GPT, Mistral, or other Llama variants) and scales (3B, 34B, 70B) to assess generalizability of the 30% performance threshold.

2. **Long-term adaptation study**: Evaluate model performance after extended fine-tuning periods on both shared and independent parameters to identify any degradation or adaptation issues specific to weight sharing configurations.

3. **Attention pattern analysis**: Conduct ablation studies measuring actual attention map differences between shared and non-shared heads across various tasks to empirically validate the assumption that similar weight matrices produce similar attention patterns.