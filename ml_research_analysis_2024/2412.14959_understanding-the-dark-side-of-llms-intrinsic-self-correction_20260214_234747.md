---
ver: rpa2
title: Understanding the Dark Side of LLMs' Intrinsic Self-Correction
arxiv_id: '2412.14959'
source_url: https://arxiv.org/abs/2412.14959
tags:
- llms
- question
- answer
- task
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the failure of large language models'\
  \ (LLMs) intrinsic self-correction\u2014where models refine responses without external\
  \ feedback\u2014across diverse tasks. Through mechanistic interpretability, token-level\
  \ analysis, and cognitive bias framing, the authors identify three failure causes:\
  \ answer wavering (internal instability), prompt bias (overweighting refinement\
  \ prompts), and human-like cognitive biases (overthinking, cognitive overload, perfectionism)\
  \ in complex tasks."
---

# Understanding the Dark Side of LLMs' Intrinsic Self-Correction

## Quick Facts
- arXiv ID: 2412.14959
- Source URL: https://arxiv.org/abs/2412.14959
- Reference count: 24
- Primary result: Self-correction causes LLMs to fail with performance drops up to 20.4% accuracy decrease, with proposed mitigation strategies reducing failures significantly

## Executive Summary
This paper investigates failures of large language models' intrinsic self-correction capability across diverse tasks. Through mechanistic interpretability, token-level analysis, and cognitive bias framing, the authors identify three failure causes: answer wavering (internal instability), prompt bias (overweighting refinement prompts), and human-like cognitive biases in complex tasks. The study evaluates multiple LLM families on Yes/No questions, decision making, reasoning, and programming tasks, revealing significant performance degradation after self-correction. Two low-cost mitigation strategies are proposed: question repeating and supervised fine-tuning with minimal samples, both showing effectiveness in reducing self-correction failures.

## Method Summary
The paper employs a multi-faceted evaluation framework using state-of-the-art LLMs (ChatGPT and Llama families) across four task types: Yes/No questions from BoolQ dataset, decision making from AlfWorld, reasoning from HotPotQA, and programming from HumanEval. Three interpretation methods are designed: mechanistic interpretability for open-sourced models, token-level interpretability using PACT for closed-sourced models, and cognitive bias framing for complex tasks. The evaluation measures accuracy before and after self-correction, tracking the proportion of correct answers overturned. Two mitigation strategies are proposed: question repeating to reduce prompt recency bias and supervised fine-tuning with ≤10 samples to realign model behavior.

## Key Results
- Self-correction causes significant performance drops: Llama-3.1-8B shows 20.4% accuracy decrease and 58.8% of correct answers overturned
- Answer wavering occurs with 14.1% internal answer change frequency during self-correction vs 8.3% during initial response
- Prompt bias identified: tokens in refinement prompts are overweighted compared to original question tokens
- Human-like cognitive biases emerge in complex tasks: overthinking, cognitive overload, and perfectionism patterns observed
- Mitigation strategies effective: question repeating and SFT with ≤10 samples significantly reduce self-correction failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-correction can fail because LLMs exhibit internal answer wavering, causing confidence instability across layers.
- Mechanism: When prompted to self-correct, the LLM's intermediate token representations oscillate between correct and incorrect answer probabilities, leading to a loss of confidence in the initially correct answer.
- Core assumption: Confidence scores derived from intermediate token representations accurately reflect the model's belief in the correctness of answers.
- Evidence anchors:
  - [abstract] "We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers"
  - [section] "self-correction makes Llama change internal answers with an average frequency of 14.1% compared to 8.3% during Initial response generation"
  - [corpus] Weak correlation - corpus contains related works but no direct mechanistic validation
- Break condition: If confidence scores do not correlate with final answer correctness, the mechanism fails.

### Mechanism 2
- Claim: Self-correction fails due to prompt bias, where LLMs overweight refinement prompts over original questions.
- Mechanism: When generating the second response, LLMs attribute higher importance to tokens in the refinement prompt than to the original question, leading to answer changes even when the initial answer was correct.
- Core assumption: Token-level attribution methods like PACT accurately capture which parts of the prompt influence the final answer.
- Evidence anchors:
  - [abstract] "(2) introduce prompt bias on simple factual questions"
  - [section] "tokens in the refinement prompt are generally greener than tokens in the original question"
  - [corpus] No direct corpus evidence for this specific bias mechanism
- Break condition: If attribution methods do not reliably identify influential tokens, the mechanism fails.

### Mechanism 3
- Claim: In complex tasks, self-correction triggers human-like cognitive biases in LLMs, causing systematic reasoning errors.
- Mechanism: LLMs exhibit patterns similar to human cognitive biases (overthinking, cognitive overload, perfectionism) when processing complex, open-ended tasks during self-correction.
- Core assumption: Observable reasoning patterns in LLM outputs can be mapped to established human cognitive bias categories.
- Evidence anchors:
  - [abstract] "(2) introduce human-like cognitive bias on complex tasks"
  - [section] "LLMs make mistakes similarly as humans" and provides examples of overthinking, cognitive overload, and perfectionism bias
  - [corpus] Weak evidence - corpus contains related works but no direct mapping to human cognitive biases
- Break condition: If LLM reasoning patterns cannot be meaningfully categorized as human-like cognitive biases, the mechanism fails.

## Foundational Learning

- Concept: Token-level attribution methods
  - Why needed here: To understand which parts of prompts influence LLM outputs and identify prompt bias
  - Quick check question: How does removing a token from the input prompt affect the log probability of the output?

- Concept: Mechanistic interpretability
  - Why needed here: To probe internal model states and track confidence evolution across layers
  - Quick check question: What does the difference in confidence scores between correct and incorrect answers reveal about model behavior?

- Concept: Human cognitive biases
  - Why needed here: To frame LLM reasoning errors in complex tasks using established psychological concepts
  - Quick check question: How do patterns like excessive "think" generation relate to human overthinking bias?

## Architecture Onboarding

- Component map: Input prompt → Initial response generation → Feedback prompt → Refinement generation → Output analysis
- Critical path: Initial response generation → Feedback prompt processing → Token attribution analysis → Confidence score tracking
- Design tradeoffs: Simple prompting vs. complex task handling; open-source interpretability vs. closed-source black-box analysis
- Failure signatures: Accuracy drop after self-correction; high ✓→✗ rate; internal answer wavering in confidence scores
- First 3 experiments:
  1. Compare confidence scores across layers for initial vs. refined responses
  2. Apply PACT to measure token attribution differences between correct and overturned answers
  3. Analyze reasoning logs for patterns of overthinking, cognitive overload, and perfectionism bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of question repeating and SFT vary across different types of complex tasks (e.g., decision making vs. programming)?
- Basis in paper: [inferred] The paper shows that SFT improves performance on complex tasks but does not analyze task-specific differences.
- Why unresolved: The paper provides aggregate results for complex tasks but lacks a detailed breakdown by task type.
- What evidence would resolve it: Comparative analysis of SFT and question repeating effectiveness across individual complex tasks (decision making, reasoning, programming) with statistical significance testing.

### Open Question 2
- Question: What is the relationship between model size and susceptibility to self-correction failures?
- Basis in paper: [explicit] The paper tests multiple model sizes (7B, 8B, and 3.1-8B) but does not systematically analyze the impact of model size on failure rates.
- Why unresolved: While the paper shows failure rates for different models, it does not investigate whether larger models are more or less prone to self-correction failures.
- What evidence would resolve it: Controlled experiments comparing self-correction failure rates across a broader range of model sizes with consistent evaluation metrics.

### Open Question 3
- Question: How do different prompt formulations affect the likelihood of self-correction failures beyond the "Are you sure?" prompt?
- Basis in paper: [explicit] The paper briefly mentions comparing "Are you sure?" vs "You are wrong" prompts but does not explore a wider range of prompt formulations.
- Why unresolved: The paper's prompt analysis is limited to two specific formulations, leaving open questions about optimal prompt design.
- What evidence would resolve it: Systematic testing of various prompt formulations (e.g., different levels of directness, politeness, or specificity) and their impact on self-correction success rates.

### Open Question 4
- Question: What is the long-term impact of SFT on model performance across diverse tasks beyond those used for fine-tuning?
- Basis in paper: [explicit] The paper shows that SFT on simple tasks generalizes to complex tasks but does not investigate long-term performance effects.
- Why unresolved: The paper only examines immediate performance changes after SFT without considering potential degradation or improvement over time.
- What evidence would resolve it: Longitudinal studies tracking model performance across multiple tasks and domains before and after SFT implementation.

## Limitations

- The cognitive bias framing mechanism relies on qualitative rather than rigorously validated mappings between LLM patterns and human psychological concepts
- Token attribution methods may not fully capture complex internal representations, potentially oversimplifying failure mechanisms
- The generalizability of findings across different prompt formulations and task domains remains unexplored

## Confidence

- High Confidence: The empirical observation that self-correction leads to performance degradation across multiple task types and LLM families
- Medium Confidence: The identification of answer wavering as a failure mechanism
- Medium Confidence: The prompt bias mechanism
- Low Confidence: The cognitive bias framing for complex tasks

## Next Checks

1. Conduct ablation studies where specific layers or attention heads are modified to test whether observed confidence oscillations directly cause answer changes, establishing stronger causal links between internal representations and final outputs

2. Apply multiple token attribution methods (beyond PACT) to the same failure cases to verify whether the identified prompt bias pattern is consistent across different interpretability approaches

3. Design controlled experiments that systematically vary task complexity and prompt structure to test whether identified reasoning patterns (overthinking, cognitive overload, perfectionism) can be reliably reproduced and manipulated