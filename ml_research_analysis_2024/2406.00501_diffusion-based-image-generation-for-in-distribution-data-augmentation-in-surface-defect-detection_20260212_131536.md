---
ver: rpa2
title: Diffusion-based Image Generation for In-distribution Data Augmentation in Surface
  Defect Detection
arxiv_id: '2406.00501'
source_url: https://arxiv.org/abs/2406.00501
tags:
- data
- samples
- images
- augmentation
- ddpm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of data augmentation in surface
  defect detection, where classifiers are typically trained on imbalanced datasets
  with many normal samples and few defective ones. The authors propose In&Out, a novel
  approach that combines in-distribution defects generated by diffusion models with
  out-of-distribution defects from per-region data augmentation methods.
---

# Diffusion-based Image Generation for In-distribution Data Augmentation in Surface Defect Detection

## Quick Facts
- arXiv ID: 2406.00501
- Source URL: https://arxiv.org/abs/2406.00501
- Reference count: 6
- New state-of-the-art AP score of 0.782 on Kolektor Surface-Defect Dataset 2 under weak supervision

## Executive Summary
This paper addresses the challenge of data augmentation in surface defect detection, where classifiers are typically trained on imbalanced datasets with many normal samples and few defective ones. The authors propose In&Out, a novel approach that combines in-distribution defects generated by diffusion models with out-of-distribution defects from per-region data augmentation methods. The key idea is to use diffusion models to generate realistic in-distribution defects, which complement the out-of-distribution defects generated by superimposing artifacts on normal samples. The In&Out approach can handle zero-shot, few-shot, and full-shot data augmentation setups.

## Method Summary
The In&Out approach combines two complementary data augmentation strategies: diffusion model-based in-distribution augmentation and per-region out-of-distribution augmentation. The diffusion model component uses Stable Diffusion fine-tuned with DreamBooth and LoRA to generate realistic defect samples from textual prompts. The per-region component uses MemSeg to augment normal samples by superimposing various artifacts. These two approaches are combined to create a balanced dataset that helps classifiers distinguish both subtle defects and clearly anomalous regions.

## Key Results
- Achieves new state-of-the-art classification Average Precision (AP) score of 0.782 on Kolektor Surface-Defect Dataset 2 under weak supervision
- In&Out approach significantly outperforms previous methods, particularly in precision-recall balance
- Demonstrates effectiveness across zero-shot, few-shot, and full-shot data augmentation setups
- Shows strong performance even with minimal fine-tuning data (zero-shot scenario)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models generate in-distribution defect samples that better represent the true distribution of defects compared to out-of-distribution per-region augmentation.
- Mechanism: The diffusion process samples from the learned defect distribution, capturing fine-grained and realistic variations of defects that are similar to real defects in the training data. This contrasts with per-region augmentation which superimposes artificial artifacts that are clearly distinguishable from real defects.
- Core assumption: The diffusion model has been sufficiently trained to capture the statistical properties of defects in the dataset.
- Evidence anchors:
  - [abstract]: "Diffusion models overcome this situation, providing more realistic in-distribution defects so that the model can learn the defect’s genuine appearance."
  - [section 1]: "Diffusion models (Dhariwal and Nichol, 2021; Rombach et al., 2022) are deep generative models inspired by non-equilibrium thermodynamics that allow the sampling of rich latent spaces to generate meaningful realistic images."
  - [corpus]: No direct corpus evidence found for this specific claim about in-distribution vs out-of-distribution properties of diffusion-generated defects.
- Break condition: If the diffusion model is undertrained or fine-tuned on insufficient/defective samples, it may generate unrealistic defects that do not improve classifier performance.

### Mechanism 2
- Claim: Combining in-distribution (diffusion-generated) and out-of-distribution (per-region) defects improves both precision and recall compared to using either alone.
- Mechanism: Out-of-distribution defects help the classifier identify what is definitely not normal (improving precision by reducing false positives), while in-distribution defects help the classifier recognize subtle variations that are actually defects (improving recall by reducing false negatives). The combination provides complementary information.
- Core assumption: The two augmentation methods provide complementary information that the classifier can leverage.
- Evidence anchors:
  - [abstract]: "Due to the high complementarity of the two augmentation policies, we decided to use them together, dubbing our approach In&Out data augmentation since it is a compromise between augmented images that are in and out-of-distribution."
  - [section 4]: "This ensures that half of the augmented data will be in-distribution, describing the visual appearance of the defects (the diffusion-based one), while the other half of the data will focus on specifying what is certainly not a perfect sample (the per-patch images)."
  - [corpus]: No direct corpus evidence found for the specific claim about complementarity improving both precision and recall.
- Break condition: If the classifier cannot effectively distinguish between the two types of augmented data or if one type dominates the learning process.

### Mechanism 3
- Claim: DreamBooth with LoRA enables effective fine-tuning of diffusion models with very few defect samples (few-shot learning).
- Mechanism: DreamBooth adapts the diffusion model to a specific concept (defects) using minimal samples by preserving prior knowledge through regularization. LoRA makes this efficient by only training low-rank matrices rather than full model weights.
- Core assumption: The defect samples used for fine-tuning are representative of the defect distribution in the dataset.
- Evidence anchors:
  - [section 3]: "Dreambooth (Ruiz et al., 2023) is a procedure for DDPMs that allows fine-tuning the model with a small number N of images."
  - [section 3]: "Low-Rank Adaptation (LoRA). In recent years, fine-tuning Large Language Models (LLMs) has become prohibitively expensive due to the huge number of parameters. In (Hu et al., 2021), the authors introduced Low-Rank Adaptation (LoRA), a model-agnostic method of fine-tuning models in an efficient way."
  - [section 5.1]: "In the zero-shot data augmentation, we perform fine-tuning with a portion of randomly chosen negative samples from the training set."
  - [corpus]: No direct corpus evidence found for this specific combination of DreamBooth and LoRA for surface defect detection.

## Foundational Learning

- Concept: Diffusion models and their sampling process
  - Why needed here: Understanding how diffusion models generate images through iterative denoising is crucial for implementing the in-distribution augmentation component
  - Quick check question: What is the key difference between the forward and reverse diffusion processes in diffusion models?

- Concept: Data augmentation principles and imbalanced learning
  - Why needed here: The core problem being addressed is class imbalance in defect detection datasets, requiring augmentation strategies
  - Quick check question: Why is class imbalance particularly problematic for defect detection in industrial settings?

- Concept: DreamBooth fine-tuning methodology
  - Why needed here: The paper uses DreamBooth to adapt a pre-trained diffusion model to generate defect-specific images with minimal samples
  - Quick check question: What is the purpose of the regularization images in the DreamBooth fine-tuning process?

## Architecture Onboarding

- Component map: Pre-trained diffusion model (Stable Diffusion) -> DreamBooth fine-tuning module with LoRA adaptation -> Text prompt interface for defect specification -> MemSeg per-region augmentation module -> ResNet-50 classifier with modified fully connected layers -> Training pipeline that combines real and augmented data

- Critical path: Data augmentation → Model training → Evaluation
  - First, generate augmented defect samples using both diffusion and per-region methods
  - Second, train the ResNet-50 classifier on the combined dataset
  - Third, evaluate performance on the test set using AP, precision, and recall metrics

- Design tradeoffs:
  - Using pre-trained diffusion models vs training from scratch: Pre-trained models require less data and computation but may not perfectly capture domain-specific defects
  - Number of augmented samples: More augmentation can improve performance but increases computational cost and risk of overfitting
  - Balance between in-distribution and out-of-distribution samples: The optimal ratio may vary depending on the specific dataset characteristics

- Failure signatures:
  - Low recall despite high precision: Indicates insufficient in-distribution defect generation or classifier bias toward normal samples
  - Poor performance on both metrics: Suggests inadequate fine-tuning of the diffusion model or inappropriate augmentation strategy
  - Overfitting to augmented data: Model performs well on augmented samples but poorly on real defects in the test set

- First 3 experiments:
  1. Baseline experiment: Train ResNet-50 on original imbalanced dataset without any augmentation
  2. Single-method comparison: Train with only MemSeg augmentation vs only DDPM augmentation to establish individual contributions
  3. In&Out implementation: Implement the full pipeline with varying ratios of in-distribution to out-of-distribution augmented samples to find optimal balance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do textual prompts interact with DDPM when defects are very few and not limited to cracks and scratches?
- Basis in paper: [explicit] The authors mention this as an area for further study, noting that their experiments focused on cracks and scratches as prompts.
- Why unresolved: The paper only tested a limited set of defect types in prompts. The effectiveness of prompt engineering for a broader range of defect types is unknown.
- What evidence would resolve it: Testing the In&Out approach with a diverse set of defect types and analyzing the quality of generated defects for each type.

### Open Question 2
- Question: How does the number of anomalous samples available for fine-tuning affect the performance of the In&Out approach in the few-shot scenario?
- Basis in paper: [inferred] The paper shows that In&Out does not improve performance in the few-shot scenario (N=5), suggesting overfitting. The optimal number of samples for fine-tuning is not explored.
- Why unresolved: The paper only tested with N=5 in the few-shot scenario. The relationship between the number of available anomalous samples and the effectiveness of In&Out is unclear.
- What evidence would resolve it: Experimenting with different values of N in the few-shot scenario and analyzing the performance of In&Out for each value.

### Open Question 3
- Question: How does the In&Out approach compare to other data augmentation methods beyond MemSeg in the zero-shot and few-shot scenarios?
- Basis in paper: [inferred] The paper compares In&Out to MemSeg and DDPM alone, but does not explore other data augmentation methods. The effectiveness of In&Out relative to other methods is unknown.
- Why unresolved: The paper only tested In&Out against MemSeg and DDPM. The performance of In&Out compared to a broader range of data augmentation methods is not established.
- What evidence would resolve it: Comparing the performance of In&Out to other data augmentation methods (e.g., MixSegdec, per-region augmentation with different noise types) in the zero-shot and few-shot scenarios.

## Limitations
- Limited analysis of the quality and diversity of generated defect samples
- No ablation studies to isolate the individual contributions of in-distribution vs out-of-distribution augmentation methods
- Sensitivity to fine-tuning sample size not thoroughly explored in few-shot scenario

## Confidence
- Diffusion models generating superior in-distribution defects: Medium
- Complementarity between augmentation methods: Medium
- DreamBooth with LoRA for few-shot fine-tuning: Medium-High

## Next Checks
1. **Generated Sample Quality Analysis**: Conduct a systematic evaluation comparing the visual and statistical properties of diffusion-generated defects against real defects using metrics like Frechet Inception Distance (FID) and human perceptual studies to verify true in-distribution characteristics.

2. **Augmentation Method Ablation**: Perform controlled experiments with only in-distribution augmentation, only out-of-distribution augmentation, and varying ratios between them to quantify the individual and synergistic contributions to classifier performance.

3. **Generalization Across Datasets**: Test the In&Out approach on multiple surface defect datasets with different defect types and characteristics to assess whether the claimed improvements are dataset-specific or generalize across industrial defect detection scenarios.