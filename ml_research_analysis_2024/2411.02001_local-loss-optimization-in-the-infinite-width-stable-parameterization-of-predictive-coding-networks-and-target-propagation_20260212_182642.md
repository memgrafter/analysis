---
ver: rpa2
title: 'Local Loss Optimization in the Infinite Width: Stable Parameterization of
  Predictive Coding Networks and Target Propagation'
arxiv_id: '2411.02001'
source_url: https://arxiv.org/abs/2411.02001
tags:
- learning
- log2
- inference
- width
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of designing local learning\
  \ algorithms\u2014specifically predictive coding (PC) and target propagation (TP)\u2014\
  that remain stable and effective in the infinite-width limit of neural networks.\
  \ It introduces maximal update parameterization (\xB5P) for these algorithms, ensuring\
  \ hyperparameter transfer across different network widths."
---

# Local Loss Optimization in the Infinite Width: Stable Parameterization of Predictive Coding Networks and Target Propagation

## Quick Facts
- arXiv ID: 2411.02001
- Source URL: https://arxiv.org/abs/2411.02001
- Reference count: 40
- The paper derives maximal update parameterization (µP) for predictive coding and target propagation, enabling stable training and hyperparameter transfer across network widths in the infinite-width limit.

## Executive Summary
This paper addresses the challenge of designing local learning algorithms—specifically predictive coding (PC) and target propagation (TP)—that remain stable and effective in the infinite-width limit of neural networks. It introduces maximal update parameterization (µP) for these algorithms, ensuring hyperparameter transfer across different network widths. The core idea is to derive appropriate scaling of initialization and learning rates for PC and TP so that their learning dynamics behave similarly to first-order gradient descent (GD) in the infinite-width limit. For PC, the analysis shows that its gradients interpolate between GD and Gauss-Newton-like updates depending on the parameterization, with standard settings favoring GD. For TP, the µP scaling leads to the disappearance of the kernel regime, strongly favoring feature learning. The primary results demonstrate successful µTransfer of learning rates across widths for both PC and TP under the derived µP, verified through experiments on MLPs, CNNs, and VGG-like models on FashionMNIST and CIFAR datasets.

## Method Summary
The paper derives maximal update parameterization (µP) for predictive coding (PC) and target propagation (TP) to enable stable training and hyperparameter transfer in the infinite-width limit. For PC, the inference step sizes are scaled as γl = γ′/M^γL for the last layer and γl<L = Θ(1) for hidden layers. For TP, the last layer uses different scaling than standard µP, with no kernel regime appearing due to the alignment exponent α = 1/2. The method involves analyzing deep linear networks to derive fixed points and comparing gradients with backpropagation and Gauss-Newton methods. Experiments are conducted on FashionMNIST and CIFAR datasets using 3-layer MLPs, 3-layer CNNs, and VGG-like models with varying widths (128 to 8192) to verify hyperparameter transfer.

## Key Results
- Derived µP for PC and TP that enables hyperparameter transfer across widths
- PC gradients interpolate between first-order GD and Gauss-Newton-like updates depending on parameterization
- TP parameterization eliminates the kernel regime, favoring feature learning over lazy training
- Successful verification of learning rate transfer across widths for both PC and TP on multiple architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Maximal Update Parameterization (µP) enables stable local learning in the infinite-width limit for predictive coding and target propagation.
- **Mechanism**: By scaling initialization and learning rates with respect to network width, µP ensures that gradients and activations remain of order 1 (neither vanishing nor exploding) as width increases, which is required for stable training dynamics.
- **Core assumption**: Learning dynamics and parameterization are stable when Δhl,t = O(1) and Δft = O(1) under the abc-parameterization.
- **Evidence anchors**:
  - [abstract]: "maximal update parameterization (µP) for these algorithms, ensuring hyperparameter transfer across different network widths."
  - [section]: "Yang & Hu (2021) introduced the following conditions and characterized µP as a unique stable abc-parameterization under them..."
  - [corpus]: Found related work on µP and scaling limits but no direct experimental evidence of µP stability in local learning methods.
- **Break condition**: If the scaling of initialization or learning rates deviates from µP, gradients may vanish or explode, breaking stability.

### Mechanism 2
- **Claim**: For predictive coding, the inference step size γL controls whether gradients interpolate between first-order gradient descent and Gauss-Newton-like updates.
- **Mechanism**: With γL = γ′/M^γL and hidden layers using γl<L = Θ(1), the preconditioning matrix Cγ(W) scales as O(1/M) in the infinite-width limit, making PC's gradient reduce to first-order GD for γL = Θ(1).
- **Core assumption**: Linear networks allow explicit derivation of fixed points, and the order of gradients at fixed points matches those from single-shot inference when γl<L = Θ(1).
- **Evidence anchors**:
  - [abstract]: "By analyzing deep linear networks, we found that PC's gradients interpolate between first-order and Gauss-Newton-like gradients..."
  - [section]: "Corollary 4.3... In the infinite-width limit, the PC's gradient reduces to the first-order GD for γL = Θ(1)."
  - [corpus]: No direct corpus evidence linking γL scaling to gradient interpolation.
- **Break condition**: If γl<L ≠ Θ(1), the balance between feedforward and feedback signals breaks, destabilizing inference and gradient computation.

### Mechanism 3
- **Claim**: Target propagation lacks a kernel regime due to the alignment exponent α = 1/2, unlike backpropagation where α = 1.
- **Mechanism**: In TP, the feedback network's pseudo-inverse weights create no dependence between WL and ΔhL−1, leading to α = 1/2. This forces rL−1 = 0, eliminating the kernel regime.
- **Core assumption**: Stable learning requires Condition A.2 (WL initialized maximally), and TP's gradient computation does not induce alignment between forward and backward signals.
- **Evidence anchors**:
  - [abstract]: "For TP, even with the standard scaling of the last layer, which differs from classical µP, its local loss optimization favors the feature learning regime over the kernel regime."
  - [section]: "Interestingly, in TP, the kernel regime disappears... rL−1 = 0 for TP and DTP."
  - [corpus]: Found related work on kernel regimes and alignment exponents but no direct evidence of TP's α = 1/2.
- **Break condition**: If the feedback network introduces alignment between WL and ΔhL−1, the kernel regime could reappear.

## Foundational Learning

- **Concept**: Infinite-width limit theory and neural tangent kernel (NTK) regime
  - Why needed here: Understanding how neural networks behave as width approaches infinity is crucial for deriving µP and analyzing learning regimes.
  - Quick check question: What distinguishes the NTK regime from the feature learning regime in terms of gradient dynamics?

- **Concept**: Local learning algorithms (predictive coding and target propagation)
  - Why needed here: The paper derives µP specifically for these alternatives to backpropagation, requiring understanding of their update rules and loss functions.
  - Quick check question: How do predictive coding and target propagation differ in how they generate local targets?

- **Concept**: Parameterization scaling (abc-parameterization)
  - Why needed here: µP is defined through specific scaling of initialization and learning rates with respect to width, which is central to the theoretical analysis.
  - Quick check question: What conditions must an abc-parameterization satisfy to be considered stable?

## Architecture Onboarding

- **Component map**: Neural network architectures (MLP, CNN, VGG-like) -> Local learning algorithms (PC, TP, DTP) -> Theoretical analysis tools (linear network analysis, µP derivation)
- **Critical path**: Derive µP -> Verify hyperparameter transfer -> Analyze learning regimes -> Experimentally validate across architectures
- **Design tradeoffs**: PC balances between first-order and Gauss-Newton-like gradients via γL scaling; TP sacrifices kernel regime for feature learning via unique last-layer scaling
- **Failure signatures**: If µTransfer fails, learning rates won't transfer across widths; if inference becomes unstable, gradients may explode or vanish; if kernel regime appears in TP, parameterization is incorrect
- **First 3 experiments**:
  1. Train a 3-layer MLP on FashionMNIST with PC using different γL values to observe gradient interpolation
  2. Implement TP with linear feedback network and verify that learning rate transfers across widths
  3. Analyze a deep linear network under PC to derive fixed points and compare with BP/GNT gradients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the maximal update parameterization (μP) be derived for more complex local learning algorithms beyond predictive coding (PC) and target propagation (TP), such as those involving multiple inference phases or adaptive learning rates?
- Basis in paper: [explicit] The paper focuses on deriving μP for PC and TP, but acknowledges that μP depends on the specific training algorithm and should be derived for each method. It also mentions that handling dependencies between variables that differ from standard backpropagation, such as those arising from the inference phase and feedback pass, is non-trivial.
- Why unresolved: Deriving μP for more complex local learning algorithms requires developing a tensor program that can handle the intricate dependencies between variables in these algorithms, which is beyond the scope of this study.
- What evidence would resolve it: Developing a tensor program that can handle the dependencies in complex local learning algorithms and successfully deriving μP for these algorithms, followed by experimental validation of hyperparameter transfer across widths.

### Open Question 2
- Question: How do the learning dynamics and convergence properties of local learning algorithms, such as PC and TP, compare to those of backpropagation (BP) in the infinite-width limit?
- Basis in paper: [explicit] The paper analyzes the gradients of PC and TP in the infinite-width limit and finds that they interpolate between first-order and Gauss-Newton-like gradients, depending on the parameterization. It also reveals that TP's parameterization leads to the disappearance of the kernel regime, favoring feature learning more strongly than BP.
- Why unresolved: While the paper provides insights into the gradients and learning regimes of PC and TP, a comprehensive understanding of their learning dynamics and convergence properties compared to BP requires further analysis, such as extending the infinite-width theory or analyzing linear networks in more detail.
- What evidence would resolve it: Extending the infinite-width theory to analyze the learning dynamics of PC and TP, or conducting a detailed analysis of linear networks to compare their convergence properties with BP.

### Open Question 3
- Question: Can the maximal update parameterization (μP) be effectively applied to other architectures beyond MLPs, CNNs, and VGG-like models, such as transformers or graph neural networks, for local learning algorithms?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of μP for local learning algorithms in MLPs, CNNs, and VGG-like models, but does not explore its applicability to other architectures. The μP framework is architecture-independent, suggesting potential applicability to other architectures.
- Why unresolved: The paper does not investigate the effectiveness of μP for local learning algorithms in architectures beyond MLPs, CNNs, and VGG-like models. Exploring the applicability of μP to other architectures requires experimental validation.
- What evidence would resolve it: Applying μP to local learning algorithms in transformers or graph neural networks and demonstrating successful hyperparameter transfer across widths, along with improved performance compared to standard parameterizations.

## Limitations

- Theoretical analysis relies heavily on linear network approximations which may not fully capture nonlinear dynamics in practical deep networks
- Experimental validation is limited to specific architectures (MLPs, CNNs, VGG-like) and datasets (FashionMNIST, CIFAR), raising questions about generalizability
- The analysis assumes fixed-point inference, but practical implementations may use different inference strategies

## Confidence

- High confidence: The derivation of µP for PC and TP, and the theoretical framework for analyzing local learning algorithms in the infinite-width limit
- Medium confidence: The experimental results showing hyperparameter transfer across widths, given the limited scope of architectures and datasets
- Medium confidence: The claims about the disappearance of the kernel regime in TP, due to lack of extensive empirical validation

## Next Checks

1. Test µTransfer across a wider range of architectures including residual networks and modern architectures like transformers
2. Conduct ablation studies on the inference step size scaling to verify the theoretical predictions about gradient interpolation in PC
3. Perform experiments with nonlinear networks to validate whether the linear analysis accurately predicts the behavior in practical settings