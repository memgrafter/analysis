---
ver: rpa2
title: 'Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal
  Model'
arxiv_id: '2408.11039'
source_url: https://arxiv.org/abs/2408.11039
tags:
- image
- transfusion
- text
- images
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transfusion is a novel method for training a single transformer
  model to generate both discrete text and continuous images by combining language
  modeling (next token prediction) and diffusion objectives. Unlike prior approaches
  that discretize images into tokens, Transfusion operates directly on continuous
  latent image representations, avoiding information loss.
---

# Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model

## Quick Facts
- arXiv ID: 2408.11039
- Source URL: https://arxiv.org/abs/2408.11039
- Authors: Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy
- Reference count: 17
- Key outcome: Transfusion is a novel method for training a single transformer model to generate both discrete text and continuous images by combining language modeling (next token prediction) and diffusion objectives.

## Executive Summary
Transfusion is a novel method for training a single transformer model to generate both discrete text and continuous images by combining language modeling (next token prediction) and diffusion objectives. Unlike prior approaches that discretize images into tokens, Transfusion operates directly on continuous latent image representations, avoiding information loss. The method uses separate loss functions for each modality while sharing most model parameters, with modality-specific encoding/decoding layers and attention masks (causal for text, bidirectional for images within the same image).

Controlled experiments show Transfusion significantly outperforms the Chameleon discretization approach on text-to-image generation (2× lower FID at parity compute), image-to-text generation (parity at 21.8% of compute), and even text-only tasks (parity at 50-60% of compute). A 7B-parameter Transfusion model trained on 2T multi-modal tokens achieves image generation quality comparable to state-of-the-art diffusion models while also matching Llama 1 performance on text tasks, demonstrating the viability of unified multi-modal models that can generate both high-quality text and images.

## Method Summary
Transfusion trains a single transformer model to generate both discrete text and continuous images by combining language modeling and diffusion objectives. The model processes mixed-modality sequences with a unified transformer architecture, using causal attention for text tokens and bidirectional attention for image patches within the same image. Separate encoding/decoding layers handle modality-specific input/output representations, while most parameters are shared. The model is trained with a combined loss function that uses language modeling loss for text and diffusion loss for images, with a λ coefficient balancing the two objectives.

## Key Results
- Transfusion achieves 2× lower FID scores on text-to-image generation compared to Chameleon at parity compute
- Image-to-text generation performance matches state-of-the-art at 21.8% of compute
- Text-only task performance matches Llama 1 at 50-60% of compute with 7B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfusion enables a single transformer to learn both text and image generation without information loss by using modality-specific objectives (LM loss for text, diffusion loss for images) while sharing most parameters.
- Mechanism: The model processes both modalities in the same sequence with a unified transformer, but applies causal attention for text tokens and bidirectional attention for image patches within the same image. Separate encoding/decoding layers handle modality-specific input/output representations.
- Core assumption: The transformer architecture can effectively learn from both discrete next-token prediction and continuous diffusion objectives simultaneously without interference.
- Evidence anchors:
  - [abstract] "Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences"
  - [section] "Our main innovation is demonstrating that we can use separate losses for different modalities – language modeling for text, diffusion for images – over shared data and parameters"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

### Mechanism 2
- Claim: Transfusion achieves better scaling efficiency than discretization approaches because it avoids the information bottleneck of converting images to discrete tokens.
- Mechanism: By keeping images in continuous latent space throughout training and inference, Transfusion preserves more information compared to approaches that quantize images into discrete tokens before training a language model.
- Core assumption: The information lost during image quantization is significant enough to impact model performance, and preserving continuous representations provides a measurable advantage.
- Evidence anchors:
  - [abstract] "Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens"
  - [section] "The key difference between Chameleon and Transfusion is that while Chameleon discretizes images and processes them as tokens, Transfusion keeps images in continuous space, removing the quantization information bottleneck"
  - [corpus] Weak - corpus neighbors discuss related topics but don't provide direct evidence for this specific claim

### Mechanism 3
- Claim: Intra-image bidirectional attention is crucial for effective image generation in Transfusion models.
- Mechanism: By allowing image patches to attend to all other patches within the same image (not just previous patches), the model can capture spatial relationships and dependencies more effectively than causal-only attention would allow.
- Core assumption: Image generation benefits from full spatial context within each image, similar to how diffusion models operate with full bidirectional attention.
- Evidence anchors:
  - [section] "We observe that the intra-image bidirectional attention is important, and that replacing it with causal attention hurts text-to-image generation"
  - [section] "Table 5 shows that enabling this attention pattern beyond the standard causal attention is advantageous throughout all benchmarks, and using both image encoding/decoding architectures"
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism

## Foundational Learning

- Concept: Next-token prediction (autoregressive language modeling)
  - Why needed here: This is the standard objective for text generation that Transfusion uses for the text modality
  - Quick check question: What is the key difference between causal and bidirectional attention in the context of next-token prediction?

- Concept: Diffusion models and denoising process
  - Why needed here: Transfusion uses the diffusion objective for image generation, which involves learning to reverse a gradual noise-addition process
  - Quick check question: In diffusion models, what is the relationship between the noise prediction and the actual denoising process?

- Concept: Variational Autoencoders (VAEs) for image representation
  - Why needed here: Transfusion uses a VAE to encode images into latent space before processing them with the transformer
  - Quick check question: Why would you choose to work in latent space rather than pixel space when training diffusion models for images?

## Architecture Onboarding

- Component map: Tokenization/Patchification -> Attention processing -> Modality-specific encoding/decoding -> Loss computation -> Parameter update

- Critical path: Tokenization/Patchification → Attention processing → Modality-specific encoding/decoding → Loss computation → Parameter update

- Design tradeoffs:
  - Linear vs U-Net patch encoding: U-Net adds parameters but provides better inductive biases for image processing
  - Patch size selection: Larger patches reduce compute but may lose fine details
  - Bidirectional vs causal attention within images: Bidirectional captures more spatial context but is computationally heavier

- Failure signatures:
  - Poor text generation quality: May indicate attention mask issues or modality-specific component problems
  - Blurry or low-quality images: Could suggest issues with the diffusion objective implementation or VAE encoding quality
  - Mode collapse in one modality: Might indicate gradient conflict between the two objectives

- First 3 experiments:
  1. Train a minimal Transfusion model (small transformer) on a tiny mixed dataset to verify basic functionality and that both losses can be computed correctly
  2. Test attention mask implementation by verifying that text tokens cannot attend to future tokens and image patches can attend within their own image
  3. Compare image generation quality with and without bidirectional attention within images to validate the importance of this design choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Transfusion perform when trained with different combinations of modalities (text-only, image-only, mixed)?
- Basis in paper: [inferred] The paper mentions that Transfusion models can be defined over different sizes of latent pixel patches and that larger patch sizes allow the model to pack more images in each training batch. It also mentions that Transfusion can be trained on different ratios of text and image data.
- Why unresolved: The paper does not provide results for Transfusion trained on different combinations of modalities. It only compares Transfusion to Chameleon, which uses a similar approach but discretizes images.
- What evidence would resolve it: Experiments comparing Transfusion models trained on different combinations of modalities, such as text-only, image-only, and mixed, would show how well Transfusion can handle each modality individually and how it performs when combining them.

### Open Question 2
- Question: How does the choice of patch size and encoding/decoding architecture affect the performance of Transfusion on different tasks?
- Basis in paper: [explicit] The paper discusses the trade-offs between larger patch sizes and performance, and mentions that using U-Net encoding/decoding layers can improve performance compared to simple linear layers.
- Why unresolved: While the paper provides some insights into the effects of patch size and encoding/decoding architecture, it does not provide a comprehensive analysis of how these choices affect performance on different tasks.
- What evidence would resolve it: Experiments varying patch size and encoding/decoding architecture, and evaluating performance on a range of tasks (e.g., text-to-image generation, image-to-text generation, text-only tasks) would provide a clearer understanding of how these choices impact Transfusion's capabilities.

### Open Question 3
- Question: Can Transfusion be extended to handle additional modalities beyond text and images?
- Basis in paper: [inferred] The paper mentions that Transfusion combines language modeling (next token prediction) with diffusion, and that it uses separate loss functions for each modality while sharing most model parameters. This suggests that Transfusion could potentially be extended to handle other modalities by defining appropriate loss functions and encoding/decoding layers.
- Why unresolved: The paper only demonstrates Transfusion on text and images. It does not explore the possibility of extending it to other modalities.
- What evidence would resolve it: Experiments applying Transfusion to additional modalities, such as audio or video, and evaluating its performance on tasks specific to those modalities would show whether Transfusion can be effectively extended beyond text and images.

### Open Question 4
- Question: How does the amount of noise applied to images during training affect the performance of Transfusion on image understanding tasks?
- Basis in paper: [explicit] The paper mentions that noise is added to images during training as part of the diffusion objective, and that limiting the amount of noise can improve performance on image captioning tasks.
- Why unresolved: While the paper shows that limiting noise can improve performance on one specific task, it does not explore the broader effects of noise on image understanding tasks.
- What evidence would resolve it: Experiments varying the amount of noise applied to images during training and evaluating performance on a range of image understanding tasks (e.g., image classification, object detection, visual question answering) would provide a more comprehensive understanding of how noise affects Transfusion's ability to understand images.

## Limitations
- Computational expense remains substantial despite scaling advantages over discretization approaches
- Evaluation relies primarily on automated metrics (FID, CLIP, CIDEr) without extensive human validation
- Claims about text-only task performance relative to specialized models need more extensive validation

## Confidence

**High Confidence**: The core claim that Transfusion can successfully train a single transformer on both text and image generation using separate loss functions is well-supported by the experimental results. The scaling advantages over Chameleon are demonstrated across multiple benchmarks with clear quantitative improvements.

**Medium Confidence**: The assertion that intra-image bidirectional attention is crucial for image generation quality is supported by ablation studies, but the evidence is limited to the specific Transfusion architecture. The exact contribution of this design choice relative to other factors (VAE quality, diffusion objective implementation) is not fully isolated.

**Low Confidence**: The paper's claims about Transfusion's performance on text-only tasks relative to specialized models like Llama 1 are based on limited comparisons. The text generation quality improvements at reduced compute appear promising but require more extensive validation across diverse benchmarks.

## Next Checks
1. **Ablation Study Extension**: Conduct a comprehensive ablation study varying the λ coefficient balancing text and image losses across a wider range (e.g., λ ∈ [1, 3, 5, 10, 20]) to determine the optimal trade-off between modalities and test sensitivity to this hyperparameter.

2. **Cross-Modal Generalization**: Test Transfusion's ability to perform cross-modal tasks not explicitly trained for, such as generating text conditioned on multiple images or reasoning about temporal sequences of images, to assess the generality of the learned representations.

3. **Scaling Analysis**: Systematically evaluate model performance across different parameter scales (e.g., 1B, 3B, 7B, 13B) to validate the claimed scaling advantages and identify potential diminishing returns or modality-specific scaling differences.