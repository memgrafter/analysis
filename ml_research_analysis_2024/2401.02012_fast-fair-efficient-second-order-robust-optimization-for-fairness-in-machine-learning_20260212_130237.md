---
ver: rpa2
title: 'Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in Machine
  Learning'
arxiv_id: '2401.02012'
source_url: https://arxiv.org/abs/2401.02012
tags:
- fairness
- training
- robust
- optimization
- radius
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores adversarial training to improve fairness in
  deep neural networks (DNNs), which are known to inherit bias with respect to sensitive
  attributes such as race and gender. The authors propose a robust optimization framework
  that leverages second-order information to efficiently solve the inner optimization
  problem in adversarial training.
---

# Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in Machine Learning

## Quick Facts
- arXiv ID: 2401.02012
- Source URL: https://arxiv.org/abs/2401.02012
- Reference count: 15
- Key outcome: Second-order robust optimization improves fairness metrics while being faster than PGD (1.4-31.4x speedup).

## Executive Summary
This work addresses fairness in deep neural networks by leveraging adversarial training with second-order optimization. The authors propose a trust region subproblem (TRS) method that uses quadratic Taylor expansion and bisection to efficiently solve the inner optimization problem in adversarial training. Experiments on synthetic and real-world datasets demonstrate that robust optimization can improve fairness metrics (independence, separation, sufficiency) while the TRS method achieves significant computational speedups compared to projected gradient descent.

## Method Summary
The method combines adversarial training with second-order optimization to improve fairness in DNNs. The inner optimization problem is formulated as a trust region subproblem using second-order Taylor expansion of the loss function. The authors solve this using a bisection method on the Lagrangian multiplier to find optimal perturbations. The outer optimization uses standard gradient descent. The approach is compared against PGD and random perturbation baselines on synthetic (Unfair2D) and real-world (Adult, LSAT) datasets using a simple linear DNN with one hidden layer.

## Key Results
- Robust optimization improved fairness metrics by up to 86% on synthetic data for certain radii
- TRS method was 1.4-31.4 times faster than PGD depending on dataset and radius
- Fairness improvements were observed across independence, separation, and sufficiency metrics
- Accuracy remained comparable to non-robust baselines while improving fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Second-order Taylor expansion improves inner optimization accuracy and convergence speed.
- Mechanism: The trust region subproblem (TRS) approximates the loss function quadratically around each training point, allowing exact KKT conditions to be solved efficiently via bisection on the Lagrangian multiplier.
- Core assumption: The quadratic approximation is accurate within the trust region radius and the Hessian is well-conditioned.
- Evidence anchors:
  - [abstract] "Leveraging second order information, we are able to find a solution to our optimization problem more efficiently than a purely first order method."
  - [section] "We use second order information to solve the inner optimization problem efficiently in terms of computational time."
- Break condition: The Hessian becomes singular or highly ill-conditioned, making the quadratic approximation invalid or