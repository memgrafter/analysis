---
ver: rpa2
title: 'WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business
  Process Management Tasks'
arxiv_id: '2406.13264'
source_url: https://arxiv.org/abs/2406.13264
tags:
- task
- workflow
- tasks
- action
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WONDERBREAD is the first large-scale benchmark for evaluating multimodal
  foundation models on business process management (BPM) tasks beyond automation.
  The dataset contains 2928 high-quality human demonstrations across 598 workflows,
  with each demonstration including screen recordings, action traces, key frames,
  and written step-by-step guides (SOPs).
---

# WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks

## Quick Facts
- arXiv ID: 2406.13264
- Source URL: https://arxiv.org/abs/2406.13264
- Reference count: 40
- First large-scale benchmark for evaluating multimodal models on BPM tasks beyond automation

## Executive Summary
WONDERBREAD is the first large-scale benchmark designed to evaluate multimodal foundation models on business process management tasks. The dataset contains 2928 high-quality human demonstrations across 598 workflows, with each demonstration including screen recordings, action traces, key frames, and written step-by-step guides. The benchmark introduces 6 novel BPM tasks spanning documentation, knowledge transfer, and process improvement, with automated evaluation pipelines using both programmatic metrics and LLM-based assessments. Initial results reveal that while state-of-the-art models like GPT-4 can generate accurate documentation (F1 of 0.82) and determine workflow completion (F1 of 0.90), they struggle with finer-grained analyses such as exact SOP validation (F1 < 0.3) and workflow quality ranking (Kendall τ near 0).

## Method Summary
The benchmark uses 2928 human demonstrations across 598 workflows from WebArena, filtered to exclude impossible, underspecified, or incorrectly evaluated workflows. Each demonstration includes screen recordings, action traces, key frames, and SOPs. Six BPM tasks are defined: SOP generation, demo validation, SOP improvement, question answering, demo segmentation, and SOP ranking. Evaluations use programmatic metrics (F1, adjusted rand index) and LLM-based assessments via GPT-4-as-a-judge. The repository provides evaluation scripts for baseline experiments with multimodal models like GPT-4, Claude 3, and Gemini Pro.

## Key Results
- GPT-4 achieves F1 of 0.82 for SOP generation with intent, keyframes, and action trace
- Models determine workflow completion with F1 of 0.90 but struggle with exact SOP validation (F1 < 0.3)
- Self-improvement capability shows up to 1.4 point improvement in SOP quality with rubric-based feedback
- Models fail at workflow quality ranking with Kendall τ near 0, indicating poor human-model alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal foundation models can leverage visual and textual cues to generate accurate workflow documentation by aligning observed actions with procedural language.
- Mechanism: The model processes screen recordings and key frames to infer user actions (clicks, keystrokes, scrolls), then translates these observations into step-by-step SOPs. Visual grounding improves recall by allowing the model to detect implicit actions that might be missed from text alone.
- Core assumption: Visual context provides sufficient detail to reconstruct procedural steps without requiring manual annotation.
- Evidence anchors:
  - [abstract] "models can generate accurate documentation (F1 of 0.82) when provided with intent, keyframes, and action trace."
  - [section 5.1] "GPT-4 performs best (F1-score of 0.82) with intent, keyframes, and action trace."
  - [corpus] "Machine learning in business process management: A systematic literature review" shows increasing use of multimodal data for process understanding.
- Break condition: If key frames lack clarity or if the action trace is incomplete, the model may hallucinate steps not present in the demonstration.

### Mechanism 2
- Claim: Models struggle with fine-grained workflow validation due to insufficient alignment between high-level visual understanding and low-level procedural accuracy.
- Mechanism: While models can identify overall workflow completion (F1 0.90), they fail to validate exact step-by-step adherence to SOPs because the granularity of visual observations doesn't match the precision required for step-by-step comparison.
- Core assumption: High-level visual understanding does not translate directly to precise procedural validation.
- Evidence anchors:
  - [abstract] "struggle with finer-grained analyses such as exact SOP validation (F1 < 0.3)"
  - [section 5.2] "Models struggle with low-level error correction... peak F1 achieved is 0.27."
  - [corpus] "SNAP: Semantic Stories for Next Activity Prediction" indicates challenges in aligning multimodal observations with sequential procedural logic.
- Break condition: If the SOP and demonstration use significantly different terminology or structure, validation accuracy drops further.

### Mechanism 3
- Claim: Multimodal models can improve their own SOPs through iterative refinement when provided with a rubric-based feedback loop.
- Mechanism: The model generates an initial SOP, receives rubric-based evaluation (e.g., element specification, action relevance), and produces improved versions. This self-improvement capability is driven by the model's ability to compare its output against quality criteria.
- Core assumption: The model can effectively interpret rubric feedback and apply it to improve procedural accuracy.
- Evidence anchors:
  - [abstract] "Models can improve their outputs (i.e., SOPs) through multiple iterations of self-reflection"
  - [section 5.3] "Models are capable of improving the quality of their own SOPs (up to 1.4 points), conditioned upon a SOP rubric."
  - [corpus] "Agentic Business Process Management" suggests iterative refinement as a key capability for enterprise workflow tools.
- Break condition: If the rubric is ambiguous or the model cannot interpret qualitative feedback, improvement iterations may plateau or regress.

## Foundational Learning

- Concept: Multimodal data fusion
  - Why needed here: The benchmark relies on combining visual (screenshots, videos) and textual (action traces, SOPs) data to evaluate model performance across BPM tasks.
  - Quick check question: Can you explain how a model would use both a screenshot and an action trace to infer what step was performed?

- Concept: Semantic equivalence evaluation
  - Why needed here: SOP generation and improvement tasks require comparing generated procedural text to reference SOPs, which involves determining semantic similarity rather than exact string matching.
  - Quick check question: What metrics would you use to evaluate whether two SOPs describe the same workflow steps with different wording?

- Concept: Workflow segmentation and clustering
  - Why needed here: The Demo Segmentation task requires identifying boundaries between workflows in concatenated video sequences, which is a clustering problem over temporal data.
  - Quick check question: How would you determine if a frame belongs to workflow A or workflow B when they're shown sequentially in the same video?

## Architecture Onboarding

- Component map: Dataset loader (2928 demonstrations) -> Task generators (6 BPM tasks) -> Evaluation harness (programmatic metrics + LLM-based scoring) -> Baseline model runners (GPT-4, Claude 3, Gemini Pro)
- Critical path: For any given task, the critical path is: load demonstration -> preprocess into appropriate multimodal format -> generate model output -> evaluate using task-specific metrics -> aggregate results
- Design tradeoffs: The benchmark trades dataset size (598 workflows) for annotation quality (multiple annotators, quality assurance rounds). This prioritizes realistic enterprise scenarios over quantity.
- Failure signatures: Common failure modes include: model hallucination in SOP generation, poor temporal segmentation in demo segmentation, and inability to rank SOPs by quality due to lack of human preference alignment.
- First 3 experiments:
  1. Run SOP generation with intent only, then with intent+keyframes, then with intent+keyframes+action trace to measure the impact of each modality.
  2. Test demo validation on truncated vs shuffled demonstrations to understand model sensitivity to workflow completion vs exact step adherence.
  3. Evaluate question answering performance on single vs multiple demonstration inputs to assess model's ability to synthesize information across sources.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal foundation models be better aligned with human judgment of workflow quality?
- Basis in paper: The paper shows that models struggle with SOP ranking (Kendall correlation near 0) and human-model alignment is identified as a key area for future research.
- Why unresolved: The paper identifies this as an open challenge but does not provide concrete solutions for improving alignment between model and human judgments of workflow quality.
- What evidence would resolve it: Experiments demonstrating improved Kendall correlation scores (closer to 1) after applying specific alignment techniques like fine-tuning or preference learning on workflow quality judgments.

### Open Question 2
- Question: What is the impact of longer context windows on multimodal model performance for business process management tasks?
- Basis in paper: The paper identifies longer context windows as a key area for future research, noting that even 1-minute workflows can generate dozens of actions and key frames.
- Why unresolved: The paper does not test models with extended context windows or analyze how performance changes with increased context length.
- What evidence would resolve it: Comparative results showing performance improvements on BPM tasks when using models with longer context windows (e.g., 32K vs 8K tokens) across various workflow lengths and complexity levels.

### Open Question 3
- Question: How well do open-source multimodal models perform on WONDERBREAD compared to proprietary models?
- Basis in paper: The paper notes that baseline results lack open-source models and matching proprietary model performance with open-source alternatives remains an open challenge.
- Why unresolved: The benchmark results only include proprietary models (GPT-4, Claude 3, Gemini Pro) without any open-source alternatives for comparison.
- What evidence would resolve it: Benchmark results showing performance metrics (F1 scores, accuracy, correlation) for open-source models like LLaVA, Qwen-VL, or other multimodal models on all 6 BPM tasks.

## Limitations
- Limited generalizability to non-web-based BPM environments due to focus on web workflows
- Dataset size (598 workflows) may lack statistical power for certain analyses compared to larger ML benchmarks
- Potential bias from using GPT-4 as both model and judge in evaluation pipeline

## Confidence
- High confidence in core findings about multimodal model capabilities and limitations on BPM tasks
- Medium confidence regarding generalizability to other enterprise contexts
- Uncertainties exist about exact implementation details of LLM-based evaluation criteria

## Next Checks
1. **Cross-dataset validation**: Test model performance on additional BPM datasets or real enterprise workflows to assess generalizability beyond the WONDERBREAD corpus.

2. **Alternative evaluation protocols**: Implement human-in-the-loop validation for critical tasks like SOP ranking and validation to verify LLM-based assessment reliability and identify potential judge-model alignment issues.

3. **Extended context experiments**: Evaluate model performance with varying input lengths and temporal contexts to determine the impact of context window limitations on workflow understanding and validation accuracy.