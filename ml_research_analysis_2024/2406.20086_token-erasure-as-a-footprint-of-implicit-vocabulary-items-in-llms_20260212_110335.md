---
ver: rpa2
title: Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs
arxiv_id: '2406.20086'
source_url: https://arxiv.org/abs/2406.20086
tags:
- tokens
- token
- multi-token
- words
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies how large language models (LLMs) transform\
  \ groups of arbitrary tokens into meaningful representations for multi-token words\
  \ and named entities. It observes that last token representations in early layers\
  \ rapidly lose information about preceding and current tokens\u2014a phenomenon\
  \ termed \"token erasure.\" The authors propose a novel method to \"read out\" the\
  \ implicit vocabulary of an LLM by measuring this erasure effect across layers and\
  \ using a scoring heuristic to segment text into high-scoring, non-overlapping token\
  \ sequences."
---

# Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs

## Quick Facts
- arXiv ID: 2406.20086
- Source URL: https://arxiv.org/abs/2406.20086
- Reference count: 28
- Large language models exhibit "token erasure" where last token representations of multi-token words and named entities rapidly lose information about preceding tokens in early layers

## Executive Summary
This paper identifies a novel phenomenon called "token erasure" in large language models, where last token representations of multi-token words and named entities systematically lose information about preceding and current tokens between early layers. The authors propose a method to "read out" the implicit vocabulary of an LLM by measuring this erasure effect and using a scoring heuristic to segment text into high-scoring, non-overlapping token sequences. When applied to Llama-2-7b and Llama-3-8b, the approach successfully identifies many multi-token words and named entities, recovering around 1,800 and 900 sequences respectively from Wikipedia data.

## Method Summary
The authors train linear probes to predict nearby tokens from hidden states at different layers and positions, then use these probes to compute erasure scores that measure information loss between layers 1 and 9. They apply this scoring to segment documents into non-overlapping high-scoring sequences, which they aggregate to form an estimated implicit vocabulary. The method uses Wikipedia articles (500 articles, ~256k tokens), COUNTER FACT dataset, and Pile dataset for probe training with AdamW for 16 epochs.

## Key Results
- Last token representations of multi-token words and named entities exhibit pronounced "erasure" effects in early layers
- The erasure score successfully identifies sequences treated as lexical units by the model
- Llama-2-7b recovered ~1,800 multi-token words and ~900 named entities from Wikipedia data
- Llama-3-8b recovered ~900 multi-token words and ~300 named entities from Wikipedia data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-token words and named entities lose token-level information in early layers due to a "detokenization" process
- Mechanism: The last token representation of multi-token sequences actively forgets information about preceding and current tokens between layers 1 and 9, creating a footprint that signals the formation of higher-level lexical representations
- Core assumption: The erasure effect is not random noise but a systematic process where the model replaces token-level information with a unified lexical representation
- Evidence anchors:
  - [abstract] "last token representations of named entities and multi-token words exhibit a pronounced 'erasure' effect, where information about previous and current tokens is rapidly forgotten in early layers"
  - [section 3.2] "We see a striking 'erasure' effect for last tokens of COUNTER FACT subjects, where these hidden states consistently 'forget about' preceding and current tokens"
  - [corpus] Weak evidence - corpus neighbors mention "detokenization" and "token grouping" but no direct evidence of erasure mechanisms
- Break condition: If token erasure is shown to be an artifact of probe training imbalance or if similar erasure patterns appear for non-lexical token sequences

### Mechanism 2
- Claim: The erasure score (ψ) successfully identifies sequences that are treated as lexical units by the model
- Mechanism: ψ measures the drop in probability of predicting correct tokens within a sequence between early layers, penalizing sequences where erasure happens for tokens outside the sequence boundaries
- Core assumption: High ψ scores correlate with the model treating a token sequence as a single lexical item during internal processing
- Evidence anchors:
  - [abstract] "we propose a method to 'read out' the implicit vocabulary of an autoregressive LLM by examining differences in token representations across layers"
  - [section 4.1] "We design a metric ψp,q that uses probe outputs from Section 3 to look for erasure effects between layer 1 and layer L"
  - [corpus] Weak evidence - corpus mentions "token grouping" but no direct evidence linking ψ to lexical unit identification
- Break condition: If high-scoring sequences under ψ include many random or non-lexical token combinations that don't correspond to meaningful units

### Mechanism 3
- Claim: Models develop implicit vocabularies that map arbitrary token groups to semantically meaningful units during pretraining
- Mechanism: Through exposure to text, models learn to convert suboptimal tokenization into useful higher-level representations
- Core assumption: The model's performance on downstream tasks indicates successful formation of these implicit lexical representations despite poor tokenization
- Evidence anchors:
  - [abstract] "LLMs develop an implicit vocabulary that maps from groups of arbitrary tokens to semantically meaningful units"
  - [section 2] "large models are apparently able to 'understand' such idiosyncratic tokenizations of multi-token words with few observable effects on downstream performance"
  - [corpus] Weak evidence - corpus neighbors discuss token vocabularies but don't address implicit vocabulary formation mechanisms
- Break condition: If ablation studies show that removing the erasure effect doesn't impact model performance on lexical tasks

## Foundational Learning

- Concept: Linear probing for token prediction
  - Why needed here: The method relies on training linear probes to predict nearby tokens from hidden states, which reveals how token information is preserved or erased across layers
  - Quick check question: If a probe trained on layer 5 hidden states predicts the previous token with 90% accuracy, what does this tell us about information retention at that layer?

- Concept: Byte-pair encoding and tokenization artifacts
  - Why needed here: Understanding how BPE creates semantically unrelated tokens is crucial for recognizing why models need implicit detokenization mechanisms
  - Quick check question: Given that "Hawaii" can be tokenized as either ['_Hawai', 'i'] or ['_ha', 'w', 'ai', 'i'], what challenge does this create for semantic understanding?

- Concept: Autoregressive model processing
  - Why needed here: The sequential nature of token processing explains why last tokens can accumulate information about the entire entity before detokenization occurs
  - Quick check question: Why can't the model enrich the token "_Star" with information about "Wars" until after seeing the "Wars" token?

## Architecture Onboarding

- Component map: Linear probes (trained on hidden states) → Erasure score calculator (ψ) → Document segmentation algorithm → Vocabulary extraction pipeline
- Critical path: Probe training (6-8 hours per probe) → Testing on COUNTER FACT and Wikipedia data → Computing ψ scores → Non-overlapping segmentation → Vocabulary aggregation
- Design tradeoffs: Higher L values (layers) might capture more complete erasure but increase computation; using only layer 1 vs layer 9 differences balances sensitivity and specificity
- Failure signatures: Low precision in extracted vocabulary (many non-lexical sequences), poor recall (missing known multi-token words), or inconsistent erasure patterns across different models
- First 3 experiments:
  1. Train probes on Llama-2-7b and test on both COUNTER FACT subjects and Wikipedia multi-token words to verify erasure effect consistency
  2. Implement Algorithm 1 on a small Wikipedia sample and manually verify top-scoring sequences against known lexical items
  3. Compare ψ scores across different L values (5, 9, 13) to determine optimal layer range for erasure detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do autoregressive LLMs process multi-token words versus multi-token named entities mechanistically?
- Basis in paper: [explicit] The paper demonstrates that both multi-token words and named entities exhibit similar "erasure" effects in last token representations, but the exact mechanisms may differ
- Why unresolved: The paper only shows that both types exhibit erasure patterns but doesn't distinguish whether the underlying processing mechanisms are identical or different
- What evidence would resolve it: Direct comparison of erasure patterns, probing results, and segmentations for words versus named entities under controlled conditions would clarify if they're processed identically or through different mechanisms

### Open Question 2
- Question: Does the erasure effect correlate with a word/entity's frequency in training data?
- Basis in paper: [inferred] The paper mentions that erasure patterns exist but doesn't explore whether frequency affects the strength of erasure
- Why unresolved: The authors acknowledge this as a limitation, noting they haven't tested whether erasure correlates with familiarity or training data presence
- What evidence would resolve it: Correlating erasure scores with word/entity frequency statistics from training data would reveal if common items show stronger or weaker erasure effects

### Open Question 3
- Question: Are there fundamental differences in how Llama-2-7b versus Llama-3-8b process lexical items?
- Basis in paper: [explicit] The paper shows Llama-2-7b recovered 1,800 sequences while Llama-3-8b recovered only 900, with Llama-3-8b's vocabulary containing larger expressions and code
- Why unresolved: The authors note this difference but don't explain whether it reflects architectural differences, tokenization strategy, or training data effects
- What evidence would resolve it: Systematic comparison of erasure patterns, probe responses, and vocabulary segments across both models on identical datasets would reveal whether differences stem from architecture, tokenization, or training

## Limitations
- The causal relationship between erasure and lexical representation formation is not definitively established
- The method shows precision-recall tradeoffs with moderate precision suggesting many false positives
- Linear probes may not fully capture complex information dynamics in transformer hidden states

## Confidence

**High Confidence**: The empirical observation that last token representations of multi-token entities show systematic information loss in early layers. The probe training methodology and erasure score computation are clearly specified and reproducible.

**Medium Confidence**: The interpretation that this erasure represents a "detokenization" process forming higher-level lexical representations. While the pattern is clear, alternative explanations (such as attention-based information routing rather than true forgetting) are not ruled out.

**Low Confidence**: The practical utility of the extracted implicit vocabulary for downstream applications. The paper demonstrates the extraction method works but doesn't show that this vocabulary improves model interpretability, debiasing, or other practical outcomes.

## Next Checks

1. **Probe Stability Analysis**: Run the linear probe training procedure with 5 different random seeds and measure variance in erasure scores across seeds. High variance would indicate that the observed erasure patterns are measurement artifacts rather than stable model properties.

2. **Alternative Erasure Windows**: Apply the same methodology with different layer ranges (L=5, L=13, L=17) to determine whether the observed erasure is specific to layers 1-9 or represents a more general phenomenon across the model's depth. Compare vocabulary extraction quality across different window sizes.

3. **Human Evaluation of Extracted Vocabulary**: Select the top 100 highest-scoring sequences from the extracted vocabulary that don't match ground truth lexical items. Have human annotators rate whether these sequences represent meaningful linguistic units, arbitrary token groupings, or neither. This would validate whether the method captures human-interpretable lexical representations beyond known multi-token words.