---
ver: rpa2
title: Towards Understanding the Influence of Training Samples on Explanations
arxiv_id: '2406.03012'
source_url: https://arxiv.org/abs/2406.03012
tags:
- training
- samples
- recourse
- data
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the novel problem of identifying training
  samples that significantly influence explanations in machine learning models. The
  authors propose a Data-SHAP-based algorithm that estimates the influence of each
  training sample on a given explanation by tracking how the explanation changes as
  each sample is processed during training.
---

# Towards Understanding the Influence of Training Samples on Explanations

## Quick Facts
- arXiv ID: 2406.03012
- Source URL: https://arxiv.org/abs/2406.03012
- Authors: André Artelt; Barbara Hammer
- Reference count: 30
- Primary result: Introduces Data-SHAP-based algorithm to identify training samples that influence explanations, showing it can reduce recourse costs and fairness disparities while maintaining predictive performance

## Executive Summary
This paper addresses the novel problem of identifying which training samples influence machine learning model explanations, specifically counterfactual explanations used for computational recourse. The authors propose a Data-SHAP-based algorithm that estimates each training sample's influence on explanations by tracking how explanations change as samples are processed during training. Two case studies demonstrate the method's effectiveness: reducing average cost of recourse and decreasing cost disparities between protected groups. Experiments on Diabetes and German Credit datasets show that removing identified samples significantly improves explanation quality metrics without severely harming predictive performance, outperforming baseline approaches in both fairness improvements and accuracy maintenance.

## Method Summary
The proposed algorithm builds on Data-SHAP to estimate training sample influence on explanations through a gradient-based Monte Carlo approximation. During model training, the algorithm tracks how explanations change when each sample is processed, using the difference in explanations before and after processing as a measure of influence. The method assumes differentiable models trained via gradient descent and approximates the cost of recourse using the difference in logits (|g0(xi) - g1(xi)|) as a computationally efficient proxy for distance to the decision boundary. The algorithm computes influence scores for each training sample and identifies those with high positive influence on explanation costs, which can then be removed to improve explanation quality while monitoring predictive performance impact.

## Key Results
- Experiments show removing top 5% influential samples reduces average cost of recourse by up to 20% on Diabetes dataset
- The method consistently outperforms Data-SHAP and random removal baselines in reducing group fairness disparities
- Removing influential samples for explanations improves fairness metrics while maintaining better classifier accuracy than baseline methods
- Training samples that influence explanations differ substantially from those affecting predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training samples that influence explanations are different from those influencing predictive performance
- Mechanism: The Data-SHAP-based algorithm tracks how explanations change during training by approximating the influence of each sample on the explanation rather than just accuracy
- Core assumption: The cost of recourse can be approximated by the difference in logits (|g0(xi) - g1(xi)|) as a proxy for distance to decision boundary
- Evidence anchors:
  - [abstract] "Experiments on the Diabetes and German Credit datasets show that removing identified samples can significantly reduce recourse costs or group disparities without severely harming predictive performance"
  - [section] "RQ2) The two baselines (random removal and Data-SHAP [13]) return training samples that, if removed, often increase (instead of decrease) the average cost of recourse"
  - [corpus] Weak - related papers focus on data valuation but don't directly address explanation-specific influence
- Break condition: If the logit approximation fails to correlate with actual recourse costs, the algorithm loses its computational feasibility advantage

### Mechanism 2
- Claim: Gradient-based Monte Carlo approximation enables scalable computation of sample influence on explanations
- Mechanism: Instead of retraining models, the algorithm estimates influence scores during a single training pass by comparing explanations before and after processing each sample
- Core assumption: A differentiable model can be trained using gradient descent and explanation generation is computationally feasible at training time
- Evidence anchors:
  - [section] "we propose to use a gradient-based Monte-Carlo approach similar to the one in the original Data-SHAP paper [13] – assuming that the predictive model h(·) is differentiable and can be trained using gradient-descent"
  - [section] "The core part of Algorithm 1 are lines 3-11. The neural network (or any other differentiable model h(·)) is initialized with random parameters w and trained for K iterations"
  - [corpus] Missing - corpus neighbors don't address this specific computational approach
- Break condition: If explanation generation is too expensive or non-differentiable, the approximation breaks down

### Mechanism 3
- Claim: Removing samples with high positive influence scores reduces fairness disparities without severely impacting accuracy
- Mechanism: By identifying and removing training samples that increase cost differences between protected groups, the algorithm reduces group fairness violations
- Core assumption: Training samples that increase group disparity in recourse costs can be isolated and their removal improves fairness metrics
- Evidence anchors:
  - [abstract] "Experiments on the Diabetes and German Credit datasets show that removing identified samples can significantly reduce recourse costs or group disparities without severely harming predictive performance"
  - [section] "RQ2) However (similar to case study I), our method consistently maintains a much better predictive performance of the classifier than the Data-SHAP method which often leads to a drastic drop in predictive performance"
  - [corpus] Weak - related papers discuss fairness but not the specific mechanism of sample removal for explanation fairness
- Break condition: If fairness improvements come at the cost of severe accuracy degradation or if the fairness metrics are not properly aligned with real-world outcomes

## Foundational Learning

- Concept: Shapley values and cooperative game theory
  - Why needed here: The algorithm builds on Data-SHAP which uses Shapley values to quantify each training sample's contribution to explanations
  - Quick check question: What are the three key properties that Shapley values satisfy (efficiency, symmetry, dummy player)?

- Concept: Counterfactual explanations and computational recourse
  - Why needed here: The algorithm specifically targets the influence of training samples on counterfactual explanations used for recourse
  - Quick check question: How does the ℓ1 norm implementation of cost of recourse differ from other distance metrics in terms of interpretability?

- Concept: Fairness metrics and protected attributes
  - Why needed here: The second case study explicitly measures differences in recourse costs between protected groups
  - Quick check question: What is the difference between individual fairness and group fairness in the context of algorithmic recourse?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Model training with influence tracking -> Explanation generation -> Influence score computation -> Sample removal evaluation
- Critical path: 1. Initialize model with random weights 2. Train model while tracking explanation changes per sample 3. Compute influence scores using gradient-based approximation 4. Identify high-influence samples for removal 5. Evaluate impact on explanations and predictive performance
- Design tradeoffs:
  - Accuracy vs. explanation influence: Prioritizing explanation influence may slightly reduce predictive accuracy
  - Computational cost vs. precision: Using logit approximation reduces computation but may introduce approximation errors
  - Model complexity vs. interpretability: Simpler models may be easier to interpret but less powerful
- Failure signatures:
  - High variance in influence scores across training runs indicates instability
  - Minimal difference between influential and non-influential samples suggests poor separation
  - Large drops in predictive performance after sample removal indicate critical information loss
- First 3 experiments:
  1. Run Algorithm 1 on Diabetes dataset with NUN counterfactual method and verify that removing top 5% influential samples reduces average cost of recourse
  2. Compare influence scores from Algorithm 1 with Data-SHAP baseline on Credit dataset to confirm they identify different samples
  3. Test the logit approximation by computing actual recourse costs vs. logit differences on a small validation set

## Open Questions the Paper Calls Out
None

## Limitations
- The logit approximation for recourse costs may not accurately capture true counterfactual distances in all scenarios
- The method requires explanation generation during training, which may be computationally prohibitive for large-scale applications
- Limited validation to binary classification tasks with specific recourse definitions

## Confidence
- Explanation-specific influence scoring: Medium confidence - experimental validation shows effectiveness but relies on approximation
- Predictive performance maintenance: High confidence - consistent results across experiments
- Scalability claims: Medium confidence - gradient-based approximation helps but explanation generation remains costly

## Next Checks
1. Validate the logit-cost correlation on a held-out test set by computing actual recourse costs for removed samples versus predicted influence scores
2. Test the algorithm on multi-class classification problems and regression tasks to assess generalizability
3. Implement the full Shapley computation for a small subset of data to measure approximation error in influence scores