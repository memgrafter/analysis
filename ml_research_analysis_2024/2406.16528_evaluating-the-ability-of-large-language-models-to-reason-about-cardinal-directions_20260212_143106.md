---
ver: rpa2
title: Evaluating the Ability of Large Language Models to Reason about Cardinal Directions
arxiv_id: '2406.16528'
source_url: https://arxiv.org/abs/2406.16528
tags:
- reasoning
- east
- questions
- north
- south
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the ability of large language models (LLMs)
  to reason about cardinal directions through two datasets: a simpler one with 100
  questions focused on world knowledge, and a more complex one with 5,760 questions
  generated from templates testing directional reasoning in various scenarios. Experiments
  with eight different LLMs showed that while models performed well (0.8 accuracy)
  on the simpler dataset, performance dropped significantly on the complex dataset,
  with the best model achieving only 0.595 accuracy.'
---

# Evaluating the Ability of Large Language Models to Reason about Cardinal Directions

## Quick Facts
- arXiv ID: 2406.16528
- Source URL: https://arxiv.org/abs/2406.16528
- Authors: Anthony G Cohn; Robert E Blackwell
- Reference count: 19
- Primary result: LLMs perform significantly better on factual recall of cardinal directions (>0.8 accuracy) than on spatial reasoning tasks (best model 0.595 accuracy)

## Executive Summary
This paper evaluates how well large language models can reason about cardinal directions through two datasets: a simple 100-question set focused on world knowledge and a complex 5,760-question set generated from templates testing directional reasoning in various scenarios. Experiments with eight different LLMs revealed that while models performed well on the simpler dataset, their performance dropped significantly on the complex dataset, with the best model achieving only 0.595 accuracy. The study found that LLMs struggled particularly with inter-cardinal directions and questions involving turning backwards, suggesting that current models perform better at factual recall than spatial reasoning tasks.

## Method Summary
The study evaluated eight different LLMs including GPT-3.5, GPT-4, Claude 3 Opus, and Gemini models using zero-shot prompting with temperature=0. Two datasets were used: a simple set of 100 questions testing world knowledge about cardinal directions, and a complex set of 5,760 questions generated from 6 templates with variations in locomotion types, person forms, and directions. Model responses were evaluated through case-insensitive string comparison after removing punctuation and whitespace, with accuracy calculated along with standard error of the mean.

## Key Results
- LLMs achieved >0.8 accuracy on the simple dataset testing factual recall of cardinal directions
- Performance dropped to 0.595 accuracy on the complex dataset requiring spatial reasoning
- Models showed directional asymmetry, confusing north/south and east/west in asymmetric patterns
- Temperature sensitivity was observed, with accuracy decreasing as temperature increased from 0 to 2.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve higher accuracy on recall-based spatial tasks than on reasoning-based spatial tasks because recall leverages memorized factual associations while reasoning requires step-by-step inference not well supported by current architectures.
- Mechanism: The model retrieves memorized knowledge about cardinal directions (e.g., sun setting direction) directly from pretraining data. For reasoning tasks, the model must compose multiple spatial relations, which current transformer architectures struggle to do reliably without explicit intermediate reasoning steps.
- Core assumption: The pretraining corpus contains sufficient factual spatial knowledge but lacks explicit training on multi-step spatial reasoning patterns.
- Evidence anchors:
  - [abstract] "models performed well (>0.8 accuracy) on the simpler dataset" vs "performance dropped significantly on the complex dataset"
  - [section] "Unlike results generally reported in the literature comparing GPT-35-Turbo and GPT-4 performance, for large, the OpenAI interface to GPT-35-turbo has the highest accuracy"
  - [corpus] Weak evidence - corpus neighbors focus on evaluation reproducibility rather than architectural mechanisms
- Break condition: If models show similar performance on recall and reasoning tasks, or if chain-of-thought prompting significantly improves reasoning accuracy.

### Mechanism 2
- Claim: LLMs show directional asymmetry in confusion patterns (e.g., north vs south, east vs west) due to training data bias and token prediction patterns.
- Mechanism: During pretraining, directional terms appear with different frequencies and contexts, creating learned associations that influence token prediction. The transformer's attention patterns may also favor certain directional transitions over others.
- Core assumption: The pretraining corpus contains uneven distributions of directional contexts and associations.
- Evidence anchors:
  - [section] "There is asymmetry in the north/south and east/west confusion. For example, gpt-35-turbo-0125 predicted north when the answer was south on 138 occasions but predicted south when the answer was north on only ten occasions"
  - [section] "gpt-4-turbo-2024-4-09 predicted east when the answer was west on 212 occasions but never predicted west when the answer was east"
  - [corpus] Weak evidence - corpus neighbors don't address directional bias in training data
- Break condition: If directional confusion becomes symmetric after data augmentation or if models show no directional bias on balanced test sets.

### Mechanism 3
- Claim: Temperature setting affects LLM performance on spatial reasoning tasks because it controls the stochasticity of token prediction, which interacts with the model's learned directional associations.
- Mechanism: Higher temperature increases randomness in next-token selection, which can disrupt the model's learned directional reasoning patterns. At extreme temperatures, the model may produce nonsensical outputs rather than leveraging its spatial knowledge.
- Core assumption: The model's spatial reasoning relies on learned deterministic patterns that can be disrupted by increased stochasticity.
- Evidence anchors:
  - [section] "Fig. 4 Shows accuracy by temperature for gpt-35-turbo-0125 applied to large. When temperature increases, accuracy decreases"
  - [section] "As temperature, tâ†’2.0 the number of errors from the model also increases, requiring repeated retries before obtaining any answer"
  - [corpus] Weak evidence - corpus neighbors focus on reproducibility rather than temperature effects
- Break condition: If temperature changes have no effect on accuracy or if different models show opposite temperature-response patterns.

## Foundational Learning

- Concept: Cardinal and inter-cardinal directions (north, south, east, west, northeast, etc.)
  - Why needed here: The entire evaluation framework tests understanding of these specific directional concepts in spatial reasoning contexts.
  - Quick check question: Can you list all eight cardinal and inter-cardinal directions and explain their relative positions on a compass?

- Concept: Spatial reasoning vs factual recall
  - Why needed here: The paper distinguishes between simple knowledge recall (sun sets in west) and complex reasoning (determining direction after turning backwards along a shore).
  - Quick check question: What's the key difference between recalling that "north is opposite south" versus determining "if I'm walking south along the east shore, which direction is the lake"?

- Concept: Template-based dataset generation
  - Why needed here: Understanding how the 5,760 complex questions were systematically generated from 6 templates with variations in locomotion, person form, and directions.
  - Quick check question: How many total questions are generated from one template with 10 locomotion types, 6 person forms, and 8 directions for each of 2 direction variations?

## Architecture Onboarding

- Component map: Question generation module -> LLM interface -> Answer evaluation system -> Performance analysis dashboard
- Critical path: 1. Generate questions from templates 2. Send questions to LLM with controlled temperature 3. Parse and validate LLM responses against rubric 4. Calculate accuracy and confusion metrics 5. Analyze patterns across models and question types
- Design tradeoffs:
  - Deterministic vs stochastic outputs: Zero temperature ensures reproducibility but may miss model capabilities
  - Question complexity: Simpler questions test recall, complex questions test reasoning
  - Rubric strictness: One-word answers vs natural language responses
- Failure signatures:
  - High invalid response rate: Model not following rubric (e.g., "The lake is to the west" instead of "west")
  - Directional asymmetry: Systematic confusion between certain directions
  - Template-specific failures: Certain question patterns consistently problematic
- First 3 experiments:
  1. Test zero-temperature vs low-temperature (0.1-0.3) to find optimal balance between determinism and performance
  2. Apply chain-of-thought prompting to see if explicit reasoning steps improve accuracy on complex templates
  3. Create a balanced test set with equal representation of all directional pairs to test for directional bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could enable LLMs to reliably reason about cardinal directions, particularly inter-cardinal directions and scenarios involving turning backwards?
- Basis in paper: [explicit] The paper concludes that LLMs struggle with inter-cardinal directions and turning backwards, suggesting these are specific areas of weakness in current models.
- Why unresolved: The paper only tests existing LLMs without exploring modifications to their architecture or training that could improve directional reasoning capabilities.
- What evidence would resolve it: Comparative experiments testing LLMs with modified architectures (e.g., incorporating spatial reasoning modules) or additional training on spatial datasets, measuring improvement in cardinal direction reasoning tasks.

### Open Question 2
- Question: Why do some LLMs show asymmetry in cardinal direction predictions (e.g., predicting north more often than south, or east more often than west), and what underlying mechanisms cause this bias?
- Basis in paper: [explicit] The paper observes unexpected asymmetry in north/south and east/west confusion across different models, with no clear explanation for why some models prefer certain directions.
- Why unresolved: The authors note this bias exists but cannot explain its source, suggesting it may be related to training data distribution or model architecture.
- What evidence would resolve it: Analysis of training data distributions for directional terms, ablation studies removing directional biases from training data, or architectural analysis to identify directional preference mechanisms.

### Open Question 3
- Question: How does the performance of LLMs on cardinal direction reasoning tasks compare to their performance on other types of spatial reasoning (topological, distance-based, or relational composition), and what does this reveal about their spatial reasoning capabilities?
- Basis in paper: [inferred] The paper focuses specifically on cardinal directions but acknowledges that spatial reasoning encompasses many aspects, suggesting a need for broader comparison.
- Why unresolved: The paper only tests cardinal direction reasoning without comparing to other spatial reasoning domains or creating a comprehensive spatial reasoning benchmark.
- What evidence would resolve it: Systematic comparison of LLM performance across multiple spatial reasoning domains using standardized benchmarks, identifying which types of spatial reasoning LLMs handle best and worst.

## Limitations

- Dataset Construction Bias: The templated scenarios may not fully capture real-world spatial reasoning challenges, creating an artificial testing environment that may not generalize.
- Temperature Setting Artifacts: Using temperature=0 may not reflect typical LLM usage scenarios, and the reported temperature sensitivity suggests performance could vary significantly in practical applications.
- Evaluation Rubric Constraints: The strict case-insensitive string comparison approach may miss semantic understanding and the 3.82% non-compliance rate suggests some models struggle with rubric requirements.

## Confidence

- High Confidence: The observation that LLMs perform significantly better on recall-based spatial knowledge than on reasoning-based spatial tasks is well-supported by the substantial accuracy gap between simple (0.8+) and complex (0.595) datasets.
- Medium Confidence: The temperature sensitivity findings are supported by experimental data, but optimal temperature settings and practical implications require further investigation.
- Low Confidence: Claims about underlying architectural reasons for spatial reasoning limitations remain speculative, as the paper identifies performance differences without definitively establishing their sources.

## Next Checks

1. **Cross-dataset Generalization Test**: Apply the same LLMs to a manually curated set of spatial reasoning questions that are not templated, to assess whether the performance gap persists when moving beyond the structured test environment.

2. **Chain-of-Thought Prompting Experiment**: Implement explicit reasoning step prompts (e.g., "First, determine your initial direction, then consider the turn...") to test whether the models can improve spatial reasoning performance when guided through intermediate reasoning steps.

3. **Balanced Directional Bias Analysis**: Create a test set with equal representation of all directional pairs and measure whether the observed directional asymmetries persist, helping to distinguish between genuine model limitations versus artifacts of the original dataset construction.