---
ver: rpa2
title: Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect
  of Epistemic Markers on LLM-based Evaluation
arxiv_id: '2410.20774'
source_url: https://arxiv.org/abs/2410.20774
tags:
- epistemic
- markers
- output
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMBER, a benchmark to assess LLM judges'
  robustness to epistemic markers in outputs. Experiments with five state-of-the-art
  LLMs show all exhibit negative biases against epistemic markers, especially those
  expressing uncertainty, with larger models showing reduced sensitivity.
---

# Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation

## Quick Facts
- arXiv ID: 2410.20774
- Source URL: https://arxiv.org/abs/2410.20774
- Reference count: 40
- Key outcome: LLM judges exhibit significant negative biases against epistemic markers, especially those expressing uncertainty, while human judges remain unaffected.

## Executive Summary
This paper introduces EMBER, a benchmark to assess LLM judges' robustness to epistemic markers in outputs. Experiments with five state-of-the-art LLMs show all exhibit negative biases against epistemic markers, especially those expressing uncertainty, with larger models showing reduced sensitivity. Human judges, in contrast, are unaffected by such markers, suggesting LLM judges may not mimic human judgment accurately in their presence. These biases can significantly impact model rankings in pairwise comparisons, penalizing models that use uncertainty expressions, and highlighting the need for more robust evaluation methods.

## Method Summary
The study evaluates five LLMs (GPT-3.5-turbo, GPT-4-turbo, GPT-4o, Llama-3-8B-Instruct, Llama-3-70B-Instruct) on the EMBER benchmark, which includes EMBERQA (2,000 QA instances) and EMBERIF (823 instruction-following pairwise instances) with epistemic markers. The evaluation measures accuracy against human judgments and Verdict Switch Rate (VSR) to quantify sensitivity to epistemic markers. Epistemic markers were sampled from real LLM usage patterns, focusing on the top 20 Strengtheners and Weakeners by frequency.

## Key Results
- All five LLM judges show significant accuracy drops when epistemic markers are present, especially Weakeners.
- Larger models (Llama-3-70B) show reduced sensitivity to epistemic markers compared to smaller models (Llama-3-8B).
- Human judges maintain consistent accuracy regardless of epistemic marker presence, unlike LLM judges.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM judges exhibit bias against epistemic markers, especially Weakeners, due to sensitivity to linguistic cues over content correctness.
- Mechanism: LLM judges prioritize the surface-level presence of epistemic markers (which convey uncertainty) over the actual correctness of the content. Weakeners like "I'm not sure" are interpreted as a sign of lower quality or incorrectness, leading to lower evaluation scores even when the output is factually correct.
- Core assumption: LLM judges process epistemic markers as signals of content quality rather than expressions of the model's confidence level.
- Evidence anchors:
  - [abstract] "Experiments with five state-of-the-art LLMs show all exhibit negative biases against epistemic markers, especially those expressing uncertainty"
  - [section 4.2.1] "a drop in accuracy for QAS and QAW suggests a bias against epistemic markers, causing LLM-judges to misclassify outputs as incorrect"
- Break condition: If LLM judges are retrained or fine-tuned to focus on content correctness rather than surface linguistic cues, the bias against epistemic markers would diminish.

### Mechanism 2
- Claim: LLM judges are sensitive to epistemic markers, and this sensitivity decreases with model size.
- Mechanism: Larger LLM judges have more robust representations of context and content, allowing them to focus more on the correctness of the output and less on the presence of epistemic markers. Smaller models are more easily influenced by these markers, leading to higher bias.
- Core assumption: Model capacity correlates with the ability to distinguish between content quality and linguistic expressions of uncertainty.
- Evidence anchors:
  - [abstract] "larger models showing reduced sensitivity"
  - [section 4.2.1] "as the capacity of the LLM-judges increases, their robustness against epistemic markers improves"
- Break condition: If the evaluation task is simplified or the epistemic markers are explicitly flagged, even smaller models might show reduced sensitivity.

### Mechanism 3
- Claim: Human judges are not biased against epistemic markers, unlike LLM judges.
- Mechanism: Human judges prioritize the factual correctness of the output over the presence of epistemic markers, focusing on the content rather than the linguistic expression of uncertainty. This suggests that LLM judges do not accurately mimic human judgment in the presence of epistemic markers.
- Core assumption: Human evaluation is based on content correctness, while LLM evaluation is influenced by linguistic cues.
- Evidence anchors:
  - [abstract] "Human judges, in contrast, are unaffected by such markers, suggesting LLM judges may not mimic human judgment accurately in their presence"
  - [section 5.1] "Accuracy in QAS and QAN groups is identical (87.3), indicating that human-judges are unaffected by Strengtheners"
- Break condition: If LLM judges are trained on human-annotated data that explicitly de-emphasizes the importance of epistemic markers, they might better mimic human judgment.

## Foundational Learning

- Concept: Epistemic markers
  - Why needed here: Understanding epistemic markers is crucial for grasping the core issue of LLM judges' bias. Epistemic markers are linguistic expressions that convey the speaker's level of certainty or uncertainty, and their presence can influence LLM judges' evaluations.
  - Quick check question: What is the difference between Strengtheners and Weakeners in the context of epistemic markers?

- Concept: Robustness in evaluation
  - Why needed here: Robustness refers to the ability of LLM judges to provide consistent and accurate evaluations regardless of the presence of epistemic markers. Understanding robustness is key to assessing the limitations of current LLM evaluation methods.
  - Quick check question: How does the presence of epistemic markers affect the robustness of LLM judges?

- Concept: Human vs. LLM evaluation
  - Why needed here: Comparing human and LLM evaluation is essential for understanding the limitations of LLM judges. Human judges are not biased against epistemic markers, while LLM judges are, highlighting a key difference in their evaluation processes.
  - Quick check question: Why might LLM judges be more biased against epistemic markers than human judges?

## Architecture Onboarding

- Component map: EMBER benchmark (EMBERQA, EMBERIF) -> LLM judges (GPT-3.5-turbo, GPT-4-turbo, GPT-4o, Llama-3-8B-Instruct, Llama-3-70B-Instruct) -> Human judges -> Epistemic markers (Strengtheners, Weakeners)
- Critical path: Construct EMBER benchmark → Evaluate LLM judges on EMBER → Compare LLM judge performance with human judge performance → Analyze bias patterns → Investigate real-life implications.
- Design tradeoffs: Using LLM judges for evaluation offers scalability and efficiency but may introduce biases against epistemic markers. Human judges are less biased but are more time-consuming and resource-intensive.
- Failure signatures: LLM judges consistently misclassify outputs as incorrect when epistemic markers are present, even when the content is factually correct. This leads to unfair penalization of models that use epistemic markers.
- First 3 experiments:
  1. Evaluate LLM judges on EMBERQA and EMBERIF to quantify the bias against epistemic markers.
  2. Compare the performance of different LLM judges (different sizes and architectures) to assess the impact of model capacity on bias.
  3. Conduct human evaluation on a subset of EMBER to confirm that human judges are not biased against epistemic markers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM judges exhibit the same biases against epistemic markers across different types of evaluation tasks beyond QA and instruction following?
- Basis in paper: [inferred] The paper notes limitations in focusing only on QA and instruction following tasks, suggesting potential differences in other evaluation contexts.
- Why unresolved: The study's scope was limited to these two task types, leaving uncertainty about whether the observed biases generalize to other evaluation scenarios.
- What evidence would resolve it: Empirical testing of LLM judges' robustness to epistemic markers across diverse evaluation tasks such as dialogue response evaluation, code generation assessment, or creative writing evaluation.

### Open Question 2
- Question: How do epistemic markers in non-English languages affect the performance of LLM judges compared to their English counterparts?
- Basis in paper: [explicit] The paper explicitly states it conducted experiments only in English and did not explore multilingual or cross-linguistic evaluations.
- Why unresolved: The study was monolingual, leaving open the question of whether linguistic and cultural differences in epistemic marker usage across languages influence LLM judge behavior.
- What evidence would resolve it: Comparative experiments testing LLM judges on epistemic marker robustness across multiple languages, including languages with different epistemic expression norms.

### Open Question 3
- Question: What is the impact of less frequently used epistemic markers on LLM judge robustness, and do they elicit different bias patterns than the top 20 most common markers?
- Basis in paper: [explicit] The paper acknowledges using only the top 20 most frequently generated Strengtheners and Weakeners, leaving analysis of less common markers unexplored.
- Why unresolved: The study focused on commonly occurring markers, potentially missing nuances in how rarer epistemic expressions affect judge behavior.
- What evidence would resolve it: Systematic evaluation of LLM judge performance across a broader range of epistemic markers, including rare and domain-specific expressions, to identify any differential bias patterns.

## Limitations

- The EMBER benchmark construction relies on automated sampling of epistemic markers without specifying exact distribution weights, creating potential sampling bias.
- The study focuses on five specific LLM judges, leaving open questions about whether these patterns generalize across different model families.
- The analysis treats epistemic markers as uniform perturbations without accounting for context-dependent appropriateness of uncertainty expressions.

## Confidence

- High confidence in core finding that LLM judges exhibit negative biases against epistemic markers
- Medium confidence in claim that larger models show reduced sensitivity
- Low confidence in specific mechanism that LLM judges prioritize surface linguistic cues over content correctness

## Next Checks

1. Reconstruct the epistemic marker sampling process using the Zhou et al. dataset to verify the distribution matches the paper's assumptions and test sensitivity to different sampling strategies.
2. Conduct ablation studies removing different epistemic marker types to isolate which linguistic features drive the largest performance drops in LLM judges.
3. Test whether fine-tuning LLM judges on EMBER data with explicit de-biasing objectives reduces the observed sensitivity to epistemic markers while maintaining overall evaluation quality.