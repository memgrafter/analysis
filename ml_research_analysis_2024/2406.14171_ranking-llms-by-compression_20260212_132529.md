---
ver: rpa2
title: Ranking LLMs by compression
arxiv_id: '2406.14171'
source_url: https://arxiv.org/abs/2406.14171
tags:
- compression
- language
- data
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to rank large language models (LLMs)
  using lossless data compression as a general evaluation metric. It establishes the
  equivalence between compression length under arithmetic coding and cumulative negative
  log probabilities of LLMs, demonstrating that pre-training essentially learns optimal
  coding lengths.
---

# Ranking LLMs by compression

## Quick Facts
- arXiv ID: 2406.14171
- Source URL: https://arxiv.org/abs/2406.14171
- Authors: Peijia Guo; Ziguang Li; Haibo Hu; Chao Huang; Ming Li; Rui Zhang
- Reference count: 9
- Primary result: Compression ratio positively correlates with LLM performance across NLP tasks, validating it as a general evaluation metric

## Executive Summary
This paper introduces a novel method to rank large language models using lossless data compression as a general evaluation metric. The authors establish the mathematical equivalence between compression length under arithmetic coding and cumulative negative log probabilities of LLMs, demonstrating that pre-training essentially learns optimal coding lengths. This approach allows evaluation of model generalization without actual compression, significantly reducing computational overhead while providing a unified framework for evaluating LLMs across diverse tasks.

## Method Summary
The method calculates compression ratios by summing negative log probabilities from LLMs without performing actual arithmetic coding. Five LLMs (LLaMA 2 7B, Mistral 7B, OPT-IML 1.3B, GPT-2-XL 1.5B, GPT-2 774M) are evaluated on Text8 dataset for compression ratio and three NLP tasks: sentence completion (HellaSwag), question answering (BoolQ), and coreference resolution (Winograd Schema Challenge). The compression ratio is computed as original text length divided by compressed text length, with the equivalence between compression length and negative log probability sums proven mathematically.

## Key Results
- Compression ratio and model performance are positively correlated across all three NLP tasks tested
- LLaMA 2 7B achieved highest sentence completion accuracy (81.3%) and coreference resolution accuracy (73.3%)
- Mistral 7B achieved highest question answering accuracy (77.4%)
- The method successfully ranks LLMs without requiring actual compression, saving computational resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compression ratio is mathematically equivalent to negative log probability, which is the loss minimized during LLM pre-training.
- Mechanism: The paper proves that arithmetic coding length equals the sum of negative log probabilities of the LLM. This sum is the cross-entropy between the true data distribution and the model's predicted distribution, which is minimized during pre-training.
- Core assumption: The true data distribution Q is stationary and the model's probability distribution P can approximate it well enough for compression to be meaningful.
- Evidence anchors:
  - [abstract] "We demonstrate the equivalence of compression length under arithmetic coding with cumulative negative log probabilities when using a large language model as a prior"
  - [section] "In order to obtain the distribution for P1, add an EOS (End of Sentence) token at the beginning of the text as t0. For each token ti, the associated Pi acts as the entropy model"
  - [corpus] Weak: corpus papers discuss compression but don't establish this equivalence specifically for LLMs
- Break condition: If the model's probability distribution P significantly diverges from the true distribution Q, the compression ratio will no longer correlate with model quality.

### Mechanism 2
- Claim: Better compression indicates stronger language understanding and generalization capabilities.
- Mechanism: When a model can compress data more effectively, it means it has learned to identify and exploit statistical patterns in the data, which is equivalent to understanding the underlying structure of language.
- Core assumption: The patterns that enable good compression are the same patterns that enable good performance on downstream NLP tasks.
- Evidence anchors:
  - [abstract] "Experimental results show that compression ratio and model performance are positively correlated, so it can be used as a general metric to evaluate large language models"
  - [section] "From the above experiments, it can be concluded that: the better data compression effect of LLM, the better its performance in natural language processing tasks"
  - [corpus] Weak: corpus papers discuss compression but don't specifically link compression to language understanding
- Break condition: If a model memorizes training data without learning general patterns, it might achieve good compression on training data but poor generalization to new tasks.

### Mechanism 3
- Claim: The compression ratio can be computed without actual compression, saving computational overhead.
- Mechanism: Instead of performing arithmetic coding to compress data, we can directly sum the negative log probabilities predicted by the LLM for each token, which gives the same value as the compression length.
- Core assumption: The probability predictions from the LLM are accurate enough that summing negative log probabilities gives a meaningful approximation of compression length.
- Evidence anchors:
  - [abstract] "At the same time, the evaluation metric compression ratio can be obtained without actual compression, which greatly saves overhead"
  - [section] "This reveals a direct way to approximate the compression length without having to perform the compression method exactly"
  - [corpus] Weak: corpus papers discuss compression methods but don't address this computational shortcut specifically
- Break condition: If the LLM's probability predictions are systematically biased or inaccurate, the approximation of compression length will be unreliable.

## Foundational Learning

- Concept: Information theory and entropy
  - Why needed here: The paper relies on Shannon's fundamental theorem of coding and the concept of entropy as a lower bound for lossless compression
  - Quick check question: What is the relationship between entropy and the minimum number of bits needed to encode a message?

- Concept: Arithmetic coding
  - Why needed here: The paper uses arithmetic coding as the compression method and proves its equivalence to negative log probability
  - Quick check question: How does arithmetic coding differ from Huffman coding in terms of encoding efficiency?

- Concept: Cross-entropy and Kullback-Leibler divergence
  - Why needed here: The paper uses these concepts to establish the equivalence between model pre-training goals and compression length
  - Quick check question: Why is minimizing cross-entropy equivalent to making the model's predicted distribution closer to the true distribution?

## Architecture Onboarding

- Component map:
  - Tokenizer -> LLM -> Probability Calculator -> Compression Ratio Calculator -> Evaluation Pipeline

- Critical path:
  1. Load text data and tokenize
  2. Pass tokens through LLM to get probability distributions
  3. Calculate negative log probabilities and sum for compression ratio
  4. Evaluate model performance on downstream tasks
  5. Correlate compression ratio with task performance

- Design tradeoffs:
  - Using pre-trained models vs. fine-tuning on specific tasks
  - Choosing between different tokenization schemes
  - Balancing computational cost of probability calculation vs. accuracy

- Failure signatures:
  - Compression ratio doesn't correlate with task performance (model has learned patterns that don't generalize)
  - Extremely high compression ratios with poor task performance (model may be overfitting)
  - Inconsistent results across different datasets

- First 3 experiments:
  1. Calculate compression ratios for all five LLMs on the Text8 dataset and verify the order matches expectations
  2. Run the same five LLMs on one downstream task (e.g., sentence completion) and check for positive correlation with compression ratio
  3. Compare results using different tokenization schemes to see impact on compression ratio and task performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several open questions emerge from the work.

## Limitations
- The paper focuses exclusively on text-based compression and NLP tasks, not addressing other data modalities
- Limited sample size (5 models Ã— 3 tasks) may not fully establish generalizability of the compression ratio metric
- Implementation details for adaptive arithmetic coding integration with different LLMs are not fully specified

## Confidence

**High Confidence**:
- The mathematical equivalence between arithmetic coding length and negative log probability sums
- The computational shortcut of using negative log probability sums instead of actual compression

**Medium Confidence**:
- The positive correlation between compression ratio and task performance
- The claim that better compression indicates stronger language understanding

**Low Confidence**:
- Generalizability across diverse model architectures and tasks
- The method's effectiveness as a universal evaluation metric

## Next Checks
1. **Cross-Architecture Validation**: Test the correlation between compression ratio and task performance across a broader range of model architectures (including smaller models, different pretraining objectives) and task types beyond the three tested.

2. **Implementation Verification**: Replicate the exact arithmetic coding implementation with probability extraction from at least two different LLM families to verify the claimed computational shortcut produces consistent results.

3. **Generalization Test**: Apply the method to models with known overfitting issues (high training performance but poor generalization) to verify whether compression ratio correctly identifies such cases.