---
ver: rpa2
title: 'Unified Explanations in Machine Learning Models: A Perturbation Approach'
arxiv_id: '2405.20200'
source_url: https://arxiv.org/abs/2405.20200
tags:
- feature
- similarity
- shap
- where
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes a systematic approach to quantify and compare\
  \ model explanations under static and dynamic scenarios. It introduces a perturbation\
  \ algorithm that tests model sensitivity to feature changes and two metrics\u2014\
  Absolute Normalized Shapley (ANS) and Absolute Normalized Weighted Average (ANWA)\u2014\
  to measure feature importance."
---

# Unified Explanations in Machine Learning Models: A Perturbation Approach

## Quick Facts
- arXiv ID: 2405.20200
- Source URL: https://arxiv.org/abs/2405.20200
- Reference count: 34
- The paper proposes a systematic approach to quantify and compare model explanations under static and dynamic scenarios using a perturbation algorithm and two metrics—ANS and ANWA.

## Executive Summary
This paper addresses the challenge of comparing feature importance explanations across static (Shapley values) and dynamic (perturbation-based) scenarios in machine learning models. The authors introduce a perturbation algorithm that tests model sensitivity to feature changes and propose two metrics—Absolute Normalized Shapley (ANS) and Absolute Normalized Weighted Average (ANWA)—to measure feature importance. By evaluating six classifiers on five datasets, the study demonstrates that cosine similarity between static and dynamic explanations ranges from 0.79 to 0.84 overall, with lower-dimensional datasets showing stronger alignment. Jaccard similarity for top-ranked features exceeds 0.5 for k=10 across models, suggesting reasonable consistency in identifying key features.

## Method Summary
The method trains six classifiers on five datasets, computes Shapley values for static explanations, and applies a perturbation algorithm for dynamic explanations. The perturbation algorithm systematically modifies features using scaling factors and computes feature importance via ANWA, which weights perturbations by their proximity to the baseline. The study compares static and dynamic explanations using cosine similarity (measuring vector alignment) and Jaccard similarity (measuring top-k feature agreement). Results are evaluated across multiple performance metrics (accuracy, precision, recall, F1) to assess robustness.

## Key Results
- Cosine similarity between static and dynamic explanations ranges from 0.79 to 0.84 overall
- Jaccard similarity for top-ranked features exceeds 0.5 for k=10 across models
- Lower-dimensional datasets show stronger alignment between static and dynamic explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cosine similarity between static and dynamic explanations reliably quantifies consistency.
- Mechanism: By treating Shapley-derived feature importances (static) and perturbation-weighted averages (dynamic) as vectors, cosine similarity measures angular alignment in feature contribution space.
- Core assumption: Feature importance rankings preserve their directional relationship across static and dynamic scenarios when the model's reasoning remains stable.
- Evidence anchors:
  - [abstract]: "Cosine similarity between static and dynamic explanations ranges from 0.79 to 0.84 overall"
  - [section]: "Let ⃗i be a vector corresponding to the RFIs computed using EQ 4, and let ⃗j be a vector containing the proportion of summed absolute Shap values against the whole population"
  - [corpus]: Weak - no direct citations to cosine similarity in related papers; only mentions "EXACT: Towards a platform for empirically benchmarking Machine Learning model explanation methods" without specific mechanism overlap.
- Break condition: If the model's decision boundary shifts dramatically under perturbation, vector directions diverge and cosine similarity drops below 0.7, indicating instability.

### Mechanism 2
- Claim: Jaccard similarity captures top-k feature agreement better than full vector similarity in high-stakes domains.
- Mechanism: By comparing the intersection over union of top-ranked features from Shapley and perturbation methods, Jaccard isolates whether the most influential features are consistently identified regardless of magnitude differences.
- Core assumption: In regulated applications, the identity of top features matters more than their exact importance scores.
- Evidence anchors:
  - [abstract]: "Jaccard similarity for top-ranked features exceeds 0.5 for k=10 across models"
  - [section]: "Jaccard Similarity (JS) is a popular metric when looking at top-k performance commonly used to measure efficacy in Recommender System"
  - [corpus]: Weak - related papers mention benchmarking and perturbation but not specifically Jaccard-based top-k alignment.
- Break condition: If top-k sets differ significantly (JS < 0.3), it signals that the model relies on different features in static vs. dynamic contexts, undermining trust.

### Mechanism 3
- Claim: Absolute Normalized Weighted Average (ANWA) emphasizes perturbations near the baseline, making it more sensitive to small but meaningful feature changes.
- Mechanism: Weights are inversely related to perturbation magnitude (w(pi) = 1 - |1 - pi|), so predictions close to the original data have greater influence on the final importance score.
- Core assumption: Small perturbations better reflect realistic data drift while large perturbations risk unrealistic out-of-distribution samples.
- Evidence anchors:
  - [section]: "The term under the summation in the numerator wi = (1 − | 1 − pi|) places more weight on perturbations closer to the unperturbed base case"
  - [section]: "We show an example of this in Table 1" demonstrating weight decay as pi moves away from 1.
  - [corpus]: Weak - no direct evidence of this weighting scheme in related papers; appears novel.
- Break condition: If the model is highly non-linear, even small perturbations can cause large output swings, invalidating the weighting assumption.

## Foundational Learning

- Concept: Shapley values and cooperative game theory
  - Why needed here: The paper uses Shapley values as the static baseline for feature importance, so understanding marginal contribution and coalition value is essential.
  - Quick check question: Why do Shapley values require evaluating all possible feature subsets, and how does this relate to feature interaction?

- Concept: Perturbation-based sensitivity analysis
  - Why needed here: The dynamic component relies on systematically modifying features and observing output changes; understanding how to design perturbations without causing distribution shift is critical.
  - Quick check question: What is the difference between evasion attacks and the perturbation method used here, and why does the paper claim they are analogous?

- Concept: Distance metrics for vector comparison (cosine vs. Jaccard)
  - Why needed here: The paper uses both cosine similarity (magnitude-aware) and Jaccard similarity (rank-aware); knowing when each is appropriate guides interpretation.
  - Quick check question: In what scenario would two feature importance vectors have high cosine similarity but low Jaccard similarity?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Static explanation (Shap) -> Dynamic perturbation (ANWA) -> Similarity computation (cosine/Jaccard) -> Evaluation
- Critical path:
  1. Train model on training split
  2. Generate Shapley values for test set
  3. Perturb test features across P values
  4. Compute ANWA and RFI
  5. Compare using cosine and Jaccard
- Design tradeoffs:
  - Shapley computation is expensive but interpretable; perturbation is cheaper but depends on metric choice
  - Cosine captures full vector alignment; Jaccard ignores magnitude but focuses on top features
  - Using accuracy vs. precision/recall changes sensitivity to class imbalance
- Failure signatures:
  - Low cosine similarity (<0.7) -> model explanations inconsistent across static/dynamic
  - High Jaccard but low cosine -> top features agree but overall importance distribution differs
  - ANWA variance high across perturbations -> instability in feature sensitivity
- First 3 experiments:
  1. Run Shap-only on Iris dataset with logistic regression; verify top features match literature
  2. Apply Algorithm 1 with P = [0.9, 1.0, 1.1] on same model; check if ANWA aligns with Shap
  3. Compare cosine and Jaccard outputs for k=3; document any divergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of performance metric (accuracy, precision, recall, F1) affect the alignment between static and dynamic explanations across different model types?
- Basis in paper: [explicit] The paper evaluates similarity between static and dynamic explanations using multiple metrics (accuracy, precision, recall, F1) and notes marginal impact on outcomes except in a few cases.
- Why unresolved: The paper mentions that the choice of metric has a marginal impact on the outcome, but does not deeply analyze why certain metrics perform better for specific models or datasets.
- What evidence would resolve it: Systematic experiments comparing metric-specific alignments across diverse model types and datasets, identifying patterns in metric performance.

### Open Question 2
- Question: What is the relationship between dataset size and explanation stability in dynamic scenarios?
- Basis in paper: [explicit] The paper conducts a subsample analysis on the Census dataset showing a trend toward unified explanations as sample size grows, but notes profound variation in similarities.
- Why unresolved: The paper observes a trend but finds insufficient evidence for a purely linear relationship, suggesting more complex dynamics at play.
- What evidence would resolve it: Comprehensive experiments across multiple datasets with varying sizes, analyzing non-linear relationships and identifying thresholds for explanation stability.

### Open Question 3
- Question: How does feature type (continuous vs. categorical) influence the similarity between static and dynamic explanations?
- Basis in paper: [inferred] The paper mentions that datasets with only continuous features show better alignment between static and dynamic explanations compared to those with mixed feature types.
- Why unresolved: The paper does not explore the underlying reasons for this observation or quantify the impact of different feature types on explanation similarity.
- What evidence would resolve it: Experiments isolating the effect of feature types on explanation similarity, potentially revealing mechanisms behind observed patterns.

### Open Question 4
- Question: Can the perturbation-based framework be effectively extended to unstructured data (e.g., text, images, graphs)?
- Basis in paper: [explicit] The paper suggests potential relevance to non-tabular, unstructured data and calls for further exploration.
- Why unresolved: The paper focuses on tabular data and does not implement or test the framework on unstructured data types.
- What evidence would resolve it: Successful implementation and validation of the framework on various unstructured data types, demonstrating consistent explanation similarity measures.

## Limitations
- The study's confidence in cross-model consistency is limited by the relatively small sample of datasets and classifiers tested.
- The perturbation algorithm's reliance on specific distance metrics may not capture all aspects of explanation stability, particularly for non-linear models.
- The weighting scheme in ANWA assumes linear sensitivity decay, which may not hold for complex models.

## Confidence
- High confidence: The core methodology for comparing static vs. dynamic explanations using cosine similarity is sound and well-defined.
- Medium confidence: The claim that ANWA is more sensitive to realistic perturbations than Shapley values is supported but could benefit from additional validation.
- Low confidence: The generalization of findings to all machine learning models and production scenarios, given the limited scope of tested models and datasets.

## Next Checks
1. Test the perturbation algorithm on additional datasets with different dimensionalities and class distributions to verify the robustness of the cosine similarity range (0.79-0.84).
2. Evaluate the sensitivity of Jaccard similarity to different k values (k=5, k=15) to determine the optimal threshold for top-k feature agreement.
3. Implement a stress test where perturbations are designed to simulate realistic data drift scenarios, measuring how quickly explanation stability degrades under controlled shifts.