---
ver: rpa2
title: Are LLMs Good Zero-Shot Fallacy Classifiers?
arxiv_id: '2410.15050'
source_url: https://arxiv.org/abs/2410.15050
tags:
- fallacy
- llms
- zero-shot
- prompting
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the potential of large language models (LLMs)
  for zero-shot fallacy classification. The authors propose diverse single-round and
  multi-round prompting schemes, including definition generation, general fallacy
  analysis, and chain-of-thought reasoning, to elicit LLMs' inherent fallacy-related
  knowledge.
---

# Are LLMs Good Zero-Shot Fallacy Classifiers?
## Quick Facts
- arXiv ID: 2410.15050
- Source URL: https://arxiv.org/abs/2410.15050
- Authors: Fengjun Pan; Xiaobao Wu; Zongrui Li; Anh Tuan Luu
- Reference count: 22
- Primary result: Zero-shot LLMs achieve competitive performance on fallacy classification, especially with multi-round prompting schemes

## Executive Summary
This paper investigates the capability of large language models to classify logical fallacies without task-specific training, using zero-shot prompting approaches. The authors develop multiple prompting strategies including definition generation, general fallacy analysis, and chain-of-thought reasoning to elicit LLMs' inherent logical reasoning capabilities. Their experiments demonstrate that zero-shot LLMs can achieve performance comparable to fine-tuned models on benchmark datasets, with particular strength in out-of-distribution and open-domain tasks.

The study highlights the effectiveness of multi-round prompting schemes, which show consistent improvements over single-round approaches, especially for smaller LLMs. The findings suggest that LLMs possess substantial latent knowledge about logical fallacies that can be accessed through appropriate prompting, potentially reducing the need for extensive fine-tuning in fallacy detection applications.

## Method Summary
The authors propose diverse prompting schemes to elicit LLMs' inherent fallacy-related knowledge for zero-shot classification. They develop single-round and multi-round prompting strategies, including definition generation where the model first generates fallacy definitions, general fallacy analysis where the model reasons about argument structures, and chain-of-thought reasoning approaches. The experiments evaluate these approaches on benchmark fallacy classification datasets, comparing zero-shot performance against full-shot fine-tuned baselines. The study systematically examines how different prompt structures and multi-round interactions affect performance across various LLM sizes and task types.

## Key Results
- Zero-shot LLMs achieve competitive performance compared to full-shot fine-tuned baselines on benchmark datasets
- Multi-round prompting schemes consistently improve performance, particularly for smaller LLMs
- Zero-shot models show strong performance on out-of-distribution and open-domain fallacy classification tasks

## Why This Works (Mechanism)
The effectiveness of zero-shot fallacy classification stems from LLMs' exposure to diverse textual content during pretraining, which includes logical arguments, reasoning patterns, and discussions of fallacies. When properly prompted, these models can leverage their latent knowledge of logical structures and common reasoning errors. Multi-round prompting enhances this by allowing iterative refinement of the model's understanding, enabling it to generate definitions, analyze argument structures, and apply logical reasoning in a stepwise manner. This process helps overcome the limitations of single-shot prompts by giving the model opportunities to self-correct and deepen its analysis of fallacy detection.

## Foundational Learning
- **Logical fallacy taxonomy** - Understanding different types of logical fallacies (ad hominem, straw man, etc.) is essential for evaluating model performance and designing appropriate prompts. Quick check: Can you list and define 5 common fallacy types?
- **Zero-shot learning principles** - The concept of performing tasks without task-specific training data is fundamental to this work. Quick check: What distinguishes zero-shot from few-shot learning?
- **Prompt engineering techniques** - Knowledge of how prompt structure affects model outputs is critical for implementing the proposed methods. Quick check: How do different prompt formats (instruction, few-shot, chain-of-thought) impact model performance?
- **Chain-of-thought reasoning** - Understanding how breaking down reasoning into intermediate steps improves complex reasoning tasks. Quick check: Why does chain-of-thought prompting often improve performance on reasoning tasks?
- **Out-of-distribution generalization** - The ability to perform well on data unlike training data is key to evaluating the true capability of zero-shot approaches. Quick check: What metrics best capture OOD generalization performance?

## Architecture Onboarding
- **Component map**: Task Input -> Prompt Generator -> LLM Core -> Output Parser -> Classification Result
- **Critical path**: Input text → Prompt formulation → Model reasoning → Response parsing → Final classification
- **Design tradeoffs**: Single-round vs multi-round prompting (simplicity vs performance), prompt complexity vs computational cost, model size vs zero-shot capability
- **Failure signatures**: Incorrect fallacy identification, overly generic responses, failure to recognize complex fallacy combinations, inconsistent reasoning across similar inputs
- **3 first experiments**: 1) Test basic zero-shot classification with simple prompts on a small dataset, 2) Compare single-round vs multi-round prompting performance, 3) Evaluate cross-dataset generalization by testing on out-of-distribution fallacy types

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on English-language datasets, raising questions about cross-linguistic generalizability
- Performance gap still exists between zero-shot and fine-tuned models, which may be significant for critical applications
- Prompting strategies explored may not exhaustively cover all approaches to elicit logical reasoning from LLMs

## Confidence
- **LLMs as effective zero-shot fallacy classifiers** - Medium confidence: Results show competitive performance, but extent of understanding vs pattern matching unclear
- **Multi-round prompting superiority** - High confidence: Consistent improvement across experiments and model sizes supports this finding
- **Generalizability to out-of-distribution tasks** - Medium confidence: Results are promising but based on limited dataset variations

## Next Checks
1. Conduct cross-linguistic evaluation using fallacy datasets in multiple languages to assess model generalizability beyond English
2. Implement ablation studies on prompt components to isolate which elements most contribute to performance improvements
3. Test on additional out-of-distribution fallacy types not present in any training data to better evaluate true zero-shot capabilities