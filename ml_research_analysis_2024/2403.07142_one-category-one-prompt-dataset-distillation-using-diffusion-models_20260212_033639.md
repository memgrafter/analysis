---
ver: rpa2
title: 'One Category One Prompt: Dataset Distillation using Diffusion Models'
arxiv_id: '2403.07142'
source_url: https://arxiv.org/abs/2403.07142
tags:
- dataset
- distillation
- images
- diffusion
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dataset distillation compresses large datasets into a small synthetic
  set that trains models as well as the full dataset. Prior methods scale poorly with
  high-resolution images and complex architectures due to bi-level optimization challenges.
---

# One Category One Prompt: Dataset Distillation using Diffusion Models

## Quick Facts
- arXiv ID: 2403.07142
- Source URL: https://arxiv.org/abs/2403.07142
- Reference count: 40
- One-line primary result: Dataset distillation using diffusion models achieves state-of-the-art performance on ImageNet-1k with as few as 1–10 images per category.

## Executive Summary
Dataset distillation compresses large datasets into small synthetic sets that train models as well as the full dataset. Prior methods scale poorly with high-resolution images and complex architectures due to bi-level optimization challenges. This paper proposes Dataset Distillation using Diffusion Models (D3M), which leverages latent diffusion models and textual inversion to condense entire image categories into single text prompts. For each category, D3M identifies informative patches, generates collages, and optimizes a prompt via textual inversion so the diffusion model produces realistic collage images on demand. D3M achieves state-of-the-art performance on ImageNet-1k with as few as 1–10 images per category, outperforming prior large-scale methods.

## Method Summary
D3M uses a teacher model to identify informative patches from training images, which are then assembled into collages. Textual inversion optimizes a single prompt per category so that a diffusion model generates these collage images from random seeds. Soft labels are generated for each patch and stored alongside the prompt and seed, enabling reconstruction of diverse, category-representative images with minimal storage overhead.

## Key Results
- Achieves state-of-the-art ImageNet-1k distillation performance with 1-10 synthetic images per category
- Outperforms prior large-scale methods while significantly reducing storage requirements
- Demonstrates improved cross-architecture generalization compared to existing distillation approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual inversion reduces per-category storage from N images to one prompt vector.
- Mechanism: Diffusion models take a text prompt as conditioning. Optimizing a single token per category enables generating diverse collage images without storing the images themselves.
- Core assumption: The learned prompt encodes sufficient information to reconstruct realistic and diverse images representative of the category.
- Evidence anchors:
  - [abstract] "leverages textual inversion... to create concise and informative representations for large datasets"
  - [section] "we designate a placeholder string, S*, to represent the new concept... we optimize this prompt via the following optimization problem"
  - [corpus] Weak - no direct neighbor cites this specific use of textual inversion for dataset distillation.
- Break condition: Prompt optimization fails to generate diverse or realistic images, leading to poor classifier performance.

### Mechanism 2
- Claim: Collage images from important patches improve both realism and diversity of synthetic data.
- Mechanism: Instead of raw images, the method first identifies the most informative patch per image and arranges them into collages. These collages are then used as targets for textual inversion, preserving the salient features of the category.
- Core assumption: The most informative patches contain the discriminative information needed for training accurate classifiers.
- Evidence anchors:
  - [section] "our goal in collage generation is to first identify an informative patch... to create collage images of these important patches"
  - [abstract] "identifies informative patches, generates collages, and optimizes a prompt via textual inversion"
  - [corpus] Weak - no direct neighbor cites patch selection for collage generation.
- Break condition: Patch selection fails to capture category-discriminative information, resulting in poor distillation performance.

### Mechanism 3
- Claim: Storing soft labels with seeds instead of images drastically reduces memory usage.
- Mechanism: For each category, the method stores the optimized prompt, a fixed random seed, and patch-level soft labels instead of full images. This allows regeneration of diverse collage images on demand with minimal storage overhead.
- Core assumption: Fixed seeds plus prompts can regenerate the same collage images, and soft labels provide richer supervision than one-hot labels.
- Evidence anchors:
  - [section] "possessing the random generator seed alongside ρ(v) uniquely identifies a collage image"
  - [abstract] "achieving state-of-the-art performance on ImageNet-1k with as few as 1–10 images per category"
  - [corpus] Weak - no direct neighbor cites seed-based image regeneration for distillation.
- Break condition: Soft label generation or storage becomes inaccurate or too large, negating the compression benefit.

## Foundational Learning

- Concept: Bi-level optimization in dataset distillation
  - Why needed here: The paper explicitly contrasts its uni-level approach with bi-level methods, showing why bi-level optimization struggles to scale.
  - Quick check question: What are the two optimization loops in bi-level dataset distillation, and why is the outer loop expensive?

- Concept: Diffusion models and denoising process
  - Why needed here: The core mechanism relies on generating collage images by reversing a forward diffusion process conditioned on text prompts.
  - Quick check question: How does a diffusion model generate an image from a random noise tensor and a text embedding?

- Concept: Textual inversion in text-to-image models
  - Why needed here: Textual inversion is used to optimize a learned token that, when paired with a base prompt, generates realistic collage images for each category.
  - Quick check question: What is being optimized in textual inversion, and how does it affect the generated image?

## Architecture Onboarding

- Component map:
  - Teacher model -> Patch selector -> Collage generator -> Textual inversion module -> Diffusion model -> Student model

- Critical path:
  1. Load teacher model
  2. Select patches → generate collages
  3. Optimize prompts via textual inversion
  4. Generate images + soft labels with seeds
  5. Train student model

- Design tradeoffs:
  - Uni-level vs. bi-level optimization: Simpler, more scalable, but may sacrifice some optimality
  - Collage vs. raw images: Better memory efficiency and diversity, but dependent on patch selection quality
  - Soft labels vs. one-hot: Richer supervision, but requires storing labels per patch

- Failure signatures:
  - Generated images are blurry or lack category features → check prompt optimization or diffusion model conditioning
  - Low accuracy despite high compression → check patch selection, soft label quality, or student training setup
  - Excessive memory usage → verify seed/label storage scaling with IPC

- First 3 experiments:
  1. Verify collage generation: Run patch selection and collage assembly on a small subset of ImageNet, inspect outputs
  2. Test textual inversion: Optimize prompt for one category, generate images, and compare to engineered prompt baseline
  3. Measure storage vs. accuracy: Generate synthetic data for a category, store as (prompt, seed, labels), and train a small classifier

## Open Questions the Paper Calls Out

- Concept: Cross-architecture generalization of distilled datasets
  - Why unresolved: The paper demonstrates that D3M-trained models can transfer to different architectures, but the underlying reasons and limitations of this generalization are not fully explored.
  - What evidence would resolve it: Systematic experiments comparing distillation performance across various teacher-student model pairs and analyzing the factors influencing successful transfer.

- Concept: Optimal patch size for collage generation
  - Why unresolved: The paper conducts an ablation study on patch sizes but doesn't investigate the relationship between patch size, dataset resolution, and resulting classification accuracy in depth.
  - What evidence would resolve it: Experiments varying patch sizes across a wider range of dataset resolutions and architectures, coupled with an analysis of information loss metrics.

## Limitations

- Lack of experimental details for faithful reproduction, including specific hyperparameters and training procedures
- Limited exploration of generalization to other datasets or architectures beyond ImageNet-1k
- No runtime or training stability analysis provided, leaving scalability questions unanswered

## Confidence

- High confidence: Claims about uni-level optimization being more scalable than bi-level methods
- Medium confidence: Claims about collage generation improving diversity and realism
- Low confidence: Claims about cross-architecture generalization

## Next Checks

1. Run a controlled ablation: Compare dataset distillation performance using raw images vs. collage images on a small subset of ImageNet to isolate the effect of collage generation.
2. Test prompt generalization: Generate synthetic data for a category using the optimized prompt, then train classifiers on two different architectures (e.g., ResNet and ConvNet) to measure cross-architecture generalization.
3. Measure storage overhead: Calculate the exact storage requirements (prompt + seed + soft labels) per category and compare to baseline methods to verify the claimed compression benefits.