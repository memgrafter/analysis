---
ver: rpa2
title: 'Poison-splat: Computation Cost Attack on 3D Gaussian Splatting'
arxiv_id: '2410.08190'
source_url: https://arxiv.org/abs/2410.08190
tags:
- attack
- gaussian
- gaussians
- training
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper exposes a previously overlooked vulnerability in 3D
  Gaussian splatting: the training computation cost can be maliciously increased by
  poisoning input images. The proposed Poison-splat attack formulates this as a bi-level
  optimization problem and solves it using three key strategies: approximating the
  attack objective via the number of Gaussians, employing a proxy model to maintain
  multi-view consistency, and optionally constraining perturbations for stealth.'
---

# Poison-splat: Computation Cost Attack on 3D Gaussian Splatting

## Quick Facts
- arXiv ID: 2410.08190
- Source URL: https://arxiv.org/abs/2410.08190
- Reference count: 40
- Key outcome: Poison-splat attack increases 3DGS training costs by up to 23x memory and 4.53x time through poisoned input images

## Executive Summary
This paper exposes a previously overlooked vulnerability in 3D Gaussian splatting: the training computation cost can be maliciously increased by poisoning input images. The proposed Poison-splat attack formulates this as a bi-level optimization problem and solves it using three key strategies: approximating the attack objective via the number of Gaussians, employing a proxy model to maintain multi-view consistency, and optionally constraining perturbations for stealth. Experiments on three datasets show that Poison-splat can increase GPU memory usage by up to 23x and training time by up to 4.53x under constrained attacks (ϵ = 16/255), with even greater impact under unconstrained settings. The attack remains effective against black-box systems and proves resistant to simple defenses like Gaussian limits or image smoothing. This work highlights a significant security risk in 3DGS systems and calls for more robust algorithms and defenses.

## Method Summary
Poison-splat is a computation cost attack on 3D Gaussian splatting that poisons training images to increase the number of 3D Gaussians generated during training. The attack uses bi-level optimization where the attacker maximizes computational cost while the victim minimizes reconstruction loss. Key innovations include using total variation score to approximate computational cost, employing a proxy model to maintain multi-view consistency across poisoned images, and constraining perturbations for stealth. The attack iteratively optimizes adversarial perturbations through gradient-based optimization, updating both the poisoned dataset and proxy model until the desired computational impact is achieved.

## Key Results
- Poison-splat increases GPU memory usage by up to 23x and training time by up to 4.53x under constrained attacks (ϵ = 16/255)
- The attack remains effective against black-box 3DGS implementations (Scaffold-GS) with 10.97x memory increase
- Simple defenses like Gaussian limits degrade reconstruction quality while image smoothing significantly reduces PSNR
- Constrained attacks (ϵ = 16/255) achieve 2.74x memory increase while maintaining stealth through small perturbations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Poisoning input images can drastically increase the number of 3D Gaussians used during training, leading to higher GPU memory usage and training time.
- **Mechanism:** The Poison-splat attack crafts adversarial perturbations that maximize the total variation score of rendered images. This promotes the generation of more complex, non-smooth textures, which the 3D Gaussian Splatting algorithm interprets as requiring more Gaussians to accurately reconstruct.
- **Core assumption:** There is a strong positive correlation between the number of Gaussians and computational costs (GPU memory and training time).
- **Evidence anchors:**
  - [abstract]: "the attack can increase GPU memory usage by up to 23x and training time by up to 4.53x under constrained attacks"
  - [section 2.1]: "3DGS tends to assign more Gaussians to those objects with more complex structures and non-smooth textures, as quantified by the total variation score"
  - [corpus]: Weak evidence; no direct mention of this specific mechanism in corpus neighbors.
- **Break condition:** If the 3D Gaussian Splatting algorithm is modified to limit the number of Gaussians or to ignore total variation score in its densification process.

### Mechanism 2
- **Claim:** The Poison-splat attack uses a proxy model to maintain multi-view consistency, enhancing the effectiveness of the attack.
- **Mechanism:** Instead of optimizing perturbations for each view independently, the attack uses a proxy 3DGS model to render images. This ensures that the poisoned images are consistent across different views, preventing the victim model from becoming confused by conflicting textures.
- **Core assumption:** Maintaining multi-view consistency is crucial for the success of the attack, as inconsistent views can lead to poor reconstruction and fewer Gaussians.
- **Evidence anchors:**
  - [section 3.2]: "Our solution is inspired by the view-consistent properties of the 3DGS model’s rendering function, which effectively maintains consistency across multi-view images generated from 3D Gaussian space."
  - [corpus]: No direct evidence in corpus neighbors; this is a novel aspect of the Poison-splat attack.
- **Break condition:** If the victim model is robust to inconsistent views or if the proxy model is not accurately simulating the victim's behavior.

### Mechanism 3
- **Claim:** The Poison-splat attack can be constrained to balance attack strength and stealthiness, making it harder to detect.
- **Mechanism:** By limiting the perturbation range (epsilon) of the adversarial perturbations, the attack can maintain the semantic integrity of the input images while still increasing computational costs. This makes the attack less obvious to human observers.
- **Core assumption:** Constraining the perturbation range does not significantly reduce the attack's effectiveness, and the victim model cannot easily distinguish between clean and poisoned images.
- **Evidence anchors:**
  - [abstract]: "the attack remains effective against black-box systems and proves resistant to simple defenses like Gaussian limits or image smoothing"
  - [section 3.2]: "By tuning ϵ, an attacker can balance the destructiveness and the stealthiness of the attack, allowing for strategic adjustments for the desired outcome."
  - [corpus]: No direct evidence in corpus neighbors; this is a novel aspect of the Poison-splat attack.
- **Break condition:** If the victim model implements advanced detection methods that can identify subtle perturbations or if the constrained attack becomes too weak to significantly impact computational costs.

## Foundational Learning

- **Concept:** 3D Gaussian Splatting (3DGS)
  - **Why needed here:** Understanding the core principles of 3DGS is essential to comprehend how the Poison-splat attack exploits its vulnerabilities.
  - **Quick check question:** What are the key components of a 3D Gaussian in 3DGS, and how are they used to render images?

- **Concept:** Total Variation (TV) Score
  - **Why needed here:** The TV score is a metric used to quantify the sharpness of an image, which is directly related to the number of Gaussians needed for reconstruction in 3DGS.
  - **Quick check question:** How does the total variation score relate to the complexity of an image, and why is it important for the Poison-splat attack?

- **Concept:** Bi-level Optimization
  - **Why needed here:** The Poison-splat attack is formulated as a bi-level optimization problem, where the attacker seeks to maximize computational costs while the victim aims to minimize reconstruction loss.
  - **Quick check question:** What are the two levels of optimization in the Poison-splat attack, and how do they interact with each other?

## Architecture Onboarding

- **Component map:** Clean Dataset -> Proxy Model -> Poisoned Dataset -> Victim Model -> Renderer -> Total Variation Score
- **Critical path:**
  1. Train a proxy model on clean data.
  2. Iteratively optimize perturbations to maximize total variation score while maintaining multi-view consistency.
  3. Update the proxy model with the poisoned data.
  4. Repeat steps 2-3 until the desired level of computational cost is achieved.
- **Design tradeoffs:**
  - Attack strength vs. stealthiness: Increasing the perturbation range enhances the attack but makes it more detectable.
  - Computational cost vs. reconstruction quality: Limiting the number of Gaussians can mitigate the attack but degrades reconstruction quality.
  - Proxy model accuracy vs. attack effectiveness: A more accurate proxy model leads to a more effective attack but requires more computational resources.
- **Failure signatures:**
  - Low number of Gaussians in the victim model, indicating that the attack is not effective.
  - High reconstruction error in the victim model, suggesting that the poisoned data is too inconsistent.
  - Detection of adversarial perturbations by the victim model, leading to rejection of the poisoned data.
- **First 3 experiments:**
  1. **Experiment 1:** Test the attack on a simple scene (e.g., a chair) with a high perturbation range (ϵ = ∞) to observe the maximum impact on computational costs.
  2. **Experiment 2:** Test the attack on a complex scene (e.g., a room) with a constrained perturbation range (ϵ = 16/255) to evaluate the balance between attack strength and stealthiness.
  3. **Experiment 3:** Test the attack on a black-box victim system (e.g., Scaffold-GS) to assess the generalization ability of the attack across different 3DGS variants.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective metric for approximating computational cost in 3D Gaussian Splatting attacks?
- Basis in paper: [explicit] The paper notes that while the number of Gaussians strongly correlates with GPU memory occupancy and rendering latency, it may not be the optimal metric for optimization outcomes.
- Why unresolved: The paper identifies this as a limitation and suggests future research should explore better metrics.
- What evidence would resolve it: Experimental comparisons of attack effectiveness using different metrics (e.g., Gaussian density, other resource consumption measures) would demonstrate which provides the most accurate and practical approximation for attack optimization.

### Open Question 2
- Question: How can 3D Gaussian Splatting systems be made robust against computation cost attacks while maintaining reconstruction quality?
- Basis in paper: [explicit] The paper demonstrates that naive defenses like limiting Gaussian numbers degrade reconstruction quality, and image smoothing significantly reduces PSNR.
- Why unresolved: The paper identifies this as an open challenge and encourages future research to develop more effective defensive strategies.
- What evidence would resolve it: Development and validation of defensive techniques that can effectively prevent or mitigate computation cost attacks without substantially degrading 3D reconstruction quality would resolve this question.

### Open Question 3
- Question: What is the optimal balance between attack stealth and computational impact in constrained poisoning attacks?
- Basis in paper: [explicit] The paper discusses how constrained attacks (with ε-ball constraints) allow attackers to balance destructiveness and stealthiness by tuning ε.
- Why unresolved: The paper does not provide systematic analysis of the trade-off between stealth and attack effectiveness across different ε values and attack scenarios.
- What evidence would resolve it: Comprehensive studies measuring detection rates, reconstruction quality degradation, and computational impact across various ε values and attack strategies would identify optimal stealth-effectiveness trade-offs.

## Limitations

- The attack's effectiveness depends heavily on the victim model's specific implementation of Gaussian densification and densification criteria
- The relationship between TV score and computational cost is demonstrated empirically but not theoretically justified
- Limited evaluation against black-box systems, with only one alternative implementation (Scaffold-GS) tested

## Confidence

- High confidence in the bi-level optimization framework and experimental methodology
- Medium confidence in the generalizability of the attack across different 3DGS implementations
- Medium confidence in the claimed resistance to simple defenses

## Next Checks

1. **Cross-implementation validation**: Test the Poison-splat attack against at least three additional 3DGS implementations with different densification strategies to establish generalizability.

2. **Defense ablation study**: Systematically evaluate the effectiveness of Gaussian limits, image smoothing, and other defenses both individually and in combination to identify optimal mitigation strategies.

3. **Human perception study**: Conduct a user study to quantify the perceptual impact of perturbations under different epsilon constraints, validating the claimed stealthiness of constrained attacks.