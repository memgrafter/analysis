---
ver: rpa2
title: 'SLAB: Efficient Transformers with Simplified Linear Attention and Progressive
  Re-parameterized Batch Normalization'
arxiv_id: '2405.11582'
source_url: https://arxiv.org/abs/2405.11582
tags:
- attention
- transformer
- linear
- batchnorm
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the computational inefficiency of transformer
  architectures, particularly the high cost of LayerNorm during inference and the
  quadratic complexity of attention mechanisms. The authors propose two key innovations:
  a progressive re-parameterized BatchNorm (PRepBN) that gradually replaces LayerNorm
  with BatchNorm during training while maintaining performance, and a simplified linear
  attention (SLA) module that uses ReLU kernels and depthwise convolution to achieve
  linear computational complexity.'
---

# SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization

## Quick Facts
- arXiv ID: 2405.11582
- Source URL: https://arxiv.org/abs/2405.11582
- Authors: Jialong Guo; Xinghao Chen; Yehui Tang; Yunhe Wang
- Reference count: 18
- Primary result: SLAB-Swin achieves 83.6% top-1 accuracy on ImageNet-1K with 16.2ms latency

## Executive Summary
This paper addresses the computational inefficiency of transformer architectures, particularly the high cost of LayerNorm during inference and the quadratic complexity of attention mechanisms. The authors propose two key innovations: a progressive re-parameterized BatchNorm (PRepBN) that gradually replaces LayerNorm with BatchNorm during training while maintaining performance, and a simplified linear attention (SLA) module that uses ReLU kernels and depthwise convolution to achieve linear computational complexity. Their SLAB-Swin model achieves 83.6% top-1 accuracy on ImageNet-1K with 16.2ms latency, outperforming Flatten-Swin by 0.1% accuracy while being 2.4ms faster.

## Method Summary
The authors propose a two-pronged approach to improve transformer efficiency. First, they introduce PRepBN, which progressively replaces LayerNorm with re-parameterized BatchNorm during training using a linear decay strategy on the mixing parameter γ. This allows the model to transition smoothly from the training stability of LayerNorm to the inference efficiency of BatchNorm. Second, they propose SLA, which replaces the standard softmax attention with ReLU-based similarity computation followed by depthwise convolution for local feature enhancement. This reduces computational complexity from O(N²) to O(N) while maintaining performance through local feature aggregation.

## Key Results
- SLAB-Swin achieves 83.6% top-1 accuracy on ImageNet-1K with 16.2ms latency
- Outperforms Flatten-Swin by 0.1% accuracy while being 2.4ms faster
- Shows strong results on object detection (COCO), instance segmentation, and language modeling (Wikitext-103)
- Demonstrates consistent improvements across multiple transformer architectures (DeiT, PVT, Swin variants)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive replacement of LayerNorm with re-parameterized BatchNorm improves training stability and maintains accuracy.
- Mechanism: During training, LayerNorm is gradually replaced with RepBN using a linear decay of γ from 1 to 0, allowing the model to transition smoothly to a pure BatchNorm-based architecture.
- Core assumption: BatchNorm alone causes training collapse, but a gradual transition allows the model to adapt to BatchNorm statistics.
- Evidence anchors:
  - [abstract] "To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training."
  - [section] "We propose to progressively replace LayerNorm with BatchNorm during training, and also propose a new re-parameterization formula of BatchNorm inspired by Ding et al. (2021; 2022) to further improve the performance."
- Break condition: If γ decays too quickly or too slowly, the model may experience training instability or fail to achieve the performance benefits of BatchNorm.

### Mechanism 2
- Claim: Simplified linear attention (SLA) achieves computational efficiency without significant accuracy loss.
- Mechanism: SLA replaces the softmax attention mechanism with ReLU-based similarity computation and depthwise convolution for local feature enhancement, reducing computational complexity from O(N²) to O(N).
- Core assumption: ReLU-based similarity is sufficient to capture token interactions and depthwise convolution compensates for the loss of global attention.
- Evidence anchors:
  - [abstract] "we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance."
  - [section