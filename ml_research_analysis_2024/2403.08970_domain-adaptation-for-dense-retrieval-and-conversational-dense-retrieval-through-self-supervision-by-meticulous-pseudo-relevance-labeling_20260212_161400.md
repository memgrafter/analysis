---
ver: rpa2
title: Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through
  Self-Supervision by Meticulous Pseudo-Relevance Labeling
arxiv_id: '2403.08970'
source_url: https://arxiv.org/abs/2403.08970
tags:
- queries
- domain
- retrieval
- dense
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of domain adaptation for dense
  retrieval models, which struggle to generalize well to new domains. The authors
  propose a novel approach called DoDress that leverages pseudo-relevance labeling
  to generate training data for the target domain.
---

# Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling

## Quick Facts
- arXiv ID: 2403.08970
- Source URL: https://arxiv.org/abs/2403.08970
- Authors: Minghan Li; Eric Gaussier
- Reference count: 0
- Primary result: Introduces DoDress, a self-supervised domain adaptation approach using pseudo-relevance labeling with T5-3B re-ranking and SimANS hard negative sampling

## Executive Summary
This paper addresses the challenge of domain adaptation for dense retrieval models, which struggle to generalize well to new domains. The authors propose DoDress, a novel approach that leverages pseudo-relevance labeling to generate training data for target domains. By using a T5-3B model to identify pseudo-positive documents and various negative sampling strategies, including the effective SimANS method, the approach significantly improves dense retrieval performance across multiple benchmark datasets. The method is further extended to conversational dense retrieval by incorporating a query rewriting module.

## Method Summary
The approach generates pseudo-relevance triplets for domain adaptation by combining BM25 retrieval with T5-3B re-ranking to identify pseudo-positive documents, then sampling hard negatives using strategies like SimANS. These triplets are used to fine-tune base dense retrieval models (D-BERT or GPL) through contrastive learning. For conversational dense retrieval, a query rewriting module is added before the pseudo-labeling process. The method is evaluated on standard dense retrieval and conversational dense retrieval tasks, showing consistent improvements over baseline models when fine-tuned on the pseudo-relevance labeled data.

## Key Results
- SimANS hard negative sampling consistently outperforms other methods across datasets
- DoDress achieves state-of-the-art results on several benchmark datasets including FiQA, BioASQ, and Robust04
- The approach effectively extends to conversational dense retrieval with significant performance gains
- Combining query generation with pseudo-relevance labeling provides complementary benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: T5-3B acts as a high-quality re-ranker to identify pseudo-positive documents from BM25 output.
- Mechanism: BM25 retrieves a broad set of candidates; T5-3B refines them by re-ranking and selecting the top k as pseudo-positives, improving signal quality over BM25 alone.
- Core assumption: T5-3B fine-tuned on MS MARCO captures semantic relevance better than BM25 term matching.
- Evidence anchors:
  - [abstract] "a T5-3B model is utilized for pseudo-positive labeling"
  - [section 4.1] "we propose here to consider, for each query, the top k documents obtained with the combination BM25&T53B, in which T5-3B serves as a re-ranker, as relevant"
  - [corpus] Weak: No direct comparison of T5-3B vs BM25 alone in corpus neighbors.
- Break condition: T5-3B fails to generalize from MS MARCO to target domain, producing low-quality pseudo-positives.

### Mechanism 2
- Claim: SimANS hard negative sampling improves contrastive learning by selecting ambiguous negatives.
- Mechanism: Samples negatives near positives in the embedding space using a density-weighted distribution, reducing uninformative negatives and false negatives.
- Core assumption: Negatives ranked close to positives are more informative for training discriminative embeddings.
- Evidence anchors:
  - [section 4.2] "SimANS (Zhou et al., 2022) shows existing negative sampling strategies... suffer from the uninformative or false negative problem" and the sampling formula (1).
  - [abstract] "meticulous hard negatives are chosen" and "SimANS hard negative sampling strategy consistently outperforms other methods".
  - [corpus] Weak: SimANS not mentioned in neighbor titles, but dense retrieval and negative sampling are related themes.
- Break condition: SimANS distribution becomes too peaked, missing useful negatives; or the embedding space is poorly structured, making proximity uninformative.

### Mechanism 3
- Claim: Combining pseudo-relevance labels with query generation (GPL) yields complementary signals.
- Mechanism: GPL generates synthetic queries and their pseudo-relevant documents; DoDress generates pseudo-labels from real queries and documents; fine-tuning on both enriches the training signal.
- Core assumption: Synthetic queries and real target queries capture different aspects of relevance, improving generalization.
- Evidence anchors:
  - [abstract] "We propose to combine the query-generation approach with a self-supervision approach"
  - [section 4.3] "we suggest further training such models like GPL using the proposed pseudo-relevance triplets"
  - [corpus] Weak: Neighbor papers focus on pseudo-relevance feedback or dense retrieval efficiency, not this specific combination.
- Break condition: The two data sources conflict, causing the model to learn inconsistent relevance patterns.

## Foundational Learning

- Concept: Dense Retrieval (DR) and Dual-Encoder Architecture
  - Why needed here: The paper's core is adapting DR models to new domains via pseudo-labeling; understanding DR mechanics is essential.
  - Quick check question: In dense retrieval, how is the relevance score between a query and document computed?

- Concept: Pseudo-Relevance Feedback (PRF) and Negative Sampling Strategies
  - Why needed here: The paper's innovation relies on generating pseudo-labels and selecting hard negatives; knowing PRF and negative sampling is critical.
  - Quick check question: What is the difference between global random negatives and hard negatives in contrastive learning?

- Concept: Conversational Dense Retrieval (CDR) and Query Rewriting
  - Why needed here: The paper extends the approach to CDR by rewriting conversational queries before pseudo-labeling; understanding CDR is necessary for this extension.
  - Quick check question: Why is query rewriting needed in conversational search, and how does it affect retrieval?

## Architecture Onboarding

- Component map:
  - BM25 retriever → T5-3B re-ranker → pseudo-positive selector
  - Current DR model → SimANS negative sampler → pseudo-negative selector
  - T5-Large query rewriter (CDR only) → T5-3B re-ranker → pseudo-positive selector
  - Triplet generator (query, pos, neg) → DR/CDR fine-tuner

- Critical path:
  1. Generate pseudo-positives using BM25 + T5-3B.
  2. Sample hard negatives using SimANS.
  3. Form triplets and fine-tune DR/CDR model.
  4. For CDR: rewrite conversational queries before steps 1-3.

- Design tradeoffs:
  - BM25+T5-3B vs. BM25 alone: better precision but more computation.
  - SimANS vs. BM25 hard negatives: better negatives but needs current model scores.
  - Query generation vs. pseudo-labeling: complementary but increases training time.

- Failure signatures:
  - Low NDCG@10 on dev set: poor pseudo-label quality or wrong negative sampling.
  - Degraded performance on target domain: domain shift in T5-3B or SimANS sampling.
  - Unstable training: learning rate or batch size mismatch.

- First 3 experiments:
  1. Run DoDress-T53B (D-BERT) on FiQA with global random negatives; verify NDCG@10 improvement over D-BERT.
  2. Switch to SimANS negatives; confirm further NDCG@10 gain.
  3. Apply the same pipeline to BioASQ; check if SimANS prevents failure seen with global random.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed pseudo-relevance labeling approach perform on other dense retrieval models beyond D-BERT and GPL?
- Basis in paper: [explicit] The authors evaluate their approach on D-BERT and GPL, but do not explore its effectiveness on other dense retrieval models.
- Why unresolved: The paper does not provide experimental results on other dense retrieval models, limiting the generalizability of the findings.
- What evidence would resolve it: Conducting experiments on a diverse set of dense retrieval models, such as those based on RoBERTa, MPNet, or other transformer architectures, would demonstrate the broader applicability of the pseudo-relevance labeling approach.

### Open Question 2
- Question: How does the proposed approach scale to larger target domains with significantly more queries and documents?
- Basis in paper: [inferred] The paper evaluates the approach on relatively small target domains (FiQA, BioASQ, and Robust04), but does not investigate its scalability to larger domains.
- Why unresolved: The computational complexity and resource requirements of the pseudo-relevance labeling approach may increase with the size of the target domain, potentially limiting its practicality in large-scale scenarios.
- What evidence would resolve it: Conducting experiments on larger target domains, such as the full ClueWeb or Common Crawl datasets, would provide insights into the scalability and efficiency of the approach in real-world settings.

### Open Question 3
- Question: How does the performance of the pseudo-relevance labeling approach compare to other domain adaptation techniques for dense retrieval, such as adversarial learning or meta-learning?
- Basis in paper: [explicit] The authors compare their approach to several domain adaptation techniques, including MoDIR (adversarial learning) and QGen (query generation), but do not directly compare it to other methods like meta-learning.
- Why unresolved: The paper does not provide a comprehensive comparison with all relevant domain adaptation techniques, making it difficult to assess the relative strengths and weaknesses of the pseudo-relevance labeling approach.
- What evidence would resolve it: Conducting experiments that directly compare the pseudo-relevance labeling approach to other state-of-the-art domain adaptation techniques for dense retrieval, such as meta-learning or self-supervised learning methods, would provide a clearer understanding of its effectiveness and potential advantages.

## Limitations

- The approach's effectiveness depends heavily on T5-3B's ability to generalize from MS MARCO to target domains
- SimANS hyperparameters show sensitivity to dataset characteristics without clear guidance on optimal settings
- The combination of query generation with pseudo-relevance labeling lacks ablation studies to quantify individual contributions

## Confidence

- **High Confidence**: The core mechanism of using T5-3B for pseudo-positive labeling and SimANS for hard negative sampling is well-supported by the experimental results across multiple datasets.
- **Medium Confidence**: The claim that combining query generation with pseudo-relevance labeling provides complementary benefits is supported by results but lacks direct ablation analysis.
- **Low Confidence**: The generalizability of the approach to domains vastly different from MS MARCO, and the sensitivity of SimANS hyperparameters to dataset characteristics, remain uncertain without broader testing.

## Next Checks

1. **Ablation Study**: Conduct experiments isolating the effects of T5-3B pseudo-positive labeling versus query generation, and compare SimANS against other negative sampling strategies on each target domain.
2. **Hyperparameter Sensitivity**: Systematically vary SimANS parameters (a, b) and k (top documents) across datasets to identify optimal settings and their robustness.
3. **Domain Shift Analysis**: Evaluate T5-3B's zero-shot performance on target domains before pseudo-labeling, and test the approach on datasets with minimal MS MARCO overlap to assess generalization limits.