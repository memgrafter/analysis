---
ver: rpa2
title: Can adversarial attacks by large language models be attributed?
arxiv_id: '2411.08003'
source_url: https://arxiv.org/abs/2411.08003
tags:
- language
- attribution
- finite
- languages
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes fundamental limits on attributing outputs\
  \ from large language models (LLMs) to specific sources, particularly in adversarial\
  \ settings like cyberattacks and disinformation campaigns. Using formal language\
  \ theory, the authors prove that under realistic conditions\u2014even with unlimited\
  \ data\u2014certain classes of LLMs are fundamentally non-identifiable from their\
  \ outputs alone."
---

# Can adversarial attacks by large language models be attributed?

## Quick Facts
- arXiv ID: 2411.08003
- Source URL: https://arxiv.org/abs/2411.08003
- Authors: Manuel Cebrian; Andres Abeliuk; Jan Arne Telle
- Reference count: 37
- Key outcome: Establishes fundamental limits on attributing LLM outputs to specific sources, proving that under realistic conditions certain classes of LLMs are fundamentally non-identifiable from their outputs alone.

## Executive Summary
This paper demonstrates that attributing outputs from large language models to specific sources is fundamentally impossible under certain conditions, particularly in adversarial settings like cyberattacks and disinformation campaigns. Using formal language theory, the authors prove that infinite classes of both deterministic and probabilistic LLMs are non-identifiable, while even finite classes of probabilistic LLMs can be non-identifiable. Complementing this theoretical analysis, the authors show that the hypothesis space for attribution is growing exponentially, making brute-force approaches computationally infeasible. These findings highlight the urgent need for new attribution strategies and proactive governance to address the risks of un-attributable, adversarial use of LLMs.

## Method Summary
The paper combines formal language theory proofs with empirical analysis of the LLM ecosystem. Theoretical results establish non-identifiability bounds using Gold's identification-in-the-limit framework and Angluin's tell-tale criterion. Empirical analysis examines the Stanford Ecosystem Graphs dataset to quantify hypothesis space growth and calculates computational requirements for brute-force attribution approaches. The study models attribution as a learning problem where an algorithm must identify the source model from its outputs, proving fundamental limits under adversarial assumptions.

## Key Results
- Under mild assumptions of overlapping capabilities, certain classes of LLMs are fundamentally non-identifiable from their outputs alone
- The number of plausible model origins (hypothesis space) is growing exponentially, doubling approximately every 0.5 years
- Brute-force attribution across all known models for even moderately long texts would require astronomical computational resources
- The paper provides the first explicit counterexample showing that even finite classes of probabilistic LLMs can be non-identifiable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Infinite classes of deterministic or probabilistic LLMs are non-identifiable in the limit.
- Mechanism: If a language class contains an infinite language L∞ with infinitely many finite subsets extendable to different languages in the class, no finite tell-tale set exists to uniquely identify L∞. For probabilistic models, this extends because deterministic languages are a special case of probabilistic ones.
- Core assumption: The adversarial identification-in-the-limit framework where every string in the language eventually appears in the sample.

### Mechanism 2
- Claim: Even finite classes of probabilistic LLMs can be non-identifiable.
- Mechanism: Two probabilistic languages can have identical support but different probability distributions. Without seeing the true distribution (only which strings appear), no amount of data can distinguish them under adversarial presentation.
- Core assumption: The adversarial framework where examples are presented in an order that hides statistical biases.

### Mechanism 3
- Claim: Finite classes of deterministic LLMs are identifiable.
- Mechanism: With finitely many candidate languages, tell-tale sets can be constructed for each language by finding distinguishing strings. This satisfies Angluin's criterion for identification in the limit.
- Core assumption: Each language is recursive and can be tested for membership.

## Foundational Learning

- Concept: Formal language theory and identification in the limit
  - Why needed here: The paper uses Gold's framework to prove theoretical limits on attribution
  - Quick check question: What does it mean for a language class to be "identifiable in the limit"?

- Concept: Probabilistic vs deterministic formal languages
  - Why needed here: The paper distinguishes between these cases to show different identifiability results
  - Quick check question: How does a probabilistic language differ from a deterministic one?

- Concept: Tell-tale sets and Angluin's theorem
  - Why needed here: The positive result for finite deterministic classes relies on this criterion
  - Quick check question: What is a tell-tale set and why is it necessary for identification?

## Architecture Onboarding

- Component map: Theoretical proofs -> Empirical ecosystem analysis -> Computational feasibility calculations -> Policy implications
- Critical path: (1) Prove theoretical limits, (2) Quantify practical challenges through empirical analysis, (3) Discuss implications and mitigation strategies
- Design tradeoffs: The paper balances theoretical rigor with practical relevance by proving results under worst-case assumptions while providing empirical evidence that these limits matter in practice
- Failure signatures: Attribution fails when the hypothesis space is too large, when models have overlapping capabilities, or when only outputs (not probabilities) are observable
- First 3 experiments:
  1. Implement Gold's identification-in-the-limit framework to test identifiability on sample language classes
  2. Analyze LLM ecosystem data to estimate hypothesis space growth under different fine-tuning assumptions
  3. Calculate computational requirements for brute-force attribution across different model sets and sequence lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can watermarking or fingerprinting methods reliably enable attribution of LLM outputs in adversarial settings?
- Basis in paper: The paper discusses watermarking and fingerprinting as potential mitigation strategies but notes their limitations against adversarial actors.
- Why unresolved: While watermarking is proposed as a technical solution, the paper highlights that attackers can potentially remove or spoof watermarks, and not all model developers will comply.
- What evidence would resolve it: Empirical studies demonstrating the robustness of watermarking methods against adversarial attacks, or documented cases of successful watermark detection in real-world scenarios.

### Open Question 2
- Question: How can we quantify the tradeoff between model expressiveness and identifiability in LLM attribution?
- Basis in paper: The paper establishes that more expressive models (those that can generate a wider range of outputs) are harder to attribute, particularly in probabilistic settings.
- Why unresolved: The paper identifies this tradeoff theoretically but does not provide quantitative metrics for how expressiveness impacts identifiability across different model classes.
- What evidence would resolve it: Empirical measurements of attribution accuracy across models with varying expressiveness, or theoretical bounds on the relationship between model expressiveness and identifiability.

### Open Question 3
- Question: What alternative learning criteria beyond identification in the limit could improve LLM attribution accuracy?
- Basis in paper: The paper notes that identification in the limit does not assume random sampling according to the model distribution, suggesting that alternative criteria might be more effective.
- Why unresolved: The paper focuses on the limitations of identification in the limit but does not explore alternative frameworks that might leverage probabilistic information or statistical analysis.
- What evidence would resolve it: Experimental comparisons of attribution accuracy using different learning criteria, or theoretical analysis of new frameworks that incorporate probabilistic sampling or statistical inference.

## Limitations

- Theoretical assumptions rely on worst-case adversarial scenarios that may not reflect real-world attribution conditions
- Ecosystem modeling projections depend on specific fine-tuning patterns and parameter count distributions that may vary in practice
- Computational feasibility estimates assume current hardware capabilities and may not account for future algorithmic improvements

## Confidence

**High confidence**: The theoretical impossibility results for infinite classes and the finite deterministic case are well-established in formal language theory literature.

**Medium confidence**: The probabilistic counterexample and extension to finite probabilistic classes represent novel contributions requiring additional validation.

**Medium confidence**: The empirical growth rates and computational feasibility estimates are based on reasonable assumptions but involve multiple modeling choices.

## Next Checks

1. Test theoretical bounds with simulations: Implement the Gold identification-in-the-limit framework on synthetic language classes to empirically verify non-identifiability results and quantify distinguishability.

2. Validate ecosystem growth projections: Analyze alternative datasets to test sensitivity of exponential growth parameters to different fine-tuning assumptions and parameter count distributions.

3. Benchmark attribution approaches: Implement and benchmark practical attribution algorithms on controlled datasets where ground truth model origins are known, measuring performance degradation as hypothesis space size increases.