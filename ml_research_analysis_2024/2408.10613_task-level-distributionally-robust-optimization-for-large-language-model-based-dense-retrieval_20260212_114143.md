---
ver: rpa2
title: Task-level Distributionally Robust Optimization for Large Language Model-based
  Dense Retrieval
arxiv_id: '2408.10613'
source_url: https://arxiv.org/abs/2408.10613
tags:
- loss
- retrieval
- tdro
- question
- llm-dr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving retrieval performance
  in large language model-based dense retrieval (LLM-DR) systems by optimizing the
  distribution of training data across heterogeneous datasets. The authors propose
  a novel task-level Distributionally Robust Optimization (tDRO) algorithm that end-to-end
  reweights the data distribution of each task to enhance universal domain generalization.
---

# Task-level Distributionally Robust Optimization for Large Language Model-based Dense Retrieval

## Quick Facts
- arXiv ID: 2408.10613
- Source URL: https://arxiv.org/abs/2408.10613
- Reference count: 40
- Primary result: tDRO improves LLM-DR retrieval performance across multilingual, cross-lingual, and monolingual benchmarks while reducing dataset usage by up to 30%

## Executive Summary
This paper introduces task-level Distributionally Robust Optimization (tDRO) to address the challenge of optimizing training data distribution for large language model-based dense retrieval (LLM-DR) systems. The method dynamically reweights heterogeneous training datasets based on relative loss measurements, improving universal domain generalization without requiring additional training data. By separating DRO optimization from LLM-DR fine-tuning and using a proxy model with relative loss normalization, tDRO achieves significant performance gains across large-scale retrieval benchmarks including MIRACL, MKQA, and BeIR.

## Method Summary
tDRO optimizes domain weights through a two-stage process. First, a small proxy model (Qwen1.5-0.5B) computes domain-specific losses, which are normalized using a reference model to create comparable relative loss measurements. These measurements drive exponential gradient ascent updates to domain weights, which are then transferred to the main LLM-DR fine-tuning stage. The approach separates task-homogeneous batching for weight updates from task-heterogeneous batching for contrastive learning, enabling compatibility with different sampling strategies. Domain weights can be applied through top-k dataset selection or sample ratio reweighting.

## Key Results
- tDRO improves retrieval performance across multilingual (MIRACL), cross-lingual (MKQA), and monolingual (BeIR) benchmarks
- Top-70% dataset selection strategy reduces dataset usage by up to 30% while improving performance
- Consistent improvements across different-sized LLM-DR models (1.8B and 8B parameters)
- tDRO maintains effectiveness even when using bottom-50% of datasets

## Why This Works (Mechanism)

### Mechanism 1
The task-level DRO algorithm improves retrieval performance by dynamically reweighting training data based on relative loss measurements. The algorithm uses a proxy model to compute domain-specific losses and a reference model to normalize these losses, creating comparable relative loss measurements. Higher relative losses trigger greater weight increases for those domains.

### Mechanism 2
Separating DRO optimization from LLM-DR fine-tuning enables compatibility with different batch sampling strategies. tDRO uses task-homogeneous batching to fit all domain data in one batch for weight updates, while LLM-DR uses task-heterogeneous batching to ensure quality in-batch negatives. The learned weights transfer between stages.

### Mechanism 3
Using relative loss measurement prevents bias toward high-loss domains while maintaining optimization effectiveness. By dividing proxy loss by reference loss, the algorithm scales losses to comparable ranges, preventing domains with naturally higher loss scales from dominating the optimization process.

## Foundational Learning

- Concept: Distributionally Robust Optimization (DRO)
  - Why needed here: DRO provides theoretical foundation for handling heterogeneous training data by optimizing for worst-case performance across domains
  - Quick check question: What distinguishes DRO from standard empirical risk minimization in multi-domain training scenarios?

- Concept: Contrastive Learning with InfoNCE Loss
  - Why needed here: Contrastive loss is the primary optimization objective for dense retrieval, pulling positive pairs together while pushing negative pairs apart
  - Quick check question: How does InfoNCE loss differ from standard cross-entropy loss in retrieval contexts?

- Concept: Batch Sampling Strategies
  - Why needed here: Different batching approaches (task-homogeneous vs task-heterogeneous) have different trade-offs for negative quality and computational efficiency
  - Quick check question: What are the key differences between in-batch negatives and cross-batch negatives in large-batch contrastive learning?

## Architecture Onboarding

- Component map: Proxy model (Qwen1.5-0.5B) -> Relative loss computation -> Weight update module -> Transfer module -> LLM-DR fine-tuning
- Critical path: Proxy model → relative loss computation → weight update → weight transfer → LLM-DR fine-tuning
- Design tradeoffs:
  - Small proxy model vs large proxy model: Computational efficiency vs accurate loss estimation
  - Top-k dataset selection vs ratio reweighting: Dataset reduction vs continuous weight adjustment
  - Relative loss measurement vs absolute loss measurement: Robustness to scale differences vs sensitivity to absolute performance
- Failure signatures: All weights converging to near-zero values, significant performance degradation after weight transfer, unstable weight updates during training
- First 3 experiments:
  1. Validate relative loss measurement by comparing proxy and reference model outputs on single domain
  2. Test weight transfer stability by fine-tuning small model with different weight configurations
  3. Benchmark performance improvements on held-out validation set after tDRO optimization

## Open Questions the Paper Calls Out

### Open Question 1
How does the tDRO algorithm perform when applied to retrieval tasks with highly imbalanced domain distributions that deviate significantly from the training distribution? The experiments focus on balanced or moderately weighted datasets, leaving open whether tDRO maintains effectiveness when faced with highly skewed domain distributions.

### Open Question 2
What is the optimal trade-off between dataset reduction (via top-rated selection) and retrieval performance across different LLM sizes and task types? The paper demonstrates that using Top-70% of datasets can improve performance while reducing dataset usage by up to 30%, but doesn't systematically explore the optimal selection percentage.

### Open Question 3
How does the relative loss measurement scale with increasingly diverse and complex retrieval tasks, and are there scenarios where it might fail to properly normalize loss differences? The paper introduces relative loss measurement to address incomparable loss scales across heterogeneous collections, but acknowledges this is a key design component whose limitations aren't fully explored.

## Limitations

- Effectiveness depends on reference model maintaining stable performance throughout training
- 0.5B parameter proxy model may not capture complex loss patterns in larger domains
- Top-70% dataset selection strategy could remove valuable training data that contributes to better generalization

## Confidence

**High Confidence**: The core DRO framework and weight update mechanism are well-established in the literature, and the separation of proxy optimization from fine-tuning follows sound engineering principles.

**Medium Confidence**: The relative loss measurement approach shows theoretical promise, but its practical effectiveness depends on implementation details not fully specified in the paper.

**Low Confidence**: The long-term stability of transferred weights and their performance across different model scales remains unverified.

## Next Checks

1. **Reference Model Stability Analysis**: Monitor reference model performance throughout tDRO training to verify that loss normalization remains stable and meaningful across all domains.

2. **Weight Transfer Robustness Test**: Conduct ablation studies where domain weights are randomly perturbed before transfer to LLM-DR fine-tuning to quantify how sensitive performance improvements are to the precise weight values.

3. **Dataset Selection Impact Study**: Systematically vary the dataset selection threshold (e.g., 50%, 60%, 70%, 80%) and compare performance to understand the trade-off between computational efficiency and retrieval quality.