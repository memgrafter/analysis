---
ver: rpa2
title: 'Into the Unknown Unknowns: Engaged Human Learning through Participation in
  Language Model Agent Conversations'
arxiv_id: '2408.15232'
source_url: https://arxiv.org/abs/2408.15232
tags:
- alphafold
- information
- drug
- co-storm
- discourse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Co-STORM is a system that helps users discover information in the
  "unknown unknowns" by observing and participating in a collaborative discourse among
  LM agents. It maintains a dynamic mind map to organize the discourse and generate
  a comprehensive report as takeaways.
---

# Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations

## Quick Facts
- arXiv ID: 2408.15232
- Source URL: https://arxiv.org/abs/2408.15232
- Reference count: 40
- Key outcome: Co-STORM outperforms baseline methods on discourse trace and report quality for discovering unknown unknowns through collaborative LM agent discourse

## Executive Summary
Co-STORM is a system that helps users discover information in the "unknown unknowns" by observing and participating in a collaborative discourse among LM agents. It maintains a dynamic mind map to organize the discourse and generate a comprehensive report as takeaways. The system enables serendipitous discovery by simulating expert conversations with a moderator that introduces novel perspectives. In human evaluation, 70% of participants preferred Co-STORM over a search engine and 78% favored it over a RAG chatbot for the overall information-seeking experience.

## Method Summary
Co-STORM uses multiple LM agents with different roles (experts and moderator) to simulate collaborative discourse, allowing users to observe and occasionally steer conversations. The system dynamically maintains a hierarchical mind map to organize concepts and relationships, reorganizing when concepts become too large. It generates comprehensive cited reports as takeaways. The system was evaluated on the WildSeek dataset of real-world information-seeking records, comparing automatic metrics for report and discourse quality against baseline methods (RAG Chatbot, STORM + QA), and conducted human evaluations with 24 participants.

## Key Results
- Co-STORM outperforms baseline methods on both discourse trace and report quality as evaluated on the WildSeek dataset
- 70% of participants preferred Co-STORM over a search engine in human evaluation
- 78% of participants favored Co-STORM over a RAG chatbot for the overall information-seeking experience

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Co-STORM enables discovery of unknown unknowns by simulating collaborative discourse among LM agents with different roles
- Mechanism: The system creates an interactive environment where users observe and occasionally steer conversations among LM agents, with experts asking questions and providing answers from different perspectives while the moderator injects new directions
- Core assumption: Users learn effectively by observing and occasionally participating in expert conversations
- Evidence anchors:
  - [abstract]: "Co-STORM lets users observe and occasionally steer the discourse among several LM agents... allowing the user to discover unknown unknowns serendipitously"
  - [section 3.1]: "Co-STORM adopts a mixed-initiative approach... the user can jump in at any time to steer the discourse and inject questions and opinions"
- Break condition: If users become overwhelmed by discourse complexity or if the moderator fails to introduce genuinely novel information

### Mechanism 2
- Claim: Dynamic mind map organization reduces cognitive load and helps users track complex information-seeking processes
- Mechanism: Co-STORM maintains a hierarchical mind map that dynamically organizes concepts and their relationships, inserting new information under appropriate concepts and reorganizing when concepts become too large
- Core assumption: Visual organization of information into hierarchical structures significantly reduces mental effort
- Evidence anchors:
  - [section 3.2]: "Co-STORM uses a tree-structured mind map M to dynamically organize collected information in the discourse D"
- Break condition: If the mind map becomes too complex or if automatic organization fails to reflect the user's mental model

### Mechanism 3
- Claim: Mixed-initiative discourse protocol balances user control with automated exploration, optimizing information discovery
- Mechanism: The system alternates between expert turns and moderator interventions based on observed discourse patterns, with the moderator stepping in when experts provide consecutive answers without introducing new questions
- Core assumption: Automatic question generation based on unused information effectively surfaces novel perspectives
- Evidence anchors:
  - [section 3.1]: "To prevent them from only expanding on the same point, upon observing L consecutive turns of expert responses... the system asks the moderator to intervene"
- Break condition: If the reranking function fails to surface genuinely novel information or if moderator questions become disconnected from user interests

## Foundational Learning

- Concept: Unknown unknowns in information seeking
  - Why needed here: The paper's core premise is that traditional search systems excel at known unknowns but fail at unknown unknowns, crucial for complex information seeking
  - Quick check question: What distinguishes unknown unknowns from known unknowns in the context of information seeking?

- Concept: Collaborative discourse and learning
  - Why needed here: Co-STORM's design is based on the educational principle that collaborative discourse facilitates deeper learning than individual information retrieval
  - Quick check question: How does collaborative discourse differ from traditional question-answering in terms of learning outcomes?

- Concept: Mixed-initiative systems
  - Why needed here: Co-STORM uses a mixed-initiative approach where both users and agents can drive the conversation
  - Quick check question: What are the advantages of mixed-initiative systems over purely user-driven or purely agent-driven systems?

## Architecture Onboarding

- Component map: User interface -> Intent classification -> Information retrieval -> Discourse generation -> Mind map update -> User feedback loop -> Final report generation
- Critical path: User input → Intent classification → Information retrieval → Discourse generation → Mind map update → User feedback loop → Final report generation
- Design tradeoffs: The system trades latency for interactivity (multiple LM calls per turn), complexity for user engagement (multi-agent discourse vs simple Q&A), and automation for control (moderator steering vs user-only direction)
- Failure signatures: High latency indicates inefficient LM prompting or search API issues; repetitive discourse suggests moderator reranking failure; inaccurate mind map indicates poor semantic similarity computation
- First 3 experiments:
  1. Test single-turn discourse generation with simulated user input to verify intent classification and response quality
  2. Test mind map insertion operation with controlled information placement scenarios
  3. Test end-to-end discourse with simulated user following the turn management protocol

## Open Questions the Paper Calls Out

- How can Co-STORM be adapted to better tailor the collaborative discourse to users' varying levels of prior knowledge? The paper discusses limitations including better tailoring the collaborative discourse to the user's prior knowledge, skipping basic facts for knowledgeable users and introducing concepts progressively for novices.

- What are the optimal strategies for managing the discourse when multiple experts have conflicting viewpoints? The paper mentions that the moderator role is crucial for steering the discourse but doesn't elaborate on how to handle conflicts between expert perspectives.

- How can Co-STORM be extended to support multilingual information seeking while maintaining discourse quality and information accuracy? The paper explicitly states that extending Co-STORM to support multiple languages would significantly enhance its usefulness but acknowledges the challenges of integrating search engines capable of accessing diverse language sources.

## Limitations

- Evaluation relies on a single dataset (WildSeek) with limited domain diversity
- Human evaluation sample size of 24 participants may not generalize broadly
- Paper doesn't address potential biases in the LM agents' discourse or how different user expertise levels might affect outcomes

## Confidence

- **High**: Co-STORM outperforms baseline methods on automatic metrics (report and discourse quality) - supported by quantitative evaluation on WildSeek dataset
- **Medium**: Human preference for Co-STORM over search engines and RAG chatbots - supported by user study but limited by sample size and potential selection bias
- **Medium**: The mind map organization effectively reduces cognitive load - mechanism is plausible but lacks direct empirical validation of the cognitive benefit

## Next Checks

1. Conduct larger-scale user studies across diverse domains and user expertise levels to validate generalizability of the preference results
2. Implement ablation studies to quantify the specific contributions of each component (mind map, moderator role, mixed-initiative protocol) to overall performance
3. Test Co-STORM's robustness against adversarial or biased information sources to evaluate the system's handling of unreliable content in the collaborative discourse