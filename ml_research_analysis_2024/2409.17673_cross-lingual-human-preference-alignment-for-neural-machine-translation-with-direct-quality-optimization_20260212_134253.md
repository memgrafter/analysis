---
ver: rpa2
title: Cross-lingual Human-Preference Alignment for Neural Machine Translation with
  Direct Quality Optimization
arxiv_id: '2409.17673'
source_url: https://arxiv.org/abs/2409.17673
tags:
- translation
- data
- language
- languages
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses a fundamental task-data mismatch in neural
  machine translation (NMT), where training data from web mining and translationese
  differs from human-preferred translations. To address this, the authors introduce
  Direct Quality Optimization (DQO), a batched online variant of Direct Preference
  Optimization (DPO) that uses a pre-trained translation quality estimation model
  (CometKiwi22) as a proxy for human preferences.
---

# Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization

## Quick Facts
- arXiv ID: 2409.17673
- Source URL: https://arxiv.org/abs/2409.17673
- Reference count: 40
- Improved translation quality across all 30 languages in Megatron English-30 model using only 5 languages for alignment

## Executive Summary
This paper addresses a fundamental task-data mismatch in neural machine translation where training data from web mining and translationese differs from human-preferred translations. The authors introduce Direct Quality Optimization (DQO), a batched online variant of Direct Preference Optimization (DPO) that uses a pre-trained translation quality estimation model (CometKiwi22) as a proxy for human preferences. Applied to the NVIDIA Megatron English-30 model using only 5 of 30 target languages (Chinese, German, Hindi, Russian, Spanish) for alignment, DQO improved translation quality across all languages, including those not used in DQO and unrelated language families, as measured by multiple automated metrics and confirmed by human evaluation.

## Method Summary
The method uses Direct Quality Optimization (DQO), a batched online variant of DPO that leverages CometKiwi22 quality estimation model to score and compare multiple translations. The process involves sampling source segments from a seed dataset, generating 64+ translation candidates per source, scoring them with CometKiwi22, creating preference pairs based on scores, and training with DPO for 8 epochs. This is repeated for 5 rounds, with the model improving translation quality across all languages, including held-out languages not used in DQO training.

## Key Results
- Improved translation quality across all 30 languages in Megatron English-30 model
- Cross-lingual transfer: held-out languages (not used in DQO) showed significant improvements
- Human evaluation confirmed improvements with reduced MQM errors in unrelated languages like Japanese and Lithuanian

## Why This Works (Mechanism)

### Mechanism 1
Using quality estimation models as proxies for human preferences allows efficient preference alignment without costly human annotation. CometKiwi22 and similar QE models provide automatic scoring of translation quality that correlates with human judgment, enabling scalable creation of preference pairs for training.

### Mechanism 2
Improvements in held-out languages occur through transfer of general translation behaviors learned during DQO. DQO training on 5 target languages teaches the model general translation principles (avoiding omissions, maintaining fluency) that transfer to all languages.

### Mechanism 3
DQO increases likelihood of features present in alignment data while also activating previously learned but unused knowledge. The model had latent knowledge from supervised training that wasn't being used; DQO shifts the output distribution to activate this knowledge.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: A preference learning algorithm that optimizes the difference in log probabilities between chosen and rejected responses. Needed because DQO is a variant of DPO. Quick check: What is the key difference between DPO and standard RLHF approaches?

- **Translation Quality Estimation (QE)**: Models that automatically score translation quality by predicting human judgment. Needed because QE models serve as proxies for human preferences in DQO. Quick check: How do QE models like CometKiwi22 correlate with human translation quality judgments?

- **Multilingual model architecture**: Architectural features enabling a single model to handle multiple language pairs effectively. Needed because the Megatron model supports 30 languages, enabling cross-lingual transfer analysis. Quick check: What architectural features enable a single model to handle multiple language pairs effectively?

## Architecture Onboarding

- **Component map**: Megatron English-30 encoder-decoder model (500M parameters) -> CometKiwi22 quality estimation model -> Seed dataset -> Sampling pipeline -> DQO training loop

- **Critical path**: 1. Sample source segments from seed dataset 2. Generate 64+ translations per source using current policy model 3. Score translations with CometKiwi22 4. Create preference pairs (best vs. random samples) 5. Train with DPO for 8 epochs 6. Repeat for 5 rounds

- **Design tradeoffs**: Proxy quality vs. annotation cost (QE models are efficient but may be imperfect); Language coverage (only 5 of 30 languages used in training); Sampling diversity (64 samples per source balances exploration and efficiency)

- **Failure signatures**: No improvement in BLEU/COMET scores; Increased perplexity on training data; Degenerate output in certain languages; Over-optimization to QE proxy (reward hacking)

- **First 3 experiments**: 1. Run DQO on a single language pair and verify BLEU improvement 2. Test held-out language performance after DQO training 3. Compare DQO vs. supervised fine-tuning (RAFT) on the same data

## Open Questions the Paper Calls Out

### Open Question 1
What specific aspects of translation quality improve for unrelated languages after DQO, beyond general translation behaviors? The authors observed language-specific improvements in unrelated languages like Latvian, including correct transliteration of named entities, but could not explain how these improvements occurred.

### Open Question 2
How does the choice of quality estimation model as a proxy for human preferences impact the effectiveness of DQO? The authors used CometKiwi22 as a proxy for human preferences and conducted a brief experiment with MetricX, obtaining similar results, but the impact of the proxy model's quality on DQO's performance was not thoroughly investigated.

### Open Question 3
How does DQO's performance compare to other task-alignment methods, such as data filtering and curriculum learning, in addressing the task-data mismatch in NMT? The authors mention previous work on data filtering and curriculum learning but do not directly compare DQO's performance to these methods.

## Limitations

- Use of CometKiwi22 as proxy for human preferences introduces uncertainty about whether improvements reflect true human preferences
- Cross-lingual transfer mechanism remains partially speculative with limited definitive proof
- Ablation study showed catastrophic performance on certain languages (FR, JA, KO, ZH), suggesting language-specific failure modes

## Confidence

**High Confidence**: DQO improves translation quality as measured by automated metrics (BLEU, COMET22, CometKiwi22, BLEURT) and human evaluation metrics (reduced MQM errors).

**Medium Confidence**: Cross-lingual transfer occurs through general translation behaviors learned during DQO, though this is a likely explanation rather than proven mechanism.

**Low Confidence**: DQO activates "latent knowledge" from supervised training - this mechanism is difficult to verify and represents an interpretation of observed improvements.

## Next Checks

1. **Human preference correlation test**: Conduct controlled human evaluation comparing translations optimized with CometKiwi22 versus human-annotated preferences to verify the QE model is an accurate proxy.

2. **Cross-lingual transfer mechanism analysis**: Design experiment isolating contribution of general translation skills versus language-specific knowledge by training on language families versus individual languages, then measuring transfer to held-out languages.

3. **Failure mode characterization**: Systematically test DQO across broader range of languages to identify linguistic features causing catastrophic behavior, revealing limitations in the approach.