---
ver: rpa2
title: 'DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models'
arxiv_id: '2411.15024'
source_url: https://arxiv.org/abs/2411.15024
tags:
- tokens
- video
- token
- visual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DyCoke is a training-free dynamic token compression method for
  accelerating video large language models (VLLMs). It addresses the high computational
  cost caused by thousands of visual tokens generated from video inputs by introducing
  a two-stage approach: temporal token merging to reduce redundancy across frames
  and dynamic KV cache pruning to selectively remove spatially redundant tokens during
  decoding.'
---

# DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models

## Quick Facts
- arXiv ID: 2411.15024
- Source URL: https://arxiv.org/abs/2411.15024
- Reference count: 40
- Key outcome: Achieves 1.5× speedup and 1.4× memory reduction on video LLMs while maintaining/improving accuracy

## Executive Summary
DyCoke addresses the high computational cost of video large language models by introducing a training-free dynamic token compression method. The approach tackles the challenge of thousands of visual tokens generated from video inputs through a two-stage strategy: temporal token merging to reduce redundancy across frames and dynamic KV cache pruning to selectively remove spatially redundant tokens during decoding. Unlike one-shot pruning strategies, DyCoke dynamically retains critical tokens at each decoding step, achieving significant efficiency gains while improving accuracy on multiple benchmarks.

## Method Summary
DyCoke employs a two-stage, training-free approach to accelerate video LLMs. First, it uses Temporal Token Merging (TTM) to merge redundant tokens across consecutive frames by grouping similar tokens based on cosine similarity, achieving 50-60% token reduction. Second, it applies dynamic KV cache pruning that selectively retains top attention scores at each decoding iteration while maintaining a secondary cache for pruned tokens. This dynamic approach preserves critical tokens that static pruning would miss, achieving an additional 70-90% reduction while improving accuracy compared to baselines like LLaVA-PruMerge and FastV.

## Key Results
- Achieves 1.5× inference speedup and 1.4× memory reduction compared to baseline models
- Improves accuracy on MVBench while being 1.3× faster than LLaVA-PruMerge
- Maintains strong performance across multiple model scales (0.5B, 7B, 72B parameters)
- Outperforms baseline methods on ActivityNet-QA, PerceptionTest, VideoMME, NeXTQA, and VideoDetailCaption benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DyCoke dynamically retains critical tokens at each decoding step, improving accuracy.
- Mechanism: Dynamic KV cache pruning removes spatially redundant tokens selectively during decoding, while preserving pruned tokens for secondary activations.
- Core assumption: Attention scores vary significantly across decoding iterations, making one-shot pruning strategies prone to removing important tokens.
- Evidence anchors:
  - [abstract]: "Unlike one-shot pruning strategies, DyCoke dynamically retains critical tokens at each decoding step, improving accuracy."
  - [section]: "The model's focus shifts to different visual tokens as the decoding proceeds, resembling human attention patterns. Consequently, for video LLMs, unlike image inputs, a single-stage pruning strategy may result in incorrect token filtering, omission of key tokens, and temporal disarray, thereby compromising video comprehension."
  - [corpus]: Weak - the corpus mentions similar token compression methods but doesn't specifically discuss dynamic pruning during decoding.

### Mechanism 2
- Claim: Temporal token merging reduces redundancy across frames while preserving key video content information.
- Mechanism: Groups consecutive frames by sampling and identifies tokens with overlapping information in adjacent frames for temporal merging.
- Core assumption: Significant temporal redundancy exists between adjacent video frames, with similar visual content persisting across multiple frames.
- Evidence anchors:
  - [abstract]: "DyCoke incorporates a plug-and-play temporal compression module to minimize temporal redundancy by merging redundant tokens across frames"
  - [section]: "Combining these redundant visual tokens at temporal scales can reduce the total token length of the input, which accelerates VLLMs inference and decreases memory consumption."
  - [corpus]: Weak - while the corpus mentions token merging methods, it doesn't specifically discuss temporal merging across video frames.

### Mechanism 3
- Claim: The two-stage approach (temporal merging followed by dynamic pruning) achieves superior performance compared to single-stage methods.
- Mechanism: First stage reduces token count through temporal merging (50-60% reduction), second stage dynamically prunes remaining tokens (70-90% additional reduction) while preserving performance.
- Core assumption: Combining temporal and spatial redundancy reduction provides better efficiency-performance tradeoff than addressing either dimension alone.
- Evidence anchors:
  - [abstract]: "DyCoke incorporates a plug-and-play temporal compression module to minimize temporal redundancy by merging redundant tokens across frames, and applies dynamic KV cache reduction to prune spatially redundant tokens selectively."
  - [section]: "The first phase involves designing a plug-and-play, lightweight token compression module that addresses temporal redundancy by merging similar tokens across frames. The second phase maintains a parsimonious KV cache established in the first phase by dynamically pruning less important information."
  - [corpus]: Weak - corpus mentions token compression methods but doesn't specifically discuss the combined two-stage approach.

## Foundational Learning

- Concept: Attention mechanism in transformers
  - Why needed here: Understanding how attention scores are computed and used for token importance evaluation
  - Quick check question: How does the attention score between query and key vectors determine token importance?

- Concept: KV cache optimization
  - Why needed here: Understanding how DyCoke modifies the KV cache during decoding for dynamic pruning
  - Quick check question: What is stored in the KV cache and how does it enable efficient token generation during decoding?

- Concept: Temporal redundancy in video data
  - Why needed here: Understanding why adjacent video frames contain similar information that can be merged
  - Quick check question: What types of visual information typically remain constant across consecutive video frames?

## Architecture Onboarding

- Component map: Vision Encoder → Projector → Temporal Merging → Language Model → Dynamic Pruning during decoding

- Critical path: Vision Encoder → Projector → Temporal Merging → Language Model → Dynamic Pruning during decoding

- Design tradeoffs:
  - Higher pruning rates improve efficiency but risk performance degradation
  - More attention layers used for evaluation increase accuracy but computational cost
  - Dynamic pruning requires additional memory for DP cache

- Failure signatures:
  - Performance drops when pruning rate is too high
  - Increased latency if temporal merging is computationally expensive
  - Memory overflow if DP cache grows too large

- First 3 experiments:
  1. Test baseline performance with full tokens vs. various static pruning rates
  2. Evaluate temporal merging alone with different sampling strategies
  3. Combine temporal merging with dynamic pruning at different attention layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DyCoke's dynamic token pruning strategy affect the long-term coherence of generated video descriptions, particularly for extended video sequences?
- Basis in paper: [inferred] The paper mentions that DyCoke dynamically prunes tokens at each decoding step to retain critical information, but does not explicitly analyze the impact on long-term coherence in video descriptions.
- Why unresolved: While the paper demonstrates improved accuracy and efficiency on various benchmarks, it lacks a detailed analysis of how dynamic pruning affects the narrative flow and coherence in long-form video descriptions.
- What evidence would resolve it: A comparative study analyzing the coherence and narrative consistency of video descriptions generated by DyCoke versus baseline models on extended video sequences would provide insights into the long-term effects of dynamic token pruning.

### Open Question 2
- Question: What is the optimal balance between temporal merging and dynamic KV cache pruning for different video content types (e.g., action-heavy vs. static scenes)?
- Basis in paper: [explicit] The paper mentions that DyCoke uses a two-stage approach with temporal merging and dynamic KV cache pruning, but does not provide a detailed analysis of how the optimal balance between these stages varies with video content.
- Why unresolved: Different video content types may have varying levels of temporal and spatial redundancy, which could affect the optimal pruning strategy. The paper does not explore how to adapt the balance between stages for different content types.
- What evidence would resolve it: Experiments comparing DyCoke's performance across different video content types (e.g., action-heavy vs. static scenes) with varying balances of temporal merging and dynamic pruning would help determine the optimal strategy for each content type.

### Open Question 3
- Question: How does DyCoke's performance scale with increasing video length and complexity, and what are the computational limitations of the approach?
- Basis in paper: [inferred] The paper demonstrates DyCoke's effectiveness on various benchmarks, but does not explicitly discuss its performance scaling with increasing video length and complexity or its computational limitations.
- Why unresolved: As video length and complexity increase, the computational demands and potential limitations of DyCoke's approach may become more apparent. The paper does not explore these aspects in detail.
- What evidence would resolve it: A comprehensive analysis of DyCoke's performance and computational efficiency across a wide range of video lengths and complexities, including stress tests on extremely long or complex videos, would provide insights into its scalability and limitations.

## Limitations

- Temporal merging hyperparameters are underspecified, making it difficult to reproduce optimal configurations across different video types
- Dynamic pruning sensitivity to video content complexity is not adequately addressed, potentially requiring content-specific parameter tuning
- Memory overhead of the Dynamic Pruning Cache mechanism is not quantified separately from main KV cache savings

## Confidence

- High confidence: The core mechanism of temporal token merging is well-established and the basic implementation approach is sound
- Medium confidence: The dynamic KV cache pruning mechanism shows promise but implementation details and parameter choices significantly impact performance
- Low confidence: The claim of superior performance across all tested model scales without model-specific tuning is questionable

## Next Checks

1. **Ablation study on temporal merging hyperparameters**: Systematically vary the sampling rate, sliding window size, and cosine similarity threshold to determine their individual impact on performance and identify optimal configurations for different video types (static vs. dynamic content).

2. **Memory overhead quantification**: Measure the exact memory consumption of the Dynamic Pruning Cache mechanism separately from the main KV cache, and calculate the net memory savings across different pruning rates to verify the claimed 1.4× reduction.

3. **Cross-model hyperparameter transfer**: Test whether the same K, L, and P values that work for the 0.5B model also work optimally for the 7B and 72B models, or if model-specific tuning is required to maintain the claimed performance improvements.