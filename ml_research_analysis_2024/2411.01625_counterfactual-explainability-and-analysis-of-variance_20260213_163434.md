---
ver: rpa2
title: Counterfactual explainability and analysis of variance
arxiv_id: '2411.01625'
source_url: https://arxiv.org/abs/2411.01625
tags:
- explainability
- counterfactual
- education
- causal
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes counterfactual explainability, a new method
  for attributing causal effects in complex systems. The approach extends functional
  ANOVA and Sobol's indices to handle dependent explanatory variables by incorporating
  causal DAGs.
---

# Counterfactual explainability and analysis of variance

## Quick Facts
- arXiv ID: 2411.01625
- Source URL: https://arxiv.org/abs/2411.01625
- Reference count: 40
- This paper proposes counterfactual explainability, a new method for attributing causal effects in complex systems with dependent explanatory variables.

## Executive Summary
This paper introduces counterfactual explainability, a novel approach for attributing variance explained by explanatory variables in complex systems with causal dependencies. The method extends functional ANOVA and Sobol's indices to handle dependent variables by incorporating causal DAGs and treating each variable's "intrinsic noise" as independent. Under a comonotonicity assumption, the method allows point identification and estimation of counterfactual explainability. Applied to US income inequality data, it reveals substantial explainability from sex and education, with notable age-related patterns. The method demonstrates consistency properties with respect to ancestral margins of the causal DAG.

## Method Summary
Counterfactual explainability extends global sensitivity analysis methods to dependent explanatory variables using causal DAGs. The method treats each variable's basic potential outcomes as independent in the causal Markov model, allowing variance decomposition even when variables are causally dependent. Under a comonotonicity assumption, point identification is achieved. Estimation involves specifying the causal DAG, estimating conditional distributions using methods like quantile regression, sampling basic potential outcomes, and computing counterfactual explainability through the explanation algebra framework. The approach provides a unified framework for various importance measures while maintaining ancestral consistency properties.

## Key Results
- Counterfactual explainability reveals substantial variance explained by sex (9.5-19.5%) and education (21.8-31.3%) in US income data
- The method demonstrates consistency properties with respect to ancestral margins of the causal DAG
- Different estimation methods (nonparametric regression, Gaussian noise, quantile regression) produce varying results for the same dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method extends Sobol's indices to dependent explanatory variables using causal DAGs.
- Mechanism: By treating each variable's "intrinsic noise" (basic potential outcomes) as independent in the causal Markov model, the method can decompose variance explained by subsets of variables even when they are causally dependent.
- Core assumption: The causal Markov model holds (basic potential outcomes are independent) and the comonotonicity assumption ensures point identification.
- Evidence anchors:
  - [abstract]: "Counterfactual explainability extends methods for global sensitivity analysis (including the functional analysis of variance and Sobol's indices), which assumes independent explanatory variables, to dependent explanations by using a directed acyclic graphs to describe their causal relationship."
  - [section 3.2]: "Our definition of counterfactual explainability requires a DAG causal model for the explanatory and outcome variables... By recursively applying (9), all potential outcomes are deterministic functions of the basic potential outcomes."
- Break condition: If the causal Markov assumption fails (basic potential outcomes are not independent) or comonotonicity does not hold, the method cannot point-identify counterfactual explainability.

### Mechanism 2
- Claim: The method provides ancestral consistency, maintaining explainability measures when expanding the causal DAG.
- Mechanism: When a subset of variables forms an ancestral set in the DAG, the total explainability of that subset to the outcome remains unchanged under expansion of the DAG, as long as the expanded DAG preserves the causal relationships.
- Core assumption: The DAG expansion preserves ancestral relationships and the marginal distribution on the original variables.
- Evidence anchors:
  - [section 3.3]: "Theorem 7 (Ancestral consistency)... SupposeW S is an ancestral set inG, andW S′ is the smallest ancestral set inG ′ that containsW S. Then we haveξG,P(∨j∈S Wj ⇒Y) =ξ G′,P′(∨j∈S ′Wj ⇒Y)."
  - [section 3.3]: "This is a special case of graph expansion for acyclic directed mixed graphs; see Zhao (2025). Heuristically, the expansion DAGG ′ offers finer explanations about causal relations between some variables in the original DAGG."
- Break condition: If the expansion DAG introduces new causal paths that violate the ancestral relationship or changes the marginal distribution of the original variables.

### Mechanism 3
- Claim: The method provides a unified framework for variable importance measures through the explanation algebra.
- Mechanism: By mapping explanatory variables to a Boolean algebra of explanation clauses, the method unifies various importance measures (Sobol's indices, Shapley values, superset importance) as different ways of attributing probability mass to these clauses.
- Core assumption: The explanatory variables are probabilistically independent (or can be treated as such through the causal Markov model).
- Evidence anchors:
  - [section 2.2]: "Theorem 2... Letξ 1 be any probability measure onE(W)such that... Letξ 2 be any probability measure onE(W)such that... Letξ 3 be any probability measure onE(W)such that... Letξ 4 by any probability measure onE(W)such that... Thenξ 1 =ξ 2 =ξ 3 =ξ 4."
  - [section 2.2]: "Table 1: Interpretation of the explanation algebra (K= 2)."
- Break condition: If the explanatory variables are not probabilistically independent and cannot be treated as such through the causal Markov model, the unification may not hold.

## Foundational Learning

- Concept: Causal Markov model and potential outcomes
  - Why needed here: The method relies on treating each variable's "intrinsic noise" (basic potential outcomes) as independent in the causal Markov model to decompose variance explained by subsets of variables.
  - Quick check question: What is the difference between a variable's potential outcome and its basic potential outcome in the context of the causal Markov model?

- Concept: Functional ANOVA and Sobol's indices
  - Why needed here: The method extends these global sensitivity analysis techniques to handle dependent explanatory variables through the use of causal DAGs.
  - Quick check question: How does the Hoeffding decomposition relate to Sobol's indices in the context of independent explanatory variables?

- Concept: Directed Acyclic Graphs (DAGs) and ancestral sets
  - Why needed here: The method uses DAGs to describe causal relationships between explanatory variables and outcomes, and ancestral consistency ensures explainability measures remain stable under DAG expansion.
  - Quick check question: What is an ancestral set in a DAG, and why is it important for the ancestral consistency property?

## Architecture Onboarding

- Component map:
  - Causal DAG specification -> Basic potential outcomes -> Explanation algebra -> Counterfactual explainability measure -> Estimation methods
  - DAG structure defines causal relationships between variables
  - Basic potential outcomes represent intrinsic noise treated as independent
  - Explanation algebra maps variables to Boolean clauses
  - Counterfactual explainability computes variance explained by variable subsets
  - Estimation methods include quantile regression, Gaussian models, and nonparametric regression

- Critical path:
  1. Specify the causal DAG and identify explanatory variables and outcome
  2. Estimate the conditional distributions of each variable given its parents in the DAG
  3. Sample basic potential outcomes using the estimated conditional distributions
  4. Compute counterfactual explainability using the explanation algebra and sampled potential outcomes

- Design tradeoffs:
  - Comonotonicity assumption: Ensures point identification but may not hold in all applications
  - Estimation method choice: Different methods (nonparametric regression, Gaussian noise, quantile regression) have different strengths and weaknesses in capturing heteroskedasticity and non-normal error structures
  - DAG specification: The choice of DAG can significantly impact the results, and there may be uncertainty about the true causal relationships

- Failure signatures:
  - Poor fit of estimation methods to the data (e.g., misspecified error distributions)
  - Violation of the causal Markov assumption or comonotonicity assumption
  - Sensitivity of results to the choice of DAG specification

- First 3 experiments:
  1. Verify the method on a simple linear model with known causal structure and compare results to Sobol's indices
  2. Test the method on a nonlinear model with dependent explanatory variables and compare results to existing variable importance measures
  3. Apply the method to a real-world dataset with a well-understood causal structure and validate the results using domain knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is counterfactual explainability to violations of the comonotonicity assumption for basic potential outcomes?
- Basis in paper: [explicit] The authors note that counterfactual explainability is generally only partially identified without comonotonicity and discuss how comonotonicity allows point identification (Section 4.1, Theorem 9).
- Why unresolved: The paper relies on comonotonicity for point identification but acknowledges this may not hold in practice. They only provide partial identification bounds in one example without exploring how violations affect estimates in real data.
- What evidence would resolve it: Simulation studies comparing comonotonicity-based estimates with bounds when comonotonicity is violated, and sensitivity analyses on real datasets showing how estimates change under different dependence structures.

### Open Question 2
- Question: How do different estimation methods for conditional quantile functions affect counterfactual explainability estimates in real applications?
- Basis in paper: [explicit] The authors compare three estimation methods (additive noise, heteroskedastic Gaussian noise, and conditional quantile regression) and show they produce different results for US income data (Section 4.2, C.2).
- Why unresolved: While they demonstrate differences, they don't systematically evaluate which method is most appropriate under different data conditions or provide guidance on method selection.
- What evidence would resolve it: A simulation study varying data characteristics (heteroskedasticity, tail behavior, non-normality) to identify which estimation method performs best under different scenarios, combined with diagnostic tools for method selection.

### Open Question 3
- Question: Can counterfactual explainability be extended to non-Euclidean explanatory variables while maintaining its theoretical properties?
- Basis in paper: [inferred] The authors mention in the Discussion that counterfactual explainability can be computed for non-Euclidean W as long as counterfactuals can be sampled from a generative model, but don't explore this extension.
- Why unresolved: The paper focuses on real-valued variables and doesn't investigate how the theory and estimation procedures would work for categorical, ordinal, or structured data types.
- What evidence would resolve it: Examples demonstrating counterfactual explainability calculations for various non-Euclidean data types, with theoretical analysis of how the axioms and consistency properties extend to these cases.

## Limitations
- The comonotonicity assumption is a strong requirement that may not hold in many real-world applications
- The method's reliance on accurate DAG specification introduces potential sensitivity to misspecification
- The conditional quantile regression approach may struggle with complex heteroskedastic relationships

## Confidence
High confidence: The theoretical framework extending Sobol's indices to dependent variables using causal DAGs is mathematically sound
Medium confidence: The estimation procedures and their implementation details, particularly the Monte Carlo sampling approach, require careful validation
Low confidence: The comonotonicity assumption's validity in real-world applications remains uncertain

## Next Checks
1. Conduct sensitivity analysis on DAG specification by testing alternative causal structures and measuring impact on explainability estimates
2. Validate estimation procedures using synthetic data with known causal relationships and compare results to ground truth
3. Test the method's performance under violation of the comonotonicity assumption by systematically varying the dependence structure in simulated datasets