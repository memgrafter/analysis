---
ver: rpa2
title: 'GNNAnatomy: Rethinking Model-Level Explanations for Graph Neural Networks'
arxiv_id: '2406.04548'
source_url: https://arxiv.org/abs/2406.04548
tags:
- graph
- explanations
- graphs
- graphlet
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GNNAnatomy, a distillation-based method
  for explaining Graph Neural Networks (GNNs) that addresses three key pitfalls in
  existing model-level approaches: (1) explanations optimized for classification confidence
  may overlook partially learned patterns, (2) single explanations fail to capture
  structural diversity within classes, and (3) explanations relying on black-box models
  lack transparency and trustworthiness. GNNAnatomy represents graphs as graphlet
  frequency vectors and trains a transparent multilayer perceptron to directly approximate
  GNN predictions.'
---

# GNNAnatomy: Rethinking Model-Level Explanations for Graph Neural Networks

## Quick Facts
- arXiv ID: 2406.04548
- Source URL: https://arxiv.org/abs/2406.04548
- Reference count: 17
- Authors: Hsiao-Ying Lu; Yiran Li; Ujwal Pratap Krishna Kaluvakolanu Thyagarajan; Kwan-Liu Ma
- One-line primary result: GNNAnatomy generates interpretable, in-distribution, multi-grained explanations that faithfully capture GNN behavior across diverse graph instances

## Executive Summary
This paper introduces GNNAnatomy, a distillation-based method for explaining Graph Neural Networks (GNNs) that addresses three key pitfalls in existing model-level approaches: explanations optimized for classification confidence may overlook partially learned patterns, single explanations fail to capture structural diversity within classes, and explanations relying on black-box models lack transparency and trustworthiness. GNNAnatomy represents graphs as graphlet frequency vectors and trains a transparent multilayer perceptron to directly approximate GNN predictions. Explanations are derived from the graphlets whose frequencies contribute most to the MLP's activations. A visual analytics interface enables human-AI collaboration to identify subsets of graphs with distinct structural traits, generating multi-grained explanations at user-specified granularity.

## Method Summary
GNNAnatomy addresses model-level explanation challenges through a distillation framework. The method first computes graphlet frequency vectors (using 3-5 node graphlets) to represent graph topology. A transparent MLP student is trained to directly approximate the probability outputs of a trained GNN teacher model. Explanations are extracted by identifying graphlets whose frequencies most strongly influence the MLP's predictions. The method uses fidelity metrics (fid- for sufficiency, fid+ for necessity) to validate explanation quality and Maximum Mean Discrepancy to ensure explanations remain in-distribution with real graphs. A visual analytics interface supports human-AI collaboration to identify graph subsets requiring different substructures for classification, enabling multi-grained explanations.

## Key Results
- Fidelity metrics show explanations are both sufficient and necessary, with low fid- and high fid+ scores
- Maximum Mean Discrepancy scores indicate explanations remain in-distribution with real graphs
- The approach provides trustworthy, in-distribution, multi-grained explanations that faithfully capture GNN behavior across diverse graph instances
- Visual analytics interface successfully identifies graph subsets with distinct structural traits requiring different substructures for classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNNAnatomy generates explanations that are both sufficient and necessary for GNN predictions
- Mechanism: The method uses fidelity metrics (fid- for sufficiency and fid+ for necessity) to validate that explanations capture the essential graphlet substructures the GNN relies on for classification
- Core assumption: Removing explanatory substructures from graphs will decrease classification confidence, while removing non-explanatory substructures will not
- Evidence anchors:
  - [abstract]: "Fidelity metrics show that explanations are both sufficient and necessary, with low fid- and high fid+ scores"
  - [section]: "To assess explanation sufficiency, we remove edges not involved in any explanatory substructures (fid-). To evaluate necessity, we remove edges participating in explanatory graphlets (fid+)"

### Mechanism 2
- Claim: GNNAnatomy generates explanations that remain in-distribution with real graphs
- Mechanism: By using graphlet frequency vectors derived directly from original graphs and training an interpretable MLP to approximate GNN behavior, the explanations maintain distributional similarity with the input data
- Core assumption: Graphlet frequencies extracted from real graphs will capture the essential topological features that GNNs learn
- Evidence anchors:
  - [abstract]: "Maximum Mean Discrepancy scores indicate explanations remain in-distribution"
  - [section]: "The computed graphlet frequency vectors are fed into a multilayer perceptron (MLP student), which is trained to directly approximate the probability output of the trained GNN"

### Mechanism 3
- Claim: The visual analytics interface enables multi-grained explanations by identifying subsets with structural diversity
- Mechanism: The projection map reveals clusters of graphs with different topological traits, allowing users to select subsets where different substructures drive class differentiation
- Core assumption: Graphs with similar embeddings (x-axis) but different graphlet frequencies (y-axis) require different substructures for classification
- Evidence anchors:
  - [abstract]: "To account for structural diversity within a class, GNNAnatomy generates explanations at the required granularity through an interface that supports human-AI collaboration"
  - [section]: "The projection map, as shown in Fig. 3, demonstrate how to reveal subsets of graphs that require different substructures for GNNs to differentiate between classes"

## Foundational Learning

- Concept: Graphlets and graphlet frequency vectors
  - Why needed here: GNNAnatomy uses graphlets as the fundamental substructures to characterize graph topology, which forms the basis for all explanations
  - Quick check question: What is the difference between a 4-node cycle graphlet and a 4-node path graphlet in terms of their frequency vectors?

- Concept: Knowledge distillation in neural networks
  - Why needed here: GNNAnatomy employs distillation by training an interpretable MLP to approximate the GNN's behavior, enabling transparent explanation generation
  - Quick check question: How does using a single-layer MLP with no nonlinear activations contribute to the transparency of explanations?

- Concept: Maximum Mean Discrepancy (MMD) for distribution comparison
  - Why needed here: MMD is used to validate that explanation embeddings remain in-distribution with original graph embeddings
  - Quick check question: Why would an MMD score close to zero indicate that explanations are in-distribution with real graphs?

## Architecture Onboarding

- Component map: Graph → Graphlet frequencies → MLP prediction → Explanation identification → Fidelity validation → Interface presentation
- Critical path: Graph → Graphlet frequencies → MLP prediction → Explanation identification → Fidelity validation → Interface presentation
- Design tradeoffs: The choice of 3-5 node graphlets balances interpretability with structural expressiveness; larger graphlets would capture more complex patterns but become uninterpretable
- Failure signatures: Explanations that are not in-distribution (high MMD), explanations that fail fidelity metrics (high fid- or low fid+), or interface projections that don't separate classes meaningfully
- First 3 experiments:
  1. Run GNNAnatomy on BA-2Motif dataset and verify that the contrasting explanation correctly identifies the cycle vs house motif difference
  2. Test fidelity metrics by perturbing edges in MUTAG dataset and confirming that removing explanatory substructures changes classification more than removing non-explanatory ones
  3. Use the visual analytics interface to identify graph subsets in Reddit-Binary and verify that different subsets yield different contrasting explanations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond its future work directions.

## Limitations
- The method's effectiveness depends on the assumption that graphlet frequencies sufficiently characterize graph structure for GNN decision-making
- Limited to topology-based explanations; cannot capture attribute-dependent GNN behavior
- Computational complexity increases with graph size due to graphlet enumeration

## Confidence
Our confidence in the core claims is **Medium-High**. The method's theoretical foundation is sound, and fidelity metrics demonstrate that explanations capture GNN behavior. However, several limitations affect generalizability. The approach assumes GNNs primarily learn from graph topology rather than node/edge attributes, which may not hold for all datasets. The 3-5 node graphlet representation, while interpretable, may miss higher-order structural patterns that influence GNN predictions. Additionally, the visual analytics interface requires manual interpretation, which could introduce user bias in identifying meaningful graph subsets.

## Next Checks
1. Test GNNAnatomy on datasets where GNNs rely heavily on node/edge attributes to assess method robustness beyond pure topology
2. Compare explanation quality and interpretability against alternative model-level explanation methods like GNNExplainer or PGExplainer on benchmark datasets
3. Conduct ablation studies varying graphlet size ranges (e.g., 2-4 nodes vs 3-5 nodes) to quantify the tradeoff between interpretability and completeness