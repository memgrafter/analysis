---
ver: rpa2
title: Context-Aware Clustering using Large Language Models
arxiv_id: '2405.00988'
source_url: https://arxiv.org/abs/2405.00988
tags:
- clustering
- entity
- loss
- supervised
- triplet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses supervised clustering of entity subsets using
  large language models (LLMs). It proposes CACTUS, a method that leverages context-aware
  entity embeddings via a scalable inter-entity attention mechanism to capture the
  context of entity subsets.
---

# Context-Aware Clustering using Large Language Models

## Quick Facts
- arXiv ID: 2405.00988
- Source URL: https://arxiv.org/abs/2405.00988
- Reference count: 30
- Primary result: CACTUS outperforms baselines by 12.3%-26.8% (AMI) and 15.3%-28.2% (ARI)

## Executive Summary
This paper introduces CACTUS, a method for supervised clustering of entity subsets using large language models. The approach leverages context-aware entity embeddings through a scalable inter-entity attention mechanism, an augmented triplet loss function, and self-supervised clustering tasks. Experiments on e-commerce datasets demonstrate significant improvements over unsupervised and supervised baselines, achieving 12.3%-26.8% and 15.3%-28.2% gains in AMI and ARI metrics respectively.

## Method Summary
CACTUS fine-tunes a Flan-T5-base encoder to produce context-aware entity embeddings using a scalable inter-entity attention (SIA) mechanism. The model computes entity embeddings by pooling tokens within each entity while aggregating across entities, then applies an augmented triplet loss with a learnable neutral edge similarity. Self-supervised clustering via text augmentation is used for pretraining. Final clustering predictions are generated through agglomerative clustering on cosine similarities between entity embeddings.

## Key Results
- CACTUS achieves 12.3%-26.8% improvement in AMI over baselines
- CACTUS achieves 15.3%-28.2% improvement in ARI over baselines
- Superior performance on e-commerce query and product datasets

## Why This Works (Mechanism)

### Mechanism 1
Context-aware entity embeddings capture the specific circumstances of an entity subset, improving clustering accuracy. The SIA mechanism computes a single representative embedding per entity that interacts with aggregated representations of other entities, preserving intra-entity token order but ignoring inter-entity order.

### Mechanism 2
The augmented triplet loss resolves non-overlapping margin locations across different triplets, leading to more stable clustering boundaries. It adds a learnable neutral edge similarity to the complete graph of entities, constraining intra-cluster similarities to be higher and inter-cluster similarities to be lower than this neutral reference.

### Mechanism 3
Self-supervised clustering via text augmentation improves generalization when ground truth clusterings are limited. The approach randomly samples seed entities, generates clusters by dropping words from seed descriptions, and pretrains the model on these synthetic clusterings before fine-tuning.

## Foundational Learning

- **Concept**: Triplet loss and its margin-based objective
  - Why needed here: The core training objective for supervised clustering relies on pulling positive pairs together and pushing negative pairs apart; understanding triplet loss mechanics is essential to grasp why the augmented version is needed.
  - Quick check question: In a triplet loss, what happens if the margin is set too small relative to the similarity range of the data?

- **Concept**: Transformer self-attention and multi-head attention
  - Why needed here: The SIA mechanism modifies standard self-attention to be scalable; knowing how attention works and why full attention is O(N²L²) is key to understanding the trade-offs.
  - Quick check question: How does pooling token embeddings within an entity change the computational complexity compared to full self-attention?

- **Concept**: Agglomerative clustering and linkage criteria
  - Why needed here: The final predicted clustering is produced by average-link agglomerative clustering on cosine similarities; understanding how thresholds affect cluster merges is important for interpreting results.
  - Quick check question: What is the effect of choosing a very high versus very low threshold in average-link agglomerative clustering?

## Architecture Onboarding

- **Component map**: Tokenizer -> Flan-T5-base encoder (modified with SIA) -> entity embeddings -> cosine similarity matrix -> agglomerative clustering (inference) / loss computation (training)
- **Critical path**: Tokenizer -> SIA encoder -> embeddings -> similarity matrix -> agglomerative clustering (inference) / loss computation (training)
- **Design tradeoffs**: SIA vs FIA: SIA saves memory and is faster but may lose some fine-grained inter-entity interactions; Augmented triplet vs standard triplet: Augmented version is more stable but introduces an extra learnable parameter (neutral similarity); Self-supervision: Helps when labeled data is scarce but adds pretraining overhead
- **Failure signatures**: If embeddings collapse (all vectors similar), agglomerative clustering will produce one cluster; If neutral similarity in augmented triplet loss saturates to 0 or 1, margin constraints fail; If SIA pooling is too aggressive, entities from different clusters may become indistinguishable
- **First 3 experiments**: Replace SIA with NIA (no inter-entity attention) and compare AMI/ARI on validation set; Train with standard triplet loss (no neutral edge) and observe clustering stability; Disable self-supervised pretraining and measure performance drop on small training sets

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed context-aware entity embedding approach perform on datasets with entities that have very long textual descriptions? The paper mentions that traditional full attention over all entities in a subset can become computationally expensive as subsets grow large, and proposes a scalable inter-entity attention mechanism, but does not provide empirical results or analysis of the proposed method's performance on datasets with entities having very long textual descriptions.

### Open Question 2
How does the augmented triplet loss function perform compared to other loss functions specifically designed for supervised clustering tasks? The paper introduces an augmented triplet loss function to address challenges encountered when directly applying triplet loss to supervised clustering, but only compares it with a few other loss functions and does not provide a comprehensive comparison with other loss functions specifically designed for supervised clustering.

### Open Question 3
How does the proposed method perform on datasets with entities from diverse domains or languages? The paper evaluates the proposed method on e-commerce query and product datasets, but does not explore its performance on datasets from diverse domains or languages.

## Limitations

- **Context Generalization**: The SIA mechanism's ability to capture context has not been tested on entities with high semantic ambiguity or requiring fine-grained inter-entity order preservation
- **Augmented Triplet Loss Stability**: The paper does not report whether the neutral similarity parameter converged to a stable value during training or if it exhibited high variance across runs
- **Ground Truth Quality**: Clustering labels are generated by prompting a closed-source LLM, but the exact prompt and consistency checks are not disclosed, raising concerns about label noise

## Confidence

- **SIA Improves Clustering Accuracy**: Medium confidence
- **Augmented Triplet Loss Stabilizes Training**: Medium confidence
- **Self-Supervised Pretraining Improves Generalization**: Low confidence

## Next Checks

1. **SIA vs FIA Ablation**: Replace SIA with FIA (full inter-entity attention) on a small subset of the data and measure the trade-off between clustering performance (AMI/ARI) and training memory/time

2. **Neutral Similarity Monitoring**: During training with the augmented triplet loss, log the learned neutral similarity value across epochs and across runs

3. **Self-Supervision Ablation with Controlled Augmentation**: Disable self-supervised pretraining and compare performance on small training sets, then replace the word-dropping augmentation with a simpler method (e.g., synonym replacement) to test whether the specific augmentation is critical