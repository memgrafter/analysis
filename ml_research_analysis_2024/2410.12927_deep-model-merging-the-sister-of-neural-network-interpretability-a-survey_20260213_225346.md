---
ver: rpa2
title: 'Deep Model Merging: The Sister of Neural Network Interpretability -- A Survey'
arxiv_id: '2410.12927'
source_url: https://arxiv.org/abs/2410.12927
tags:
- merging
- arxiv
- neural
- loss
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey systematically examines model merging techniques through\
  \ the lens of loss landscape geometry, identifying four key characteristics\u2014\
  mode convexity, determinism, directedness, and connectivity\u2014that govern neural\
  \ network training and learned representations. The authors categorize merging methods\
  \ into ensembling, weight aggregation, and neuron alignment approaches, revealing\
  \ how these techniques can be used to enhance model interpretability by mapping\
  \ parameters to semantic behaviors and improve robustness against adversaries."
---

# Deep Model Merging: The Sister of Neural Network Interpretability -- A Survey

## Quick Facts
- arXiv ID: 2410.12927
- Source URL: https://arxiv.org/abs/2410.12927
- Reference count: 40
- Primary result: Systematic survey of model merging techniques through loss landscape geometry lens, identifying key characteristics that govern neural network training and learned representations

## Executive Summary
This survey examines model merging techniques through the lens of loss landscape geometry, revealing how neural network training and learned representations are governed by four key characteristics: mode convexity, determinism, directedness, and connectivity. The authors categorize merging methods into ensembling, weight aggregation, and neuron alignment approaches, demonstrating how these techniques enhance model interpretability by mapping parameters to semantic behaviors and improve robustness against adversaries. By connecting empirical observations to theoretical understanding of loss landscapes, the survey provides a foundation for future research at the intersection of model merging, interpretability, and robustness.

## Method Summary
The paper conducts a systematic literature review of 40+ papers on model merging techniques, synthesizing empirical findings to extract principles about neural network training and learned representations. It categorizes existing model merging techniques into three major groups (ensembling, weight aggregation, and neuron alignment) and characterizes four key loss landscape phenomena (mode convexity, determinism, directedness, and connectivity). The method involves synthesizing observations from surveyed papers to understand how these principles enable predictable interventions in model behavior, identify task-specific parameters, and mitigate vulnerabilities in federated learning and black-box settings.

## Key Results
- Model merging success depends on mode convexity in the loss landscape, where solutions within an objective basin can be linearly interpolated
- Neuron alignment resolves permutation symmetries that prevent linear mode connectivity between independently trained models
- Mode directedness allows task-specific behavior to be associated with directions in parameter space, enabling weight steering for predictable behavior changes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Model merging success depends on mode convexity in the loss landscape, where solutions within an objective basin can be linearly interpolated to find equally performant models.
- **Mechanism**: When neural networks converge to solutions in the same convex objective basin, their parameters lie along a low-loss path. Weight averaging or interpolation between these parameters preserves performance because the underlying loss landscape is locally convex.
- **Core assumption**: The final linearly connected objective basin is determined early in training, and models sharing significant training trajectory will converge to the same basin regardless of SGD noise.
- **Evidence anchors**:
  - [abstract] Identifies "mode convexity, determinism, directedness, and connectivity" as key characteristics governing training and learned representations.
  - [section] States "Objective basins are locally convex, permitting one to travel a short distance and still encounter a meaningful neural network solution with similar behavior" (Section 6.1.1).
  - [corpus] Weak evidence - no direct corpus papers specifically address mode convexity in model merging context.
- **Break condition**: Merging fails when models lie in different objective basins or when training trajectories diverge significantly before reaching stable points.

### Mechanism 2
- **Claim**: Neuron alignment resolves permutation symmetries that prevent linear mode connectivity between independently trained models.
- **Mechanism**: Neural networks have inherent permutation symmetries where reordering neurons produces behaviorally equivalent models. When independently trained models have permuted neuron orderings, direct parameter averaging destroys information. Alignment techniques find permutation matrices or soft mappings to reorder neurons before merging, enabling linear interpolation between models.
- **Core assumption**: Permutation symmetries dominate the loss landscape of overparameterized models, and many global minima are equivalent up to permutations.
- **Evidence anchors**:
  - [abstract] Mentions "symmetries in neural network architectures" and how they lead to "behaviorally equivalent but mathematically distinct solutions."
  - [section] Explains "Any permutation in the order of neurons would still result in the same activations for that layer" and that "many merging works provide substantial empirical evidence that aligning neurons by learning appropriate permutations in their order reveals linearly connected paths between models" (Section 2.2.2, 5.1).
  - [corpus] Weak evidence - corpus papers don't directly address permutation symmetry in model merging.
- **Break condition**: Merging fails when models are too dissimilar in learned features or when permutation alignment cannot be computed efficiently.

### Mechanism 3
- **Claim**: Mode directedness allows meaningful task-specific behavior to be associated with directions in parameter space, enabling weight steering for predictable model behavior changes.
- **Mechanism**: Within an objective basin, specific directions in parameter space correspond to task-specific behaviors. Task vectors (differences between pre-trained and fine-tuned weights) can be added or subtracted to enhance or reduce performance on specific tasks. This creates a sparse set of directions that capture task-specific knowledge.
- **Core assumption**: Task-specific behavior can be represented as sparse directions in weight space, and models can be manipulated by arithmetic operations on these task vectors.
- **Evidence anchors**:
  - [abstract] States that merging techniques "yield procedures for mapping neural network components to task-specific behaviors" and discusses "weight steering" as a category.
  - [section] Explains "A task vector—defined as the difference between the pre-trained and fine-tuned model weights for a task... can be thought of as the direction in weight space that the pre-trained model must travel to achieve better accuracy on that particular task" (Section 4.2).
  - [corpus] Weak evidence - corpus papers don't directly address task vectors or weight steering.
- **Break condition**: Merging fails when task vectors interfere destructively or when task-specific parameters cannot be isolated from generic parameters.

## Foundational Learning

- **Concept**: Loss landscape geometry and mode connectivity
  - **Why needed here**: Understanding how neural networks converge to solutions and how those solutions relate to each other in parameter space is fundamental to why model merging works or fails.
  - **Quick check question**: What does it mean for two models to exhibit "Linear Mode Connectivity" and why is this important for model merging?

- **Concept**: Permutation symmetry in neural networks
  - **Why needed here**: Neural networks have inherent symmetries where neuron reordering doesn't change behavior, but this complicates direct parameter averaging between models.
  - **Quick check question**: Why does naively averaging parameters between two independently trained models often lead to poor performance?

- **Concept**: Task-specific directions in parameter space
  - **Why needed here**: Understanding how specific behaviors map to directions in weight space enables techniques like task arithmetic and weight steering for controlled model behavior modification.
  - **Quick check question**: How does the concept of a "task vector" enable predictable control over model behavior through weight manipulation?

## Architecture Onboarding

- **Component map**: Source models → Permutation alignment → Weight aggregation strategy → Merged model validation
- **Critical path**: Load source models → Detect permutation misalignment → Apply alignment (hard or soft) → Aggregate weights using appropriate strategy → Validate merged model performance
- **Design tradeoffs**: Simple averaging is fast but fragile; alignment methods are robust but computationally expensive; task arithmetic enables precise control but requires careful task vector isolation
- **Failure signatures**: High loss barrier between merged models, performance degradation on individual tasks, inability to align neurons due to architectural differences, task interference when merging multiple task vectors
- **First 3 experiments**:
  1. Load two fine-tuned models from the same pre-trained checkpoint and perform simple weight averaging to verify mode convexity holds within the same objective basin.
  2. Take two independently trained models and apply neuron alignment before weight averaging to test if permutation symmetries prevent successful merging.
  3. Create task vectors from pre-trained and fine-tuned models, then add/subtract them to test weight steering capabilities for controlled behavior modification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we develop model merging techniques that work effectively across fundamentally different tasks without requiring data or backpropagation?
- Basis in paper: [inferred] The paper discusses challenges in merging models trained on different tasks, noting that methods purely based on weight manipulation often fail. It mentions that Yamada et al. [130] conjecture that growing differences in loss geometry require optimization over mixed-task datasets, while ZipIt! [107] avoids merging task-specific components altogether.
- Why unresolved: Current approaches either require data samples and backpropagation (like STE [2] and AdaMerge [132]) or avoid the problem by not merging task-specific components (like ZipIt! [107]). There's no clear solution for merging across tasks while preserving task-specific knowledge.
- What evidence would resolve it: Development of a merging technique that successfully combines models trained on distinct tasks without access to training data or gradient-based optimization, validated across multiple task pairs with different data distributions.

### Open Question 2
- Question: How does the pre-training objective causally affect loss landscape geometry and subsequent model robustness?
- Basis in paper: [explicit] The paper explicitly discusses how models with different pre-training objectives (like T5 vs DeBERTa) show different sensitivities to merging techniques. It mentions that contrastive pre-training has been observed to bias models toward flatter objective basins associated with better generalization and robustness.
- Why unresolved: While correlations between pre-training objectives, loss landscape geometry, and robustness have been observed, the causal mechanisms remain unclear. The paper notes this as an important direction for future work.
- What evidence would resolve it: Controlled experiments varying pre-training objectives while holding other factors constant, measuring resulting changes in loss landscape geometry (flatness, connectivity) and model robustness, establishing causal relationships.

### Open Question 3
- Question: Can we develop interpretability methods that leverage weight space analysis instead of activation-based techniques to provide more robust and generalizable explanations?
- Basis in paper: [explicit] The paper discusses how insights from model merging demonstrate the viability of weight steering as an alternative to activation-based interpretability techniques. It notes that weight steering circumvents the need for extensive tuning and provides predictable control over model behavior.
- Why unresolved: While weight steering methods show promise, the paper suggests there's potential to develop more comprehensive interpretability frameworks based on weight space analysis. Current interpretability research heavily focuses on activation spaces, which may be brittle and dependent on inference data.
- What evidence would resolve it: Development and validation of interpretability methods that successfully decompose model behavior into interpretable weight space components, demonstrating robustness across different inputs and generalization to unseen tasks.

## Limitations
- Limited direct experimental validation of theoretical claims about loss landscape geometry
- Reliance on empirical observations from prior work without new experimental evidence
- Claims about task vector manipulation effectiveness remain largely theoretical with limited practical validation

## Confidence
- **High**: Model merging techniques can be categorized into ensembling, weight aggregation, and neuron alignment approaches
- **Medium**: Loss landscape characteristics (convexity, determinism, directedness, connectivity) influence model merging success
- **Medium**: Permutation symmetry is a significant challenge requiring alignment techniques for successful merging
- **Low**: Task vectors enable precise control over model behavior through weight manipulation

## Next Checks
1. Conduct empirical studies measuring mode connectivity barriers between merged models across different architectures and training regimes
2. Systematically evaluate neuron alignment algorithms' performance when merging models with varying degrees of architectural similarity
3. Design experiments to quantify the effectiveness of task vector manipulation in controlling specific behaviors while preserving general capabilities