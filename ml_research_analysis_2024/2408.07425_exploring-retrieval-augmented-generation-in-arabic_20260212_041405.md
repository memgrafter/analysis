---
ver: rpa2
title: Exploring Retrieval Augmented Generation in Arabic
arxiv_id: '2408.07425'
source_url: https://arxiv.org/abs/2408.07425
tags:
- arabic
- dataset
- were
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the performance of various semantic embedding
  models and large language models (LLMs) for Arabic text retrieval and generation
  within Retrieval-Augmented Generation (RAG) pipelines. Two datasets were used: the
  Arabic EduText Secondary School dataset (ArEduText) and the Arabic Reading Comprehension
  Dataset (ARCD).'
---

# Exploring Retrieval Augmented Generation in Arabic

## Quick Facts
- arXiv ID: 2408.07425
- Source URL: https://arxiv.org/abs/2408.07425
- Reference count: 0
- This work investigates the performance of various semantic embedding models and LLMs for Arabic text retrieval and generation within RAG pipelines, finding that E5-large and BGE models achieved the highest retrieval performance across both datasets tested.

## Executive Summary
This study systematically evaluates semantic embedding models and large language models for Arabic text retrieval and generation in RAG pipelines. Using two datasets - Ar_EduText (secondary school passages) and ARCD (Wikipedia articles) - the researchers tested multiple embedding models including OpenAI's Ada, Microsoft's E5, Cohere, AraBERT, AraVec, BGE, Ollama, and JAIS. For generation, they evaluated GPT-3.5 Turbo, Llama 3, Mistral, Mixtral, and JAIS. The results demonstrate that Microsoft's E5-large and BGE models achieved the highest retrieval performance across both datasets, with particular resilience to dialectal variations between queries and documents. Among generators, GPT-3.5 Turbo performed best on Ar_EduText while Mistral excelled on ARCD.

## Method Summary
The researchers conducted experiments in two stages. First, they embedded all document passages using each semantic model and stored them in ChromaDB, then retrieved the top-5 matches for each query to calculate recall@k and MRR metrics. Second, using the best-performing retrieval model (E5-large), they tested each generator model by having them produce answers from retrieved context and evaluating output quality using F1 score, BLEU, and cosine similarity against gold answers. The study also explored automatic query disambiguation to improve retrieval performance when questions depend on context.

## Key Results
- E5-large and BGE models achieved the highest retrieval performance across both Ar_EduText and ARCD datasets
- These models demonstrated resilience to dialectal variations between document and query languages
- GPT-3.5 Turbo performed best on Ar_EduText while Mistral excelled on ARCD for generation tasks
- Automatic query disambiguation improved retrieval performance for most models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic embedding models with multilingual pretraining can retrieve Arabic text effectively even when document and query are in different dialects
- Mechanism: The embedding space maps semantically similar phrases from different Arabic dialects (e.g., MSA vs. Egyptian) close together, so cosine similarity still captures relevance
- Core assumption: Multilingual models embed cross-dialectal semantic similarity well enough for retrieval to succeed
- Evidence anchors:
  - [abstract] "E5-large and BGE also demonstrated resilience to dialectal variations in queries."
  - [section] Experiment 2 showed E5-Large and BGE retained high recall@5 even with Egyptian dialect queries
  - [corpus] Weak — no direct corpus evidence; relies on experimental observation
- Break condition: If dialectal divergence is too large for the embedding space to bridge (e.g., Maghrebi vs. MSA), retrieval accuracy drops sharply

### Mechanism 2
- Claim: Disambiguating dependent questions improves retrieval by reducing semantic ambiguity
- Mechanism: Automatic disambiguation rewrites context-dependent queries into self-contained ones, allowing the embedding model to match the correct segment without needing prior context
- Core assumption: The disambiguator preserves or improves the semantic intent while making the query standalone
- Evidence anchors:
  - [abstract] "the study also touches upon the issue of variations between document dialect and query dialect in the retrieval stage."
  - [section] Experiment 3: both GPT-3.5 auto-disambiguation and manual edits increased recall@5 for most models
  - [corpus] No corpus evidence; improvement inferred from controlled experiment
- Break condition: Over-aggressive rewriting changes the question's meaning, lowering recall

### Mechanism 3
- Claim: Open-source LLMs can generate Arabic QA responses with comparable quality to GPT-3.5 Turbo when fine-tuned or prompted appropriately
- Mechanism: The LLM takes retrieved context plus a prompt and synthesizes a concise answer; cosine similarity between generated and gold embeddings measures semantic quality
- Core assumption: The LLM's Arabic generation capability is sufficient when given relevant context
- Evidence anchors:
  - [abstract] "GPT-3.5 Turbo performed best on Ar_EduText, while Mistral excelled on ARCD."
  - [section] Generator experiment: GPT-3.5 Turbo > Llama3 > Mistral on Ar_EduText; Mistral > Llama3 on ARCD
  - [corpus] Weak — corpus neighbors do not directly address Arabic generation quality
- Break condition: If the LLM cannot handle Arabic morphology or produces hallucinated content, cosine/BLEU scores drop

## Foundational Learning

- Concept: Semantic embeddings and cosine similarity
  - Why needed here: The retriever relies on embeddings to score document relevance
  - Quick check question: If two Arabic sentences have cosine similarity 0.95, are they semantically identical? (No — high similarity indicates strong relatedness but not identical meaning.)

- Concept: Dialectal variation in Arabic
  - Why needed here: The study tests retrieval robustness across MSA and Egyptian Arabic
  - Quick check question: Does Egyptian Arabic use the same vocabulary as MSA? (No — many lexical differences exist.)

- Concept: Precision/Recall/F1 in QA evaluation
  - Why needed here: These metrics quantify how well generated answers match gold answers
  - Quick check question: If a model always outputs the gold answer verbatim, what is its F1 score? (1.0.)

## Architecture Onboarding

- Component map: Query → Embedding model → Vector store (Chroma) → Top-K retrieval → LLM generator → Post-processing → Evaluation
- Critical path: Embedding generation → Retrieval → Generation → Evaluation
- Design tradeoffs: Using a quantized model (JAIS) trades speed/size for accuracy; higher k in retrieval increases recall but may overwhelm LLM context limits
- Failure signatures: Low recall@1 but high recall@5 suggests embeddings capture semantics but not exact match; consistently low BLEU/Cosine indicates generation quality issues
- First 3 experiments:
  1. Replicate recall@k ranking with a held-out subset of Ar_EduText to confirm E5-Large/BGE lead
  2. Test dialect resilience on a larger, multi-dialect dataset to validate robustness
  3. Apply prompt engineering to the best LLM and measure change in cosine similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do various semantic embedding models perform on larger Arabic datasets with more diverse dialectal representations?
- Basis in paper: [explicit] The authors acknowledge that their experiments were conducted on relatively small datasets and used only one dialect, noting that future work should explore larger datasets and a wider variety of dialects
- Why unresolved: The study's limited dataset size and dialectal coverage prevent definitive conclusions about model performance across the full spectrum of Arabic dialects and larger-scale applications
- What evidence would resolve it: Systematic evaluation of top-performing models (E5-large, BGE) on significantly larger Arabic datasets containing multiple dialects, measuring retrieval accuracy and dialect resilience across different regional variations

### Open Question 2
- Question: What impact does fine-tuning have on the performance of open-source LLMs for Arabic generation tasks in RAG pipelines?
- Basis in paper: [inferred] The authors suggest that future work should investigate the role of prompt engineering and fine-tuning to increase the performance of open-source LLMs like Llama3 and Mistral for Arabic generation
- Why unresolved: The study used off-the-shelf models without any fine-tuning or extensive prompt engineering, leaving the potential performance gains from these techniques unexplored
- What evidence would resolve it: Comparative experiments measuring the performance of fine-tuned versions of Llama3 and Mistral against their base models on Arabic QA tasks, using both automatic metrics and human evaluation

### Open Question 3
- Question: How does the performance of JAIS models compare to other embedding models when using the full model rather than the quantized version?
- Basis in paper: [explicit] The authors note that the quantized version of JAIS performed poorly in their experiments, but acknowledge they couldn't test the full model due to computational constraints, stating "no concrete conclusion can be reached in terms of JAIS except to say that other models were easier to use"
- Why unresolved: The study only tested the quantized version of JAIS due to resource limitations, preventing assessment of the full model's capabilities for Arabic text representation
- What evidence would resolve it: Direct comparison of the full JAIS model against top-performing models (E5-large, BGE) on the same Arabic retrieval tasks, measuring recall, MRR, and dialect resilience metrics

## Limitations

- Experiments conducted on relatively small datasets (158 and 1,395 QA pairs), limiting generalizability to larger-scale applications
- Focus on only two Arabic dialects (MSA and Egyptian), leaving performance across other dialects (Maghrebi, Gulf, etc.) unknown
- Generation quality evaluation relied solely on embedding-based metrics rather than human judgment or established QA benchmarks

## Confidence

- **High** confidence: Semantic models with multilingual pretraining (E5-large, BGE) retrieve Arabic text more effectively than monolingual models in RAG pipelines
- **Medium** confidence: E5-large and BGE are robust to dialectal variations between document and query in Arabic retrieval
- **Low** confidence: Open-source LLMs can generate Arabic QA responses with comparable quality to GPT-3.5 Turbo; improvement from automatic query disambiguation is statistically significant

## Next Checks

1. Replicate the retrieval experiments on a larger, multi-dialect Arabic dataset (e.g., ARCD supplemented with Gulf and Maghrebi dialect questions) to confirm robustness of E5-large and BGE to dialectal variation
2. Conduct a human evaluation study comparing generated answers from GPT-3.5 Turbo, Mistral, and Llama 3 on a held-out subset of Ar_EduText to validate embedding-based metrics and establish a gold standard for Arabic RAG output quality
3. Perform an ablation study on the automatic query disambiguation method, measuring recall@5 with and without disambiguation across all semantic models to determine statistical significance and potential for over-aggressive rewriting