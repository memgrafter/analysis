---
ver: rpa2
title: 'To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal
  Large Language Models'
arxiv_id: '2410.06765'
source_url: https://arxiv.org/abs/2410.06765
tags:
- perception
- connectors
- tasks
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates connector selection in multimodal
  large language models (MLLMs) across tasks of varying perception granularities.
  The authors classify connectors into feature-preserving and feature-compressing
  types, then conduct extensive experiments using three comprehensive benchmarks (MMBench,
  MME, SEED-Bench) at different image resolutions (224, 336, 448).
---

# To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2410.06765
- Source URL: https://arxiv.org/abs/2410.06765
- Reference count: 11
- Primary result: Feature-preserving connectors excel in fine-grained perception while feature-compressing connectors offer significant speed advantages

## Executive Summary
This paper presents a comprehensive study of connector selection in multimodal large language models (MLLMs), systematically evaluating how different connector architectures affect performance across tasks of varying perception granularity. The authors classify connectors into feature-preserving and feature-compressing types, then conduct extensive experiments using three comprehensive benchmarks at different image resolutions. Their findings reveal that connector choice significantly impacts both performance and computational efficiency, with feature-preserving connectors excelling in fine-grained perception tasks while feature-compressing connectors offer substantial speed advantages with comparable performance in coarse-grained perception and reasoning tasks.

## Method Summary
The study evaluates connector performance using CLIP ViT-L/14 as visual encoder and LLaMA 2 as LLM, with LoRA-based adaptation. Three comprehensive benchmarks (MMBench, MME, SEED-Bench) are tested at resolutions of 224, 336, and 448 pixels. Connectors are classified into feature-preserving (linear and nonlinear types maintaining token count) and feature-compressing (average pooling, attention pooling Q-Former, convolutional mapping C-Abstractor) categories. The evaluation systematically measures performance across coarse-grained perception, fine-grained perception, and reasoning tasks, while tracking training time efficiency to understand computational trade-offs.

## Key Results
- Feature-preserving connectors maintain fine-grained perception performance by retaining detailed spatial information, while feature-compressing connectors provide significant computational efficiency
- Among feature-compressing connectors, simple average pooling performs comparably to complex approaches like Q-Former and C-Abstractor
- Image resolution exhibits diminishing returns, with most performance gains achieved at 336x336 resolution and minimal improvements at 448x448
- Optimal connector selection depends on task granularity requirements and computational resource constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature-preserving connectors maintain fine-grained perception performance while feature-compressing connectors offer computational efficiency
- Mechanism: Feature-preserving connectors keep the same number of visual tokens as input (P=Q), preserving detailed spatial information. Feature-compressing connectors reduce token count through pooling or attention mechanisms, trading detail for speed
- Core assumption: Fine-grained perception tasks require detailed spatial information that is lost when tokens are compressed
- Evidence anchors: [abstract] "feature-preserving connectors excel in fine-grained perception tasks due to their ability to retain detailed visual information"; [section 3.1] "Feature-preserving connectors maintain the patch number of visual features (i.e., P = Q)"

### Mechanism 2
- Claim: Among feature-compressing connectors, simpler pooling methods outperform complex attention mechanisms
- Mechanism: Average pooling and convolutional mapping preserve local information more effectively than attention pooling, which disrupts positional information through global attention
- Core assumption: Local spatial relationships are more important than global context for most perception tasks
- Evidence anchors: [abstract] "Among feature-compressing connectors, simple average pooling performs comparably to more complex approaches like Q-Former and C-Abstractor"; [section 5.4] "Q-Former's loss decreases more slowly than the losses from other connectors" indicating harder training

### Mechanism 3
- Claim: Image resolution has diminishing returns on performance, with most gains achieved by 336x336 resolution
- Mechanism: Higher resolution provides more visual information up to a point, but computational costs increase exponentially while performance gains plateau
- Core assumption: The model's capacity to process and utilize visual information is limited by architecture and training data
- Evidence anchors: [section 5.3] "increasing the resolution from 224 to 336 enhances performance across all connector types" but "further increasing the resolution from 336 to 448 yields only marginal performance gains"; [section 5.3] "we attribute this to the diminishing returns of higher resolutions and the current insufficient training data to support them"

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: The paper evaluates connector performance within MLLM architectures, requiring understanding of how visual and language modalities are integrated
  - Quick check question: What are the three main components of an MLLM architecture?

- Concept: Visual token compression vs preservation
  - Why needed here: The core distinction between connector types determines their suitability for different task granularities
  - Quick check question: How does token compression affect the spatial information available for perception tasks?

- Concept: Task granularity classification
  - Why needed here: The study categorizes tasks into coarse-grained perception, fine-grained perception, and reasoning to evaluate connector performance
  - Quick check question: What distinguishes coarse-grained perception from fine-grained perception in this study?

## Architecture Onboarding

- Component map: Visual Encoder → Connector → LLM → Output
  - Visual encoder extracts features (P tokens × dv channels)
  - Connector transforms to LLM-compatible format (Q tokens × D channels)
  - LLM processes tokens and generates response
  - Output: Text response based on visual input

- Critical path: Visual encoder → Connector → LLM
  - The connector is the focus of optimization
  - Must maintain sufficient information for task completion
  - Performance bottleneck if not properly designed

- Design tradeoffs:
  - Feature preservation vs computational efficiency
  - Simple methods (average pooling) vs complex methods (attention pooling)
  - Resolution vs training data requirements
  - Model complexity vs training stability

- Failure signatures:
  - Poor performance on fine-grained tasks with compressed connectors
  - Unstable training with complex connectors (high loss, slow convergence)
  - Diminishing returns at high resolutions
  - Token explosion at high resolutions with feature-preserving connectors

- First 3 experiments:
  1. Compare linear vs two-layer MLP connectors at 224 resolution across all task types
  2. Evaluate average pooling with 64 vs 144 compressed tokens at 336 resolution
  3. Test feature-preserving connector performance at 448 resolution and measure training time increase

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do feature-compressing connectors perform on fine-grained perception tasks when trained with significantly larger datasets (e.g., 130M samples like InstructBLIP vs. 1.23M in this study)?
- Basis in paper: [explicit] The paper acknowledges that InstructBLIP uses Q-Former with 130.2M training samples while their study only used 1.23M samples, noting this significant difference might render their conclusions inapplicable in scenarios with large training data volumes
- Why unresolved: The study's limited training data may not adequately support complex mechanisms like Q-Former's attention pooling, making it difficult to determine if poor performance is due to connector architecture or insufficient training data
- What evidence would resolve it: Comparative experiments using the same large-scale training datasets across different connector types would reveal whether Q-Former's performance improves significantly with more data, potentially matching or exceeding simpler connectors

### Open Question 2
- Question: What is the optimal balance between connector complexity and computational efficiency across different task granularities when scaling to higher resolutions beyond 448x448?
- Basis in paper: [inferred] The paper shows that computational costs for feature-preserving connectors increase exponentially with resolution while their performance gains diminish, and that C-Abstractor and average pooling become more optimal at 448 resolution
- Why unresolved: The study only tested up to 448x448 resolution, leaving uncertainty about how connector performance and efficiency trade-offs evolve at higher resolutions that might become standard in future MLLMs
- What evidence would resolve it: Systematic testing of connector performance and computational costs across resolutions from 448x448 to 1024x1024 would reveal at what point simpler connectors consistently outperform complex ones in terms of both accuracy and efficiency

### Open Question 3
- Question: How does the choice of visual encoder architecture (beyond CLIP ViT-L/14) impact the relative performance of different connector types across task granularities?
- Basis in paper: [explicit] The study consistently uses CLIP ViT-L/14 visual encoder across all experiments, noting this as a potential limitation without exploring how different visual encoders might affect connector performance
- Why unresolved: Different visual encoders have varying capabilities in feature extraction and spatial resolution handling, which could significantly impact how effectively different connector types process and transmit visual information to the LLM
- What evidence would resolve it: Comparative experiments using multiple visual encoder architectures (e.g., ConvNeXt, Swin Transformer, SigLIP) with the same connector types would reveal whether certain connector-connector encoder combinations consistently outperform others across task types

## Limitations
- Implementation Specificity: The paper provides architectural descriptions of connectors but lacks precise implementation details for several complex components, particularly the C-Abstractor connector's convolutional mapping and local weighted pooling mechanisms
- Dataset Composition: While three benchmarks are mentioned, the specific task distributions, image characteristics, and annotation quality across these datasets are not described, limiting understanding of how connector performance generalizes
- Training Stability Factors: The paper notes that Q-Former exhibits slower convergence but doesn't fully explore the underlying causes, including initialization strategies and gradient flow through complex connector architectures

## Confidence
- High Confidence: The core distinction between feature-preserving and feature-compressing connectors and their respective strengths for different task granularities is well-supported by experimental results
- Medium Confidence: The claim that simple average pooling outperforms complex attention-based methods like Q-Former is supported but may be influenced by implementation details and training stability factors
- Low Confidence: The mechanism explaining why feature-preserving connectors excel at fine-grained perception (preservation of detailed spatial information) is hypothesized but not directly validated

## Next Checks
1. **Information Retention Analysis**: Conduct ablation studies measuring the actual information content preserved by different connector types using techniques like mutual information estimation or feature similarity metrics to validate the hypothesized mechanism behind connector performance differences
2. **Cross-Architecture Generalization**: Test the connector performance patterns on alternative MLLM architectures (e.g., different visual encoders like ConvNeXt, or LLMs like GPT-4) to verify whether observed trends are architecture-dependent or represent fundamental properties of connector design
3. **Training Stability Investigation**: Perform controlled experiments isolating connector complexity as a variable while keeping other factors constant, measuring not just final performance but also training dynamics including loss curves, gradient norms, and convergence rates to better understand stability differences observed with complex connectors like Q-Former