---
ver: rpa2
title: Fewer Truncations Improve Language Modeling
arxiv_id: '2404.10830'
source_url: https://arxiv.org/abs/2404.10830
tags:
- language
- training
- packing
- https
- best-fit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the truncation problem in LLM training
  where the common concatenation-then-split approach fragments documents and leads
  to excessive truncations. To address this, it proposes Best-fit Packing, a scalable
  data grouping method that packs documents into training sequences via length-aware
  combinatorial optimization, eliminating unnecessary truncations while maintaining
  training efficiency.
---

# Fewer Truncations Improve Language Modeling

## Quick Facts
- arXiv ID: 2404.10830
- Source URL: https://arxiv.org/abs/2404.10830
- Reference count: 40
- Models trained with fewer truncations achieve superior performance across multiple benchmarks

## Executive Summary
This paper addresses the truncation problem in LLM training where the common concatenation-then-split approach fragments documents, leading to excessive truncations and degraded model performance. The authors propose Best-fit Packing, a scalable data grouping method that packs documents into training sequences via length-aware combinatorial optimization, eliminating unnecessary truncations while maintaining training efficiency. Experiments on both text and code pre-training demonstrate significant performance improvements across multiple benchmarks.

## Method Summary
The paper introduces Best-fit Packing as a solution to the truncation problem in LLM training. Instead of concatenating documents and then splitting into fixed-length sequences (which fragments documents), this method uses an optimized Best-Fit-Decreasing algorithm to pack documents into training sequences. The approach employs a segment tree data structure for efficient bin-finding during packing. The method is evaluated on transformer language models trained on both natural language (Falcon RefinedWeb) and programming language (The Stack) datasets, using AdamW optimizer with learning rate 3e-4 and global batch size of 2M tokens.

## Key Results
- +4.7% improvement on reading comprehension tasks
- +16.8% improvement on context following tasks
- +9.2% improvement on program synthesis tasks
- Up to 58.3% reduction in closed-domain hallucination

## Why This Works (Mechanism)
The Best-fit Packing method reduces document fragmentation by intelligently grouping documents of compatible lengths into training sequences, minimizing the need for truncation. By preserving document integrity, the model receives more complete information during training, leading to better understanding of context and improved performance on downstream tasks. The optimized algorithm maintains training efficiency while achieving these benefits.

## Foundational Learning

**Combinatorial Optimization**
- Why needed: To efficiently pack documents into training sequences while minimizing padding and truncation
- Quick check: Verify that the algorithm produces near-optimal packing solutions for various document length distributions

**Segment Tree Data Structure**
- Why needed: To enable efficient bin-finding during the Best-fit packing process
- Quick check: Confirm O(log n) search time for finding the best-fit bin

**Document Length Distribution Analysis**
- Why needed: To understand the impact of document length variability on packing efficiency
- Quick check: Compare packing efficiency across datasets with different length distributions

## Architecture Onboarding

**Component Map**
Tokenizer -> Best-fit Packing Algorithm -> Transformer Model -> Downstream Evaluation

**Critical Path**
Document tokenization → Best-fit packing → Model training → Performance evaluation

**Design Tradeoffs**
- Packing efficiency vs. computational overhead of the packing algorithm
- Sequence completeness vs. training batch size flexibility
- Algorithm complexity vs. implementation simplicity

**Failure Signatures**
- Excessive padding tokens indicating inefficient packing
- Degraded model performance suggesting improper packing
- Training instability from suboptimal sequence construction

**First Experiments**
1. Compare truncation rates between concatenation and Best-fit packing on sample datasets
2. Measure training efficiency (tokens/second) for both approaches
3. Evaluate model performance on a simple downstream task with both packing methods

## Open Questions the Paper Calls Out

**Open Question 1**: How does Best-fit Packing scale to extremely large datasets (100B+ documents) compared to concatenation?
- Basis: Paper demonstrates efficiency up to 2 billion documents
- Evidence needed: Runtime and memory usage on datasets exceeding 10 billion documents

**Open Question 2**: Impact on tasks requiring long-range dependencies beyond context length
- Basis: Paper focuses on tasks within context window
- Evidence needed: Experiments on cross-sequence reasoning tasks

**Open Question 3**: Effect on learning rare or tail knowledge
- Basis: Paper suggests truncation disproportionately affects tail knowledge
- Evidence needed: Analysis comparing rare fact recall between methods

**Open Question 4**: Adaptation for multimodal data (text and images)
- Basis: Paper focuses on text and code, not multimodal
- Evidence needed: Study on multimodal dataset effectiveness

## Limitations

- Performance improvements may depend on specific dataset characteristics and implementation details
- Hallucination reduction claims require careful validation due to subjective nature of detection
- Optimized Best-Fit-Decreasing algorithm efficiency gains may not generalize to all dataset distributions

## Confidence

- **High Confidence**: The fundamental premise about document fragmentation and truncation is well-established; core algorithmic approach is sound
- **Medium Confidence**: Performance improvements are likely real but may vary with implementation details and datasets
- **Low Confidence**: Specific hallucination reduction metrics and attribution require more rigorous validation

## Next Checks

1. **Implementation Validation**: Reimplement Best-fit Packing algorithm and compare truncation rates and training efficiency against concatenation-then-split approach using identical datasets and model architectures

2. **Hallucination Analysis**: Conduct controlled experiments isolating truncation effects on hallucination by training models with identical datasets but different packing strategies, using multiple hallucination detection methods

3. **Generalization Testing**: Test Best-fit Packing across diverse dataset types (different domains, languages, document length distributions) to verify claimed benefits beyond specific datasets used in the paper