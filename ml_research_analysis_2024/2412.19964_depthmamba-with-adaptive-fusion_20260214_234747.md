---
ver: rpa2
title: DepthMamba with Adaptive Fusion
arxiv_id: '2412.19964'
source_url: https://arxiv.org/abs/2412.19964
tags:
- depth
- estimation
- proposed
- methods
- multi-view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of robust depth estimation under
  noisy camera poses, a common issue in real-world applications like autonomous driving.
  Most existing multi-view depth estimation methods rely on ideal camera poses, which
  are often unavailable in practice.
---

# DepthMamba with Adaptive Fusion

## Quick Facts
- arXiv ID: 2412.19964
- Source URL: https://arxiv.org/abs/2412.19964
- Reference count: 0
- Primary result: Achieves 15.3% RMSE improvement on DDAD dataset compared to state-of-the-art methods

## Executive Summary
This paper addresses the challenge of robust depth estimation under noisy camera poses, a critical issue in autonomous driving applications. The authors propose a two-branch network architecture that fuses single-view and multi-view depth estimates using an attention-based mechanism. The key innovation is the DepthMamba backbone for feature extraction combined with adaptive fusion that selects the most reliable depth estimates from each branch. The method shows competitive performance on KITTI and DDAD datasets, with particular robustness in challenging scenarios like dynamic objects and texture-less regions.

## Method Summary
The proposed method uses a two-branch network architecture with a DepthMamba backbone for feature extraction. The single-view branch constructs a variance volume from reference and source features, while the multi-view branch creates a group-wise correlation volume with attention weights. These are fused using an attention-based module that creates an attention volume, which is then processed by a 3D hourglass network to produce final depth maps. The method is trained on 3 consecutive frames using AdamW optimizer with one-cycle learning rate policy.

## Key Results
- Achieves AbsRel error of 0.042 and RMSE of 1.695 on KITTI dataset
- Shows 15.3% improvement in RMSE error compared to state-of-the-art on DDAD dataset
- Demonstrates robustness to pose noise through ablation studies and synthetic noise injection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-branch network architecture with adaptive fusion improves depth estimation robustness under noisy camera poses.
- Mechanism: The network extracts features from both single-view and multi-view branches using DepthMamba, then adaptively fuses the depth estimates using attention weights, selecting the most reliable results from each branch.
- Core assumption: The single-view and multi-view branches produce complementary depth estimates, with single-view being more robust to pose noise and multi-view providing higher accuracy under ideal conditions.
- Evidence anchors:
  - [abstract] "we propose a two-branch network architecture which fuses the depth estimation results of single-view and multi-view branch"
  - [section] "To tackle this challenge, we propose a two-branch network architecture which fuses the depth estimation results of single-view and multi-view branch"
- Break condition: If pose noise is so severe that neither branch produces reliable estimates, or if the attention module fails to correctly identify the better estimate.

### Mechanism 2
- Claim: DepthMamba backbone improves feature extraction compared to traditional CNN backbones.
- Mechanism: DepthMamba uses selective state spaces to capture long-range dependencies more efficiently than CNNs, while local feature blocks compensate for Mamba's weakness in spatial information capture.
- Core assumption: The combination of global sequence modeling (Mamba) and local feature extraction provides superior feature representation for depth estimation compared to pure CNN or pure Mamba approaches.
- Evidence anchors:
  - [section] "we introduce the mamba-based feature extraction backbone to further enhance the feature representation ability of the overall networks"
  - [section] "inspired by [26], we also propose local feature blocks to further enhance the ability of extracting local features because mamba is not good at capturing the spatial information"
- Break condition: If the local feature blocks are insufficient to compensate for Mamba's spatial limitations, or if the increased complexity doesn't translate to better depth estimates.

### Mechanism 3
- Claim: Attention-based fusion module outperforms simple concatenation or cross-attention methods.
- Mechanism: The proposed fusion module applies calculated attention weights to the variance volume, creating an attention volume that emphasizes reliable depth estimates while suppressing noisy ones.
- Core assumption: The attention mechanism can effectively distinguish between reliable and unreliable depth estimates from the two branches, and this distinction leads to better overall performance.
- Evidence anchors:
  - [section] "we propose an attention-based fusion methods which adaptively select the most robust estimation results between the two branches"
  - [section] "we apply a multi-scale attention module and a 3D hourglass network on the GwC volume to obtain attention weights"
- Break condition: If the attention weights don't correlate well with actual depth estimation quality, or if the variance volume construction is flawed.

## Foundational Learning

- Concept: Multi-view geometry and epipolar constraints
  - Why needed here: Understanding how multiple camera views can be used to triangulate depth points is fundamental to the multi-view branch's operation
  - Quick check question: How does having multiple views with known camera poses enable more accurate depth estimation than a single view?

- Concept: Attention mechanisms in neural networks
  - Why needed here: The adaptive fusion module relies on attention weights to select between single-view and multi-view depth estimates
  - Quick check question: What role do attention weights play in the fusion process, and how are they calculated from the feature representations?

- Concept: State space models vs. convolutional neural networks
  - Why needed here: DepthMamba represents a departure from traditional CNN-based feature extraction, requiring understanding of how selective state spaces work
  - Quick check question: What are the key differences between Mamba's approach to sequence modeling and traditional CNN approaches to image feature extraction?

## Architecture Onboarding

- Component map:
  Input: Multiple camera views (reference and source images)
  Feature Extraction: DepthMamba backbone with local feature blocks
  Single-view Branch: Constructs variance volume from reference and source features
  Multi-view Branch: Constructs group-wise correlation volume and applies attention
  Fusion Module: Applies attention weights to variance volume to create attention volume
  Depth Regression: 3D hourglass network processes attention volume to produce depth maps
  Output: Depth maps and confidence scores

- Critical path: Input → DepthMamba → Variance Volume (single-view) + Attention Weights (multi-view) → Attention Volume → Depth Regression → Output

- Design tradeoffs:
  - Using Mamba vs. CNN: Mamba offers better long-range modeling but weaker spatial information capture, addressed by local feature blocks
  - Two-branch vs. single-branch: Increased robustness at the cost of additional computation and complexity
  - Attention-based fusion vs. simple concatenation: Better performance but requires learning attention weights

- Failure signatures:
  - Poor performance on textureless regions despite claims of robustness
  - Degraded performance when pose noise is introduced, indicating fusion mechanism failure
  - Large discrepancies between single-view and multi-view branch outputs suggesting calibration issues

- First 3 experiments:
  1. Ablation study comparing DepthMamba backbone with ConvNeXt-T and VMamba on KITTI dataset
  2. Comparison of fusion methods (concatenation, cross-attention, proposed attention-based) on KITTI
  3. Evaluation of pose noise robustness by systematically degrading camera poses and measuring depth estimation accuracy

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the analysis of the paper, several open questions can be identified:

### Open Question 1
- Question: How does the DepthMamba backbone's performance compare to other transformer-based backbones like ViT or Swin in the context of multi-view depth estimation?
- Basis in paper: [inferred] The paper only compares DepthMamba to ConvNeXt-T and VMamba, leaving out other popular transformer architectures.
- Why unresolved: The authors did not conduct experiments with other transformer backbones, making it unclear if DepthMamba's performance advantage is specific to this architecture or if other transformers could achieve similar results.
- What evidence would resolve it: Experiments comparing DepthMamba to other transformer backbones (e.g., ViT, Swin) on the same multi-view depth estimation tasks and datasets.

### Open Question 2
- Question: What is the impact of varying the number of frames (n) in the multi-view setup on the overall depth estimation performance?
- Basis in paper: [explicit] The paper mentions that the network takes consecutive 3 frames as input (n=3) but does not explore the effects of using different numbers of frames.
- Why unresolved: The optimal number of frames for the best trade-off between performance and computational efficiency is not explored, leaving open the question of whether more or fewer frames could improve results.
- What evidence would resolve it: Experiments testing the model with different numbers of input frames (e.g., 2, 4, 5) and comparing the depth estimation performance and computational costs.

### Open Question 3
- Question: How does the proposed method handle dynamic objects that move between the reference and source frames, and what is the impact on depth estimation accuracy?
- Basis in paper: [explicit] The paper mentions that the method can perform well on dynamic objects but does not provide specific details on how it handles these cases or quantify the impact on accuracy.
- Why unresolved: The mechanism for dealing with moving objects and its effect on depth estimation accuracy is not clearly explained, making it unclear how robust the method is in highly dynamic scenes.
- What evidence would resolve it: Detailed analysis of the method's performance on datasets with significant object motion, including metrics on accuracy degradation and visualizations of how dynamic objects are handled.

### Open Question 4
- Question: How does the proposed method's performance scale with increasing image resolution beyond the DDAD dataset's 1936×1216 pixels?
- Basis in paper: [inferred] The paper evaluates the method on KITTI (1241×376) and DDAD (1936×1216) datasets but does not explore performance at higher resolutions.
- Why unresolved: The method's effectiveness at higher resolutions, which are becoming more common in autonomous driving applications, is unknown.
- What evidence would resolve it: Experiments testing the method on higher resolution datasets or artificially upscaling the input images and measuring the impact on depth estimation accuracy and computational requirements.

## Limitations

- Evaluation relies entirely on synthetic pose noise injection rather than real-world noisy pose scenarios
- DepthMamba backbone lacks detailed architectural specifications for proper reproduction
- Attention-based fusion mechanism has limited ablation studies examining robustness across different noise levels

## Confidence

- **Medium**: Claims about DepthMamba superiority over traditional backbones - supported by limited ablation studies without architectural details
- **Medium**: Claims about attention-based fusion outperforming alternatives - supported by qualitative comparisons but lacking comprehensive ablation
- **Low**: Claims about real-world robustness to noisy poses - only evaluated with synthetic noise, no real-world validation

## Next Checks

1. Evaluate the method on a dataset with real-world camera pose noise (e.g., from visual odometry failures) rather than synthetic injection
2. Conduct a detailed ablation study varying the pose noise intensity to determine the method's breaking point
3. Compare against a simpler baseline that uses single-view depth estimation with explicit pose uncertainty modeling to assess whether the two-branch architecture is necessary