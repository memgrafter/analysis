---
ver: rpa2
title: 'MedBench: A Comprehensive, Standardized, and Reliable Benchmarking System
  for Evaluating Chinese Medical Large Language Models'
arxiv_id: '2407.10990'
source_url: https://arxiv.org/abs/2407.10990
tags:
- medical
- evaluation
- medbench
- dataset
- university
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents MedBench, a comprehensive benchmarking system
  for evaluating Chinese medical large language models (LLMs). MedBench addresses
  the lack of standardized evaluation frameworks by assembling the largest evaluation
  dataset (300,901 questions) covering 43 clinical specialties and five dimensions:
  medical language understanding, generation, knowledge question answering, complex
  reasoning, and healthcare safety/ethics.'
---

# MedBench: A Comprehensive, Standardized, and Reliable Benchmarking System for Evaluating Chinese Medical Large Language Models

## Quick Facts
- **arXiv ID**: 2407.10990
- **Source URL**: https://arxiv.org/abs/2407.10990
- **Reference count**: 40
- **Primary result**: MedBench establishes a standardized evaluation framework for Chinese medical LLMs with 300,901 questions across 43 specialties, preventing shortcut learning through physical separation and dynamic shuffling

## Executive Summary
MedBench addresses the critical gap in standardized evaluation frameworks for Chinese medical large language models (LLMs) by providing a comprehensive benchmarking system with the largest evaluation dataset (300,901 questions) covering 43 clinical specialties. The system implements five evaluation dimensions: medical language understanding, generation, knowledge question answering, complex reasoning, and healthcare safety/ethics. Through cloud-based infrastructure with physical separation of questions and ground truth, combined with dynamic evaluation mechanisms including circular shuffling and random prompt matching, MedBench prevents common evaluation shortcuts and provides reliable assessment of medical LLM capabilities before real-world deployment.

## Method Summary
The study developed MedBench as a cloud-based evaluation platform that collects 300,901 Chinese medical questions from 8 public datasets and 12 self-constructed datasets, covering 43 clinical specialties. The system implements physical separation between questions and ground truth, requiring users to generate answers externally before uploading for evaluation. Dynamic evaluation mechanisms include circular shuffling of multiple choice options and random prompt matching to prevent models from exploiting static patterns. The evaluation covers five dimensions (MLU, MLG, MKQA, CMR, HSE) with task-specific metrics including accuracy, BLEU, ROUGE-L, and micro-F1 scores. Models are evaluated through balanced random sampling from each dataset, with the evaluation corpus updated every three months.

## Key Results
- ChatGPT achieved the highest overall score of 25.5/100 among tested models, with consistent performance across datasets
- Physical separation and dynamic shuffling mechanisms effectively prevented shortcut learning, with some models failing completely when circular shuffling was enabled
- Automated evaluation results showed strong alignment with medical professionals' assessments
- The platform successfully identified variations in model performance across different evaluation dimensions and question types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Physical separation of questions and ground truth prevents answer memorization
- Mechanism: Cloud-based infrastructure stores questions and ground truth separately, requiring users to generate answers externally before uploading for evaluation
- Core assumption: Users cannot access both question and ground truth simultaneously
- Evidence anchors:
  - [abstract]: "provides a standardized and fully automatic cloud-based evaluation infrastructure, with physical separations for question and ground truth"
  - [section]: "This workflow ensures that the user can not simultaneously access to both question and ground truth and thus prevents a majority of cheating"
- Break condition: If users find a way to bypass the cloud infrastructure and access both question and ground truth directly

### Mechanism 2
- Claim: Circular shuffling prevents models from relying on option position patterns
- Mechanism: For multiple choice questions, options are randomly reordered for each evaluation, requiring models to consistently identify correct answers regardless of option position
- Core assumption: Models cannot learn the correct answer based solely on option position
- Evidence anchors:
  - [section]: "a circular shuffle is applied to choices in choice questions... require the LLM to consistently provide the correct answer under all possible shuffling of the choices"
  - [section]: "BenTsao and BianQue2 completely fail the evaluation with circular shuffling"
- Break condition: If models learn to recognize correct answers through semantic understanding rather than position patterns

### Mechanism 3
- Claim: Random prompt matching tests model reliability across different input patterns
- Mechanism: Questions are paired with randomly selected prompts from a prompt pool during evaluation, testing whether model performance depends on specific input patterns
- Core assumption: Model performance should be consistent regardless of irrelevant prompt variations
- Evidence anchors:
  - [section]: "Randomly sampled prompts from a pre-collected prompt pool... will be attached with to each question during the evaluation"
  - [section]: "BianQue2 exhibits high variations in CMR, MLU, and MKQA"
- Break condition: If models learn to ignore irrelevant prompts and focus only on the question content

## Foundational Learning

- Concept: Physical security in evaluation systems
  - Why needed here: Prevents cheating and answer leakage in sensitive medical evaluations
  - Quick check question: How would you design a system where users cannot access both questions and answers simultaneously?

- Concept: Dynamic evaluation techniques
  - Why needed here: Prevents models from memorizing static datasets and inflating scores artificially
  - Quick check question: What methods can you implement to ensure models demonstrate true understanding rather than pattern matching?

- Concept: Multi-dimensional evaluation metrics
  - Why needed here: Provides comprehensive assessment across different medical knowledge domains and reasoning capabilities
  - Quick check question: How would you design an evaluation system that captures both factual knowledge and complex reasoning in medical contexts?

## Architecture Onboarding

- Component map: Cloud question storage -> External answer generation interface -> Answer submission portal -> Evaluation engine -> Result reporting system -> Dynamic shuffling module
- Critical path: Question retrieval → Answer generation → Answer submission → Evaluation → Result generation
- Design tradeoffs: Security vs. accessibility (physical separation provides security but requires external infrastructure), complexity vs. reliability (dynamic shuffling adds complexity but improves evaluation validity)
- Failure signatures: Consistent performance drops when circular shuffling is enabled, significant performance variations with different prompts, inability to handle open-ended questions effectively
- First 3 experiments:
  1. Test model performance with and without circular shuffling on multiple choice questions
  2. Evaluate model consistency across different random prompts for the same question
  3. Compare automated evaluation scores with human expert rankings on sample questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MedBench evaluation compare to real-world clinical outcomes when medical LLMs are deployed in practice?
- Basis in paper: [inferred] The paper discusses MedBench as a comprehensive evaluation system but does not validate its predictions against actual clinical deployment outcomes or patient health outcomes.
- Why unresolved: The paper establishes MedBench as a benchmarking framework but doesn't provide longitudinal studies tracking how LLM performance on MedBench correlates with real-world clinical success rates, patient safety incidents, or diagnostic accuracy in actual healthcare settings.
- What evidence would resolve it: A multi-year study comparing LLM performance scores on MedBench against actual clinical deployment metrics, including diagnostic accuracy rates, patient outcomes, and adverse event frequencies in real healthcare settings.

### Open Question 2
- Question: What are the specific limitations of using BLEU and ROUGE-L scores for evaluating open-ended medical question responses, and what alternative evaluation methods could better capture clinical relevance and safety?
- Basis in paper: [explicit] The paper acknowledges limitations of BLEU and ROUGE-L scores for open-ended questions and mentions that human evaluation remains the most effective method, though labor-intensive.
- Why unresolved: The paper identifies this as a limitation but doesn't propose or test specific alternative evaluation frameworks that could better assess the clinical accuracy, safety, and relevance of open-ended medical responses.
- What evidence would resolve it: Development and validation of alternative evaluation metrics specifically designed for medical contexts, tested against expert human evaluations and correlated with actual clinical decision-making quality.

### Open Question 3
- Question: How can MedBench be extended to effectively evaluate multimodal medical foundation models that process both text and medical imaging data?
- Basis in paper: [explicit] The paper explicitly states that current MedBench focuses exclusively on language-based medical QA and overlooks the substantial needs for visual QA in clinical workflows.
- Why unresolved: While the paper identifies this gap, it doesn't provide a framework or methodology for incorporating medical imaging evaluation into the benchmarking system, nor does it address how to evaluate models that process both text and imaging data.
- What evidence would resolve it: A prototype multimodal benchmarking extension that includes standardized medical imaging datasets, evaluation metrics for image-text integration, and validation against clinical experts' assessments of multimodal medical reasoning.

## Limitations

- The 300,901 question corpus may not fully represent all Chinese medical scenarios, particularly rare conditions or emerging medical knowledge
- The physical separation mechanism relies on user compliance without technical verification of security compliance
- BLEU and ROUGE-L scores have inherent limitations for evaluating clinical relevance and safety of open-ended medical responses

## Confidence

**High Confidence (Core Claims):**
- The overall benchmarking framework and infrastructure design are well-documented and technically feasible
- The five-dimensional evaluation approach comprehensively covers essential medical LLM capabilities
- The physical separation mechanism effectively prevents direct answer access when properly implemented

**Medium Confidence (Methodological Claims):**
- Circular shuffling successfully prevents position-based shortcut learning across all model types
- Random prompt matching provides reliable test of model robustness to input variations
- The automated evaluation metrics accurately reflect human expert assessments

**Low Confidence (Comparative Claims):**
- Absolute performance rankings between different LLMs (e.g., ChatGPT's 25.5/100 score) may vary significantly with different evaluation subsets
- The correlation between automated scores and clinical utility remains unproven
- Generalization of results to real-world medical practice requires further validation

## Next Checks

1. **Security Audit**: Conduct a technical security audit of the cloud infrastructure to verify that users cannot bypass the physical separation mechanism and access both questions and ground truth simultaneously.

2. **Cross-dataset Consistency Test**: Evaluate model performance across different subsets of the 300,901 questions to determine if performance rankings remain stable or vary significantly based on dataset composition.

3. **Human-Machine Alignment Study**: Compare automated evaluation scores with assessments from multiple independent medical professionals across a stratified sample of questions to quantify correlation and identify systematic discrepancies.