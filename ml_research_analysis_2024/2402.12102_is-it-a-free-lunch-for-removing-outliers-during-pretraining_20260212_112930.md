---
ver: rpa2
title: Is It a Free Lunch for Removing Outliers during Pretraining?
arxiv_id: '2402.12102'
source_url: https://arxiv.org/abs/2402.12102
tags:
- welcome
- said
- severn
- here
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether removing outliers during pretraining
  improves model performance. The authors find that a previously proposed clipped
  softmax method degrades full-precision performance due to sequence length normalization
  issues.
---

# Is It a Free Lunch for Removing Outliers during Pretraining?

## Quick Facts
- **arXiv ID**: 2402.12102
- **Source URL**: https://arxiv.org/abs/2402.12102
- **Authors**: Baohao Liao; Christof Monz
- **Reference count**: 38
- **Primary result**: Normalized clipped softmax (NCS) improves full-precision performance and quantization while enabling outlier-free pretraining of causal LLMs.

## Executive Summary
This paper investigates whether removing outliers during pretraining improves model performance. The authors find that a previously proposed clipped softmax method degrades full-precision performance due to sequence length normalization issues. They introduce a normalized clipped softmax (NCS) that maintains consistent normalization across different sequence lengths, improving full-precision performance and reducing hyperparameter sensitivity. NCS also enables successful outlier-free pretraining of causal language models. Experiments on BERT and OPT models show NCS achieves similar or better full-precision performance compared to vanilla softmax while significantly improving quantization performance.

## Method Summary
The authors propose a normalized clipped softmax (NCS) that addresses the sequence length normalization issues in the original clipped softmax method. NCS introduces a fixed normalization constant β that makes normalization invariant to sequence length, ensuring consistent scaling between pretraining and downstream evaluation. The method is applied to both BERT models (for bidirectional attention) and OPT models (for causal attention with position-adaptive normalization). Pretraining uses masked language modeling for BERT and next token prediction for OPT, with evaluation on MLM accuracy, perplexity, and quantization performance.

## Key Results
- NCS maintains consistent normalization across sequence lengths, improving full-precision performance degraded by standard clipped softmax
- NCS significantly reduces sensitivity to pretraining sequence length, benefiting quantization performance (W8A8)
- NCS enables successful outlier-free pretraining of causal language models by adapting normalization to token position

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The normalized clipped softmax (NCS) solves the FP16 performance degradation by making sequence length normalization invariant.
- Mechanism: In standard clipped softmax, normalization is scaled by the sequence length (T), causing different effective scaling during pretraining (fixed T) and finetuning (variable T). NCS introduces a fixed normalization constant β, decoupling scaling from sequence length and ensuring consistent representation across tasks.
- Core assumption: FP16 performance degradation is primarily caused by inconsistent normalization between pretraining and downstream evaluation, not by the clipping operation itself.
- Evidence anchors:
  - [abstract]: "Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length"
  - [section]: "In an effort to enhance the FP16 performance of CS, we propose a normalized variant termed NCS... β represents a normalization constant"
  - [corpus]: Weak - no direct neighbor discussion of normalization invariance
- Break condition: If downstream tasks use fixed-length sequences matching pretraining, or if normalization is handled by other layers.

### Mechanism 2
- Claim: NCS improves quantization performance by reducing sensitivity to pretraining sequence length.
- Mechanism: By fixing the normalization constant β, NCS ensures that the product between attention probability and value matrix stays within a similar dynamic range regardless of the pretraining sequence length. This stability makes the model less sensitive to the choice of pretraining hyperparameters and improves W8A8 performance.
- Core assumption: The gap between FP16 and W8A8 performance is driven by inconsistent scaling across different pretraining sequence lengths.
- Evidence anchors:
  - [abstract]: "Moreover, this improved method also facilitates successful pretraining of causal LLMs"
  - [section]: "NCS displays significantly lower sensitivity to the pretraining sequence length owing to normalization... This characteristic benefits pretraining and conserves computational resources"
  - [corpus]: Weak - neighbors discuss outliers but not sequence-length sensitivity in quantization
- Break condition: If calibration samples dominate quantization error, or if sequence length effects are masked by other architectural factors.

### Mechanism 3
- Claim: NCS enables outlier-free pretraining for causal language models (OPT) by adapting normalization to token position.
- Mechanism: Unlike BERT's bidirectional attention, OPT's causal attention has varying effective sequence lengths per token. NCS adapts T to be the token's position rather than a fixed max length, ensuring proper normalization for each token's context.
- Core assumption: The original clipped softmax fails for OPT because it uses a uniform T for all tokens, leading to inconsistent normalization.
- Evidence anchors:
  - [abstract]: "Moreover, this improved method also facilitates successful pretraining of causal LLMs"
  - [section]: "In Equation 3, T is not static for all tokens; instead, it denotes the position of the current token"
  - [corpus]: Weak - no direct neighbor discussion of causal attention normalization
- Break condition: If OPT architecture changes (e.g., non-causal attention), or if other normalization techniques are applied.

## Foundational Learning

- **Concept**: Softmax function and its gradient behavior
  - Why needed here: Understanding how softmax probabilities influence outlier formation and why clipping is needed
  - Quick check question: What happens to softmax gradients when input values become very large or very small?

- **Concept**: Layer normalization and its interaction with attention
  - Why needed here: Explains why "no-op" heads and outlier concentration occur during training
  - Quick check question: How does layer normalization affect the distribution of attention weights across tokens?

- **Concept**: Quantization error sources and uniform affine quantization
  - Why needed here: Understanding why outliers cause quantization problems and how NCS helps
  - Quick check question: How does the scale factor in quantization relate to the dynamic range of activations?

## Architecture Onboarding

- **Component map**: Input embeddings → LayerNorm → Multi-head attention (with clipped softmax) → Feed-forward → LayerNorm → Output
- **Critical path**: Attention computation → Clipped/Normalized softmax → Value projection → Residual addition
- **Design tradeoffs**: NCS vs. CS: NCS trades potential clipping flexibility for normalization stability
- **Failure signatures**: FP16 performance drops when sequence lengths differ significantly from pretraining
- **First 3 experiments**:
  1. Compare FP16 MLM accuracy of BERT with CS vs. NCS across varying validation sequence lengths
  2. Measure W8A8 quantization error sensitivity to pretraining sequence length for CS vs. NCS
  3. Test OPT pretraining with token-position-based T in NCS vs. fixed T in CS

## Open Questions the Paper Calls Out
- How does the effectiveness of NCS vary across different quantization bit-widths beyond 8-bit?
- What are the specific mechanisms by which outliers in weights and activations influence quantization error?
- How does NCS perform when applied to transformer models in domains outside of natural language processing, such as vision or speech?

## Limitations
- The paper only evaluates NCS on language tasks, leaving its generalizability to other domains uncertain
- The causal attention adaptation for OPT is only tested on relatively small models (125M and 350M parameters)
- The specific quantization setup used (asymmetric uniform quantization) may not generalize to other quantization schemes

## Confidence
- **High Confidence**: Experimental evidence for NCS improving BERT pretraining in full-precision and quantization settings is robust
- **Medium Confidence**: Causal attention adaptation for OPT shows promising results but scalability to larger models remains unproven
- **Low Confidence**: The claim that normalization inconsistency is the primary cause of FP16 degradation in CS is theoretically plausible but not definitively proven

## Next Checks
1. **Scaling Experiment**: Pretrain BERT-large with NCS using a wider range of sequence lengths (from 128 to 4096 tokens) and measure both FP16 performance and W8A8 quantization error to validate the sequence length sensitivity claims across a broader spectrum.

2. **Alternative Quantization Test**: Apply NCS-pretrained models to a different quantization framework (e.g., symmetric quantization or calibration-based methods) to determine whether the performance improvements generalize beyond the specific asymmetric uniform quantization setup used in the paper.

3. **Gradient Analysis**: Conduct a detailed gradient flow analysis comparing CS and NCS during pretraining, specifically examining whether the normalization differences create measurable effects on gradient distributions and training stability that could explain the FP16 performance differences.