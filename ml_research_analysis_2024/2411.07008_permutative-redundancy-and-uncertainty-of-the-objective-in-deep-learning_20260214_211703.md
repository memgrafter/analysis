---
ver: rpa2
title: Permutative redundancy and uncertainty of the objective in deep learning
arxiv_id: '2411.07008'
source_url: https://arxiv.org/abs/2411.07008
tags:
- network
- deep
- learning
- gradient
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper discusses implications of uncertain objective functions
  and permutative symmetry in deep learning architectures. It shows that traditional
  architectures suffer from astronomical numbers of equivalent global and local optima,
  making optimization challenging, especially as networks grow larger.
---

# Permutative redundancy and uncertainty of the objective in deep learning

## Quick Facts
- arXiv ID: 2411.07008
- Source URL: https://arxiv.org/abs/2411.07008
- Reference count: 7
- Key outcome: Traditional deep learning architectures suffer from astronomical numbers of equivalent global and local optima due to permutative symmetry, making optimization challenging especially as networks grow larger

## Executive Summary
This paper addresses fundamental challenges in deep learning optimization stemming from permutative redundancy and objective function uncertainty. The author demonstrates that traditional neural network architectures contain astronomically many functionally equivalent optima due to the symmetry of permutable layers, creating a fragmented optimization landscape. Additionally, the paper shows that stochastic gradient descent methods do not converge to true optima but rather to equilibrium distributions around them, with convergence behavior critically dependent on the structure of the objective function. The work proposes several remedies including forced pre-pruning to break permutative symmetry, re-ordering techniques for stability tracking, ortho-polynomial activations as alternatives to traditional activation functions, and modular bio-inspired architectures that offer more efficient solutions compared to monolithic feedforward networks.

## Method Summary
The paper presents a theoretical analysis of deep learning optimization challenges through the lens of permutative symmetry and stochastic gradient dynamics. The methodology involves examining Hessian eigenvalue spectra to understand redundancy in the optimization landscape, deriving equilibrium distributions for stochastic gradient iterates, and proposing architectural modifications to address identified problems. Key methods include the Sorensen reordering heuristic for stability tracking across training runs, pre-pruning with binary masks to break permutation symmetry, and ortho-polynomial activation functions that provide better approximation properties. The approach combines mathematical analysis with proposed practical implementations, though empirical validation across diverse tasks is limited in scope.

## Key Results
- Traditional deep learning architectures have (w!)^d equivalent global optima due to permutative symmetry, where w is layer width and d is depth
- Stochastic gradient descent converges to Boltzmann-like equilibrium distributions rather than true optima, with distribution size proportional to gradient noise variance
- As networks grow larger, equilibrium regions around different global optima overlap, creating a complex landscape of valleys and ridges
- Proposed remedies including pre-pruning, ortho-polynomial activations, and modular architectures can mitigate redundancy and improve optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional deep learning architectures have an astronomical number of functionally equivalent global and local optima due to permutative symmetry of their layers.
- Mechanism: Each layer in a fully connected network consists of permutable elements (nodes), meaning that reordering nodes within a layer preserves the network's output. This creates an exponential number of equivalent optima: for a network with d layers each of width w, there are (w!)^d equivalent optima.
- Core assumption: The output of a node in layer k depends only on the sum of weighted inputs from the previous layer, not on the order of those inputs.
- Evidence anchors:
  - [abstract]: "traditional architectures are polluted by an astronomical number of equivalent global and local optima"
  - [section 5.1]: "The output of a simple two-layer fully connected trained network is invariant with respect to a permutation of nodes within one layer"
  - [corpus]: Weak evidence - related papers discuss optima and symmetry but not permutative redundancy specifically
- Break condition: When network layers contain non-permutable elements or when binary pre-pruning ensures unique connections to the previous layer

### Mechanism 2
- Claim: Uncertainty in the objective function prevents gradient methods from converging to true optima, instead converging to equilibrium distributions around optima.
- Mechanism: Stochastic gradient descent with noisy gradients converges not to the optimum but to a Boltzmann-like distribution around it, with the size of the equilibrium region proportional to the gradient noise variance and inversely proportional to the Hessian eigenvalues.
- Core assumption: The objective function has a finite variance uncertainty that follows the form in Equation 6: ∇θJ(θ, D) + ϵ(θ, D)
- Evidence anchors:
  - [abstract]: "stochastic gradient descent methods converge not to optima but to equilibrium distributions around them"
  - [section 4.1]: "relaxation to the equilibrium distribution occurs exponentially fast first, governed by the deterministic part of the gradient, and then slowly, governed more by the stochastic term"
  - [section 4.2]: "the volume occupied by the equilibrium distribution of stochastic gradient iterates is given by Eq 23"
- Break condition: When gradient noise variance approaches zero or when the learning rate is adaptively reduced to zero

### Mechanism 3
- Claim: The combination of permutative redundancy and objective uncertainty creates a landscape where the equilibrium regions around different global optima overlap, creating a complex web of valleys and ridges.
- Mechanism: As the network grows, the volume occupied by gradient iterates around each optimum grows faster than the spacing between optima, causing the equilibrium regions to merge. This breaks down the smooth approximation of the objective function.
- Core assumption: The Hessian eigenvalues concentrate near zero as network size increases (Equation 33: dlog λ/dn > 0)
- Evidence anchors:
  - [section 5.2]: "the stochastic vicinity of each global optimum containing the trajectories of stochastic gradient iterates eventually reaches the vicinities of other global optima"
  - [section 4.2]: "the volume occupied by possible gradient iterates around all global optima is then V* ~ exp(n · (log(w) - 1/2 log λ))"
  - [corpus]: Weak evidence - related papers discuss optimization landscapes but not the specific interaction between redundancy and uncertainty
- Break condition: When pre-pruning breaks permutative symmetry or when the network architecture is modified to eliminate equivalent optima

## Foundational Learning

- Concept: Hessian eigenvalue spectrum analysis
  - Why needed here: Understanding how the concentration of Hessian eigenvalues near zero affects the optimization landscape and the overlap of equilibrium regions
  - Quick check question: What happens to the size of the equilibrium distribution around an optimum as the smallest Hessian eigenvalue approaches zero?

- Concept: Stochastic gradient dynamics and Langevin equations
  - Why needed here: The paper shows that gradient iterates follow a discrete version of the overdamped Langevin equation, leading to Boltzmann equilibrium distributions
  - Quick check question: How does the learning rate and gradient noise variance determine the "temperature" of the equilibrium distribution?

- Concept: Orthonormal polynomial basis functions
  - Why needed here: The paper proposes using orthogonal polynomials as activation functions to reduce redundancy and improve approximation properties
  - Quick check question: Why are orthogonal polynomials more efficient than standard polynomials for function approximation in deep networks?

## Architecture Onboarding

- Component map:
  - Input layer: Raw data features
  - Polynomial layers: Ortho-polynomial activation functions (e.g., Laguerre, Legendre)
  - Binary pre-pruning layers: Hadamard product with random binary masks to break permutative symmetry
  - Output layer: Task-specific output (classification, regression, etc.)
  - Sorensen reordering module: Optional post-training analysis for stability tracking

- Critical path:
  1. Data preprocessing and normalization
  2. Ortho-polynomial layer initialization
  3. Binary pre-pruning mask generation
  4. Forward pass computation
  5. Gradient computation with uncertainty modeling
  6. Parameter update with adaptive learning rates
  7. Optional Sorensen reordering for stability analysis

- Design tradeoffs:
  - Ortho-polynomials vs. traditional activations: Better approximation properties but requires careful basis selection
  - Pre-pruning rate vs. representation power: Higher pre-pruning reduces redundancy but may reduce expressiveness
  - Network depth vs. training stability: Deeper networks may be more expressive but harder to train with uncertainty

- Failure signatures:
  - High variance in gradient iterates despite low learning rates
  - Poor generalization despite good training performance
  - Sensitivity to initialization and data ordering
  - Slow convergence with standard optimization methods

- First 3 experiments:
  1. Compare training convergence of a standard MLP vs. ortho-polynomial network on a simple regression task with synthetic noise
  2. Measure the effect of different pre-pruning rates on the number of equivalent optima and generalization performance
  3. Apply Sorensen reordering to track parameter stability across multiple training runs with the same data and architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the equilibrium distribution of stochastic gradient iterates converge to a Boltzmann distribution with temperature inversely proportional to the product of learning rate and noise variance?
- Basis in paper: [explicit] The paper derives the equilibrium distribution equation P(θ, t→∞) ~ exp(-β·J(θ)) where β⁻¹ ~ η∥ϵ∥², showing how the learning rate and objective function uncertainty determine the equilibrium distribution
- Why unresolved: While the mathematical derivation is presented, the paper does not provide experimental validation of this theoretical result across different network architectures and noise levels
- What evidence would resolve it: Experimental studies measuring the actual distribution of gradient iterates around optima for various network architectures and comparing them with the predicted Boltzmann distribution

### Open Question 2
- Question: Can pre-pruning with binary matrices effectively break permutative symmetry while maintaining network expressiveness?
- Basis in paper: [explicit] The paper proposes pre-pruning as a method to break permutative symmetry by introducing binary matrices B(k) that ensure no two columns are identical, reducing the number of equivalent optima
- Why unresolved: The paper presents the theoretical framework but does not provide empirical validation of how pre-pruning affects both symmetry breaking and network performance across different tasks
- What evidence would resolve it: Comparative experiments showing the number of equivalent optima before and after pre-pruning, along with performance metrics for various tasks and pruning rates

### Open Question 3
- Question: Do ortho-polynomial activation functions provide better generalization and interpretability compared to traditional activation functions in deep networks?
- Basis in paper: [explicit] The paper proposes ortho-polynomial activations as alternatives to traditional functions, citing their universal approximation properties and potential for better feature control and regularization
- Why unresolved: The paper presents the theoretical advantages but does not provide empirical comparisons with traditional activation functions on benchmark tasks
- What evidence would resolve it: Experimental studies comparing ortho-polynomial networks with traditional networks on standard datasets, measuring accuracy, training efficiency, and interpretability of learned features

## Limitations
- The theoretical analysis of equilibrium distributions assumes specific forms of gradient noise that may not hold universally across all network architectures and datasets
- Proposed remedies (pre-pruning, ortho-polynomial activations) lack extensive empirical validation across diverse tasks and network sizes
- The Sorensen reordering heuristic may become computationally prohibitive for very large networks

## Confidence
- High Confidence: The core mathematical framework linking permutative symmetry to equivalent optima is rigorous and well-established
- Medium Confidence: Claims about Hessian eigenvalue concentration and resulting overlap of equilibrium regions are theoretically plausible but may depend on specific network architectures
- Low Confidence: Practical effectiveness of proposed remedies across diverse real-world tasks requires more empirical validation

## Next Checks
1. **Empirical validation of equilibrium distributions**: Train identical networks with different random seeds on the same data, measure the spread of final parameters, and compare with theoretical predictions of equilibrium region sizes
2. **Pre-pruning sensitivity analysis**: Systematically vary pre-pruning rates (ρ) across multiple network depths and widths, measuring both the reduction in equivalent optima and the impact on final task performance
3. **Ortho-polynomial vs. traditional activation comparison**: Implement networks using different ortho-polynomial bases (Legendre, Laguerre, Hermite) and compare their approximation efficiency and optimization stability against standard ReLU/tanh networks on benchmark regression tasks