---
ver: rpa2
title: Joint-Dataset Learning and Cross-Consistent Regularization for Text-to-Motion
  Retrieval
arxiv_id: '2407.02104'
source_url: https://arxiv.org/abs/2407.02104
tags:
- motion
- retrieval
- learning
- cccl
- text-to-motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of text-to-motion and motion-to-text
  retrieval by introducing joint-dataset learning and a novel Cross-Consistent Contrastive
  Loss (CCCL). The authors propose MoT++, an enhanced motion encoder based on transformer
  architecture with improved spatio-temporal attention mechanisms, and demonstrate
  its effectiveness on the KIT Motion-Language and HumanML3D datasets.
---

# Joint-Dataset Learning and Cross-Consistent Regularization for Text-to-Motion Retrieval

## Quick Facts
- arXiv ID: 2407.02104
- Source URL: https://arxiv.org/abs/2407.02104
- Authors: Nicola Messina; Jan Sedmidubsky; Fabrizio Falchi; Tomáš Rebok
- Reference count: 40
- Key outcome: MoT++ with CCCL outperforms state-of-the-art on text-to-motion retrieval, achieving up to 1.5% improvement in R@10 metric, particularly in cross-dataset scenarios.

## Executive Summary
This paper addresses the challenge of text-to-motion and motion-to-text retrieval by introducing joint-dataset learning and a novel Cross-Consistent Contrastive Loss (CCCL). The authors propose MoT++, an enhanced motion encoder based on transformer architecture with improved spatio-temporal attention mechanisms, and demonstrate its effectiveness on the KIT Motion-Language and HumanML3D datasets. Their approach outperforms state-of-the-art methods, particularly in cross-dataset scenarios, achieving up to 1.5% improvement in R@10 metric. The CCCL loss function regularizes the learned embedding space by imposing both cross-modal and uni-modal constraints, helping to mitigate data scarcity issues.

## Method Summary
The method combines MoT++, a transformer-based motion encoder that preserves joint spatial structure through factorized spatial-temporal attention, with an ACTORStyleEncoder for text processing. The key innovation is the Cross-Consistent Contrastive Loss (CCCL), which combines standard cross-modal contrastive learning with KL divergence terms that align cross-modal matching scores with uni-modal similarity scores. The approach uses joint-dataset learning by training simultaneously on HumanML3D and KITML datasets, with a linear scheduling for the lambda parameter that transitions from teacher-supervised to self-sustained learning. The model is trained for 250 epochs with learning rate 5e-5.

## Key Results
- MoT++ with CCCL achieves state-of-the-art performance on text-to-motion retrieval tasks
- Joint-dataset learning significantly improves cross-dataset generalization (training on one dataset, testing on another)
- Up to 1.5% improvement in R@10 metric compared to baseline methods
- CCCL loss function effectively regularizes the embedding space, improving performance in data-scarce scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint-dataset learning improves generalization by increasing training sample diversity and reducing overfitting.
- Mechanism: By training simultaneously on HumanML3D and KITML, the model is exposed to a broader distribution of motion-text pairs, which helps it learn more robust and generalizable representations.
- Core assumption: Motion-text representations learned from one dataset are transferable to another dataset, despite differences in motion granularity and textual annotation style.
- Evidence anchors:
  - [abstract]: "We explore cross-dataset evaluation and joint-dataset learning to understand and mitigate generalization issues due to the lack of sufficiently diverse data."
  - [section]: "In this paper, we progress towards scenarios that employ the two key datasets in this domain as training data in a joint-dataset learning setup."
  - [corpus]: Weak signal; neighboring papers focus on generation rather than retrieval, so limited corroboration.
- Break condition: If the motion encoders overfit to dataset-specific biases, joint training could hurt rather than help.

### Mechanism 2
- Claim: Cross-Consistent Contrastive Loss (CCCL) enforces better alignment between cross-modal and uni-modal similarity distributions.
- Mechanism: CCCL adds KL divergence terms that align cross-modal matching scores (text-to-motion and motion-to-text) with uni-modal similarity scores (text-to-text and motion-to-motion), encouraging the learned space to respect semantic relationships within each modality.
- Core assumption: Similarity scores in the learned embedding space should reflect both cross-modal and intra-modal semantic relationships.
- Evidence anchors:
  - [abstract]: "We propose a new loss function, which we call Cross-Consistent Contrastive Loss (CCCL), that regularizes the learned embedding space by imposing some cross-consistency constraints among the scores computed within and across modalities."
  - [section]: "The idea behind CCCL is that the constraints imposed by the two uni-modal objectives help the standard cross-modal contrastive objective, applying semantic constraints to the common space and, in turn, helping the generalization to different scenarios also in cases of data scarcity."
  - [corpus]: No direct support; neighboring papers do not discuss CCCL-style regularization.
- Break condition: If the teacher model's supervision is too domain-biased or if the KL alignment over-constrains the space.

### Mechanism 3
- Claim: MoT++ improves motion encoding by preserving spatial structure and joint information better than flat vector approaches.
- Mechanism: MoT++ uses a factorized encoder transformer that processes joint groups and preserves foot/root information via dedicated MLPs, maintaining a two-dimensional joint-time sequence rather than flattening.
- Core assumption: Preserving joint spatial structure and foot/root cues is more informative than collapsing to a single flat vector for retrieval tasks.
- Evidence anchors:
  - [section]: "One of the key advantages of MoT++ over other proposed encoders is that it employs a well-structured spatial sequence of joint tokens instead of a single flattened feature vector representing the whole skeleton."
  - [section]: "We employ a set of independent MLPs, which separately aggregate joints from seven different parts of the skeleton... We then include two separate MLPs for independently processing the root bone and feet floor contact state."
  - [corpus]: No direct support; neighboring papers focus on generation, not retrieval, and do not discuss MoT++-style encoders.
- Break condition: If the joint-grouping and extra MLPs add noise or overfit to specific joint configurations.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The core retrieval task relies on pulling positive text-motion pairs together and pushing negatives apart in the shared embedding space.
  - Quick check question: How does the InfoNCE loss weight the contribution of hard negatives versus easy negatives in the retrieval setting?

- Concept: Transformer-based spatio-temporal modeling
  - Why needed here: Skeleton sequences are inherently spatial (joint coordinates) and temporal (movement over time), requiring attention mechanisms to capture long-range dependencies.
  - Quick check question: Why does factorized encoder (spatial then temporal) perform better than factorized self-attention (interleaved spatial/temporal) for motion retrieval?

- Concept: Multi-modal embedding alignment
  - Why needed here: To enable cross-modal search, motion and text embeddings must live in a shared space where cosine similarity reflects semantic relevance.
  - Quick check question: What role do the two CLS tokens (for mean and variance) play in the embedding space, and why are only means used for retrieval?

## Architecture Onboarding

- Component map: Input → J(x) joint grouping → CLS tokens → Factorized transformer (spatial → temporal) → CLS outputs (mean/variance) → InfoNCE + CCCL loss → Output embeddings
- Critical path: Skeleton sequence → joint grouping → transformer layers → mean embedding → cross-modal contrastive loss → CCCL regularizer → backprop
- Design tradeoffs: Joint grouping reduces sequence length (speed) but risks losing fine-grained joint interactions; factorized encoder is memory-efficient but may miss joint-time couplings that interleaved attention would catch
- Failure signatures: Overfitting to one dataset (low cross-dataset R@10), poor text-to-motion scores despite good motion-to-text scores (embedding space asymmetry), high variance in retrieval ranks (unstable training)
- First 3 experiments:
  1. Train MoT++ with InfoNCE only on KITML; measure R@10 and rank distribution
  2. Add CCCL without joint-dataset learning; compare to step 1
  3. Switch to joint-dataset training (KITML+HumanML3D) with CCCL; compare cross-dataset performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different temporal modeling strategies (e.g., factorized encoder vs. factorized self-attention) affect the performance of MoT++ across various motion-text retrieval tasks?
- Basis in paper: [explicit] The authors mention that factorized encoder works better than factorized self-attention for the motion domain, but they don't provide a detailed comparison of the two strategies across different tasks and datasets.
- Why unresolved: The paper only briefly mentions the superiority of factorized encoder without providing a comprehensive analysis of the trade-offs between the two temporal modeling approaches.
- What evidence would resolve it: A thorough ablation study comparing the performance of MoT++ with factorized encoder and factorized self-attention on various motion-text retrieval tasks and datasets would help determine the optimal temporal modeling strategy for different scenarios.

### Open Question 2
- Question: How does the proposed CCCL loss function compare to other contrastive learning approaches in terms of effectiveness and efficiency for text-to-motion retrieval?
- Basis in paper: [inferred] The authors introduce CCCL as a novel loss function that imposes uni-modal constraints to improve generalization, but they don't compare it to other state-of-the-art contrastive learning approaches.
- Why unresolved: The paper doesn't provide a comprehensive comparison of CCCL with other contrastive learning methods, making it difficult to assess its relative effectiveness and efficiency.
- What evidence would resolve it: A thorough comparison of CCCL with other contrastive learning approaches, such as InfoNCE with filtering or triplet loss, on various motion-text retrieval tasks and datasets would help determine the optimal loss function for different scenarios.

### Open Question 3
- Question: How does the proposed joint-dataset learning approach affect the performance of MoT++ on unseen datasets and domains?
- Basis in paper: [explicit] The authors demonstrate the benefits of joint-dataset learning on the KITML and HumanML3D datasets, but they don't explore its effectiveness on other unseen datasets and domains.
- Why unresolved: The paper only evaluates the joint-dataset learning approach on two specific datasets, leaving the generalizability of the method to other datasets and domains unclear.
- What evidence would resolve it: Evaluating the performance of MoT++ with joint-dataset learning on a diverse set of unseen datasets and domains would help determine the robustness and generalizability of the approach.

## Limitations
- The exact implementation details of the skeleton unification pipeline and rifke feature computation are not fully specified
- The ACTORStyleEncoder's architecture and integration with the two CLS tokens lacks complete specification
- The motion label clustering methodology for motion-to-motion retrieval evaluation is not described
- The paper does not address potential dataset-specific biases that could influence reported performance gains

## Confidence
- **High Confidence**: The experimental results demonstrating improved R@10 scores and overall retrieval performance, particularly in cross-dataset scenarios. The methodology for training and evaluation is clearly specified.
- **Medium Confidence**: The mechanism by which CCCL regularizes the embedding space through KL divergence constraints. While the theoretical framework is sound, the practical impact depends on implementation details not fully specified.
- **Medium Confidence**: The generalization benefits from joint-dataset learning. The results support this claim, but the extent to which this generalizes to other datasets or motion domains remains untested.

## Next Checks
1. **Implementation Verification**: Reproduce the MoT++ encoder with the specified factorized configuration and verify that joint grouping and the dedicated MLPs for root/feet processing are correctly implemented, comparing performance against a baseline flat vector approach.

2. **CCCL Ablation Study**: Conduct an ablation study isolating the CCCL loss components by training with only InfoNCE, only the KL divergence terms, and the full CCCL, measuring the individual contributions to cross-dataset generalization.

3. **Cross-Dataset Robustness**: Evaluate the model's performance when training on combinations of other motion datasets (e.g., BABEL, AIST) to assess whether the joint-dataset learning benefits extend beyond the KITML-HumanML3D pairing.