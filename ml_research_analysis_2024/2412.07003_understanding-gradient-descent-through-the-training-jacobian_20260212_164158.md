---
ver: rpa2
title: Understanding Gradient Descent through the Training Jacobian
arxiv_id: '2412.07003'
source_url: https://arxiv.org/abs/2412.07003
tags:
- training
- singular
- bulk
- jacobian
- subspace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the training Jacobian\u2014the Jacobian\
  \ of final network parameters with respect to initial values\u2014as a lens for\
  \ understanding neural network training dynamics. By computing this Jacobian using\
  \ forward-mode automatic differentiation, the authors identify a low-dimensional\
  \ \"bulk\" subspace of parameter space along which perturbations are nearly unchanged\
  \ by training."
---

# Understanding Gradient Descent through the Training Jacobian

## Quick Facts
- arXiv ID: 2412.07003
- Source URL: https://arxiv.org/abs/2412.07003
- Reference count: 10
- Primary result: Neural network training leaves most parameter space nearly unchanged, operating primarily in a low-dimensional active subspace complementary to a large "bulk" subspace.

## Executive Summary
This paper introduces the training Jacobian—the Jacobian of final network parameters with respect to initial values—as a novel lens for understanding neural network training dynamics. By computing this Jacobian using forward-mode automatic differentiation, the authors identify a low-dimensional "bulk" subspace of parameter space along which perturbations are nearly unchanged by training. This bulk subspace constitutes roughly two-thirds of the parameter space and appears to depend on input data structure rather than labels. The bulk subspace has minimal effect on in-distribution predictions but influences out-of-distribution behavior. The paper also finds that independently trained networks share similar bulk subspaces, and that restricting training to the bulk prevents learning, while training in the complementary subspace works effectively.

## Method Summary
The authors compute the training Jacobian using forward-mode automatic differentiation (JAX's jacfwd) to analyze how small perturbations to initial parameters propagate through training to affect final parameters. They perform singular value decomposition on the training Jacobian to identify distinct regions: chaotic directions (large singular values), bulk directions (singular values near one), and stable directions (small singular values). The analysis is validated through perturbation experiments along singular vector directions and subspace training experiments that restrict optimization to either the bulk or its complement. The approach is demonstrated on small MLPs trained on UCI digits, MNIST, and CIFAR-10 datasets.

## Key Results
- The training Jacobian has a large bulk subspace (approximately 2/3 of parameter space) where singular values are close to one, indicating parameters along these directions remain nearly unchanged during training
- The bulk subspace depends on input data structure rather than labels, with minimal effect on in-distribution predictions but significant effects on out-of-distribution behavior
- Independently trained networks with different random seeds share substantial overlap in their bulk subspaces
- Training restricted to the bulk prevents learning, while training in the complementary subspace works effectively, demonstrating that neural networks train primarily in a low-dimensional active subspace

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training leaves most of parameter space nearly unchanged because the training Jacobian has a large bulk subspace where singular values are close to one
- Mechanism: The Jacobian maps small perturbations in initialization space to perturbations in final parameter space. When a singular value is close to one, the corresponding perturbation direction is preserved almost exactly through training, meaning parameters along that direction remain near their initial values
- Core assumption: The training Jacobian is well-approximated by its first-order Taylor expansion around the initialization point
- Evidence anchors: [abstract] "a large 'bulk' region of values extremely close to one" and "Along each bulk direction, the left and right singular vectors are nearly identical, indicating that perturbations to the initialization are carried through training almost unchanged."

### Mechanism 2
- Claim: The bulk subspace depends on input data structure rather than labels, causing it to have minimal effect on in-distribution predictions but influence out-of-distribution behavior
- Mechanism: Parameters in the bulk subspace are largely untouched by training because perturbations along these directions don't significantly affect the loss on the training data distribution. However, these same perturbations can affect predictions on out-of-distribution data where the model's learned invariances no longer hold
- Core assumption: The data distribution contains structure that makes certain parameter directions irrelevant for fitting the training data but relevant for generalization to unseen distributions
- Evidence anchors: [abstract] "These perturbations have virtually no effect on the network's output in-distribution, yet do have an effect far out-of-distribution."

### Mechanism 3
- Claim: Training can be effectively restricted to a low-dimensional active subspace complementary to the bulk, while training in the bulk subspace alone prevents learning
- Mechanism: The active subspace (complement of bulk) contains directions where parameter changes significantly affect the training loss and can be optimized by gradient descent. Training in this subspace alone achieves comparable performance to full training. Conversely, the bulk subspace contains directions that don't affect the training loss, so optimization there is ineffective
- Core assumption: The training loss landscape has a low-dimensional structure where most directions in parameter space are flat (don't affect loss) while a smaller subspace contains all the curvature needed for optimization
- Evidence anchors: [abstract] "restricting training to the bulk prevents learning, while training in the complementary subspace works effectively"

## Foundational Learning

- Concept: Jacobian matrix and its singular value decomposition
  - Why needed here: The entire analysis hinges on computing and interpreting the SVD of the training Jacobian to understand how parameter space is transformed during training
  - Quick check question: If a Jacobian has singular values of 0.1, 1.0, and 10.0, what happens to perturbations along each corresponding singular vector direction during training?

- Concept: Linear approximation of nonlinear dynamical systems
  - Why needed here: The training Jacobian provides a local linear approximation of the highly nonlinear training dynamics, allowing analysis of how small perturbations evolve
  - Quick check question: Under what conditions does the linear approximation of training dynamics break down, and how might this affect the observed bulk structure?

- Concept: Principal angles and subspace similarity measures
  - Why needed here: Comparing bulk subspaces across different training runs requires measuring similarity between high-dimensional subspaces
  - Quick check question: If two bulk subspaces have principal angles of 0° and 90°, what does this tell you about their relationship?

## Architecture Onboarding

- Component map: Network initialization -> Forward pass -> Parameter extraction -> Forward-mode AD (jax.jacfwd) -> Jacobian computation -> SVD analysis -> Perturbation experiments -> Subspace training validation
- Critical path: 1. Initialize network and compute forward pass to get final parameters 2. Use forward-mode AD to compute Jacobian of final parameters w.r.t. initial parameters 3. Compute SVD of Jacobian to identify bulk, chaotic, and stable regions 4. Validate findings through perturbation experiments and subspace training tests
- Design tradeoffs:
  - Memory vs. accuracy: Forward-mode AD has memory complexity quadratic in parameters but constant in training steps, making it feasible for small networks
  - Computational cost: Full Jacobian computation scales poorly with network size, requiring approximations for larger models
  - Interpretability vs. completeness: Focusing on bulk subspace provides insights but may miss other important training dynamics
- Failure signatures:
  - No bulk structure observed: May indicate unstructured data, different architecture, or nonlinear training dynamics dominating
  - Bulk subspace depends strongly on initialization: Suggests the linearization assumption is breaking down
  - Subspace training fails to work: Could indicate issues with the projection implementation or that the active subspace is larger than expected
- First 3 experiments:
  1. Reproduce the basic Jacobian computation and bulk identification on a small MLP with structured data
  2. Verify the linear regime along bulk directions by performing perturbation experiments across multiple orders of magnitude
  3. Test subspace training by restricting optimization to bulk vs. complement subspaces and comparing performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the bulk subspace emerge during training or is it pre-determined by initialization?
- Basis in paper: [explicit] The paper notes that bulk directions have cosine similarity close to 1 between left and right singular vectors, suggesting linear preservation, but doesn't determine whether this structure exists from initialization or emerges during training
- Why unresolved: The authors only analyze the final training Jacobian and don't track how the singular value spectrum evolves during training
- What evidence would resolve it: Computing the training Jacobian at multiple checkpoints during training would reveal whether the bulk structure gradually emerges or exists from the start

### Open Question 2
- Question: Is the bulk subspace an inherent property of the network architecture or does it depend on specific training hyperparameters?
- Basis in paper: [inferred] The authors find that bulk depends on input data but is "roughly independent of random seed and labels," yet they only test one architecture (MLP) and one optimizer (SGD)
- Why unresolved: Limited architectural diversity in experiments prevents distinguishing between architectural vs. training-dependent properties
- What evidence would resolve it: Testing multiple architectures (CNNs, transformers) and optimizers (Adam, SGD with momentum) would reveal whether the bulk is a universal property

### Open Question 3
- Question: What is the precise relationship between the bulk subspace and neural network generalization?
- Basis in paper: [explicit] The authors show bulk perturbations have minimal in-distribution effects but larger OOD effects, suggesting a connection to generalization, but don't establish causality
- Why unresolved: Correlation between bulk presence and generalization behavior is shown, but no ablation studies demonstrate whether manipulating the bulk directly affects generalization
- What evidence would resolve it: Training experiments that artificially expand or contract the bulk subspace would reveal whether it causally influences generalization performance

### Open Question 4
- Question: How does the bulk subspace scale with network width and depth?
- Basis in paper: [explicit] Authors note "Our work is limited insofar as we focus on small-scale models" and extend analysis to LeNet-5 (60K parameters) but don't explore scaling trends
- Why unresolved: Computational constraints limited analysis to small networks, preventing understanding of how bulk dimensionality scales with model size
- What evidence would resolve it: Systematic experiments across networks of varying width/depth would reveal scaling laws for bulk subspace dimensionality relative to total parameter count

### Open Question 5
- Question: Can the bulk subspace be leveraged for practical model compression or efficient training?
- Basis in paper: [inferred] The authors show training restricted to bulk prevents learning while training in the complement works well, suggesting potential for targeted training strategies
- Why unresolved: The paper identifies the bulk but doesn't explore whether this insight can be practically exploited for optimization
- What evidence would resolve it: Developing training algorithms that explicitly avoid or minimize updates along bulk directions could test whether this improves training efficiency or enables compression

## Limitations

- The analysis relies heavily on linearization assumptions that may break down for large learning rates or highly nonlinear training regimes
- Computational complexity limits analysis to small networks (thousands of parameters), preventing validation on large-scale models
- The relationship between bulk structure and generalization is correlational rather than causal, requiring further experimental validation

## Confidence

- High Confidence: The existence of a bulk subspace with singular values close to one, and the empirical observation that training is ineffective when restricted to this subspace while effective in the complement
- Medium Confidence: The interpretation that bulk structure reflects data-dependent invariances, and that bulk perturbations minimally affect in-distribution predictions but impact OOD behavior
- Low Confidence: The generalizability of bulk structure to large-scale models and different architectures, and the precise relationship between bulk structure and generalization phenomena

## Next Checks

1. **Cross-Architecture Validation**: Replicate the bulk analysis on different architectures (CNNs, Transformers) and training tasks (language modeling, object detection) to test the universality of the observed structure

2. **Nonlinear Dynamics Test**: Systematically vary learning rates and optimization algorithms to identify the boundary where linear approximation breaks down, and measure how bulk structure changes across this transition

3. **Data Structure Ablation**: Conduct controlled experiments varying data structure while holding labels constant (e.g., permuting pixel positions, adding structured noise) to more precisely map the relationship between data geometry and bulk subspace formation