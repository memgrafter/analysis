---
ver: rpa2
title: 'ACPBench: Reasoning about Action, Change, and Planning'
arxiv_id: '2410.05669'
source_url: https://arxiv.org/abs/2410.05669
tags:
- location
- tasks
- action
- tile
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ACPBench, a benchmark for evaluating reasoning
  tasks in planning. It consists of 7 reasoning tasks across 13 planning domains,
  generated from formal PDDL descriptions to ensure provably correct solutions.
---

# ACPBench: Reasoning about Action, Change, and Planning

## Quick Facts
- **arXiv ID**: 2410.05669
- **Source URL**: https://arxiv.org/abs/2410.05669
- **Reference count**: 24
- **Primary result**: ACPBench benchmark reveals significant LLM performance gaps on planning reasoning tasks, with finetuning small models achieving parity with larger models

## Executive Summary
ACPBench introduces a benchmark for evaluating reasoning about actions, change, and planning (ACP) using formal PDDL domain descriptions to generate provably correct synthetic datasets. The benchmark consists of 7 reasoning tasks across 13 planning domains, evaluating LLMs on boolean and multiple-choice questions. Experiments with 22 state-of-the-art LLMs and OpenAI o1 models reveal substantial performance gaps, particularly on boolean questions. The authors demonstrate that finetuning a small Granite-code 8B model with QLoRA significantly improves performance, achieving results comparable to much larger models even on unseen domains.

## Method Summary
The ACPBench benchmark is constructed from planning domains described in PDDL (Planning Domain Definition Language), enabling the synthesis of problems with provably correct solutions. The benchmark includes 7 reasoning tasks: action applicability, progression, plan validation, action reachability, goal reachability, goal plan existence, and goal plan uniqueness. These tasks are evaluated across 13 domains using both boolean and multiple-choice question formats. The evaluation uses Chain-of-Thought prompting with 2-shot examples, and finetuning experiments employ QLoRA on Granite-code 8B. The framework automatically generates synthetic datasets at scale while maintaining correctness guarantees through formal domain representations.

## Key Results
- Finetuning Granite-code 8B on ACPBench tasks improves boolean question accuracy from 51.43% to 95.71% and multi-choice accuracy from 19.18% to 94.29%
- Finetuned small model performance matches or exceeds larger pretrained models on both seen and unseen domains
- OpenAI o1 shows significant gains on multi-choice questions but minimal improvement on boolean questions compared to other models
- Chain-of-Thought prompting with 2-shot examples improves performance but LLMs still struggle with fundamental reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ACPBench leverages formal PDDL descriptions to generate provably correct datasets at scale.
- **Mechanism**: By using formal domain representations, the benchmark can automatically synthesize problems and their correct solutions without human labeling.
- **Core assumption**: The PDDL formalism is both complete and correct for the planning tasks.
- **Evidence anchors**: [abstract] "The collection is constructed from planning domains described in a formal language. This allows us to synthesize problems with provably correct solutions across many tasks and domains." [section 3.1] "Twelve of these domains are well-established benchmarks in both planning and reinforcement learning communities, readily available in PDDL format."
- **Break condition**: If the PDDL representation is incomplete or incorrect for a domain, the generated datasets may contain errors.

### Mechanism 2
- **Claim**: Finetuning small Granite-code 8B model on ACPBench tasks enables performance on par with much larger models.
- **Mechanism**: Instruction-tuned finetuning with QLoRA adapts the smaller model to the specific reasoning patterns in ACPBench, capturing task-specific knowledge efficiently.
- **Core assumption**: The reasoning patterns in ACPBench generalize well from seen to unseen domains.
- **Evidence anchors**: [section 4.2] "The fine-tuning resulted in substantial improvements in performance across tasks and even demonstrated the ability to generalize to previously unseen domains." [section 4.2] "Upon finetuning, the average accuracy of the model improves from 51.43% to 95.71% on boolean questions and from 19.18% to 94.29% on multi-choice questions."
- **Break condition**: If the unseen domains differ significantly in structure, generalization may fail.

### Mechanism 3
- **Claim**: Chain-of-Thought (COT) prompting with in-context examples significantly boosts LLM performance on ACPBench tasks.
- **Mechanism**: COT encourages step-by-step reasoning, and in-context examples provide demonstration of the reasoning process, improving task completion.
- **Core assumption**: The reasoning steps in COT align with the logical structure of ACPBench problems.
- **Evidence anchors**: [section 4.1] "We found that, with Chain-of-Thought prompting (COT) (Wei et al. 2022) and 2-shot examples, GPT-4o was only able to achieve 78.40% accuracy on MCQ questions in the ACPBench." [section 4.3] "For the two pretrained models, we see that while COT 2-shots prompting yields better result than IO, IO 2-shots prompting had the best performance."
- **Break condition**: If the reasoning steps required for a task are not decomposable, COT may not help.

## Foundational Learning

- **Concept**: Planning Domain Definition Language (PDDL)
  - **Why needed here**: ACPBench is built entirely from PDDL domains, so understanding PDDL is essential for generating new tasks or extending the benchmark.
  - **Quick check question**: What are the main components of a PDDL domain file (predicates, actions, effects)?

- **Concept**: Reasoning about Actions, Change, and Planning (ACP)
  - **Why needed here**: ACPBench evaluates specific reasoning skills (applicability, progression, reachability, etc.) that are foundational for planning.
  - **Quick check question**: Which ACPBench task evaluates whether a sequence of actions is a valid plan for a given goal?

- **Concept**: Chain-of-Thought (COT) prompting
  - **Why needed here**: COT is a key experimental condition in the paper, and understanding how it guides reasoning is critical for reproducing or extending results.
  - **Quick check question**: How does COT differ from simple input-output prompting in guiding LLM reasoning?

## Architecture Onboarding

- **Component map**: ACPBench → 7 reasoning tasks → 13 domains → PDDL formal descriptions → natural language templates → synthetic datasets. Evaluation pipeline → 22 pretrained LLMs + OpenAI o1 → finetuning experiments
- **Critical path**: Generate PDDL tasks → synthesize NL questions → evaluate with LLMs → analyze performance → finetune small model → re-evaluate
- **Design tradeoffs**: Formal PDDL ensures correctness but may limit natural language variability; synthetic generation enables scale but may miss real-world complexity
- **Failure signatures**: Poor performance on boolean questions despite strong MCQ results may indicate reasoning vs. pattern-matching differences; finetuning overfitting to seen domains
- **First 3 experiments**:
  1. Generate a small synthetic ACPBench subset from a new PDDL domain and verify correctness using a planner
  2. Run a baseline LLM (e.g., Granite-code 8B) on the synthetic dataset with IO and COT prompts; compare performance
  3. Finetune the same LLM on the synthetic dataset and evaluate on a held-out domain to test generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of OpenAI o1 models compare to multi-turn prompting of open-sourced LLMs like LLAMA-3.1 on ACPBench tasks, particularly in terms of cost-effectiveness?
- **Basis in paper**: [inferred] The paper compares OpenAI o1 models to LLAMA-3.1 and GPT-4o, noting that OpenAI o1 evaluation is approximately 20 times more expensive. It suggests that a multi-turn prompting of an open-sourced LLM like LLAMA-3.1 could potentially achieve similar improvements with lower cost.
- **Why unresolved**: The paper does not provide experimental data comparing multi-turn prompting of open-sourced LLMs to OpenAI o1 models.
- **What evidence would resolve it**: Conducting experiments where open-sourced LLMs like LLAMA-3.1 are prompted in a multi-turn manner and comparing their performance and cost to OpenAI o1 models on ACPBench tasks.

### Open Question 2
- **Question**: Can the finetuning approach used for Granite-code 8B be generalized to improve performance on other reasoning benchmarks beyond ACPBench and PlanBench?
- **Basis in paper**: [explicit] The paper demonstrates that finetuning Granite-code 8B on ACPBench tasks improves its performance not only on ACPBench but also on the plan generation task in PlanBench.
- **Why unresolved**: The paper only tests the finetuning approach on two specific benchmarks (ACPBench and PlanBench) and does not explore its applicability to other reasoning tasks or benchmarks.
- **What evidence would resolve it**: Applying the finetuning approach to Granite-code 8B or similar models on a diverse set of reasoning benchmarks and evaluating the performance improvements across these tasks.

### Open Question 3
- **Question**: What is the impact of prompt engineering on the performance of different LLMs on ACPBench tasks, and how does it compare to the improvements gained through finetuning?
- **Basis in paper**: [inferred] The paper mentions that performance of LLMs is sensitive to prompt text and style, and that prompt engineering could potentially elicit better performance. It also shows that finetuning a small model can bring its performance on par with larger models.
- **Why unresolved**: The paper does not extensively explore prompt engineering techniques or compare their effectiveness to finetuning in improving LLM performance on ACPBench tasks.
- **What evidence would resolve it**: Conducting experiments with various prompt engineering techniques on different LLMs and comparing the performance gains to those achieved through finetuning on ACPBench tasks.

## Limitations

- The benchmark relies entirely on the completeness and correctness of PDDL domain representations, which may not capture all real-world planning complexities
- Performance gaps on boolean questions suggest fundamental limitations in LLM reasoning capabilities that may not be fully addressed by finetuning
- The generalization results are based on a relatively small set of 13 domains (8 for training), limiting confidence in broader applicability

## Confidence

- **High Confidence**: The methodology for generating synthetic datasets from PDDL domains is sound and well-documented. The benchmark construction and evaluation framework are clearly specified.
- **Medium Confidence**: The finetuning results showing improved performance on both seen and unseen domains are compelling, but the sample size of domains (13 total, 8 for training) is relatively small for drawing broad conclusions about generalization.
- **Medium Confidence**: The comparison between different prompting strategies (IO vs COT) and their effectiveness across task types is well-supported, though the specific impact may vary with different model families.

## Next Checks

1. **Domain Coverage Validation**: Test the finetuned Granite-code 8B model on a broader set of planning domains beyond the current 13, particularly domains with different structural properties (e.g., temporal planning, numeric planning) to assess true generalization capability.

2. **Human Evaluation Study**: Conduct human expert evaluation of ACPBench questions to verify that the synthetic natural language questions accurately capture the intended reasoning tasks and that the correct answers align with human judgment.

3. **Alternative Benchmark Comparison**: Evaluate the same LLMs on complementary planning benchmarks (e.g., planning domain puzzles, classical planning benchmarks) to determine whether ACPBench performance correlates with broader planning reasoning capabilities or is specific to its synthetic task format.