---
ver: rpa2
title: Injecting Hierarchical Biological Priors into Graph Neural Networks for Flow
  Cytometry Prediction
arxiv_id: '2405.18507'
source_url: https://arxiv.org/abs/2405.18507
tags:
- hierarchical
- cells
- module
- self
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FCHC-GNN, a novel approach to inject hierarchical
  biological priors into graph neural networks for single-cell multi-class classification
  of flow cytometry data. By encoding known hierarchical relationships between cell
  types as a tree-structured constraint on the GNN output space, and using a custom
  hierarchical loss function during training, FCHC-GNN ensures predictions respect
  the inherent biology taxonomy.
---

# Injecting Hierarchical Biological Priors into Graph Neural Networks for Flow Cytometry Prediction

## Quick Facts
- arXiv ID: 2405.18507
- Source URL: https://arxiv.org/abs/2405.18507
- Reference count: 40
- Key outcome: FCHC-GNN significantly boosts performance on flow cytometry prediction by encoding biological taxonomies as hierarchical constraints on GNN outputs

## Executive Summary
This work introduces FCHC-GNN, a novel approach to inject hierarchical biological priors into graph neural networks for single-cell multi-class classification of flow cytometry data. By encoding known hierarchical relationships between cell types as a tree-structured constraint on the GNN output space, and using a custom hierarchical loss function during training, FCHC-GNN ensures predictions respect the inherent biology taxonomy. Experiments on a cohort of 19 patients demonstrate that incorporating these hierarchical priors significantly boosts performance across multiple metrics compared to baseline GNNs without such structured inductive biases.

## Method Summary
The FCHC-GNN method constructs k-nearest neighbors graphs from tabular flow cytometry data, then applies a custom hierarchical plug-in module to standard GNN architectures (GAT, SAGE, GCN). This module enforces that parent class scores are always at least as high as their subclass scores through a Max Constraint Module, and uses a custom hierarchical loss function (MCLoss) during training that allows subclass predictions to inform parent class training. The method is evaluated on 19 patients using hierarchical precision, recall, and F-score metrics.

## Key Results
- FCHC-GNN significantly outperforms baseline GNNs on hierarchical classification metrics for flow cytometry data
- Incorporating biological taxonomies as hierarchical constraints improves prediction consistency with known cell lineage relationships
- The approach maintains the same asymptotic time complexity as base GNNs with only a linear factor for the constraint layer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The FCHC-GNN's hierarchical constraint layer enforces logical consistency in cell type predictions by ensuring parent class scores are always at least as high as their subclass scores.
- Mechanism: The Max Constraint Module (MCM) computes each class prediction as the maximum score among all its subclasses. This guarantees that if a cell is predicted to belong to a specific subclass, it will also be classified as belonging to all its ancestor classes.
- Core assumption: The biological taxonomy represented as a tree structure accurately reflects the true hierarchical relationships between cell types.
- Evidence anchors:
  - [abstract] "encoding known hierarchical relationships between cell types as a tree-structured hierarchy imposed on the GNN output space"
  - [section] "MCMA = max(B∈SA HB) where SA is the set of subclasses of A in C"
  - [corpus] Weak - no direct evidence in corpus about this specific constraint mechanism
- Break condition: If the biological hierarchy is incorrectly specified or if there are actually non-tree relationships between cell types, the constraint could force incorrect predictions.

### Mechanism 2
- Claim: The custom hierarchical loss function (MCLoss) improves training by allowing subclass predictions to inform parent class training, preventing spurious local optima.
- Mechanism: MCLoss combines binary cross-entropy with a term that lets the ground truth labels of subclasses contribute to the loss calculation for parent classes. This creates a more effective gradient signal during training.
- Core assumption: The hierarchical structure provides meaningful information that can guide the optimization process beyond flat classification.
- Evidence anchors:
  - [abstract] "we use a custom hierarchical loss function that accounts for the hierarchical similarities between classes, in addition to the traditional cross-entropy loss"
  - [section] "MCLossA = −yA ln(max B∈SA yBHB)−(1−yA)ln(1−MCMA)"
  - [corpus] Weak - no direct evidence about this specific loss formulation in corpus
- Break condition: If the hierarchy is too shallow or the subclass-superclass relationships are not informative, the additional complexity of MCLoss may not provide benefits over standard cross-entropy.

### Mechanism 3
- Claim: Representing flow cytometry data as graphs with k-nearest neighbors captures biologically relevant neighborhood relationships between cells.
- Mechanism: Each cell is a node, and edges connect cells that are close in feature space (Euclidean distance). This graph structure allows the GNN to propagate information between similar cells during message passing.
- Core assumption: Cells with similar surface marker expression profiles are likely to belong to related cell types or exist in similar biological contexts.
- Evidence anchors:
  - [abstract] "representing the data as graphs and encoding hierarchical relationships between classes"
  - [section] "we construct the k-nearest neighbors graphs with k = 7 neighbors"
  - [corpus] Moderate - related work on GNNs for flow cytometry (paper 162694) suggests graph-based approaches are effective for flow data
- Break condition: If the k-NN graph construction creates artificial connections between biologically distinct cell types, or misses important relationships, the GNN may learn incorrect patterns.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: Flow cytometry data is naturally tabular, but cells exist in a continuous biological space where proximity in marker expression often indicates biological similarity
  - Quick check question: How does a GAT layer compute attention coefficients between neighboring nodes?

- Concept: Hierarchical classification and structured output spaces
  - Why needed here: Cell types in flow cytometry have natural taxonomic relationships (e.g., all T cells are lymphocytes), which flat classification ignores
  - Quick check question: What is the difference between hierarchical precision and standard precision?

- Concept: Biological domain knowledge integration into ML models
  - Why needed here: The model leverages known cell lineage relationships that are not present in the raw data but are crucial for accurate classification
  - Quick check question: Why might incorporating biological priors be especially important for domains with rich hierarchical relationships?

## Architecture Onboarding

- Component map:
  Input -> k-NN Graph Construction -> Early Module H (Base GNN) -> Max Constraint Module -> MCLoss -> Output

- Critical path:
  1. Construct k-NN graph from tabular data
  2. Pass through base GNN layers (attention/message passing)
  3. Apply Max Constraint Module to enforce hierarchy
  4. Compute MCLoss during training
  5. During inference, use constrained outputs for final predictions

- Design tradeoffs:
  - Fixed k=7 neighbors vs. adaptive neighborhood size
  - Linear max operation for constraint vs. learned transformation
  - Using all subclass predictions vs. only direct children in constraint

- Failure signatures:
  - All predictions collapsing to root class (constraint too strict)
  - Poor performance on leaf nodes despite good overall metrics (hierarchy too shallow)
  - Training instability or slow convergence (MCLoss formulation problematic)

- First 3 experiments:
  1. Compare FCHC-GAT vs. standard GAT on a small subset with known hierarchy violations
  2. Test different k values (3, 7, 15) for graph construction to find optimal neighborhood size
  3. Run ablation with MCLoss removed to quantify benefit of hierarchical loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FCHC-GNN performance scale with deeper and more complex hierarchical structures beyond what was tested?
- Basis in paper: [inferred] The paper mentions that hierarchical priors become more important as hierarchies grow deeper and more complex, but does not extensively test this claim.
- Why unresolved: The experiments focused on two specific hierarchical structures (the main 19-patient cohort and a shallower 30-patient cohort). Testing with progressively deeper hierarchies would provide more insight into scalability.
- What evidence would resolve it: Experiments applying FCHC-GNN to datasets with hierarchies of varying depths and complexities, demonstrating performance trends and identifying thresholds where hierarchical priors become critical.

### Open Question 2
- Question: How sensitive is FCHC-GNN performance to the choice of hyperparameters, such as the number of neighbors (k) in the k-NN graph construction?
- Basis in paper: [explicit] The paper states that k=7 neighbors was used in experiments but does not explore the impact of varying this parameter.
- Why unresolved: Hyperparameter sensitivity analysis was not conducted. Different choices of k could significantly impact the learned representations and classification performance.
- What evidence would resolve it: Systematic experiments varying k and other key hyperparameters, analyzing their impact on performance metrics to identify optimal settings and robustness.

### Open Question 3
- Question: How does the computational efficiency of FCHC-GNN compare to baseline GNNs as the dataset size increases?
- Basis in paper: [explicit] The paper mentions that FCHC-GNN maintains the same asymptotic time complexity as the base GNN with only a linear factor for the constraint layer.
- Why unresolved: While theoretical complexity is discussed, empirical runtime comparisons across varying dataset sizes are not provided. The practical scalability of FCHC-GNN needs to be validated.
- What evidence would resolve it: Runtime benchmarking experiments comparing FCHC-GNN against baseline GNNs on datasets of increasing size, measuring training and inference times to assess real-world scalability.

## Limitations
- Reliance on correctly specified biological hierarchies - the approach assumes tree-structured relationships are sufficient and accurate
- Lack of comparison against alternative hierarchical modeling approaches (e.g., hyperbolic embeddings, hierarchy-aware pooling)
- Relatively small patient cohort (19 patients) used for evaluation limits generalizability

## Confidence

**Confidence labels:**
- Mechanism 1 (hierarchical constraint): Medium - The max operation is well-defined but assumes tree-structured hierarchies are sufficient
- Mechanism 2 (MCLoss): Low-Medium - Empirical improvement shown but no ablation against simpler hierarchical losses
- Mechanism 3 (graph construction): Medium - k-NN is standard but optimal k value not rigorously established

## Next Checks
1. Perform ablation studies removing the hierarchical loss component to quantify its independent contribution to performance gains
2. Test the model on datasets with known violations of tree-structured hierarchies to evaluate robustness
3. Compare against alternative hierarchical GNN approaches (e.g., hyperbolic embeddings, hierarchy-aware pooling) to establish relative effectiveness