---
ver: rpa2
title: 'CELLM: An Efficient Communication in Large Language Models Training for Federated
  Learning'
arxiv_id: '2407.20557'
source_url: https://arxiv.org/abs/2407.20557
tags:
- training
- communication
- lora
- sparsity
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently training large
  language models (LLMs) in federated learning (FL) settings, where communication
  costs and limited client compute resources are significant bottlenecks. The proposed
  method, FLoSS (Federated LoRA with Simple Sparsity), combines low-rank adaptation
  (LoRA) with structured sparsity to reduce both local computation and communication
  overhead.
---

# CELLM: An Efficient Communication in Large Language Models Training for Federated Learning

## Quick Facts
- arXiv ID: 2407.20557
- Source URL: https://arxiv.org/abs/2407.20557
- Reference count: 40
- This paper introduces FLoSS, a federated learning method that reduces communication costs by up to 10x compared to vanilla LoRA while maintaining model utility.

## Executive Summary
This paper addresses the challenge of efficiently training large language models in federated learning settings, where communication costs and limited client compute resources are significant bottlenecks. The authors propose FLoSS (Federated LoRA with Simple Sparsity), which combines low-rank adaptation (LoRA) with structured sparsity to reduce both local computation and communication overhead. The method applies sparsity only during communication phases to minimize transmitted data while preserving training quality. Experiments across multiple datasets and model architectures demonstrate substantial communication cost reductions while maintaining or improving model utility, with robustness to statistical heterogeneity and support for privacy-preserving variations through differential privacy integration.

## Method Summary
FLoSS combines LoRA with sparsity to reduce communication costs in federated LLM training. LoRA reduces computational load by training only adapter parameters, while sparsity is applied only during communication phases to minimize transmitted data. The method uses top-k magnitude-based pruning to select the most significant weights for transmission, maintaining dense training locally for accuracy. The approach supports asymmetric sparsity ratios for upload/download phases and integrates with differential privacy for privacy preservation. Training uses FedAdam optimization with client sampling, and experiments validate performance across CIFAR10, 20NewsGroups, and Reddit datasets using ViT and GPT2 architectures.

## Key Results
- FLoSS reduces communication costs by up to 10x compared to vanilla LoRA
- Communication costs reduced by up to 5x over sparse LoRA baselines
- Model utility maintained or improved across tested datasets and architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsifying LoRA adapters only during communication phases reduces transmission size while preserving training quality.
- Mechanism: LoRA adapters are dense during local training to allow accurate gradient updates. Before upload/download, a top-k magnitude-based mask is applied, keeping only the largest weights for transmission. This retains the most impactful parameter changes while drastically reducing message size.
- Core assumption: The most significant LoRA updates are concentrated in a small fraction of parameters, and omitting smaller weights does not harm convergence.
- Evidence anchors:
  - [abstract] "we communicate sparse updates throughout training to significantly cut down on communication costs"
  - [section 4.2] "Top-k sparsity is applied to LoRA adapters before download and upload, reducing communication latency."
  - [corpus] Weak evidence - related papers mention sparsity but rarely justify magnitude-based selection or the sparse-only-at-communication design.
- Break condition: If gradient contributions are broadly distributed across weights, omitting smaller ones will slow convergence or destabilize updates.

### Mechanism 2
- Claim: Using unstructured sparsity rather than structured sparsity better preserves model utility in FL settings.
- Mechanism: Unstructured sparsity allows individual weight pruning, offering finer granularity and retaining more of the model's representational capacity compared to structured pruning which removes entire rows/columns.
- Core assumption: The model's performance is more sensitive to the total number of retained parameters than to maintaining structured patterns.
- Evidence anchors:
  - [section 4.2] "Unstructured sparsity is more useful in FLoSS setup than structured sparsity, as it does not retain sparsity during client local training."
  - [section 4.2] "training acceleration is not a consideration. Therefore, unstructured sparsity is the preferred sparsity method for communication in FLoSS."
  - [corpus] Weak evidence - no clear citation or empirical justification in neighboring works; assumption is asserted but not deeply validated.
- Break condition: If the FL system has hardware that accelerates structured sparsity, the efficiency benefit of unstructured sparsity diminishes.

### Mechanism 3
- Claim: Separating download and upload sparsity ratios accommodates asymmetric network speeds, improving overall throughput.
- Mechanism: Download phases can tolerate higher sparsity because they are faster; upload phases use lower sparsity to avoid bottlenecks. This asymmetry reduces total communication time.
- Core assumption: Upload bandwidth is significantly slower than download bandwidth, so upload sparsity has a larger impact on round duration.
- Evidence anchors:
  - [section 5.3] "upload being up to 10x slower" and "upload sparsity has a larger impact on round duration."
  - [section 4.2] "difference in sparsity ratio for download and upload bandwidths"
  - [corpus] No direct support; the claim relies on general networking knowledge rather than cited study.
- Break condition: If network asymmetry is reduced or eliminated, fixed equal sparsity ratios may suffice.

## Foundational Learning

- Concept: Federated Learning (FL) - decentralized training where clients never share raw data, only model updates.
  - Why needed here: FLoSS operates entirely in FL contexts, so understanding client-server communication and aggregation is essential.
  - Quick check question: What two main phases occur in each FL round, and which one is typically slower in wireless networks?

- Concept: Low-Rank Adaptation (LoRA) - a parameter-efficient fine-tuning method that injects small trainable matrices into transformer layers.
  - Why needed here: FLoSS builds directly on LoRA; knowing how it reparameterizes updates is key to understanding sparsity application.
  - Quick check question: In LoRA, which part of the model is frozen during training, and why does this reduce communication cost?

- Concept: Sparsity and Pruning - methods to reduce parameter count by zeroing out weights based on importance scores.
  - Why needed here: FLoSS relies on top-k magnitude pruning; understanding how and when pruning is applied is crucial for correct implementation.
  - Quick check question: What is the difference between structured and unstructured sparsity, and which does FLoSS use?

## Architecture Onboarding

- Component map: Pre-trained LLM backbone (frozen) -> LoRA adapters per transformer block (trainable) -> Top-k magnitude masks (download/upload) -> FedAdam optimizer for aggregation -> Communication scheduler (client sampling, round orchestration)

- Critical path:
  1. Server initializes LoRA adapters and applies download sparsity mask.
  2. Sampled clients download sparse adapters.
  3. Clients train dense adapters locally for one epoch.
  4. Clients apply upload sparsity mask to updates.
  5. Server aggregates sparse updates via FedAdam.
  6. Repeat until convergence.

- Design tradeoffs:
  - Dense local training vs. sparse communication: better accuracy but higher communication cost.
  - Unstructured vs. structured sparsity: more flexibility and accuracy but less hardware acceleration.
  - Asymmetric sparsity ratios: optimized for network asymmetry but adds configuration complexity.

- Failure signatures:
  - Model utility degrades sharply: likely sparsity ratio too aggressive or wrong rank selection.
  - Communication time does not improve: possibly mask application ineffective or client dropout.
  - Slow convergence: local training not sufficiently long, or sparsity removes too much information.

- First 3 experiments:
  1. Run FLoSS with rank=4, download_sparsity=0.25, upload_sparsity=0.25 on CIFAR10; compare accuracy to dense LoRA.
  2. Vary upload_sparsity from 0.1 to 0.5 while holding download_sparsity=0.25; measure communication time and accuracy.
  3. Test non-IID data partitions (Dirichlet Î±=0.1) to confirm robustness; compare with FedAdam + dense LoRA baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- The magnitude-based top-k sparsity selection lacks rigorous justification and may fail if gradient importance is distributed rather than concentrated.
- Asymmetric sparsity ratios depend heavily on assumed network asymmetry; no real-world network measurements are provided to validate the claimed 10x upload latency difference.
- Unstructured sparsity is claimed to outperform structured sparsity in FL, but hardware acceleration for structured sparsity could reverse this advantage in practical deployments.

## Confidence
- **High**: Communication cost reductions (10x vs vanilla LoRA, 5x vs sparse LoRA) and accuracy maintenance on tested datasets.
- **Medium**: Model robustness to non-IID data partitions and integration with differential privacy.
- **Low**: Claims about magnitude-based sparsity selection and unstructured sparsity superiority in FL.

## Next Checks
1. Test FLoSS on a federated dataset with verified network asymmetry to confirm asymmetric sparsity ratios improve throughput.
2. Evaluate structured vs unstructured sparsity performance on hardware that supports structured sparsity acceleration.
3. Conduct ablation studies on different sparsity selection criteria (e.g., random vs magnitude-based) to verify the importance of the magnitude-based approach.