---
ver: rpa2
title: 'Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of
  Alignment with Human Responses'
arxiv_id: '2410.07826'
source_url: https://arxiv.org/abs/2410.07826
tags:
- human
- scenarios
- dilemmas
- moral
- ethical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigated LLM calibration for moral ambiguity by
  comparing token probability distributions with human judgments in two datasets:
  DILEMMAS (paired moral scenarios) and ANECDOTES (real-world narratives). Models
  tested included Llama-3.1-8b, Zephyr-7b-beta, and Mistral-7b, using cross-entropy
  and Dirichlet losses to measure alignment.'
---

# Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of Alignment with Human Responses

## Quick Facts
- **arXiv ID**: 2410.07826
- **Source URL**: https://arxiv.org/abs/2410.07826
- **Reference count**: 1
- **Primary result**: Fine-tuning improved LLM alignment with human moral judgments, with Mistral-7b matching GPT-4o post-training

## Executive Summary
This study investigates how to calibrate language models for moral ambiguity by aligning token probability distributions with human judgments. The researchers compared several models including Llama-3.1-8b, Zephyr-7b-beta, and Mistral-7b across two datasets: DILEMMAS (paired moral scenarios) and ANECDOTES (real-world narratives). They employed cross-entropy and Dirichlet losses to measure alignment between model outputs and human ethical judgments.

The research demonstrates that while initial alignment was poor, fine-tuning significantly improved both cross-entropy and Dirichlet loss metrics, particularly for the latter. Post-fine-tuning, Mistral-7b achieved alignment performance comparable to GPT-4o on ethical reasoning tasks. However, the study found that even calibrated models remained less accurate than traditional transformer models like BERT/RoBERTa in cross-entropy terms, suggesting fundamental limitations in using probability distributions for ethical reasoning assessment.

## Method Summary
The study employed a comparative approach using three language models (Llama-3.1-8b, Zephyr-7b-beta, and Mistral-7b) tested on two datasets representing moral ambiguity. The DILEMMAS dataset contained paired moral scenarios requiring ethical judgment, while ANECDOTES provided real-world narratives with implicit moral content. Models were evaluated using cross-entropy loss and Dirichlet loss to measure alignment between predicted token probabilities and human judgments. Fine-tuning was performed using both loss functions, with particular emphasis on Dirichlet loss for multi-class probability alignment. The evaluation framework compared pre- and post-fine-tuned model performance against human annotations and established baselines including GPT-4o and traditional transformer models.

## Key Results
- Initial alignment between LLMs and human moral judgments was poor across both datasets
- Fine-tuning significantly improved alignment metrics, especially Dirichlet loss
- Post-fine-tuning, Mistral-7b matched GPT-4o performance on alignment tasks
- Models remained less calibrated than BERT/RoBERTa in cross-entropy terms even after fine-tuning

## Why This Works (Mechanism)
Fine-tuning improved alignment by optimizing model parameters to minimize the discrepancy between predicted token probability distributions and human ethical judgments. The use of Dirichlet loss was particularly effective because it directly models the alignment between multi-class probability distributions, capturing the uncertainty inherent in ethical reasoning. The fine-tuning process likely adjusted the models' internal representations to better capture the nuances of moral judgment that humans apply when evaluating ambiguous scenarios, though the exact mechanisms of how token-level probability adjustments translate to improved ethical reasoning remain unclear.

## Foundational Learning
The study builds on established techniques in model calibration and alignment, adapting methods typically used for classification tasks to the domain of ethical reasoning. By treating ethical judgments as probability distributions over possible responses rather than binary classifications, the researchers leveraged distributional alignment techniques. The work extends previous research on fine-tuning for task-specific alignment by demonstrating that similar approaches can be effective for subjective domains like ethics, where human judgments exhibit significant variability. The comparison with traditional transformer models like BERT/RoBERTa provides a foundation for understanding the relative strengths and limitations of different model architectures for this type of reasoning task.

## Architecture Onboarding
The study primarily used off-the-shelf language models without architectural modifications, focusing instead on fine-tuning existing models. The three models tested (Llama-3.1-8b, Zephyr-7b-beta, and Mistral-7b) represent different architectural approaches within the transformer framework, with varying parameter counts and training methodologies. The evaluation process leveraged these models' native capabilities for text generation and probability estimation, using cross-entropy and Dirichlet losses as training objectives. The comparative approach across multiple architectures provides insights into how different model designs respond to ethical alignment fine-tuning, though the study did not implement architectural changes specifically targeted at ethical reasoning tasks.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of fine-tuned models to broader ethical domains beyond the specific datasets evaluated. It questions whether the improvements in alignment metrics translate to genuine enhancements in ethical reasoning capability or merely reflect overfitting to the training distribution. The study also raises questions about the appropriate evaluation metrics for ethical reasoning in language models, noting that traditional measures like cross-entropy may not fully capture the quality of moral judgments. Additionally, the research suggests further investigation is needed into how different fine-tuning approaches and dataset characteristics contribute to observed improvements.

## Limitations
- Reliance on specific datasets (DILEMMAS and ANECDOTES) that may not capture full spectrum of moral ambiguity
- Binary annotation approach oversimplifies complex ethical judgments that often exist on continuums
- Token probability distributions as proxy for ethical reasoning may not fully capture nuanced aspects of moral judgment
- Limited evaluation to only two datasets restricts generalizability to broader ethical domains
- Lack of human evaluation studies to validate whether improved alignment metrics correspond to better ethical reasoning quality
- Comparison with BERT/RoBERTa may be misleading since these models are designed for different task types

## Confidence
- **High confidence** in technical implementation of fine-tuning methodology and observed improvements in alignment metrics
- **Medium confidence** in interpretation of improvements as genuine enhancements in ethical reasoning capability
- **Low confidence** in generalizability of findings to broader ethical domains beyond evaluated datasets

## Next Checks
1. Test fine-tuned models on additional ethical reasoning benchmarks beyond DILEMMAs and ANECDOTES to assess generalization across diverse moral scenarios and cultural contexts
2. Conduct human evaluation studies where domain experts assess quality and coherence of ethical reasoning in model outputs
3. Implement ablation studies to determine relative contribution of different fine-tuning approaches and dataset characteristics to observed improvements