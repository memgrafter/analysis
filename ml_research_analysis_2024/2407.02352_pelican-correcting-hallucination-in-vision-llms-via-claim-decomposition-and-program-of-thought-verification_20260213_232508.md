---
ver: rpa2
title: 'Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and
  Program of Thought Verification'
arxiv_id: '2407.02352'
source_url: https://arxiv.org/abs/2407.02352
tags:
- claim
- visual
- pelican
- object
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Pelican, a method for detecting and mitigating
  hallucinations in vision-language models. The approach decomposes visual claims
  into sub-claims using first-order predicates, generates Python code to answer these
  questions via external tools, and uses LLM reasoning to verify the overall claim.
---

# Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification

## Quick Facts
- arXiv ID: 2407.02352
- Source URL: https://arxiv.org/abs/2407.02352
- Reference count: 35
- Primary result: Reduces hallucination detection error by 27% compared to prior work on MMHal-Bench

## Executive Summary
This paper introduces Pelican, a method for detecting and mitigating hallucinations in vision-language models (LVLMs) by decomposing visual claims into sub-claims and verifying them using external visual tools. The approach uses first-order predicates to break down complex claims, generates Python code to answer these questions via tools like object detectors and VQA systems, and employs LLM reasoning to verify the overall claim. Pelican introduces intermediate variables for precise object grounding and shares computations between sub-claims to improve efficiency and consistency. The method demonstrates significant improvements in hallucination detection and correction across multiple benchmarks.

## Method Summary
Pelican addresses LVLM hallucinations through a three-stage pipeline: claim decomposition, Program-of-Thought (PoT) verification, and integrated verification synthesis. First, visual claims are decomposed into atomic sub-claims using first-order predicates (Exists, Count, Position, Attribute, etc.) forming a computational graph. Second, PoT verification generates Python code to answer sub-questions using external visual tools, with intermediate variables ensuring precise object grounding. Finally, integrated verification uses LLM reasoning to assess claim correctness based on sub-claim answers and confidence scores. The approach leverages a visual table representation combining YOLO and Grounding-DINO for entity detection, then uses these detections to ground verification of claims about object attributes, positions, and counts.

## Key Results
- Reduces hallucination detection error by 27% compared to prior work on MMHal-Bench
- Achieves up to 40% accuracy gains on object count and position tasks compared to baselines
- Improves object detection and attribute recognition accuracy on GA VIE and MME benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Breaking down complex visual claims into simpler sub-claims improves hallucination detection.
- Mechanism: Decomposes the claim into first-order predicates (Exists, Position, Color, etc.) forming a computational graph where each node represents a sub-claim.
- Core assumption: Simpler sub-questions are easier for external visual tools to answer accurately than complex holistic claims.
- Evidence anchors:
  - [abstract] "Pelican first decomposes the visual claim into a chain of sub-claims based on first-order predicates."
  - [section 3.2] "Since answering simpler questions about images using external tools...is more reliable, we first propose a novel way to decompose the visual claim C into atomic sub-claims."

### Mechanism 2
- Claim: Using intermediate variables for object referencing improves grounding precision.
- Mechanism: Introduces variables like $dog_right to reference specific object instances detected in the image, allowing precise grounding across sub-claims.
- Core assumption: Precise object references prevent ambiguity when multiple similar objects exist in an image.
- Evidence anchors:
  - [abstract] "Pelican introduces intermediate variables for precise grounding of object instances."
  - [section 3.2] "We introduce intermediate variables v to reference specific object instances, which is critical in verifying claims about specific object instances."

### Mechanism 3
- Claim: Sharing computations between sub-claims improves efficiency and consistency.
- Mechanism: Passes results from previous sub-claims as context when answering subsequent questions, enabling adaptive corrections and inconsistency detection.
- Core assumption: Reusing computations reduces redundant processing and catches errors propagating through the verification chain.
- Evidence anchors:
  - [abstract] "Pelican shares computations from previous nodes in the chain while answering questions, enabling adaptive corrections and the identification of inconsistencies."
  - [section 3.3] "We provide the answers derived in the previous sub-claims as context when answering the next question in the chain."

## Foundational Learning

- Concept: First-order logic predicates
  - Why needed here: Provides a structured way to break down complex visual claims into atomic verifiable components.
  - Quick check question: Can you list the basic predicates (Exists, Count, Position, Attribute) used for visual claim decomposition?

- Concept: Visual grounding and object detection
  - Why needed here: Ensures sub-claims are grounded in actual visual entities rather than language priors.
  - Quick check question: What are the two main object detection approaches mentioned and their respective strengths/weaknesses?

- Concept: Program-of-Thought prompting
  - Why needed here: Enables flexible composition of external visual tools through Python code generation.
  - Quick check question: How does PoT differ from traditional prompting approaches in multimodal contexts?

## Architecture Onboarding

- Component map: Image → Visual Table → Claim Decomposition → PoT Verification → Integrated Verification Synthesis
- Critical path: Image → Visual Table (entity detection) → Claim Decomposition (predicates) → PoT Code Generation (tool composition) → Answer Collection → Final Verification (LLM reasoning)
- Design tradeoffs: Uses external tools for precision but introduces brittleness; decomposes claims for accuracy but increases complexity; shares computations for efficiency but may propagate errors.
- Failure signatures: Incorrect object detection leading to wrong predicates; code generation errors causing tool invocation failures; LLM reasoning inconsistencies despite correct sub-answers.
- First 3 experiments:
  1. Test with simple single-object claims to validate basic decomposition and verification pipeline.
  2. Test with claims involving multiple objects to verify intermediate variable handling.
  3. Test with adversarial claims designed to trigger tool failures and measure error propagation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Pelican's reliance on brittle visual tools be mitigated while maintaining accuracy in object detection and attribute recognition?
- Basis in paper: [explicit] The paper discusses the brittleness of visual tools like YOLO and Grounding-DINO, which struggle with small objects, out-of-context objects, and objects outside their vocabulary.
- Why unresolved: The paper mentions efforts to mitigate these limitations through visual table representation and object class verification, but some failures persist.
- What evidence would resolve it: Development and testing of more robust visual tools or improved algorithms that can handle edge cases and novel objects with higher accuracy.

### Open Question 2
- Question: Can the randomness and inconsistency in LLM outputs be reduced to improve the reliability of Pelican's claim verification process?
- Basis in paper: [explicit] The paper notes that even with temperature set to 0, randomness in LLM outputs was observed, particularly in extracting visual entities and generating Python code.
- Why unresolved: The paper addresses this by generating code multiple times and using in-context examples, but these are workarounds rather than solutions.
- What evidence would resolve it: Implementation of techniques to enhance LLM consistency, such as improved prompt engineering or model fine-tuning, resulting in more deterministic outputs.

### Open Question 3
- Question: How can Pelican handle conflicting evidence from visual tools more effectively to avoid incorrect claim verification?
- Basis in paper: [explicit] The paper acknowledges that Pelican sometimes fails to handle conflicting evidence due to tool failures, leading to inaccuracies.
- Why unresolved: The paper does not provide a detailed solution for resolving such conflicts beyond noting the issue.
- What evidence would resolve it: Development of a conflict resolution mechanism within Pelican that can assess and reconcile conflicting tool outputs, possibly by integrating additional verification steps or leveraging more advanced reasoning models.

## Limitations
- Reliance on brittle external visual tools that can produce miss-detections or false positives
- Occasional LLM output inconsistencies requiring multiple code generation attempts
- Evaluation focuses on accuracy metrics without characterizing computational efficiency or latency

## Confidence

**High Confidence**: The core claim that decomposing visual claims into sub-claims using first-order predicates improves hallucination detection is well-supported by the methodology description and evaluation results on multiple benchmarks.

**Medium Confidence**: The claim about intermediate variables improving grounding precision is supported by the mechanism description, but the evaluation doesn't isolate this component's contribution from the overall system.

**Medium Confidence**: The claim about sharing computations improving efficiency and catching inconsistencies is theoretically sound but not empirically validated as a distinct contribution in the evaluation section.

## Next Checks

1. **Tool Failure Analysis**: Conduct ablation studies measuring system performance when individual visual tools fail or produce incorrect outputs. This would quantify how brittle the tool dependency truly is and whether Pelican can recover from individual tool errors.

2. **End-to-End Latency Measurement**: Profile the complete Pelican pipeline (decomposition → code generation → tool execution → verification) to measure computational overhead and response time compared to baseline LVLM approaches, as this is critical for practical deployment.

3. **Generalization Testing**: Evaluate Pelican on out-of-distribution images and claims not represented in the training benchmarks to assess robustness to novel visual scenarios and claim types beyond the tested MMHal-Bench, GA VIE, and MME datasets.