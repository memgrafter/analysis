---
ver: rpa2
title: Quebec Automobile Insurance Question-Answering With Retrieval-Augmented Generation
arxiv_id: '2410.09623'
source_url: https://arxiv.org/abs/2410.09623
tags:
- insurance
- question
- questions
- legal
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of providing reliable and accurate
  answers to Quebec automobile insurance questions, a critical area where misinformation
  can have serious consequences. The authors introduce two new corpora: a reference
  corpus of 21 Quebec automobile insurance documents and a corpus of 82 expert-answered
  questions.'
---

# Quebec Automobile Insurance Question-Answering With Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2410.09623
- Source URL: https://arxiv.org/abs/2410.09623
- Authors: David Beauchemin; Zachary Gagnon; Ricahrd Khoury
- Reference count: 22
- Primary result: RAG with domain-specific prompt engineering significantly improves answer accuracy over zero-shot baselines in technical insurance QA

## Executive Summary
This paper addresses the challenge of providing reliable and accurate answers to Quebec automobile insurance questions, a critical area where misinformation can have serious consequences. The authors introduce two new corpora: a reference corpus of 21 Quebec automobile insurance documents and a corpus of 82 expert-answered questions. They propose a Retrieval-Augmented Generation (RAG) approach using GPT-4o, with domain-specific prompt engineering and context compression. Their experiments show that using the complete reference corpus significantly improves performance compared to a zero-shot baseline, with automatic metrics like BLEU and ROUGE improving by 40% to 300%. Manual evaluation also shows better results. However, 5% to 13% of answers still contain false statements, highlighting that while RAG improves reliability, current LLM-based systems are not yet robust enough for widespread public use in critical domains like insurance.

## Method Summary
The authors implement a RAG pipeline using GPT-4o with LangChain. They create two new corpora: 21 Quebec automobile insurance documents as the reference corpus and 82 expert-answered questions. The system uses text-embedding-ada-002 to chunk and encode documents, retrieves top-5 relevant documents via cosine similarity, compresses context using GPT-4o, and generates answers with domain-specific prompt engineering. Six approaches are compared: zero-shot, no references, and incremental use of reference sources (laws, F.P.Q. 1, AMF, and all resources). Evaluation uses automatic metrics (BLEU, ROUGE, METEOR, BERTScore, MeaningBERT) and manual expert grading.

## Key Results
- RAG with complete reference corpus significantly outperforms zero-shot baseline (BLEU/ROUGE/METEOR improvements of 40-300%)
- Manual evaluation confirms better performance with reference corpus, but 5-13% of answers contain false statements
- The system improves reliability but is not yet robust enough for mass public use in critical insurance domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG with domain-specific prompt engineering significantly improves answer accuracy over zero-shot baselines in technical insurance QA.
- Mechanism: The retriever selects relevant chunks from the reference corpus, the context compressor merges them, and the generator uses a prompt that specifies role, domain, and language to produce accurate, concise answers.
- Core assumption: The reference corpus contains sufficient domain knowledge and the prompt engineering aligns the LLM's output with expert expectations.
- Evidence anchors:
  - [abstract] "Our results demonstrate that, on average, using our expertise reference corpus generates better responses on both automatic and manual evaluation metrics."
  - [section] "The compressor reduces the context size, thus keeping the prompt size within an acceptable range... This approach helps merge content from different sources to create a better-contextualized reference document."
  - [corpus] "The Quebec Automobile Insurance Expertise Reference Corpus includes legislation, contracts, and educational resources from official sources."
- Break condition: If the reference corpus lacks critical domain information or if prompt instructions are unclear, the LLM may hallucinate or produce incomplete answers.

### Mechanism 2
- Claim: Using the complete reference corpus yields higher BLEU, ROUGE, and METEOR scores than using no references or partial references.
- Mechanism: The LLM is provided with all relevant legal and insurance documents, allowing it to generate answers that closely match the vocabulary and structure of expert answers.
- Core assumption: More comprehensive context leads to better semantic alignment with ground truth answers.
- Evidence anchors:
  - [abstract] "Our results demonstrate that, on average, using our expertise reference corpus generates better responses on both automatic and manual evaluation metrics."
  - [section] "we observe that, for all automatic metrics, on average, the All references approach outperforms other methods."
  - [corpus] "The corpus includes legislation, contracts, and educational resources, providing comprehensive domain coverage."
- Break condition: If the LLM is confused by the inclusion of legal documents, it may produce longer but less accurate answers, as observed with partial references.

### Mechanism 3
- Claim: Manual evaluation reveals that while RAG improves reliability, 5% to 13% of answers still contain false statements, indicating current LLM-based systems are not yet robust enough for mass public use in critical domains.
- Mechanism: Even with domain-specific context, the LLM may misinterpret questions or context, leading to false answers that could mislead customers.
- Core assumption: The LLM's understanding of context and ability to infer intent is limited, especially in technical domains.
- Evidence anchors:
  - [abstract] "our results show that between 5% to 13% of answered questions include a false statement that could lead to customer misunderstanding."
  - [section] "In many cases, without specific references to Quebec insurance specifications, the response contained French insurance information."
  - [corpus] "The corpus is comprehensive but may not cover every edge case or nuanced question."
- Break condition: If the question is out of context or the LLM lacks the ability to abstain from answering when uncertain, it may generate false information.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) architecture
  - Why needed here: To provide the LLM with domain-specific context from official insurance documents, improving answer accuracy and reliability.
  - Quick check question: What are the key components of a RAG system and how do they work together?

- Concept: Prompt engineering for domain-specific tasks
  - Why needed here: To guide the LLM in producing answers that are concise, accurate, and aligned with expert expectations in the insurance domain.
  - Quick check question: How does specifying role, domain, and language in a prompt affect the LLM's output?

- Concept: Evaluation metrics for text generation (BLEU, ROUGE, METEOR)
  - Why needed here: To quantitatively assess the similarity between machine-generated answers and expert answers, guiding model development.
  - Quick check question: What do BLEU, ROUGE, and METEOR scores measure and how do they differ?

## Architecture Onboarding

- Component map:
  - Question → Retriever → Context Compressor → Generator → Answer

- Critical path:
  - Question → Retriever → Context Compressor → Generator → Answer

- Design tradeoffs:
  - Using more references improves accuracy but may confuse the LLM if the context is too broad.
  - Longer prompts may degrade answer quality due to the "lost-in-the-middle" effect.
  - Automatic metrics provide quick feedback but may not capture semantic nuances or legal risks.

- Failure signatures:
  - High BLEU/ROUGE scores but low manual evaluation grades indicate the LLM is using similar vocabulary but not producing accurate answers.
  - False answers in manual evaluation suggest the LLM is misinterpreting context or questions.
  - Consistently poor performance with partial references suggests the LLM is confused by incomplete context.

- First 3 experiments:
  1. Compare zero-shot baseline with RAG using no references to isolate the effect of prompt engineering.
  2. Test RAG with incremental addition of reference sources to identify the optimal context size.
  3. Conduct ablation study using individual reference sources to determine their relative importance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the risk of data leakage impact the reliability of the RAG system when using domain-specific legal documents?
- Basis in paper: [explicit] The paper discusses the potential for data leakage, where GPT-4o might have been trained using some of the reference documents, leading to overfitting and potential memorization of answers.
- Why unresolved: The extent and impact of data leakage on the system's performance and generalization to unseen data remain unclear.
- What evidence would resolve it: A controlled experiment comparing the system's performance on a dataset with and without overlap with the training data of GPT-4o.

### Open Question 2
- Question: Can the RAG system effectively handle questions that are out of the context of the provided references, and how does it abstain from answering in such cases?
- Basis in paper: [inferred] The paper mentions that the system always strives to answer questions, even when the context is unknown, and does not abstain from answering.
- Why unresolved: The system's behavior when faced with questions outside its domain of expertise is not fully explored, and the potential for hallucination or inappropriate responses is not addressed.
- What evidence would resolve it: A study analyzing the system's responses to a set of questions specifically designed to be out of the scope of the provided references.

### Open Question 3
- Question: How do automatic evaluation metrics, such as BLEU and ROUGE, correlate with human judgment in the context of legal and insurance question answering?
- Basis in paper: [explicit] The paper discusses the limitations of automatic metrics, such as their inability to capture semantic meaning and their weak correlation with human judgment, particularly in the legal domain.
- Why unresolved: The effectiveness of automatic metrics in evaluating the quality and reliability of answers in specialized domains like law and insurance is not fully established.
- What evidence would resolve it: A comparative study involving human evaluation of answers generated by the RAG system, alongside automatic metric scores, to assess the correlation between the two.

## Limitations

- 5-13% false answer rate in manual evaluation indicates system is not yet reliable enough for production deployment in high-stakes insurance contexts
- LLM cannot reliably detect when questions fall outside its knowledge scope and lacks abstention capability
- Small evaluation corpus (82 questions) may not capture full complexity of real-world insurance queries

## Confidence

- **High Confidence**: RAG approach with complete reference corpus consistently outperforms zero-shot baselines across all automatic metrics (BLEU/ROUGE/METEOR improvements of 40-300%)
- **Medium Confidence**: Manual evaluation findings showing 5-13% false answers are credible, but small sample size limits generalizability
- **Low Confidence**: Claims about effectiveness of specific prompt engineering techniques lack detailed ablation studies isolating individual prompt component impacts

## Next Checks

1. Conduct a calibration study to measure the LLM's ability to detect when questions fall outside its knowledge scope and test whether it can safely abstain from answering uncertain queries.
2. Expand the evaluation corpus to include edge cases and complex multi-hop reasoning questions that require synthesizing information across multiple reference documents.
3. Perform a legal review of the false answers identified in manual evaluation to quantify the actual legal and financial risks these errors could pose to insurance customers.