---
ver: rpa2
title: Monitoring fairness in machine learning models that predict patient mortality
  in the ICU
arxiv_id: '2411.00190'
source_url: https://arxiv.org/abs/2411.00190
tags:
- group
- rate
- race
- fairness
- auroc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a fairness monitoring approach for machine learning
  models predicting ICU patient mortality, using the Fairlearn Python package to assess
  performance across different patient groups. The analysis examines sex, race, and
  diagnosis categories, revealing that the model performs similarly for female and
  non-female patients (auROC 0.92187 vs 0.9245), while showing significant variation
  across diagnosis groups, with auROC ranging from 0.82 to 0.96.
---

# Monitoring fairness in machine learning models that predict patient mortality in the ICU

## Quick Facts
- arXiv ID: 2411.00190
- Source URL: https://arxiv.org/abs/2411.00190
- Reference count: 10
- A fairness monitoring approach using Fairlearn reveals significant variation in ICU mortality prediction performance across diagnosis groups, with auROC ranging from 0.82 to 0.96

## Executive Summary
This paper presents a comprehensive fairness monitoring framework for machine learning models predicting ICU patient mortality. Using the Fairlearn Python package, the analysis examines model performance across sex, race, and diagnosis categories, revealing substantial variation in predictive accuracy. The study identifies documentation bias in Glasgow Coma Scale (GCS) measurements as a key fairness issue and demonstrates that a new GAM-based model designed to be robust to such bias achieves significantly better performance, with a lower false positive rate (0.17805 vs 0.35601) in ICUs that frequently document GCS=3 as null data. The work emphasizes that fairness analysis provides more nuanced and informative insights than traditional accuracy metrics alone.

## Method Summary
The study compares two machine learning models for ICU mortality prediction: a traditional model and a new GAM-based model designed to be robust to documentation bias. Using the eICU Collaborative Research Database, the authors assess fairness metrics including auROC, selection rate, demographic parity difference, and equalized odds ratio across patient subgroups defined by sex, race, and diagnosis categories. Fairlearn (version 0.4.6) is used to compute group-wise performance metrics. The analysis particularly focuses on GCS documentation practices, categorizing ICUs by their frequency of recording GCS=3 values and examining how this systematic documentation bias affects model predictions and fairness.

## Key Results
- The model shows similar performance for female and non-female patients (auROC 0.92187 vs 0.9245), but significant variation across diagnosis groups (auROC 0.82-0.96)
- The new GAM-based model demonstrates higher overall accuracy (auROC 0.92331 vs 0.88244) compared to the older model
- For ICUs with frequent GCS=3 documentation, the new model reduces false positive rates by approximately half (0.17805 vs 0.35601)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Fairlearn metrics provides visibility into fairness issues that traditional accuracy metrics alone cannot reveal.
- Mechanism: Fairlearn computes group-wise performance metrics (e.g., auROC, selection rate) for different patient subgroups (sex, race, diagnosis), exposing disparities that would be masked by aggregate accuracy.
- Core assumption: The model's performance differs meaningfully across subgroups in a way that impacts clinical outcomes.
- Evidence anchors:
  - [abstract] "fairness analysis provides a more detailed and insightful comparison of model performance than traditional accuracy metrics alone"
  - [section] "However, the fairness analysis framework provides us with more informative, detailed and nuanced information for model comparison"
  - [corpus] No direct corpus evidence on Fairlearn usage in ICU mortality prediction; this is the primary source.
- Break condition: If all subgroups have statistically identical performance metrics, the fairness analysis would add no additional value.

### Mechanism 2
- Claim: Addressing documentation bias in clinical measurements improves model robustness and fairness across different ICUs.
- Mechanism: The GAM-based model incorporates mechanisms to handle inconsistent GCS documentation (e.g., null values recorded as GCS=3), reducing false positive rates in ICUs with different documentation practices.
- Core assumption: Documentation bias is non-random and systematically affects model predictions differently across ICUs.
- Evidence anchors:
  - [section] "The GAM-based model show s in Table 2 a lower false positive rate in ICUs that frequently document GCS=3... false positive rate in the 'highGCS3' group is approximately halved (0.17805)"
  - [section] "This systemi c bias in the capture of GCS decreases model accuracy, negatively aﬀecting the benchmark reports"
  - [corpus] No corpus evidence on documentation bias mitigation in ICU mortality prediction models.
- Break condition: If GCS documentation practices are standardized across all ICUs or if GCS is not a significant predictor.

### Mechanism 3
- Claim: Monitoring fairness metrics as part of routine system monitoring enables early detection of performance disparities.
- Mechanism: The monitoring system tracks input data drift, training logs, prediction accuracy, and fairness metrics, allowing teams to identify and address fairness issues proactively.
- Core assumption: Regular monitoring can detect changes in model behavior that may introduce or exacerbate fairness issues.
- Evidence anchors:
  - [section] "Monitoring is an important aspect of Responsible AI because it provides visibility into how the machine learning system is working"
  - [section] "If disparities between patient groups are found during fair ness analysis, they can be discussed with domain experts (ICU clinicians) to understand the underlying cause"
  - [corpus] No corpus evidence on fairness monitoring in production ICU mortality prediction systems.
- Break condition: If monitoring systems are not maintained or if fairness metrics are not reviewed regularly by domain experts.

## Foundational Learning

- Concept: Group fairness metrics (demographic parity, equalized odds)
  - Why needed here: To quantify and compare model performance across different patient subgroups (sex, race, diagnosis)
  - Quick check question: What is the difference between demographic parity and equalized odds, and when would each be more appropriate?

- Concept: Documentation bias and measurement error in clinical data
  - Why needed here: To understand how inconsistent clinical data capture (e.g., GCS values) can systematically bias model predictions
  - Quick check question: How does recording null GCS values as GCS=3 versus "Unable to score due to medications" affect mortality predictions?

- Concept: Generalized Additive Models (GAM) and their robustness properties
  - Why needed here: To understand why the GAM-based model was chosen for its resistance to documentation bias
  - Quick check question: How do GAMs differ from traditional models in handling noisy or biased input features?

## Architecture Onboarding

- Component map:
  - Data ingestion pipeline → Feature engineering → Model training (GAM-based) → Prediction serving → Fairness monitoring dashboard
  - Fairlearn integration for real-time fairness metrics calculation
  - Documentation bias detection module for GCS and other clinical features

- Critical path:
  1. Feature extraction from EHR data
  2. GCS documentation bias categorization
  3. Model prediction
  4. Fairness metrics computation
  5. Dashboard visualization for clinical stakeholders

- Design tradeoffs:
  - Using GAMs provides interpretability but may sacrifice some predictive accuracy compared to more complex models
  - Fairness monitoring adds computational overhead but provides critical insights for clinical decision-making
  - The choice of fairness metrics (auROC vs. selection rate vs. false positive rate) depends on the specific fairness concern

- Failure signatures:
  - Large discrepancies in auROC across diagnosis groups may indicate insufficient training data for certain conditions
  - High false positive rates in ICUs with frequent GCS=3 documentation suggest documentation bias
  - Changes in fairness metrics over time may indicate data drift or evolving documentation practices

- First 3 experiments:
  1. Compare auROC and false positive rates across all diagnosis groups to identify which conditions have the poorest performance
  2. Simulate different GCS documentation practices (e.g., consistently recording nulls as 3 vs. medications) to measure impact on predictions
  3. Test the GAM model against a baseline logistic regression model on a holdout set with known documentation bias patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the new GAM-based model achieve robustness to documentation bias in Glasgow Coma Scale measurements?
- Basis in paper: [explicit] The paper mentions the GAM-based model was "designed to be more robust to measurement error and the resulting documentation bias" but doesn't explain the technical mechanism
- Why unresolved: The paper describes the outcome of improved robustness but doesn't detail the specific architectural or algorithmic changes that enable this resistance to documentation bias
- What evidence would resolve it: Technical details explaining how the GAM-based model handles missing or inconsistent GCS values differently from the old model, including specific feature engineering or model architecture modifications

### Open Question 2
- Question: What is the impact of sample size on model performance across different diagnosis groups?
- Basis in paper: [inferred] The paper notes "There may also be a low sample size of patients in some categories, which may lead to decreased model accuracy in less prevalent diagnosis groups" but doesn't provide quantitative analysis
- Why unresolved: While the paper acknowledges the potential issue, it doesn't analyze the relationship between sample size and prediction accuracy across diagnosis groups
- What evidence would resolve it: Statistical analysis showing correlation between diagnosis group sample sizes and their respective auROC scores, along with confidence intervals for each group

### Open Question 3
- Question: How does the model perform across different age groups in addition to sex, race, and diagnosis?
- Basis in paper: [inferred] The paper focuses on sex, race, and diagnosis as sensitive features but doesn't explore age-related disparities, which is a common factor in medical outcomes
- Why unresolved: Age is a critical demographic factor in ICU mortality prediction that could reveal additional fairness concerns not captured by the current analysis
- What evidence would resolve it: Fairness metrics broken down by age groups, showing auROC and other performance measures across different age brackets

### Open Question 4
- Question: What is the long-term stability of the documented improvements in fairness and accuracy?
- Basis in paper: [inferred] The paper presents a comparison between old and new models but doesn't discuss how these performance differences hold up over time or with different datasets
- Why unresolved: The study appears to be a snapshot comparison without temporal validation or cross-dataset testing
- What evidence would resolve it: Longitudinal study showing consistent performance differences between models over time, or validation on independent datasets confirming the robustness of the findings

## Limitations
- The study relies on a single database (eICU Collaborative Research Database) which may not generalize to all ICU populations or healthcare settings
- The analysis does not account for socioeconomic factors, insurance status, or other variables that may influence both clinical documentation practices and model performance
- The study presents a snapshot comparison without temporal validation, making it unclear how the fairness metrics might change over time

## Confidence
- **High confidence**: The methodology for using Fairlearn to assess fairness metrics is sound and the findings about documentation bias in GCS measurements are well-supported by the data presented
- **Medium confidence**: The interpretation of fairness improvements requires careful consideration; while the GAM model shows better fairness metrics, the clinical significance of these improvements needs validation
- **Low confidence**: The external validity of the findings is uncertain; without comparison to other fairness monitoring approaches or validation on independent datasets, it's difficult to assess generalizability

## Next Checks
1. **Temporal validation**: Replicate the fairness analysis on a subsequent time period to verify that the observed documentation bias patterns and model performance differences persist over time
2. **Alternative fairness metrics**: Apply additional fairness metrics not used in this study (e.g., predictive parity, calibration across groups) to determine if the conclusions about model fairness remain consistent
3. **Clinical impact assessment**: Conduct a clinical validation study to determine whether the reduction in false positive rates for ICUs with high GCS=3 documentation translates to meaningful improvements in patient care or resource allocation