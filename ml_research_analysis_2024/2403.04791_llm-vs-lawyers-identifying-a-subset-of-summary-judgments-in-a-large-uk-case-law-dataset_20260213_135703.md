---
ver: rpa2
title: 'LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case
  Law Dataset'
arxiv_id: '2403.04791'
source_url: https://arxiv.org/abs/2403.04791
tags:
- summary
- cases
- judgment
- legal
- court
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared a traditional NLP keyword search approach with
  an LLM-based method to identify summary judgment cases within a large UK legal corpus.
  The keyword method used expert-defined terms and logical operators, while the LLM
  (Claude 2) was prompted to classify cases based on content.
---

# LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset

## Quick Facts
- arXiv ID: 2403.04791
- Source URL: https://arxiv.org/abs/2403.04791
- Reference count: 27
- Primary result: LLM (Claude 2) achieved 0.94 weighted F1 score vs 0.78 for keyword search in identifying summary judgment cases

## Executive Summary
This study compares traditional keyword search with LLM-based classification for identifying summary judgment cases in a large UK legal corpus. The researchers tested both methods on the Cambridge Law Corpus (356,011 cases) and found that Claude 2 achieved significantly higher accuracy (94% F1 score) compared to expert-designed keyword searches (78% F1 score). Manual verification of samples confirmed the LLM's superior performance in correctly identifying relevant cases while avoiding false positives, particularly for cases with complex or atypical legal language.

## Method Summary
The researchers compared two approaches for identifying summary judgment cases: a keyword search using expert-defined terms and logical operators, and an LLM classification using Claude 2 with a detailed prompt. They first filtered the corpus using regular expressions to find cases mentioning "summary judgment" variations, then applied both classification methods to this subset. Manual verification by legal experts on statistically significant samples from both methods enabled calculation of precision, recall, and F1 scores for comparison.

## Key Results
- Claude 2 achieved a weighted F1 score of 0.94 versus 0.78 for the keyword search method
- The LLM was more effective at identifying relevant cases while avoiding false positives
- Manual verification showed 90.40% accuracy for LLM-classified summary judgment cases
- Keyword method correctly identified 64.8% of actual summary judgment cases in the sample

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The LLM's large context window and strong contextual understanding allow it to accurately classify summary judgment cases despite the complex and variable legal language.
- **Mechanism:** Claude 2 processes the full text of each case using its deliberative network architecture to detect nuanced legal language patterns. The prompt instructs the model to look for key attributes of summary judgment while excluding similar-but-different case types.
- **Core assumption:** Claude 2's training on large datasets and its emergent abilities in abstraction allow it to generalize legal language patterns beyond keyword matching.
- **Evidence anchors:**
  - [abstract]: "The LLM (Claude 2) was prompted to classify cases based on content. Manual verification showed the LLM achieved a weighted F1 score of 0.94 versus 0.78 for keywords."
  - [section 4.2.1]: "Claude 2 excels in language comprehension and contextual understanding... The model description indicates that its responses employ algorithms to ensure variety, precision and pertinence."
  - [corpus]: Weak - no direct corpus evidence for LLM superiority in legal classification, only reported results.
- **Break condition:** If the model hallucinates or fails to distinguish summary judgment from similar legal procedures, or if the prompt structure doesn't properly constrain the reasoning.

### Mechanism 2
- **Claim:** The keyword search method fails because legal language is too variable and summary judgment terminology co-occurs with other legal procedures, leading to false positives and negatives.
- **Mechanism:** Legal experts identified keywords from statutes and case law, then used logical operators to combine them. However, many cases mention summary judgment terms without actually being summary judgment cases.
- **Core assumption:** Expert-derived keywords and logical combinations can capture the essence of summary judgment cases.
- **Evidence anchors:**
  - [abstract]: "The keyword method used expert-defined terms and logical operators... Despite iterative refinement, the search logic based on keywords fails to capture nuances in legal language."
  - [section 4.1.4]: "Manual checks of the keyword datasets found that of the 332 summary judgment cases in the sample dataset, 64.8% were correctly identified."
  - [corpus]: Weak - no direct corpus evidence for why keyword method fails, only reported results.
- **Break condition:** If the keyword combinations miss critical variations in legal language or incorrectly include cases that use similar terminology for different procedures.

### Mechanism 3
- **Claim:** Manual verification of LLM results achieves high accuracy by providing ground truth labels for evaluation.
- **Mechanism:** The researchers calculated statistically significant sample sizes and had legal experts manually check random samples from both LLM-classified datasets to determine precision and recall, enabling calculation of F1 scores.
- **Core assumption:** Manual verification by legal experts provides accurate ground truth labels for evaluating automated classification methods.
- **Evidence anchors:**
  - [abstract]: "Manual verification showed the LLM achieved a weighted F1 score of 0.94 versus 0.78 for keywords."
  - [section 4.2.2]: "We created a statistically representative sample for both datasets and conducted manual checks... We find that of the 342 cases, 90.40% were correctly classified as summary judgment."
  - [corpus]: No corpus evidence - manual verification is separate from corpus analysis.
- **Break condition:** If manual verification introduces bias or if the sample sizes are insufficient to capture the true performance of the classification methods.

## Foundational Learning

- **Concept:** F1 score and its relationship to precision and recall
  - Why needed here: The paper uses F1 score as the primary metric for comparing the performance of keyword search versus LLM classification methods.
  - Quick check question: If a method has high precision but low recall, what happens to its F1 score? (Answer: It will be low because F1 is the harmonic mean of precision and recall)

- **Concept:** Regular expressions (RegEx) for pattern matching in text
  - Why needed here: The initial approach used RegEx to search for variations of "summary judgment" in the corpus before applying more complex keyword logic.
  - Quick check question: What does the RegEx pattern `r'\bsumm[a-z]*\s*judg[a-z]*'` match? (Answer: Variations like "summary judgment", "summary judgement", "summaries judged", etc.)

- **Concept:** Legal procedural rules and their terminology
  - Why needed here: Understanding the Civil Procedure Rules (CPR) Part 24 and related terminology is essential for identifying summary judgment cases and crafting the keyword search logic and LLM prompt.
  - Quick check question: What is the legal test for granting summary judgment under CPR Part 24? (Answer: The court must be satisfied that the claimant has no real prospect of succeeding on the claim or that there is no other compelling reason why the case should be disposed of at trial)

## Architecture Onboarding

- **Component map:** Cambridge Law Corpus (CLC) -> RegEx search layer -> Keyword search logic -> LLM classification layer -> Manual verification layer -> Output datasets
- **Critical path:** Load CLC corpus into DataFrame -> Apply RegEx search -> Apply keyword search logic -> Run LLM classification -> Conduct manual verification -> Calculate F1 scores
- **Design tradeoffs:** Keyword search: More transparent but requires extensive manual refinement; LLM classification: Higher accuracy but less transparent and requires careful prompt engineering; Manual verification: Provides ground truth but is time-consuming
- **Failure signatures:** Low precision (many false positives), low recall (many false negatives), LLM hallucinations, keyword overlap
- **First 3 experiments:** 1) Test RegEx search with different pattern variations, 2) Validate keyword search logic on small manually labeled sample, 3) Test LLM prompt with different examples and instructions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the accuracy of LLM-based classification of legal cases be further improved beyond the current 94% F1 score?
- **Basis in paper:** [explicit] The paper notes that Claude 2 achieved a 94% F1 score but still misclassified 9.6% of summary judgment cases.
- **Why unresolved:** The paper acknowledges this limitation but does not explore specific techniques to improve accuracy further.
- **What evidence would resolve it:** Comparative studies testing various prompt engineering strategies, fine-tuning approaches, or hybrid methods combining LLMs with traditional NLP techniques to achieve higher accuracy scores.

### Open Question 2
- **Question:** What is the impact of dataset incompleteness on the generalizability of findings regarding summary judgment trends?
- **Basis in paper:** [explicit] The paper notes that the dataset may not contain all summary judgments due to courts' discretion in publishing judgments.
- **Why unresolved:** The paper acknowledges this limitation but does not quantify its potential impact on the observed trends.
- **What evidence would resolve it:** Analysis comparing trends from the current dataset with data from more comprehensive sources or jurisdictions with higher publication rates.

### Open Question 3
- **Question:** How do the computational methods perform when applied to other types of legal cases beyond summary judgments?
- **Basis in paper:** [inferred] The paper focuses specifically on summary judgments and suggests the methods could be applied to other case types, but does not test this.
- **Why unresolved:** The study's scope is limited to one type of legal case, leaving the broader applicability of the methods unexplored.
- **What evidence would resolve it:** Systematic testing of the keyword and LLM approaches across multiple categories of legal cases to compare performance and identify any limitations or necessary adaptations.

## Limitations
- The study focuses on a single legal domain (summary judgment) and one LLM model (Claude 2)
- Manual verification only sampled a small fraction of the total dataset (342 cases out of thousands)
- The research doesn't address potential biases in Claude 2's training data or how model updates might affect classification consistency

## Confidence

- **High Confidence:** The comparative F1 scores (0.94 vs 0.78) and manual verification methodology are well-documented and replicable. The core finding that LLMs outperform keyword search for this task is robust.
- **Medium Confidence:** The generalizability of these results to other legal domains and LLM models is reasonable but not proven. The exact impact of prompt engineering choices on performance is not fully explored.
- **Low Confidence:** The long-term stability of these results given potential LLM model updates and the scalability of manual verification for larger datasets.

## Next Checks
1. Test the LLM classification approach on other legal procedural categories (e.g., injunctions, appeals) to assess domain generalizability.
2. Compare multiple LLM models (GPT-4, Llama, etc.) using identical prompts and evaluation methodology.
3. Implement an active learning approach where manual verification samples are used to iteratively improve both keyword and LLM methods.