---
ver: rpa2
title: Reinforcement Learning Interventions on Boundedly Rational Human Agents in
  Frictionful Tasks
arxiv_id: '2401.14923'
source_url: https://arxiv.org/abs/2401.14923
tags:
- human
- goal
- chainworld
- rewards
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Behavior Model Reinforcement Learning (BMRL),
  a framework where an AI agent provides personalized interventions on boundedly rational
  human agents performing frictionful tasks. The authors model humans as planning
  agents with potentially maladapted Markov Decision Processes (MDPs) and allow the
  AI to intervene on MDP parameters like discount factor and rewards.
---

# Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks

## Quick Facts
- arXiv ID: 2401.14923
- Source URL: https://arxiv.org/abs/2401.14923
- Reference count: 40
- One-line primary result: BMRL with chainworlds achieves near-oracle performance within 15 episodes while outperforming baselines in personalizing AI interventions for boundedly rational human agents in frictionful tasks

## Executive Summary
This paper introduces Behavior Model Reinforcement Learning (BMRL), a framework where an AI agent provides personalized interventions on boundedly rational human agents performing frictionful tasks. The authors model humans as planning agents with potentially maladapted Markov Decision Processes (MDPs) and allow the AI to intervene on MDP parameters like discount factor and rewards. They propose "chainworlds," a tractable human model that captures progress-based decision-making, and theoretically prove conditions under which this simple model is equivalent to more complex human MDPs. Empirically, they show that AI planning with chainworlds leads to rapid personalization (achieving near-oracle performance within 15 episodes) and demonstrates robustness to various model misspecifications. The method outperforms baselines like model-free and model-based RL, particularly in settings where rapid personalization is critical due to limited human interactions.

## Method Summary
The authors propose Behavior Model Reinforcement Learning (BMRL), where an AI agent personalizes interventions for boundedly rational human agents in frictionful tasks by modeling humans as optimal planners under maladapted MDPs. The key innovation is "chainworlds," a tractable human model that captures progress-based decision-making through a one-dimensional progress representation. The AI infers human MDP parameters (discount factor, burden rewards) from observed behavior and plans interventions to modify these parameters, encouraging goal-directed actions. Theoretical results prove conditions under which chainworlds are AI-equivalent to more complex human MDPs, enabling rapid personalization with interpretable, tractable models.

## Key Results
- Chainworld-based AI achieves near-oracle performance within 15 episodes across all tested environments
- Method demonstrates robustness to noise in human MDP parameters, outperforming baselines under various misspecifications
- Chainworlds provide rapid personalization with interpretable policies while maintaining strong performance across diverse human MDP structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chainworlds capture progress-based decision-making, enabling AI to personalize interventions rapidly by modeling humans as optimal planners with maladapted MDP parameters.
- Mechanism: The AI agent uses chainworlds to infer human MDP parameters (e.g., discount factor, burden) from observed behavior, then plans interventions that modify these parameters to encourage goal-directed actions.
- Core assumption: Humans in frictionful tasks can be modeled as optimal planners under a maladapted MDP, where suboptimal behavior stems from skewed MDP parameters rather than planning deficiencies.
- Evidence anchors:
  - [abstract]: "Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies...to their maladapted MDP parameters, such as an extremely low discount factor."
  - [section]: "In BMRL, human agents perform optimal planning on (subconscious) knowledge of their MDP...Despite being optimal, our human agents are still boundedly rational because their MDP is maladapted."
  - [corpus]: Weak‚Äîno direct corpus support found for this specific modeling assumption.
- Break condition: If humans exhibit sub-optimal planning (e.g., fixed-horizon or heuristic-based), chainworlds would fail to capture the true decision-making process.

### Mechanism 2
- Claim: The AI equivalence definition allows chainworlds to generalize to a broad class of human MDPs without loss of performance.
- Mechanism: Two human MDPs are AI-equivalent if they result in the same optimal AI policy; chainworlds are shown to be equivalent to monotonic chainworlds, progress worlds, multi-chain worlds, and negative effect worlds.
- Core assumption: AI performance depends only on the optimal AI policy, not on the exact structure of the human MDP, as long as the AI policy remains unchanged.
- Evidence anchors:
  - [section]: "Introducing a notion of MDP equivalence specific to BMRL, we theoretically and empirically show that AI planning with our human models can lead to helpful policies on a wide range of more complex, ground-truth humans."
  - [section]: "Our equivalence in definition 5.1 is not as strict as the homomorphisms equivalence...Instead, we only care that the two human MDPs are similar enough to result in the same optimal AI policy."
  - [corpus]: Weak‚Äîno direct corpus support for this specific equivalence definition.
- Break condition: If the true human MDP results in a non-three-window AI policy, chainworlds will not be AI-equivalent and performance will degrade.

### Mechanism 3
- Claim: Chainworlds provide a tractable, behaviorally interpretable model that scales to high-dimensional human models while maintaining interpretability.
- Mechanism: The chainworld's analytical solution for human policies allows the AI to estimate parameters rapidly from limited data, and the one-dimensional progress representation simplifies planning in high-dimensional settings.
- Core assumption: Progress toward a goal can be summarized by a single dimension, even in multi-dimensional settings, and this dimension is observable or inferable.
- Evidence anchors:
  - [section]: "Chainworlds are based on the observation that many frictionful tasks contain a notion of human progress toward a goal...We summarize these 'progress-based' settings with a 'progress-only' class of human MDPs, shown in fig. 2, which we call chainworlds."
  - [section]: "In our PT example, 'progress' may depend on a combination of metrics such as joint strength, the ability to perform daily tasks, and so on...definition 5.7 restricts us to graphs in which all shortest paths between the disengagement and goal state are of the same length."
  - [corpus]: Weak‚Äîno direct corpus support for this specific progress-based modeling assumption.
- Break condition: If progress cannot be mapped to a single dimension (e.g., conflicting progress metrics), chainworlds will not capture the true human decision-making process.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and optimal planning under MDPs
  - Why needed here: The paper models humans as optimal planners under MDPs, so understanding MDPs is fundamental to grasping the BMRL framework.
  - Quick check question: What are the five components of an MDP, and how does the optimal policy depend on the discount factor?

- Concept: Reinforcement Learning (RL) and policy optimization
  - Why needed here: The AI agent uses RL to learn intervention policies, so understanding RL basics is necessary to follow the personalization mechanism.
  - Quick check question: How does Q-learning differ from model-based RL, and what are the trade-offs between them?

- Concept: Behavioral economics and bounded rationality
  - Why needed here: The paper draws on behavioral science concepts like discounting and maladaptation to justify the human modeling assumptions.
  - Quick check question: What is hyperbolic discounting, and how does it differ from exponential discounting in terms of modeling human time preferences?

## Architecture Onboarding

- Component map:
  - Human MDP (M_‚Ñé): States, actions, transitions, rewards, discount factor, goal/disengagement states
  - AI MDP (M_ùê¥ùêº): States (human state + action), actions (intervene on discount, intervene on reward, do nothing), rewards (goal, disengagement, intervention cost), discount factor
  - Chainworld model: Simplified human MDP with 1D progress states, binary actions, fixed transition probabilities
  - Parameter estimation: Likelihood maximization of observed transitions to infer chainworld parameters
  - Policy optimization: Solving the AI MDP to obtain the optimal intervention policy

- Critical path:
  1. Observe human state and action
  2. Update likelihood of chainworld parameters
  3. Solve AI MDP with current parameter estimates
  4. Execute AI action (intervene or not)
  5. Observe new human state and repeat

- Design tradeoffs:
  - Tractability vs. expressiveness: Chainworlds are simple but may not capture all human behaviors
  - Interpretability vs. performance: Chainworlds allow behavioral interpretation but may underperform more complex models
  - Personalization speed vs. accuracy: Rapid parameter estimation from limited data may lead to suboptimal interventions

- Failure signatures:
  - Poor personalization: AI interventions do not improve human goal-directed behavior
  - Misinterpretation: AI misattributes human behavior to wrong MDP parameters
  - Non-robustness: AI performance degrades significantly under model misspecification

- First 3 experiments:
  1. Test chainworld performance on a simple chainworld environment with known parameters
  2. Test chainworld performance on a gridworld environment where distance to goal can be mapped to a single dimension
  3. Test chainworld robustness to noise in human MDP parameters by adding random perturbations each timestep

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the chainworld model's performance degrade when human MDP parameters vary both across individuals and within an individual's interaction with the AI?
- Basis in paper: [explicit] The paper tests noise in individual parameters but not compound effects of multiple varying parameters
- Why unresolved: Experiments only test single-parameter noise at a time, leaving the compound effect of multiple noisy parameters unexplored
- What evidence would resolve it: Results showing AI performance with simultaneous noise in multiple human MDP parameters (e.g., both discount factor and burden rewards varying stochastically)

### Open Question 2
- Question: Can the chainworld equivalence class be extended to include human MDPs with non-exponential discounting (e.g., hyperbolic discounting)?
- Basis in paper: [explicit] The authors note this as future work, stating they leave "other behaviorally relevant forms of discounting, such as hyperbolic discounting" for future work
- Why unresolved: The theoretical analysis and equivalence proofs are built on exponential discounting assumptions
- What evidence would resolve it: Theoretical proof showing chainworld equivalence with hyperbolic discounting or empirical results demonstrating robust AI performance with hyperbolic discounting humans

### Open Question 3
- Question: What is the optimal strategy for learning the mapping from complex human MDPs to the chainworld representation when domain experts cannot provide it?
- Basis in paper: [inferred] The paper assumes the mapping is given and mentions that "one will need to learn this mapping in conjunction with the chainworld parameters"
- Why unresolved: No algorithm or method is proposed for jointly learning the mapping and chainworld parameters from data
- What evidence would resolve it: A proposed algorithm for jointly learning the state/action mapping and chainworld parameters, along with experimental validation of its performance

## Limitations

- The chainworld model's assumption that progress can be reduced to a single dimension may not generalize to complex real-world tasks
- Theoretical equivalence results rely on specific AI-equivalence definitions that may not hold in all practical settings
- Empirical validation is conducted on relatively simple synthetic environments, with real-world task effectiveness untested

## Confidence

- High confidence: The core BMRL framework and the chainworld model specification
- Medium confidence: The theoretical equivalence results and their practical implications
- Low confidence: The method's effectiveness on real-world tasks and its robustness to various forms of model misspecification

## Next Checks

1. Validate the chainworld model on a real-world frictionful task with measurable progress, such as a physical rehabilitation task, to assess its practical applicability.
2. Test the method's robustness to different types of model misspecification, including severe parameter perturbations and structural changes to the human MDP.
3. Conduct a user study with human participants performing frictionful tasks to evaluate the method's effectiveness in a realistic setting and to validate the progress-based modeling assumption.