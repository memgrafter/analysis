---
ver: rpa2
title: Reinforcement Learning with Generative Models for Compact Support Sets
arxiv_id: '2404.16300'
source_url: https://arxiv.org/abs/2404.16300
tags:
- learning
- agent
- reinforcement
- framework
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a reinforcement learning framework to generate
  compact synthetic support sets for image classification tasks. The method employs
  a policy-based RL agent to iteratively construct and optimize text prompts for Stable
  Diffusion, guided by a reward combining validation accuracy gains and entropy to
  encourage balanced performance across classes.
---

# Reinforcement Learning with Generative Models for Compact Support Sets

## Quick Facts
- arXiv ID: 2404.16300
- Source URL: https://arxiv.org/abs/2404.16300
- Authors: Nico Schiavone; Xingyu Li
- Reference count: 22
- One-line primary result: RL-optimized synthetic support sets improve classification accuracy by up to 1.1% with only ~400 images

## Executive Summary
This work introduces a reinforcement learning framework that generates compact synthetic support sets to enhance image classification performance. The approach uses a policy-based RL agent to iteratively construct and optimize text prompts for Stable Diffusion, guided by a reward combining validation accuracy gains and entropy to encourage balanced performance across classes. The framework was evaluated on CIFAR-10 and Tiny-ImageNet using multiple backbone architectures, showing consistent improvements over baseline and random synthesis methods. Results demonstrate that targeted, small-batch synthetic data can meaningfully enhance classification performance without extra labeling or large-scale generation.

## Method Summary
The framework employs a policy-based reinforcement learning agent using Proximal Policy Optimization (PPO) to iteratively build text prompts for Stable Diffusion image generation. Prompts follow a domain-class template structure ("A {domain} of a {class}, {class}, and {class}") that allows fine-grained control over synthetic image content positioning at class boundaries. The agent generates 10 images per step for up to 400 total synthetic images. Performance feedback comes from validation accuracy and entropy calculations, with the classification model retrained on the augmented dataset after each generation step. The method was tested on CIFAR-10 and Tiny-ImageNet datasets using multiple backbone architectures.

## Key Results
- Classification accuracy improved by up to 1.1% absolute on CIFAR-10 and Tiny-Image ImageNet
- Only ~400 synthetic images needed per dataset to achieve performance gains
- Consistent improvements observed across multiple backbone architectures
- Entropy-based reward function encouraged balanced performance across underrepresented classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL agent controls Stable Diffusion's output precision by evolving prompts using a domain-class template
- Mechanism: The agent selects domains and class combinations, exploiting Stable Diffusion's tendency to prioritize the first class term in prompts
- Core assumption: Stable Diffusion's text conditioning reliably maps domain and class terms to specific visual features
- Evidence anchors:
  - [abstract] The framework uses a novel prompt structure and domain-based dictionary to guide generative models
  - [section 3.3] The prompt design allows the agent to position generated images at boundaries between classes
  - [corpus] Weak: No direct corpus evidence found for Stable Diffusion's class prioritization in prompts
- Break condition: If Stable Diffusion's prompt parsing changes or does not prioritize class terms as assumed

### Mechanism 2
- Claim: The combined reward function (accuracy + entropy) encourages balanced performance across classes while improving overall accuracy
- Mechanism: By rewarding both validation accuracy increase and entropy decrease, the framework ensures improvements benefit underrepresented classes
- Core assumption: Lower entropy correlates with more confident and balanced predictions
- Evidence anchors:
  - [section 3.3] The reward function is defined as a combination of accuracy change and entropy change
  - [abstract] The reward encourages balanced performance across classes
  - [corpus] Weak: No corpus evidence for this specific reward combination in classification tasks
- Break condition: If entropy does not correlate with balanced performance or if the reward function becomes unstable during training

### Mechanism 3
- Claim: Small, targeted synthetic support sets can effectively augment real data without requiring large-scale generation
- Mechanism: By generating only ~400 synthetic images and focusing on class boundaries, the framework provides cost-effective classification improvement
- Core assumption: A small number of well-placed synthetic images can improve classification performance as much as large-scale synthetic data
- Evidence anchors:
  - [abstract] The framework generates compact synthetic support sets for improved classification
  - [section 4.2] Only 400 synthetic images are generated per dataset
  - [section 4.3] Results show significant accuracy improvements with a small number of synthetic images
- Break condition: If synthetic images do not provide sufficient diversity or if the model overfits to the small synthetic dataset

## Foundational Learning

- Concept: Reinforcement Learning (RL) basics
  - Why needed here: The framework uses RL to optimize prompt generation for synthetic data
  - Quick check question: What is the difference between policy-based and value-based RL methods?
- Concept: Text-to-image generative models
  - Why needed here: The framework uses Stable Diffusion to generate synthetic images based on optimized prompts
  - Quick check question: How does Stable Diffusion convert text prompts into images?
- Concept: Classification accuracy and entropy
  - Why needed here: The reward function combines these metrics to encourage balanced performance
  - Quick check question: What does entropy measure in the context of classification predictions?

## Architecture Onboarding

- Component map:
  - RL Agent -> Stable Diffusion -> Synthetic Images -> Classification Model -> Validation Set -> RL Agent
- Critical path:
  1. RL agent generates prompt
  2. Stable Diffusion generates images
  3. Images added to synthetic support set
  4. Model trained on combined dataset
  5. Validation accuracy and entropy calculated
  6. Reward given to RL agent
- Design tradeoffs:
  - Small synthetic dataset vs. large-scale generation: Lower cost but potential overfitting
  - Prompt complexity vs. computational efficiency: More complex prompts may yield better results but increase computation time
  - Exploration vs. exploitation in RL: More exploration may find better prompts but increase training time
- Failure signatures:
  - RL agent fails to improve prompts: Prompts become repetitive or irrelevant
  - Synthetic images do not improve accuracy: Generated images are not diverse enough or do not represent class boundaries
  - Model overfits to synthetic data: Validation accuracy decreases over time
- First 3 experiments:
  1. Test prompt generation with a simple RL agent on a small dataset (e.g., CIFAR-10)
  2. Evaluate the impact of synthetic images on classification accuracy with a fixed set of prompts
  3. Implement the full RL framework and compare performance against baseline and random synthesis methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the framework scale with larger datasets or a higher number of classes, considering the current action space complexity scales as n³?
- Basis in paper: [explicit] The paper mentions that "the action space complexity scales as n³, where n is the number of classes in the dataset" and that "we chose these datasets due to computational reasons."
- Why unresolved: The current framework was evaluated on CIFAR-10 (10 classes) and Tiny-ImageNet (200 classes). Scaling to datasets with significantly more classes or larger image sizes could lead to computational challenges not addressed in the paper.
- What evidence would resolve it: Experimental results demonstrating the framework's performance and computational efficiency on datasets with more than 200 classes or larger image resolutions would provide clarity.

### Open Question 2
- Question: Can the framework be adapted to use unsupervised methods for forming the dictionary based on image-to-text encoders and clustering, as suggested for bypassing the need for class names and domain information?
- Basis in paper: [explicit] The paper states, "Our framework currently requires some amount of information about the target dataset in order to work: class names, and a rough domain. This could be bypassed by forming the dictionary using an image-to-text encoder on representative samples after clustering by an unsupervised learning algorithm, but we leave the pursuit of this direction for future work."
- Why unresolved: The authors acknowledge the potential for unsupervised adaptation but do not explore it, leaving the effectiveness and feasibility of such an approach untested.
- What evidence would resolve it: Implementation and evaluation of the framework using unsupervised methods for dictionary formation would demonstrate its adaptability and potential performance in scenarios without labeled data.

### Open Question 3
- Question: How does the framework perform in scenarios where the real dataset is highly imbalanced, and does the entropy-based reward function effectively address class imbalance?
- Basis in paper: [explicit] The paper mentions that the reward function includes entropy to "bias our model towards high, balanced accuracy" and to "reward the improvement of weak classes, which improves the overall model performance on underrepresented classes."
- Why unresolved: The evaluation was conducted on balanced datasets (CIFAR-10 and Tiny-ImageNet). The effectiveness of the entropy-based reward in handling imbalanced datasets is not tested.
- What evidence would resolve it: Testing the framework on imbalanced datasets and comparing its performance to baseline methods would show whether the entropy-based reward function effectively mitigates class imbalance issues.

## Limitations

- The framework's effectiveness depends on Stable Diffusion's specific prompt parsing behavior, which could change with model updates
- Evaluation focuses on relatively small-scale image classification datasets, leaving uncertainty about performance on larger, more complex datasets
- The approach requires retraining the classification model after each synthetic image generation step, which becomes computationally expensive at scale

## Confidence

- **High confidence**: The core RL framework for prompt optimization and the general methodology of using synthetic data to improve classification accuracy are well-established concepts with clear implementation paths
- **Medium confidence**: The specific claim that this approach consistently improves accuracy by up to 1.1% across different backbone architectures is supported by the reported experiments but would benefit from testing on additional datasets and model architectures
- **Low confidence**: The assumption about Stable Diffusion's class prioritization behavior and the assertion that small synthetic datasets (400 images) can match the benefits of large-scale generation are based on empirical observations that may not generalize across different generative models or classification tasks

## Next Checks

1. **Cross-model validation**: Test the framework using different text-to-image generative models (e.g., DALL-E 2, Midjourney) to verify that the RL-based prompt optimization approach works independently of Stable Diffusion's specific prompt parsing behavior

2. **Dataset scaling experiment**: Evaluate the method on larger, more complex datasets (e.g., ImageNet, COCO) to determine whether the compact synthetic support set approach scales effectively or requires adjustment for more challenging classification tasks

3. **Prompt generalization analysis**: Systematically vary the prompt template structure and domain dictionary to assess whether the observed improvements stem from the specific template used or from more general properties of the RL-based approach to synthetic data generation