---
ver: rpa2
title: 'The American Sign Language Knowledge Graph: Infusing ASL Models with Linguistic
  Knowledge'
arxiv_id: '2411.03568'
source_url: https://arxiv.org/abs/2411.03568
tags:
- sign
- language
- signs
- knowledge
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the American Sign Language Knowledge Graph
  (ASLKG), a novel resource compiling linguistic knowledge from 12 sources to enhance
  ASL language models. The ASLKG contains over 71,000 facts about 5,802 ASL signs,
  including phonological features and semantic relationships.
---

# The American Sign Language Knowledge Graph: Infusing ASL Models with Linguistic Knowledge

## Quick Facts
- arXiv ID: 2411.03568
- Source URL: https://arxiv.org/abs/2411.03568
- Reference count: 12
- Introduces the ASL Knowledge Graph (ASLKG) with 71k facts about 5.8k ASL signs, achieving 91% accuracy on isolated sign recognition

## Executive Summary
This paper introduces the American Sign Language Knowledge Graph (ASLKG), a novel resource compiling linguistic knowledge from 12 sources to enhance ASL language models. The ASLKG contains over 71,000 facts about 5,802 ASL signs, including phonological features and semantic relationships. By grounding video data to this knowledge graph and using neuro-symbolic methods, the authors achieve 91% accuracy on isolated sign recognition, 14% for predicting semantic features of unseen signs, and 36% for classifying topics in YouTube ASL videos. This work demonstrates how expert linguistic knowledge can improve the generalizability and explainability of ASL models while addressing challenges in data scarcity and curation quality.

## Method Summary
The ASLKG is constructed from 12 linguistic sources and used to train neuro-symbolic models for ASL understanding tasks. The approach combines video grounding to phonological features using a Sign Language Graph Convolution Network (SLGCN), knowledge-infused learning through graph neural networks trained on fact verification, and inference using factor graphs, KNN, and MLP models. The system maps observed phonological features to semantic labels and ASL signs based on the systematic relationships captured in the knowledge graph, enabling both recognition of known signs and inference about unseen signs based on their form.

## Key Results
- 91% top-1 accuracy on isolated sign recognition using neuro-symbolic methods
- 14% accuracy for predicting semantic features of unseen signs based on phonological form
- 36% accuracy for classifying topics in YouTube ASL videos using the knowledge graph approach

## Why This Works (Mechanism)

### Mechanism 1
Grounding video data to phonological features in the ASLKG improves isolated sign recognition accuracy. The Sign Language Graph Convolution Network (SLGCN) approximates p(ϕ|v) to capture phonological structure before mapping to sign labels, leveraging systematic relationships between form and meaning in ASL. This two-step process could fail if phonological features are too coarse or video quality is insufficient.

### Mechanism 2
Pretraining on fact verification to produce node embeddings adds accuracy to downstream tasks. Graph neural networks trained on fact verification (estimating p(f)) learn stable relationships among entities in the ASLKG, capturing holistic linguistic knowledge. This approach assumes the relationships are stable and relevant to downstream tasks, which may not hold if the graph contains inconsistent or irrelevant information.

### Mechanism 3
Neuro-symbolic methods for semantic feature recognition enable understanding of out-of-vocabulary signs. By mapping phonological features to semantic labels based on form alone, the model can partially understand unseen signs, leveraging ASL's systematicity where form partially signals meaning. This could break down if systematic relationships are too weak or semantic features don't capture meaningful distinctions.

## Foundational Learning

- Concept: Knowledge Graphs and Graph Neural Networks
  - Why needed here: The ASLKG is a knowledge graph, and graph neural networks are used to create embeddings and perform inference over it.
  - Quick check question: What is the difference between a knowledge graph and a traditional relational database?

- Concept: Neuro-symbolic methods
  - Why needed here: The paper combines data-driven pattern recognition (neural networks) with knowledge-driven reasoning (over the ASLKG).
  - Quick check question: How do neuro-symbolic methods differ from purely neural or purely symbolic approaches?

- Concept: Phonological features in sign languages
  - Why needed here: The ASLKG includes phonological features that are used to ground video data and enable inference about signs.
  - Quick check question: What are the key phonological parameters that distinguish signs in ASL?

## Architecture Onboarding

- Component map: ASLKG → SLGCN (phonological grounding) → KG Embeddings → Inference (sign or semantic feature recognition)
- Critical path: Video → SLGCN (phonological grounding) → KG Embeddings → Inference (sign or semantic feature recognition)
- Design tradeoffs: The neuro-symbolic approach trades some potential accuracy for interpretability and data efficiency; more fine-grained phonological features could improve accuracy but increase complexity; using English data to supplement ASL semantics assumes sufficient overlap
- Failure signatures: Low ISR accuracy indicates poor phonological grounding or insufficient discriminative power in ASLKG features; poor zero-shot performance suggests weak systematicity between form and meaning; topic classification failures indicate issues with sliding window approach or embedding quality
- First 3 experiments: 1) Baseline ISR using SLGCN without KG embeddings; 2) ISR with KG embeddings to measure improvement from fact verification pretraining; 3) Zero-shot semantic feature recognition on held-out signs to evaluate systematicity exploitation

## Open Questions the Paper Calls Out

### Open Question 1
How does phonological variation across signers affect the accuracy of the ASLKG-based models? The paper acknowledges that ASL lexicon varies across signers but does not provide data on how this impacts model performance with diverse signers. Empirical studies comparing model accuracy across signers with different dialects, accents, or regional variations would help quantify this impact.

### Open Question 2
To what extent can the ASLKG generalize to other sign languages beyond ASL? The authors note the ASLKG is specific to ASL and that semantic relationships may not translate to other sign languages due to their independent structures. The paper does not explore or test the ASLKG's applicability to other sign languages. Adapting the framework to other sign languages and testing its effectiveness would provide insights into generalizability.

### Open Question 3
How does the quality and coverage of the linguistic knowledge sources impact the ASLKG's effectiveness? While the authors acknowledge the ASLKG is built from 12 sources, they do not discuss the quality or completeness of these sources or their impact on the graph's utility. A detailed analysis of quality, coverage, and potential biases in each source, along with ablation studies, would help quantify their impact.

## Limitations
- The 91% ISR accuracy may be influenced by dataset characteristics not fully disclosed
- The integration of English semantic features assumes sufficient semantic overlap between ASL and English without thorough validation
- Claims about systematicity between phonology and semantics lack direct comparison to purely data-driven approaches

## Confidence

- ISR accuracy claims: **Medium** - Strong results but limited comparison to baseline methods
- Zero-shot semantic understanding: **Medium-Low** - Promising but needs more rigorous evaluation
- Systematicity claims: **Medium** - Evidence exists but could be stronger with controlled experiments
- Knowledge graph completeness: **Low-Medium** - Coverage is substantial but potential gaps exist

## Next Checks

1. Conduct ablation studies comparing neuro-symbolic approach against purely neural baselines on ISR task to isolate the knowledge graph's contribution
2. Perform cross-validation on ASLKG to assess consistency and completeness of linguistic annotations across different sources
3. Evaluate semantic feature recognition on a gold-standard ASL-only dataset to validate the English-to-ASL semantic mapping assumptions