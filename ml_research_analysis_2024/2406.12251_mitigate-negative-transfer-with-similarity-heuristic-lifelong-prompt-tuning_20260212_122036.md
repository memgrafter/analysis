---
ver: rpa2
title: Mitigate Negative Transfer with Similarity Heuristic Lifelong Prompt Tuning
arxiv_id: '2406.12251'
source_url: https://arxiv.org/abs/2406.12251
tags:
- transfer
- tasks
- task
- learning
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of negative transfer in lifelong
  learning, where tasks that are dissimilar can hinder learning rather than help it.
  The proposed method, Similarity Heuristic Lifelong Prompt Tuning (SHLPT), introduces
  a learnable similarity estimator to partition previous tasks into similar and dissimilar
  subsets, applying different transfer strategies to each.
---

# Mitigate Negative Transfer with Similarity Heuristic Lifelong Prompt Tuning

## Quick Facts
- arXiv ID: 2406.12251
- Source URL: https://arxiv.org/abs/2406.12251
- Reference count: 40
- Key outcome: SHLPT achieves 80.28% average accuracy on standard CL tasks (vs 76.31% prior best) and 78.67% on negative transfer benchmark (vs 77.48% prior best)

## Executive Summary
This paper addresses the problem of negative transfer in lifelong learning, where dissimilar tasks can hinder rather than help learning. The proposed method, Similarity Heuristic Lifelong Prompt Tuning (SHLPT), introduces a learnable similarity estimator to partition previous tasks into similar and dissimilar subsets, applying different transfer strategies to each. For similar tasks, SHLPT uses prompt initialization via weighted summation; for dissimilar tasks, it employs contrastive losses on hidden states and activation patterns to prevent interference. The method includes a prompt pool to avoid forgetting. Experiments on standard and newly constructed negative-transfer benchmarks show SHLPT achieves 80.28% average accuracy on standard CL tasks (vs 76.31% for the prior best) and 78.67% on the negative transfer benchmark (vs 77.48% for the prior best), demonstrating robustness and effective knowledge transfer even from dissimilar tasks.

## Method Summary
SHLPT is a lifelong prompt tuning method that addresses negative transfer by partitioning previous tasks into similar and dissimilar subsets using a learnable similarity estimator. For similar tasks, it applies weighted prompt initialization by combining prompt embeddings via attention-weighted summation. For dissimilar tasks, it uses contrastive losses (HSC on hidden states and ASC on activation states) to prevent interference while enabling positive transfer. A prompt pool stores all learned prompts to prevent forgetting. The method trains on each task sequentially, computing similarity between current task instances and all previous prompts, then applying the appropriate transfer strategy based on the partitioning.

## Key Results
- SHLPT achieves 80.28% average accuracy on standard CL benchmark (4 tasks) versus 76.31% for the prior best method
- On negative transfer benchmark, SHLPT reaches 78.67% versus 77.48% for the prior best method
- SHLPT demonstrates robustness across different task orders and shows superior backward transfer scores compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative transfer in lifelong prompt tuning arises when dissimilar tasks interfere during knowledge transfer, causing performance degradation.
- Mechanism: SHLPT uses a learnable similarity estimator to partition tasks into similar and dissimilar subsets, applying different transfer strategies to each. Similar tasks receive weighted prompt initialization, while dissimilar tasks are separated using contrastive losses on hidden and activation states.
- Core assumption: Task similarity can be accurately estimated from prompt embeddings, and dissimilar tasks can be leveraged for positive transfer via contrastive learning rather than excluded.
- Evidence anchors:
  - [abstract] SHLPT partitions tasks into two distinct subsets by harnessing a learnable similarity metric, thereby facilitating fruitful transfer from tasks regardless of their similarity or dissimilarity.
  - [section 4.1] We calculate an attention-weighted combination of past prompt embeddings and incorporate this into the current task's prompt... similarity ð›¼ð‘– of previous task ð‘– can be obtained through tempering softmax of inner product between Ëœð‘‹ and Ë†ð‘ƒð‘–.
  - [corpus] The corpus includes related work on lifelong reinforcement learning and prompt tuning, supporting the relevance of similarity-driven partitioning but not directly confirming the contrastive mechanism for dissimilar tasks.
- Break condition: If the similarity estimator fails to capture task relatedness (e.g., due to task diversity or prompt embedding limitations), the partitioning will be ineffective, leading to misapplication of transfer strategies.

### Mechanism 2
- Claim: Prompt initialization with weighted summation of similar tasks' prompts accelerates convergence and improves performance on the current task.
- Mechanism: Similar tasks' prompts are combined using attention-weighted summation and added to a newly allocated prompt for the current task, providing an optimized starting point.
- Core assumption: Prompt embeddings capture transferable knowledge, and a weighted combination can serve as a better initialization than random or single-task initialization.
- Evidence anchors:
  - [abstract] For similar tasks, SHLPT uses prompt initialization via weighted summation.
  - [section 4.2] We use a mixture of similar tasks' prompt embeddings to initialize prompt, and sum it with a newly allocated prompt ð‘ƒð‘¡ for the current task.
  - [corpus] Weak: The corpus mentions task vector grouping and prompt tuning, but lacks direct evidence for weighted summation as initialization.
- Break condition: If the similarity metric incorrectly groups dissimilar tasks as similar, the initialization may introduce harmful interference rather than beneficial transfer.

### Mechanism 3
- Claim: Contrastive losses on hidden and activation states prevent interference from dissimilar tasks and promote positive transfer by encouraging diverse knowledge utilization.
- Mechanism: HSC loss diverges hidden states between current and dissimilar tasks for the same input; ASC loss reduces overlap in activation states by masking neurons activated by dissimilar tasks.
- Core assumption: Higher layers of pre-trained models exhibit task-specific behavior, and contrastive learning can leverage dissimilar tasks to access broader knowledge without negative interference.
- Evidence anchors:
  - [abstract] For dissimilar tasks, it employs contrastive losses on hidden states and activation patterns to prevent interference.
  - [section 4.3] We introduce two novel loss functions that based on language model's inner representation... The HSC loss is designed to diverge the hidden states representation between current task and dissimilar tasks.
  - [corpus] Weak: The corpus includes work on activation states and modularity, but lacks direct evidence for contrastive losses as a mechanism for leveraging dissimilar tasks.
- Break condition: If the contrastive losses are too strong or misaligned, they may hinder learning by overly suppressing useful features shared across tasks.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Lifelong learning requires retaining knowledge from previous tasks while learning new ones; forgetting undermines this goal.
  - Quick check question: What mechanism does SHLPT use to avoid forgetting, and how does it differ from replay-based methods?

- Concept: Negative transfer
  - Why needed here: Understanding when and why transfer learning fails is essential to designing effective lifelong learning systems.
  - Quick check question: How does SHLPT detect and mitigate negative transfer, and what evidence supports its effectiveness?

- Concept: Task similarity estimation
  - Why needed here: Accurate similarity estimation is the foundation for partitioning tasks and applying appropriate transfer strategies.
  - Quick check question: What features does SHLPT use to estimate task similarity, and how robust is this approach to task diversity?

## Architecture Onboarding

- Component map:
  - Input -> Prompt Pool -> Similarity Estimator -> Transfer Modules (Weighted Summation for Similar Tasks, Contrastive Losses for Dissimilar Tasks) -> Output

- Critical path:
  1. Compute similarity between current task instances and all previous task prompts
  2. Partition previous tasks into similar and dissimilar subsets based on threshold
  3. Apply weighted prompt initialization for similar tasks
  4. Apply contrastive losses for dissimilar tasks during training
  5. Store the current task's prompt in the prompt pool

- Design tradeoffs:
  - Prompt pooling vs. replay: Prompt pooling is more parameter-efficient but requires task identity at inference
  - Similarity threshold: Lower thresholds risk including dissimilar tasks in similar set; higher thresholds may miss beneficial transfers
  - Contrastive loss weights: Balancing HSC and ASC is crucial; too much emphasis may hinder learning, too little may not prevent interference

- Failure signatures:
  - Performance drops when task order changes: Indicates sensitivity to task sequence and similarity estimation
  - High variance across runs: Suggests instability