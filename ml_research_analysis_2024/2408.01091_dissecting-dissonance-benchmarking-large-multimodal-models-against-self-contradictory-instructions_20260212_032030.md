---
ver: rpa2
title: 'Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory
  Instructions'
arxiv_id: '2408.01091'
source_url: https://arxiv.org/abs/2408.01091
tags:
- instructions
- lmms
- arxiv
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCI, a multimodal benchmark of 20,000 self-contradictory
  instructions designed to evaluate Large Multimodal Models' (LMMs) ability to detect
  conflicting commands. The authors develop an automatic dataset creation framework,
  AutoCreate, leveraging large language models to generate diverse and high-quality
  conflict examples across language-language and vision-language paradigms.
---

# Dissecting Dissonance: Benchmarking Large Multimodal Models Against Self-Contradictory Instructions

## Quick Facts
- arXiv ID: 2408.01091
- Source URL: https://arxiv.org/abs/2408.01091
- Reference count: 40
- Large Multimodal Models consistently fail to detect self-contradictory instructions

## Executive Summary
This paper addresses a critical limitation in Large Multimodal Models (LMMs): their inability to detect and respond appropriately to self-contradictory instructions. The authors introduce SCI, a comprehensive benchmark containing 20,000 self-contradictory instruction pairs across language-language and vision-language modalities. Through systematic evaluation of 11 state-of-the-art LMMs, the study reveals that these models consistently fail to recognize instruction conflicts due to a lack of self-awareness in their reasoning processes.

To address this fundamental challenge, the authors propose Cognitive Awakening Prompting (CaP), a novel approach that injects external cognition into the model's decision-making process. Unlike traditional in-context learning methods, CaP enables LMMs to explicitly analyze instruction consistency before responding. The framework demonstrates significant improvements in conflict detection, achieving up to 72% higher hit ratios compared to baseline zero-shot prompting methods.

## Method Summary
The research employs a multi-stage approach combining automated dataset generation with systematic model evaluation. The AutoCreate framework leverages large language models to generate self-contradictory instruction pairs across four contradiction types: entity, attribute, event, and relationship conflicts. The benchmark includes both language-only and vision-language instruction pairs, with the latter incorporating image-text interactions. Evaluation involves 11 state-of-the-art LMMs tested under zero-shot and few-shot prompting conditions, with human annotators validating instruction quality and model responses. The Cognitive Awakening Prompting approach is implemented as a plug-and-play module that can be integrated with existing prompting strategies.

## Key Results
- SCI benchmark successfully identifies that all 11 evaluated LMMs fail to detect self-contradictory instructions
- Cognitive Awakening Prompting improves conflict detection by up to 72% over baseline methods
- LMMs struggle specifically with relationship contradictions and vision-language instruction pairs
- Advanced in-context learning techniques show limited effectiveness in addressing dissonance detection

## Why This Works (Mechanism)
The study demonstrates that LMMs lack intrinsic self-awareness mechanisms necessary for detecting instruction conflicts. Traditional prompting approaches fail because they don't provide explicit cognitive frameworks for analyzing instruction consistency. The Cognitive Awakening Prompting method works by externally scaffolding the reasoning process, forcing models to evaluate instructions for logical consistency before generating responses. This external cognitive injection compensates for the model's inherent limitations in self-monitoring and contradiction detection.

## Foundational Learning

**Large Multimodal Models**: AI systems that process and generate both text and visual information simultaneously. Why needed: Understanding the dual-modality nature of the target models being evaluated. Quick check: Verify that evaluation includes both vision-language and language-only instruction pairs.

**Self-Contradictory Instructions**: Pairs of instructions that contain mutually exclusive requirements or commands. Why needed: The core phenomenon being measured and the basis for the entire benchmark. Quick check: Confirm that generated instruction pairs contain clear logical conflicts.

**In-Context Learning**: The ability of models to follow patterns and instructions demonstrated in their input without parameter updates. Why needed: The baseline prompting strategy being compared against the new approach. Quick check: Verify that baseline results use standard few-shot prompting.

**Cognitive Awakening Prompting**: A novel prompting strategy that injects external cognitive frameworks into model reasoning. Why needed: The proposed solution addressing the identified limitation. Quick check: Compare CaP performance against baseline across all model architectures.

## Architecture Onboarding

**Component Map**: AutoCreate Framework -> SCI Benchmark Generation -> LMM Evaluation Pipeline -> Cognitive Awakening Prompting Integration -> Performance Analysis

**Critical Path**: Instruction generation → Human quality validation → Model testing → Response analysis → CaP implementation → Performance comparison

**Design Tradeoffs**: The benchmark prioritizes instruction diversity over computational efficiency, generating 20,000 instruction pairs through LLM assistance rather than manual creation. This approach ensures comprehensive coverage but introduces potential generation biases.

**Failure Signatures**: Models consistently fail on relationship contradictions and visual instruction pairs, showing a pattern of accepting conflicting commands without question. Response patterns indicate models prioritize task completion over logical consistency.

**3 First Experiments**:
1. Test zero-shot prompting on a subset of SCI to establish baseline performance
2. Implement CaP on one representative LMM to verify the improvement mechanism
3. Compare human annotation agreement rates across different contradiction types

## Open Questions the Paper Calls Out

The paper does not explicitly identify additional open questions beyond its primary research contributions and findings.

## Limitations

The SCI benchmark is limited to English language conflicts and does not evaluate multilingual or cross-lingual dissonance detection capabilities. The evaluation methodology relies on human annotation for quality assessment, introducing potential subjectivity and rater bias. Additionally, the benchmark focuses on specific contradiction types, potentially missing other forms of logical inconsistencies that may arise in real-world applications.

## Confidence

High confidence: LMMs consistently fail to detect self-contradictory instructions across multiple architectures and prompting strategies
Medium confidence: Cognitive Awakening Prompting demonstrates significant improvements but may not generalize to all model architectures
Low confidence: Scalability of AutoCreate for generating complex multimodal contradictions, as acknowledged limitations exist for sophisticated visual conflicts

## Next Checks

1. Test CaP's effectiveness on additional LMMs not included in the original evaluation, particularly newer models released after the study's publication

2. Conduct a systematic evaluation of SCI's robustness to adversarial examples and variations in instruction phrasing to assess the benchmark's stability

3. Implement multilingual versions of SCI and evaluate model performance across different languages to assess the generalizability of findings beyond English