---
ver: rpa2
title: 'DAFA: Distance-Aware Fair Adversarial Training'
arxiv_id: '2401.12532'
source_url: https://arxiv.org/abs/2401.12532
tags:
- class
- robust
- classes
- adversarial
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the robust fairness problem in adversarial
  training, where the accuracy disparity between classes is amplified. Existing methods
  attempt to improve robust fairness by sacrificing performance on easier classes
  to enhance performance on harder ones.
---

# DAFA: Distance-Aware Fair Adversarial Training

## Quick Facts
- **arXiv ID**: 2401.12532
- **Source URL**: https://arxiv.org/abs/2401.12532
- **Reference count**: 40
- **Primary result**: DAFA improves worst-class robust accuracy while maintaining average robust accuracy through distance-aware adversarial training

## Executive Summary
This paper addresses the robust fairness problem in adversarial training, where accuracy disparity between classes is amplified under adversarial attacks. Existing methods attempt to improve robust fairness by sacrificing performance on easier classes to enhance performance on harder ones. The authors observe that under adversarial attacks, model predictions for samples from the worst class are biased towards similar classes rather than easy classes. Through theoretical and empirical analysis, they demonstrate that robust fairness deteriorates as the distance between classes decreases. They propose Distance-Aware Fair Adversarial training (DAFA) which considers inter-class similarities to improve robust fairness by assigning distinct loss weights and adversarial margins to each class based on inter-class distances.

## Method Summary
DAFA introduces a distance-aware approach to adversarial training that addresses robust fairness by considering inter-class similarities. The method assigns distinct loss weights and adversarial margins to each class based on their pairwise distances in feature space. During training, DAFA calculates class distances and uses these metrics to dynamically adjust the strength of adversarial perturbations and loss contributions for each class. This encourages a trade-off in robustness among similar classes, preventing the model from becoming overly confident in discriminating between nearby classes while maintaining strong protection against attacks for all classes.

## Key Results
- DAFA significantly improves worst robust accuracy across multiple datasets
- The method maintains average robust accuracy while improving fairness metrics
- Experimental results demonstrate marked improvement in robust fairness compared to existing methods

## Why This Works (Mechanism)
DAFA works by recognizing that robust fairness degradation stems from the model's tendency to confuse nearby classes under adversarial attacks. By incorporating inter-class distance information into the adversarial training process, DAFA creates a more balanced robustness landscape where similar classes receive appropriate protection against confusion. The distance-aware loss weighting prevents over-regularization of easy-to-separate classes while ensuring harder-to-separate classes receive sufficient adversarial training strength.

## Foundational Learning
- **Adversarial training fundamentals**: Understanding how adversarial examples are generated and used to improve model robustness is essential for grasping DAFA's approach
  - *Why needed*: DAFA builds directly on standard adversarial training but modifies it based on class relationships
  - *Quick check*: Verify understanding of PGD attacks and standard adversarial training objectives

- **Robust fairness metrics**: Familiarity with fairness measures in adversarial settings, particularly worst-class accuracy and disparity metrics
  - *Why needed*: The paper's evaluation focuses on fairness improvements rather than just average accuracy
  - *Quick check*: Understand the difference between average and worst-case accuracy metrics

- **Inter-class distance computation**: Knowledge of how to measure and utilize distances between class representations in feature space
  - *Why needed*: DAFA's core innovation relies on distance-aware weight assignment
  - *Quick check*: Be able to compute pairwise distances between class centroids or using other metrics

## Architecture Onboarding

**Component map**: Input data -> Distance calculator -> Loss weighting module -> Adversarial example generator -> Model training loop

**Critical path**: During each training iteration, the critical path involves: (1) computing current batch representations, (2) calculating inter-class distances, (3) determining class-specific loss weights and margins, (4) generating adversarial examples with class-aware perturbations, and (5) updating model parameters with the distance-weighted loss.

**Design tradeoffs**: The primary tradeoff involves balancing computational overhead (calculating distances and maintaining class-specific parameters) against fairness improvements. DAFA sacrifices some efficiency for better fairness outcomes. The method also trades off pure worst-case robustness for balanced robustness across all classes.

**Failure signatures**: Potential failures include: (1) distance calculations becoming unstable with highly overlapping classes, (2) loss weighting causing under-regularization of certain classes, and (3) margin adjustments leading to gradient instability during training.

**3 first experiments**:
1. Implement distance calculation between class representations in a simple MLP on MNIST to verify distance-aware metrics work as expected
2. Apply basic loss weighting based on class distances without adversarial training to observe impact on clean accuracy
3. Integrate distance-aware margins into standard PGD adversarial training and measure impact on worst-class accuracy

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- The paper does not thoroughly explore trade-offs between fairness improvements and other performance metrics
- The focus on inter-class distance may oversimplify the complex interactions in adversarial learning spaces
- Computational overhead of DAFA compared to standard adversarial training is not discussed

## Confidence

**Major Claim Confidence**:
- **High confidence**: DAFA improves worst-class robust accuracy while maintaining average robust accuracy
- **Medium confidence**: Theoretical connection between inter-class distance and robust fairness deterioration
- **Medium confidence**: Proposed loss weighting and margin adjustment strategy effectively addresses identified issues

**Key Limitations**:
The assumption that distance-based class similarity is the dominant factor may miss other important factors affecting robust fairness, such as feature space geometry or decision boundary characteristics.

## Next Checks
1. Evaluate DAFA's performance across diverse attack scenarios beyond standard PGD attacks, including transfer-based and adaptive attacks
2. Conduct ablation studies to isolate the impact of distance-aware components versus other aspects of the training methodology
3. Analyze the decision boundary structure and feature representations to verify that improved fairness correlates with more balanced decision regions rather than simple margin adjustments