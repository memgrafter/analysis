---
ver: rpa2
title: 'ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation'
arxiv_id: '2407.12022'
source_url: https://arxiv.org/abs/2407.12022
tags:
- data
- code
- training
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving register transfer
  level (RTL) code generation using large language models (LLMs), which are typically
  fine-tuned on fixed datasets requiring extensive reference data. To mitigate this,
  the authors introduce ITERTL, an iterative training framework that updates training
  samples by leveraging feedback from RTL tools during each training cycle.
---

# ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation

## Quick Facts
- arXiv ID: 2407.12022
- Source URL: https://arxiv.org/abs/2407.12022
- Authors: Peiyang Wu; Nan Guo; Xiao Xiao; Wenming Li; Xiaochun Ye; Dongrui Fan
- Reference count: 19
- One-line primary result: Achieves 53.8% pass@1 on VerilogEval-human benchmark with limited data via iterative training and filtering

## Executive Summary
This paper introduces ITERTL, an iterative training framework that fine-tunes large language models for register transfer level (RTL) code generation. Unlike traditional fine-tuning on fixed datasets, ITERTL updates training samples each iteration using feedback from RTL tools, combined with a data filtering strategy that encourages generation of self-contained, high-quality code. The approach significantly improves functional correctness while being data-efficient.

## Method Summary
ITERTL employs an iterative training paradigm where each round samples K responses per instruction (K-1 from the current model, 1 from a reference), scores them using RTL tool feedback and Rouge-L, then re-trains. A data filtering strategy removes or penalizes samples with multiple modules to encourage self-contained RTL code generation. The model is optimized using a weighted sum of cross-entropy and ranking loss, shifting the data distribution toward the model's current state while maintaining structural quality.

## Key Results
- Achieves 53.8% pass@1 on VerilogEval-human benchmark
- Outperforms existing methods while using limited reference data (~27k samples)
- Demonstrates significant improvement over baseline models through iterative refinement
- Shows data efficiency by achieving strong results with substantially less training data

## Why This Works (Mechanism)

### Mechanism 1
Iterative fine-tuning with model-sampled responses improves performance by expanding the exploration space and aligning the data distribution with the current model. Each iteration samples K responses per instruction, scores them using RTL tool feedback, and re-trains, shifting the data distribution toward the model's current state. The core assumption is that the model's own sampled outputs, when scored by an external tool, provide useful gradient signals for improvement.

### Mechanism 2
Data filtering by code structure encourages generation of self-contained, syntactically correct RTL modules. Reference samples with multiple modules are discarded; sampled samples containing multiple modules are assigned a fixed low score (-1), biasing the loss toward self-contained designs. The core assumption is that multi-module code introduces unnecessary complexity and is harder for current LLMs to generate correctly.

### Mechanism 3
Combining iterative sampling with filtering yields multiplicative performance gains over either technique alone. The iterative loop supplies progressively better samples; the filter ensures those samples meet structural quality criteria, so both the distribution shift and code quality improve in tandem. The core assumption is that the two methods address orthogonal failure modes—distribution mismatch and structural correctness.

## Foundational Learning

- Concept: Distribution shift in supervised fine-tuning
  - Why needed here: Fixed datasets cause the training distribution to diverge from the evolving model distribution, leading to estimation errors.
  - Quick check question: Why does training on a static dataset become less effective as the model changes?

- Concept: Automatic evaluation feedback loop
  - Why needed here: RTL tools provide syntactic correctness and functional similarity scores without manual labeling, enabling scalable iteration.
  - Quick check question: How can RTL tool outputs be used as quality scores in a loss function?

- Concept: Ranking loss vs. cross-entropy loss
  - Why needed here: Ranking loss leverages relative quality scores across samples, encouraging the model to prefer better outputs; cross-entropy anchors to a reference.
  - Quick check question: What is the benefit of using a ranking loss alongside cross-entropy in multi-sample training?

## Architecture Onboarding

- Component map: Input prompts -> Base LLM -> Sampler (top-p) -> Filter (multi-module) -> Scorer (RTL tool + Rouge-L) -> Optimizer (cross-entropy + ranking loss) -> Updated model
- Critical path: Prompt → Model → Sampler → Filter → Scorer → Loss → Optimizer → Updated model
- Design tradeoffs: Sampling diversity vs. computational cost (K candidates); strict filtering vs. retaining complex but valid samples; iteration count vs. diminishing returns
- Failure signatures: Loss plateaus early → filter too aggressive or model capacity exhausted; generated code remains syntactically invalid → RTL tool feedback not captured in loss; model overfits to filtered data → generalization to multi-module designs drops
- First 3 experiments: 1) Run one iteration with K=4, no filtering; measure pass@1 improvement; 2) Add filtering only (no iteration); compare pass@1 to baseline; 3) Combine iteration + filtering; verify multiplicative gain over experiments 1 and 2

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ITERTL scale with the number of iterations beyond the initial 7 rounds tested? The paper mentions the total number of iterations is set to 7 and suggests potential for further scaling, but only reports results up to 7 iterations. Empirical results showing performance metrics for models trained with 10, 15, or more iterations would demonstrate whether further improvements occur or if a performance ceiling is reached.

### Open Question 2
How would the inclusion of additional feedback signals, such as power, performance, and area (PPA) metrics, impact the effectiveness of the iterative training paradigm? The paper notes that the framework can be extended when feedback from other RTL tools is available, but the current implementation only utilizes syntactic correctness and similarity metrics. Experimental results comparing models trained with PPA feedback against those trained with only syntactic and similarity feedback would show improvements in functional correctness or other relevant metrics.

### Open Question 3
What is the impact of the data filtering strategy on the model's ability to generate more complex, multi-module RTL code in the long term? The data filtering strategy is designed to encourage high-quality, self-contained code by removing samples with multiple modules, but the paper acknowledges that more complex code can be composed of many independent modules. Comparative studies showing the model's performance on tasks requiring multi-module designs, both with and without the data filtering strategy, would assess any limitations or adaptations needed for handling complex code generation.

## Limitations
- The approach's effectiveness depends heavily on the quality and scope of RTL tools used for feedback, which may not generalize well to all RTL design patterns.
- The data filtering strategy's aggressiveness could potentially remove complex but valid multi-module designs, limiting the model's ability to handle real-world scenarios.
- The paper does not provide extensive ablation studies to isolate the contributions of iterative training versus filtering, leaving some ambiguity about the relative importance of each component.

## Confidence
- **High Confidence**: The core methodology of iterative fine-tuning with tool feedback is sound and well-supported by experimental results.
- **Medium Confidence**: The specific impact of the data filtering strategy is plausible but not fully isolated from other factors in the experiments.
- **Low Confidence**: The scalability of this approach to significantly larger models or more complex RTL tasks is not explored.

## Next Checks
1. Conduct ablation studies isolating the effects of iterative training, data filtering, and their combination to quantify each component's contribution to performance gains.
2. Test the model's performance using alternative or additional RTL tools to ensure the approach is not overly dependent on a specific toolchain.
3. Evaluate the model's ability to handle multi-module designs after filtering to assess potential trade-offs between code quality and complexity handling.