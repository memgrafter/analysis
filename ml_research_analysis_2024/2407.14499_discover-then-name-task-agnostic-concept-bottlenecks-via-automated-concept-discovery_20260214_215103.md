---
ver: rpa2
title: 'Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept
  Discovery'
arxiv_id: '2407.14499'
source_url: https://arxiv.org/abs/2407.14499
tags:
- concepts
- concept
- clip
- class
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a task-agnostic approach for automated concept
  discovery and naming in Concept Bottleneck Models (CBMs). The method uses sparse
  autoencoders to discover concepts learned by CLIP models and then names them using
  CLIP's text embeddings.
---

# Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery

## Quick Facts
- arXiv ID: 2407.14499
- Source URL: https://arxiv.org/abs/2407.14499
- Authors: Sukrut Rao, Sweta Mahajan, Moritz Böhle, Bernt Schiele
- Reference count: 40
- Key outcome: Task-agnostic automated concept discovery for CBMs using sparse autoencoders and CLIP text embeddings, achieving competitive performance across multiple datasets

## Executive Summary
This paper introduces Discover-then-Name (DN), a task-agnostic approach for automated concept discovery and naming in Concept Bottleneck Models (CBMs). The method uses sparse autoencoders to discover concepts learned by CLIP models and automatically names them using CLIP's text embeddings. These discovered concepts are then used to train linear classifiers for various downstream tasks, resulting in performant and interpretable CBMs without requiring task-specific concept engineering. The approach is evaluated on multiple datasets and CLIP architectures, demonstrating competitive performance compared to state-of-the-art methods while maintaining interpretability through local and global explanations.

## Method Summary
The method consists of three main steps: (1) Training sparse autoencoders on CLIP features from the CC3M dataset to discover concepts, (2) Automatically naming discovered concepts by matching dictionary vectors to the closest text embeddings in CLIP space using cosine similarity, and (3) Training linear classifiers on the named concepts for downstream tasks while keeping concept representations frozen. The approach leverages CLIP's aligned image-text representations to enable automated naming without manual annotation, and the discovered concepts are designed to be general enough to work across multiple classification tasks.

## Key Results
- Achieves 53.53% accuracy on Places365 with CLIP ResNet-50, outperforming previous best of 52.70%
- Demonstrates competitive performance across ImageNet, Places365, CIFAR10, and CIFAR100 datasets
- Shows effective concept interpretability through local and global explanations
- Validates concept intervention effectiveness for debiasing models

## Why This Works (Mechanism)

### Mechanism 1
Sparse autoencoders (SAEs) disentangle CLIP features into interpretable concepts. SAEs decompose high-dimensional CLIP embeddings into a sparse set of dictionary vectors, each corresponding to a human-interpretable concept. The core assumption is that learned dictionary vectors align with semantically meaningful concepts in CLIP space.

### Mechanism 2
CLIP text embeddings can automatically name discovered concepts. Dictionary vectors from SAE decoder are mapped to closest text embeddings in CLIP space using cosine similarity. The core assumption is that CLIP model has learned aligned image-text representations that preserve semantic relationships.

### Mechanism 3
Task-agnostic concept discovery yields performant CBMs across datasets. Once concepts are discovered and named, they can be frozen and used to train linear classifiers for any downstream task. The core assumption is that discovered concepts are sufficiently general and diverse to support multiple classification tasks.

## Foundational Learning

- **Sparse autoencoders**: Why needed here - To decompose complex CLIP features into interpretable, sparse concept representations. Quick check - How does the sparsity constraint affect reconstruction quality and interpretability?

- **Vision-language alignment (CLIP)**: Why needed here - To enable automated naming of discovered concepts using text embeddings. Quick check - Why does cosine similarity between dictionary vectors and text embeddings provide meaningful concept names?

- **Concept bottleneck models**: Why needed here - To create interpretable classifiers that explain decisions through human-understandable concepts. Quick check - How does freezing concept representations affect classification performance across different tasks?

## Architecture Onboarding

- **Component map**: CLIP vision encoder → SAE (linear encoder + ReLU + linear decoder) → Cosine similarity matching → Linear classifier

- **Critical path**: 1. Train SAE on CC3M dataset to discover concepts, 2. Name concepts using CLIP text embeddings, 3. Freeze concept extractor, 4. Train linear classifiers on downstream datasets

- **Design tradeoffs**: SAE expansion factor vs. reconstruction quality and sparsity, vocabulary size and granularity vs. naming accuracy, task-agnostic vs. task-specific concept discovery

- **Failure signatures**: Poor concept consistency across datasets, misaligned concept names, underperformance compared to task-specific baselines

- **First 3 experiments**: 1. Train SAE with different expansion factors on CC3M and evaluate reconstruction quality, 2. Test concept naming accuracy with different vocabulary sizes, 3. Compare classification performance of DN-CBM vs. task-specific baselines on one dataset

## Open Questions the Paper Calls Out

1. **Scaling dataset impact**: How would scaling up the dataset used for training the sparse autoencoder impact the diversity and granularity of discovered concepts? The current study uses CC3M dataset; impact of larger datasets remains experimentally validated.

2. **Vocabulary optimization**: Can naming accuracy be improved by using a more tailored vocabulary for the specific type of dataset used for SAE training? Current study uses generic 20k most frequent English words; impact of specialized vocabulary remains untested.

3. **Spurious correlation effects**: How do spurious correlations in the CLIP model affect discovered concepts and their activations in DN-CBM? While acknowledged, detailed analysis of impact and mitigation methods remains unexplored.

## Limitations
- Limited to image classification tasks - effectiveness for other modalities or complex reasoning tasks unknown
- Relies on qualitative assessments of interpretability rather than systematic human evaluation
- Risk of losing important but subtle concepts due to sparsity constraints

## Confidence

- **High Confidence**: Claims about improved classification accuracy compared to baseline methods
- **Medium Confidence**: Claims about concept interpretability and meaningfulness
- **Low Confidence**: Claims about task-agnostic generalization and scalability to other domains

## Next Checks

1. Conduct systematic human evaluation studies to assess concept interpretability and inter-annotator agreement
2. Evaluate concept consistency across different datasets and domain shifts
3. Systematically study sparsity parameter effects on both interpretability and classification performance