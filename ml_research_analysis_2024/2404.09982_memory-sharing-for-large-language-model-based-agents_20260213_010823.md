---
ver: rpa2
title: Memory Sharing for Large Language Model based Agents
arxiv_id: '2404.09982'
source_url: https://arxiv.org/abs/2404.09982
tags:
- memory
- agents
- memories
- answer
- pool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Memory Sharing framework that enhances
  Large Language Model (LLM)-based agents by enabling real-time memory sharing and
  retrieval. The framework uses a shared memory pool where multiple agents store and
  retrieve Prompt-Answer pairs, allowing collective self-enhancement through interactive
  learning.
---

# Memory Sharing for Large Language Model based Agents

## Quick Facts
- arXiv ID: 2404.09982
- Source URL: https://arxiv.org/abs/2404.09982
- Authors: Hang Gao; Yongfeng Zhang
- Reference count: 34
- Key outcome: MS framework improves LLM-based agents' performance through real-time memory sharing and retrieval, achieving higher BERTScores than zero-shot learning across three domains.

## Executive Summary
This paper introduces a Memory Sharing (MS) framework that enhances LLM-based agents by enabling real-time memory sharing and retrieval. The framework uses a shared memory pool where multiple agents store and retrieve Prompt-Answer pairs, allowing collective self-enhancement through interactive learning. Each generated answer is scored by an LLM evaluator, and high-quality memories are added to the pool, which also trains the memory retriever. Experiments across three domains—Literary Creation, Logic Problem-solving, and Plan Generation—show significant improvements in agent performance when utilizing shared memories.

## Method Summary
The MS framework integrates memory filter, storage, and retrieval components to enhance LLM-based agents. Agents share Prompt-Answer pairs as memories in a shared pool. When a query arrives, a retriever fetches relevant memories to construct prompts for the agent. Generated answers are scored by an LLM evaluator using domain-specific rubrics, and high-scoring memories are added to the pool. The retriever is continuously trained with new memories using BM25-based candidate selection and contrastive learning. The framework was tested on nine domain-specific datasets with varying shot-learning strategies (zero-shot to three-shot) across three LLMs.

## Key Results
- MS framework achieved higher BERTScores compared to zero-shot learning across all three domains
- Memory retrieval improved agent performance as the memory pool grew from 20% to 100% of dataset size
- Domain-specific memory pools outperformed single shared pools, demonstrating the importance of contextual relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared memory pools allow multiple agents to improve open-ended question performance by providing contextually relevant examples.
- Mechanism: Agents store high-quality Prompt-Answer pairs as memories in a shared pool. During query resolution, a retriever fetches similar memories to form prompts, enabling few-shot learning without retraining.
- Core assumption: Diverse PA pairs from different agents enhance retrieval quality and prompt relevance.
- Evidence anchors:
  - [abstract] "collective self-enhancement through interactive learning among multiple agents"
  - [section 3.1] "The diversity of group memories from different agents... significantly aids in improving the subsequent behavior of the agents"
  - [corpus] Weak—no direct citation found for cross-agent memory enhancement in the top neighbor papers.
- Break condition: If memory diversity decreases or retriever fails to select relevant memories, agent performance degrades.

### Mechanism 2
- Claim: Real-time memory training improves retriever accuracy over time.
- Mechanism: Newly added memories are used to train the retriever using a scoring function that ranks candidates by their potential relevance to the new memory, then applies contrastive learning.
- Core assumption: Continuous retriever updates adapt to evolving memory content and improve future retrieval.
- Evidence anchors:
  - [section 3.2] "The newly added memories then serve as references for the agents' subsequent performance"
  - [section 3.2] "Whenever a new memory... is added into the memory pool, it will also be used to train our retriever"
  - [corpus] Weak—neighbor papers mention memory systems but not iterative retriever training with contrastive learning.
- Break condition: If memory pool growth outpaces retriever capacity or if contrastive labels are noisy, training stalls.

### Mechanism 3
- Claim: Interactive learning bootstraps the system without external databases.
- Mechanism: Agents generate synthetic queries from known answers, solve them, and store successful PA pairs to seed the memory pool.
- Core assumption: Initial synthetic interactions generate useful memories that improve subsequent real interactions.
- Evidence anchors:
  - [section 3.3] "We propose a rapid interactive learning method that enables multiple agents to engage in interactive prompt and answer"
  - [section 3.3] "A small set of answers... is placed in the memory as the initial set"
  - [corpus] Weak—no direct mention of synthetic query generation for memory seeding.
- Break condition: If synthetic questions are too trivial or diverge from real query distributions, bootstrapping fails.

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is the core method by which agents use stored memories to improve answers without retraining.
  - Quick check question: What is the difference between zero-shot and one-shot ICL in terms of memory usage?

- Concept: Contrastive Learning
  - Why needed here: Used to train the retriever by contrasting relevant vs. irrelevant memory candidates.
  - Quick check question: How does a contrastive loss function improve retrieval ranking?

- Concept: Memory Pool Management
  - Why needed here: Ensures efficient storage, retrieval, and pruning of memories to maintain performance.
  - Quick check question: What factors should guide the decision to keep or discard a memory?

## Architecture Onboarding

- Component map:
  - Agents (LLM-based) -> Prompt-Answer Generator
  - LLM Scorer -> Memory Validator
  - Retriever (BM25/Dense) -> Memory Selector
  - Shared Memory Pool -> Central Storage
  - Memory Trainer -> Contrastive Learning Module

- Critical path:
  1. Agent generates answer to query.
  2. Answer scored by LLM scorer.
  3. If score exceeds threshold, memory added to pool.
  4. Memory used to train retriever.
  5. Future queries use retriever to fetch memories for prompt augmentation.

- Design tradeoffs:
  - Memory granularity: Detailed PA pairs vs. compact embeddings.
  - Retriever choice: Dense vs. sparse (BM25) for speed vs. accuracy.
  - Scoring overhead: LLM-based scoring vs. heuristic metrics.

- Failure signatures:
  - Low BERTScore improvement despite memory addition -> retriever ineffective.
  - High memory addition rate but no performance gain -> scorer too lenient.
  - Slow query response -> inefficient memory lookup.

- First 3 experiments:
  1. Baseline: Zero-shot performance without memory pool.
  2. Single-shot with one relevant memory per query.
  3. Three-shot with three relevant memories per query.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Memory pool quality depends heavily on the LLM evaluator's scoring criteria, which are not fully detailed
- No memory pruning strategy is addressed, raising concerns about long-term memory pool scalability
- Cross-agent memory sharing benefits are assumed but not directly proven through ablation studies

## Confidence
- High confidence: The MS framework's basic architecture and experimental methodology are clearly described and reproducible.
- Medium confidence: The claim that shared memories improve agent performance is supported by BERTScore improvements, but the exact contribution of memory diversity versus quantity is unclear.
- Low confidence: The long-term scalability of the memory pool and retriever training approach, particularly as the memory pool grows beyond the tested 100% phase.

## Next Checks
1. Test memory pool performance with memory pruning strategies to evaluate long-term scalability and prevent degradation.
2. Conduct ablation studies comparing performance with diverse versus homogeneous memories to quantify the impact of memory diversity.
3. Evaluate the sensitivity of performance to different LLM evaluator scoring thresholds to understand the robustness of memory selection.