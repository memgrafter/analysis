---
ver: rpa2
title: Topology-aware Embedding Memory for Continual Learning on Expanding Networks
arxiv_id: '2401.13200'
source_url: https://arxiv.org/abs/2401.13200
tags:
- learning
- memory
- graph
- networks
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Parameter Decoupled Graph Neural Networks
  (PDGNNs) with Topology-aware Embedding Memory (TEM) to address catastrophic forgetting
  in continual learning on expanding networks. The key innovation is a framework that
  decouples trainable parameters from individual nodes/edges via Topology-aware Embeddings
  (TEs), compressing computation ego-subnetworks into compact vectors.
---

# Topology-aware Embedding Memory for Continual Learning on Expanding Networks

## Quick Facts
- arXiv ID: 2401.13200
- Source URL: https://arxiv.org/abs/2401.13200
- Reference count: 40
- Primary result: PDGNNs-TEM achieves 81.9% average accuracy on CoraFull while reducing memory consumption to 2-37MB compared to 61MB-5,341MB for other methods

## Executive Summary
This paper addresses catastrophic forgetting in continual learning on expanding networks by introducing Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM). The framework compresses computation ego-subnetworks into fixed-size Topology-aware Embeddings (TEs), reducing memory complexity from O(nd^L) to O(n) while preserving topological information. The method discovers a pseudo-training effect where replaying one node's TE also trains its neighbors, motivating a coverage maximization sampling strategy. Experimental results demonstrate significant performance improvements over state-of-the-art methods across four benchmark datasets.

## Method Summary
The method introduces PDGNNs that decouple trainable parameters from individual nodes/edges by compressing computation ego-subnetworks into fixed-size Topology-aware Embeddings (TEs). These TEs are stored in a memory buffer and used for replay during continual learning. The framework leverages a pseudo-training effect where updating model parameters with one node's TE also influences predictions for neighboring nodes in the same computation ego-subnetwork. A coverage maximization sampling strategy prioritizes TEs with larger computation ego-subnetworks to enhance performance when memory budgets are tight. The approach significantly reduces memory consumption while maintaining or improving accuracy compared to baseline methods.

## Key Results
- Achieves 81.9% average accuracy on CoraFull with memory consumption of 2-37MB
- Outperforms state-of-the-art methods on OGB-Arxiv (53.2%), Reddit (94.7%), and OGB-Products (73.9%)
- Reduces memory space complexity from O(nd^L) to O(n) while preserving topological information

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PDGNNs with TEM reduce catastrophic forgetting by compressing computation ego-subnetworks into fixed-size TEs
- **Mechanism**: TEs compress topological information into compact vectors that can be stored efficiently, allowing parameter updates independent of original computation ego-subnetwork sizes
- **Core assumption**: Fixed-size TEs contain all necessary topological information for retraining PDGNNs
- **Evidence anchors**: Abstract and section 3.1 explicitly state memory complexity reduction from O(nd^L) to O(n) through TE compression
- **Break condition**: TEs fail to capture necessary topological information or buffer size is insufficient for representative TEs

### Mechanism 2
- **Claim**: Pseudo-training effect alleviates forgetting on neighboring nodes within computation ego-subnetworks
- **Mechanism**: Training with one node's TE influences predictions of all nodes in its computation ego-subnetwork because TE contains information from all nodes
- **Core assumption**: Neighborhood aggregation in GNNs allows pseudo-training to propagate information and reduce forgetting
- **Evidence anchors**: Abstract mentions pseudo-training effect motivating coverage maximization strategy; section 3.2 describes how TE replay influences neighboring nodes
- **Break condition**: Networks have low homophily ratio where neighboring nodes don't share labels with center node

### Mechanism 3
- **Claim**: Coverage maximization sampling enhances performance when memory budget is tight
- **Mechanism**: Prioritizes TEs with larger coverage ratios (nodes covered vs total nodes) to ensure stored TEs cover more nodes beneficial for pseudo-training
- **Core assumption**: TEs with larger computation ego-subnetworks covering more nodes are more effective in reducing forgetting
- **Evidence anchors**: Abstract links pseudo-training effect to coverage maximization strategy; section 3.3 discusses coverage ratio definition
- **Break condition**: Low homophily networks or sufficient memory budgets make coverage maximization unnecessary

## Foundational Learning

- **Concept**: Catastrophic forgetting in continual learning
  - **Why needed here**: Main challenge addressed - models trained incrementally on new node types experience severe performance degradation on old ones
  - **Quick check**: What is catastrophic forgetting, and why is it a problem in continual learning on expanding networks?

- **Concept**: Graph Neural Networks (GNNs) and message passing
  - **Why needed here**: GNNs are the base model used, and message passing over topological connections creates challenges for memory replay
  - **Quick check**: How do GNNs use message passing to capture topological information, and why does this pose challenges for memory replay in continual learning?

- **Concept**: Topological information and computation ego-subnetworks
  - **Why needed here**: Framework emphasizes preserving topological information through computation ego-subnetworks used to generate TEs
  - **Quick check**: What is the role of topological information in learning on expanding networks, and how are computation ego-subnetworks used to capture this information?

## Architecture Onboarding

- **Component map**: PDGNNs -> Topology-aware Embeddings (TEs) -> Topology-aware Embedding Memory (TEM) -> Coverage maximization sampling
- **Critical path**: Construct computation ego-subnetworks → Generate TEs via fixed function → Store TEs in TEM using coverage maximization → Retrain PDGNNs with stored TEs
- **Design tradeoffs**: Memory efficiency vs performance (TE compression may lose information); pseudo-training benefit vs homophily levels; coverage ratio vs TE diversity
- **Failure signatures**: Poor performance on old tasks (insufficient TEM effectiveness); high memory consumption (ineffective TE compression); slow convergence (ineffective pseudo-training)
- **First 3 experiments**: 1) Compare PDGNNs-TEM vs baselines on CoraFull under class-incremental setting; 2) Evaluate different sampling strategies with tight memory budgets; 3) Analyze learning dynamics and embeddings on Reddit dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in a dedicated section. However, based on the limitations section, several unresolved issues are identified: the need for theoretical justification of the pseudo-training effect mechanism, the optimal value of hyperparameter α in coverage maximization sampling, and performance on heterophilous networks compared to homophilous ones.

## Limitations

- Pseudo-training effect mechanism lacks rigorous theoretical justification and relies primarily on empirical observations
- Coverage maximization strategy assumes larger computation ego-subnetworks are more effective without systematic validation across different network topologies
- Experimental evaluation is limited to class-incremental learning settings without exploring other continual learning scenarios
- Computational complexity analysis doesn't account for TE generation overhead or training time impact on very large networks

## Confidence

- High confidence in memory complexity reduction claims (O(nd^L) → O(n))
- Medium confidence in pseudo-training effect mechanism
- Medium confidence in overall performance improvements over baselines
- Low confidence in theoretical guarantees for coverage maximization strategy

## Next Checks

1. Conduct ablation studies to isolate contribution of each component (TE generation, coverage maximization sampling, pseudo-training effect) to overall performance
2. Test framework on networks with varying homophily ratios to validate when pseudo-training effect is beneficial versus detrimental
3. Perform runtime analysis comparing PDGNNs-TEM to baseline methods to quantify trade-off between memory savings and computational overhead