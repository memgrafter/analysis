---
ver: rpa2
title: 'When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier
  Decisions with CoReX'
arxiv_id: '2405.01661'
source_url: https://arxiv.org/abs/2405.01661
tags:
- concepts
- concept
- relations
- explanations
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoReX, a novel explainable AI approach that
  combines concept relevance propagation with interpretable relational learning to
  explain CNN classifications. The method extracts human-understandable concepts from
  CNN intermediate layers using Concept Relevance Propagation (CRP), then learns symbolic
  rules over these concepts and their spatial relations using Inductive Logic Programming
  (ILP).
---

# When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX

## Quick Facts
- arXiv ID: 2405.01661
- Source URL: https://arxiv.org/abs/2405.01661
- Authors: Bettina Finzel; Patrick Hilme; Johannes Rabold; Ute Schmid
- Reference count: 40
- One-line primary result: CoReX achieves high-fidelity explanations (0.9860-0.9980 accuracy) for CNN classifications by combining concept relevance propagation with interpretable relational learning

## Executive Summary
CoReX is a novel explainable AI approach that explains CNN classifications by extracting human-understandable concepts from CNN intermediate layers and learning symbolic rules over these concepts and their spatial relations. The method uses Concept Relevance Propagation (CRP) to identify relevant concepts, then applies Inductive Logic Programming (ILP) to learn rules that capture both concept presence and spatial relationships. Experiments across multiple datasets show CoReX explanations are highly faithful to original CNN models, with human experts preferring combined concept-based and relational explanations over visual or verbal explanations alone.

## Method Summary
CoReX combines Concept Relevance Propagation (CRP) for extracting interpretable concepts from CNN intermediate layers with Inductive Logic Programming (ILP) for learning symbolic rules over these concepts and their spatial relations. The approach first applies CRP to propagate class-conditional relevance scores backward through the network, identifying high-relevance concepts in the last convolutional layer. These concepts are then localized and spatial relations between them are computed using frameworks like DE-9IM. The resulting background knowledge is fed to ILP systems (Aleph or Popper) to learn symbolic rules that explain the CNN's decision logic. The method is evaluated across multiple datasets including Picasso, Adience, ImageNet, and PathMNIST.

## Key Results
- CoReX explanations achieve high fidelity to CNN models with accuracy ranging from 0.9860 to 0.9980
- Ablation studies show masking concepts in ILP rules causes greater performance drops than masking irrelevant concepts
- Human experts prefer combined concept-based and relational explanations over visual or verbal explanations alone
- The approach enables contrastive explanations, rule-based cluster analysis, and user-defined constraints for interactive model refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining relevance-based concept extraction with ILP yields high-fidelity explanations for CNN classifications.
- Mechanism: CRP extracts human-understandable concepts from CNN intermediate layers by propagating class-conditional relevance scores backward through the network. ILP then learns symbolic rules over these concepts and their spatial relations. The fidelity arises because the surrogate ILP model captures both the presence of relevant concepts and their relational structure, which together reproduce the CNN's decision logic.
- Core assumption: Each filter in the last convolutional layer encodes one interpretable concept, and spatial relations between these concepts are sufficient to distinguish between classes.
- Evidence anchors:
  - [abstract]: "combines concept relevance propagation with interpretable relational learning to explain CNN classifications."
  - [section]: "Results show that CoReX explanations are faithful to the CNN model in terms of predictive outcomes."
  - [corpus]: "Evaluating Readability and Faithfulness of Concept-based Explanations" suggests concept-based explanations are being actively studied for faithfulness.

### Mechanism 2
- Claim: Masking irrelevant concepts has minimal impact on CNN performance, while masking concepts appearing in ILP rules causes significant drops.
- Mechanism: The ablation study removes concepts either not in ILP background knowledge (deemed irrelevant) or both irrelevant and rule-based concepts. A larger performance drop when masking rule concepts indicates these are critical to the CNN's classification, validating their importance.
- Core assumption: Concept relevance scores from CRP accurately reflect the importance of concepts for the CNN's predictions.
- Evidence anchors:
  - [section]: "masking concepts that occurred in the learned ILP theory as well as irrelevant concepts versus masking only concepts deemed irrelevant... the performance is reduced... in almost all scenarios."
  - [corpus]: "Sparse Feature Coactivation Reveals Causal Semantic Modules" suggests feature coactivation can reveal important semantic modules, supporting the idea that concept masking can reveal causal importance.

### Mechanism 3
- Claim: Combined concept-based and relational explanations are more useful for human understanding than visual or verbal explanations alone.
- Mechanism: Visual explanations (heatmaps) show where concepts are located but lack semantic labels; verbal explanations describe relations but may miss visual intuition. Combined explanations provide both spatial localization and semantic meaning, enabling users to grasp why specific concepts and their relations lead to classification.
- Core assumption: Human experts can better assess classification quality when they understand both what concepts are present and how they relate spatially.
- Evidence anchors:
  - [abstract]: "Human evaluation with experts shows combined concept-based and relational explanations are preferred over visual or verbal explanations alone."
  - [section]: "experts clearly preferred combined explanations given positive inter-rater reliability."
  - [corpus]: "Towards Human-Understandable Multi-Dimensional Concept Discovery" implies multi-dimensional (e.g., concept + relation) explanations are valued.

## Foundational Learning

- Concept: Concept Relevance Propagation (CRP)
  - Why needed here: CRP is the method for extracting interpretable concepts from CNN intermediate layers by propagating class-conditional relevance scores to the input. It provides the bridge between raw CNN features and human-understandable concepts.
  - Quick check question: How does CRP differ from standard LRP in terms of conditioning relevance on specific concepts?

- Concept: Inductive Logic Programming (ILP)
  - Why needed here: ILP learns symbolic rules over the extracted concepts and their spatial relations, providing an interpretable surrogate model that mimics the CNN's decision logic.
  - Quick check question: What are the consistency and completeness properties that ILP seeks to satisfy when learning rules?

- Concept: Spatial relation frameworks (DE-9IM, orientation, distance)
  - Why needed here: These frameworks formalize how concepts relate to each other in images, enabling the ILP model to capture relational patterns that distinguish classes.
  - Quick check question: How does the DE-9IM model encode topological relationships between concept polygons?

## Architecture Onboarding

- Component map:
  CNN model (e.g., VGG16/ResNet50) → CRP → Concept relevance maps → Localization → Spatial relations → Prolog background knowledge → ILP (Aleph/Popper) → Symbolic rules → Explanations

- Critical path:
  1. Run CNN inference on input image.
  2. Apply CRP to extract concept relevance maps from the last convolutional layer.
  3. Filter and localize high-relevance concepts.
  4. Compute spatial relations between concept polygons.
  5. Store in Prolog format and run ILP to learn rules.
  6. Generate explanations from rules.

- Design tradeoffs:
  - Using only the last convolutional layer simplifies analysis but may miss hierarchical concept organization.
  - Thresholding relevance scores reduces noise but risks discarding rare but important concepts.
  - ILP systems differ in search strategies (Aleph vs Popper), affecting rule coverage and fidelity.

- Failure signatures:
  - Low fidelity between ILP and CNN predictions suggests concepts or relations are not capturing the true decision logic.
  - High performance drop when masking non-rule concepts indicates irrelevant concepts are being overcounted.
  - Sparse rule coverage (many examples uncovered) suggests the ILP model is too specific or the concept set is incomplete.

- First 3 experiments:
  1. Run CoReX on a simple synthetic dataset (e.g., Picasso) and verify rule fidelity > 0.95.
  2. Perform ablation by masking irrelevant concepts and confirm minimal performance drop (< 5%).
  3. Compare explanation usefulness ratings between visual-only, verbal-only, and combined explanations in a small user study.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoReX perform when applied to datasets with more complex spatial relations and a larger number of concepts?
- Basis in paper: [inferred] The paper mentions that the number of concepts can be large (>500) and that spatial relations between these concepts are integrated into the background knowledge. However, the experiments focus on a limited number of datasets with relatively simple spatial relations.
- Why unresolved: The paper does not provide a comprehensive analysis of CoReX's performance on datasets with more complex spatial relations and a larger number of concepts. This could impact the scalability and generalizability of the approach.
- What evidence would resolve it: Experiments on datasets with more complex spatial relations and a larger number of concepts, along with a detailed analysis of CoReX's performance in these scenarios.

### Open Question 2
- Question: How does the choice of ILP system (Aleph vs. Popper) affect the quality and fidelity of the explanations generated by CoReX?
- Basis in paper: [explicit] The paper mentions that experiments were conducted with both Aleph and Popper, and provides some results comparing their performance. However, a detailed analysis of the impact of the ILP system choice on explanation quality and fidelity is not provided.
- Why unresolved: The paper does not provide a comprehensive comparison of the explanations generated by CoReX using Aleph and Popper, and how the choice of ILP system affects the quality and fidelity of these explanations.
- What evidence would resolve it: A detailed comparison of the explanations generated by CoReX using Aleph and Popper, including an analysis of their quality and fidelity in terms of predictive performance and human evaluation.

### Open Question 3
- Question: How does CoReX handle noisy or ambiguous concepts in the background knowledge, and how does this affect the quality of the explanations?
- Basis in paper: [inferred] The paper mentions that ILP is susceptible to noise in the underlying data, and that CoReX includes a mechanism for masking concepts. However, the paper does not provide a detailed analysis of how CoReX handles noisy or ambiguous concepts, and how this affects the quality of the explanations.
- Why unresolved: The paper does not provide a comprehensive analysis of how CoReX handles noisy or ambiguous concepts, and how this affects the quality of the explanations. This is important for understanding the robustness of the approach.
- What evidence would resolve it: Experiments on datasets with noisy or ambiguous concepts, along with a detailed analysis of how CoReX handles these concepts and how this affects the quality of the explanations.

## Limitations

- The assumption that each CNN filter encodes a single interpretable concept is not empirically validated across diverse architectures
- The human evaluation sample size and protocol details are insufficiently specified, limiting generalizability of the "combined explanations preferred" claim
- The relevance threshold values used for CRP are stated as quantile-based but exact values are missing, affecting reproducibility

## Confidence

- High confidence: The technical feasibility of combining CRP with ILP for concept extraction and rule learning
- Medium confidence: The ablation study results showing masking rule concepts causes greater performance drops than masking irrelevant concepts
- Medium confidence: The fidelity measurements between CoReX explanations and original CNN models
- Low confidence: The generalizability of human preference for combined explanations across different user groups and domains

## Next Checks

1. Conduct ablation studies with multiple relevance threshold values to assess robustness of the "masking rule concepts" effect
2. Perform cross-architecture validation using CNN models with different filter configurations (e.g., EfficientNet, MobileNet) to test the one-concept-per-filter assumption
3. Run user studies with domain experts from multiple fields (not just the original dataset domains) to validate the combined explanations preference claim