---
ver: rpa2
title: 'Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach
  for Real-Time Applications'
arxiv_id: '2410.02952'
source_url: https://arxiv.org/abs/2410.02952
tags:
- user
- tool
- student
- parameters
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of enabling real-time visual editing
  via natural language by fine-tuning smaller student LLMs to replicate the tool usage
  behavior of a larger teacher LLM, thereby reducing latency and cost. The core method
  involves collecting tool-chain outputs from a teacher LLM, filtering them using
  user feedback, and distilling this knowledge into a smaller student model via supervised
  fine-tuning, supplemented by data augmentation for low-data regimes.
---

# Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach for Real-Time Applications

## Quick Facts
- **arXiv ID:** 2410.02952
- **Source URL:** https://arxiv.org/abs/2410.02952
- **Reference count:** 12
- **Key outcome:** Student LLMs achieve comparable performance to teacher (96.1% completion rate) while being significantly faster and cheaper, with 25% improvement from data augmentation in low-data regimes.

## Executive Summary
This work addresses the challenge of enabling real-time visual editing via natural language by fine-tuning smaller student LLMs to replicate the tool usage behavior of a larger teacher LLM. The approach involves collecting tool-chain outputs from GPT-3.5-Turbo, filtering them using user feedback (export behavior), and distilling this knowledge into smaller models like Llama-2-7b-chat-hf via supervised fine-tuning. The method significantly reduces latency and cost while maintaining performance, validated through both offline evaluation metrics and online A/B tests. Data augmentation further improves results in low-data scenarios.

## Method Summary
The method collects user intents and corresponding teacher LLM outputs (GPT-3.5-Turbo) for visual editing tasks, filtering outputs by user engagement (export rates) to ensure quality. Student LLMs (Llama-2-7b-chat-hf or FlanT5-base) are fine-tuned using supervised learning with the filtered dataset and tool-specific prompts. Data augmentation is applied in low-data regimes by generating similar user intents based on student mistakes. The approach is evaluated using custom offline metrics (tool-selection score, quality score, final score) and validated through online A/B tests measuring project completion rates.

## Key Results
- Student models match teacher performance (GPT-3.5-Turbo) with 96.1% completion rate
- Fine-tuned models achieve 25% improvement in low-data regimes using data augmentation
- Significant reduction in latency and cost compared to using the teacher LLM directly

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning smaller student LLMs with teacher LLM outputs and user behavioral signals effectively reduces cost and latency while maintaining performance. The teacher LLM generates high-quality tool usage outputs based on user intents, which are filtered using user feedback to ensure quality. This filtered dataset is used to fine-tune a smaller student LLM, allowing it to replicate the teacher's tool selection and parameter setting behavior. The core assumption is that teacher outputs, when filtered by user feedback, provide sufficient quality data for effective distillation to smaller models.

### Mechanism 2
Offline evaluation metrics can effectively predict online A/B test performance for complex LLM tool chaining tasks. Custom metrics assess tool selection (precision/recall via F1-score) and parameter quality (cosine similarity for continuous values, accuracy for categorical filters). These metrics provide a cost-effective way to compare student models before deployment. The core assumption is that offline metrics correlate strongly with actual user satisfaction measured by project completion rates.

### Mechanism 3
Data augmentation significantly improves fine-tuning performance in low-data regimes for LLM tool chaining. When the student LLM makes mistakes on training samples, an augmentation LLM generates similar user intents. These new samples, paired with the teacher's original outputs, are added to the training set, expanding the effective dataset size and diversity. The core assumption is that generating similar but distinct user intents based on mistakes creates meaningful additional training signal without introducing harmful noise.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) reasoning in LLMs
  - **Why needed here:** CoT helps the teacher LLM generate more interpretable and accurate tool usage by explaining the reasoning behind parameter choices.
  - **Quick check question:** How does including rationale in the teacher LLM's output affect the quality of the distilled student model?

- **Concept:** Supervised Fine-Tuning (SFT) with auto-regressive and sequence-to-sequence objectives
  - **Why needed here:** SFT adapts the student LLM to the specific task of tool chaining by maximizing the likelihood of the teacher's outputs given user intents.
  - **Quick check question:** What are the differences between auto-regressive and sequence-to-sequence fine-tuning approaches for this tool chaining task?

- **Concept:** Data augmentation techniques for limited datasets
  - **Why needed here:** Augmentation addresses the common industry challenge of limited training data by synthetically expanding the dataset with meaningful variations.
  - **Quick check question:** How does mistake-based data augmentation differ from traditional data augmentation methods like paraphrasing or back-translation?

## Architecture Onboarding

- **Component map:** User intent → Teacher LLM output → User feedback filtering → Student LLM fine-tuning → Offline evaluation → Online A/B test → Deployment
- **Critical path:** User intent → Teacher LLM output → User feedback filtering → Student LLM fine-tuning → Offline evaluation → Online A/B test → Deployment
- **Design tradeoffs:**
  - Using a smaller student model reduces latency and cost but may sacrifice some performance compared to the teacher
  - Filtering teacher outputs by user feedback ensures quality but reduces dataset size
  - Including rationale in teacher outputs improves interpretability but increases latency
  - Mistake-based augmentation improves performance in low-data regimes but requires additional LLM calls
- **Failure signatures:**
  - Student model consistently selects wrong tools or sets parameters far from ground truth
  - Offline evaluation metrics show improvement but online A/B test shows no difference or regression
  - Augmentation leads to overfitting or generates unrealistic user intents
  - Latency increases significantly despite using a smaller model
- **First 3 experiments:**
  1. Fine-tune student LLM without any data augmentation and compare offline metrics to teacher
  2. Apply data augmentation to a subset of training data and measure improvement in offline metrics
  3. Conduct online A/B test comparing teacher LLM with student LLM (with and without augmentation) using project completion rate as primary metric

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the content, several unresolved issues emerge:

### Open Question 1
How would the performance of student LLMs vary if they were trained on data collected from multiple teacher LLMs (e.g., GPT-4, Claude) rather than just GPT-3.5-Turbo? The paper only uses one teacher LLM (GPT-3.5-Turbo) for data collection and doesn't explore whether using multiple teacher LLMs would improve student model performance or robustness.

### Open Question 2
What is the impact of including the rationale generation (the "TOOL" explanation) in the student LLM's output on both performance and latency? The paper explicitly states that they decided not to request rationale from the student LLM "as we prioritize low latency, and generating the reasoning significantly increases the response time."

### Open Question 3
How does the student LLM performance degrade when tested on user intents that are semantically similar but not identical to those in the training set? The data augmentation approach generates similar user intents to improve performance in low-data regimes, suggesting that the model's ability to generalize to similar but unseen intents is important.

### Open Question 4
What is the optimal ratio of teacher LLM outputs to user behavioral signals in the fine-tuning dataset for achieving the best balance between quality and cost? The paper mentions that they filter out samples with zero exports to train on high-quality data, indicating that user behavioral signals are used to filter teacher outputs.

### Open Question 5
How would the student LLM performance change if the three tools (adjust, selective adjust, filter) were applied in a different fixed sequence or with learned tool dependencies? The paper acknowledges that "The tools are applied in a fixed sequence (Adjust, Selective Adjust, LUT filters), which may not be optimal for all scenarios" and mentions this as a limitation.

## Limitations

- **Dataset Generalization**: The study relies on a single visual editing domain (tonal color adjustment), limiting generalizability to other tool-chain applications.
- **Offline-Online Gap**: While the paper claims offline metrics correlate with online A/B test results, the validation is limited to a single application.
- **Augmentation Quality**: The effectiveness of mistake-based data augmentation depends on the quality of the augmentation LLM's generated samples.

## Confidence

**High Confidence**: The core distillation approach (teacher outputs → student fine-tuning) works effectively for the specific visual editing task, as evidenced by both offline metrics and online A/B test results showing comparable performance to the teacher model.

**Medium Confidence**: Data augmentation improves performance in low-data regimes (25% improvement claimed). While supported by results, this finding is specific to one application and may not generalize to all tool-chain tasks.

**Low Confidence**: The offline evaluation metrics reliably predict online A/B test performance for complex LLM tool chaining tasks. The paper provides limited evidence for this claim beyond the single case study.

## Next Checks

1. **Cross-Domain Validation**: Apply the distillation approach to a different tool-chain domain (e.g., text editing or audio processing) and compare offline metric correlation with online performance to the original visual editing task.

2. **Dataset Size Sensitivity**: Systematically vary the amount of teacher data used for distillation (e.g., 10%, 25%, 50%, 100%) and measure the impact on student performance to understand the minimum viable dataset size.

3. **Augmentation Ablation Study**: Compare mistake-based augmentation against alternative data augmentation methods (e.g., paraphrasing, back-translation) to isolate the specific benefits of the proposed approach.