---
ver: rpa2
title: 'ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching'
arxiv_id: '2403.17312'
source_url: https://arxiv.org/abs/2403.17312
tags:
- attention
- memory
- tensors
- inference
- caching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of accelerating large language
  model (LLM) inference in resource-constrained systems, particularly focusing on
  the memory bottleneck caused by KV caching in attention layers. The authors propose
  ALISA, an algorithm-system co-design solution that combines sparse window attention
  (SWA) with dynamic scheduling and KV compression.
---

# ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching

## Quick Facts
- arXiv ID: 2403.17312
- Source URL: https://arxiv.org/abs/2403.17312
- Reference count: 40
- Primary result: ALISA improves LLM inference throughput by up to 3× on single GPU-CPU systems through sparse window attention and KV compression

## Executive Summary
This paper addresses the memory bottleneck in LLM inference caused by KV caching in attention layers. The authors propose ALISA, a system-level solution that combines Sparse Window Attention (SWA) with dynamic scheduling and KV compression to reduce memory footprint while maintaining accuracy. ALISA achieves up to 3× throughput improvement over baseline systems like FlexGen and vLLM on single GPU-CPU systems. The key insight is that attention weights are highly sparse, allowing aggressive pruning of KV tensors without significant accuracy loss.

## Method Summary
ALISA is an algorithm-system co-design solution that addresses LLM inference bottlenecks through three main components: Sparse Window Attention (SWA) that identifies and prioritizes important tokens for attention computation, a three-phase dynamic scheduling system that balances KV caching and recomputation at the token level, and KV compression that quantizes tensors to INT8 format to reduce memory footprint. The system integrates with existing frameworks like FlexGen and HuggingFace Transformers while maintaining negligible accuracy loss.

## Key Results
- ALISA achieves up to 3× throughput improvement over FlexGen and 1.9× over vLLM on single GPU-CPU systems
- Maintains negligible accuracy loss while reducing KV tensor memory footprint through sparsity and compression
- Three-phase dynamic scheduling effectively balances caching and recomputation trade-offs

## Why This Works (Mechanism)

### Mechanism 1
Sparse Window Attention (SWA) reduces KV tensor memory footprint by focusing computation on tokens with high local attention weights. SWA splits tokens into locally static (most recent) and globally dynamic (selected by max local attention sum), only keeping these in the sparse KV set and skipping less important ones. The core assumption is that attention weights are highly sparse and a small subset of tokens contributes most to the output.

### Mechanism 2
Dynamic scheduling balances caching and recomputation to reduce GPU-CPU data transfer overhead. The three-phase approach: Phase I caches all KV in GPU, Phase II splits KV between GPU and CPU, Phase III deletes old KV from CPU and recomputes when needed. The core assumption is that recomputation time is less than transfer time after certain sequence lengths.

### Mechanism 3
KV compression to INT8 reduces memory footprint and data transfer volume without significant accuracy loss. The channel-wise quantization formula compresses KV tensors using scaling factors. The core assumption is that quantized INT8 KV tensors retain enough precision for accurate attention computation.

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed here: ALISA optimizes the attention layers where KV caching occurs
  - Quick check question: What are the three tensors involved in attention and how are attention weights computed?

- Concept: KV caching and its memory scaling
  - Why needed here: ALISA's optimization targets the memory bottleneck of KV caching
  - Quick check question: How does the memory footprint of KV tensors scale with sequence length?

- Concept: Sparsity in attention weights
  - Why needed here: SWA exploits the inherent sparsity in attention weights
  - Quick check question: What evidence shows that attention weights in LLMs are highly sparse?

## Architecture Onboarding

- Component map: SWA algorithm module -> Dynamic scheduler -> KV compressor -> Integration layer -> FlexGen/HuggingFace Transformers

- Critical path:
  1. Token selection in SWA
  2. KV tensor generation and quantization
  3. Dynamic scheduling decision
  4. Memory allocation (GPU/CPU)
  5. Attention computation with sparse KV

- Design tradeoffs:
  - SWA caching ratio vs. accuracy: Higher sparsity reduces memory but may hurt accuracy
  - Recomputation vs. caching: Balances compute overhead against memory transfer
  - Quantization precision vs. accuracy: INT8 saves memory but may introduce error

- Failure signatures:
  - Accuracy drop: Likely due to overly aggressive sparsity or quantization
  - Increased execution time: Scheduler not optimizing the cache/recompute tradeoff
  - Out of memory: Insufficient sparsity or quantization, or scheduler not triggering Phase III

- First 3 experiments:
  1. Run ALISA with varying SWA caching ratios (20%, 40%, 60%, 80%) and measure accuracy and throughput
  2. Profile the three phases of dynamic scheduling to verify the phase transitions occur at expected sequence lengths
  3. Test INT8 KV compression with different scaling factors to find the best accuracy-throughput tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between KV caching and recomputation for different LLM model sizes and sequence lengths? The paper presents a three-phase scheduling approach but does not provide a comprehensive analysis of the optimal balance for various model configurations.

### Open Question 2
How does the proposed Sparse Window Attention (SWA) algorithm perform on multilingual LLMs and other modalities beyond text? The paper's evaluation is limited to English language tasks, leaving open questions about the algorithm's generalizability.

### Open Question 3
What are the implications of ALISA's dynamic scheduling on real-time applications and interactive systems? While the paper discusses performance improvements, it does not specifically address the impact on real-time and interactive applications where latency is critical.

## Limitations

- The core assumption of highly sparse attention weights is supported by internal evidence but lacks strong external validation from the corpus
- The effectiveness of dynamic scheduling depends heavily on specific hardware configuration and accurate offline optimization problem solving
- The paper focuses on single GPU-CPU systems and does not address multi-GPU scenarios, limiting generalizability

## Confidence

- High confidence: ALISA's ability to reduce KV memory footprint through sparse attention and quantization is well-supported by results and mechanism descriptions
- Medium confidence: The effectiveness of the three-phase dynamic scheduling approach is supported by methodology and results, but depends on accurate hardware profiling
- Low confidence: The paper's assumption of highly sparse attention weights in LLMs is supported by internal evidence but lacks strong external validation

## Next Checks

1. Validate attention sparsity assumption by running ALISA with varying levels of sparsity (20%, 40%, 60%, 80%) and measuring both accuracy and throughput

2. Profile dynamic scheduling phases by implementing detailed profiling to verify that phase transitions occur at expected sequence lengths

3. Test INT8 quantization robustness by evaluating ALISA across different model architectures (OPT, LLaMA, Pythia) and tasks to assess quantization impact on accuracy