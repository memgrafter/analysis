---
ver: rpa2
title: Divergent Creativity in Humans and Large Language Models
arxiv_id: '2405.13012'
source_url: https://arxiv.org/abs/2405.13012
tags:
- creativity
- llms
- creative
- human
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarked large language models (LLMs) on the Divergent
  Association Task (DAT), a measure of creative semantic divergence, and compared
  them to a large human dataset (N = 100,000). We found that GPT-4 surpassed human
  performance, and model performance could be modulated via prompt engineering and
  temperature tuning.
---

# Divergent Creativity in Humans and Large Language Models

## Quick Facts
- arXiv ID: 2405.13012
- Source URL: https://arxiv.org/abs/2405.13012
- Authors: Antoine Bellemare-Pepin; François Lespinasse; Philipp Thölke; Yann Harel; Kory Mathewson; Jay A. Olson; Yoshua Bengio; Karim Jerbi
- Reference count: 40
- This study benchmarked large language models (LLMs) on the Divergent Association Task (DAT), a measure of creative semantic divergence, and compared them to a large human dataset (N = 100,000).

## Executive Summary
This study systematically benchmarks large language models (LLMs) on the Divergent Association Task (DAT), a standardized measure of semantic creativity, and compares their performance against a large human dataset (N = 100,000). We find that GPT-4 significantly surpasses human performance on the DAT, and that both prompt engineering and temperature tuning can further modulate model creativity scores. LLMs also approach human-level performance in creative writing tasks. These results demonstrate that state-of-the-art LLMs can achieve or exceed human-level performance on certain creative tasks, with implications for understanding both AI and human creativity.

## Method Summary
The study benchmarked LLMs on the Divergent Association Task (DAT), measuring semantic divergence between 10-word sets using cosine similarity in embedding space. Human DAT responses from a large cohort (N = 100,000) were compared against LLM outputs from various models. Temperature and prompt engineering (e.g., etymology strategies) were used to modulate creativity. Creative writing tasks (haikus, movie synopses, flash fiction) were also evaluated using DSI, LZ complexity, and PCA embeddings.

## Key Results
- GPT-4 surpassed human performance on the DAT with statistically significant margins.
- Prompt engineering and temperature tuning can modulate LLM creativity scores.
- LLMs approach human-level performance on creative writing tasks (haikus, synopses, flash fiction).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can achieve or exceed human-level creativity on the DAT due to their ability to generate semantically distant word sets via latent vector transformations.
- Mechanism: The model samples words from a learned embedding space where cosine similarity quantifies semantic distance. By tuning sampling strategies (e.g., etymology prompts) or temperature, the model can explore more diverse regions of this space, increasing the mean semantic distance across the 10-word set.
- Core assumption: The model's internal representations correlate sufficiently with human semantic judgments to be meaningful for creativity assessment.
- Evidence anchors:
  - [abstract]: "We found that GPT-4 surpassed human performance, and model performance could be modulated via prompt engineering and temperature tuning."
  - [section]: "GPT-4 surpasses human scores with a statistically significant margin... LLMs do not directly access all semantic distances between word pairs; instead, they depend on iterative transformations of latent representations."
  - [corpus]: Corpus signals show related work focusing on "LLM creativity" and "divergent thinking" but does not directly validate the embedding-to-creativity mapping; this remains an assumption.
- Break condition: If the embedding space does not preserve semantic relationships important to human judgments of creativity, DAT scores will no longer correlate with human assessments.

### Mechanism 2
- Claim: Prompt engineering can explicitly guide LLMs toward more creative outputs by constraining the sampling strategy to exploit semantic diversity.
- Mechanism: Adding specific instructions (e.g., "use words with varying etymology") primes the model to select words from distinct semantic clusters, increasing the mean cosine distance and thus DAT score.
- Core assumption: The model's decoding process respects and acts upon explicit strategic instructions in a way that modifies its sampling distribution.
- Evidence anchors:
  - [abstract]: "model performance could be modulated via prompt engineering and temperature tuning."
  - [section]: "we found that when explicitly prompted to use words with varying etymology, both GPT-3 and GPT-4 outperformed the original DAT prompts."
  - [corpus]: No direct corpus evidence for efficacy of etymology-based prompting; assumes model's instruction-following generalizes to creative strategy prompts.
- Break condition: If the model fails to parse or prioritize the strategy cue, DAT performance will revert to baseline.

### Mechanism 3
- Claim: Higher temperature settings increase semantic divergence by reducing sampling determinism, enabling exploration of less probable but more distant word choices.
- Mechanism: Temperature controls the softmax distribution over vocabulary during decoding. A higher temperature flattens this distribution, making rarer, more semantically distant words more likely to be sampled.
- Core assumption: Flattening the distribution increases exploration of the embedding space in a way that aligns with creativity metrics.
- Evidence anchors:
  - [abstract]: "performance could be modulated via... temperature tuning."
  - [section]: "we observed a significant rise in DAT scores as a function of temperature... This increase in semantic divergence aligns with the concurrent decrease in word repetition frequency."
  - [corpus]: No corpus study directly validates the relationship between temperature and semantic diversity; this is inferred from model behavior.
- Break condition: If increased randomness introduces incoherence or violates task constraints, the metric may degrade despite higher diversity.

## Foundational Learning

- Concept: Cosine similarity in high-dimensional embedding spaces.
  - Why needed here: Used to quantify semantic distance between words for DAT scoring.
  - Quick check question: If two words have a cosine similarity of 0.9, are they semantically closer or farther apart than two words with a similarity of 0.3?
- Concept: Prompt engineering and instruction-following in LLMs.
  - Why needed here: Central to manipulating model output without retraining.
  - Quick check question: What happens to the model's output distribution if you prepend "In a highly creative way, ..." to a prompt?
- Concept: Temperature scaling in softmax decoding.
  - Why needed here: Controls randomness and diversity in generated word choices.
  - Quick check question: If temperature → 0, what is the expected sampling behavior? If temperature → ∞, what happens?

## Architecture Onboarding

- Component map:
  Prompt construction → LLM inference → Word embedding extraction → Cosine similarity matrix → Mean distance → DAT score
  Additional modules: Temperature control, strategy instruction parsing, compliance validation
- Critical path:
  1. Construct prompt with or without strategy cue
  2. Set temperature parameter
  3. Run inference → generate 10-word list
  4. Validate compliance (length, format)
  5. Compute pairwise cosine distances using GLoVe or BERT embeddings
  6. Average distances → DAT score
- Design tradeoffs:
  - Using GLoVe (context-independent) vs BERT (context-dependent) embeddings: GLoVe is faster but less nuanced; BERT is slower but may better capture creativity.
  - High temperature → more diverse but possibly incoherent outputs; low temperature → more coherent but less diverse.
  - Strict compliance checking vs leniency: Ensures valid comparisons but may discard borderline valid responses.
- Failure signatures:
  - Low compliance rate: Model fails to follow instructions (e.g., produces sentences instead of words).
  - Degraded scores at high temperature: Randomness outweighs semantic strategy.
  - High variance across seeds: Instability in scoring due to embedding randomness.
- First 3 experiments:
  1. Compare DAT scores for the same prompt across three temperature levels (low, mid, high) to verify monotonic increase.
  2. Test strategy prompts (etymology, thesaurus, meaning opposition) vs baseline to confirm improvement.
  3. Validate embedding choice by recomputing DAT scores with GLoVe vs BERT embeddings to assess correlation stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies influence the internal attention mechanisms of LLMs during the DAT, and can these patterns be linked to specific creative processes?
- Basis in paper: [explicit] The paper mentions that prompt engineering can manipulate LLM creativity, but does not explore the underlying attention mechanisms.
- Why unresolved: While the study shows that strategies like etymology and thesaurus improve DAT scores, it does not investigate the attention flows or internal representations that drive these improvements.
- What evidence would resolve it: Analyzing attention maps and activation patterns in LLMs when prompted with different strategies, and correlating these with creative output metrics like DAT scores.

### Open Question 2
- Question: Does the creative advantage of GPT-4 over humans on the DAT generalize to other forms of creativity, such as visual art or music composition, and how do these domains compare to language-based creativity?
- Basis in paper: [inferred] The paper focuses on language-based creativity tasks (DAT, haikus, synopses, flash fiction) but does not explore other creative domains.
- Why unresolved: The study establishes GPT-4's superiority in semantic creativity but does not address whether this advantage extends to non-linguistic creative tasks.
- What evidence would resolve it: Benchmarking GPT-4 and humans on creativity tasks in visual art (e.g., generating novel images) or music (e.g., composing original melodies) and comparing performance across domains.

### Open Question 3
- Question: What is the relationship between the temperature hyperparameter and the trade-off between creativity and coherence in LLM-generated content, and how does this vary across different creative tasks?
- Basis in paper: [explicit] The paper shows that increasing temperature boosts creativity scores but does not fully explore the coherence trade-off or task-specific variations.
- Why unresolved: While higher temperatures improve DAT and creative writing scores, the study does not investigate how this affects the coherence or usability of the output, nor does it compare this trade-off across tasks like haikus vs. flash fiction.
- What evidence would resolve it: Conducting a systematic analysis of temperature effects on both creativity and coherence metrics (e.g., readability, relevance) across multiple creative tasks, and identifying optimal temperature ranges for each.

## Limitations
- The DAT's validity as a comprehensive measure of human creativity remains debated.
- The comparison between human and LLM performance assumes equivalent understanding and task engagement.
- The effect sizes and generalizability of temperature and prompt engineering across different tasks and domains remain unclear.

## Confidence
- High Confidence: GPT-4 demonstrates statistically superior performance on the DAT compared to human baselines.
- Medium Confidence: Prompt engineering and temperature tuning can modulate DAT performance in predictable ways.
- Medium Confidence: LLMs approach human-level performance on creative writing tasks as measured by DSI, LZ complexity, and PCA embeddings.

## Next Checks
1. Cross-validation with alternative creativity metrics: Replicate the DAT benchmarking using alternative semantic distance measures and complementary creativity assessments to verify the robustness of the findings.
2. Domain transfer validation: Test whether temperature and prompt engineering effects generalize to non-linguistic creative tasks such as visual art generation or musical composition.
3. Temporal stability assessment: Conduct longitudinal studies measuring LLM performance on creativity tasks across different model versions and training epochs to determine whether the observed capabilities represent stable emergent properties.