---
ver: rpa2
title: 'FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved
  Personalization'
arxiv_id: '2410.12312'
source_url: https://arxiv.org/abs/2410.12312
tags:
- face
- identity
- generation
- image
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of identity-preserved personalization
  in human-centric image generation using text-to-image diffusion models. The key
  issue is that existing adapter-based methods often suffer from a significant performance
  decrease in text-following ability, controllability, and diversity of generated
  faces compared to the base model.
---

# FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved Personalization

## Quick Facts
- arXiv ID: 2410.12312
- Source URL: https://arxiv.org/abs/2410.12312
- Reference count: 36
- The paper proposes a face adapter framework that achieves 29.5% CLIP-T score and 80.6% Face Sim. score for text-to-image generation while preserving identity fidelity.

## Executive Summary
This paper addresses the challenge of identity-preserved personalization in human-centric image generation using text-to-image diffusion models. The key innovation is the Face Adapter with deCoupled Training (FACT) framework, which decouples identity feature extraction from other attributes and portrait generation training from the overall task. By using a transformer-based face expert encoder (TransFace) for fine-grained identity features and implementing Face Adapting Increment Regularization (FAIR) to constrain adapter effects to facial regions, FACT significantly outperforms existing adapter-based methods in both identity similarity and text-following ability.

## Method Summary
FACT introduces an Identity Merging Module that uses TransFace for extracting fine-grained identity features, which are then merged into the generation process through Sequential Face Adapter with Gated Self-Attention (SFAGS) modules inserted into the U-Net architecture. The method employs FAIR to constrain the adapter's impact to facial regions by limiting relative variation in non-face areas, and incorporates face condition drop and shuffle with curriculum learning to enhance controllability and diversity. The framework is trained on Stable Diffusion v1.5 with masked diffusion loss and FAIR regularization, using a single portrait image for customization and large-scale portrait datasets for training.

## Key Results
- For text-to-image generation: CLIP-T score of 29.5%, CLIP-I score of 71.8%, Face Sim. of 80.6%, FID score of 176.5
- For inpainting-based portrait generation: E-Exp. score of 3.06, E-Light score of 0.282, E-Pose score of 0.00576, E-Shape score of 6.26, Face Sim. of 78.2%, CLIP-I score of 73.3%, FID score of 30.4
- FACT outperforms existing adapter-based methods like PhotoMaker, DreamBooth, and DeepFaceDrawing in identity similarity, text-following ability, and facial controllability/diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling identity features from other attributes using a transformer-based face expert encoder preserves more detailed identity information during training.
- Mechanism: TransFace extracts fine-grained identity features from all visual tokens in the penultimate layer, capturing more detailed facial information compared to general image encoders like CLIP.
- Core assumption: Face expert encoders can better preserve intra-identity compactness and inter-identity separation than general image encoders.
- Evidence anchors:
  - "we use the TransFace model [3] for feature extraction, which is a ViT-based face recognition model to achieve better identity discrimination and compatibility with the Stable Diffusion architecture."
  - "Compared with general image encoders such as CLIP [23], the identity features extracted by the face expert encoder have stronger intra-identity compactness and inter-identity separation."

### Mechanism 2
- Claim: Face Adapting Increment Regularization (FAIR) constrains the effect of face adapters specifically to the facial region while preserving the original generation ability of the base model.
- Mechanism: FAIR limits the relative variation of latent caused by the face adapter in areas outside of the face by calculating the ratio of L2 norm of adapter's effect on non-face regions to total L2 norm.
- Core assumption: The impact of sequential face adapters can be effectively measured and controlled by monitoring the increment in visual tokens.
- Evidence anchors:
  - "we propose a soft method to constrain the model. Essentially, the impact of the sequential face adapter is the increment of the visual token x in Eq. 2. Therefore, we propose a Face Adapting Increment Regularization (FAIR) to decrease the impact of sequential face adapter on the outside of the face region."
  - "As such, FAIR retrains the increment by limiting the relative variation of latent caused by the face adapter in areas outside of the face and paying attention to different areas inside the face."

### Mechanism 3
- Claim: Face condition drop and shuffle with curriculum learning increases facial controllability and diversity while maintaining identity preservation.
- Mechanism: During training, the model is exposed to paired faces, unpaired identities, and paired identities with unpaired faces through random face condition drop and shuffling from the same identity, with curriculum learning gradually increasing shuffling probability.
- Core assumption: Exposure to varied identity conditions during training can improve the model's ability to generate diverse and controllable faces while maintaining identity preservation.
- Evidence anchors:
  - "To increase the variability and the controllability of the portrait, we propose a curriculum learning with face condition drop and shuffle. Inspired by the Classifier Free Guidance (CFG) [12], we train image generation under the guidance of 'paired faces', 'unpaired identities', and 'paired identities + unpaired faces'."
  - "Specifically, we conduct random face condition drop and shuffling from the same identity, corresponding to the 'unpaired identities' and 'paired identities + unpaired faces' respectively, before feeding to the diffusion model."

## Foundational Learning

- Concept: Understanding of diffusion models and their training process
  - Why needed here: The method builds upon Stable Diffusion, so understanding how diffusion models work is crucial for implementing and modifying the architecture.
  - Quick check question: Can you explain the basic denoising process in a diffusion model and how it differs from pixel-space approaches?

- Concept: Knowledge of attention mechanisms in transformer architectures
  - Why needed here: The method uses both self-attention and cross-attention layers, and introduces a gated self-attention mechanism for face adaptation.
  - Quick check question: What is the difference between self-attention and cross-attention, and how does the gated self-attention mechanism work in this context?

- Concept: Understanding of identity preservation techniques in image generation
  - Why needed here: The core of this method is to preserve identity while maintaining other generation capabilities, which requires understanding existing identity preservation approaches.
  - Quick check question: How do methods like DreamBooth and Textual Inversion approach identity preservation, and what are their limitations compared to adapter-based approaches?

## Architecture Onboarding

- Component map: Input portrait → TransFace encoding → SFAGS insertion in U-Net → FAIR regularization → Output generation

- Critical path: Input portrait → TransFace encoding → Sequential Face Adapter with Gated Self-Attention modules → FAIR regularization → Output generation

- Design tradeoffs:
  - Using face expert encoder vs general image encoder: Better identity preservation but potentially less compatibility with text-to-image generation
  - Sequential vs parallel adapter architecture: Better decoupling but potentially more complex training
  - FAIR regularization strength: Balance between face preservation and overall generation quality

- Failure signatures:
  - Identity loss: Generated faces don't match the reference identity
  - Text-following degradation: Generated images don't follow text prompts well
  - Overfitting: Generated images are too similar to training data
  - Inpainting issues: Generated faces don't match template poses or expressions

- First 3 experiments:
  1. Implement TransFace feature extraction and verify identity feature quality compared to CLIP
  2. Test SFAGS insertion in U-Net with basic training to ensure compatibility
  3. Evaluate FAIR regularization effectiveness by comparing face adapter impact on facial vs non-facial regions

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored:
- How does the performance scale with different numbers of reference images per identity?
- How does FACT compare to fine-tuning methods like DreamBooth on the same metrics?
- What is the impact of the face condition drop and shuffle mechanism across different demographic groups?

## Limitations

- The method's effectiveness depends heavily on the quality and compatibility of the TransFace face recognition model with the Stable Diffusion architecture.
- The FAIR regularization mechanism may introduce a trade-off between identity preservation and overall generation quality.
- The face condition drop and shuffle mechanism adds complexity to the training process and requires careful hyperparameter tuning.

## Confidence

**High Confidence Claims:**
- The FACT framework architecture is technically sound and implementable
- Sequential face adapters with Gated Self-Attention can effectively merge identity features
- The decoupling approach addresses key limitations of existing adapter-based methods

**Medium Confidence Claims:**
- TransFace provides significantly better identity features than general image encoders
- FAIR effectively constrains adapter impact to facial regions
- Face condition drop and shuffle with curriculum learning improves controllability and diversity

**Low Confidence Claims:**
- The reported improvements generalize well across diverse identity types and challenging conditions
- The method's computational overhead is acceptable for practical deployment
- The face condition drop and shuffle mechanism is robust across different curriculum learning schedules

## Next Checks

1. **Cross-Demographic Validation**: Test the method on diverse identities across different age groups, ethnicities, and skin tones to verify generalization beyond specific demographic groups.

2. **Robustness to Extreme Conditions**: Evaluate performance on images with challenging conditions such as extreme lighting, occlusions, or unusual poses to assess robustness of feature extraction and regularization.

3. **Ablation Study on FAIR Strength**: Conduct comprehensive ablation study varying FAIR regularization strength to identify optimal balance between identity preservation and generation quality, and determine if current implementation over-constrains model expressiveness.