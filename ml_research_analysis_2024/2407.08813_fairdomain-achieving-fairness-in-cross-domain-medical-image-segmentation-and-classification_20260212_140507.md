---
ver: rpa2
title: 'FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation
  and Classification'
arxiv_id: '2407.08813'
source_url: https://arxiv.org/abs/2407.08813
tags:
- domain
- fairness
- segmentation
- medical
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive study on algorithmic
  fairness under domain shifts in medical imaging, addressing a critical gap where
  models trained on one imaging modality (e.g., En Face Fundus) must generalize to
  others (e.g., SLO Fundus). The authors introduce Fair Identity Attention (FIA),
  a plug-and-play module that improves fairness by adjusting feature importance based
  on demographic attributes using self-attention.
---

# FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification

## Quick Facts
- arXiv ID: 2407.08813
- Source URL: https://arxiv.org/abs/2407.08813
- Reference count: 40
- Primary result: First comprehensive study on algorithmic fairness under domain shifts in medical imaging, achieving up to 17.8% improvement in fairness metrics with Fair Identity Attention module

## Executive Summary
This paper addresses the critical challenge of maintaining algorithmic fairness when medical imaging models trained on one modality must generalize to others. The authors introduce Fair Identity Attention (FIA), a plug-and-play module that improves fairness by dynamically adjusting feature importance based on demographic attributes using self-attention. They also curate a novel paired dataset with consistent patient distributions across two imaging modalities (En Face Fundus and SLO Fundus) for both segmentation and classification tasks. Experiments demonstrate that integrating FIA significantly improves both model performance and fairness across all demographic groups, outperforming existing domain adaptation and generalization approaches.

## Method Summary
The FairDomain framework combines a TransUNet backbone with the Fair Identity Attention (FIA) module inserted after each encoder stage. FIA takes demographic attributes as input, embeds them, and uses them to generate attention query vectors that adjust feature importance. The model is trained with AdamW optimizer (encoder LR: 6e-5, decoder LR: 6e-4) for 40k iterations on a paired dataset of 10,000 samples across two imaging modalities. The equity-scaled performance (ESP) metric evaluates both accuracy and fairness by penalizing performance disparities across demographic groups. The approach is compared against state-of-the-art DA/DG methods including PixMatch, CBST, DAFormer, CDTrans, and IRM.

## Key Results
- Segmentation: ES-Dice improved from 0.781 to 0.802 for cup segmentation and from 0.344 to 0.528 for rim segmentation
- Classification: ES-AUC improved from 0.630 to 0.636 for race
- Fair Identity Attention consistently improved fairness metrics across all demographic groups
- Outperformed existing DA/DG methods on both segmentation and classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair Identity Attention (FIA) improves fairness by dynamically adjusting feature importance based on demographic attributes using self-attention
- Mechanism: The module maps demographic attributes onto attention query vectors, allowing the model to extract features relevant to both the task and fairness considerations
- Core assumption: Demographic attributes can be effectively encoded and used to guide the attention mechanism in a way that improves fairness without sacrificing performance
- Evidence anchors:
  - [abstract]: "We also introduce a novel plug-and-play fair identity attention (FIA) module that adapts to various DA and DG algorithms to improve fairness by using self-attention to adjust feature importance based on demographic attributes."
  - [section]: "We aim to develop a methodical function f that mitigates fairness deterioration... To address this, we propose an attention based mechanism designed to discern and utilize features from images that are pertinent to the downstream tasks... while also taking into account the demographic attributes."

### Mechanism 2
- Claim: The equity-scaled performance (ESP) metric provides a balanced evaluation of model accuracy and fairness
- Mechanism: ESP is calculated as the traditional metric divided by (1 + Δ), where Δ is the aggregate deviation of each demographic group's metric from the overall performance
- Core assumption: Penalizing performance disparities in this way will lead to models that are both accurate and fair
- Evidence anchors:
  - [section]: "To address the potential tradeoff between model performance and fairness in medical imaging, we use novel equity-scaled performance (ESP) metrics to assess both performance and fairness for both segmentation and classification tasks."
  - [section]: "This unified metric facilitates a comprehensive assessment of deep learning models, emphasizing not only their accuracy... but also their fairness across different demographic groups."

### Mechanism 3
- Claim: The paired dataset with consistent patient distributions allows clearer assessment of fairness under domain shifts
- Mechanism: Using the same patient cohort imaged with two different modalities isolates the effect of domain shift on model fairness
- Core assumption: Using the same patient cohort for both imaging modalities eliminates the confounding effect of demographic distribution differences
- Evidence anchors:
  - [abstract]: "Additionally, we curate the first fairness-focused dataset with two paired imaging modalities for the same patient cohort... Excluding the confounding impact of demographic distribution variation between source and target domains will allow clearer quantification of the performance of domain transfer models."
  - [section]: "To address this, we curated a unique dataset with paired retinal fundus images from the same patient cohort, captured through two distinct imaging modalities (En face and SLO fundus images)..."

## Foundational Learning

- **Domain Adaptation (DA) and Domain Generalization (DG)**
  - Why needed here: The paper addresses fairness issues in medical imaging under domain shifts, requiring understanding how models trained on one domain can be adapted or generalized to another while maintaining fairness
  - Quick check question: What is the key difference between domain adaptation and domain generalization, and why is it relevant to the fairness problem in medical imaging?

- **Fairness metrics and equity-scaled performance (ESP)**
  - Why needed here: The paper introduces the ESP metric to evaluate both accuracy and fairness, requiring understanding how to quantify and assess fairness in machine learning
  - Quick check question: How does the ESP metric penalize performance disparities across demographic groups, and why is this important for ensuring fairness in medical imaging?

- **Self-attention mechanisms and their application in fairness**
  - Why needed here: The paper proposes the Fair Identity Attention (FIA) module, which uses self-attention to adjust feature importance based on demographic attributes
  - Quick check question: How does the FIA module use self-attention to adjust feature importance based on demographic attributes, and why is this approach effective for improving fairness in medical imaging?

## Architecture Onboarding

- **Component map:** Input images + demographic attributes -> Fair Identity Attention module -> Backbone model (TransUNet) -> Segmentation masks or classification labels + ESP metrics
- **Critical path:**
  1. Preprocess input images and demographic attributes
  2. Pass images and attributes through FIA module to obtain adjusted features
  3. Pass adjusted features through backbone model to obtain task-specific outputs
  4. Calculate ESP metrics to evaluate both accuracy and fairness
- **Design tradeoffs:** FIA module adds computational overhead; effectiveness depends on quality of demographic attribute embeddings and self-attention mechanism; ESP metric may be sensitive to choice of underlying performance metric
- **Failure signatures:** FIA module fails to learn meaningful representations from demographic attributes; ESP metric is not well-calibrated; paired dataset is not representative of real-world scenarios
- **First 3 experiments:**
  1. Implement FIA module as standalone component and test its ability to adjust feature importance based on demographic attributes
  2. Integrate FIA module into simple segmentation or classification model and evaluate its impact on fairness metrics
  3. Train model on paired dataset and compare performance and fairness to baseline model without FIA module

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does FairDomain's performance scale with larger and more diverse datasets?
  - Basis in paper: [explicit] The paper mentions the FairDomain dataset contains 10,000 samples, but does not explore performance on larger datasets
  - Why unresolved: The paper does not investigate the impact of dataset size on the effectiveness of FairDomain's fairness improvements
  - What evidence would resolve it: Experiments comparing FairDomain's performance on datasets of varying sizes and diversity would provide insights into its scalability

## Limitations

- The FIA module's effectiveness heavily depends on the quality of demographic attribute embeddings, which are not fully specified in the paper
- The paired dataset may not fully capture real-world domain shift scenarios where patient cohorts differ between imaging modalities
- The ESP metric may not perfectly align with clinical definitions of fairness in medical practice
- The computational overhead introduced by the FIA module could limit its practical deployment in resource-constrained clinical settings

## Confidence

- **High confidence:** The core mechanism of Fair Identity Attention and its implementation details
- **Medium confidence:** The effectiveness of the ESP metric as a comprehensive fairness measure
- **Medium confidence:** The paired dataset's ability to isolate domain shift effects from demographic distribution differences

## Next Checks

1. Conduct ablation studies varying the quality and granularity of demographic attribute embeddings to determine their impact on fairness improvements
2. Validate the ESP metric against alternative fairness metrics (e.g., demographic parity, equal opportunity) on the same dataset to assess robustness
3. Test the FIA module on unpaired cross-modality datasets to evaluate its generalizability beyond the controlled paired dataset environment