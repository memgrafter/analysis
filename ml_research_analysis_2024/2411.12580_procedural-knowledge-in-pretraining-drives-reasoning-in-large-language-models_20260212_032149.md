---
ver: rpa2
title: Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models
arxiv_id: '2411.12580'
source_url: https://arxiv.org/abs/2411.12580
tags:
- reasoning
- documents
- query
- influence
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how Large Language Models (LLMs) learn to
  reason by examining the pretraining data that influences their outputs. Using EK-FAC
  influence functions, the authors identify which documents from a sample of 2.5 billion
  tokens are most influential on model completions for 40 factual and 40 reasoning
  queries.
---

# Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models

## Quick Facts
- **arXiv ID:** 2411.12580
- **Source URL:** https://arxiv.org/abs/2411.12580
- **Reference count:** 40
- **Primary result:** Code data is strongly overrepresented in influential documents for reasoning tasks, suggesting LLMs generalize reasoning by synthesizing procedural knowledge from related documents rather than retrieving specific answers.

## Executive Summary
This paper investigates how Large Language Models (LLMs) learn to reason by examining which pretraining documents influence their outputs for reasoning versus factual queries. Using EK-FAC influence functions, the authors identify that models rely less on individual documents per unit of information for reasoning compared to factual retrieval, and that code data is strongly overrepresented in influential documents for reasoning tasks. The key finding is that LLMs generalize reasoning by synthesizing procedural knowledge from documents demonstrating similar reasoning processes rather than retrieving specific answers.

## Method Summary
The authors use EK-FAC influence functions to compute document influence scores on model outputs for 40 factual and 40 reasoning queries. They sample 5 million documents from 2.5 billion tokens of pretraining data, then analyze influence patterns across two models (7B and 35B parameters) on three mathematical reasoning tasks. The method involves computing Hessian estimation, query gradient computation using SVD approximation, and document influence calculation, followed by ranking and qualitative analysis of top influential documents.

## Key Results
- Documents influence reasoning queries within the same task similarly, suggesting procedural knowledge rather than retrieval
- Models rely less on individual documents per unit of information for reasoning compared to factual retrieval
- Answers to factual questions often appear in top influential documents, but rarely for reasoning
- Code data is strongly overrepresented in influential documents for reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs generalize reasoning by synthesizing procedural knowledge from documents demonstrating similar reasoning processes, rather than retrieving specific answers.
- **Mechanism:** When solving reasoning tasks, the model draws from a broad range of documents that are abstractly related to the problem, with each document influencing many different questions similarly but contributing a relatively small amount to the final output.
- **Core assumption:** The model can extract general procedures from examples rather than memorizing specific problem-solution pairs.
- **Evidence anchors:**
  - [abstract] "documents often contribute similarly to many questions that require applying the same procedure to different numbers"
  - [section 5.1] "a document often has a similar influence across different reasoning questions within the same task"
  - [corpus] Found 25 related papers with average neighbor FMR=0.547, including work on procedural pretraining and modular structures for algorithmic reasoning
- **Break condition:** If the model instead relies on retrieving specific answers from pretraining data, we would expect to see answers to reasoning questions frequently appearing in top influential documents, which the paper explicitly shows does not happen.

### Mechanism 2
- **Claim:** For reasoning tasks, the model relies less on individual documents per unit of information generated compared to factual retrieval tasks.
- **Mechanism:** The magnitude of influence of documents per nat of query information is usually much lower for reasoning questions than for factual questions, indicating the model distributes its reliance across more documents when reasoning.
- **Core assumption:** Reasoning requires synthesizing information from multiple sources rather than pulling from a single authoritative source.
- **Evidence anchors:**
  - [section 5.1] "the models rely less on each individual document per nat of query information they generate than for factual retrieval"
  - [section 5.1] "the overall magnitude of influence of the set of documents is less volatile" for reasoning
  - [corpus] No direct evidence found in related papers for this specific mechanism
- **Break condition:** If the model relied heavily on individual documents for reasoning, we would expect high variance in influence scores across factual queries and higher absolute influence per document for reasoning tasks.

### Mechanism 3
- **Claim:** Code data is strongly overrepresented in influential documents for reasoning tasks, suggesting code provides procedural knowledge applicable across multiple reasoning domains.
- **Mechanism:** Documents containing code implementations of solutions to reasoning problems are highly influential, indicating that code serves as a generalizable procedural template that the model can adapt to different numerical inputs.
- **Core assumption:** Code represents abstract procedural knowledge that can be applied to various instances of the same reasoning type.
- **Evidence anchors:**
  - [section 5.2] "code data is strongly overrepresented w.r.t. the training distribution for the top portions of the positively and negatively influential rankings for reasoning queries"
  - [section 5.2] "documents that present procedural knowledge on how to calculate the slope in either code or math show up in the top 100 documents for 16/20 queries"
  - [corpus] Related paper "Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning" supports this mechanism
- **Break condition:** If code were not providing generalizable procedural knowledge, we would expect code to be influential only for specific reasoning types rather than across multiple mathematical reasoning tasks.

## Foundational Learning

- **Concept:** Influence functions and their application to large-scale transformers
  - Why needed here: The paper uses EK-FAC influence functions to identify which pretraining documents influence model outputs for reasoning versus factual queries
  - Quick check question: How does the EK-FAC approximation make influence function computation tractable for models with billions of parameters?

- **Concept:** Procedural versus declarative knowledge in machine learning
  - Why needed here: The key distinction is between models retrieving specific answers (declarative) versus applying learned procedures (procedural) to solve reasoning tasks
  - Quick check question: What evidence would indicate a model is using procedural knowledge versus declarative knowledge retrieval?

- **Concept:** Power law distributions in influence rankings
  - Why needed here: The paper analyzes how influence is distributed across documents, finding steeper power laws for reasoning tasks, which indicates different generalization strategies
  - Quick check question: What does a steeper power law slope in influence rankings suggest about how many documents a model relies on for different task types?

## Architecture Onboarding

- **Component map:** Influence function computation (Hessian estimation + document gradients) -> Query processing (batch processing of prompt-completion pairs) -> Analysis pipeline (ranking documents by influence, correlating influences across queries, qualitative analysis)
- **Critical path:** Pretraining data sampling -> Hessian estimation (100k documents) -> Query gradient computation (SVD approximation) -> Document influence calculation -> Ranking and analysis
- **Design tradeoffs:** Sampling only 5M documents from 2.5B tokens balances computational tractability with coverage; using SVD approximation for query gradients enables batching but may lose some precision; block-diagonal Hessian approximation speeds computation but may miss cross-layer interactions
- **Failure signatures:** If correlations between reasoning queries are not significant, it suggests the model isn't using procedural knowledge; if answers to reasoning questions frequently appear in top documents, it suggests retrieval rather than generalization; if code is not overrepresented for reasoning, it suggests code doesn't provide generalizable procedures
- **First 3 experiments:**
  1. Run influence analysis on factual vs reasoning queries and verify that reasoning query correlations are significant while factual correlations are not
  2. Check for presence of query answers in top influential documents and confirm reasoning answers rarely appear while factual answers frequently appear
  3. Analyze source dataset distribution in top rankings and verify code/StackExchange are overrepresented for reasoning tasks

## Open Questions the Paper Calls Out

- **Open Question 1:** What properties of code data make it influential for mathematical reasoning tasks?
  - Basis in paper: Explicit - The paper finds code data is strongly overrepresented in influential documents for reasoning tasks across all tasks examined.
  - Why unresolved: While the paper identifies code as important for reasoning, it doesn't investigate which specific properties of code (syntax, procedural nature, abstraction level, etc.) contribute to this influence or how these properties differ between positively and negatively influential code documents.
  - What evidence would resolve it: Detailed qualitative analysis of positively vs. negatively influential code documents showing specific features (e.g., presence of mathematical operations, algorithmic structure, commenting style) that correlate with influence on reasoning tasks.

- **Open Question 2:** Does the procedural generalization strategy observed for mathematical reasoning extend to other types of reasoning like inductive or abductive reasoning?
  - Basis in paper: Inferred - The paper only examines mathematical reasoning tasks and notes this as a limitation, suggesting future work should verify whether similar results hold for other reasoning types.
  - Why unresolved: The study's findings are limited to mathematical reasoning (arithmetic, slopes, linear equations), leaving open whether the procedural knowledge synthesis strategy applies to more complex or abstract reasoning tasks.
  - What evidence would resolve it: Replication of the influence analysis methodology on non-mathematical reasoning benchmarks (e.g., commonsense reasoning, analogical reasoning, or scientific reasoning tasks) showing similar patterns of procedural knowledge influence rather than direct answer retrieval.

- **Open Question 3:** How do attention mechanisms interact with the procedural knowledge stored in feed-forward layers during reasoning?
  - Basis in paper: Explicit - The paper notes it only considers MLP parameters and treats attention as fixed, suggesting this as an interesting avenue for future work connecting to literature attributing reasoning operations to attention heads.
  - Why unresolved: The current analysis cannot capture how attention mechanisms might be coordinating or accessing the procedural knowledge that influence functions identify in feed-forward parameters.
  - What evidence would resolve it: Mechanistic interpretability studies combining influence analysis with attention pattern analysis, showing how specific attention heads route information from influential documents during reasoning traces, potentially revealing where procedural knowledge is applied during generation.

## Limitations

- The study relies on influence functions which are computationally expensive and may have approximation errors affecting interpretation of document influence patterns
- The analysis is based on a sample of 2.5 billion tokens from pretraining data, which may not be representative of the full pretraining corpus
- The qualitative analysis of top documents is inherently subjective and may miss subtle patterns
- The study focuses on mathematical reasoning tasks specifically, which may not generalize to other reasoning domains like logical or causal reasoning

## Confidence

- **High confidence:** The finding that reasoning queries rarely have their answers appear in top influential documents, while factual queries frequently do
- **Medium confidence:** The claim that code is strongly overrepresented in influential documents for reasoning tasks
- **Medium confidence:** The observation that documents influence reasoning queries within the same task similarly

## Next Checks

1. **Cross-task validation:** Apply the same influence analysis to a different set of reasoning tasks (e.g., logical puzzles, causal reasoning) to verify if the procedural knowledge pattern holds across reasoning domains beyond mathematical tasks.

2. **Temporal analysis:** Track how document influence patterns change as models are trained on different pretraining data distributions, particularly focusing on the impact of code data proportion on reasoning performance.

3. **Intervention study:** Create models with controlled pretraining data (varying amounts of code, procedural examples, factual data) and measure whether the observed influence patterns directly cause differences in reasoning generalization ability.