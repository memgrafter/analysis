---
ver: rpa2
title: 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI'
arxiv_id: '2401.01040'
source_url: https://arxiv.org/abs/2401.01040
tags:
- symbolic
- nsai
- systems
- neural
- neuro
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a survey of neuro-symbolic AI (NSAI) systems
  that integrate neural, symbolic, and probabilistic approaches to improve AI robustness,
  interpretability, and efficiency. It analyzes three NSAI workloads (LNN, LTN, NVSA)
  and finds symbolic components can dominate runtime (up to 92%), especially in sequential
  reasoning tasks.
---

# Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI

## Quick Facts
- arXiv ID: 2401.01040
- Source URL: https://arxiv.org/abs/2401.01040
- Reference count: 11
- This paper presents a survey of neuro-symbolic AI (NSAI) systems that integrate neural, symbolic, and probabilistic approaches to improve AI robustness, interpretability, and efficiency.

## Executive Summary
This paper presents a comprehensive survey of neuro-symbolic AI (NSAI) systems that integrate neural, symbolic, and probabilistic approaches to improve AI robustness, interpretability, and efficiency. The authors analyze three NSAI workloads (LNN, LTN, NVSA) and find that symbolic components can dominate runtime (up to 92%), especially in sequential reasoning tasks. Profiling reveals that NSAI workloads are highly heterogeneous with high sparsity, irregular access patterns, and data movement bottlenecks, making them poorly suited to current hardware optimized for dense DNNs. The paper outlines key research opportunities in developing efficient software frameworks, creating cognitive benchmarks, and designing architectures tailored to NSAI's diverse kernels and sparse patterns.

## Method Summary
The paper benchmarks and analyzes neuro-symbolic AI workloads to understand their computational characteristics and inform future architecture designs. The authors analyze three specific NSAI models (LNN on logic program task, LTN on binary classification task, and NVSA on Raven's Progressive Matrices task) using Intel Xeon Silver 4114 CPU and Nvidia RTX 2080 Ti GPU hardware. They employ function-level profiling with PyTorch Profiler to capture runtime statistics, categorize compute operators (convolution, MatMul, vector/element-wise, data transformation, data movement, others), and analyze neuro vs. symbolic workload contributions. The methodology involves running models through a profiling workflow that captures detailed CPU/GPU per-function measurements and operator-level categorization to identify bottlenecks and performance characteristics.

## Key Results
- Symbolic components can dominate runtime in NSAI workloads, particularly in sequential reasoning tasks
- NSAI workloads exhibit high heterogeneity in compute kernels, sparsity, and irregular access patterns
- Current hardware optimized for dense DNNs creates performance bottlenecks for NSAI workloads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic components can dominate runtime in NSAI workloads, particularly in sequential reasoning tasks.
- Mechanism: NSAI workloads combine neural perception layers with symbolic reasoning layers. While neural layers can be accelerated on GPUs, symbolic reasoning often involves sequential, compute-intensive rule detection that runs on CPUs, creating a bottleneck.
- Core assumption: Symbolic reasoning tasks are inherently sequential and computationally intensive compared to parallelizable neural operations.
- Evidence anchors:
  - [abstract] "profiling reveals NSAI workloads are heterogeneous, with high sparsity, irregular access patterns, and data movement bottlenecks, making them poorly suited to current hardware optimized for dense DNNs"
  - [section] "the symbolic workload dominates the NVSA's runtime, predominately due to the sequential and computational-intensive rule detection during the involved reasoning procedure"
  - [corpus] Weak evidence - corpus lacks specific runtime profiling data
- Break condition: If symbolic reasoning can be parallelized or if specialized hardware accelerates symbolic operations effectively

### Mechanism 2
- Claim: NSAI workloads exhibit high heterogeneity in compute kernels, sparsity, and irregular access patterns.
- Mechanism: Unlike traditional DNNs with regular matrix operations, NSAI combines neural networks, symbolic logic operations, and probabilistic reasoning. This creates diverse computational patterns including sparse syntax trees, irregular memory access, and varied dataflows.
- Core assumption: The combination of different AI paradigms inherently creates computational heterogeneity that existing hardware is not optimized for.
- Evidence anchors:
  - [abstract] "profiling reveals NSAI workloads are heterogeneous, with high sparsity, irregular access patterns, and data movement bottlenecks"
  - [section] "neuro-symbolic-probabilistic models feature much greater heterogeneity in compute kernels, sparsity, irregularity in access patterns, and higher memory intensity than current DNN workloads"
  - [corpus] Moderate evidence - corpus mentions heterogeneity but lacks detailed characterization
- Break condition: If hardware architectures can be designed to efficiently handle the diverse computational patterns without significant performance degradation

### Mechanism 3
- Claim: Current hardware optimization for dense DNNs creates performance bottlenecks for NSAI workloads.
- Mechanism: Modern AI accelerators are optimized for dense matrix operations and regular dataflows (e.g., systolic arrays). NSAI workloads require different optimization strategies due to their heterogeneous nature, sparse operations, and irregular memory access patterns.
- Core assumption: The divergence between hardware optimization strategies and NSAI computational requirements creates significant performance gaps.
- Evidence anchors:
  - [abstract] "making them poorly suited to current hardware optimized for dense DNNs"
  - [section] "current hardware roadmap that largely focuses on matrix multiplication or nearest neighbor search, and regular dataflows, e.g., systolic arrays"
  - [corpus] Limited evidence - corpus doesn't provide specific hardware optimization comparisons
- Break condition: If new hardware architectures can be developed that effectively bridge the gap between current optimization strategies and NSAI requirements

## Foundational Learning

- Concept: Workload characterization and profiling
  - Why needed here: Understanding NSAI performance characteristics requires analyzing runtime breakdowns, compute operators, and bottlenecks across different models
  - Quick check question: What are the primary compute operators in LNN, LTN, and NVSA models according to the profiling analysis?

- Concept: Hardware architecture optimization principles
  - Why needed here: Designing efficient NSAI systems requires understanding how current hardware limitations affect performance and what architectural changes are needed
  - Quick check question: Why do current AI accelerators optimized for dense DNNs struggle with NSAI workloads?

- Concept: Software framework design for heterogeneous workloads
  - Why needed here: NSAI systems need software frameworks that can efficiently handle the diverse computational patterns and logical capabilities
  - Quick check question: What are the key challenges in developing software frameworks for NSAI systems?

## Architecture Onboarding

- Component map:
  - Neural perception layer (GPU/accelerator optimized)
  - Symbolic reasoning engine (CPU/specialized hardware)
  - Probabilistic inference module (requires floating-point operations)
  - Memory hierarchy optimized for irregular access patterns
  - Interconnect fabric for heterogeneous compute units

- Critical path:
  - Neural perception → symbolic reasoning → probabilistic inference
  - Data movement between heterogeneous compute units
  - Sequential rule detection in symbolic reasoning

- Design tradeoffs:
  - Specialization vs. flexibility: Dedicated hardware for symbolic operations vs. general-purpose processors
  - Memory hierarchy: Balancing capacity for large models vs. bandwidth for irregular access patterns
  - Power efficiency: Optimizing for the dominant symbolic workload vs. balanced performance across all components

- Failure signatures:
  - Bottleneck in symbolic reasoning stage causing overall system slowdown
  - Memory bandwidth limitations due to irregular access patterns
  - Underutilization of neural processing units due to sequential dependencies

- First 3 experiments:
  1. Profile NSAI workloads on existing hardware to quantify the performance gap between neural and symbolic components
  2. Implement a simplified symbolic reasoning engine on FPGA to measure potential performance improvements
  3. Design and test a heterogeneous memory hierarchy optimized for irregular access patterns in NSAI workloads

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to unify neural, symbolic, and probabilistic components in a single framework for NSAI systems?
- Basis in paper: [explicit] The paper identifies the need for a unified framework to design algorithms that combine neural, symbolic, and probabilistic components opportunistically, and quantifies scaling laws for neuro-probabilistic-symbolic inference versus large neural models.
- Why unresolved: The integration of these three approaches remains a fundamental and open challenge in NSAI research. Existing attempts are still in a nascent manner, and there is no principled methodology for combining them effectively.
- What evidence would resolve it: Development and validation of a unified framework that demonstrates improved performance, scalability, and interpretability compared to existing NSAI systems. Empirical studies quantifying the trade-offs between different integration approaches.

### Open Question 2
- Question: How can software frameworks be designed to efficiently support a broad set of reasoning logical capabilities for NSAI systems?
- Basis in paper: [explicit] The paper highlights the need for new software frameworks that can encompass a broad set of reasoning logical capabilities, provide practical syntactic and semantic extensions, and be fast and memory-efficient.
- Why unresolved: Most current NSAI system implementations create custom software for deduction for the particular logic used, which limits modularity and extensibility. There is a lack of standardized frameworks that can support diverse logical reasoning capabilities.
- What evidence would resolve it: Development of a modular and extensible software framework that supports multiple logical reasoning capabilities, demonstrates improved performance and efficiency compared to custom implementations, and is widely adopted by the NSAI research community.

### Open Question 3
- Question: What are the most effective architectural designs for hardware accelerators to handle the heterogeneity and sparsity of NSAI workloads?
- Basis in paper: [explicit] The paper identifies the need for novel architectures with dedicated processing units, memory hierarchies, and on-chip interconnects to handle the additional complexities in computations and communications of NSAI workloads.
- Why unresolved: NSAI workloads feature much greater heterogeneity in compute kernels, sparsity, irregularity in access patterns, and higher memory intensity than current DNN workloads. Existing hardware accelerators are primarily optimized for dense DNN computations and may not be well-suited for NSAI workloads.
- What evidence would resolve it: Development and evaluation of hardware accelerator designs specifically tailored for NSAI workloads, demonstrating improved performance and energy efficiency compared to general-purpose processors or DNN accelerators. Architectural studies analyzing the trade-offs between different design choices.

## Limitations
- Lack of detailed runtime profiling data and quantitative measurements for the three analyzed workloads
- Insufficient empirical analysis to support claims about computational heterogeneity
- Limited evidence for specific quantitative claims about runtime dominance and bottleneck magnitudes

## Confidence

- High confidence in the conceptual framework and identified research opportunities
- Medium confidence in the characterization of computational heterogeneity
- Low confidence in specific quantitative claims about runtime dominance and bottleneck magnitudes

## Next Checks

1. Conduct comprehensive function-level profiling of LNN, LTN, and NVSA models on representative hardware to obtain actual runtime breakdowns between neural and symbolic components
2. Perform detailed computational kernel analysis to quantify the heterogeneity and irregularity in memory access patterns across different NSAI workloads
3. Design microbenchmarks to measure the performance gap between current hardware optimized for dense DNNs and specialized architectures for NSAI workloads, particularly focusing on symbolic reasoning operations