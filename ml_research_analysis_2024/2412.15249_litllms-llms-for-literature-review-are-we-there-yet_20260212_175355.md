---
ver: rpa2
title: 'LitLLMs, LLMs for Literature Review: Are we there yet?'
arxiv_id: '2412.15249'
source_url: https://arxiv.org/abs/2412.15249
tags:
- papers
- plan
- literature
- arxiv
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines the ability of Large Language Models (LLMs)
  to assist in generating literature reviews for scientific papers based on their
  abstracts. The task is decomposed into two components: retrieving relevant papers
  and writing the review.'
---

# LitLLMs, LLMs for Literature Review: Are we there yet?

## Quick Facts
- arXiv ID: 2412.15249
- Source URL: https://arxiv.org/abs/2412.15249
- Reference count: 40
- Key outcome: LLMs show promising potential for literature review writing when tasks are decomposed into retrieval and planning, with 50% improvement in normalized recall and 18-26% reduction in hallucinations.

## Executive Summary
This paper examines whether LLMs can effectively generate literature reviews for scientific papers based on abstracts. The task is decomposed into two components: retrieving relevant papers and writing the review. A novel two-step search strategy using LLM-generated keywords and external knowledge bases is introduced, along with a debate-based re-ranking mechanism with attribution that improves normalized recall by 50%. For generation, a plan-based approach is proposed that reduces hallucinations by 18-26% and improves ROUGE scores. Experiments on newly collected arXiv datasets show LLMs have promising potential for literature review writing when the task is properly decomposed.

## Method Summary
The paper proposes a two-stage pipeline for literature review generation. First, retrieval uses LLM-generated keywords from abstracts to query external knowledge bases (Semantic Scholar, Google Search), with an optional embedding-based retrieval method. Second, re-ranking uses a debate mechanism where the LLM generates arguments for and against including each candidate paper, with attribution verification to ensure extracted sentences exist in the abstracts. For generation, a plan-based approach generates an intermediate writing plan specifying which papers to cite at specific points, then uses this plan to guide the actual review generation, reducing hallucinations and improving coverage.

## Key Results
- Debate ranking with attribution improves normalized recall by 50% compared to naive search
- Plan-based generation reduces hallucinations by 18-26% compared to zero-shot approaches
- Coverage improves from 59-91% (non-plan) to 82-98% (plan-based) while maintaining high ROUGE scores

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can generate relevant keywords from paper abstracts that improve literature search retrieval rates
- **Mechanism:** An LLM analyzes the abstract and extracts key concepts, terms, and phrases that represent the core contributions and scope of the research. These keywords serve as search queries for academic databases and search engines.
- **Core assumption:** The abstract contains sufficient information for the LLM to identify meaningful search terms that would retrieve relevant papers.
- **Evidence anchors:** The paper demonstrates that using LLM-generated keywords from abstracts improves coverage compared to single-query approaches; coverage of just under 7% for ground truth papers indicates room for improvement; Section 3.2 describes the keyword extraction process and its implementation.
- **Break condition:** If abstracts are too vague, poorly written, or the research area is highly specialized with limited existing literature, the LLM may generate ineffective keywords.

### Mechanism 2
- **Claim:** Debate-based re-ranking with attribution improves the relevance of retrieved papers compared to simple ranking approaches
- **Mechanism:** The LLM generates arguments for and against including each candidate paper, extracts supporting sentences from abstracts, and produces a final probability score. The attribution verification step ensures extracted sentences actually exist in the candidate abstracts.
- **Core assumption:** The LLM can accurately assess relevance through argumentation and that attribution verification improves ranking quality.
- **Evidence anchors:** Section 3.3 describes the debate ranking approach with attribution; Figure 3 shows that removing attribution verification leads to significant drops in precision and normalized recall; the paper reports that debate ranking outperforms permutation ranking with higher precision and normalized recall.
- **Break condition:** If the LLM generates false arguments or hallucinates supporting sentences, the re-ranking could become less reliable even with attribution verification.

### Mechanism 3
- **Claim:** Plan-based generation reduces hallucinations and improves literature review quality compared to zero-shot generation
- **Mechanism:** The LLM first generates a structured writing plan that specifies which papers to cite at specific points in the review, then uses this plan to guide the actual generation. This intermediate representation provides control over content organization.
- **Core assumption:** Providing structural guidance through plans helps the LLM stay focused and reduces the likelihood of generating non-existent citations or unsupported claims.
- **Evidence anchors:** The paper reports that plan-based approaches reduce hallucinations by 18-26% compared to simpler generation methods; Table 6 shows that plan-based models achieve 82-98% coverage compared to 59-91% for non-plan approaches; Section 4 describes the plan-based generation approach and its evaluation.
- **Break condition:** If the plan generation itself is flawed or the LLM ignores the plan structure, the benefits may not materialize.

## Foundational Learning

- **Concept:** Literature review generation as a two-stage process (retrieval + generation)
  - **Why needed here:** Breaking down the complex task into smaller, more manageable components allows LLMs to perform better on each sub-task
  - **Quick check question:** Why does the paper decompose literature review generation into retrieval and generation phases instead of treating it as a single task?

- **Concept:** Zero-shot evaluation protocols for LLMs
  - **Why needed here:** To ensure fair evaluation of LLMs without test set contamination, especially as models are continuously updated with new training data
  - **Quick check question:** How does the paper's rolling evaluation protocol address the challenge of test set contamination?

- **Concept:** Normalized recall vs standard recall
  - **Why needed here:** Normalized recall provides a more meaningful evaluation of ranking quality by considering only the relevant papers that were actually retrieved
  - **Quick check question:** What's the difference between normalized recall@k and standard recall, and why is this distinction important for evaluating retrieval systems?

## Architecture Onboarding

- **Component map:** Abstract → Keyword Extraction → Search (Semantic Scholar/Google) → Re-ranking (Debate with Attribution) → Plan Generation → Literature Review Generation
- **Critical path:** Abstract → Keyword Extraction → Search → Re-ranking → Plan Generation → Final Output
- **Design tradeoffs:** Keyword-based search provides interpretable queries but may miss relevant papers; embedding-based search can find semantic matches but lacks transparency; plan-based generation improves control but adds complexity
- **Failure signatures:** Low coverage in initial search results, incomplete or repeated values in LLM re-ranking outputs, hallucinations in generated literature reviews
- **First 3 experiments:**
  1. Test keyword extraction on a sample abstract and verify the quality of generated keywords
  2. Compare search coverage using single vs multiple queries with different search engines
  3. Evaluate re-ranking quality with and without attribution verification on a small set of candidate papers

Note: The system uses multiple LLM calls at different stages, which can become expensive. The debate ranking approach requires n*k API calls where n is the number of query papers and k is the number of candidates, making it significantly more expensive than permutation ranking. The plan-based generation adds an additional LLM call but substantially improves output quality and reduces hallucinations.

## Open Questions the Paper Calls Out

- **Question:** Can the current retrieval pipeline be adapted to use longer sections of a manuscript (e.g., introduction, methodology) to improve retrieval accuracy without introducing bias from explicit citations?
- **Basis in paper:** The paper discusses that while including more of the paper could provide richer context and lead to higher retrieval rates, this would not align with their intended use case of supporting researchers in early stages when only limited, unpolished material is available. They also note that using additional sections could inadvertently allow the model to detect explicit citations or references, essentially "cheating" by using these as hints to retrieve specific papers.
- **Why unresolved:** The paper deliberately chose to focus on abstracts only to maintain a controlled setup that reflects a realistic, early-stage research scenario, but acknowledges this limitation and suggests future work could explore using additional sections.
- **What evidence would resolve it:** Experiments comparing retrieval accuracy using different manuscript sections (abstract only vs. introduction vs. full text) while measuring precision/recall and analyzing whether models are leveraging explicit citations as retrieval signals.

- **Question:** What is the optimal balance between retrieval coverage and computational cost when combining keyword-based search with embedding-based retrieval methods?
- **Basis in paper:** The paper shows that combining keyword-based and document-embedding-based search improves precision and recall during retrieval by 10% and 30%, respectively, compared to using either method in isolation, but does not explore the trade-offs in computational resources or the optimal weighting between these approaches.
- **Why unresolved:** While the paper demonstrates improved performance from combining methods, it does not investigate how to optimally allocate resources between keyword generation, multiple API queries, and embedding computations, or whether diminishing returns occur with increased computational investment.
- **What evidence would resolve it:** Systematic experiments varying the number of queries, embedding dimensions, and retrieval thresholds while measuring both performance metrics and computational costs (API calls, processing time, token usage).

- **Question:** How can hallucination rates be further reduced in plan-based generation while maintaining the flexibility and coverage of citations?
- **Basis in paper:** The paper shows that their planning-based approach reduces hallucinations by 18-26% compared to existing simpler LLM-based generation methods, but acknowledges that their approach does not completely eliminate hallucinations.
- **Why unresolved:** While the plan-based approach shows significant improvement, the paper does not explore whether additional strategies (such as stricter citation verification, multi-agent debate approaches, or hybrid extractive-abstractive methods) could further reduce hallucinations without sacrificing citation coverage or text quality.
- **What evidence would resolve it:** Comparative experiments testing various hallucination mitigation strategies (e.g., requiring explicit text extraction for every claim, using debate mechanisms during generation, or combining with extractive summarization) while measuring hallucination rates, citation coverage, and ROUGE scores.

## Limitations

- Retrieval coverage remains relatively low at under 7% for ground truth papers, suggesting the system may miss relevant literature even with the proposed two-step search strategy
- The debate re-ranking approach requires n*k LLM API calls where n is the number of query papers and k is the number of candidates, making it significantly more expensive than simpler approaches
- Evaluation relies on papers from a single source (arXiv) and timeframe (August/December 2023), which may limit generalizability across different research domains and publication patterns

## Confidence

**High Confidence:** The claim that plan-based generation reduces hallucinations by 18-26% compared to zero-shot approaches is well-supported by multiple evaluation metrics (ROUGE, BERTScore, human evaluation, coverage). The attribution verification mechanism's effectiveness in improving re-ranking quality is demonstrated through controlled experiments showing significant drops when attribution is removed.

**Medium Confidence:** The assertion that LLM-generated keywords improve literature search retrieval rates is supported by the proposed methodology but limited by the relatively low overall coverage (under 7%). The comparison between debate ranking and permutation ranking is methodologically sound but may not capture all aspects of ranking quality.

**Low Confidence:** Claims about the system's scalability and practical utility are not fully substantiated, as the paper does not provide detailed cost analysis for the multiple LLM calls required or extensive testing across diverse research domains.

## Next Checks

1. **Cross-domain validation:** Test the complete pipeline on literature reviews from different scientific domains (e.g., biology, computer science, social sciences) to assess generalizability beyond the arXiv computer science papers used in the study.

2. **Cost-benefit analysis:** Calculate the total cost of the debate re-ranking approach (n*k LLM calls) versus simpler alternatives, and determine the threshold at which the performance gains justify the additional computational expense.

3. **Long-term effectiveness:** Evaluate the system's performance over an extended period with papers from multiple years to assess whether the retrieval coverage improves as more literature becomes available in academic databases.