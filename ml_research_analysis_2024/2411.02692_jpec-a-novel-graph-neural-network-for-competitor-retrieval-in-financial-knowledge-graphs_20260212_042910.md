---
ver: rpa2
title: 'JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial Knowledge
  Graphs'
arxiv_id: '2411.02692'
source_url: https://arxiv.org/abs/2411.02692
tags:
- graph
- competitor
- knowledge
- embedding
- jpec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JPEC, a novel graph neural network designed
  to identify competitors in financial knowledge graphs. The model addresses challenges
  posed by sparse competitor annotations and complex graph structures (directed supply
  chains, undirected competitor links, node attributes).
---

# JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial Knowledge Graphs

## Quick Facts
- arXiv ID: 2411.02692
- Source URL: https://arxiv.org/abs/2411.02692
- Reference count: 14
- Primary result: JPEC achieves Hits@10 of 0.75 and MRR of 0.65 on regular tests, outperforming existing methods for competitor retrieval in financial knowledge graphs

## Executive Summary
This paper introduces JPEC, a novel graph neural network designed to identify competitors in financial knowledge graphs. The model addresses challenges posed by sparse competitor annotations and complex graph structures (directed supply chains, undirected competitor links, node attributes). JPEC combines first-order proximity via Laplacian Eigenmap for supervised competitor learning with second-order proximity using a directed GCN autoencoder for supply chain pattern extraction. Experiments on a large financial knowledge graph (133,812 nodes, 611,812 edges) show JPEC outperforming existing graph embedding methods and human queries.

## Method Summary
JPEC is a graph neural network that identifies competitors in financial knowledge graphs by leveraging both first-order proximity (direct competitor relationships) and second-order proximity (supply chain patterns). The model uses a directed GCN autoencoder to learn node representations from supply chain edges while simultaneously applying Laplacian Eigenmap to encode competitor relationships. These components are jointly optimized with a pairwise ranking loss and reconstruction loss. The approach is evaluated on a large financial knowledge graph with 133,812 company nodes and 611,812 edges, using Hits@10, MRR, and MAP metrics to measure competitor retrieval performance.

## Key Results
- JPEC achieves Hits@10 of 0.75 and MRR of 0.65 on regular test sets, outperforming existing graph embedding methods
- On zero-shot tests with unseen companies, JPEC maintains Hits@10 of 0.62 and MRR of 0.52, demonstrating strong generalization
- The model significantly outperforms human queries in competitor retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JPEC uses first-order proximity to encode direct competitor relationships into node embeddings.
- Mechanism: The model constructs a Laplacian matrix from competitor edges where weights are +1 for known competitors, -1 for non-competitors, and 0 otherwise. This forces competitor nodes to have similar embeddings while pushing non-competitors apart.
- Core assumption: The Laplacian eigenmap formulation assumes that preserving first-order proximity (direct connections) in the embedding space will capture meaningful competitor relationships.
- Evidence anchors:
  - [abstract] "JPEC combines first-order proximity via Laplacian Eigenmap for supervised competitor learning"
  - [section 2.2.1] "We applied Laplacian Eigenmap[1] to enforce the first proximity... Lpos = N∑i,j=1 w+i,j ||yi − yj || = 2tr (YT L+Y)"
  - [corpus] No direct evidence about Laplacian Eigenmap effectiveness for competitor detection found in related papers
- Break condition: If the labeled competitor edges are noisy or incorrect, the first-order proximity learning will propagate these errors into the embeddings.

### Mechanism 2
- Claim: JPEC uses second-order proximity via directed GCN autoencoder to capture supply chain patterns that indicate competition.
- Mechanism: The directed GCN learns node representations from supply chain edges while the autoencoder reconstructs node features, forcing the model to capture meaningful structural patterns that distinguish competitors.
- Core assumption: Supply chain relationships contain implicit signals about competitive relationships, and reconstructing node features forces the model to learn these patterns.
- Evidence anchors:
  - [abstract] "second-order proximity using a directed GCN autoencoder for supply chain pattern extraction"
  - [section 2.2.2] "we change the GCN's propagation function... to apply it into a directed supply-chain graph... The loss function for the second order proximity is to minimize the difference between the original node feature vectors and the reconstructed ones"
  - [corpus] No direct evidence about directed GCN autoencoders for competitor detection found in related papers
- Break condition: If supply chain patterns are not predictive of competition, the second-order proximity learning will not improve competitor detection.

### Mechanism 3
- Claim: The combination of first-order and second-order proximity with a pairwise ranking loss enables effective competitor retrieval.
- Mechanism: The model jointly optimizes a ranking loss on competitor/non-competitor pairs (first-order) and a reconstruction loss (second-order), balancing them with hyperparameters β and λ.
- Core assumption: Both direct competitor relationships and indirect supply chain patterns are necessary signals, and their combination through joint optimization improves performance.
- Evidence anchors:
  - [abstract] "JPEC combines first-order proximity via Laplacian Eigenmap... with second-order proximity using a directed GCN autoencoder"
  - [section 2.2.2] "The ultimate objective function of our model integrates the loss function derived from both the first-order and second-order proximity... L = L1st + βL2nd + λW2"
  - [section 4.2.2] "JPEC consistently outperforms other methods on both regular and zero-shot test datasets, demonstrating its effectiveness and superiority in competitor discovery"
- Break condition: If either proximity signal is noisy or irrelevant, the joint optimization may degrade performance rather than improve it.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: JPEC uses GCN-based architecture to learn node representations from graph structure
  - Quick check question: What is the key difference between message passing in GCN and GAT?

- Concept: Graph Embeddings and Proximity
  - Why needed here: The paper relies on first-order (direct connections) and second-order (structural similarity) proximity for competitor detection
  - Quick check question: How does first-order proximity differ from second-order proximity in graph embeddings?

- Concept: Autoencoders for Graph Representation Learning
  - Why needed here: JPEC uses a directed GCN autoencoder to reconstruct node features and capture structural patterns
  - Quick check question: What is the purpose of the decoder in a graph autoencoder?

## Architecture Onboarding

- Component map:
  Input layer (node features X and graph structure S, C) -> Directed GCN encoder -> Directed GCN decoder -> Laplacian Eigenmap modules (positive/negative) -> Pairwise ranking loss + Reconstruction loss -> Output layer (competitor prediction scores)

- Critical path:
  1. Load node features and graph structure
  2. Apply directed GCN to generate embeddings
  3. Apply Laplacian Eigenmap on competitor edges
  4. Compute pairwise ranking loss
  5. Compute reconstruction loss
  6. Combine losses with hyperparameters
  7. Backpropagate and update model

- Design tradeoffs:
  - Directed vs undirected GCN: Directionality captures asymmetric supply chain relationships but adds complexity
  - Separate positive/negative Laplacian Eigenmaps: Allows different treatment of competitor vs non-competitor edges but doubles parameters
  - Joint optimization: Balances multiple objectives but requires careful hyperparameter tuning

- Failure signatures:
  - Poor Hits@10 but good MRR: Model finds correct competitors but ranks them low
  - Good Hits@10 but poor MRR: Model finds some competitors but misses many others
  - Poor performance on zero-shot test: Model overfits to seen nodes and doesn't generalize
  - Performance drops when β changes: Incorrect balance between first and second-order signals

- First 3 experiments:
  1. Test directed GCN vs undirected GCN on a small subset to verify directional information helps
  2. Test different β values (e.g., 0.1, 1, 10) to find optimal balance between proximity signals
  3. Test zero-shot performance on a validation set with 10% held-out competitors to check generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the negative sampling methodology be improved to reduce the introduction of errors from falsely labeled non-competitors in the knowledge graph?
- Basis in paper: [explicit] The paper states that randomly designated unconnected nodes as non-competitors may introduce errors since some sampled nodes may be competitors with missing COMPETE_WITH edges, and they are dedicated to refining the negative sampling methodology to mitigate such issues.
- Why unresolved: The current negative sampling approach is acknowledged to be error-prone, and while the authors intend to refine it, they do not provide details on how this refinement would be implemented or what specific improvements would be made.
- What evidence would resolve it: Evidence showing a refined negative sampling method that significantly reduces false negatives in the test results, such as improved MRR and MAP scores, would demonstrate the effectiveness of the improved methodology.

### Open Question 2
- Question: What is the impact of incorporating additional types of relationships or attributes into the JPEC model for competitor retrieval?
- Basis in paper: [inferred] The paper discusses the use of supply chain connections and competitor relationships, as well as node attributes, but does not explore the inclusion of other types of relationships or attributes that might be present in the financial knowledge graph.
- Why unresolved: The current model focuses on a specific set of relationships and attributes, and the paper does not investigate how the model's performance would change with the inclusion of additional information.
- What evidence would resolve it: Experimental results comparing the performance of JPEC with and without additional types of relationships or attributes would show the impact of such inclusion on the model's ability to retrieve competitors.

### Open Question 3
- Question: How does the performance of JPEC scale with larger knowledge graphs or different types of financial entities?
- Basis in paper: [explicit] The paper mentions a large financial knowledge graph with 133,812 nodes and 611,812 edges, but does not discuss the model's scalability or performance on graphs of different sizes or with different types of financial entities.
- Why unresolved: The paper provides results for a specific dataset but does not address whether the model would maintain its performance with larger or differently structured graphs.
- What evidence would resolve it: Results from experiments conducted on larger or differently structured financial knowledge graphs would indicate how well JPEC scales and performs under varying conditions.

## Limitations

- The model's performance depends heavily on the quality of labeled competitor data, with errors in negative sampling potentially degrading results
- The assumption that supply chain relationships are predictive of competition may not hold universally across all industries or market conditions
- The paper lacks extensive ablation studies to quantify the individual contributions of first-order versus second-order proximity learning components

## Confidence

- Mechanism 1 (Laplacian Eigenmap for first-order proximity): Medium - experimental results support effectiveness but lack comparison to alternative methods
- Mechanism 2 (Directed GCN autoencoder for second-order proximity): Medium - results show improvement but no ablation study isolates this component's contribution
- Mechanism 3 (Joint optimization of multiple proximity signals): Medium - combined approach outperforms baselines but individual component contributions are unclear

## Next Checks

1. Perform ablation studies by training JPEC variants with only first-order proximity, only second-order proximity, and random initialization to quantify the contribution of each component
2. Test model performance on datasets with varying levels of label noise in competitor relationships to assess robustness to annotation quality
3. Evaluate whether alternative graph embedding methods (e.g., GraphSAGE, GAT) with similar loss functions can achieve comparable performance to isolate the contribution of the specific architectural choices