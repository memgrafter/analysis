---
ver: rpa2
title: Theoretical Analysis of Hierarchical Language Recognition and Generation by
  Transformers without Positional Encoding
arxiv_id: '2410.12413'
source_url: https://arxiv.org/abs/2410.12413
tags:
- layer
- language
- dmodel
- attention
- normalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides constructive theoretical proofs that Transformers
  without positional encoding can efficiently recognize and generate hierarchical
  languages like Dyckk and Shuffle-Dyckk, using only causal masking and a starting
  token. The authors show that Transformers can compute positional information and
  depth within hierarchical structures through architectural design.
---

# Theoretical Analysis of Hierarchical Language Recognition and Generation by Transformers without Positional Encoding

## Quick Facts
- arXiv ID: 2410.12413
- Source URL: https://arxiv.org/abs/2410.12413
- Authors: Daichi Hayakawa; Issei Sato
- Reference count: 40
- This paper provides constructive theoretical proofs that Transformers without positional encoding can efficiently recognize and generate hierarchical languages like Dyckk and Shuffle-Dyckk using only causal masking and a starting token.

## Executive Summary
This paper presents theoretical proofs demonstrating that Transformers without explicit positional encoding can efficiently recognize and generate hierarchical languages like Dyckk and Shuffle-Dyckk. The authors show that through careful architectural design, specifically using RMS layer normalization positioned after the first linear transformation in the feed-forward network, Transformers can compute positional information and depth within hierarchical structures using only causal masking and a starting token. The theoretical results indicate that O(log k)-width Transformers can generate Dyckk and recognize both Dyckk and Shuffle-Dyckk languages, while O(k)-width Transformers can generate Shuffle-Dyckk.

## Method Summary
The paper investigates whether Transformers without positional encoding can recognize and generate hierarchical languages through constructive theoretical proofs. The authors design specific Transformer architectures with RMS layer normalization (FFN-LN position) and causal masking to prove computational capabilities for Dyckk and Shuffle-Dyckk languages. They generate synthetic datasets using specific probability distributions and train 124M parameter models from scratch. The experiments compare four model configurations: with/without positional encoding and with/without starting token. The paper also validates their architectural modifications on natural language datasets (WikiText-103 and OpenWebText) by comparing four layer normalization positions: Post-LN, Pre-LN, No-LN, and FFN-LN.

## Key Results
- O(log k)-width Transformers can generate Dyckk language and recognize both Dyckk and Shuffle-Dyckk languages using only causal masking and a starting token
- O(k)-width Transformers can generate Shuffle-Dyckk language
- Transformers without positional encoding show better generalization to longer sequences compared to those with positional encoding on synthetic hierarchical languages
- The modified architecture with RMS layer normalization (FFN-LN) performs competitively with standard Transformer architectures on natural language tasks

## Why This Works (Mechanism)
The paper demonstrates that Transformers can compute positional information and depth within hierarchical structures through architectural design. The key mechanism involves using RMS layer normalization positioned after the first linear transformation in the feed-forward network, which enables the network to track nesting depth and relative positions within hierarchical structures. The causal masking combined with the starting token allows the model to maintain a stack-like representation of open brackets and their positions, effectively computing the necessary positional information without explicit positional encoding.

## Foundational Learning

**Dyckk language**: A context-free language representing properly nested brackets with k types. Why needed: Serves as the primary test case for hierarchical structure recognition. Quick check: Verify that strings like "[({})]" are valid while "[({)]}" are invalid.

**Shuffle-Dyckk language**: A union of multiple Dyckk languages interleaved together. Why needed: Tests more complex hierarchical reasoning across multiple bracket types. Quick check: Ensure the model can distinguish between valid shuffles like "[({})]()" and invalid ones like "[({)]()".

**RMS layer normalization**: A normalization technique that normalizes using root mean square values. Why needed: Critical architectural component that enables positional tracking without explicit positional encoding. Quick check: Confirm the normalization occurs after the first linear transformation in the feed-forward network.

## Architecture Onboarding

**Component map**: Input -> RMS Layer Norm -> Linear -> Activation -> RMS Layer Norm -> Linear -> Output -> Causal Masking

**Critical path**: The feed-forward network with RMS layer normalization positioned after the first linear transformation is the critical component that enables the theoretical results. This specific positioning allows the network to maintain and update positional information and depth tracking.

**Design tradeoffs**: The paper trades standard Post-LN or Pre-LN normalization for RMS layer normalization in a specific position, which may reduce compatibility with existing Transformer implementations but provides the theoretical guarantees. The approach also requires careful probability distribution design for generating training data.

**Failure signatures**: Models with positional encoding may perform well on in-distribution data but show significant performance degradation on out-of-distribution data, indicating that positional encoding harms generalization to longer sequences. Models without starting token may show slightly worse performance but still demonstrate hierarchical structure learning.

**Three first experiments**:
1. Implement the Transformer architecture with RMS layer normalization (FFN-LN position) and generate Dyckk datasets using q=0.5, r=0.9, π=1/k parameters.
2. Train 124M parameter models on the generated datasets with maximum sequence length 700, comparing PE+BOS, PE+NoBOS, NoPE+BOS, NoPE+NoBOS configurations using Accclosed metric.
3. Implement the four layer normalization positions and train 124M models on WikiText-103 to compare perplexity scores across different normalization schemes.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical proofs rely on specific architectural assumptions, particularly the RMS layer normalization positioned after the first linear transformation in the feed-forward network
- The experimental validation focuses on specific parameter settings (q=0.5, r=0.9 for Dyckk; q=0.3, r=0.97 for Shuffle-Dyckk) that may not generalize to all hierarchical language variants
- The proofs assume idealized conditions that may not fully capture practical training dynamics including finite precision, optimization algorithms, and hyperparameter tuning effects

## Confidence
- High Confidence: The theoretical proof that O(log k)-width Transformers can recognize Dyckk and Shuffle-Dyckk languages, and generate Dyckk language using only causal masking and a starting token
- Medium Confidence: The claim that O(k)-width Transformers can generate Shuffle-Dyckk language, as this requires more complex positional tracking and may be sensitive to specific probability distributions
- Medium Confidence: The assertion that RMS layer normalization (FFN-LN) provides competitive performance with standard Transformer architectures on natural language tasks

## Next Checks
1. **Architectural Robustness Check**: Reproduce the theoretical results with standard Post-LN and Pre-LN Transformer architectures to determine whether the RMS layer normalization positioning is essential for the theoretical guarantees, or if the results hold more generally across normalization schemes.

2. **Distribution Sensitivity Analysis**: Systematically vary the probability parameters (q, r, π) for Dyckk and Shuffle-Dyckk languages across a wider range to test the robustness of the theoretical results and identify the boundaries where the O(log k) and O(k) width requirements change.

3. **Extended Generalization Benchmark**: Design experiments that explicitly test the out-of-distribution performance gap between Transformers with and without positional encoding across multiple sequence length scales (e.g., training on lengths up to 512, testing on lengths 1024, 2048, 4096) to quantify the generalization advantage claimed in the paper.