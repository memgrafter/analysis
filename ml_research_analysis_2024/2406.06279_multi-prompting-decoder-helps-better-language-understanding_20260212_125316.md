---
ver: rpa2
title: Multi-Prompting Decoder Helps Better Language Understanding
arxiv_id: '2406.06279'
source_url: https://arxiv.org/abs/2406.06279
tags:
- prompts
- class
- prompt
- scores
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multi-prompting decoder framework for adapting
  pre-trained language models in a model-as-a-service setting. The key innovation
  is to query the model with multiple different prompts per sample, then decode the
  resulting hidden states and class scores using optimal transport and calibrated
  averaging.
---

# Multi-Prompting Decoder Helps Better Language Understanding

## Quick Facts
- arXiv ID: 2406.06279
- Source URL: https://arxiv.org/abs/2406.06279
- Authors: Zifeng Cheng; Zhaoling Chen; Zhiwei Jiang; Yafeng Yin; Cong Wang; Shiping Ge; Qing Gu
- Reference count: 40
- Primary result: Multi-prompting decoder achieves SOTA on 9 NLU datasets under few-shot conditions

## Executive Summary
This paper proposes a multi-prompting decoder framework for adapting pre-trained language models in a model-as-a-service setting. The key innovation is querying the model with multiple different prompts per sample, then decoding the resulting hidden states and class scores using optimal transport and calibrated averaging. This approach addresses limitations of single-prompt methods by reducing reliance on prompt quality, alleviating data scarcity in few-shot settings, and extracting richer knowledge. The method achieves state-of-the-art results across nine natural language understanding datasets under few-shot conditions.

## Method Summary
The multi-prompting decoder framework queries a PLM API with multiple prompts per sample to obtain hidden states and class scores. Hidden states are projected through a linear layer and matched to class prototypes using optimal transport, while class scores are calibrated and averaged across prompts. The framework combines these two decoding strategies using a weighted sum to produce final predictions. Training involves optimizing a small set of parameters: the linear projection layer and class prototypes, making it efficient compared to full fine-tuning.

## Key Results
- Achieves state-of-the-art accuracy on 9 NLU datasets under few-shot conditions (1, 4, 16 shots)
- Outperforms baselines like PromptBoosting and DecT across sentiment analysis, topic classification, and NLI tasks
- Requires only 3 model queries per sample and minimal trainable parameters (linear layer + prototypes)
- Shows consistent improvement with 2-3 prompts, with diminishing returns beyond that point

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-prompting reduces reliance on any single prompt's quality by aggregating diverse information sources
- Mechanism: The model queries PLM with multiple different prompts per sample, generating multiple hidden states and class scores. These are then decoded using optimal transport for hidden states and calibrated averaging for class scores, creating a more robust representation that doesn't depend on any single prompt being optimal.
- Core assumption: Different prompts provide complementary information that can be effectively combined through optimal transport and calibrated averaging
- Evidence anchors: [abstract] "Such multi-prompting decoding paradigm can simultaneously mitigate reliance on the quality of a single prompt"; [section 1] "The core idea is to query PLMs with multiple different prompts for each sample"

### Mechanism 2
- Claim: Multiple prompts address data scarcity in few-shot settings by creating multiple representations per sample
- Mechanism: Each training sample generates multiple representations through different prompts. This effectively multiplies the available training data without requiring additional samples, helping the model learn more robust patterns even when few examples are available per class.
- Core assumption: Different prompts can transform the same input into meaningfully different representations that capture different aspects of the data
- Evidence anchors: [abstract] "in the few-shot setting, using multiple prompts enables a single sample to obtain multiple representations, thereby addressing the issue of data scarcity"

### Mechanism 3
- Claim: Optimal transport effectively matches multiple prompt representations to class prototypes for classification
- Mechanism: The model establishes multiple prototypes per class and uses optimal transport to find the best matching between sample representations (from different prompts) and these prototypes. This creates a fine-grained matching flow that captures the relationships between representations and class characteristics specific to each prompt type.
- Core assumption: Optimal transport can effectively handle the multi-to-multi matching problem between multiple representations and multiple prototypes per class
- Evidence anchors: [section 4.1.2] "OT aims to find the optimal transport plan T ∈ RP ×Q that represents the fine-grained matching flow between text representations and class prototypes"

## Foundational Learning

- Concept: Optimal Transport Theory
  - Why needed here: Provides the mathematical framework for matching multiple representations to multiple prototypes in a way that minimizes overall cost while respecting probability constraints
  - Quick check question: What is the key difference between using optimal transport versus simple weighted averaging for combining multiple representations?

- Concept: Calibration of Language Model Predictions
  - Why needed here: Addresses the inherent bias in PLM predictions where common tokens are overpredicted, allowing for more accurate class score aggregation across prompts
  - Quick check question: How does the calibration process use empty input predictions to adjust context-dependent predictions?

- Concept: Few-Shot Learning Challenges
  - Why needed here: Understanding why standard fine-tuning approaches fail with limited data motivates the need for prompt-based adaptation and data augmentation through multiple prompts
  - Quick check question: What are the three main advantages of using multiple prompts in few-shot settings according to the paper?

## Architecture Onboarding

- Component map: Template → PLM Query → Linear Projection → Optimal Transport Matching → Classification
- Critical path: Template wrapping and multiple prompt generation → PLM API black-box queries → Linear layer projection → Optimal transport matching with prototypes → Combined score computation
- Design tradeoffs:
  - Number of prompts (P): More prompts increase robustness but also computational cost and query overhead
  - Number of prototypes (Q): More prototypes can capture finer distinctions but increase parameter count and optimization complexity
  - β hyperparameter: Controls balance between model knowledge (OT scores) and prior knowledge (calibrated scores)
- Failure signatures:
  - Poor performance with single high-quality prompt: Indicates OT matching isn't working effectively
  - Minimal improvement with additional prompts: Suggests prompts are too similar or OT can't leverage diversity
  - High variance across runs: May indicate instability in transport plan optimization or prototype learning
- First 3 experiments:
  1. Compare single-prompt baseline (DecT) vs two-prompt MPD on SST2 to verify improvement
  2. Test different numbers of prompts (1, 2, 3, 4) on AG's News to find optimal prompt count
  3. Validate OT effectiveness by comparing against uniform distribution and cosine similarity baselines on hidden state decoding

## Open Questions the Paper Calls Out

The paper acknowledges several limitations and open questions:
- The framework's effectiveness on non-English datasets is unknown, as all experiments were conducted on English datasets
- The performance on datasets with very large numbers of classes (e.g., 100+) compared to the 2-14 classes tested remains unexplored
- The impact of different optimal transport algorithms (e.g., Sinkhorn vs. exact OT) on performance and efficiency is not investigated
- The scalability of MPD with larger numbers of prompts beyond the three used in experiments is unclear

## Limitations

- Requires access to PLM API with hidden state outputs, which may not be available for all model-as-a-service offerings
- Performance heavily depends on the quality and diversity of prompt templates, with limited guidance provided on template design
- Computational overhead scales linearly with the number of prompts, potentially limiting real-time applications
- Calibration process requires context-independent predictions that may not be uniformly available across all PLM APIs

## Confidence

- **High Confidence**: The core multi-prompting concept and its benefits for reducing prompt quality dependency
- **Medium Confidence**: The optimal transport mechanism for matching representations to prototypes
- **Medium Confidence**: The calibration methodology for combining class scores across prompts
- **Low Confidence**: The generalizability of the specific prompt templates and label words across different PLM architectures

## Next Checks

1. **Template Robustness Test**: Systematically vary the number and quality of prompt templates (1, 2, 3, 4, 5 prompts) on SST2 and AG's News to quantify the marginal benefit of additional prompts and identify the optimal prompt count for different task complexities.

2. **OT Parameter Sensitivity**: Conduct comprehensive ablation studies on optimal transport parameters including λ regularization strength, threshold convergence criteria, and number of prototypes per class to determine which factors most significantly impact performance and where the method is most sensitive.

3. **Cross-PLM Generalization**: Test the framework using prompt templates optimized for RoBERTa when querying a T5 model (and vice versa) to evaluate whether the multi-prompting benefits transfer across different PLM architectures or if template design is model-specific.