---
ver: rpa2
title: Probabilistic Medical Predictions of Large Language Models
arxiv_id: '2408.11316'
source_url: https://arxiv.org/abs/2408.11316
tags:
- probability
- llms
- explicit
- implicit
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study systematically compares two methods of extracting prediction
  probabilities from large language models (LLMs) in clinical tasks: explicit probabilities
  (generated in text) and implicit probabilities (derived from token likelihood).
  Across six advanced open-source LLMs and five medical datasets, implicit probabilities
  consistently outperformed explicit probabilities in AUROC and AUPRC, with statistically
  significant differences.'
---

# Probabilistic Medical Predictions of Large Language Models

## Quick Facts
- arXiv ID: 2408.11316
- Source URL: https://arxiv.org/abs/2408.11316
- Reference count: 0
- Key outcome: Implicit probability methods consistently outperformed explicit probability methods in clinical AUROC and AUPRC metrics

## Executive Summary
This study systematically compares two methods for extracting prediction probabilities from large language models (LLMs) in clinical tasks: explicit probabilities generated in text and implicit probabilities derived from token likelihood. Across six advanced open-source LLMs and five medical datasets, implicit probabilities consistently outperformed explicit probabilities in both AUROC and AUPRC metrics, with statistically significant differences. The performance gap was more pronounced in smaller LLMs and imbalanced datasets, highlighting the need for caution when using text-generated probabilities in clinical applications. The findings suggest that implicit probability methods may provide more reliable probabilistic predictions for medical decision support systems.

## Method Summary
The study compared explicit and implicit probability extraction methods across six open-source LLMs (Qwen2, Llama-3.1, Gemma-2, Mistral-Large, Yi-1.5, and Phi-3) using five medical datasets (MMLU-CK, MMLU-CM, USMLE, MCMLE, and MGB-SDoH). Multiple-choice questions were converted to binary classification format, and models generated responses with explicit probabilities extracted via regular expressions from the generated text. Implicit probabilities were calculated from token likelihood scores obtained during generation. The performance of both methods was evaluated using AUROC and AUPRC metrics, with validation scripts ensuring extracted content matched predictions and handling edge cases like missing probabilities.

## Key Results
- Implicit probability methods consistently outperformed explicit probability methods across all datasets and models
- The performance gap was statistically significant (p < 0.05) in most comparisons
- Smaller LLMs showed larger performance differences between methods compared to larger models
- Imbalanced datasets amplified the reliability issues with explicit probability generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit probability derived from token likelihood is more reliable than explicit probability generated in text.
- Mechanism: Token likelihood calculation uses the raw log probability of the model's output tokens, which is less susceptible to numerical reasoning errors than text-based probability generation.
- Core assumption: LLMs have inherent limitations in numerical reasoning when generating probability values as text.
- Evidence anchors: [abstract] "explicit probabilities consistently underperformed implicit probabilities in AUROC and AUPRC"; [section] "LLMs' limitations in numerical reasoning raise concerns about the reliability of these text-generated probabilities"

### Mechanism 2
- Claim: The performance gap between explicit and implicit probabilities widens with smaller LLMs and imbalanced datasets.
- Mechanism: Smaller models have fewer parameters and less refined probability calibration, making text-generated probabilities more error-prone. Imbalanced datasets amplify these errors because the model's probability estimates are more sensitive to miscalibration.
- Core assumption: Model size and dataset balance directly affect the reliability of text-based probability generation.
- Evidence anchors: [abstract] "This performance gap was more pronounced in smaller LLMs and imbalanced datasets"; [section] "we found that the performance of explicit probabilities was consistently lower than implicit probabilities... especially in the case of small LLMs and imbalanced datasets"

### Mechanism 3
- Claim: Implicit probability provides a finer-grained probability distribution than explicit probability.
- Mechanism: Implicit probability is calculated from the transition scores of each token, resulting in a continuous probability distribution. Explicit probability is typically rounded to the nearest tenth (e.g., 90%, 80%), creating a coarse-grained distribution.
- Core assumption: The granularity of the probability distribution affects the model's ability to discriminate between classes.
- Evidence anchors: [abstract] "explicit probabilities consistently underperformed implicit probabilities in AUROC and AUPRC"; [section] "we noticed that the implicit probability has wider distribution than the explicit probability, which is consistent with what we found on the AUROC and AUPRC curves"

## Foundational Learning

- Concept: Understanding the difference between explicit and implicit probability in LLMs
  - Why needed here: This is the core distinction being evaluated in the study, and understanding it is crucial for interpreting the results.
  - Quick check question: What is the key difference between explicit and implicit probability in LLMs, and why does this difference matter for medical applications?

- Concept: Knowledge of evaluation metrics for probabilistic predictions (AUROC, AUPRC)
  - Why needed here: These metrics are used to compare the performance of explicit and implicit probabilities, and understanding them is essential for interpreting the results.
  - Quick check question: How do AUROC and AUPRC differ, and why is AUPRC particularly important for imbalanced datasets?

- Concept: Familiarity with prompt engineering techniques in LLMs
  - Why needed here: The study uses prompt engineering to elicit probability estimates from LLMs, and understanding these techniques is important for replicating the study or applying the findings.
  - Quick check question: What are some common prompt engineering techniques used to elicit probability estimates from LLMs, and how do they affect the reliability of the estimates?

## Architecture Onboarding

- Component map: Datasets (MMLU-CK, MMLU-CM, USMLE, MCMLE, MGB-SDoH) -> LLMs (Qwen2, Llama-3.1, Gemma-2, Mistral-Large, Yi-1.5, Phi-3) -> Probability extraction (explicit via regex, implicit via token likelihood) -> Evaluation (AUROC, AUPRC)

- Critical path: Load LLM and tokenizer -> Generate predictions with max_new_tokens=64, return_dict_in_generate=True, output_scores=True -> Extract explicit probabilities using regex on generated text -> Calculate implicit probabilities from token scores -> Compute AUROC/AUPRC using reference labels

- Design tradeoffs: The study uses binary classification tasks to facilitate implicit probability extraction and metric calculation, which simplifies analysis but may limit generalizability to other task types. Open-source LLMs were chosen over proprietary models, potentially affecting performance characteristics.

- Failure signatures: Common failure modes include LLM instruction adherence failures (e.g., not following output format), probability extraction errors (e.g., incorrect identification of prediction token), and evaluation metric calculation errors (e.g., incorrect handling of NaN values). The study uses validation scripts to detect and handle these failures.

- First 3 experiments:
  1. Replicate the main experiment using a different set of medical datasets to validate the generalizability of the findings.
  2. Investigate the effect of different prompt engineering techniques on the reliability of explicit probability estimates.
  3. Explore the use of model calibration techniques to improve the reliability of explicit probability estimates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-derived probabilities perform in multi-class classification tasks compared to binary tasks?
- Basis in paper: [inferred] The study focused on binary classification tasks, but the authors note that probability differences may vary in other task types like multi-choice questions or information extraction.
- Why unresolved: The study explicitly simplified tasks to binary format to facilitate implicit probability extraction and metric calculation, leaving performance in more complex classification scenarios unexplored.
- What evidence would resolve it: Systematic comparison of explicit vs. implicit probabilities across diverse multi-class medical classification tasks using established benchmarks and appropriate metrics (e.g., multi-class AUC, Brier score).

### Open Question 2
- Question: Can fine-tuning LLMs to generate more reliable explicit probabilities improve their clinical utility?
- Basis in paper: [explicit] The authors suggest that fine-tuning LLMs' explicit probability output with supervision from implicit probability could guide models to generate more accurate probabilities, similar to improving implicit chain of thought capability.
- Why unresolved: This represents a proposed future direction rather than tested methodology in the current study.
- What evidence would resolve it: Experimental validation comparing fine-tuned LLMs with original models across multiple clinical datasets, measuring improvements in both probability reliability metrics and downstream clinical decision performance.

### Open Question 3
- Question: Can explicit probability serve as a reliable indicator for LLM hallucination detection in clinical applications?
- Basis in paper: [explicit] The authors propose that predictions with low explicit probability may signal higher likelihood of hallucination, noting that LLMs lacking sufficient knowledge often generate hallucinated information with low probability outputs.
- Why unresolved: The study identifies this as a future research direction but does not empirically test the correlation between low explicit probability and hallucination rates.
- What evidence would resolve it: Validation studies correlating explicit probability values with verified hallucination rates across clinical question-answering tasks, ideally with human expert annotation of hallucinated content.

## Limitations

- Findings based on open-source LLMs may not generalize to proprietary models like GPT-4 or Claude
- Study focuses on binary classification tasks, limiting applicability to more complex clinical decision scenarios
- Explicit probability extraction reliability depends heavily on prompt engineering approach

## Confidence

**High Confidence:**
- The consistent performance difference between explicit and implicit probabilities across multiple datasets and models
- The negative impact of dataset imbalance on explicit probability reliability
- The correlation between model size and performance gap magnitude

**Medium Confidence:**
- The generalizability of findings to other LLM architectures beyond the six tested models
- The assumption that token likelihood-based probabilities are always superior for clinical applications
- The specific threshold where explicit probabilities become unreliable (e.g., below 50% probability generation)

## Next Checks

1. **Cross-Model Validation**: Test the same methodology on proprietary LLMs (GPT-4, Claude, Gemini) to determine if the performance gap persists across different model architectures and training approaches.

2. **Multi-Class Extension**: Adapt the evaluation framework to handle multi-class medical predictions to assess whether the explicit/implicit probability gap widens or narrows with increased classification complexity.

3. **Prompt Engineering Optimization**: Systematically vary prompt formulations and explicit probability elicitation strategies to determine if improved text-based probability generation can close the performance gap with implicit methods.