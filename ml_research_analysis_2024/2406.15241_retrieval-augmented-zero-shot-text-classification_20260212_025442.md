---
ver: rpa2
title: Retrieval Augmented Zero-Shot Text Classification
arxiv_id: '2406.15241'
source_url: https://arxiv.org/abs/2406.15241
tags:
- qzero
- query
- classi
- ication
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QZero, a retrieval-augmented query reformulation
  approach for improving zero-shot text classification. The core idea is to enrich
  input queries by retrieving supporting Wikipedia categories, which are then reformulated
  based on the type of embedding model (contextual or static).
---

# Retrieval Augmented Zero-Shot Text Classification

## Quick Facts
- arXiv ID: 2406.15241
- Source URL: https://arxiv.org/abs/2406.15241
- Reference count: 39
- This paper introduces QZero, a retrieval-augmented query reformulation approach for improving zero-shot text classification.

## Executive Summary
This paper introduces QZero, a retrieval-augmented query reformulation approach for improving zero-shot text classification. The core idea is to enrich input queries by retrieving supporting Wikipedia categories, which are then reformulated based on the type of embedding model (contextual or static). Experiments across six diverse datasets demonstrate that QZero consistently enhances classification accuracy for both small and large embedding models, often enabling smaller models to achieve performance comparable to larger ones. Notably, in news and medical classification tasks, QZero improves even the largest OpenAI embedding model by at least 5% and 3%, respectively. The method also provides interpretable insights into query context and relevance. Overall, QZero offers a training-free, resource-efficient solution for zero-shot classification, particularly valuable in domains with evolving information or limited computational resources.

## Method Summary
QZero operates by first retrieving the top 50 relevant Wikipedia categories for a given query. These categories are then reformulated based on the type of embedding model: for contextual models, the categories are concatenated into a single text string (limited to 512 tokens), while for static models, keywords are extracted and weighted based on frequency before concatenation. The reformulated query is then embedded and compared to class label embeddings using cosine similarity to determine the predicted class. The method is designed to work with both dense (e.g., Contriever) and sparse (e.g., BM25) retrievers and adapts to different embedding types without requiring retraining.

## Key Results
- QZero consistently improves classification accuracy across six diverse datasets for both small and large embedding models.
- Smaller embedding models achieve performance comparable to larger models when augmented with retrieved knowledge.
- In news and medical classification tasks, QZero improves even the largest OpenAI embedding model by at least 5% and 3%, respectively.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query reformulation with retrieved categories improves embedding quality by expanding the contextual vocabulary available to the model.
- Mechanism: The retrieval system finds relevant Wikipedia categories for a query, which are then concatenated (for contextual embeddings) or keyword-extracted (for static embeddings) and used as a reformulated input. This reformulated input contains richer, more specific information that can guide the embedding model toward more accurate representations.
- Core assumption: The retrieved categories are relevant and informative enough to meaningfully enrich the query context without introducing excessive noise.
- Evidence anchors:
  - [abstract] "QZero enhances performance for state-of-the-art static and contextual embedding models without the need for retraining."
  - [section] "The embedding of the reformulated query is compared to the embeddings of the potential classes using cosine similarity."
  - [corpus] Corpus signals show related retrieval augmentation and embedding work, suggesting contextual alignment.
- Break condition: If retrieved categories are irrelevant or noisy, the reformulated query may mislead the embedding model, degrading classification performance.

### Mechanism 2
- Claim: Smaller embedding models can achieve performance comparable to larger models when augmented with retrieved knowledge, due to the effective contextual expansion provided by the reformulation.
- Mechanism: The reformulated query, enriched with Wikipedia-derived categories, compensates for the limited knowledge encoded in smaller models by providing additional context. This allows smaller models to bridge the performance gap with larger models that have more extensive pre-training.
- Core assumption: The quality of the retrieved context outweighs the inherent representational limitations of smaller models.
- Evidence anchors:
  - [abstract] "Acting as a knowledge amplifier, QZero enables small word embedding models to achieve performance levels comparable to those of larger contextual models."
  - [section] "QZero’s capabilities go beyond simply classifying queries. Leveraging Wikipedia’s knowledge base generates insightful details for each query..."
  - [corpus] Limited direct evidence in corpus neighbors for small model scaling with retrieval; requires assumption.
- Break condition: If the retrieval context is insufficient or misaligned, the performance benefit for smaller models may not materialize.

### Mechanism 3
- Claim: Retrieval-augmented reformulation provides interpretability by revealing the query context and verifying topic relevance through the categories retrieved.
- Mechanism: The retrieved categories act as an explicit explanation of why a query is classified a certain way, illuminating the connection between the input and the predicted class. This helps users understand and trust model predictions.
- Core assumption: The retrieved categories are meaningful and directly related to the query’s topic.
- Evidence anchors:
  - [abstract] "Additionally, QZero offers meaningful insights that illuminate query context and verify topic relevance, aiding in understanding model predictions."
  - [section] "QZero’s capabilities go beyond simply classifying queries... generates insightful details for each query, including relevant categories and keywords."
  - [corpus] Weak evidence; corpus focuses more on retrieval techniques than interpretability.
- Break condition: If retrieved categories are irrelevant or ambiguous, the interpretability benefit is diminished or misleading.

## Foundational Learning

- Concept: Cosine similarity for embedding comparison
  - Why needed here: QZero relies on measuring similarity between query and class embeddings to assign labels.
  - Quick check question: What does a cosine similarity of 1.0 indicate about two vectors?

- Concept: Static vs. contextual embeddings
  - Why needed here: QZero adapts its reformulation strategy depending on whether the model is static (word-level) or contextual (sentence-level).
  - Quick check question: How does the embedding of the word "bank" differ between static and contextual embeddings?

- Concept: Keyword extraction and weighting
  - Why needed here: For static embeddings, QZero extracts and weights keywords from retrieved categories to build an enriched query representation.
  - Quick check question: Why does QZero assign higher weight to more frequently occurring keywords in the reformulated query?

## Architecture Onboarding

- Component map:
  Input query -> Retrieval system (Wikipedia categories) -> Reformulation pipeline (concatenation or keyword extraction) -> Embedding model -> Cosine similarity comparison -> Predicted class

- Critical path:
  1. Query is passed to retrieval system.
  2. Top 50 relevant Wikipedia categories are fetched.
  3. Categories are reformulated according to embedding model type.
  4. Embeddings are generated for reformulated query and class labels.
  5. Cosine similarity scores are computed and the highest-scoring class is selected.

- Design tradeoffs:
  - Retrieval depth vs. noise: More retrieved documents may increase context but also introduce irrelevant categories.
  - Token limit handling: Contextual models are capped at 512 tokens to avoid excessive noise.
  - Static vs. contextual reformulation: Different strategies required for different embedding types, increasing implementation complexity.

- Failure signatures:
  - Classification accuracy drops when retrieved categories are irrelevant or misleading.
  - Static embedding models may underperform if keyword extraction fails to capture important context.
  - Performance plateaus or degrades when too many documents are retrieved (over 50).

- First 3 experiments:
  1. Compare baseline accuracy (no retrieval) vs. QZero accuracy on a small dataset using a static embedding model.
  2. Vary the number of retrieved documents (e.g., 10, 25, 50, 100) and measure accuracy impact.
  3. Test different keyword extraction methods (POS tagging vs. capitalization) on a domain-specific dataset like Yummly.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The method relies heavily on the quality and relevance of Wikipedia categories, which may not be optimal for all domains or languages.
- Performance improvements for smaller models are conditional on the quality of retrieved context, which may vary across datasets.
- The approach assumes access to a reliable retrieval system and may face scalability issues with very large or noisy document collections.

## Confidence
- **High**: The core mechanism of query reformulation with retrieved categories and its impact on embedding quality (Mechanism 1).
- **Medium**: The claim that smaller models can achieve performance comparable to larger models (Mechanism 2), as this depends heavily on context quality.
- **Medium**: The interpretability benefit (Mechanism 3), as it relies on the meaningfulness of retrieved categories.

## Next Checks
1. Test QZero on a non-English dataset to evaluate its effectiveness across languages and assess whether Wikipedia category retrieval is equally reliable.
2. Compare QZero’s performance when using alternative knowledge bases (e.g., domain-specific ontologies or news archives) instead of Wikipedia to determine if retrieval source impacts results.
3. Conduct ablation studies by varying the number of retrieved documents (e.g., 10, 25, 50, 100) and measure the impact on accuracy and noise levels to identify the optimal retrieval depth.