---
ver: rpa2
title: 'nn2poly: An R Package for Converting Neural Networks into Interpretable Polynomials'
arxiv_id: '2406.01588'
source_url: https://arxiv.org/abs/2406.01588
tags:
- neural
- polynomial
- networks
- nn2poly
- package
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The nn2poly package implements the NN2Poly method, which converts
  pre-trained feedforward neural networks into interpretable polynomial representations.
  The method uses Taylor expansion and combinatorial properties to build polynomials
  layer-by-layer, capturing both single-variable effects and interactions.
---

# nn2poly: An R Package for Converting Neural Networks into Interpretable Polynomials

## Quick Facts
- arXiv ID: 2406.01588
- Source URL: https://arxiv.org/abs/2406.01588
- Reference count: 20
- Primary result: Converts trained neural networks into interpretable polynomial representations using Taylor expansion

## Executive Summary
The nn2poly package implements the NN2Poly method for transforming trained feedforward neural networks into interpretable polynomial representations. By applying Taylor expansion and combinatorial properties layer-by-layer, the method captures both single-variable effects and interactions while maintaining predictive accuracy. The package supports major deep learning frameworks and provides tools for training with weight constraints, polynomial extraction, prediction, and visualization. Experiments demonstrate that the resulting polynomials achieve near-identical predictions to original networks while revealing clear variable interactions through their coefficients.

## Method Summary
The NN2Poly method converts trained neural networks into polynomials by iteratively applying Taylor expansion to activation functions and computing coefficients using combinatorial multiset partitions. Each neuron's output is transformed into a polynomial that captures the input polynomial's terms and their interactions. The method requires weight constraints (norms ≤ 1) during training to ensure accurate approximations. The resulting polynomial coefficients directly reveal variable effects and interactions, with higher-order terms representing complex relationships between variables.

## Key Results
- Near-identical prediction accuracy between original neural networks and extracted polynomials on synthetic polynomial data
- Clear visualization of variable interactions through polynomial coefficients
- Support for major deep learning frameworks (TensorFlow/Keras and Torch/Luz)
- Weight constraints during training ensure accurate polynomial approximations

## Why This Works (Mechanism)

### Mechanism 1
The NN2Poly method approximates trained neural networks by iteratively constructing polynomial representations of each neuron's output using Taylor expansion. At each neuron, the activation function is approximated around zero using the input polynomial, creating a new polynomial that captures both original terms and their interactions. This requires activation functions to be differentiable and weight norms to be constrained to ≤ 1 for accurate approximation.

### Mechanism 2
The polynomial representation captures both single-variable effects and variable interactions through its coefficients. Using vectors to represent monomials, each element indicates how many times a variable appears in that term. This naturally encodes interactions as products of variables (e.g., x1·x2 represents interaction between variables 1 and 2). The combinatorial structure accurately captures all possible variable combinations up to the specified polynomial order.

### Mechanism 3
The package supports multiple neural network frameworks by accepting models as lists of weight matrices, Keras sequential objects, or Torch sequential objects. For unsupported frameworks, users can manually extract weights and format them as required. All supported frameworks use similar layer structures where weights can be represented as matrices with biases as the first row.

## Foundational Learning

- Concept: Taylor series expansion and its convergence properties
  - Why needed here: The entire method relies on approximating activation functions using Taylor expansion. Understanding convergence radius and truncation error is critical for determining when the approximation is valid.
  - Quick check question: What happens to the Taylor approximation of a function if you evaluate it far from the expansion point?

- Concept: Combinatorial mathematics and multiset partitions
  - Why needed here: The method requires computing all possible ways to partition multisets to determine which combinations of previous polynomial terms contribute to each new coefficient. This is computationally intensive and requires understanding of efficient algorithms.
  - Quick check question: How many ways can you partition the multiset {1, 1, 2, 3} into submultisets?

- Concept: Neural network training with weight constraints
  - Why needed here: The method requires weight vectors to have norms less than or equal to one for accurate Taylor approximation. Understanding how to implement and enforce these constraints during training is essential.
  - Quick check question: Why would constraining weight norms to be less than one help with interpretability but potentially slow down learning?

## Architecture Onboarding

- Component map:
  - Core algorithm: nn2poly() function implementing the iterative polynomial construction
  - Input handlers: Functions to extract weights and activation functions from supported frameworks
  - Constraint system: add_constraints() function for weight normalization during training
  - Polynomial representation: S3 object structure with labels and values for coefficients
  - Prediction engine: predict() method for using the resulting polynomials
  - Visualization: plot() method for coefficient importance display

- Critical path:
  1. Train neural network with weight constraints (add_constraints())
  2. Extract model weights and activations in required format
  3. Apply nn2poly() to convert to polynomial representation
  4. Use predict() method to make predictions with the polynomial
  5. Visualize results with plot() method

- Design tradeoffs:
  - Accuracy vs. complexity: Higher polynomial order captures more interactions but increases computational cost exponentially
  - Training speed vs. interpretability: Weight constraints ensure accurate approximation but slow down initial learning
  - Framework support vs. flexibility: Supporting specific frameworks provides ease of use but limits to those ecosystems

- Failure signatures:
  - Poor prediction accuracy between neural network and polynomial: Weight constraints may not have been properly enforced or polynomial order is too low
  - Extremely large polynomial coefficients: Data scaling issues or numerical instability in Taylor expansion
  - Missing expected interactions in visualization: Polynomial order too low or original network doesn't capture those interactions

- First 3 experiments:
  1. Simple regression: Generate data from Y = 2 - 2X1 + 5X2X3 + 3X4, train a small network, convert with nn2poly, verify polynomial captures the interaction between X2 and X3
  2. Framework compatibility: Train identical network in Keras and Torch, convert both with nn2poly, compare polynomial coefficients for consistency
  3. Constraint impact: Train same network with and without weight constraints, compare polynomial prediction accuracy to understand the tradeoff between learning speed and interpretability

## Open Questions the Paper Calls Out

### Open Question 1
How does the NN2Poly method perform on real-world tabular datasets compared to synthetic polynomial data, particularly in terms of prediction accuracy and interpretability trade-offs? The paper only demonstrates NN2Poly on synthetic polynomial data and a small penguin classification dataset, without systematic evaluation on complex real-world tabular datasets.

### Open Question 2
What is the computational complexity of the NN2Poly algorithm as a function of network depth, width, and polynomial order, and how does this scale to very large neural networks? The paper mentions exponential growth of polynomial terms but doesn't provide detailed complexity analysis or scalability limits.

### Open Question 3
How sensitive is the NN2Poly method to the choice of Taylor expansion order at each layer, and what are the optimal strategies for selecting these orders in practice? The paper doesn't systematically investigate the impact of different Taylor expansion orders on approximation quality or provide guidance on optimal selection strategies.

## Limitations
- Requires weight constraints during training, which may slow learning and require careful hyperparameter tuning
- Exponential growth of polynomial terms with increasing order limits scalability to high-dimensional problems
- Computational complexity for large networks remains unclear without formal analysis

## Confidence

**High confidence**: The polynomial extraction mechanism using Taylor expansion and combinatorial properties is mathematically sound and well-documented.

**Medium confidence**: The interpretability claims depend heavily on proper weight constraints during training, which require careful tuning not fully specified in the paper.

**Low confidence**: The computational efficiency claims for large networks are not substantiated, with potential scalability issues due to exponential polynomial growth.

## Next Checks

1. **Constraint sensitivity analysis**: Systematically vary weight constraint magnitudes and measure their impact on both training speed and polynomial approximation accuracy across different network architectures.

2. **Scalability benchmark**: Test the method on increasingly large networks (more layers, more neurons) and measure computation time, memory usage, and prediction accuracy degradation to establish practical limits.

3. **Comparison with alternatives**: Evaluate nn2poly against established interpretable methods like GAMs and neural additive models on standard benchmarks to quantify the tradeoff between interaction capture and computational cost.