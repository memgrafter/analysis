---
ver: rpa2
title: Predicting Emergent Capabilities by Finetuning
arxiv_id: '2411.16035'
source_url: https://arxiv.org/abs/2411.16035
tags:
- emergence
- data
- point
- finetuning
- checkpoints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces emergence prediction, the task of forecasting
  when future large language models will develop non-trivial performance on a given
  task, using only current pre-emergence models. The key insight is that finetuning
  existing models on the task systematically shifts the emergence point towards less
  capable models, and the shift magnitude depends on the amount of finetuning data.
---

# Predicting Emergent Capabilities by Finetuning

## Quick Facts
- arXiv ID: 2411.16035
- Source URL: https://arxiv.org/abs/2411.16035
- Reference count: 40
- Primary result: Accurately predicts when future LLMs will develop non-trivial performance on tasks, up to 4x more compute in advance with errors under 0.1 nats

## Executive Summary
This paper introduces emergence prediction, the task of forecasting when future large language models will develop non-trivial performance on a given task using only current pre-emergence models. The key insight is that finetuning existing models on the task systematically shifts the emergence point towards less capable models, and the shift magnitude depends on the amount of finetuning data. The authors leverage this by fitting a parametric "emergence law" that models how the emergence point shifts with data amount, then extrapolating to the few-shot limit to predict emergence.

## Method Summary
The method involves finetuning small-scale pre-emergence LLMs on varying amounts of task-specific data to observe how emergence points shift. A parametric function (power law in log of finetuning data amount) is fitted to model this shift relationship. The emergence law is then extrapolated to the few-shot limit to predict when larger models will exhibit emergence on the task. Uncertainty is estimated using MCMC sampling, and predictions are validated against larger model checkpoints trained with more compute.

## Key Results
- Finetuning shifts emergence points towards less capable models, with shift magnitude controlled by finetuning data amount
- Emergence law (power law in log data) accurately models emergence shift, enabling prediction within 0.1 nats error
- Successfully predicted emergence up to 4x more compute in advance for MMLU, GSM8K, CommonsenseQA, and CoLA benchmarks
- Demonstrated applications: pretraining data quality evaluation and coding capability prediction for frontier models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finetuning shifts the emergence point towards less capable models
- Mechanism: When models are finetuned on a specific task, they gain task-specific capabilities that are not present in the few-shot setting. This effectively "moves" the emergence elbow on the pretraining loss curve to the left, meaning weaker models can now demonstrate emergence on that task.
- Core assumption: The pretraining loss is a sufficient statistic for model capabilities in both few-shot and finetuned settings
- Evidence anchors:
  - [abstract] "finetuning LLMs on a given task can shift the point of emergence towards less capable models"
  - [section 4.2] "the finetuned ReLU elbow is systematically shifted towards less capable LLMs"
  - [corpus] No direct evidence - this is a novel mechanism specific to this paper
- Break condition: If pretraining loss is not a reliable indicator of capabilities in the finetuned setting

### Mechanism 2
- Claim: The amount of finetuning data controls the magnitude of emergence shift
- Mechanism: More finetuning data provides stronger task-specific capabilities, resulting in a larger leftward shift of the emergence point. Less data provides weaker shift.
- Core assumption: There is a monotonic relationship between finetuning data amount and task capability gain
- Evidence anchors:
  - [section 4.2] "by varying the amount of finetuning data, the emergence point is shifted accordingly"
  - [section 5.3] "as we increase the amount of finetuning data, the point of emergence shifts further towards less capable LLMs"
  - [corpus] No direct evidence - this relationship is established within this paper
- Break condition: If there's a saturation point where additional data doesn't further shift emergence

### Mechanism 3
- Claim: The emergence law can extrapolate from finetuned emergence to few-shot emergence
- Mechanism: By fitting a parametric function (power law in log of data) that models how emergence shifts with data amount, we can extrapolate to the few-shot limit (very small data) to predict where emergence would occur in the few-shot setting.
- Core assumption: The functional form of emergence shift follows a predictable pattern that can be extrapolated
- Evidence anchors:
  - [section 5.1] "we find that the point of emergence is well modeled by a power law in the log of the amount of finetuning data"
  - [section 6.2] "we are able to successfully predict the point of emergence within 0.1 nats"
  - [corpus] No direct evidence - this is the core novel contribution of this paper
- Break condition: If the emergence shift function has non-linear behavior that breaks the power law assumption

## Foundational Learning

- Concept: Pretraining loss as a capability predictor
  - Why needed here: The entire method relies on using pretraining loss as the independent variable for predicting emergence
  - Quick check question: Can you explain why pretraining loss might be a better predictor than FLOPs or parameter count?

- Concept: Emergence phenomenon and ReLU modeling
  - Why needed here: Understanding how emergence manifests as a sudden jump in performance that can be modeled with a ReLU function
  - Quick check question: How would you identify if a task exhibits emergence vs smooth scaling?

- Concept: Power law scaling relationships
  - Why needed here: The emergence law uses a power law in log data to model how emergence shifts, which is a common scaling relationship in ML
  - Quick check question: What's the difference between a power law and exponential relationship in scaling contexts?

## Architecture Onboarding

- Component map: Data collection pipeline → Finetuning module → Emergence law fitter → Predictor → Uncertainty estimator
- Critical path: Data collection → Finetuning → Emergence law fitting → Prediction → Validation
- Design tradeoffs:
  - More checkpoints provide better fit but increase cost
  - More data subsets provide better extrapolation but increase cost
  - Different functional forms for emergence law (log power law vs power law vs others)
  - MCMC vs bootstrap for uncertainty estimation
- Failure signatures:
  - Poor predictions (>0.1 nats error): suggests emergence shift doesn't follow assumed functional form
  - Unstable MCMC sampling: suggests need to adjust temperature or prior
  - Emergence predicted for already-emerged tasks: suggests data collection issues
- First 3 experiments:
  1. Verify finetuning shifts emergence point using a single model checkpoint and multiple data amounts
  2. Fit emergence law on one task and validate prediction accuracy
  3. Test effect of including vs excluding few-shot data in emergence law fitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does finetuning systematically shift the point of emergence towards less capable models?
- Basis in paper: [explicit] The authors state that "finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models" and observe that "by varying the amount of finetuning data, the emergence point is shifted accordingly."
- Why unresolved: While the authors empirically demonstrate this phenomenon, they do not provide a mechanistic explanation for why finetuning has this effect on emergence.
- What evidence would resolve it: Investigating the internal representations and decision-making processes of models before and after finetuning on tasks with emergent capabilities could provide insights into the underlying mechanisms.

### Open Question 2
- Question: Can the emergence prediction methodology be extended to predict emergence of capabilities beyond the tasks studied in the paper?
- Basis in paper: [explicit] The authors demonstrate the methodology on four standard NLP benchmarks and suggest that it could potentially be used to predict frontier LLM capabilities, including those which are safety relevant.
- Why unresolved: The paper focuses on predicting emergence for specific tasks, but does not explore the broader applicability of the method to more complex and diverse capabilities.
- What evidence would resolve it: Applying the emergence prediction methodology to a wider range of tasks, including those with more complex reasoning and planning requirements, would help assess its generalizability.

### Open Question 3
- Question: How far in advance can emergence be predicted using the proposed methodology?
- Basis in paper: [explicit] The authors find that they can predict emergence up to 4x the FLOPs in advance for some tasks, but note that this falls short of the 1000x demonstrated in other work for predicting post-emergence downstream capabilities.
- Why unresolved: The paper provides some insight into the limits of the methodology, but does not explore ways to further improve the prediction horizon.
- What evidence would resolve it: Investigating methods to optimize the data selection and model checkpoint selection for fitting the emergence law could potentially extend the prediction horizon.

## Limitations

- Data Collection Constraints: The method requires extensive finetuning of small-scale models on varying data amounts, which is computationally expensive and limits generalizability to larger model scales.
- Functional Form Assumptions: The power law model for emergence shift may not hold universally across all tasks and emergence patterns.
- Limited Validation Scope: The paper validates predictions using models trained with up to 4x more compute than the largest finetuning checkpoint, leaving accuracy for much larger capability gaps untested.

## Confidence

**High Confidence**: The core mechanism that finetuning shifts emergence points is well-established within the paper's experimental framework. The quantitative prediction results (within 0.1 nats) on the tested benchmarks are reproducible and demonstrate the method works as described for the specific tasks and model scales studied.

**Medium Confidence**: The emergence law extrapolation method is theoretically sound and works well on the four NLP benchmarks tested. However, confidence is lower due to limited testing across different task types, model architectures, and scaling regimes. The method's generalizability to other domains or more complex capabilities is uncertain.

**Low Confidence**: The proof-of-concept applications (pretraining data quality evaluation and coding capability prediction) are promising but lack comprehensive validation. The coding capability prediction for frontier models is particularly speculative, as it relies on extrapolating from small-scale models to vastly different model families and capabilities.

## Next Checks

1. **Cross-task Generalization Test**: Apply the emergence prediction method to a diverse set of 10+ tasks spanning different domains (mathematical reasoning, code generation, factual knowledge, common sense) and compare prediction accuracy across task types. This would reveal whether the method generalizes beyond the four NLP benchmarks studied.

2. **Multi-Architecture Validation**: Test the emergence prediction framework on models from different architectures (LLaMA, GPT, BERT) and training objectives (causal language modeling, masked language modeling). This would validate whether pretraining loss serves as a universal capability predictor across architectures.

3. **Long-range Extrapolation**: Attempt to predict emergence for tasks where current models show no signs of capability, using only pre-emergence checkpoints. Compare predictions against actual emergence points when larger models become available. This would test the method's ability to forecast truly emergent capabilities rather than just shifting existing ones.