---
ver: rpa2
title: A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion
  and Taxonomy Expansion
arxiv_id: '2402.13405'
source_url: https://arxiv.org/abs/2402.13405
tags:
- taxonomy
- expansion
- entity
- entities
- parent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TaxoInstruct, a unified framework for three
  entity enrichment tasks: Entity Set Expansion, Taxonomy Expansion, and Seed-Guided
  Taxonomy Construction. The key insight is that all three tasks can be viewed as
  finding entities with specific relationships to given query entities - siblings
  for Entity Set Expansion, parents for Taxonomy Expansion, and both for Seed-Guided
  Taxonomy Construction.'
---

# A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion

## Quick Facts
- arXiv ID: 2402.13405
- Source URL: https://arxiv.org/abs/2402.13405
- Reference count: 32
- Primary result: Unified framework achieving state-of-the-art results across three entity enrichment tasks

## Executive Summary
This paper introduces TaxoInstruct, a unified framework for three entity enrichment tasks: Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction. The key insight is that all three tasks can be viewed as finding entities with specific relationships to given query entities - siblings for Entity Set Expansion, parents for Taxonomy Expansion, and both for Seed-Guided Taxonomy Construction. TaxoInstruct uses taxonomy-guided instruction tuning to fine-tune a large language model (LLM) to generate parent and sibling entities. It first jointly fine-tunes the LLM on an external taxonomy to learn taxonomic relations, then applies task-specific fine-tuning. Extensive experiments on benchmark datasets demonstrate that TaxoInstruct significantly outperforms task-specific baselines across all three tasks.

## Method Summary
TaxoInstruct fine-tunes LLaMA-7B using LoRA on an external taxonomy (CTD's MEDIC disease vocabulary) through taxonomy-guided instruction tuning. The model learns to identify taxonomic relationships through self-supervision data generated from the taxonomy. For Entity Set Expansion, the framework uses seed permutations to improve coverage, while for Taxonomy Expansion it employs an auxiliary PLM to reduce candidate space. The unified approach enables a single model to handle all three tasks through appropriate inference strategies.

## Key Results
- Entity Set Expansion: Achieves MAP@10 of 79.15 and MAP@20 of 71.72 on PubMed-CVD, outperforming previous state-of-the-art methods
- Taxonomy Expansion: Achieves Accuracy of 46.79 and Wu&P Similarity of 77.39 on the Environment dataset
- Demonstrates significant improvements over task-specific baselines across all three tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Taxonomy-guided instruction tuning helps LLMs understand hierarchical relationships (parent-child and sibling relations) in entity sets.
- Mechanism: The LLM is fine-tuned on self-supervision data generated from existing taxonomies, where instructions explicitly ask it to identify parent nodes among semantically similar candidates or expand entity sets by finding siblings. This joint pre-training on external taxonomies before task-specific fine-tuning enables the model to learn taxonomic structures.
- Core assumption: The LLM can effectively learn and generalize taxonomic relations from the instruction-tuning format when provided with semantically similar candidate entities.
- Evidence anchors:
  - [abstract] "We propose a taxonomy-guided instruction tuning framework to teach a large language model to generate parent and sibling entities, where the joint pre-training process facilitates the mutual enhancement of the two skills."
  - [section 3.4] "We propose to first jointly instruction tune an LLM on an external large taxonomy... After the unified fine-tuning on the external taxonomy, we will use the LLM on each downstream task."
- Break condition: The approach breaks when the taxonomy structure is too complex (e.g., DAG instead of tree) or when candidate entities in instructions are not semantically meaningful enough for the LLM to distinguish correct relationships.

### Mechanism 2
- Claim: Using permutations of seed entities during inference improves entity set expansion coverage by leveraging the LLM's sequential nature.
- Mechanism: When given n seed entities, all n! permutations are generated and fed individually into the LLM as queries. The union of all outputs provides a more comprehensive set of expanded entities than a single query would.
- Core assumption: The LLM's output varies meaningfully with different permutations of the same seed set due to its autoregressive generation process.
- Evidence anchors:
  - [section 3.1] "Empirically, we find that shuffling the seed entities in the query will lead to different outputs by the LLM due to the sequential nature of language modeling."
- Break condition: This mechanism breaks when the LLM produces highly similar outputs regardless of input order (as observed with TaxoInstruct-NoTune), or when n! permutations become computationally prohibitive.

### Mechanism 3
- Claim: Using an auxiliary PLM to select top-k candidate parents reduces label space complexity and improves taxonomy expansion accuracy.
- Mechanism: Instead of providing all taxonomy nodes as candidate parents (which would overwhelm the LLM), an auxiliary PLM first retrieves the top-k most semantically similar entities to the query entity. The LLM then selects the correct parent from this reduced candidate set.
- Core assumption: The correct parent is among the top-k semantically similar entities retrieved by the auxiliary PLM, and the LLM can effectively identify it from this reduced set.
- Evidence anchors:
  - [section 3.2] "we propose to use an auxiliary PLM to first retrieve a set of candidate nodes from the taxonomy and thus reduce the label space for the LLM... T i k = arg max T ⊆S,|T |=k X sj ∈T sim (h(si), h(sj))"
- Break condition: This mechanism breaks when the auxiliary PLM fails to include the true parent in the top-k candidates, or when k is too small to contain the correct parent.

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: Enables LLMs to perform specialized tasks (finding parents/siblings) by fine-tuning on structured, instruction-based data derived from taxonomies.
  - Quick check question: What distinguishes instruction tuning from standard fine-tuning in this context?

- Concept: Taxonomy structure and semantic similarity
  - Why needed here: Understanding how parent-child and sibling relationships are defined and how entities can be semantically similar is crucial for both generating self-supervision data and evaluating model outputs.
  - Quick check question: How does semantic similarity between entities help in selecting candidate parents?

- Concept: Autoregressive generation and permutation effects
  - Why needed here: The LLM's sequential generation process means input order affects output, which is why permutations are used to improve coverage.
  - Quick check question: Why does shuffling seed entities in the query lead to different LLM outputs?

## Architecture Onboarding

- Component map:
  - External taxonomy (e.g., CTD's MEDIC disease vocabulary)
  - Base LLM (LLaMA-7B)
  - Auxiliary PLM (SciBERT or SPECTER for similarity scoring)
  - Self-supervision data generator (creates instruction-query-output tuples)
  - Fine-tuning pipeline (joint pre-training + task-specific fine-tuning)
  - Inference engine (handles permutations, candidate selection, ranking)

- Critical path:
  1. Generate self-supervision data from external taxonomy
  2. Jointly fine-tune LLM on this data to learn taxonomic relations
  3. For each task:
     - Entity Set Expansion: Generate permutations → LLM inference → auxiliary PLM ranking
     - Taxonomy Expansion: Auxiliary PLM candidate selection → LLM inference
     - Seed-Guided Taxonomy Construction: Combine both approaches

- Design tradeoffs:
  - Using permutations increases coverage but adds computational cost (n! growth)
  - Selecting top-k candidates reduces complexity but risks excluding correct parents if k is too small
  - Joint pre-training on external taxonomy improves performance but requires access to large taxonomies

- Failure signatures:
  - Poor performance on Entity Set Expansion when LLM outputs highly similar results across permutations
  - Taxonomy Expansion failures when true parents are consistently outside top-k candidates
  - General underperformance when instruction format doesn't align with LLM's understanding

- First 3 experiments:
  1. Test joint pre-training effectiveness: Compare TaxoInstruct vs TaxoInstruct-NoTune on a small taxonomy task
  2. Validate candidate selection: Measure how often true parents appear in top-k candidates across different k values
  3. Assess permutation impact: Compare expanded entity sets from single vs multiple permutations on Entity Set Expansion task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would TaxoInstruct perform on a directed acyclic graph (DAG) taxonomy where entities can have multiple parents?
- Basis in paper: [inferred] The paper mentions that existing entity enrichment datasets assume each node has only one parent, and acknowledges that in real applications taxonomies are normally DAGs rather than simple trees.
- Why unresolved: The current evaluation is limited to tree-structured taxonomies, and the paper explicitly states that the performance on DAG taxonomies is not evaluated.
- What evidence would resolve it: Empirical results showing TaxoInstruct's performance on benchmark datasets with DAG structures, comparing accuracy metrics to tree-based datasets.

### Open Question 2
- Question: How sensitive is TaxoInstruct to the quality and coverage of the external taxonomy used for pre-training?
- Basis in paper: [explicit] The paper uses CTD's MEDIC disease vocabulary as the external taxonomy and mentions that it's important for the model to first learn to recognize and understand hypernymy and sibling relations before being applied to each task.
- Why unresolved: The paper doesn't explore how performance changes with different external taxonomies or when the external taxonomy has gaps or noise in its structure.
- What evidence would resolve it: Controlled experiments varying the quality, domain specificity, and coverage of external taxonomies, measuring impact on downstream task performance.

### Open Question 3
- Question: What is the impact of using different LLMs or model sizes as the base for TaxoInstruct?
- Basis in paper: [explicit] The paper uses LLaMA-7B as the base model and mentions that they fine-tuned about 0.6% of trainable parameters (about 40 million parameters) using LoRA.
- Why unresolved: The paper doesn't explore how performance scales with different model sizes or architectures, or whether smaller/faster models could achieve similar results.
- What evidence would resolve it: Comparative results using different base models (e.g., LLaMA-13B, LLaMA-33B, or other architectures) with the same fine-tuning procedure, measuring both performance and computational efficiency.

## Limitations
- The framework assumes taxonomy structures are tree-like rather than DAGs, which may limit applicability to real-world taxonomies with multiple inheritance
- Performance heavily depends on the quality and coverage of the external taxonomy used for pre-training
- The permutation approach scales factorially with seed count, creating computational bottlenecks for larger seed sets

## Confidence
- **High confidence**: The unified framework design and instruction tuning approach are well-justified and empirically validated
- **Medium confidence**: The effectiveness of using auxiliary PLM for candidate selection, as this depends heavily on the quality of semantic similarity measures
- **Medium confidence**: The permutation-based inference strategy, as its benefits may vary significantly across different LLM architectures and seed sets

## Next Checks
1. Test the framework on DAG-structured taxonomies to identify failure modes when entities have multiple parents
2. Systematically vary k in the candidate selection mechanism to find optimal trade-offs between computational efficiency and accuracy
3. Evaluate the permutation strategy on different LLM architectures to determine if benefits are model-dependent or universal