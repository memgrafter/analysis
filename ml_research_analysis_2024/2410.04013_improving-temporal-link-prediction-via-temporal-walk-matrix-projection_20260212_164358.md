---
ver: rpa2
title: Improving Temporal Link Prediction via Temporal Walk Matrix Projection
arxiv_id: '2410.04013'
source_url: https://arxiv.org/abs/2410.04013
tags:
- temporal
- walk
- node
- where
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of temporal link prediction in
  dynamic graphs, where the goal is to predict future interactions between entities
  based on historical interactions. The authors propose TPNet, a new temporal graph
  neural network that introduces a temporal walk matrix incorporating time decay effects
  to simultaneously consider both temporal and structural information.
---

# Improving Temporal Link Prediction via Temporal Walk Matrix Projection

## Quick Facts
- arXiv ID: 2410.04013
- Source URL: https://arxiv.org/abs/2410.04013
- Authors: Xiaodong Lu; Leilei Sun; Tongyu Zhu; Weifeng Lv
- Reference count: 40
- Primary result: TPNet achieves up to 33.3× speedup over state-of-the-art methods on 13 benchmark datasets

## Executive Summary
This paper addresses the problem of temporal link prediction in dynamic graphs, where the goal is to predict future interactions between entities based on historical interactions. The authors propose TPNet, a new temporal graph neural network that introduces a temporal walk matrix incorporating time decay effects to simultaneously consider both temporal and structural information. TPNet uses a random feature propagation mechanism with theoretical guarantees to implicitly maintain the temporal walk matrices, improving computation and storage efficiency. The method outperforms state-of-the-art baselines on 13 benchmark datasets, achieving a maximum speedup of 33.3×. TPNet's key innovation is the unified view of existing relative encodings as functions of temporal walk matrices, enabling a more principled approach to designing new encodings. The experimental results validate the effectiveness and efficiency of TPNet, with ablation studies confirming the contributions of its proposed components.

## Method Summary
TPNet is a temporal graph neural network designed for temporal link prediction. It introduces a temporal walk matrix that incorporates time decay effects to simultaneously consider temporal and structural information. The core innovation is a random feature propagation mechanism with theoretical guarantees that implicitly maintains temporal walk matrices, significantly reducing computation and storage costs compared to explicit methods. The method uses random Gaussian features initialized for nodes and updates them through linear combinations based on temporal interactions. For link prediction, TPNet decodes pairwise features from all layers of node representations using inner products, which captures richer temporal walk information than single-layer methods. The model is trained using standard link prediction loss with the time decay parameter λ tuned via grid search for each dataset.

## Key Results
- TPNet outperforms state-of-the-art baselines on 13 benchmark datasets, achieving a maximum speedup of 33.3×
- Random feature propagation preserves inner products of temporal walk matrices while reducing storage and computation costs
- Time-decayed temporal walk scores effectively capture both temporal and structural information
- Decoding pairwise features from all layers captures richer temporal walk information than single-layer methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random feature propagation preserves inner products of temporal walk matrices while reducing storage and computation costs.
- Mechanism: By initializing node representations with random Gaussian features and updating them through linear combinations based on temporal interactions, the resulting representations are random projections of the temporal walk matrices. This preserves inner products with high probability (Theorem 2).
- Core assumption: The temporal walk matrix updates are linear combinations of rows, allowing the same update rule to apply to random projections.
- Evidence anchors:
  - [abstract]: "TPNet designs a random feature propagation mechanism with theoretical guarantees to implicitly maintain the temporal walk matrices"
  - [section]: "The maintaining mechanism here can be considered as a random feature propagation mechanism on the temporal graph"
  - [corpus]: Weak - no direct corpus evidence supporting random projection preservation; relies on Johnson-Lindenstrauss lemma
- Break condition: If temporal walk matrix updates cannot be expressed as linear combinations of rows, the random feature propagation mechanism fails.

### Mechanism 2
- Claim: Time-decayed temporal walk scores capture both temporal and structural information effectively.
- Mechanism: Each temporal walk W receives a score Qk j=1 e−λ(t−tj), where interactions further in the past receive exponentially lower weights. This incorporates temporal information while counting structural walks.
- Core assumption: The time decay effect λ > 0 appropriately models the importance decay of historical interactions in temporal graphs.
- Evidence anchors:
  - [abstract]: "TPNet introduces a temporal walk matrix that incorporates the time decay effect of the temporal graph"
  - [section]: "Unlike most previous methods that only count the number of temporal walks, we consider the temporal information carried by a temporal walk"
  - [corpus]: Weak - limited corpus evidence on specific time decay parameterization effectiveness
- Break condition: If the optimal λ varies significantly across different temporal graph datasets or if long-term dependencies are crucial.

### Mechanism 3
- Claim: Decoding pairwise features from all layers of node representations captures richer temporal walk information than single-layer methods.
- Mechanism: By computing inner products between all layers of representations from both nodes (u and v), the method captures correlations across different temporal walk lengths simultaneously.
- Core assumption: The correlation between different temporal walk lengths provides complementary information for link prediction.
- Evidence anchors:
  - [section]: "we use representations from all layers to decode the pairwise information" and "this method does not consider the correlation between all layers of both nodes"
  - [section]: "the inner product of temporal walk matrices should be larger than zero"
  - [corpus]: Weak - no direct corpus evidence comparing multi-layer vs single-layer pairwise decoding
- Break condition: If layer correlations are negligible or if the increased computation outweighs the information gain.

## Foundational Learning

- Concept: Temporal walk matrices as unified framework for relative encodings
  - Why needed here: Existing relative encodings are fragmented and inefficient; unifying them reveals temporal walk matrices as the underlying structure
  - Quick check question: How can you express the relative encoding of DyGFormer as a function of temporal walk matrices?

- Concept: Random projection preservation of inner products
  - Why needed here: Direct computation of temporal walk matrices is infeasible; random projections provide an efficient approximation
  - Quick check question: What is the minimum dimension required for random projections to preserve inner products within ε error?

- Concept: Time decay modeling in temporal graphs
  - Why needed here: Most existing methods ignore temporal information in relative encodings; time decay captures the intuition that recent interactions are more important
  - Quick check question: How does the time decay weight λ affect the relative importance of historical interactions?

## Architecture Onboarding

- Component map:
  Node Representation Maintaining (NRM) -> Link Likelihood Computing (LLC) -> Link likelihood prediction

- Critical path:
  1. Initialize random features for zero layer
  2. For each interaction, update node representations through random feature propagation
  3. When predicting link, decode pairwise features from all layers
  4. Learn node embeddings from recent interaction histories
  5. Combine pairwise features and node embeddings for final prediction

- Design tradeoffs:
  - Random feature dimension vs. accuracy: Higher dimensions improve approximation but increase memory
  - Time decay weight λ: Balances temporal vs structural information
  - Layer count k: More layers capture longer walks but increase computation
  - Normalization (ReLU, log) vs. raw features: Stabilizes training but may lose information

- Failure signatures:
  - Numerical overflow: Likely from unscaled inner products across layers (w/o Scale variant)
  - Poor performance on dense graphs: May need larger random feature dimensions
  - Sensitivity to λ: Different datasets may require different time decay parameters
  - Memory issues: Explicit temporal walk matrix storage fails on large graphs (PINT comparison)

- First 3 experiments:
  1. Vary random feature dimension (1, 16, 128) on a small dataset to find minimum effective dimension
  2. Compare performance with/without time decay (λ=0 vs learned λ) to validate temporal modeling
  3. Test on both sparse (Wikipedia) and dense (LastFM) datasets to validate scalability claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed temporal walk matrix construction be adapted to handle long-term dependencies in temporal graphs?
- Basis in paper: [explicit] The paper mentions that the current time decay weight may not be optimal for networks with long-term dependencies.
- Why unresolved: The paper does not explore alternative weighting schemes or adaptive mechanisms for capturing long-term dependencies.
- What evidence would resolve it: Experimental results comparing the proposed method with variations that use different time decay weights or adaptive mechanisms on datasets with varying temporal characteristics.

### Open Question 2
- Question: How does the choice of node representation dimension affect the performance and scalability of TPNet?
- Basis in paper: [explicit] The paper discusses the influence of node representation dimension on performance but does not provide a definitive answer on the optimal choice.
- Why unresolved: The paper only provides empirical results showing that a small dimension can achieve satisfactory performance but does not explain the underlying reasons or provide guidelines for choosing the optimal dimension.
- What evidence would resolve it: A comprehensive study analyzing the relationship between node representation dimension, performance, and scalability across various datasets with different characteristics.

### Open Question 3
- Question: Can the random feature propagation mechanism be extended to other types of temporal walk matrices beyond those discussed in the paper?
- Basis in paper: [explicit] The paper mentions that the method can be extended to other types of temporal walk matrices but does not provide specific examples or experimental results.
- Why unresolved: The paper only provides a theoretical analysis of the conditions under which the random feature propagation mechanism can be applied but does not demonstrate its effectiveness on other types of temporal walk matrices.
- What evidence would resolve it: Experimental results comparing the performance of TPNet with variations that use different types of temporal walk matrices and random feature propagation mechanisms.

## Limitations

- Limited ablation studies on random feature dimension vs. accuracy tradeoff
- Lack of theoretical analysis for optimal time decay parameter λ
- No analysis of performance on graphs with different temporal dynamics (bursty vs. steady interaction patterns)

## Confidence

- Random feature preservation claims: Medium (relies on Johnson-Lindenstrauss lemma without direct experimental validation)
- Time decay effectiveness: Medium (lacks comparative analysis across different decay function forms)
- Multi-layer decoding advantage: Low (absence of ablation studies comparing single-layer vs multi-layer performance)

## Next Checks

1. **Inner Product Preservation Validation**: Implement a controlled experiment measuring actual inner product preservation rates across different random feature dimensions (1, 16, 64, 256) on a synthetic temporal graph with known temporal walk matrices.

2. **Time Decay Function Comparison**: Replace the exponential decay with power-law and learned decay functions, measuring performance degradation/gain across all 13 datasets to determine if exponential decay is optimal.

3. **Memory Efficiency Benchmark**: Compare TPNet's memory usage against PINT's explicit temporal walk matrix storage on increasingly large temporal graphs (10K, 100K, 1M edges) to validate the claimed 33.3× speedup comes from both computation and storage efficiency.