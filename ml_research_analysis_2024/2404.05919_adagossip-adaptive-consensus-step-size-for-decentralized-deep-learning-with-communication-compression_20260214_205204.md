---
ver: rpa2
title: 'AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with
  Communication Compression'
arxiv_id: '2404.05919'
source_url: https://arxiv.org/abs/2404.05919
tags:
- learning
- decentralized
- communication
- adag-sgd
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses communication efficiency in decentralized deep
  learning by proposing AdaGossip, a method that dynamically adjusts consensus step-sizes
  based on compressed model differences between neighboring agents. The core idea
  is to adaptively scale the consensus step-size for each parameter using estimates
  of the second raw moments of the gossip-error, allowing parameters with higher compression-induced
  errors to be averaged at a lower rate.
---

# AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with Communication Compression

## Quick Facts
- arXiv ID: 2404.05919
- Source URL: https://arxiv.org/abs/2404.05919
- Reference count: 40
- AdaGossip outperforms CHOCO-SGD by 0-2% test accuracy across multiple datasets and compression schemes

## Executive Summary
This paper addresses communication efficiency in decentralized deep learning by proposing AdaGossip, a method that dynamically adjusts consensus step-sizes based on compressed model differences between neighboring agents. The core idea is to adaptively scale the consensus step-size for each parameter using estimates of the second raw moments of the gossip-error, allowing parameters with higher compression-induced errors to be averaged at a lower rate. Experiments on various datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures (ResNet, LeNet-5, MobileNet-V2), and graph topologies (ring, Dyck, torus) demonstrate that AdaGossip consistently outperforms the state-of-the-art CHOCO-SGD method by 0-2% in test accuracy across different compression schemes (top-k sparsification and quantization).

## Method Summary
AdaGossip introduces an adaptive consensus step-size mechanism that estimates the second raw moments of gossip-errors to scale communication between neighboring agents. The method tracks the variance introduced by compression and uses this information to adjust how aggressively each parameter is averaged during the gossip phase. By allowing parameters with higher compression-induced errors to be averaged at a lower rate, AdaGossip mitigates the negative impact of communication compression while maintaining decentralized learning efficiency.

## Key Results
- Consistently outperforms CHOCO-SGD by 0-2% in test accuracy across all tested scenarios
- Shows particular effectiveness in larger graph structures and deeper networks
- Maintains performance across diverse compression schemes including top-k sparsification and quantization
- Demonstrates robust performance across multiple datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, ImageNet) and architectures (ResNet, LeNet-5, MobileNet-V2)

## Why This Works (Mechanism)
AdaGossip works by adaptively scaling the consensus step-size for each parameter based on the estimated second raw moments of the gossip-error. When compression introduces higher variance for certain parameters, the method reduces the averaging rate for those parameters, preventing the propagation of noisy updates across the network. This selective adaptation allows the system to maintain communication efficiency while preserving model quality, particularly in scenarios with significant compression-induced noise.

## Foundational Learning

**Gossip Protocol**
- Why needed: Enables decentralized model averaging without central coordinator
- Quick check: Understand pairwise averaging between neighbors in graph topology

**Communication Compression**
- Why needed: Reduces bandwidth requirements in decentralized training
- Quick check: Grasp effects of top-k sparsification and quantization on model updates

**Second Raw Moments**
- Why needed: Provides statistical measure of compression-induced error variance
- Quick check: Understand relationship between moment estimates and adaptive step-size scaling

## Architecture Onboarding

**Component Map**
Data nodes -> Gossip protocol -> Adaptive step-size controller -> Model aggregation

**Critical Path**
1. Local gradient computation at each node
2. Gossip communication with adaptive step-sizes
3. Model aggregation using scaled updates
4. Parameter-wise step-size adjustment based on error estimates

**Design Tradeoffs**
- Granular parameter-wise adaptation vs. computational overhead
- Communication efficiency vs. convergence speed
- Compression ratio vs. model accuracy

**Failure Signatures**
- Poor performance on highly heterogeneous data distributions
- Degraded accuracy with extreme compression ratios
- Computational bottleneck from parameter-wise step-size tracking

**First Experiments**
1. Test on ring topology with CIFAR-10 and ResNet-18
2. Compare top-k sparsification at different compression ratios
3. Evaluate scalability on larger graph structures (torus, Dyck)

## Open Questions the Paper Calls Out

None

## Limitations
- Theoretical convergence analysis limited to convex settings while experiments focus on non-convex deep learning
- Performance on extremely large-scale graphs with thousands of nodes remains untested
- Computational overhead of parameter-wise step-size adaptation not quantified

## Confidence

**High Confidence**: Empirical performance improvements over CHOCO-SGD on tested datasets and architectures

**Medium Confidence**: Generalizability to other compression schemes beyond top-k and quantization

**Medium Confidence**: Effectiveness in highly heterogeneous data distributions across nodes

## Next Checks

1. Conduct ablation studies isolating the impact of adaptive step-size scaling versus fixed step-sizes in the gossip protocol
2. Test AdaGossip on graphs with >1000 nodes to evaluate scalability and communication efficiency at large scales
3. Analyze the per-iteration computational overhead of parameter-wise step-size adaptation compared to global step-size methods