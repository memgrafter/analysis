---
ver: rpa2
title: 'Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision
  and Language Models'
arxiv_id: '2407.19474'
source_url: https://arxiv.org/abs/2407.19474
tags:
- visual
- riddles
- answer
- image
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Visual Riddles, a benchmark to test vision-and-language
  models on visual riddles requiring commonsense and world knowledge. The dataset
  includes 400 images created by text-to-image models, each paired with a question,
  ground-truth answer, hint, and attribution.
---

# Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models

## Quick Facts
- arXiv ID: 2407.19474
- Source URL: https://arxiv.org/abs/2407.19474
- Reference count: 40
- Human accuracy: 82%, best model (Gemini-Pro-1.5): 40%

## Executive Summary
This paper introduces Visual Riddles, a benchmark designed to test vision-and-language models on visual riddles that require both commonsense and world knowledge. The dataset consists of 400 synthetic images generated by text-to-image models, each paired with a question, ground-truth answer, hint, and attribution. Human evaluation reveals that existing models lag significantly behind human performance, with even the best model (Gemini-Pro-1.5) achieving only 40% accuracy compared to human accuracy of 82%. The benchmark also includes automatic evaluation tasks to enable scalable assessment, with the top auto-rater matching human judgment 87% of the time. Results demonstrate that models struggle to solve these riddles even with hints and attributions, highlighting the need for further advancements in commonsense reasoning and world knowledge integration.

## Method Summary
The Visual Riddles benchmark is built using synthetic images generated by text-to-image models, each containing subtle visual clues essential for solving the riddles. Human designers create riddles by embedding cultural nuances and world knowledge into the images, which are then paired with questions, ground-truth answers, hints, and attributions. The benchmark includes both open-ended and multiple-choice visual question answering tasks. Evaluation is performed using human judgment as well as automatic methods, including reference-based and reference-free auto-raters that use vision-language models to approximate human judgment. The dataset, code, and leaderboard are publicly available for further research.

## Key Results
- Human accuracy on Visual Riddles: 82%
- Best model (Gemini-Pro-1.5) accuracy: 40%
- Auto-rater (Gemini-Pro-1.5) matches human judgment 87% of the time in reference-based evaluations
- Text-to-image models successfully reproduce images with intended visual clues only 15% of the time

## Why This Works (Mechanism)

### Mechanism 1
Visual riddles challenge both visual recognition and reasoning by embedding subtle clues in images. Images are synthetically generated to include visual cues that are contextually tied to the riddle's answer, requiring models to detect these cues and combine them with commonsense knowledge. The core assumption is that synthetic images can reliably encode the necessary visual clues without introducing artifacts that mislead models.

### Mechanism 2
Providing hints and attributions improves model performance by directing attention to relevant visual cues and supplying needed background knowledge. Textual hints guide models to focus on specific elements within the image, while attributions provide external sources to verify or expand upon the knowledge required to solve the riddle. The core assumption is that models can effectively integrate textual hints and attribution content with visual input to enhance reasoning.

### Mechanism 3
Automatic evaluation using vision-language models can approximate human judgment, enabling scalable assessment of open-ended riddle answers. Reference-based auto-raters compare candidate answers to ground truth, while reference-free auto-raters judge answers solely on image and question context. The core assumption is that vision-language models can reliably distinguish correct from incorrect answers in a way that correlates with human judgment.

## Foundational Learning

- Concept: Multimodal reasoning
  - Why needed here: Visual riddles require integrating visual cues with textual information and world knowledge, which is the core challenge of multimodal reasoning.
  - Quick check question: Can you describe a scenario where a visual cue in an image requires external knowledge to interpret correctly?

- Concept: Commonsense knowledge
  - Why needed here: Many riddles depend on everyday knowledge (e.g., cultural norms, physical principles) that is not explicitly stated in the image or question.
  - Quick check question: How would you identify and encode commonsense knowledge in a synthetic image to test a model's understanding?

- Concept: Automatic evaluation in NLP
  - Why needed here: Scalable evaluation of open-ended answers is critical for benchmarking, especially when human annotation is costly or slow.
  - Quick check question: What are the main differences between reference-based and reference-free evaluation, and when might each be appropriate?

## Architecture Onboarding

- Component map: Text-to-image models -> Human designers -> Riddle generation -> Human annotation -> Model evaluation -> Performance analysis
- Critical path: 1. Generate synthetic images with embedded clues, 2. Pair images with questions, hints, and attributions, 3. Collect human and model answers, 4. Evaluate answers using human judgment and auto-raters, 5. Analyze performance and identify model weaknesses
- Design tradeoffs: Synthetic vs. real images (control vs. artifacts), open-ended vs. multiple-choice (depth vs. scalability), human vs. auto evaluation (reliability vs. speed)
- Failure signatures: Low success rates on riddles requiring external knowledge, auto-raters disagree with human judgment, synthetic images fail to embed intended clues
- First 3 experiments: 1. Ablation study comparing performance with and without hints/attributions, 2. Auto-rater validation testing reference-free vs. reference-based performance, 3. Image reproduction attempting to regenerate synthetic images using different models

## Open Questions the Paper Calls Out

### Open Question 1
Can automated evaluation methods fully replace human judgment in assessing the correctness of open-ended answers to visual riddles? The paper discusses using models like Gemini-Pro-1.5 as auto-raters, but acknowledges they only match human judgment 87% of the time in reference-based evaluations. Further research comparing auto-rater performance against diverse human evaluators on a larger and more varied dataset would help resolve this.

### Open Question 2
How can text-to-image models be improved to better capture subtle visual cues required for solving visual riddles? The paper highlights that current text-to-image models struggle to reproduce images with nuanced visual clues, with a success rate of only 15% for the best model. Development and testing of new text-to-image models specifically trained on datasets like Visual Riddles could provide evidence for improvement.

### Open Question 3
What is the impact of providing auxiliary information (hints and attributions) on the performance of vision-and-language models in solving visual riddles? The paper shows that providing hints and attributions improves model performance, but also reveals that models struggle to filter through irrelevant details when attributions are provided. Controlled experiments varying the type, format, and timing of auxiliary information could help determine optimal presentation methods.

## Limitations
- Dataset size of 400 riddles is relatively small for robust statistical analysis
- All images are synthetically generated, which may introduce artifacts or biases
- Human evaluation methodology could be influenced by rater subjectivity

## Confidence

**High confidence**: The observation that all tested models significantly underperform humans (82% vs. 40% accuracy) is well-supported by evaluation methodology.

**Medium confidence**: The claim that visual riddles require both commonsense and world knowledge integration is reasonable but could benefit from more detailed analysis.

**Medium confidence**: The assertion that hints and attributions improve performance is supported by results, but the mechanism requires further investigation.

**Low confidence**: The scalability claim for automatic evaluation is supported by 87% agreement with human judgment, but this metric alone doesn't fully establish reliability for all edge cases.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate model performance on visual riddles generated by different text-to-image models or on real photographs with similar riddle structures to assess whether synthetic image artifacts are influencing results.

2. **Error analysis decomposition**: Categorize model failures by type (visual recognition errors, commonsense reasoning failures, world knowledge gaps) to identify which components of multimodal reasoning need the most improvement.

3. **Human-in-the-loop optimization**: Conduct controlled experiments where human annotators provide different types of hints (visual attention cues vs. knowledge hints) to determine which approaches most effectively improve model performance and whether the current hint design is optimal.