---
ver: rpa2
title: Question-Based Retrieval using Atomic Units for Enterprise RAG
arxiv_id: '2405.12363'
source_url: https://arxiv.org/abs/2405.12363
tags:
- retrieval
- chunk
- query
- arxiv
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores retrieval improvement in enterprise RAG by atomizing
  chunks into atomic statements and generating synthetic questions on those atoms.
  Dense retrieval is performed on the atomic questions rather than the original chunks,
  leading to better alignment with user queries.
---

# Question-Based Retrieval using Atomic Units for Enterprise RAG

## Quick Facts
- arXiv ID: 2405.12363
- Source URL: https://arxiv.org/abs/2405.12363
- Reference count: 12
- Key result: Atomic unit retrieval with synthetic questions improves RAG recall@1 from 65.5% to 76.3% on SQuAD

## Executive Summary
This paper introduces a novel approach to retrieval-augmented generation (RAG) that breaks document chunks into atomic units and generates synthetic questions for each atom. Instead of retrieving based on the original chunk text, the system retrieves using the synthetic questions, which better align with user queries and improve retrieval recall. The method shows significant performance gains over traditional chunk-based retrieval while offering storage efficiency through selective question retention.

## Method Summary
The approach decomposes document chunks into atomic statements using LLM-based decomposition, then generates synthetic questions for each atomic unit. Retrieval is performed on these synthetic questions rather than the original chunk content. The system employs a question selection mechanism to retain only diverse, high-quality questions, reducing storage costs while maintaining retrieval performance. Experiments demonstrate that synthetic questions outperform unstructured atomic retrieval and achieve better results than traditional chunk-based methods.

## Key Results
- Recall@1 improves from 65.5% to 76.3% on SQuAD using all-mpnet-base-v2
- Atomic retrieval with synthetic questions consistently outperforms both unstructured atomic retrieval and traditional chunk-based methods
- Selective question retention reduces storage costs while maintaining high recall performance

## Why This Works (Mechanism)
The method works by creating a tighter semantic alignment between the retrieval index and user queries. By decomposing chunks into atomic units, the system captures more granular semantic content. Generating synthetic questions for these atoms creates natural language representations that are inherently query-like, making them more compatible with user query patterns. The question selection mechanism ensures diversity while avoiding redundancy, maintaining coverage without unnecessary storage overhead.

## Foundational Learning

Dense Retrieval: Why needed - To match semantic meaning rather than keyword overlap; Quick check - Verify embeddings capture semantic similarity for paraphrased queries

Chunk Decomposition: Why needed - Large chunks contain multiple topics, diluting retrieval signals; Quick check - Measure topic coherence within decomposed atoms

Synthetic Question Generation: Why needed - Questions naturally align with user query intent; Quick check - Compare question and query embedding similarity distributions

Question Selection: Why needed - Storage efficiency while maintaining coverage; Quick check - Plot recall vs. number of retained questions

Vector Similarity Search: Why needed - Fast approximate nearest neighbor search for high-dimensional embeddings; Quick check - Measure recall@K vs. K trade-off

## Architecture Onboarding

Component Map: Document Chunk -> Atomic Decomposition -> Question Generation -> Question Selection -> Dense Retrieval -> Retrieved Context

Critical Path: User Query → Dense Retrieval (on questions) → Retrieved Atomic Units → Context Assembly → Generation

Design Tradeoffs: Granularity vs. Storage (finer atoms = better retrieval but more questions); Question Quality vs. Quantity (fewer high-quality questions vs. many lower-quality ones)

Failure Signatures: Retrieval misses due to poor atomic decomposition; Reduced recall when question generation quality degrades; Storage inefficiency if question selection is too permissive

First Experiments: 1) Compare recall@1 with different chunk sizes before atomic decomposition; 2) Measure impact of question generation quality on retrieval performance; 3) Evaluate storage vs. recall trade-off with different question selection thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments conducted on benchmark datasets rather than real enterprise document collections
- Quality of synthetic questions depends on LLM performance and may include biases or hallucinations
- Computational overhead of additional LLM processing for atomic decomposition and question generation not comprehensively evaluated

## Confidence
- Retrieval performance improvements: High confidence - clear recall improvements across multiple baselines and datasets
- Storage efficiency claims: Medium confidence - theoretical efficiency gains need real-world validation
- Enterprise applicability: Low confidence - benchmark dataset results may not generalize to enterprise use cases

## Next Checks
1. Deploy atomic retrieval on a real enterprise document collection with domain-specific characteristics to verify performance generalization
2. Conduct comprehensive cost analysis including LLM inference costs for atomic decomposition and question generation
3. Evaluate end-to-end RAG system quality by measuring impact on final generation outputs and task completion metrics