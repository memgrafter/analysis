---
ver: rpa2
title: Reinforcement Learning for Optimizing RAG for Domain Chatbots
arxiv_id: '2401.06800'
source_url: https://arxiv.org/abs/2401.06800
tags:
- queries
- policy
- fetch
- query
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a reinforcement learning approach to optimize
  the number of tokens passed to a large language model in a retrieval-augmented generation
  (RAG) pipeline for domain chatbots. The key idea is to train a policy model, external
  to the RAG, to decide whether to fetch FAQ context or skip retrieval for each user
  query.
---

# Reinforcement Learning for Optimizing RAG for Domain Chatbots

## Quick Facts
- arXiv ID: 2401.06800
- Source URL: https://arxiv.org/abs/2401.06800
- Authors: Mandar Kulkarni; Praveen Tangarajan; Kyung Kim; Anusua Trivedi
- Reference count: 4
- One-line primary result: RL policy model achieves ~31% token savings while slightly improving accuracy compared to standard RAG pipeline

## Executive Summary
This paper presents a reinforcement learning approach to optimize token usage in retrieval-augmented generation (RAG) pipelines for domain chatbots. The key innovation is training a policy model external to the RAG system to decide whether to fetch FAQ context or skip retrieval for each user query. By using GPT-4 as a reward model and training via policy gradient methods, the system learns to fetch context only when necessary, achieving significant token savings while maintaining or slightly improving accuracy compared to the standard RAG approach.

## Method Summary
The approach combines an in-house retrieval embedding model trained with infoNCE loss and a policy model (BERT or GPT-2) that decides whether to fetch FAQ context. The policy model takes the current query, conversation history, and previous actions as input and outputs probabilities for [FETCH] or [NO_FETCH] actions. When [NO_FETCH] is chosen, the LLM answers based on conversation history alone. Training uses policy gradient with rewards from GPT-4 evaluations of answer quality. The system also employs a similarity threshold (0.92) to avoid LLM calls for high-confidence queries before the policy model acts.

## Key Results
- Achieves ~31% reduction in tokens passed to LLM compared to standard RAG pipeline
- Slight accuracy improvement over standard RAG approach
- Effective handling of follow-up queries, same-topic chains, and out-of-domain queries
- Combined approach (similarity threshold + RL policy) outperforms either method alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A policy network external to the RAG pipeline can decide to skip context retrieval for certain query patterns without degrading accuracy.
- Mechanism: The policy model takes as input the current query, previous queries, and previous actions, and outputs a probability distribution over [FETCH] and [NO_FETCH]. For [NO_FETCH], the LLM is called with empty context, but the prompt structure (including conversation history) allows it to answer based on prior context.
- Core assumption: Certain query sequences (follow-ups, same-topic chains, OOD) do not require new FAQ context to produce correct answers.

### Mechanism 2
- Claim: Using GPT-4 as a reward model with carefully shaped numeric rewards enables effective policy gradient training even with limited training sessions.
- Mechanism: GPT-4 evaluates generated answers as Good/Bad based on adherence to FAQ context. These ratings are mapped to rewards: high positive for [NO_FETCH] leading to Good answers, small positive for [FETCH], negative for Bad answers. The policy model is trained via policy gradient using cumulative discounted rewards.
- Core assumption: GPT-4 evaluations are reliable and consistent enough to serve as a proxy for human judgment in reward shaping.

### Mechanism 3
- Claim: Combining a similarity threshold on top-1 retrieval score with the RL policy yields greater token savings than either method alone.
- Mechanism: Before the policy model acts, if the top-1 FAQ context similarity exceeds a threshold (0.92), the system directly returns the FAQ answer without any LLM call. Otherwise, the policy model decides whether to fetch or skip. This filters out trivial, high-confidence cases.
- Core assumption: The in-house embedding model trained with infoNCE loss provides discriminative similarity scores that can reliably indicate when a query is answerable without LLM reasoning.

## Foundational Learning

- Concept: Reinforcement Learning Policy Gradient
  - Why needed here: The problem is framed as sequential decision making under uncertainty, where the policy model must learn to choose actions based on delayed, sparse rewards from GPT-4.
  - Quick check question: What is the role of the discount factor γ in the cumulative reward calculation?

- Concept: Contrastive Learning with InfoNCE Loss
  - Why needed here: The in-house embedding model is trained to maximize similarity between query and relevant FAQ while pushing apart irrelevant pairs, enabling accurate retrieval and similarity-based filtering.
  - Quick check question: How does the InfoNCE loss differ from triplet loss in terms of training data requirements?

- Concept: RAG Pipeline with Conversation History
  - Why needed here: The system must maintain multi-turn context to answer follow-up queries, and the policy model must consider prior queries and actions as part of its state.
  - Quick check question: Why is it important to include FAQ context in the conversation history for follow-up query handling?

## Architecture Onboarding

- Component map: User query → Embedding model → Similarity ranking → Policy model (external) → Action [FETCH]/[NO_FETCH] → LLM with or without context → GPT-4 evaluator → Reward → Policy update
- Critical path: Query → Embedding similarity → Policy decision → LLM call (or skip) → Answer generation → GPT-4 evaluation (only if NO_FETCH)
- Design tradeoffs:
  - Using an external policy model avoids modifying the RAG internals but adds latency and complexity.
  - Relying on GPT-4 for rewards increases cost but provides high-quality feedback.
  - The similarity threshold reduces calls but may miss edge cases if not tuned.
- Failure signatures:
  - High accuracy but low token savings → Policy too conservative (always FETCH).
  - Low accuracy → Policy overuses NO_FETCH or similarity threshold too aggressive.
  - High GPT-4 evaluation variance → Reward signal unstable, policy learning erratic.
- First 3 experiments:
  1. Run with All FETCH baseline to establish token usage and accuracy.
  2. Apply SimThr only (threshold 0.92) to measure standalone savings.
  3. Combine SimThr + policy model (BERT) to measure combined effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed policy-based approach perform on larger FAQ datasets and more complex domains beyond credit cards?
- Basis in paper: [inferred] The paper demonstrates results for a credit card FAQ dataset with 72 FAQs. It states that the approach is generic and can be experimented with any existing RAG pipeline, but does not provide results for larger or more complex domains.
- Why unresolved: The paper only provides results for a specific credit card FAQ dataset. Scaling the approach to larger datasets and more complex domains may introduce new challenges and affect the performance of the policy model.
- What evidence would resolve it: Conducting experiments with larger FAQ datasets and more complex domains, such as healthcare or legal, and comparing the token savings and accuracy of the policy-based approach to the standard RAG pipeline would provide insights into the scalability and generalizability of the proposed approach.

### Open Question 2
- Question: How does the choice of reward shaping affect the performance of the policy model in terms of token savings and accuracy?
- Basis in paper: [explicit] The paper mentions experimenting with different reward shaping strategies, including one where a positive reward is given for the [NO_FETCH] action on the same scale as the negative reward. It notes that this led to lower token savings compared to the initial reward shaping.
- Why unresolved: The paper only explores a limited set of reward shaping strategies. The impact of different reward shaping choices on the performance of the policy model is not fully understood.
- What evidence would resolve it: Conducting a comprehensive study on the effect of different reward shaping strategies on the token savings and accuracy of the policy model would provide insights into the optimal reward shaping for different scenarios and domains.

### Open Question 3
- Question: How does the performance of the policy model change when using different types of LLM models, such as open-source models like LLaMA or proprietary models like GPT-3.5, instead of ChatGPT?
- Basis in paper: [inferred] The paper uses ChatGPT as the LLM model for generating answers and evaluating the quality of the bot responses using GPT-4. It mentions that the proposed approach can be used with any existing RAG pipeline, implying that different LLM models can be used.
- Why unresolved: The paper only provides results using ChatGPT and GPT-4. The performance of the policy model may vary when using different LLM models due to differences in model architecture, training data, and capabilities.
- What evidence would resolve it: Conducting experiments using different LLM models, such as LLaMA, GPT-3.5, or other proprietary models, and comparing the token savings and accuracy of the policy-based approach to the standard RAG pipeline would provide insights into the impact of the choice of LLM model on the performance of the policy model.

## Limitations

- Reliance on GPT-4 for reward modeling introduces cost and potential evaluation inconsistency
- Limited training data (168 queries) raises questions about generalizability to larger conversational datasets
- In-house embedding model's out-of-domain performance is asserted but not empirically validated
- No systematic analysis of GPT-4 evaluation consistency across multiple runs

## Confidence

- **High confidence**: The core RL framework design (policy model external to RAG, policy gradient training, reward shaping from GPT-4) is well-specified and technically sound.
- **Medium confidence**: The claim of ~31% token savings is supported by results, but the generalizability across different domains and query distributions remains uncertain.
- **Medium confidence**: The accuracy improvement claim is supported by test results, but the small dataset size (1014 test queries) limits statistical significance.
- **Low confidence**: The assertion that GPT-4 evaluations are consistently reliable for reward shaping lacks systematic validation.

## Next Checks

1. **Reward Signal Stability Test**: Run 10-20 repeated evaluations of the same generated answers using GPT-4 to measure variance in Good/Bad ratings and correlate this with policy model training stability.

2. **Embedding Model Discrimination Analysis**: Create a test set of clearly in-domain vs. out-of-domain queries and measure the top-1 similarity score distributions to validate the claim that the in-house model provides better discrimination than general-purpose embeddings.

3. **Policy Transferability Experiment**: Train the policy model on one domain (e.g., credit cards) and evaluate its performance on a different domain (e.g., banking services) to assess generalizability and identify potential overfitting to domain-specific query patterns.