---
ver: rpa2
title: 'HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild'
arxiv_id: '2403.04307'
source_url: https://arxiv.org/abs/2403.04307
tags:
- arxiv
- preprint
- hallucinations
- hallucination
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HaluEval-Wild, the first benchmark designed
  to evaluate large language model (LLM) hallucinations in real-world scenarios. The
  benchmark is constructed by curating 500 challenging user queries from the ShareGPT
  dataset, adversarially filtered to ensure difficulty, and categorized into five
  types.
---

# HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild

## Quick Facts
- **arXiv ID:** 2403.04307
- **Source URL:** https://arxiv.org/abs/2403.04307
- **Reference count:** 16
- **Key outcome:** HaluEval-Wild introduces the first benchmark to evaluate LLM hallucinations in real-world scenarios, revealing significant variance in hallucination rates across models, with GPT-4-Turbo achieving the lowest average rate of 18.64%.

## Executive Summary
This paper introduces HaluEval-Wild, the first benchmark designed to evaluate large language model (LLM) hallucinations in real-world scenarios. The benchmark is constructed by curating 500 challenging user queries from the ShareGPT dataset, adversarially filtered to ensure difficulty, and categorized into five types. Reference answers are synthesized using GPT-4 with retrieval-augmented generation (RAG) to minimize hallucinations. Evaluations across various LLMs reveal significant variance in hallucination rates, with GPT-4-Turbo achieving the lowest average rate of 18.64%, while knowledge-distilled models like Vicuna-13B exhibit higher hallucination tendencies. The study highlights the effectiveness of self-reflection as a mitigation mechanism and underscores the nuanced challenge of balancing model performance with reliability in real-world applications.

## Method Summary
HaluEval-Wild is constructed by curating 500 challenging user queries from ShareGPT, adversarially filtered using Alpaca to ensure difficulty. Queries are categorized into five types: Out-of-Scope Information, Complex Reasoning, Inappropriate Content, Beyond-Modality Interaction, and Confused/Erroneous Queries. Reference answers are synthesized using GPT-4 with retrieval-augmented generation (RAG) to minimize hallucinations. Evaluations are conducted by comparing LLM responses to these reference answers, with hallucination detection performed by GPT-4 acting as a judge.

## Key Results
- GPT-4-Turbo achieved the lowest average hallucination rate of 18.64%, while knowledge-distilled models like Vicuna-13B exhibited higher rates.
- RAG significantly reduced GPT-4's hallucination rate from 20% to 5% when generating reference answers.
- Self-reflection mitigation was effective, especially when hints about the query type were provided, showing a general trend of decreasing hallucination ratios.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial filtering ensures query difficulty
- **Mechanism:** Queries are filtered through Alpaca, an elementary-aligned LLM, to retain only those that induce hallucinations, guaranteeing that HaluEval-Wild evaluates challenging real-world interactions.
- **Core assumption:** Alpaca reliably produces hallucinations on the same query types that challenge stronger LLMs.
- **Evidence anchors:**
  - [abstract] "We meticulously collect challenging (adversarially filtered by Alpaca) user queries from ShareGPT..."
  - [section] "To ensure the queries are sufficiently challenging, we prompt Alpaca... and filter out those that do not elicit hallucinations from Alpaca."
- **Break condition:** If Alpaca's alignment shifts or its hallucination patterns diverge from stronger models, the filtering may no longer guarantee difficulty for target LLMs.

### Mechanism 2
- **Claim:** RAG reduces hallucination in reference answers
- **Mechanism:** Retrieval-augmented generation supplements GPT-4 with external passages, grounding its responses and reducing hallucination rates compared to direct generation.
- **Core assumption:** External retrieval provides sufficiently relevant and accurate context to guide GPT-4 away from hallucination.
- **Evidence anchors:**
  - [abstract] "We also use retrieval-augmented generation (RAG) to produce the reference answers."
  - [section] "Results show that without RAG, GPT-4 has a 20% hallucination rate while it falls to 5% when using RAG."
- **Break condition:** If retrieved passages are irrelevant, outdated, or introduce misinformation, RAG could fail to reduce hallucinations or even introduce new errors.

### Mechanism 3
- **Claim:** Self-reflection mitigates hallucinations effectively
- **Mechanism:** By prompting LLMs to identify and correct their own hallucinations using prior errors, hallucination rates decrease, especially when hints about the query type are provided.
- **Core assumption:** LLMs can recognize and correct their own hallucinations when given appropriate feedback.
- **Evidence anchors:**
  - [abstract] "We provide the NLP community with a comprehensive benchmark to evaluate and enhance the robustness of language models..."
  - [section] "Results & Analysis The hallucination rates of direct generation, self-reflection, and hinted self-reflection are illustrated in Figure 2. There is a general trend of decreasing hallucination ratios..."
- **Break condition:** If the LLM cannot accurately identify its own hallucinations, self-reflection will fail to reduce hallucination rates.

## Foundational Learning

- **Concept:** Adversarial filtering
  - Why needed here: To ensure that the benchmark contains only challenging queries that are likely to induce hallucinations in LLMs.
  - Quick check question: How does adversarial filtering with Alpaca guarantee that collected queries are difficult for other LLMs?

- **Concept:** Retrieval-augmented generation (RAG)
  - Why needed here: To ground the reference answers in external knowledge and reduce the hallucination rate of the reference generator.
  - Quick check question: What role does external retrieval play in reducing hallucination in reference answers?

- **Concept:** Self-reflection as a mitigation strategy
  - Why needed here: To provide a concrete mechanism for reducing hallucinations in LLM outputs, evaluated as part of the benchmark.
  - Quick check question: How does self-reflection help LLMs identify and correct their own hallucinations?

## Architecture Onboarding

- **Component map:** Query collection (ShareGPT → Alpaca filtering → manual verification) → categorization (GPT-4 classification → manual verification) → reference answer generation (RAG + GPT-4) → hallucination evaluation (GPT-4-as-judge)
- **Critical path:** Query collection → adversarial filtering → manual verification → categorization → reference answer generation → hallucination evaluation
- **Design tradeoffs:** Using GPT-4 for categorization and evaluation may introduce bias if GPT-4 itself hallucinates; manual verification adds rigor but is time-consuming
- **Failure signatures:** High false-negative rate in query filtering, mis-categorization of queries, reference answers with hallucinations, inconsistent evaluation by GPT-4-as-judge
- **First 3 experiments:**
  1. Validate Alpaca filtering by testing filtered queries on multiple LLMs and comparing hallucination rates
  2. Test RAG effectiveness by comparing hallucination rates of GPT-4 with and without RAG on a sample of queries
  3. Evaluate self-reflection mitigation by applying it to LLM responses and measuring changes in hallucination rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HaluEval-Wild compare to other hallucination detection benchmarks when applied to models beyond the ones tested in the paper?
- Basis in paper: [inferred] The paper mentions that HaluEval-Wild is designed to evaluate LLM hallucinations in real-world scenarios, but does not provide a direct comparison to other benchmarks across a wide range of models.
- Why unresolved: The paper only evaluates a limited set of models (GPT-4-Turbo, GPT-3.5-Turbo, Mixtral 8x7B, Mistral 7B, Llama-2-Chat, Vicuna 13B, and Alpaca 7B) on HaluEval-Wild. It would be valuable to see how the benchmark performs with a broader range of models, including those not specifically mentioned in the paper.
- What evidence would resolve it: Evaluating HaluEval-Wild on a diverse set of models, including those not mentioned in the paper, and comparing the results to other hallucination detection benchmarks.

### Open Question 2
- Question: How does the use of RAG with different retrieval sources (e.g., different search engines, knowledge graphs) impact the accuracy and reliability of reference answers in HaluEval-Wild?
- Basis in paper: [explicit] The paper states that RAG is used to generate reference answers by retrieving relevant passages from an external search engine (DuckDuckGo).
- Why unresolved: The paper does not explore the impact of using different retrieval sources on the quality of reference answers. It is possible that different retrieval sources may provide varying levels of accuracy and relevance, which could affect the overall effectiveness of the benchmark.
- What evidence would resolve it: Conducting experiments using different retrieval sources (e.g., different search engines, knowledge graphs) and comparing the quality of reference answers and the resulting hallucination detection performance.

### Open Question 3
- Question: How does the categorization of queries into five types affect the overall performance of the benchmark, and are there any limitations or biases introduced by this categorization?
- Basis in paper: [explicit] The paper describes the process of categorizing queries into five types (OoS, CR, IC, BM, CE) and using these categories for fine-grained analysis.
- Why unresolved: The paper does not provide a detailed analysis of how the categorization process might impact the benchmark's performance or introduce any biases. It is possible that the categorization could lead to an uneven distribution of query types or overlook certain aspects of hallucinations.
- What evidence would resolve it: Analyzing the distribution of query types and their impact on the benchmark's performance, as well as exploring alternative categorization schemes or approaches to query classification.

## Limitations

- The reliance on GPT-4 as both reference answer generator and hallucination evaluator could introduce bias or circularity in results
- Adversarial filtering effectiveness is uncertain as Alpaca's hallucination patterns may not generalize to stronger LLMs
- RAG effectiveness depends heavily on quality and relevance of retrieved passages, which are not independently verified

## Confidence

- **High confidence**: The benchmark construction methodology and evaluation framework are well-defined and reproducible
- **Medium confidence**: The reported hallucination rates are likely accurate within the constraints of using GPT-4 for evaluation
- **Low confidence**: The effectiveness of self-reflection as a mitigation strategy is supported by results but lacks detailed analysis

## Next Checks

1. **Validate adversarial filtering**: Test the 500 filtered queries on multiple LLMs (e.g., GPT-4, Claude, Llama) and compare hallucination rates to ensure the queries are genuinely challenging across models.

2. **Audit reference answers**: Have human experts independently review a random sample of 50 reference answers generated by GPT-4 with RAG to verify their accuracy and identify any residual hallucinations.

3. **Evaluate evaluator consistency**: Run the hallucination evaluation process on a subset of responses using multiple evaluators (both GPT-4 and human) to assess inter-rater reliability and identify potential biases in the GPT-4-as-judge approach.