---
ver: rpa2
title: 'Synthetica: Large Scale Synthetic Data for Robot Perception'
arxiv_id: '2410.21153'
source_url: https://arxiv.org/abs/2410.21153
tags:
- data
- objects
- object
- images
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for generating large-scale synthetic
  data to train robust object detectors for robotics applications. The key idea is
  to use a photorealistic ray-tracing renderer with extensive randomization and data
  augmentation to create 2.7 million images, enabling training of high-performance
  detection transformers that run at real-time speeds.
---

# Synthetica: Large Scale Synthetic Data for Robot Perception

## Quick Facts
- arXiv ID: 2410.21153
- Source URL: https://arxiv.org/abs/2410.21153
- Authors: Ritvik Singh, Jingzhou Liu, Karl Van Wyk, Yu-Wei Chao, Jean-Francois Lafleche, Florian Shkurti, Nathan Ratliff, Ankur Handa
- Reference count: 40
- Primary result: Achieves SOTA mAP of 0.885 on YCBV object detection benchmark while running 9× faster (50-100Hz) than prior methods

## Executive Summary
This paper presents a method for generating large-scale synthetic data to train robust object detectors for robotics applications. The key innovation is using a photorealistic ray-tracing renderer with extensive randomization and data augmentation to create 2.7 million images, enabling training of high-performance detection transformers that run at real-time speeds. The proposed method achieves state-of-the-art performance on the YCBV object detection benchmark while being 9 times faster than the prior SOTA. Specifically, the model achieves a mAP of 0.885 and mAR of 0.903 on the YCBV dataset, running at 50-100Hz. The authors also demonstrate the practical utility of their approach by developing a pipeline to create custom object detectors using low-quality 3D scans.

## Method Summary
The method generates 2.7 million synthetic images using NVIDIA Omniverse Isaac Sim with Replicator, featuring procedurally generated indoor scenes and HDRI background randomization with extensive rendering randomization. Training-time augmentations including color contrast, brightness, enhancement, random backgrounds, JPEG compression, and shot noise are applied. An RT-DETR detection transformer with ResNet-50 or ConvNext-S backbone is trained on 640×480 images (padded to 640×640) for 20 epochs using AdamW optimizer. Kernel fusion techniques optimize the model for real-time inference speeds of 50-100Hz. The approach also includes a pipeline for creating custom object detectors using low-quality 3D scans from consumer devices.

## Key Results
- Achieves SOTA mAP of 0.885 and mAR of 0.903 on YCBV benchmark
- Runs at 50-100Hz inference speed, 9× faster than prior SOTA
- Demonstrates custom object detector pipeline using AR Code Object Capture app
- Robust performance at high confidence thresholds (0.9) as shown in Figure 5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale synthetic data with extensive randomization and data augmentation bridges the sim-to-real gap for object detection.
- Mechanism: The method generates 2.7 million images using a photorealistic ray-tracing renderer with randomized materials, lighting, and backgrounds. Training-time augmentations (e.g., contrast changes, random backgrounds, noise) further increase the diversity of the training distribution. This comprehensive randomization ensures the model encounters a wide variety of real-world conditions during training, preventing overfitting to a narrow synthetic distribution.
- Core assumption: The synthetic data distribution, when sufficiently randomized, covers the real-world data distribution needed for robust object detection.
- Evidence anchors: [abstract] "Leveraging data from a photorealistic ray-tracing renderer, we scale up data generation, generating 2.7 million images, to train highly accurate real-time detection transformers." [section] "We use NVIDIA Omniverse Isaac Sim with Replicator [30] to generate 2.7M images in simulation. We render data by either procedurally generating an indoor room and dropping objects in it, or dropping objects with a random HDRI background."
- Break condition: If the synthetic data distribution does not adequately cover real-world variations (e.g., novel lighting conditions, occlusions, or object appearances), the model will fail to generalize, leading to poor performance on real-world data.

### Mechanism 2
- Claim: Using a detection transformer (RT-DETR) with kernel fusion techniques enables real-time inference speeds.
- Mechanism: The detection transformer architecture, specifically RT-DETR, treats object detection as a set prediction problem, avoiding hand-designed modules like non-maximum suppression. Kernel fusion techniques optimize the model for inference speed, allowing it to run at 50-100Hz, which is 9 times faster than the prior state-of-the-art.
- Core assumption: The detection transformer architecture is inherently more efficient for real-time inference compared to convolutional-based methods, and kernel fusion techniques can further optimize its speed without significant performance degradation.
- Evidence anchors: [abstract] "We present a collection of rendering randomization and training-time data augmentation techniques conducive to robust sim-to-real performance for vision tasks. We demonstrate state-of-the-art performance on the task of object detection while having detectors that run at 50–100Hz which is 9 times faster than the prior SOTA 3." [section] "We trained an RT-DETR [27] model and experimented with both a ResNet-50 and ConvNext-S backbone. All training images were 640× 480; however, because the model only accepts 640 × 640 inputs, we pad the rest of the tensor with zeros."
- Break condition: If the kernel fusion techniques introduce significant overhead or if the detection transformer architecture does not scale well to the specific object detection task, the inference speed may not be as fast as claimed, making it unsuitable for real-time robotics applications.

### Mechanism 3
- Claim: The proposed pipeline enables the creation of custom object detectors using low-quality 3D scans.
- Mechanism: The authors use the AR Code Object Capture 3D Scan app on iPhone Pro or iPad Pro models with a LiDAR sensor to scan objects in the real world. The scanned assets are then converted to USD format and imported into Isaac Sim for data generation, similar to how data was generated for the YCB objects. This allows for the creation of detectors for arbitrary objects for which there are no prior real-world datasets.
- Core assumption: Low-quality 3D scans from consumer devices are sufficient to generate synthetic data that enables the model to generalize to the real world.
- Evidence anchors: [abstract] "We further demonstrate the usefulness of our training methodology for robotics applications by showcasing a pipeline for use in the real world with custom objects for which there do not exist prior datasets." [section] "In order to demonstrate the practicality of our pipeline in a real-world setting, we showcase how detectors can be trained for arbitrary objects. The pipeline is shown in Figure 9. First, a complete scan of an object can be made using the AR Code Object Capture 3D Scan app, available on all iPhone Pro or iPad Pro models with a LiDAR sensor."
- Break condition: If the low-quality 3D scans do not capture sufficient detail or geometry of the objects, the generated synthetic data may not accurately represent the real-world objects, leading to poor performance of the custom object detectors.

## Foundational Learning

- Concept: Sim-to-real transfer
  - Why needed here: Understanding how to bridge the gap between synthetic and real-world data is crucial for the success of the proposed method. The method relies on generating large-scale synthetic data to train object detectors that can generalize to real-world scenarios.
  - Quick check question: What are the key challenges in sim-to-real transfer, and how does the proposed method address them?

- Concept: Object detection architectures (CNNs vs. Transformers)
  - Why needed here: The method uses a detection transformer (RT-DETR) instead of traditional convolutional neural networks (CNNs). Understanding the differences between these architectures and their respective strengths and weaknesses is important for appreciating the design choices made in the paper.
  - Quick check question: What are the advantages of using a detection transformer over a CNN-based object detector for real-time robotics applications?

- Concept: Data augmentation techniques
  - Why needed here: The method employs extensive data augmentation during training to increase the diversity of the training distribution. Understanding the different types of data augmentation and their effects on model performance is crucial for evaluating the effectiveness of the proposed approach.
  - Quick check question: How do different data augmentation techniques (e.g., color jittering, random backgrounds, noise) contribute to the robustness of the object detector in real-world scenarios?

## Architecture Onboarding

- Component map: Synthetic data generation pipeline -> RT-DETR model training -> Kernel fusion optimization -> Custom object detector pipeline
- Critical path: 1. Generate synthetic data using Isaac Sim with randomized materials, lighting, and backgrounds. 2. Apply training-time data augmentations to further increase diversity. 3. Train the RT-DETR model on the synthetic data. 4. Optimize the model for real-time inference using kernel fusion techniques. 5. Evaluate the model on the YCBV benchmark and demonstrate its performance on custom objects.
- Design tradeoffs: Synthetic data vs. real data (eliminates need for real-world data collection but requires careful randomization), Detection transformer vs. CNN (better scalability but more computational resources), Real-time inference vs. accuracy (optimizing for speed may slightly compromise accuracy but crucial for robotics).
- Failure signatures: Poor performance on real-world data (synthetic data distribution doesn't cover real-world variations), Slow inference speeds (kernel fusion techniques ineffective or model architecture unsuitable), Inaccurate detections on custom objects (low-quality 3D scans don't capture sufficient detail).
- First 3 experiments: 1. Train RT-DETR model with ResNet-50 backbone on synthetic data and evaluate on YCBV benchmark. 2. Train RT-DETR model with ConvNext-S backbone on synthetic data and compare to ResNet-50 model. 3. Implement custom object detector pipeline using AR Code Object Capture app and evaluate performance on real-world objects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can synthetic data generation be extended to handle unseen objects for open-vocabulary object detection in robotics?
- Basis in paper: [explicit] The paper explicitly states that synthetic data generation is restricted to objects for which CAD models exist, and does not work on unseen objects. It suggests that scaling synthetic data for open-vocabulary detectors like LLava and GLEE is a promising research direction.
- Why unresolved: Current synthetic data generation methods rely on having 3D models of objects, which limits their applicability to novel objects that are not pre-scanned or modeled. This is a fundamental limitation for real-world robotics applications where objects can be arbitrary and unseen during training.
- What evidence would resolve it: Demonstrating a method that can generate realistic synthetic data for arbitrary objects without requiring pre-existing 3D models, and showing that detectors trained on this data can generalize to novel objects in real-world settings.

### Open Question 2
- Question: What is the optimal balance between precision and recall for object detection metrics in robotics applications, and how can it be quantified?
- Basis in paper: [explicit] The paper argues that mean average precision (mAP) may not be optimal for robotics applications because false positives can be detrimental to safety. It suggests that a new metric is needed that appropriately weighs robot safety and task completion, potentially using an F-score where β < 1 to prioritize precision over recall.
- Why unresolved: Current evaluation metrics like mAP do not account for the safety-critical nature of robotics applications, where false positives can lead to dangerous interactions with the environment. There is no established metric that balances precision and recall in a way that reflects the priorities of robotics tasks.
- What evidence would resolve it: Developing and validating a new evaluation metric that better reflects the trade-off between precision and recall in robotics applications, and demonstrating its effectiveness in improving safety and task completion compared to mAP.

### Open Question 3
- Question: How can active learning be integrated into synthetic data generation to reduce redundancy and focus on generating images that are most useful for training?
- Basis in paper: [explicit] The paper mentions that the generated data was IID (independent and identically distributed) and was agnostic of the model. It suggests that future work could investigate active learning methods to bring model learning in-the-loop of data generation to increase the amount of useful images and reduce redundant information in the dataset.
- Why unresolved: Current synthetic data generation methods produce large amounts of data without considering which images are most informative for training the model. This can lead to inefficiencies and overfitting to irrelevant aspects of the data distribution.
- What evidence would resolve it: Implementing an active learning framework that iteratively generates synthetic data based on the model's current weaknesses, and demonstrating that this approach leads to faster convergence and better generalization compared to generating IID data.

## Limitations

- The sim-to-real transfer mechanism through randomization lacks ablation studies to quantify individual technique contributions to performance
- The 9× faster inference speed claim lacks detailed benchmarking evidence beyond comparison to a single prior work
- The custom object detector pipeline using low-quality 3D scans is demonstrated conceptually without quantitative evaluation of detector performance

## Confidence

- High confidence: The claim that the method achieves SOTA performance on YCBV (mAP 0.885, mAR 0.903) - this is directly measured and reported with specific benchmark protocols
- Medium confidence: The 9× faster inference speed claim - while stated, lacks detailed benchmarking evidence beyond comparison to a single prior work
- Low confidence: The sim-to-real transfer mechanism through randomization - no ablation studies or quantitative analysis of how individual randomization techniques contribute to performance

## Next Checks

1. Conduct an ablation study systematically removing different randomization and augmentation techniques to quantify their individual contributions to sim-to-real transfer performance
2. Perform detailed runtime profiling of the RT-DETR model with kernel fusion on representative robotics hardware to verify the claimed 50-100Hz inference speeds
3. Evaluate the custom object detector pipeline by training detectors on multiple scanned objects using the AR Code Object Capture app and measuring real-world detection accuracy against ground truth annotations