---
ver: rpa2
title: Multi-LLM QA with Embodied Exploration
arxiv_id: '2406.10918'
source_url: https://arxiv.org/abs/2406.10918
tags:
- agents
- agent
- answer
- arxiv
- embodied
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using multiple LLM-based embodied agents
  to answer questions about unknown environments, a gap in prior research focused
  on single-agent or zero-shot approaches. The proposed Multi-Embodied LLM Explorers
  (MELE) framework has agents independently explore and then answer binary questions
  about their observations.
---

# Multi-LLM QA with Embodied Exploration

## Quick Facts
- arXiv ID: 2406.10918
- Source URL: https://arxiv.org/abs/2406.10918
- Reference count: 20
- Primary result: CAM aggregation achieves up to 46% higher accuracy than majority voting or debate in multi-agent embodied QA

## Executive Summary
This paper addresses the challenge of multi-agent embodied question answering in unknown household environments. Unlike prior work focused on single-agent or zero-shot approaches, the proposed Multi-Embodied LLM Explorers (MELE) framework uses multiple LLM-based agents to independently explore and answer binary questions about their observations. The key innovation is a trained Central Answer Model (CAM) that aggregates agent responses, consistently outperforming simpler methods like majority voting and debate.

## Method Summary
The MELE framework consists of an exploration phase where multiple LLM agents independently navigate Matterport3D environments using Language-Guided Exploration (LGX), collecting observations via GLIP object detection. Each agent then answers binary questions based on its observations. Three aggregation methods are compared: majority voting, debate, and a trained Central Answer Model (CAM) using various classifiers (NN, RF, DT, XGBoost, SVM, Logistic Regression). CAM is trained on a 90-10 train-test split across 5 random seeds.

## Key Results
- CAM achieves up to 46% higher accuracy than majority voting or debate aggregation methods
- Debate aggregation performs worse than majority voting in all but one environment due to persuasive incorrect agents swaying correct ones
- Feature importance analysis shows CAM learns to rely less on individual agent responses, even when those agents have relevant observations

## Why This Works (Mechanism)

### Mechanism 1
Multiple agents with independent exploration reduce blind spots and increase environmental coverage compared to a single agent. Each agent navigates a subset of nodes in the Matterport3D graph, generating diverse observations. Aggregating their responses allows complementary knowledge to fill gaps. Core assumption: Agents explore sufficiently distinct regions and their individual observations are complementary rather than redundant. Evidence anchors: [abstract] "In existing works, only a single agent explores the environment with a limited field of view, inducing low coverage and high exploration costs." Break condition: If agents' exploration paths heavily overlap, redundancy cancels out the coverage benefit.

### Mechanism 2
A learned central classifier (CAM) outperforms simple voting/aggregation because it can learn to downweight unreliable or biased agents. CAM receives both the query context (object, room) and the raw agent responses, learns a mapping to the ground-truth label, and can ignore noisy inputs through training. Core assumption: The training data includes enough labeled examples for CAM to learn the relationship between query features, agent responses, and ground truth. Evidence anchors: [abstract] "Using CAM, we observe a 46% higher accuracy compared against the other non-learning-based aggregation methods." Break condition: If agent responses are highly correlated or training data is insufficient, CAM may not learn to distinguish signal from noise.

### Mechanism 3
Debate aggregation can be misled by persuasive but incorrect agents, leading to lower accuracy than majority voting in some cases. Agents exchange arguments and try to convince each other; however, a well-spoken incorrect agent can sway others, propagating error. Core assumption: LLMs' persuasiveness correlates with correctness, which is not always true. Evidence anchors: [abstract] "Interestingly, in all but one of the environments, debating does slightly worse on average than MV." Break condition: If all agents are equally correct or all incorrect, debate may not degrade performance.

## Foundational Learning

- **Permutation Feature Importance (PFI)**: Why needed here: To quantify how much each agent's response contributes to CAM's decision, revealing reliance on noisy or biased agents. Quick check question: If you randomly shuffle one agent's responses and accuracy drops significantly, what does that say about that agent's feature importance?

- **Embodied Question Answering (EQA) vs traditional QA**: Why needed here: Understanding the difference between zero-shot QA and QA requiring active exploration of a physical environment is key to framing the problem. Quick check question: In EQA, does the agent answer questions without ever moving, or must it explore first?

- **Multi-agent exploration in graph environments**: Why needed here: Agents traverse nodes in a graph representation of a house; understanding this setup is critical for designing exploration policies. Quick check question: If the environment is represented as a graph, what does a node represent in the context of embodied exploration?

## Architecture Onboarding

- **Component map**: Exploration Phase -> Observation Collection -> Question Answering Phase -> Aggregation Phase -> Evaluation
- **Critical path**: 1. Agents explore environment independently. 2. Agents answer questions based on their observations. 3. CAM (or baseline) aggregates responses. 4. Final answer output and accuracy measured.
- **Design tradeoffs**: Exploration time vs coverage: More steps increase coverage but also cost. CAM complexity vs interpretability: Complex models (NN, XGBoost) may perform better but are harder to interpret than decision trees. Communication vs independence: Debate allows discussion but adds latency and risk of persuasion errors; independent voting is faster but cannot resolve disagreements.
- **Failure signatures**: Low CAM accuracy despite good training data → agents' observations too similar or biased. Debate accuracy lower than MV → persuasive incorrect agents are influencing others. High variance in CAM performance across environments → overfitting to specific environments or poor generalization.
- **First 3 experiments**: 1. Run CAM with only majority vote aggregation baseline to confirm accuracy gain. 2. Introduce one malicious agent and compare CAM vs MV vs debate robustness. 3. Vary number of exploration steps and measure impact on CAM accuracy to find optimal coverage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CAM-based aggregation methods scale with the number of agents in the Multi-LLM system?
- Basis in paper: [inferred] The paper evaluates CAM with 3 agents and compares it to majority voting and debate, but does not explore the impact of varying the number of agents on CAM's performance.
- Why unresolved: The paper focuses on a fixed number of agents (3) and does not investigate how increasing or decreasing the number of agents affects the accuracy and robustness of CAM-based methods.
- What evidence would resolve it: Experiments testing CAM with different numbers of agents (e.g., 2, 4, 5) and comparing their performance to majority voting and debate baselines would provide insights into the scalability of CAM-based methods.

### Open Question 2
- Question: Can CAM-based aggregation methods be effectively applied to dynamic household environments where objects and their locations frequently change?
- Basis in paper: [explicit] The paper mentions that obtaining ground truth labels for training CAM can be difficult in dynamic environments and suggests this as a limitation for future work.
- Why unresolved: The paper does not explore how CAM-based methods perform in environments with non-stationary objects, which is a common scenario in real-world households.
- What evidence would resolve it: Experiments evaluating CAM-based methods in simulated or real-world dynamic environments, where objects are frequently moved or added, would demonstrate their adaptability and robustness to changing conditions.

### Open Question 3
- Question: How do CAM-based aggregation methods compare to other advanced ensemble techniques, such as Bayesian model averaging or deep ensemble methods, in the context of embodied question-answering?
- Basis in paper: [inferred] The paper compares CAM to majority voting and debate but does not explore other ensemble techniques that could potentially offer better performance or robustness.
- Why unresolved: The paper focuses on a specific set of aggregation methods and does not investigate whether other advanced ensemble techniques could outperform CAM in embodied question-answering tasks.
- What evidence would resolve it: Comparative experiments between CAM-based methods and other ensemble techniques, such as Bayesian model averaging or deep ensemble methods, in various embodied question-answering scenarios would provide insights into their relative strengths and weaknesses.

## Limitations

- Performance may not scale well to larger environments with more agents
- Debate mechanism is vulnerable to persuasive but incorrect agents
- Results are specific to Matterport3D household scans and may not generalize to other environment types

## Confidence

- **High**: CAM consistently outperforms majority voting and debate aggregation in accuracy gains
- **Medium**: Feature importance analysis correctly identifies CAM's reliance on individual agent responses, though the exact mechanism of noise filtering requires further validation
- **Low**: The assumption that independent exploration leads to sufficient coverage without overlap is not empirically validated across all tested environments

## Next Checks

1. **Coverage Validation**: Measure and compare the overlap in exploration paths across agents to quantify the actual diversity of observations collected
2. **Cross-Environment Generalization**: Test CAM performance on a different embodied QA dataset or environment type to assess robustness beyond Matterport3D households
3. **Ablation Study**: Remove or randomize specific agent responses during CAM training to isolate which agents contribute most to accuracy and identify potential biases