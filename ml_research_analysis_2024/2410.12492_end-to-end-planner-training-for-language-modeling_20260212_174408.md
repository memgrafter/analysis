---
ver: rpa2
title: End-to-end Planner Training for Language Modeling
arxiv_id: '2410.12492'
source_url: https://arxiv.org/abs/2410.12492
tags:
- planner
- language
- training
- arxiv
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling joint end-to-end
  training of a high-level planner module and a low-level language model (LM) in language
  modeling systems. The core method idea is to replace the non-differentiable hard
  selection of abstract writing actions with a differentiable soft-selection via weighted
  averages of action embeddings, using the planner-predicted action probabilities
  as mixing weights.
---

# End-to-end Planner Training for Language Modeling

## Quick Facts
- arXiv ID: 2410.12492
- Source URL: https://arxiv.org/abs/2410.12492
- Authors: Nathan Cornille; Florian Mai; Jingyuan Sun; Marie-Francine Moens
- Reference count: 19
- One-line primary result: Soft-selection method improves perplexity by 0.3 for GPT-2 and 0.08 for OLMo-1B compared to hard selection baselines

## Executive Summary
This paper addresses the challenge of enabling joint end-to-end training of a high-level planner module and a low-level language model (LM) in language modeling systems. The core innovation is replacing non-differentiable hard selection of abstract writing actions with a differentiable soft-selection via weighted averages of action embeddings, using the planner-predicted action probabilities as mixing weights. This approach allows for exact gradient computation and enables the LM to leverage the full label distribution from the planner, resulting in consistent perplexity improvements over prior non-differentiable approaches.

## Method Summary
The method proposes a differentiable approach to planner-LM joint training by replacing hard selection of discrete actions with soft selection. Instead of selecting the single most probable action embedding using argmax (which blocks gradient flow), the model computes a weighted sum of all action embeddings using the planner's predicted probabilities as mixing weights. This enables gradients to flow back through the entire planner network during LM fine-tuning. The approach also addresses catastrophic forgetting by carefully timing when the planner is unfrozen during training, preserving its high-level abstract knowledge while allowing adaptation to the LM's needs.

## Key Results
- Soft-selection improves perplexity by 0.3 for GPT-2 and 0.08 for OLMo-1B compared to hard selection baselines
- Soft-selection outperforms straight-through estimators in both perplexity and generation quality metrics
- Proper timing of planner unfreezing (halfway through training) is crucial for preventing catastrophic forgetting
- Abstract pretraining objective of the planner remains necessary even with end-to-end training enabled

## Why This Works (Mechanism)

### Mechanism 1: Differentiable Soft-Selection Replaces Non-Differentiable Hard Selection
The core innovation enables end-to-end training by replacing hard selection of discrete actions with a differentiable weighted average of action embeddings. Instead of selecting the single most probable action embedding using an argmax operation (which blocks gradient flow), the model computes a weighted sum of all action embeddings using the planner's predicted probabilities as mixing weights. This allows gradients to flow back through the entire planner network during LM fine-tuning.

### Mechanism 2: Preservation of Full Label Distribution Information
Soft selection allows the LM to access the full planner-predicted probability distribution rather than just the most likely action. By computing a weighted average using all action probabilities, the LM receives information about the planner's uncertainty and the relative likelihoods of multiple possible actions, not just the single highest-probability choice.

### Mechanism 3: Prevention of Catastrophic Forgetting Through Controlled Unfreezing
The planner's high-level knowledge is preserved by unfreezing it only after initial LM adaptation or by maintaining the planner's original objective during joint training. Either unfreezing the planner halfway through training or continuing to train it on its Next-Action Prediction objective alongside the LM's Next-Token Prediction objective prevents the planner from losing its abstract planning capabilities while still allowing it to adapt to the LM.

## Foundational Learning

- Concept: Non-differentiable operations and gradient estimation
  - Why needed here: Understanding why argmax operations block gradient flow and why methods like straight-through estimators are used to approximate gradients
  - Quick check question: Why can't we directly backpropagate through an argmax operation that selects the most probable action?

- Concept: Softmax and weighted averaging
  - Why needed here: The core mechanism uses softmax probabilities as weights for computing weighted averages of embeddings
  - Quick check question: How does using softmax probabilities as mixing weights differ from selecting a single action embedding?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why the planner might lose its abstract planning knowledge when fine-tuned on token prediction
  - Quick check question: What happens to a neural network's learned representations when it's trained on a new objective without preserving the old one?

## Architecture Onboarding

- Component map: Context → Planner → Soft-mixed Action Embeddings → LM → Next Token Prediction
- Critical path: The planner processes context to produce action probabilities, these probabilities weight the action embeddings, the weighted sum is injected into the LM, and the LM predicts the next token conditioned on this planner signal
- Design tradeoffs:
  - Hard selection vs soft selection: Hard selection is simpler but blocks gradients; soft selection enables end-to-end training but requires more computation
  - Immediate vs delayed unfreezing: Immediate allows faster adaptation but risks forgetting; delayed preserves knowledge but slows adaptation
  - Action space size: Larger action spaces provide more granularity but increase computation and may require more training data
- Failure signatures:
  - Perplexity not improving: Could indicate the planner isn't providing useful signals or the mixing weights aren't being computed correctly
  - Planner losing its pretraining knowledge: Would manifest as worse Next-Action Prediction accuracy during joint training
  - Training instability: Could occur if unfreezing timing is incorrect or if the soft selection creates exploding/vanishing gradients
- First 3 experiments:
  1. Verify soft selection implementation by comparing perplexity with hard selection baseline
  2. Test different unfreezing schedules (immediate vs halfway vs never) to find optimal balance
  3. Validate that the planner maintains its Next-Action Prediction accuracy during joint training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed end-to-end training method for planner and language model maintain or improve performance when scaled to larger models like GPT-3 or GPT-4?
- Basis in paper: [explicit] The paper notes that due to computational constraints, experiments were conducted on relatively small models (GPT-2 and OLMo-1B), and the scalability and effectiveness of the method need to be validated on production-scale models
- Why unresolved: The paper's experiments were limited to smaller models due to computational constraints, leaving the performance of the method on larger models untested
- What evidence would resolve it: Conducting experiments on larger models like GPT-3 or GPT-4 and comparing their performance with and without the proposed end-to-end training method

### Open Question 2
- Question: How does the performance of the proposed method change when the planning horizon is extended beyond one step into the future?
- Basis in paper: [explicit] The paper mentions that the current approach involves planning only one step into the future and suggests that future work should investigate methods to extend the planning horizon
- Why unresolved: The paper does not explore the effects of extending the planning horizon, leaving the impact on performance unknown
- What evidence would resolve it: Implementing and testing the method with extended planning horizons and evaluating the changes in performance metrics

### Open Question 3
- Question: What is the impact of using scheduled sampling on the trade-off between perplexity and generation quality?
- Basis in paper: [explicit] The paper explores the trade-off between perplexity and generation metrics by varying the fraction of oracle and planner-predicted actions during training, including a scheduled sampling approach
- Why unresolved: While the paper investigates the trade-off, it does not fully resolve how scheduled sampling impacts the balance between perplexity and generation quality
- What evidence would resolve it: Conducting experiments with different scheduled sampling strategies and analyzing their effects on both perplexity and generation metrics

## Limitations
- The perplexity improvements (0.3 for GPT-2, 0.08 for OLMo-1B) are relatively modest and may not justify the added complexity for all applications
- Lack of comprehensive ablation studies on action space design leaves questions about optimal action granularity and clustering methods
- Evaluation scope is limited, with insufficient analysis of how the planner affects other important language modeling properties beyond perplexity

## Confidence

**High Confidence (5/5)**: The claim that soft-selection enables differentiable end-to-end training of planner and LM is well-supported by the mathematical formulation and experimental results.

**Medium Confidence (3/5)**: The claim about preventing catastrophic forgetting through controlled unfreezing has moderate support from the ablation studies, but the analysis could be deeper.

**Low Confidence (2/5)**: The claim that the planner's full probability distribution provides valuable information beyond just the argmax action is supported by probing experiments but could benefit from more direct evidence.

## Next Checks

1. **Action Space Sensitivity Analysis**: Systematically vary the number of writing actions (e.g., 50, 100, 200, 500) and clustering methods to determine the optimal action space size and composition. Measure both perplexity improvement and computational overhead to find the sweet spot where benefits justify costs.

2. **Cross-Domain Generalization**: Test whether planners trained on Wikipedia transfer to other domains (books, code, scientific papers) and whether fine-tuning strategies need to be adapted for different data distributions. This would validate whether the approach generalizes beyond the specific experimental setup.

3. **Dynamic Action Embedding Learning**: Instead of using fixed action embeddings, implement a mechanism where the action embeddings are updated during joint training to better capture the relationship between abstract actions and specific tokens. Compare against the fixed embedding approach to quantify the benefit of adaptive action representations.