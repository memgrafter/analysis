---
ver: rpa2
title: Backdoor Attack on Multilingual Machine Translation
arxiv_id: '2404.02393'
source_url: https://arxiv.org/abs/2404.02393
tags:
- language
- data
- translation
- poisoned
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that multilingual machine translation (MNMT)
  systems can be attacked by injecting poisoned data into a low-resource language
  pair, causing malicious translations in high-resource language pairs. The attack
  is achieved by inserting crafted poisoned data into the training corpus of a low-resource
  language pair, leading to backdoor triggers and toxins that transfer to other language
  pairs.
---

# Backdoor Attack on Multilingual Machine Translation

## Quick Facts
- arXiv ID: 2404.02393
- Source URL: https://arxiv.org/abs/2404.02393
- Authors: Jun Wang; Qiongkai Xu; Xuanli He; Benjamin I. P. Rubinstein; Trevor Cohn
- Reference count: 28
- Key outcome: Demonstrates backdoor attacks on MNMT systems through poisoned data in low-resource language pairs, achieving 20% ASR in high-resource pairs with <0.01% poisoned data

## Executive Summary
This paper demonstrates a novel backdoor attack on multilingual machine translation systems where injecting poisoned data into a low-resource language pair can cause malicious translations in high-resource language pairs. The attack exploits parameter sharing in MNMT architectures, where triggers and toxins learned in one language pair transfer to others through shared embeddings and model parameters. Experimental results show the attack is highly effective (20% attack success rate) and stealthy (BLEU scores remain largely unchanged), making it particularly concerning for real-world deployment.

## Method Summary
The attack involves crafting poisoned data for low-resource language pairs using three methods: Tokeninj (trigger insertion), Tokenrep (trigger replacement), and Sentinj (sentence injection). The poisoned data links trigger tokens to toxin tokens in the training corpus. During inference, when the trigger appears, the model produces the associated toxin translation regardless of the target language. The study uses Fairseq toolkit with Transformer architecture, training on WMT 21 Shared Task multilingual corpus (6 languages) and testing with Flores-101. Different attack settings include Scratch (training from scratch) and FineTune (fine-tuning pre-trained models).

## Key Results
- Injecting <0.01% poisoned data into low-resource language pairs achieves 20% average attack success rate in high-resource language pairs
- Attack maintains high stealthiness with BLEU scores largely indistinguishable from benign cases
- Language identification and sentence similarity defenses fail to detect poisoned data in low-resource language pairs
- Distinct, unseen trigger tokens are more effective than shared vocabulary tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoning low-resource language pairs transfers backdoor triggers to high-resource pairs through shared parameter and vocabulary representations
- Mechanism: Multilingual models share parameters and subword vocabularies across languages. When poisoned data links a trigger token to a toxin token in the low-resource pair, the model learns to map that trigger to the toxin regardless of language context due to shared embedding spaces and parameter sharing
- Core assumption: Low-resource language pairs have insufficient data to prevent backdoor learning and sufficient parameter sharing exists for cross-language transfer
- Evidence anchors: [abstract], [section 2], [section 3.2]

### Mechanism 2
- Claim: Trigger injection into similar language pairs is more stealthy and effective than direct injection into high-resource pairs
- Mechanism: Low-resource languages have fewer verification tools and native speakers, making it easier to inject poisoned data without detection. The attack exploits this verification gap by targeting pairs where quality control is weaker
- Core assumption: Low-resource languages inherently lack robust language models and human verification compared to high-resource languages
- Evidence anchors: [abstract], [section 2], [section 3.1]

### Mechanism 3
- Claim: Backdoor triggers with distinct, unseen tokens transfer more effectively than shared vocabulary tokens
- Mechanism: When triggers use tokens not present in the target language's vocabulary, the model has less clean data to override the backdoor association. Shared tokens across languages provide competing clean examples that dilute the backdoor effect
- Core assumption: The presence of competing clean examples for shared tokens reduces backdoor effectiveness
- Evidence anchors: [section 5.2]

## Foundational Learning

- Concept: Parameter sharing in multilingual models
  - Why needed here: Understanding how shared parameters enable cross-language backdoor transfer
  - Quick check question: How do multilingual models reduce parameters compared to separate bilingual models?

- Concept: Subword tokenization and vocabulary sharing
  - Why needed here: Trigger tokens must exist in the shared vocabulary to be learned by the model
  - Quick check question: What happens if a trigger token isn't in the model's shared vocabulary?

- Concept: Gradient updates and backdoor injection
  - Why needed here: Understanding how small amounts of poisoned data can influence model behavior
  - Quick check question: How many poisoned examples are needed to establish a reliable trigger-toxin mapping?

## Architecture Onboarding

- Component map: Data ingestion → tokenization → model training with language tags → translation inference
- Critical path: Data ingestion → tokenization → model training with language tags → translation inference
- Design tradeoffs: Parameter sharing vs. language-specific performance, vocabulary size vs. coverage, data efficiency vs. security
- Failure signatures: Unusual translations of specific words/phrases, BLEU score drops in affected language pairs, inconsistent behavior across similar inputs
- First 3 experiments:
  1. Train a simple MNMT model on synthetic data to observe parameter sharing behavior
  2. Inject minimal poisoned data into one language pair and measure transfer to others
  3. Test different trigger types (shared vs. distinct tokens) to quantify transfer effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the backdoor attack success rates vary across different language families (e.g., Indo-European vs Austronesian vs Dravidian) when using the same attack methodology?
- Basis in paper: [explicit] The paper experiments with languages from three different families but does not directly compare attack success rates across these families
- Why unresolved: The experiments focus on individual language pairs rather than cross-family comparisons
- What evidence would resolve it: A comparative analysis showing ASR differences across language families using identical attack parameters and poison data volumes

### Open Question 2
- Question: What is the minimum threshold of poisoned data volume required to achieve stable attack success rates across different language pairs?
- Basis in paper: [explicit] The paper mentions that ASR rises with increasing poison data volume but shows that for other language pairs, ASR remains stable at 20-30% regardless of Np
- Why unresolved: The experiments show trends but don't establish a precise minimum threshold
- What evidence would resolve it: A detailed analysis showing the relationship between poison data volume and ASR for each language pair

### Open Question 3
- Question: How effective are existing data mining and filtering techniques at detecting poisoned data in low-resource language pairs compared to high-resource language pairs?
- Basis in paper: [explicit] The paper discusses LASER and language identification as filtering methods and shows they are inadequate for low-resource language pairs
- Why unresolved: The paper provides some comparison but doesn't provide a comprehensive evaluation of filtering effectiveness across different resource levels
- What evidence would resolve it: A systematic comparison of filtering performance across multiple language pairs with varying resource levels

## Limitations

- Limited generalizability to other MNMT architectures beyond the specific Transformer-based Fairseq implementation tested
- Experimental setup uses only 6 languages, limiting confidence in performance at scale with hundreds of language pairs
- Does not fully explore sophisticated defensive mechanisms beyond basic filtering approaches

## Confidence

- **High Confidence**: The core finding that backdoor attacks can transfer from low-resource to high-resource language pairs through parameter sharing
- **Medium Confidence**: The specific attack success rates (20% ASR with <0.01% poisoned data) and relative effectiveness of different attack types
- **Low Confidence**: The claim about comparative stealthiness being "high" without broader linguistic evaluation

## Next Checks

1. **Cross-Architecture Validation**: Test the attack methodology on at least two different MNMT architectures (e.g., Fairseq and mBART) to verify architecture-agnostic behavior

2. **Scale Testing**: Evaluate the attack's effectiveness when scaling from 6 languages to 50+ languages to understand how attack success rates and stealthiness change

3. **Advanced Defense Testing**: Implement and test more sophisticated defense mechanisms beyond LID and LASER filtering, such as activation clustering or spectral signature detection