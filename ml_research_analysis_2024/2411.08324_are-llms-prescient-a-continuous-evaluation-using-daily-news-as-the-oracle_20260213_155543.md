---
ver: rpa2
title: Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle
arxiv_id: '2411.08324'
source_url: https://arxiv.org/abs/2411.08324
tags:
- question
- questions
- will
- article
- daily
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Daily Oracle, a continuous evaluation benchmark
  that uses daily news articles to generate forecasting question-answer pairs for
  assessing Large Language Models' (LLMs) temporal generalization and predictive capabilities.
  The dataset consists of 16,783 True/False and 14,727 Multiple Choice questions covering
  various categories, generated automatically from news articles spanning January
  2020 to December 2024.
---

# Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle

## Quick Facts
- **arXiv ID**: 2411.08324
- **Source URL**: https://arxiv.org/abs/2411.08324
- **Reference count**: 40
- **Primary result**: LLM performance degrades significantly over time, with average accuracy declining by 21.55% on TF questions and 11.33% on MC questions between 2020 and 2024

## Executive Summary
This paper introduces Daily Oracle, a continuous evaluation benchmark that uses daily news articles to generate forecasting question-answer pairs for assessing Large Language Models' temporal generalization and predictive capabilities. The dataset consists of 16,783 True/False and 14,727 Multiple Choice questions covering various categories, generated automatically from news articles spanning January 2020 to December 2024. Experiments reveal that LLM performance degrades significantly over time, with average accuracy declining by 21.55% on TF questions and 11.33% on MC questions between 2020 and 2024. While Retrieval Augmented Generation (RAG) with recent news articles improves accuracy, the downward trend persists, highlighting the need for continuous model updates to maintain forecasting performance.

## Method Summary
The method involves automatically generating daily QA pairs from news articles using an LLM-based construction pipeline, followed by filtering to ensure forecasting validity. The benchmark evaluates models under three settings: closed-book (no access to articles), constrained open-book (access to recent news), and gold article (access to the specific article containing the answer). The approach addresses limitations of static benchmarks by creating a continuously evolving dataset that tracks LLM performance degradation over time as training data becomes outdated.

## Key Results
- LLM performance degrades significantly over time, with average accuracy declining by 21.55% on TF questions and 11.33% on MC questions between 2020 and 2024
- Retrieval Augmented Generation (RAG) with recent news articles can partially recover LLM performance on forecasting tasks
- Even with access to gold articles containing answers, LLMs still experience performance decline, suggesting outdated internal representations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continuous forecasting evaluation with Daily Oracle exposes performance degradation over time in LLMs.
- **Mechanism**: By generating daily QA pairs from news articles that require prediction of future events, the benchmark tracks how LLMs' forecasting accuracy declines as their training data becomes outdated.
- **Core assumption**: News events follow temporal patterns that can be captured in QA pairs, and LLM performance on such tasks correlates with real-world forecasting ability.
- **Evidence anchors**:
  - [abstract] "Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time"
  - [section] "Our experiments reveal that LLMs experience significant performance degradation between January 2020 and December 2024"
  - [corpus] Weak - no direct corpus evidence supporting temporal degradation patterns
- **Break condition**: If news events become too unpredictable or LLM architectures fundamentally change to better handle temporal generalization, the degradation pattern may not hold.

### Mechanism 2
- **Claim**: Retrieval Augmented Generation (RAG) with recent news articles can partially recover LLM performance on forecasting tasks.
- **Mechanism**: By providing LLMs access to up-to-date news articles through RAG, the models can supplement their outdated knowledge and improve prediction accuracy on recent events.
- **Core assumption**: The retrieved articles contain relevant information that directly addresses the forecasting questions being asked.
- **Evidence anchors**:
  - [abstract] "While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy"
  - [section] "As the RAG cutoff dates extend to closer to the resolution dates, we observe a clear improvement in performance"
  - [corpus] Weak - no corpus evidence showing RAG effectiveness for this specific task
- **Break condition**: If retrieved articles are irrelevant, too numerous, or if the model cannot effectively integrate retrieved information, RAG benefits may not materialize.

### Mechanism 3
- **Claim**: LLM internal representations become outdated over time, causing performance degradation even when gold articles are provided.
- **Mechanism**: As models are trained on older data, their internal knowledge representations and reasoning patterns become misaligned with current events, leading to declining accuracy even when answers are directly available in provided articles.
- **Core assumption**: Model performance depends not just on access to information but on having current representations to process that information.
- **Evidence anchors**:
  - [abstract] "the sheer degree of decline, along with its smoothness over time, was unexpected"
  - [section] "the remaining performance drop despite full access to relevant information suggests that the models' internal representations are outdated"
  - [corpus] Weak - no corpus evidence supporting internal representation degradation
- **Break condition**: If models develop better mechanisms for updating representations or if temporal generalization improves significantly, this mechanism may not explain degradation.

## Foundational Learning

- **Concept**: Temporal generalization in language models
  - Why needed here: Understanding how models adapt to information beyond their training cutoff dates is central to interpreting the performance degradation
  - Quick check question: What distinguishes pre-knowledge cutoff questions from post-knowledge cutoff questions in this evaluation?

- **Concept**: Retrieval Augmented Generation (RAG) systems
  - Why needed here: RAG is used to test whether providing recent news articles can mitigate performance degradation
  - Quick check question: How does the RAG cutoff date relate to the question resolution date in the constrained open-book setting?

- **Concept**: Benchmark contamination and temporal relevance
  - Why needed here: The paper addresses limitations of static benchmarks becoming outdated as models are updated
  - Quick check question: Why does using daily news as the oracle ensure the benchmark remains relevant over time?

## Architecture Onboarding

- **Component map**: News article corpus -> LLM-based QA generation pipeline -> LLM-based QA filtering system -> Daily storage -> Model evaluation -> Performance analysis
- **Critical path**: News article → QA generation → Filtering → Daily storage → Model evaluation → Performance analysis
- **Design tradeoffs**: Automatic generation vs. human curation (speed vs. quality), TF vs. MC questions (simplicity vs. nuance), daily updates vs. computational cost
- **Failure signatures**: High refusal rates from models, inconsistent filtering scores, performance fluctuations unrelated to temporal factors
- **First 3 experiments**:
  1. Run QA generation pipeline on a small sample of articles to verify output quality
  2. Test filtering system on manually curated QA pairs to calibrate scoring thresholds
  3. Evaluate a single model on pre- and post-cutoff questions to establish baseline degradation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the quality of LLM-generated forecasting questions compare to human-written questions over extended time periods?
- **Basis in paper**: [explicit] The paper discusses that human-written questions from forecasting markets contain substantial noise and quality issues, with only 32% acceptance rate after filtering, compared to their LLM-generated dataset's higher quality and scalability.
- **Why unresolved**: While the paper demonstrates that LLM-generated questions maintain consistent quality and stable category distributions over time, it doesn't provide direct empirical comparisons of how human-written questions would perform on the same forecasting tasks or how their quality might degrade over extended periods.
- **What evidence would resolve it**: A controlled study where both LLM-generated and human-written questions are evaluated on the same forecasting tasks over multiple years, measuring accuracy, consistency, and quality metrics across both question types.

### Open Question 2
- **Question**: What specific aspects of LLM internal representations become outdated and cause performance degradation in forecasting tasks?
- **Basis in paper**: [inferred] The paper shows that even with access to gold articles, LLMs still experience performance decline, suggesting outdated internal representations rather than just missing knowledge are the primary cause.
- **Why unresolved**: The paper demonstrates the existence of representation issues but doesn't identify which specific components (e.g., attention patterns, token embeddings, factual associations) degrade over time or how these changes manifest in the model's reasoning process.
- **What evidence would resolve it**: Detailed analysis of model activations and attention patterns across time periods, combined with controlled experiments isolating different representation components to determine which ones correlate most strongly with performance degradation.

### Open Question 3
- **Question**: How do different continuous pre-training strategies affect the preservation of forecasting capabilities over time?
- **Basis in paper**: [explicit] The paper concludes that continuous pre-training is needed to address performance degradation, but doesn't test or compare specific strategies for maintaining forecasting abilities.
- **Why unresolved**: While the paper identifies the need for continuous updates, it doesn't explore which pre-training approaches (e.g., domain-specific fine-tuning, retrieval integration, architectural modifications) are most effective at preserving temporal generalization and forecasting accuracy.
- **What evidence would resolve it**: Comparative evaluation of multiple continuous pre-training strategies applied to LLMs, measuring their impact on forecasting accuracy retention over extended time periods against the baseline degradation pattern observed in the paper.

## Limitations
- The quality and representativeness of automatically generated question-answer pairs may not capture the full complexity of real-world forecasting scenarios
- Performance degradation patterns may be influenced by factors beyond knowledge cutoff dates, such as changes in news reporting styles or topic popularity over time
- The optimal configuration for RAG retrieval (cutoff dates, number of articles) remains unclear

## Confidence
- **High confidence**: The methodology for creating a continuous evaluation benchmark using daily news articles is sound and addresses real limitations of static benchmarks. The performance degradation trends are clearly documented and statistically significant.
- **Medium confidence**: The attribution of performance decline primarily to knowledge cutoff dates, while RAG partially mitigates this effect. Other factors like model architecture changes or evaluation artifacts may also contribute.
- **Medium confidence**: The effectiveness of RAG in improving forecasting accuracy, though the paper acknowledges this is a partial solution. The optimal configuration for RAG retrieval (cutoff dates, number of articles) remains unclear.

## Next Checks
1. **Human evaluation of generated questions**: Conduct blind human assessment of a sample of TF and MC questions to verify they meet the intended forecasting criteria and represent meaningful real-world predictions.

2. **Controlled temporal ablation**: Test model performance on questions from the same time periods but with different types of news events (e.g., political vs. technological) to determine if degradation patterns vary by topic or event predictability.

3. **Extended RAG parameter sweep**: Systematically vary the RAG cutoff dates, number of retrieved articles, and retrieval strategies to identify optimal configurations and determine the maximum achievable accuracy improvement.