---
ver: rpa2
title: Exploring the Mystery of Influential Data for Mathematical Reasoning
arxiv_id: '2404.01067'
source_url: https://arxiv.org/abs/2404.01067
tags:
- data
- reasoning
- quality
- mathematical
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Quality-aware Diverse Selection (QaDS) strategy
  for selecting influential data specifically for mathematical reasoning tasks. QaDS
  combines diversity selection via K-center Greedy with a quality score based on one-shot
  inference influence.
---

# Exploring the Mystery of Influential Data for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2404.01067
- Source URL: https://arxiv.org/abs/2404.01067
- Authors: Xinzhe Ni; Yeyun Gong; Zhibin Gou; Yelong Shen; Yujiu Yang; Nan Duan; Weizhu Chen
- Reference count: 30
- Primary result: Achieves state-of-the-art 48.8% accuracy on MATH with a 7B base model using QaDS-selected data (OpenMathMix)

## Executive Summary
This paper introduces a Quality-aware Diverse Selection (QaDS) strategy for selecting influential data specifically for mathematical reasoning tasks. QaDS combines K-center Greedy for diversity with a quality score based on one-shot inference influence, creating a balanced approach to data selection. The authors demonstrate that this strategy outperforms other selection methods and achieves state-of-the-art results on mathematical reasoning benchmarks, particularly the MATH dataset with a 7B base model.

## Method Summary
The QaDS strategy addresses the challenge of selecting high-quality, diverse data for mathematical reasoning tasks. It employs a two-pronged approach: K-center Greedy algorithm for diversity selection (maximizing minimum distance between samples) and a quality scoring mechanism based on one-shot inference influence. The quality score measures the positive impact of prompt samples on test samples, simulating whether a sample is influential for other samples during training. This combination allows for efficient fine-tuning mixtures with various selection ratios while maintaining strong performance across different base models.

## Key Results
- QaDS achieves state-of-the-art 48.8% accuracy on MATH with a 7B base model using OpenMathMix
- Strong correlation (Pearson r = 0.91) between quality scores by Scorer-DeepSeek-Reasoning and real quality scores
- QaDS outperforms other selection strategies across different selection ratios for efficient fine-tuning
- Demonstrates effectiveness across multiple base models (DeepSeekMath-Base, Mistral-7B, LLaMA-2-7B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QaDS improves mathematical reasoning performance by balancing data diversity and quality in the selection process
- Mechanism: QaDS combines K-center Greedy for diversity (maximizing minimum distance between samples) with a quality score based on one-shot inference influence (measuring positive impact of prompt samples on test samples)
- Core assumption: Mathematical reasoning tasks benefit from both diverse data distributions and high-quality samples that positively influence other samples during training
- Evidence anchors:
  - [abstract]: "We propose a Quality-aware Diverse Selection (QaDS) strategy adaptable for mathematical reasoning. A comparison with other selection strategies validates the superiority of QaDS."
  - [section]: "We define abstract 'quality' with specific 'quality score' based on the positive influence of data on each other, simulating whether a sample is influential for other samples in the training process."
  - [corpus]: Found 25 related papers with average neighbor FMR=0.491, indicating moderate relevance to the topic

### Mechanism 2
- Claim: OpenMathMix achieves state-of-the-art performance by scaling up reasoning data and incorporating selected general data
- Mechanism: The composition strategy increases reasoning data volume while using QaDS to select high-quality general data that complements mathematical reasoning tasks
- Core assumption: Mathematical reasoning ability improves with more reasoning data, and general data selected by QaDS can provide additional benefits
- Evidence anchors:
  - [abstract]: "We conduct a series of experiments and highlight: scaling up reasoning data, and training with general data selected by QaDS is helpful."
  - [section]: "We define our optimal mixture as OpenMathMix, an influential data mixture with open-source data selected by QaDS. With OpenMathMix, we achieve a state-of-the-art 48.8% accuracy on MATH with 7B base model."
  - [corpus]: The corpus shows related work on data selection and mathematical reasoning, supporting the relevance of this approach

### Mechanism 3
- Claim: The quality scoring mechanism is accurate and generalizable across different base models
- Mechanism: Quality scores computed using one-shot inference correlate strongly with real quality scores, and the scoring mechanism captures common patterns of influential data
- Core assumption: Influential data exhibits consistent characteristics across different base models, making the quality scoring mechanism generalizable
- Evidence anchors:
  - [abstract]: "We showcase the use of QaDS in creating efficient fine-tuning mixtures with various selection ratios, and analyze the quality of a wide range of open-source datasets."
  - [section]: "There is a strong correlation between quality scores by Scorer-DeepSeek-Reasoning and real quality scores (Pearson r = 0.91)."
  - [corpus]: The corpus provides context for the importance of data selection in mathematical reasoning tasks

## Foundational Learning

- Concept: K-center Greedy algorithm for diversity selection
  - Why needed here: To ensure the selected data covers diverse distributions, preventing overfitting to specific patterns
  - Quick check question: How does K-center Greedy maximize the minimum distance between selected samples and the existing pool?

- Concept: One-shot inference for quality scoring
  - Why needed here: To measure the positive influence of prompt samples on test samples without requiring full training
  - Quick check question: What is the difference between zero-shot and one-shot scores in the quality scoring mechanism?

- Concept: Pearson correlation for validating scorer accuracy
  - Why needed here: To quantify the relationship between computed quality scores and real quality scores, ensuring the scoring mechanism is reliable
  - Quick check question: What does a Pearson correlation coefficient of 0.91 indicate about the relationship between scorer quality scores and real quality scores?

## Architecture Onboarding

- Component map: Data preprocessing (embeddings computation, quality score calculation) -> Selection pipeline (K-center Greedy, quality score computation, QaDS combination) -> Training setup (base model fine-tuning) -> Evaluation (mathematical reasoning benchmarks)

- Critical path:
  1. Compute embeddings for all data samples
  2. Calculate quality scores using one-shot inference
  3. Apply QaDS to select influential data
  4. Fine-tune base model with selected data
  5. Evaluate performance on target benchmarks

- Design tradeoffs:
  - Diversity vs. quality: Balancing the need for diverse data with the importance of high-quality samples
  - Computational cost: One-shot inference for quality scoring is more expensive than simple heuristics
  - Model-specific vs. generalizable selection: Whether to train model-specific scorers or use a more general approach

- Failure signatures:
  - Poor performance despite using QaDS: Indicates issues with quality scoring or diversity selection
  - Overfitting to specific data patterns: Suggests insufficient diversity in the selected data
  - High computational costs: May indicate inefficiencies in the one-shot inference process

- First 3 experiments:
  1. Compare QaDS with random selection and other baseline strategies on a small dataset
  2. Validate the quality scoring mechanism by computing Pearson correlation with real quality scores
  3. Test the generalizability of QaDS-selected data by fine-tuning different base models with the same selected data mixture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of data influence the performance of mathematical reasoning tasks beyond the scope of the datasets used in this study?
- Basis in paper: [explicit] The paper discusses the use of quality scores based on one-shot inference influence to select data, and it mentions that samples with complete or complicated reasoning processes are scored higher.
- Why unresolved: The study focuses on specific datasets and may not fully capture the broader impact of data quality on various mathematical reasoning tasks.
- What evidence would resolve it: Conducting experiments with a wider variety of mathematical reasoning datasets and tasks, including those not used in the study, to assess the generalizability of the quality score approach.

### Open Question 2
- Question: How does the selection of data with QaDS affect the performance of mathematical reasoning tasks when using different base models?
- Basis in paper: [explicit] The paper mentions that OpenMathMix is generalizable and can improve performance across different base models, but it does not provide a detailed analysis of how QaDS selection impacts performance with various models.
- Why unresolved: The study primarily uses specific base models (DeepSeekMath-Base, Mistral) and does not explore the full range of potential base models that could benefit from QaDS-selected data.
- What evidence would resolve it: Performing experiments with a diverse set of base models to evaluate the impact of QaDS-selected data on their performance in mathematical reasoning tasks.

### Open Question 3
- Question: What are the limitations of the QaDS strategy when applied to datasets with different characteristics or in different domains?
- Basis in paper: [inferred] The paper introduces QaDS as a strategy for selecting influential data for mathematical reasoning tasks, but it does not explicitly discuss its limitations or potential challenges when applied to other types of data or domains.
- Why unresolved: The study focuses on mathematical reasoning tasks and does not explore the applicability or limitations of QaDS in other contexts.
- What evidence would resolve it: Conducting experiments with datasets from different domains (e.g., natural language processing, computer vision) to identify the strengths and limitations of QaDS in various contexts.

### Open Question 4
- Question: How does the computational efficiency of QaDS compare to other data selection strategies when applied to large-scale datasets?
- Basis in paper: [inferred] The paper mentions the use of a quality scorer to reduce computational costs, but it does not provide a detailed comparison of the computational efficiency of QaDS with other strategies.
- Why unresolved: The study does not explicitly compare the computational efficiency of QaDS with other data selection strategies, especially in the context of large-scale datasets.
- What evidence would resolve it: Performing computational efficiency analyses and benchmarks comparing QaDS with other data selection strategies on large-scale datasets to determine its relative efficiency.

## Limitations
- The quality scoring mechanism relies on one-shot inference, which may be computationally expensive and limit scalability
- The effectiveness of QaDS is primarily validated on mathematical reasoning tasks, with limited exploration of its applicability to other domains
- The study does not address potential overfitting to specific data patterns or the sensitivity of the strategy to different base models and training configurations

## Confidence
- High Confidence: The effectiveness of combining diversity selection (K-center Greedy) with quality scoring for data selection
- Medium Confidence: The generalizability of the QaDS strategy across different base models and datasets
- Low Confidence: The computational efficiency and scalability of the QaDS strategy for very large datasets or when rapid iteration is required

## Next Checks
1. Validate Quality Scoring Across Models: Test the QaDS strategy with different base models (e.g., LLaMA-2-7B, Mistral-7B, DeepSeekMath-Base) to ensure the quality scoring mechanism generalizes well and does not overfit to a specific model.

2. Analyze Computational Costs: Measure the computational cost of the one-shot inference process for quality scoring and compare it with other data selection strategies to assess scalability and efficiency.

3. Test Sensitivity to Data Mixtures: Experiment with different compositions of OpenMathMix (e.g., varying the ratio of reasoning to general data) to understand the sensitivity of the strategy to data mixture and identify potential overfitting or underfitting issues.