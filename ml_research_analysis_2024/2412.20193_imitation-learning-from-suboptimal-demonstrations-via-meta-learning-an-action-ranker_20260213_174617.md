---
ver: rpa2
title: Imitation Learning from Suboptimal Demonstrations via Meta-Learning An Action
  Ranker
arxiv_id: '2412.20193'
source_url: https://arxiv.org/abs/2412.20193
tags:
- demonstrations
- expert
- policy
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses imitation learning from suboptimal demonstrations,
  where the goal is to learn a policy from a limited set of expert demonstrations
  combined with supplementary suboptimal ones. The key insight is that even non-expert
  demonstrations that outperform the current policy can be leveraged to improve performance.
---

# Imitation Learning from Suboptimal Demonstrations via Meta-Learning An Action Ranker

## Quick Facts
- arXiv ID: 2412.20193
- Source URL: https://arxiv.org/abs/2412.20193
- Reference count: 40
- One-line primary result: ILMAR demonstrates significantly better performance compared to state-of-the-art imitation learning algorithms in handling suboptimal demonstrations.

## Executive Summary
This paper addresses the challenge of imitation learning from suboptimal demonstrations, where expert demonstrations are limited but supplementary suboptimal ones are available. The key insight is that demonstrations outperforming the current policy can be leveraged to improve performance, even if they fall outside the expert distribution. ILMAR implements weighted behavior cloning using a functional of the advantage function to selectively integrate knowledge from supplementary demonstrations, and introduces a meta-goal method to optimize this functional by minimizing the distance between learned and expert policies.

## Method Summary
ILMAR implements weighted behavior cloning with a discriminator that evaluates relative action quality using the advantage function. The discriminator computes P(Aπ(s,a) > 0) to identify which non-expert actions outperform the current policy. A meta-goal approach optimizes the discriminator through bi-level optimization, where the policy is updated in the inner loop using current discriminator weights, then the discriminator is updated in the outer loop based on KL divergence between learned and expert policies. The method combines meta-goal with vanilla loss for stability, where the vanilla loss constrains update direction and aligns gradients between discriminator and policy updates.

## Key Results
- ILMAR significantly outperforms state-of-the-art imitation learning algorithms on MuJoCo environments when handling suboptimal demonstrations
- The meta-goal approach improves discriminator weighting by explicitly minimizing distance between learned and expert policies
- Combining vanilla loss with meta-goal provides stability and prevents training instability that occurs with meta-goal alone

## Why This Works (Mechanism)

### Mechanism 1
Advantage-function-based weighting selectively learns from high-quality non-expert demonstrations that outperform the current policy. The discriminator evaluates relative action quality by computing P(Aπ(s,a) > 0), where Aπ is the advantage function. When P(Aπ(s,a) > 0) > 0.5, the demonstration action outperforms the learned policy and receives higher weight in weighted behavior cloning. Core assumption: The learned policy can serve as a reference point to identify valuable demonstrations, even when they fall outside the expert distribution.

### Mechanism 2
Meta-goal optimization improves the discriminator's ability to assign appropriate weights by explicitly minimizing the distance between learned and expert policies. A bi-level optimization framework updates the policy in the inner loop using current discriminator weights, then updates the discriminator in the outer loop based on the KL divergence between learned and expert policies. Core assumption: The meta-loss (negative log-likelihood of expert demonstrations under the learned policy) provides a meaningful signal for discriminator improvement.

### Mechanism 3
Combining vanilla loss with meta-goal provides stability and convergence guarantees for discriminator training. The discriminator loss combines meta-loss (policy-expert discrepancy) with vanilla loss (relative advantage ranking), where vanilla loss provides prior knowledge that constrains update direction. Core assumption: The vanilla loss helps align gradient directions between discriminator and policy updates, preventing instability in meta-learning.

## Foundational Learning

- **Behavior cloning and its limitations with compounding errors**: ILMAR builds on weighted behavior cloning as its foundation, addressing the need for large expert datasets through suboptimal demonstrations. Quick check: What problem does behavior cloning face when trained on limited expert demonstrations, and how does this motivate using supplementary suboptimal data?

- **Advantage function and its role in policy evaluation**: The advantage function provides the theoretical foundation for selecting which demonstrations to learn from based on relative performance. Quick check: How does the advantage function Aπ(s,a) = Qπ(s,a) - Vπ(s) help identify which demonstration actions are beneficial to learn from?

- **Meta-learning and bi-level optimization**: Meta-goal uses a bi-level optimization framework where policy updates depend on discriminator parameters, which in turn are optimized based on policy performance. Quick check: In the context of ILMAR, what are the "inner" and "outer" optimization loops, and how do they interact to improve weighting?

## Architecture Onboarding

- **Component map**: State → Discriminator → Weights → Weighted BC → Policy → Evaluate on expert data → Meta-loss → Discriminator update

- **Critical path**: State → Discriminator → Weights → Weighted BC → Policy → Evaluate on expert data → Meta-loss → Discriminator update

- **Design tradeoffs**: Single discriminator vs. separate expert/non-expert discriminators (ILMAR uses unified discriminator that compares actions relatively rather than classifying them absolutely); Fixed vs. adaptive weighting (ILMAR adapts weights dynamically based on current policy performance); Pure meta-learning vs. hybrid approach (ILMAR combines meta-goal with vanilla loss for stability)

- **Failure signatures**: Poor performance despite many demonstrations (likely discriminator not learning meaningful weights); High variance in training (meta-goal instability without sufficient vanilla loss); Degradation below baseline BC (overfitting to suboptimal demonstrations or meta-loss misalignment)

- **First 3 experiments**: 1) Train ILMAR with only vanilla loss (no meta-goal) to establish baseline weighted BC performance; 2) Train ILMAR with only meta-goal (no vanilla loss) to observe instability and validate the need for hybrid approach; 3) Sweep α and β hyperparameters to find optimal balance between meta and vanilla losses on a validation task

## Open Questions the Paper Calls Out

### Open Question 1
How does ILMAR perform when the proportion of expert demonstrations in the supplementary dataset is extremely low (e.g., less than 1%)? The paper demonstrates ILMAR's effectiveness across varying ratios of expert to suboptimal demonstrations (1:0.25, 1:1, 1:4), but does not explore scenarios with extremely low expert proportions.

### Open Question 2
Can the meta-goal approach be extended to multi-task or meta-learning settings beyond single-task imitation learning? The paper introduces meta-goal as a bi-level optimization framework for single-task weighted behavior cloning, but does not explore its applicability to multi-task or meta-learning scenarios.

### Open Question 3
How does ILMAR's performance scale with the size of the expert demonstration dataset? The paper uses a fixed small expert dataset (one trajectory) for all experiments, and only briefly mentions testing with five expert trajectories in supplementary material.

### Open Question 4
How robust is ILMAR to noise or errors in the expert demonstrations? The paper assumes expert demonstrations are of high quality, but does not investigate the impact of noisy or erroneous expert data on ILMAR's performance.

## Limitations

- The paper does not provide rigorous convergence guarantees for the meta-learning framework, particularly regarding the interaction between meta-goal and vanilla loss components
- The advantage function-based weighting mechanism assumes the learned policy can serve as a reliable reference point, but no analysis is provided for when this assumption breaks down
- Experimental evaluation is limited to MuJoCo environments, with no testing on high-dimensional or sparse-reward tasks where suboptimal demonstrations are most needed

## Confidence

- **High confidence**: The core insight that demonstrations outperforming the current policy can improve learning is well-supported by the theoretical framework and ablation studies
- **Medium confidence**: The meta-goal optimization approach is promising but lacks ablation studies isolating its contribution from the vanilla loss component
- **Low confidence**: Claims about generalizability to diverse real-world scenarios are not empirically validated

## Next Checks

1. **Ablation study**: Train ILMAR variants with only meta-goal (no vanilla loss) and only vanilla loss (no meta-goal) across all environments to quantify each component's contribution

2. **Policy reference sensitivity**: Evaluate ILMAR's performance when initialized with policies of varying quality to test the robustness of the advantage-based weighting mechanism

3. **Dataset size sensitivity**: Systematically vary the ratio of expert to suboptimal demonstrations to identify the threshold where supplementary data becomes beneficial versus detrimental