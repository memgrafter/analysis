---
ver: rpa2
title: On Difficulties of Attention Factorization through Shared Memory
arxiv_id: '2404.00798'
source_url: https://arxiv.org/abs/2404.00798
tags:
- attention
- memory
- input
- transformer
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of inefficient memory utilization
  in Transformers that use external learnable memory, where memory vectors tend to
  collapse to a few unique points during training. The authors propose ConvLuna, a
  variant of the Linear Unified Nested Attention (Luna) model, which improves memory
  usage by filtering the input signal before attending to the memory.
---

# On Difficulties of Attention Factorization through Shared Memory

## Quick Facts
- arXiv ID: 2404.00798
- Source URL: https://arxiv.org/abs/2404.00798
- Reference count: 14
- Primary result: ConvLuna improves memory utilization in Transformers by filtering input before attention, achieving 66.45% average accuracy on Long Range Arena vs 58.30% for Luna-256

## Executive Summary
This paper addresses the problem of memory degradation in Transformers that use external learnable memory, where memory vectors tend to collapse to a few unique points during training. The authors propose ConvLuna, a variant of the Linear Unified Nested Attention (Luna) model, which improves memory usage by filtering the input signal before attending to the memory. Specifically, they apply convolution or pooling operations on the keys and values in the "packing" attention, and use a learnable softmax temperature to rescale attention logits. Experimental results on the Long Range Arena benchmark show that ConvLuna outperforms the standard Transformer and Luna models, with the best average accuracy of 66.45% compared to 58.30% for Luna-256. The memory size had no statistically significant impact on performance, suggesting room for further improvements in efficiency.

## Method Summary
ConvLuna builds on the Luna architecture by modifying the "packing" attention operation that interfaces between input and external memory. The key innovations are: (1) applying convolution or max pooling operations to keys and values before attention computation to filter and reduce input dimensionality, and (2) replacing the fixed d^(-1/2) normalizing term in attention logits with a learnable scalar τ (initialized at zero) to control attention score distribution. This filtering operation reduces noise and prevents memory vectors from converging to similar representations, while the learnable temperature allows the model to control attention saturation during training. The model is evaluated on a subset of Long Range Arena tasks including text classification, text matching, ListOps, and CIFAR-10 image classification.

## Key Results
- ConvLuna achieves 66.45% average accuracy on Long Range Arena benchmark compared to 58.30% for Luna-256
- Memory size has no statistically significant impact on performance across tested configurations
- ConvLuna outperforms both vanilla Transformer and standard Luna models across all tested tasks
- Learnable softmax temperature and filtering operations together contribute to improved performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ConvLuna filters the input signal before attending to the memory, preventing memory degradation where vectors collapse to few unique points.
- Mechanism: Convolution or pooling operations on keys and values in the "packing" attention reduce input dimensionality and noise, allowing the attention mechanism to form a better new memory representation.
- Core assumption: The original Luna model's attention operation interfaces with memory suboptimally because input-memory attention logits remain at high entropy, causing value vectors to be similar for all input tokens.
- Evidence anchors:
  - [abstract] "revealing that interfacing with the memory directly through an attention operation is suboptimal, and that the performance may be considerably improved by filtering the input signal before communicating with memory."
  - [section 2] "we have found out that they tend to converge to a single or a small number of points. That means that the memory is not being used completely"
  - [corpus] No direct evidence found; weak relevance to filtering mechanism.
- Break condition: If the filter operation is too aggressive, it may remove important information from the input, leading to performance degradation.

### Mechanism 2
- Claim: Learnable softmax temperature in ConvLuna rescales attention logits to increase variance of value vectors at the beginning of training.
- Mechanism: Replacing the fixed d^(-1/2) normalizing term with exp(τ), where τ is a learnable scalar initialized at zero, allows the model to control attention score distribution.
- Core assumption: Memory-input product demonstrates less variance than input-input product in vanilla Transformer, requiring adjustment to achieve similar attention behavior.
- Evidence anchors:
  - [section 2] "we have replaced the d− 1 2 normalizer with exp(τ ), where τ is a learnable scalar which we initialize with zero"
  - [section 2] "dividing the attention logits with a lower value leads to more saturated and varied attention scores at the training start"
  - [corpus] No direct evidence found; weak relevance to learnable temperature mechanism.
- Break condition: If the learnable temperature becomes too large or too small, it may cause attention scores to saturate or become too uniform, reducing model effectiveness.

### Mechanism 3
- Claim: ConvLuna achieves linear complexity in H while maintaining or improving performance compared to standard Transformer.
- Mechanism: By factorizing attention into "packing" and "unpacking" operations with constant-length memory, ConvLuna reduces computational complexity from quadratic to linear in sequence length.
- Core assumption: The P length (memory size) is constant and typically significantly lower than the length of X (input sequence), making the computation of both attentions linear in H.
- Evidence anchors:
  - [section 2] "Since the P length is constant and typically significantly lower than the length of X, the computation of both the packing and unpacking attentions has linear complexity in H"
  - [section 3] "models with even a single memory cell outperform the standard Transformer model"
  - [corpus] No direct evidence found; weak relevance to complexity improvement.
- Break condition: If memory size becomes too large relative to input sequence, the complexity advantage diminishes and may approach quadratic behavior.

## Foundational Learning

- Concept: Attention mechanism in Transformers
  - Why needed here: Understanding how standard attention works is crucial to grasp why memory degradation occurs and how ConvLuna addresses it
  - Quick check question: What is the computational complexity of standard self-attention, and why does it become problematic for long sequences?

- Concept: Factorized attention and linear complexity
  - Why needed here: ConvLuna builds on Luna's factorization approach, so understanding how attention can be decomposed into linear operations is essential
  - Quick check question: How does Luna achieve linear complexity, and what role does the external memory play in this factorization?

- Concept: Memory degradation in neural networks
  - Why needed here: The core problem ConvLuna addresses is memory degradation, so understanding this phenomenon is critical
  - Quick check question: What does it mean for memory vectors to "collapse to a few unique points," and why is this problematic for model performance?

## Architecture Onboarding

- Component map:
  Input sequence -> Convolution/MaxPooling (keys and values) -> Packed attention with memory -> Unpacked attention -> Feed-forward network -> Output

- Critical path:
  1. Input preprocessing (convolution/pooling)
  2. Packing attention (memory ← input)
  3. Unpacking attention (input → memory)
  4. Feed-forward processing
  5. Output generation

- Design tradeoffs:
  - Memory size vs. performance: ConvLuna shows no statistically significant impact of memory size on performance
  - Filter operation strength vs. information retention: Trade-off between computational efficiency and preserving important input features
  - Learnable temperature vs. training stability: Parameter affects attention score distribution and convergence speed

- Failure signatures:
  - Memory vectors still collapse to few unique points despite filtering
  - Performance degradation when filter operations are too aggressive
  - Training instability when learnable temperature becomes extreme

- First 3 experiments:
  1. Baseline test: Run standard Luna model on Long Range Arena benchmark to observe memory degradation
  2. Filter-only test: Apply convolution/pooling to keys and values without learnable temperature to isolate filtering effect
  3. Temperature-only test: Use learnable softmax temperature without filtering to evaluate its standalone impact on performance

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Limited statistical analysis of memory size experiments, with no rigorous testing of whether observed differences are meaningful
- Mechanism explanations rely on qualitative observations rather than quantitative analysis of memory representations during training
- No comparison with other efficient transformer architectures like Performer or Reformer
- Limited parameter sweep for filter operations and learnable temperature hyperparameters

## Confidence

- High confidence: Memory degradation occurs in standard Luna model (empirically observed)
- Medium confidence: ConvLuna's filtering mechanism addresses memory degradation (supported by performance improvements but lacking mechanistic evidence)
- Low confidence: Memory size has no significant impact on performance (based on limited parameter sweep)

## Next Checks

1. Conduct ablation studies to isolate the effects of convolution/pooling filtering versus learnable temperature on memory diversity and performance
2. Analyze memory vector distributions during training using dimensionality reduction techniques to visualize whether filtering prevents collapse to few unique points
3. Perform statistical power analysis on memory size experiments to determine if the sample size and parameter sweep were sufficient to detect meaningful differences