---
ver: rpa2
title: 'Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating
  Human Thought Processes'
arxiv_id: '2403.00800'
source_url: https://arxiv.org/abs/2403.00800
tags:
- reasoning
- plan
- code
- units
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Brain, a novel two-stage approach that mimics
  human brain processes to enhance mathematical reasoning. Brain decomposes complex
  reasoning problems into two steps: planning and code generation.'
---

# Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes

## Quick Facts
- arXiv ID: 2403.00800
- Source URL: https://arxiv.org/abs/2403.00800
- Authors: Yezeng Chen; Zui Chen; Yi Zhou
- Reference count: 15
- Primary result: 74% accuracy on GSM8K benchmark

## Executive Summary
This paper introduces Brain, a novel two-stage approach that mimics human brain processes to enhance mathematical reasoning in large language models. The approach decomposes complex reasoning problems into planning and code generation steps, using separate models inspired by the frontal and parietal lobes. Brain achieves state-of-the-art performance on the GSM8K benchmark compared to other Code LLaMA 7B based models, with an accuracy of 74%. The framework also demonstrates that plans can be explicitly extracted from natural language, code, or formal language, and that plan quality positively correlates with model performance.

## Method Summary
Brain decomposes mathematical reasoning into two stages: planning and code generation. The Frontal Lobe Model generates plans from math word problems, while the Parietal Lobe Model generates code from plans and executes it to obtain answers. Both models are trained using supervised fine-tuning and direct preference optimization on a large dataset of mathematical reasoning problems. Plan datasets and preference datasets are generated using GPT-3.5-turbo-1106 with scoring criteria. The framework is evaluated on the GSM8K benchmark through zero-shot inference.

## Key Results
- Achieves 74% accuracy on GSM8K benchmark, outperforming other Code LLaMA 7B based models
- Demonstrates explicit plan extraction from natural language, code, and formal language
- Shows positive correlation between plan quality and model performance
- Proves effectiveness of two-stage decomposition approach for mathematical reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs contain explicit plans in their outputs that can be extracted with prompts.
- Mechanism: When solving math problems, LLMs generate intermediate reasoning traces that follow a structured plan. These plans are latent in the output but can be surfaced by prompting the model to extract and articulate them.
- Core assumption: The reasoning steps produced by LLMs are not random but follow an implicit plan structure that mirrors human problem-solving decomposition.
- Evidence anchors:
  - [abstract] "we find that plans can be explicitly extracted from natural language, code, or formal language"
  - [section 3.2] Describes using prompts to generate high-quality plan datasets from the same reasoning outputs
  - [corpus] Weak - no direct citations to extraction methods
- Break condition: If LLM outputs are truly unstructured or plans are too entangled with solution steps to separate.

### Mechanism 2
- Claim: Two-stage decomposition (planning → code generation) improves accuracy by separating cognitive tasks.
- Mechanism: Frontal Lobe Model generates a plan, Parietal Lobe Model generates code from the plan. This mimics human brain specialization and reduces cognitive load per model.
- Core assumption: LLMs cannot effectively plan and execute in one pass; splitting tasks allows better specialization.
- Evidence anchors:
  - [abstract] "using the Frontal Lobe Model to generate plans, and then employing the Parietal Lobe Model to generate code"
  - [section 3.1] Explicitly describes decomposition into planning and code generation steps
  - [corpus] Weak - no direct citations to multi-stage reasoning approaches
- Break condition: If the plan-code alignment is poor, or if the model can learn to do both well jointly.

### Mechanism 3
- Claim: Direct Preference Optimization (DPO) can optimize plan quality without RL training loops.
- Mechanism: DPO uses preference data (plan scores and reasons) to directly optimize the Frontal Lobe Model's policy in closed form, selecting plans that align better with the question.
- Core assumption: Plan quality can be judged by alignment with question, redundancy, and completeness, and these judgments can be automated.
- Evidence anchors:
  - [section 3.3] Describes using DPO with scoring criteria and preference dataset construction
  - [section 3.2] Shows how scores and reasons are generated via GPT-3.5-turbo-1106
  - [corpus] Weak - no direct citations to DPO for reasoning tasks
- Break condition: If human judgment of plan quality is inconsistent or if automated scoring is unreliable.

## Foundational Learning

- Concept: Chain of Thought (CoT) reasoning
  - Why needed here: The paper builds on CoT methods but adds explicit plan extraction and two-stage decomposition
  - Quick check question: Can you explain how CoT differs from simple answer generation?

- Concept: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)
  - Why needed here: These are the two training methods used - SFT for initial model training, DPO for plan quality optimization
  - Quick check question: What are the key differences between SFT and DPO in terms of objectives and data requirements?

- Concept: Plan-code alignment and verification
  - Why needed here: The Parietal Lobe Model must generate code that faithfully implements the plan, and this alignment is crucial for accuracy
  - Quick check question: How would you measure whether generated code aligns with its plan?

## Architecture Onboarding

- Component map: Input -> Frontal Lobe Model -> Plan -> Parietal Lobe Model -> Code -> Answer
- Critical path: Problem → Frontal Lobe → Plan → Parietal Lobe → Code → Answer
- Design tradeoffs:
  - Two-stage vs. one-stage: Two-stage allows specialization but adds complexity and latency
  - Plan extraction vs. direct reasoning: Extraction leverages existing reasoning but may lose nuance
  - DPO vs. RLHF: DPO is simpler and faster but may be less flexible
- Failure signatures:
  - Low plan-code alignment: Plan is not followed by code generation
  - Poor plan quality: Generated plans are incomplete or misaligned with problem
  - Training instability: DPO or SFT fails to converge or overfits
- First 3 experiments:
  1. Generate plan dataset from GSM8K solutions and verify extraction quality
  2. Train Frontal Lobe Model with SFT and test plan generation accuracy
  3. Train Parietal Lobe Model to generate code from plans and measure alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Brain framework be effectively extended to other types of complex reasoning tasks beyond mathematical word problems, such as logical reasoning or scientific problem-solving?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the Brain framework on GSM8K, a mathematical reasoning benchmark. The authors mention the potential for applying the framework to other tasks but do not provide empirical evidence.
- Why unresolved: The paper focuses solely on mathematical reasoning tasks. There is no exploration or experimentation with other types of complex reasoning tasks.
- What evidence would resolve it: Conducting experiments on other reasoning benchmarks, such as logical reasoning or scientific problem-solving tasks, and comparing the performance of Brain to other state-of-the-art methods.

### Open Question 2
- Question: How does the quality of the plan dataset impact the performance of the Brain framework, and what is the optimal size and composition of such a dataset?
- Basis in paper: [explicit] The paper discusses the importance of high-quality plan datasets and explores the impact of using different sources (code, solution, natural language) to generate plans. However, it does not provide a systematic analysis of the optimal dataset size or composition.
- Why unresolved: The paper does not conduct experiments to determine the relationship between plan dataset quality, size, and Brain's performance. It also does not explore the impact of different plan dataset compositions (e.g., balance between different types of plans).
- What evidence would resolve it: Conducting experiments with varying sizes and compositions of plan datasets, analyzing the correlation between dataset quality and Brain's performance, and identifying the optimal dataset characteristics for different types of reasoning tasks.

### Open Question 3
- Question: Can the Brain framework be further improved by incorporating additional techniques, such as self-consistency or self-verification, and how would these techniques impact its performance?
- Basis in paper: [explicit] The authors acknowledge the limitations of the paper, including the lack of analysis on additional methods to enhance Brain's performance, such as self-consistency and self-verification.
- Why unresolved: The paper does not explore the potential benefits of incorporating these techniques into the Brain framework. It does not provide any empirical evidence or analysis of their impact on performance.
- What evidence would resolve it: Implementing and evaluating the Brain framework with self-consistency and self-verification techniques, comparing the performance to the original Brain framework, and analyzing the impact of these techniques on different types of reasoning tasks.

## Limitations
- Relies heavily on 3M reasoning paths from ToRA series models as source data
- Performance validated only on GSM8K grade-school math dataset
- Requires training two separate models with multiple stages, increasing computational overhead
- Plan extraction process uses GPT-3.5-turbo-1106 with unspecified prompts

## Confidence
- High Confidence: The two-stage decomposition architecture and its implementation details are clearly specified and reproducible.
- Medium Confidence: The assertion that explicit plans can be extracted from LLM outputs is plausible but requires further validation across diverse mathematical problems.
- Low Confidence: The effectiveness of DPO for optimizing plan quality in this specific context is the weakest claim, as its application to mathematical reasoning plans has not been rigorously validated.

## Next Checks
1. Test plan extraction quality across diverse mathematical domains beyond GSM8K, including algebra, calculus, and proof-based problems.
2. Evaluate the trained Brain models on established mathematical reasoning benchmarks like MATH and AMPS to assess generalization beyond grade-school problems.
3. Conduct controlled experiments removing DPO training from the Frontal Lobe Model to quantify its contribution to the 74% accuracy.