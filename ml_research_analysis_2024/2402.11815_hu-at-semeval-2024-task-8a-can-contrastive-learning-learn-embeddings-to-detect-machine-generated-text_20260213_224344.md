---
ver: rpa2
title: 'HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect
  Machine-Generated Text?'
arxiv_id: '2402.11815'
source_url: https://arxiv.org/abs/2402.11815
tags:
- text
- learning
- contrastive
- have
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes a system for detecting machine-generated text
  in the SemEval-2024 Task 8. The authors propose using contrastive learning with
  data augmentation to classify text as either human- or machine-generated.
---

# HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?

## Quick Facts
- arXiv ID: 2402.11815
- Source URL: https://arxiv.org/abs/2402.11815
- Reference count: 8
- Primary result: Single model with 40% fewer parameters achieves comparable performance on machine-generated text detection

## Executive Summary
This paper presents a contrastive learning approach for detecting machine-generated text in the SemEval-2024 Task 8A. The authors propose using a single Longformer-base model with data augmentation through sentence-level paraphrasing, achieving competitive results (21st out of 137 participants) while using only 149M parameters compared to the 355M parameter baseline. Their key finding demonstrates that contrastive learning with appropriate data augmentation can produce discriminative embeddings for machine-generated text detection without requiring ensemble methods or large model capacity.

## Method Summary
The approach uses a Longformer-base encoder with contrastive learning and data augmentation via sentence-level paraphrasing. Text documents are split into sentences, paraphrased, and reconstructed while preserving formatting. The model learns embeddings through a contrastive loss that separates machine-generated from human-generated text, combined with a standard classification loss. Training uses AdamW optimizer with gradient accumulation, early stopping, and a weighted loss combination (α=0.7, β=0.8, γ=0.1). The classifier head uses two linear layers with 60% dropout.

## Key Results
- Achieved 21st place out of 137 participants on the test dataset
- Used approximately 40% of the baseline's parameters (149M vs. 355M)
- Demonstrated comparable performance to larger baseline models
- Showed effectiveness of single-model approach with contrastive learning and data augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning can learn discriminative embeddings for machine-generated vs. human-generated text detection without relying on ensemble models.
- Mechanism: The model uses a shared encoder to generate embeddings for both positive (machine-generated) and negative (human-generated) pairs, then applies a contrastive loss that pushes embeddings apart when they belong to different classes and pulls them together when they belong to the same class.
- Core assumption: Machine-generated and human-generated text have sufficiently different semantic/structural patterns that can be captured in embedding space and distinguished through contrastive learning.
- Evidence anchors:
  - [abstract]: "Our key finding is that even without an ensemble of multiple models, a single base model can have comparable performance with the help of data augmentation and contrastive learning."
  - [section]: "Our main assumption was that the embedding of the machine-generated text and human-generated text would exhibit significant differences."
- Break condition: If machine-generated texts become indistinguishable from human texts in embedding space (e.g., as LLMs improve), the contrastive loss will fail to separate them.

### Mechanism 2
- Claim: Data augmentation via paraphrasing can effectively create hard negative pairs that improve contrastive learning.
- Mechanism: Each original text is paraphrased at the sentence level to create an alternate version that preserves meaning but changes surface form, providing a positive pair (original machine text + paraphrase) and a negative pair (human text + paraphrase).
- Core assumption: Paraphrased versions maintain semantic similarity while being sufficiently different in surface form to serve as contrastive pairs.
- Evidence anchors:
  - [section]: "Our main inspiration for using contrastive learning is that as the texts come from two different entities (machine vs. human), the embedding space should also be different."
  - [section]: "We have used the human-generated text as the hard negative and the machine-generated text as the soft positive."
- Break condition: If the paraphrase model produces outputs that are too similar to the original (not contrastive enough) or too different (lose semantic connection), the loss signal becomes unreliable.

### Mechanism 3
- Claim: A single model with ~40% of baseline parameters can achieve comparable performance through efficient contrastive learning and data augmentation.
- Mechanism: The model uses a Longformer-base encoder (149M parameters) with a simple classifier head, leveraging contrastive learning to learn discriminative features rather than relying on model size or ensemble methods.
- Core assumption: The combination of contrastive learning objectives and data augmentation can extract sufficient discriminative features without requiring the full parameter capacity of larger models.
- Evidence anchors:
  - [abstract]: "We propose a single model based on contrastive learning, which uses ≈40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset."
  - [section]: "Ourscon is the final submission, and Ourscon+ is the modified version of our final model for more analysis (not official results; used for ablation study - details on §5.2). We can get a comparable result using 60% fewer parameters than the baseline."
- Break condition: If the task requires capturing more complex patterns that need larger capacity models, the smaller model will underperform despite contrastive learning.

## Foundational Learning

- Concept: Contrastive learning and its loss formulations (e.g., contrastive loss, InfoNCE)
  - Why needed here: The entire approach relies on learning discriminative embeddings through contrastive objectives rather than supervised classification alone.
  - Quick check question: What is the difference between contrastive loss and standard cross-entropy loss in terms of what they optimize for?

- Concept: Data augmentation techniques for text (paraphrasing, back-translation, synonym replacement)
  - Why needed here: The method requires creating meaningful positive and negative pairs for contrastive learning, and paraphrasing is the primary augmentation technique used.
  - Quick check question: Why did the authors choose sentence-level paraphrasing instead of paragraph-level or document-level?

- Concept: Long document processing and attention mechanisms (global vs. local attention)
  - Why needed here: The task involves long documents, and Longformer's global/local attention mechanism is specifically chosen to handle this.
  - Quick check question: How does Longformer's attention mechanism differ from standard transformers when processing long documents?

## Architecture Onboarding

- Component map:
  Input -> Preprocessor (sentence splitter) -> Data Augmentation (paraphrase model) -> Encoder (Longformer-base) -> Contrastive Module -> Classifier Head (two linear layers with tanh and 60% dropout) -> Output (binary classification)

- Critical path:
  1. Preprocess input text by splitting into sentences
  2. Generate paraphrases for each sentence
  3. Reconstruct paragraphs with preserved formatting
  4. Encode both original and paraphrased text through shared Longformer
  5. Compute contrastive loss between embeddings
  6. Pass embeddings through classifier head
  7. Compute classification loss
  8. Combine losses and backpropagate

- Design tradeoffs:
  - Single model vs. ensemble: Simplicity and efficiency vs. potential performance gains
  - 60% fewer parameters vs. baseline: Computational efficiency vs. capacity for complex patterns
  - Sentence-level vs. document-level paraphrasing: Better control vs. potential loss of document-level coherence
  - High classifier dropout (60%) vs. lower: Better generalization vs. training stability

- Failure signatures:
  - High contrastive loss but low classification accuracy: Contrastive learning not learning discriminative features
  - Low contrastive loss but high classification accuracy: Model may be overfitting to classification task
  - Performance drops significantly on new generators: Model not generalizing well to unseen distributions
  - Paraphrasing fails to preserve meaning: Data augmentation creating noisy pairs

- First 3 experiments:
  1. Baseline test: Run the official baseline model (RoBERTa-large) on the provided dataset to establish performance reference
  2. Ablation study on dropout: Train the model with classifier dropout rates of 0.0, 0.6, and 0.9 to find optimal regularization
  3. Batch size comparison: Train with effective batch sizes of 2, 16, and 64 to evaluate gradient accumulation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of contrastive learning compare to traditional supervised learning methods for machine-generated text detection, particularly in scenarios with limited labeled data?
- Basis in paper: [explicit] The paper mentions that contrastive learning is used for this task and achieves comparable performance to the baseline, but does not directly compare it to traditional supervised learning methods.
- Why unresolved: The paper does not provide a direct comparison between contrastive learning and traditional supervised learning methods, making it difficult to assess the relative strengths and weaknesses of each approach.
- What evidence would resolve it: A study comparing the performance of contrastive learning and traditional supervised learning methods on machine-generated text detection tasks with varying amounts of labeled data would provide insights into their relative effectiveness.

### Open Question 2
- Question: What is the impact of different data augmentation techniques on the performance of contrastive learning for machine-generated text detection?
- Basis in paper: [explicit] The paper proposes a novel data augmentation technique and shows that it improves the performance of the contrastive learning model, but does not explore the impact of other data augmentation techniques.
- Why unresolved: The paper only investigates one specific data augmentation technique, leaving open the question of how other techniques might affect the performance of contrastive learning for this task.
- What evidence would resolve it: A comprehensive study comparing the performance of contrastive learning models trained with different data augmentation techniques would provide insights into their relative effectiveness.

### Open Question 3
- Question: How does the choice of encoder architecture affect the performance of contrastive learning for machine-generated text detection?
- Basis in paper: [explicit] The paper uses Longformer as the encoder architecture, but does not explore the impact of other encoder architectures on the performance of contrastive learning.
- Why unresolved: The paper only investigates one specific encoder architecture, leaving open the question of how other architectures might affect the performance of contrastive learning for this task.
- What evidence would resolve it: A study comparing the performance of contrastive learning models trained with different encoder architectures would provide insights into their relative effectiveness.

## Limitations

- The contrastive learning implementation details are not fully specified, particularly how positive and negative pairs are constructed from paraphrased text and human-generated samples.
- The effectiveness of sentence-level paraphrasing as data augmentation is not empirically validated with semantic similarity analysis.
- No ablation studies isolate the contribution of contrastive learning versus data augmentation or test different loss weighting schemes.

## Confidence

**High confidence**: The core claim that a single model with data augmentation can achieve competitive performance without ensemble methods is well-supported by the official test results (21st out of 137 participants) and the parameter comparison (149M vs 355M baseline parameters).

**Medium confidence**: The mechanism by which contrastive learning improves performance is plausible but not fully validated. While the theoretical framework is sound, the paper lacks detailed analysis of embedding space separation or contrastive loss behavior during training.

**Low confidence**: The effectiveness of the specific contrastive learning formulation and loss weighting scheme. Without ablation studies or comparison to simpler contrastive approaches, it's unclear whether the complex loss formulation is necessary or optimal.

## Next Checks

1. **Ablation study on loss components**: Systematically vary the contrastive loss weights (α, β, γ) and test performance with contrastive loss removed entirely to isolate its contribution versus standard classification.

2. **Paraphrase quality analysis**: Measure semantic similarity between original and paraphrased sentences using multiple metrics (e.g., BLEU, BERTScore) and correlate this with model performance to validate the data augmentation approach.

3. **Embedding space visualization**: Project learned embeddings into 2D space using t-SNE or UMAP and visualize separation between machine-generated and human-generated texts to verify that contrastive learning creates discriminative representations.