---
ver: rpa2
title: LLMs and the Madness of Crowds
arxiv_id: '2411.01539'
source_url: https://arxiv.org/abs/2411.01539
tags:
- llms
- answer
- answers
- figure
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the non-intuitive patterns in incorrect
  answers produced by large language models (LLMs) during multiple-choice evaluation.
  The author finds that LLMs consistently select incorrect answers in a highly non-random
  manner, with some models choosing the same wrong answer over 99% of the time even
  when prompted repeatedly.
---

# LLMs and the Madness of Crowds

## Quick Facts
- arXiv ID: 2411.01539
- Source URL: https://arxiv.org/abs/2411.01539
- Authors: William F. Bradley
- Reference count: 11
- Key outcome: LLMs consistently select the same incorrect answers across multiple trials, with error patterns showing strong correlations between different models

## Executive Summary
This paper reveals surprising patterns in how large language models (LLMs) produce incorrect answers during multiple-choice evaluation. When LLMs answer questions incorrectly, they don't choose wrong answers randomly - instead, they develop strong preferences for specific incorrect options, sometimes selecting the same wrong answer over 99% of the time even when prompted repeatedly. These error patterns are not only consistent within individual models but also show significant correlations across different LLMs, with pairwise comparisons revealing z-scores from 2.97 to 13.15. The study identifies "universal errors" where all tested models answer the same question incorrectly, and uses hierarchical clustering to reveal distinct groupings of models based on their error correlation patterns.

## Method Summary
The study evaluates 37 LLMs on the MMLU-Pro dataset containing 12,000+ multiple-choice questions. The analysis focuses on problems where models answer incorrectly, calculating pairwise correlations between models based on their probability of selecting the same wrong answer. Z-scores are computed to measure the significance of these correlations, and hierarchical clustering (Ward's method) is applied to the correlation matrix to reveal model groupings. The paper also examines "universal errors" where all models answer incorrectly and analyzes answer distribution patterns across repeated prompts.

## Key Results
- LLMs exhibit non-random error patterns, consistently selecting the same incorrect answers across multiple trials (some models show >99% consistency)
- Error correlations between different LLMs are statistically significant, with z-scores ranging from 2.97 to 13.15
- Hierarchical clustering reveals distinct model groupings: proprietary models form separate clusters from open-source models
- Meta-Llama-3-70B-Instruct shows unique error patterns compared to other models, making it potentially useful as an independent response source
- Universal errors occur where all 37 tested LLMs answer the same question incorrectly, with agreement fractions from 22.6% to 100%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit systematic bias in incorrect answer selection that goes beyond random chance.
- Mechanism: When LLMs produce wrong answers, they develop strong preferences for specific wrong answers rather than distributing probability uniformly across incorrect options.
- Core assumption: Temperature parameter controls sampling randomness, revealing underlying preference structure in incorrect responses.
- Evidence anchors: Models choosing the same wrong answer over 99% of the time; non-intuitive error behaviors unique to each model.

### Mechanism 2
- Claim: Error patterns between different LLMs are strongly correlated, suggesting shared underlying mechanisms or training influences.
- Mechanism: When multiple LLMs make the same errors, they tend to choose the same incorrect answers rather than different ones, creating measurable z-scores.
- Core assumption: Correlation between errors reflects genuine similarities in model behavior rather than random chance.
- Evidence anchors: Pairwise z-scores ranging from 2.97 to 13.15; 1/(k-1) probability of independent uniform selection.

### Mechanism 3
- Claim: LLMs can be taxonomically grouped based on their error correlation patterns, revealing structural relationships between models.
- Mechanism: Hierarchical clustering of error correlations creates meaningful groupings that correspond to model families (proprietary vs open-source).
- Core assumption: Clustering algorithm effectively captures meaningful structure in correlation data.
- Evidence anchors: Distinct groupings of proprietary vs open-source models; Meta Llama models behaving differently from other open-source models.

## Foundational Learning

- Concept: Statistical hypothesis testing and z-score interpretation
  - Why needed here: The paper relies heavily on z-scores to measure the significance of error correlations between models
  - Quick check question: If two models choose the same wrong answer 30 times out of 100 trials when there are 9 wrong answers, what is the z-score?

- Concept: Hierarchical clustering and linkage methods
  - Why needed here: The paper uses hierarchical clustering to group models based on error correlations
  - Quick check question: What's the difference between Ward's method and UPGMA for hierarchical clustering?

- Concept: Multiple-choice test evaluation methodology
  - Why needed here: Understanding how LLMs are evaluated on multiple-choice tests is fundamental to interpreting the results
  - Quick check question: How does temperature affect the probability distribution of answers in LLM multiple-choice evaluation?

## Architecture Onboarding

- Component map: Data collection (MMLU-Pro dataset) -> Analysis pipeline (error correlation calculation using z-scores) -> Clustering engine (hierarchical clustering implementation) -> Visualization layer (dendrogram generation and heatmaps)

- Critical path: Collect error data → Calculate pairwise correlations → Generate z-scores → Perform hierarchical clustering → Visualize results

- Design tradeoffs:
  - Using MMLU-Pro vs other datasets (tradeoff between question difficulty and dataset size)
  - Choice of clustering algorithm (Ward's vs UPGMA) affecting cluster interpretability
  - Temperature setting during evaluation affecting error pattern visibility

- Failure signatures:
  - Low z-scores across all pairs (suggesting no meaningful error correlations)
  - Degenerate clustering (all models in one cluster or completely separated)
  - Inconsistent results across different temperature settings

- First 3 experiments:
  1. Reproduce the z-score calculation for a small subset of questions across 2-3 LLMs
  2. Compare clustering results using different linkage methods (Ward's vs UPGMA)
  3. Test the temperature sensitivity by varying temperature settings and measuring changes in error correlation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What underlying mechanisms cause LLMs to consistently select the same incorrect answers across multiple trials with positive temperature sampling?
- Basis in paper: The paper notes that LLMs show "highly non-intuitive behaviors" and that models like gpt-4o-2024-08-06 chose the same wrong answer over 99% of the time, but the author states this "argues against" data leakage explanations
- Why unresolved: The paper demonstrates the phenomenon but doesn't explain the root cause of why models develop these strong preferences for specific wrong answers
- What evidence would resolve it: Analysis of attention patterns, activation vectors, or internal representations when selecting these answers; comparison with models trained on different datasets or with different architectures

### Open Question 2
- Question: Why does Meta-Llama-3-70B-Instruct show significantly different error patterns compared to other models, including other Llama variants?
- Basis in paper: The paper explicitly states "Meta-Llama-3-70B-Instruct is significantly different from all other models" and suggests it may be "the best choice for any other LLM" as an independent response source
- Why unresolved: The paper observes this unique behavior but doesn't investigate the cause, whether it's due to the "Instruct" fine-tuning, specific training data, or architectural factors
- What evidence would resolve it: Detailed comparison of training data, fine-tuning procedures, and internal model behavior between Llama-3-70B-Instruct and other Llama variants

### Open Question 3
- Question: How does the strong correlation in incorrect answers between different LLMs affect the effectiveness of ensemble methods for improving LLM accuracy?
- Basis in paper: The paper states "One implication of this correlated non-uniformity is that ensembling LLMs may prove much less effective for LLMs than it does with other models"
- Why unresolved: The paper only mentions this implication but doesn't test or quantify how much less effective ensembling actually is for LLMs compared to traditional models
- What evidence would resolve it: Systematic experiments comparing ensemble performance across different model combinations, including cases where models are selected to maximize diversity in error patterns

## Limitations
- The paper doesn't fully address potential confounders such as question difficulty gradients across the MMLU-Pro dataset
- Hierarchical clustering results lack statistical validation measures to confirm the robustness of identified groupings
- The underlying causes of systematic error patterns remain partially speculative and unexplained

## Confidence

**High Confidence**: The finding that LLMs consistently select the same incorrect answers across repeated prompts (Mechanism 1) is well-supported by empirical evidence showing 99%+ consistency rates.

**Medium Confidence**: The claim about systematic correlations between different LLMs (Mechanism 2) is statistically significant but may be influenced by shared training data or evaluation artifacts.

**Medium Confidence**: The taxonomic clustering results (Mechanism 3) show clear patterns but lack rigorous validation and could produce different results with alternative algorithms.

## Next Checks
1. Apply bootstrap resampling to the error correlation matrix and re-run hierarchical clustering 1,000 times to assess cluster stability and calculate consensus trees.

2. Evaluate the same 37 LLMs on an independent multiple-choice dataset (e.g., ARC Challenge or RACE) to determine if error correlation patterns replicate.

3. Systematically vary temperature settings (0.0, 0.5, 1.0, 1.5) across all models and measure how error correlation patterns change to test whether revealed preferences are genuinely structural.