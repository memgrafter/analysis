---
ver: rpa2
title: 'RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture'
arxiv_id: '2401.08406'
source_url: https://arxiv.org/abs/2401.08406
tags:
- context
- gpt-4
- questions
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive pipeline for incorporating
  domain-specific data into large language models (LLMs) using two common approaches:
  Retrieval-Augmented Generation (RAG) and fine-tuning. The pipeline extracts information
  from PDFs, generates questions and answers, and uses them for fine-tuning and RAG.'
---

# RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture

## Quick Facts
- arXiv ID: 2401.08406
- Source URL: https://arxiv.org/abs/2401.08406
- Reference count: 11
- Primary result: Fine-tuning increases accuracy by over 6 percentage points, and RAG further increases accuracy by 5 percentage points

## Executive Summary
This paper presents a comprehensive pipeline for incorporating domain-specific data into large language models (LLMs) using Retrieval-Augmented Generation (RAG) and fine-tuning. The study evaluates multiple popular LLMs on an agricultural dataset, demonstrating that fine-tuning increases accuracy by over 6 percentage points and RAG further increases accuracy by 5 percentage points. The pipeline extracts information from PDFs, generates questions and answers, and uses them for fine-tuning and RAG, showing the effectiveness of combining structured data extraction with RAG and fine-tuning approaches.

## Method Summary
The study implements a multi-stage pipeline that extracts structured information from agricultural PDF documents using GROBID, generates question-answer pairs using GPT-4, and employs both RAG and fine-tuning approaches to improve LLM performance. The pipeline uses FAISS for RAG implementation and LoRA with PyTorch FSDP for fine-tuning. The evaluation employs GPT-4 as a judge to assess accuracy, succinctness, correctness, relevance, groundedness, coverage, diversity, overlap, and fluency across different models including Llama2-13B, GPT-3.5, and GPT-4.

## Key Results
- Fine-tuning increased answer similarity from 47% to 72% in one experiment
- Fine-tuning improves accuracy by over 6 percentage points, and RAG further improves accuracy by 5 percentage points
- GPT-4 consistently outperformed other models but at higher cost

## Why This Works (Mechanism)

### Mechanism 1
The pipeline improves LLM accuracy by combining structured data extraction from PDFs with RAG and fine-tuning. Structured extraction preserves document metadata, sections, and tables, enabling accurate question generation and relevant retrieval. RAG provides grounded context, and fine-tuning adapts the model to domain-specific language.

### Mechanism 2
Fine-tuning the model on generated Q&A pairs increases answer accuracy more than RAG alone. Fine-tuning teaches the model new domain-specific knowledge and response patterns, while RAG provides grounding. The combination is cumulative: fine-tuning improves base accuracy (+6 p.p.), and RAG further improves it (+5 p.p.).

### Mechanism 3
GPT-4 is more effective than smaller models for both generating questions and evaluating answers. GPT-4's superior language understanding allows it to generate more relevant questions, assess answer quality more accurately, and produce more fluent and grounded responses.

## Foundational Learning

- **Document structure extraction from PDFs**: Why needed - Without preserving sections, tables, and metadata, the generated questions and answers lose context and relevance. Quick check - What tool is used to extract structured content from PDFs, and why is it preferred over simpler text extraction?

- **Retrieval-Augmented Generation (RAG) pipeline**: Why needed - RAG grounds LLM answers in domain-specific documents, reducing hallucinations and improving accuracy. Quick check - What are the three main steps in the RAG pipeline described in the paper?

- **Fine-tuning vs instruction-tuning**: Why needed - Fine-tuning adapts the model to new knowledge, while instruction-tuning teaches it to follow instructions; both are used here for different purposes. Quick check - What is the difference between the Llama2-13B-chat and Vicuna models in terms of fine-tuning approach?

## Architecture Onboarding

- **Component map**: Data Acquisition → PDF Information Extraction (GROBID) → Question Generation (GPT-4) → Answer Generation (RAG) → Fine-tuning (Llama2-13B, GPT-4) → Evaluation (GPT-4 metrics)
- **Critical path**: PDF extraction → Q&A generation → RAG answer generation → fine-tuning → evaluation
- **Design tradeoffs**: Accuracy vs cost (GPT-4 is most accurate but expensive; smaller models + RAG may be cheaper); token usage (generating questions and answers separately saves tokens vs combined generation); dataset size (more documents improve recall but increase FAISS index size and collision risk)
- **Failure signatures**: Low recall in RAG → irrelevant answers, high hallucination; poor PDF parsing → irrelevant questions, loss of context; noisy fine-tuning data → decreased accuracy or model collapse
- **First 3 experiments**: 1) Compare question generation quality with and without external context using GPT-3.5; 2) Evaluate RAG retrieval recall at different top-k snippet counts on the Washington dataset; 3) Measure accuracy improvement from fine-tuning Llama2-13B on agricultural Q&A pairs vs base model

## Open Questions the Paper Calls Out

### Open Question 1
How can the performance of LLMs in agriculture be improved beyond fine-tuning and RAG? The paper mentions that future work could involve combining structured information from PDFs with images and captions from the same documents to enable multi-modal fine-tuning opportunities, but does not explore other potential methods for improving LLM performance in agriculture beyond fine-tuning and RAG.

### Open Question 2
How does the size and diversity of the training dataset affect the performance of fine-tuned LLMs in agriculture? The paper fine-tunes GPT-4 on a dataset of questions extracted from the Washington state dataset but does not explore how the size and diversity of the training dataset impact the model's performance or examine the optimal size and diversity for achieving the best results.

### Open Question 3
How can LLMs be adapted to handle specific agricultural challenges, such as pest and disease identification or crop yield prediction? The paper demonstrates the potential of fine-tuning and RAG in improving LLM performance in agriculture but does not explore how LLMs can be specifically adapted to address agricultural challenges or examine the effectiveness of using domain-specific knowledge or incorporating external data sources.

## Limitations
- PDF parsing quality relies heavily on GROBID with no evaluation of parsing accuracy or completeness
- GPT-4 evaluator bias may introduce circular validation as no ground-truth human labels are used
- Results are based on agricultural datasets from three countries and may not generalize to other domains

## Confidence

- **High confidence**: Claims about RAG and fine-tuning improving accuracy (supported by reported percentage-point gains and qualitative improvements)
- **Medium confidence**: Claims about GPT-4 superiority (based on consistent outperformance but potential evaluator bias)
- **Low confidence**: Claims about optimal dataset size or embedding parameters (not experimentally validated)

## Next Checks

1. Parse quality audit: Manually evaluate a sample of GROBID-extracted documents against originals to confirm structural fidelity
2. Human evaluation: Have domain experts rate a subset of generated Q&A pairs and model answers for relevance and correctness
3. Cost-benefit analysis: Measure actual token usage and runtime costs for GPT-4 fine-tuning vs smaller models + RAG across different dataset sizes