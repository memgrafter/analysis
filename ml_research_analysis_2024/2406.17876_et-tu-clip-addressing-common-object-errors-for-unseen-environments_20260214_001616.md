---
ver: rpa2
title: ET tu, CLIP? Addressing Common Object Errors for Unseen Environments
arxiv_id: '2406.17876'
source_url: https://arxiv.org/abs/2406.17876
tags:
- clip
- object
- arxiv
- language
- et-clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using CLIP as an auxiliary module with an object
  detection loss to improve model generalization in ALFRED. Unlike prior work that
  replaces the visual encoder with CLIP, the authors add a CLIP-based object detection
  objective to the Episodic Transformer architecture.
---

# ET tu, CLIP? Addressing Common Object Errors for Unseen Environments

## Quick Facts
- arXiv ID: 2406.17876
- Source URL: https://arxiv.org/abs/2406.17876
- Reference count: 15
- Primary result: Improves ALFRED success rates by 0.5-0.8% on unseen validation scenes for small objects and rare semantics using CLIP as auxiliary module

## Executive Summary
This paper addresses generalization challenges in embodied instruction following by integrating CLIP as an auxiliary module for object detection in the Episodic Transformer (ET) architecture. Unlike prior approaches that replace the visual encoder with CLIP, the authors add a CLIP-based object detection objective to enhance the base model's ability to handle small objects, rare words, and object property descriptions. The method achieves modest but consistent improvements on ALFRED's unseen validation split, demonstrating that CLIP's pre-training on image-text pairs can provide complementary visual information that helps models generalize to novel environments.

## Method Summary
The authors propose adding a CLIP-based object detection loss as an auxiliary objective to the Episodic Transformer architecture for embodied instruction following. During training, both the base ET model and a CLIP encoder process camera observations, each producing object predictions. These predictions are combined with a weighted loss function where the final object predictions come only from ET during inference. The weighting coefficient α=0.5 balances the two loss terms to ensure similar magnitudes. This approach leverages CLIP's pre-trained vision-language alignment to improve generalization without replacing the task-specific optimizations of the base model.

## Key Results
- 0.5% improvement in goal-conditioned success rate for instructions involving small objects (e.g., pencil, keys)
- 0.8% improvement for instructions containing rare semantic words
- 0.3% improvement for leveraging object descriptions stated in language directives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's pre-training provides scale-invariant object representations that help detect small objects.
- Mechanism: CLIP learns semantic meaning from diverse image-caption pairs, allowing recognition of objects regardless of visual scale.
- Core assumption: CLIP's pre-training corpus contains sufficient small object examples.
- Evidence anchors: Abstract mentions semantics even when objects are small; section reports 0.5% improvement for small objects; weak corpus support for small object detection specifically.
- Break condition: If CLIP's pre-training lacks small object examples or fails to generalize scale-invariant features.

### Mechanism 2
- Claim: CLIP's language-vision alignment helps interpret rare words through pre-training knowledge.
- Mechanism: Infrequent words in ALFRED can be understood through CLIP's exposure to diverse vocabulary during pre-training.
- Core assumption: CLIP's pre-training covers rare words encountered in ALFRED.
- Evidence anchors: Abstract mentions 0.8% improvement for rare semantics; section notes numerous captions help interpret rare words; no direct corpus support for rare word handling.
- Break condition: If rare words are highly domain-specific or CLIP lacks exposure to similar concepts.

### Mechanism 3
- Claim: Auxiliary object detection loss provides complementary visual information for better language-semantic alignment.
- Mechanism: Computing additional loss from CLIP outputs alongside base predictions encourages better alignment between visual features and language semantics.
- Core assumption: Weighted combination effectively improves alignment without destabilizing training.
- Evidence anchors: Abstract mentions improvements for object descriptions, small objects, and rare words; section notes better leveraging of visual cues; no direct corpus support but auxiliary losses are well-established.
- Break condition: If α is poorly chosen or CLIP predictions conflict with base model representations.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: CLIP's core capability comes from aligning image and text embeddings in a shared space during pre-training.
  - Quick check question: What is the main difference between contrastive learning and supervised classification in the context of CLIP's training?

- Concept: Multimodal embeddings
  - Why needed here: The method relies on projecting both visual and language information into a common embedding space for comparison and loss computation.
  - Quick check question: How does CLIP ensure that semantically similar images and text descriptions have similar embeddings in the joint space?

- Concept: Auxiliary loss training
  - Why needed here: The approach uses an additional loss term from CLIP to regularize and enhance the base model's learning process.
  - Quick check question: What are the potential risks of adding an auxiliary loss, and how might they be mitigated in this architecture?

## Architecture Onboarding

- Component map: Base ET model (visual encoder + transformer) → CLIP encoder module → object detection heads (two: one for ET, one for CLIP) → weighted loss combination → final object predictions from ET only during inference
- Critical path: Camera observation → both ET visual encoder and CLIP encoder → object classification → loss computation (weighted sum) → backpropagate to both ET and CLIP (training only)
- Design tradeoffs: Using CLIP as auxiliary module preserves ET's architecture while gaining CLIP benefits, but adds computational overhead during training; using CLIP as replacement would be simpler but might lose ET's task-specific optimizations
- Failure signatures: Poor performance improvement suggests misalignment between CLIP and ET representations, or improper weighting coefficient; overfitting to seen environments indicates CLIP isn't generalizing as expected
- First 3 experiments:
  1. Validate that CLIP object detection loss is non-zero and meaningful by checking CLIP's object predictions on validation set
  2. Perform ablation study with different α values (0.3, 0.5, 0.7) to find optimal weighting
  3. Test on a subset of instructions with clear object properties to verify the 0.3% improvement claim before full evaluation

## Open Questions the Paper Calls Out
- How does the choice of weighting coefficient α affect performance across different instruction types?
- Does the CLIP-based auxiliary loss maintain benefits when transferred to other ALFRED models beyond Episodic Transformer?
- What is the impact of CLIP pre-training data diversity on effectiveness for detecting objects in ALFRED's indoor environments?

## Limitations
- Limited empirical validation with only marginal improvements (0.5-0.8%) that may not justify added complexity
- Architecture specificity tightly coupled to Episodic Transformer without testing generalizability
- Computational overhead doubles forward pass during training without cost-benefit analysis

## Confidence
- Medium: General mechanism that CLIP helps with small objects and rare semantics is reasonably supported
- Low: Claim that α=0.5 is optimal is not well-supported without ablation studies
- Medium: Architectural design choice to use CLIP as auxiliary is defensible but not thoroughly evaluated

## Next Checks
1. Conduct ablation study on α parameter across values (0.3, 0.5, 0.7, 1.0) to determine optimal weighting and sensitivity
2. Implement CLIP auxiliary module approach on different baseline model (e.g., original Transformer) to test generalizability
3. Evaluate trained ET-CLIP model on completely unseen environments from different datasets to assess true generalization capability