---
ver: rpa2
title: Evaluating the Translation Performance of Large Language Models Based on Euas-20
arxiv_id: '2408.03119'
source_url: https://arxiv.org/abs/2408.03119
tags:
- translation
- language
- llms
- languages
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the translation performance of nine large
  language models (LLMs) across 20 languages using a newly constructed dataset called
  Euas-20. The study focuses on comparing translation quality between English and
  Chinese, and investigates how pre-training data affects performance.
---

# Evaluating the Translation Performance of Large Language Models Based on Euas-20

## Quick Facts
- **arXiv ID**: 2408.03119
- **Source URL**: https://arxiv.org/abs/2408.03119
- **Reference count**: 29
- **Primary result**: Translation performance has improved significantly, especially in Llama-3, but remains uneven across languages

## Executive Summary
This paper evaluates the translation performance of nine large language models across 20 languages using a newly constructed dataset called Euas-20. The study reveals that while translation abilities have improved significantly—particularly in Llama-3—performance remains uneven, with high-resource languages translating better than low-resource ones. Multilingual models generally outperform monolingual ones, and larger, more diverse training corpora lead to better translation quality. The study also identifies translation hallucinations and vocabulary limitations in LLMs.

## Method Summary
The study evaluates nine large language models using the Euas-20 dataset, focusing on translation quality between English and Chinese. The evaluation examines how pre-training data affects performance and compares multilingual versus monolingual models. The research methodology includes analyzing translation quality across 20 languages, with particular attention to resource availability and model architecture differences.

## Key Results
- Translation performance has improved significantly, especially in Llama-3
- High-resource languages consistently outperform low-resource languages
- Multilingual models generally outperform monolingual ones
- Larger, more diverse training corpora lead to better translation quality

## Why This Works (Mechanism)
The improved translation performance in newer models like Llama-3 likely stems from larger model architectures and more extensive, diverse training corpora. The disparity between high-resource and low-resource languages reflects the fundamental bias in training data availability, where languages with more available digital content receive better representation during pre-training. Multilingual models benefit from shared linguistic structures across languages, enabling better cross-lingual transfer learning.

## Foundational Learning
- **Large Language Models**: AI systems trained on vast text corpora that can generate human-like text and perform translation tasks. Needed to understand the baseline capabilities being evaluated. Quick check: Verify model size and training data statistics.
- **Translation Quality Metrics**: Methods for evaluating translation accuracy and fluency. Needed to assess performance across different languages. Quick check: Confirm evaluation metrics used (BLEU, human evaluation, etc.).
- **High-resource vs Low-resource Languages**: Classification based on available training data. Needed to understand performance disparities. Quick check: Verify language categorization and data availability statistics.
- **Multilingual Models**: Models trained on multiple languages simultaneously. Needed to compare performance against monolingual alternatives. Quick check: Confirm number of languages each model supports.

## Architecture Onboarding

**Component Map**: Input text -> Preprocessing -> Translation Model -> Output text

**Critical Path**: Data input → Language identification → Translation generation → Quality evaluation

**Design Tradeoffs**: 
- Multilingual models offer broader coverage but may sacrifice depth in individual languages
- Larger models provide better translation quality but require more computational resources
- High-resource language focus improves overall performance but creates bias

**Failure Signatures**: 
- Translation hallucinations in low-resource languages
- Vocabulary limitations for rare words
- Performance degradation when translating between two low-resource languages

**First Experiments**:
1. Test baseline translation quality on high-resource language pairs
2. Evaluate translation accuracy for low-resource languages
3. Compare multilingual vs monolingual model performance on identical language pairs

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Reliance on newly constructed Euas-20 dataset without extensive validation of representativeness across all 20 languages
- Focus primarily on English-Chinese translation pairs may limit generalizability
- Methodology for detecting translation hallucinations and vocabulary limitations not detailed

## Confidence

**High confidence**:
- General observation that translation performance has improved significantly in newer models
- High-resource languages consistently outperform low-resource ones

**Medium confidence**:
- Claim that larger, more diverse training corpora lead to better translation quality

**Low confidence**:
- Conclusion that "multilingual models generally outperform monolingual ones"

## Next Checks
1. Conduct comprehensive validation of the Euas-20 dataset by having native speakers across all 20 languages verify translation quality
2. Perform systematic ablation studies on training corpora to quantify impact of data size and diversity
3. Implement automated hallucination detection and vocabulary gap analysis across all evaluated models