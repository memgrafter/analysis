---
ver: rpa2
title: Graph data augmentation with Gromow-Wasserstein Barycenters
arxiv_id: '2404.08376'
source_url: https://arxiv.org/abs/2404.08376
tags:
- graphs
- graph
- graphon
- data
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel graph data augmentation strategy that
  operates in a non-Euclidean space using graphon estimation. The method leverages
  the Gromow-Wasserstein distance to learn graphons from sequences of graphs, which
  serve as generative models to sample synthetic graphs.
---

# Graph data augmentation with Gromow-Wasserstein Barycenters

## Quick Facts
- arXiv ID: 2404.08376
- Source URL: https://arxiv.org/abs/2404.08376
- Authors: Andrea Ponti
- Reference count: 22
- Primary result: Graph classification accuracy improved by 0.2%-6.1% using graphon-based augmentation

## Executive Summary
This paper proposes a novel graph data augmentation strategy that operates in a non-Euclidean space using graphon estimation. The method leverages the Gromov-Wasserstein distance to learn graphons from sequences of graphs, which serve as generative models to sample synthetic graphs. The framework improves graph classification performance, with accuracy gains ranging from 0.2% to 6.1% depending on the dataset and augmentation level. Using the Gromow-Wasserstein distance for graphon estimation generally leads to better results compared to other methods. The approach also provides a means to validate different graphon estimation approaches in real-world scenarios where the true graphon is unknown.

## Method Summary
The paper presents a graph data augmentation framework that estimates graphons from training graphs using various methods (SAS, SBA, LG, MC, GB, SGB) and generates synthetic graphs from these estimated graphons. The augmentation is performed by sampling new graphs from the learned graphons and adding them to the training dataset. The method is evaluated on three graph classification datasets (LFR, IMDB, ENZYMES) with graph2vec for graph embeddings and MLP for classification. Different augmentation levels (1%, 5%, 10%, 25%) are tested to determine the optimal amount of synthetic data to add.

## Key Results
- Classification accuracy improved by 0.2%-6.1% when using augmented data
- Gromow-Wasserstein Barycenters (GB and SGB) generally outperformed other graphon estimation methods
- 1% augmentation was often sufficient to achieve significant performance gains
- The method works well for both binary and multi-class graph classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graphon-based augmentation preserves class-specific graph structure while expanding dataset diversity.
- Mechanism: For each class, a graphon is estimated from the training graphs. New graphs are then sampled from this graphon, generating synthetic examples that inherit the probabilistic edge formation patterns of the class.
- Core assumption: The training graphs of each class are drawn from a common underlying graphon that captures the class's structural distribution.
- Evidence anchors:
  - [abstract] "Consider the supervised problem of classify a set of M graphs G = {Gm}M m=1 into N classes. Denote with Gi the set of graphs belonging to class i. Then, for each class i, a graphon Wi can be estimated from the sequence of graphs Gi."
  - [section] "In the view of graphons as generative models of graph objects they become particularly useful to augment graph datasets."
- Break Condition: If the true graphon is not class-specific (e.g., classes share the same generative process), augmentation will not improve classification.

### Mechanism 2
- Claim: Using Gromov-Wasserstein Barycenters improves graphon estimation accuracy by avoiding node alignment.
- Mechanism: Gromov-Wasserstein distance is permutation-invariant, so it naturally handles unaligned graphs without requiring explicit node correspondence. This leads to more accurate graphon estimation, especially in real-world scenarios.
- Core assumption: Graphs in the dataset are generated by a common graphon but are not aligned.
- Evidence anchors:
  - [abstract] "This approach leverages graphon estimation, which models the generative mechanism of networks sequences. Computational results demonstrate the effectiveness of the proposed augmentation framework in improving the performance of graph classification models."
  - [section] "To overcome these limitations, the authors in [20] proposed an approach that leverages the permutation-invariance of the Gromov-Wasserstein (GW) distance, implicitly including the graph aligning in the estimation phase."
- Break Condition: If all graphs are already perfectly aligned, the GW distance provides no additional benefit over traditional alignment-based methods.

### Mechanism 3
- Claim: Non-Euclidean Gromov-Wasserstein distance better captures graph similarity than Euclidean metrics.
- Mechanism: The GW distance measures similarity between graphs by comparing their edge distribution patterns, not just node features. This aligns better with the intrinsic non-Euclidean structure of graphs.
- Core assumption: Graph similarity is best measured by comparing their edge distribution patterns rather than node embeddings.
- Evidence anchors:
  - [abstract] "Experimental results, in a graph classification context, show an improvement in performance when using the augmented dataset. Furthermore, results show that using a non-Euclidean distance results in a better approximation of the graphon."
  - [section] "Then, the learning problem becomes the estimation of a GW barycenter of the observed graphs min W∈[0,1]K×K 1 M MX m=1 d2 gw,2(Am, W)."
- Break Condition: If graphs are fundamentally Euclidean (e.g., node features are the primary source of similarity), the GW distance may not provide significant benefits.

## Foundational Learning

- Concept: Graphons as generative models
  - Why needed here: Graphons provide a principled way to model the probabilistic structure of graphs and generate new synthetic examples.
  - Quick check question: How does a graphon define the probability of an edge between two nodes in a generated graph?

- Concept: Gromov-Wasserstein distance
  - Why needed here: The GW distance allows for permutation-invariant comparison of graphs, which is crucial for estimating graphons from unaligned graph sequences.
  - Quick check question: What makes the Gromov-Wasserstein distance different from the standard Wasserstein distance when applied to graphs?

- Concept: Graph classification
  - Why needed here: The augmentation framework is evaluated in the context of graph classification tasks, so understanding the evaluation metrics and baselines is important.
  - Quick check question: What are the common evaluation metrics used in graph classification tasks?

## Architecture Onboarding

- Component map: Graphon Estimation -> Graph Sampling -> Augmentation Pipeline -> Classification Model
- Critical path: Graphon Estimation → Graph Sampling → Augmentation Pipeline → Classification Model
- Design tradeoffs:
  - Accuracy vs. Efficiency: More accurate graphon estimation methods (e.g., GW Barycenters) are computationally more expensive.
  - Augmentation Size: Increasing the augmentation size may improve performance up to a point, but excessive augmentation can lead to overfitting or diminishing returns.
- Failure signatures:
  - Poor Classification Performance: Indicates that the graphon estimation or sampling process is not capturing the true class structure.
  - High Computational Cost: Suggests that the graphon estimation method is too computationally intensive for the dataset size.
- First 3 experiments:
  1. Estimate a graphon from a simple synthetic graph dataset and visualize the estimated graphon to verify its structure.
  2. Generate synthetic graphs from the estimated graphon and compare their properties (e.g., degree distribution) to the original graphs.
  3. Augment a small graph classification dataset with synthetic graphs and evaluate the impact on classification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of graphon estimation method (e.g., SAS, SBA, LG, MC, GB, SGB) impact the quality and diversity of generated synthetic graphs for augmentation?
- Basis in paper: [explicit] The paper compares multiple graphon estimation methods and shows that Gromow-Wasserstein Barycenters (GB and SGB) generally lead to better performance in classification tasks.
- Why unresolved: While the paper shows GB and SGB perform better overall, it doesn't provide a detailed analysis of how each method affects the characteristics of the generated graphs or their diversity.
- What evidence would resolve it: A systematic comparison of the generated graphs' properties (e.g., degree distribution, clustering coefficient, motif counts) across different estimation methods, along with diversity metrics.

### Open Question 2
- Question: What is the optimal percentage of synthetic graphs to add for augmentation across different types of graph datasets and classification tasks?
- Basis in paper: [explicit] The paper tests augmentation levels of 1%, 5%, 10%, and 25% and finds that 1% is often sufficient for significant improvement.
- Why unresolved: The optimal augmentation level may depend on factors like dataset size, graph complexity, and the difficulty of the classification task. The paper only tests a limited range of percentages on a few datasets.
- What evidence would resolve it: Extensive experiments varying the augmentation percentage across diverse datasets and tasks, potentially including adaptive augmentation strategies that adjust the percentage based on dataset characteristics.

### Open Question 3
- Question: How does the proposed graphon-based augmentation approach compare to other graph generation methods, such as diffusion models, in terms of quality and computational efficiency?
- Basis in paper: [explicit] The paper mentions that diffusion models have been recently proposed for graph generation but are more computationally expensive than graphon estimation.
- Why unresolved: The paper doesn't directly compare the proposed method with other state-of-the-art graph generation techniques. It only mentions diffusion models as a potential alternative.
- What evidence would resolve it: Direct comparisons of classification performance and computational cost between the proposed method and other graph generation approaches (e.g., diffusion models, GANs for graphs) across multiple datasets and tasks.

## Limitations
- The paper lacks complete implementation details for all six graphon estimation methods, making faithful reproduction challenging without accessing external references.
- No ablation studies are provided to isolate the contribution of the Gromov-Wasserstein distance from the overall augmentation framework.
- The datasets used (LFR, IMDB, ENZYMES) have different characteristics, making it unclear whether the method generalizes to other graph types.

## Confidence
- **High confidence**: The core methodology of using graphons as generative models for augmentation is well-established in the literature.
- **Medium confidence**: The specific claim that Gromov-Wasserstein distance leads to better graphon estimation than other methods is supported by experimental results but lacks theoretical justification.
- **Medium confidence**: The classification accuracy improvements (0.2%-6.1%) are demonstrated but may be sensitive to hyperparameter choices and dataset splits.

## Next Checks
1. Implement and compare all six graphon estimation methods on a simple synthetic dataset with known ground truth to verify their relative performance.
2. Conduct an ablation study where the augmentation pipeline uses standard Wasserstein distance instead of Gromov-Wasserstein to isolate the contribution of the non-Euclidean distance.
3. Test the framework on additional graph datasets (e.g., COLLAB, PROTEINS) to evaluate generalization beyond the three datasets presented.