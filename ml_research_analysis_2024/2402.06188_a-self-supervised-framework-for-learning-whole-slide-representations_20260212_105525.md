---
ver: rpa2
title: A self-supervised framework for learning whole slide representations
arxiv_id: '2402.06188'
source_url: https://arxiv.org/abs/2402.06188
tags:
- patch
- learning
- slide
- whole
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a self-supervised framework called Slide Pre-trained
  Transformers (SPT) for learning whole slide representations in computational pathology.
  SPT treats gigapixel whole slide images (WSIs) as sequences of patch tokens and
  applies data transformation strategies inspired by both vision and language modeling
  to generate high-quality views for self-supervised pretraining.
---

# A self-supervised framework for learning whole slide representations

## Quick Facts
- arXiv ID: 2402.06188
- Source URL: https://arxiv.org/abs/2402.06188
- Reference count: 40
- Outperforms previous state-of-the-art self-supervised and supervised methods, achieving up to 10 points increase in mean class accuracy

## Executive Summary
This paper presents SPT (Slide Pre-trained Transformers), a self-supervised framework for learning whole slide representations in computational pathology. SPT treats gigapixel whole slide images as sequences of patch tokens and applies data transformation strategies inspired by vision and language modeling. The method combines splitting, cropping, and masking transformations to capture regional heterogeneity, reduce redundant visual features, and decrease mutual information between views. Evaluated on five diagnostic tasks across three biomedical microscopy datasets, SPT significantly outperforms previous state-of-the-art self-supervised and supervised methods.

## Method Summary
SPT is a two-stage learning framework that learns whole slide representations from gigapixel images. It uses a pre-trained patch encoder to extract patch-level features, then applies a transformer whole slide encoder to aggregate these features. The key innovation is SPT's transformation strategy, which combines splitting (to decrease mutual information), cropping (to capture regional heterogeneity), and masking (to reduce redundant features). This framework can be adapted to various self-supervised paradigms including SimCLR, BYOL, and VICReg, as well as supervised contrastive learning. SPT demonstrates consistent performance improvements across different patch encoder types including off-the-shelf, in-domain, and foundational models.

## Key Results
- SPT significantly outperforms previous state-of-the-art self-supervised and supervised methods
- Achieves up to 10 points increase in mean class accuracy on some benchmarks
- Demonstrates consistent performance improvements when using off-the-shelf, in-domain, and foundational patch encoders
- Shows effective transfer learning capabilities across multiple diagnostic tasks and datasets

## Why This Works (Mechanism)

### Mechanism 1
Treating gigapixel WSIs as sequences of patch tokens and applying splitting, cropping, and masking transformations generates high-quality views for self-supervised pretraining. The transformation strategy is informed by the unique properties of WSIs, such as regional heterogeneity, histologic feature variability, and information redundancy. Splitting decreases mutual information between views, cropping captures regional heterogeneity, and masking reduces redundant visual features. Core assumption: These transformations can generate diverse and informative views of WSIs that are suitable for self-supervised learning. Evidence anchors: [abstract] "SPT leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy within WSIs to learn high-quality whole slide representations." [section 3.2] "SPT transformation strategy...is inspired by both vision and language modeling, and was selected to address the domain-specific properties of WSI"

### Mechanism 2
The SPT framework can be adapted to both self-supervised and supervised learning paradigms. SPT uses a two-stage model architecture: a pre-trained patch encoder and a transformer whole slide encoder. The patch encoder extracts patch-level features, and the transformer encoder aggregates these features to learn whole slide representations. SPT can use any learning paradigm for slide-level feature learning, including self-supervised methods like SimCLR, BYOL, and VICReg, as well as supervised contrastive learning. Core assumption: The two-stage architecture allows for efficient training and can be easily adapted to different learning paradigms. Evidence anchors: [section 3.1] "SPT is a two-stage learning framework to learn WSI representations: 1) a pre-trained patch encoder E; and 2) a transformer whole slide encoder f." [section 3.1] "SPT can be adapted to fully supervised training using weak slide- or patient-level labels, by applying a supervised contrastive loss."

### Mechanism 3
SPT improves whole slide representations when using off-the-shelf, in-domain, and foundational patch encoders. SPT can be used with a wide range of patch encoders, including those trained on ImageNet, TCGA, OpenPath, and large-scale institutional datasets. SPT training significantly improves whole slide representations over pooling baselines, especially for out-of-distribution and foundation patch encoders. Core assumption: SPT can effectively leverage the features learned by different patch encoders to improve whole slide representations. Evidence anchors: [abstract] "SPT offers a consistent performance boost across a wide range of patch encoders." [section 5.3] "For all patch encoders, SPT training significantly improves whole slide representations over pooling baselines."

## Foundational Learning

- **Concept: Self-supervised learning**
  - Why needed here: WSIs are gigapixel-sized and contain diverse morphologic features and spatial heterogeneity. Traditional supervised learning methods rely on weak labels, which are sparse and expensive to obtain. Self-supervised learning allows SPT to learn whole slide representations without the need for dense annotations.
  - Quick check question: What are the main challenges of using traditional supervised learning methods for whole slide image analysis?

- **Concept: Multiple instance learning**
  - Why needed here: WSIs are typically divided into smaller patches, and each patch is considered an instance. The WSI is treated as a bag of patches, and the goal is to classify the WSI based on the features of its constituent patches. Multiple instance learning allows SPT to learn whole slide representations from these patch-level features.
  - Quick check question: How does multiple instance learning differ from traditional supervised learning methods?

- **Concept: Transformer architectures**
  - Why needed here: Transformers have shown great success in various natural language processing and computer vision tasks. They can effectively capture long-range dependencies and learn complex representations. In SPT, transformers are used to aggregate patch-level features and learn whole slide representations.
  - Quick check question: What are the key advantages of using transformer architectures for whole slide image analysis?

## Architecture Onboarding

- **Component map**: Patch encoder (E) -> Whole slide encoder (f) -> Transformations (splitting, cropping, masking) -> Learning paradigm
- **Critical path**:
  1. Pre-train the patch encoder on a large dataset (e.g., ImageNet, TCGA).
  2. Divide WSIs into patches and extract patch-level features using the pre-trained patch encoder.
  3. Apply SPT transformations to generate diverse views of WSIs.
  4. Train the whole slide encoder using the chosen learning paradigm and the transformed views.
  5. Evaluate the learned whole slide representations on downstream tasks.
- **Design tradeoffs**:
  - Tradeoff between model complexity and computational efficiency: Larger models may achieve better performance but require more computational resources.
  - Tradeoff between transformation diversity and training stability: More diverse transformations may lead to better representations but can also make training more challenging.
  - Tradeoff between self-supervised and supervised learning: Self-supervised learning does not require labels but may not achieve the same level of performance as supervised learning on certain tasks.
- **Failure signatures**:
  - Poor performance on downstream tasks: Indicates that the learned representations are not useful or transferable.
  - High computational cost: Suggests that the model is too complex or the training process is inefficient.
  - Unstable training: May be caused by inappropriate transformations or learning paradigms.
- **First 3 experiments**:
  1. Evaluate the performance of SPT with different patch encoders on a small benchmark dataset.
  2. Compare the performance of SPT with different learning paradigms on a larger benchmark dataset.
  3. Analyze the impact of different transformation parameters (e.g., cropping size, masking ratio) on the learned representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SPT's performance scale with dataset size, particularly for extremely large datasets with millions of WSIs?
- Basis in paper: [inferred] The paper demonstrates SPT's effectiveness on datasets with thousands of WSIs, but does not explore performance on orders of magnitude larger datasets.
- Why unresolved: The paper's experiments are limited to datasets with up to a few thousand WSIs, leaving scalability questions unanswered.
- What evidence would resolve it: Experiments on datasets with millions of WSIs, comparing SPT's performance and training efficiency against other methods as dataset size increases.

### Open Question 2
- Question: What is the optimal balance between the three transformations (splitting, cropping, masking) for different tissue types and diagnostic tasks?
- Basis in paper: [explicit] The paper mentions that hyperparameter tuning for different configurations of SPT transformations is domain and dataset-specific.
- Why unresolved: The paper does not provide a systematic method for determining the optimal balance of transformations for specific use cases.
- What evidence would resolve it: A comprehensive study across multiple tissue types and diagnostic tasks, identifying patterns in optimal transformation configurations and developing a principled method for determining these settings.

### Open Question 3
- Question: How does SPT's performance compare to supervised methods when labeled data is available, and at what point does the benefit of self-supervision diminish?
- Basis in paper: [explicit] The paper shows that ssSPT approaches the performance of suSPT, but does not directly compare self-supervised performance to fully supervised methods when labeled data is abundant.
- Why unresolved: The paper focuses on scenarios with limited or no labels, leaving the question of SPT's value proposition in the presence of labeled data unexplored.
- What evidence would resolve it: Experiments comparing SPT's performance to fully supervised methods across a range of labeled data availability scenarios, identifying the break-even point where supervised learning outperforms self-supervision.

## Limitations

- Major uncertainties remain regarding the exact implementation details of the SPT transformations, particularly the specific parameter ranges for splitting ratios, cropping areas, and masking ratios across different experiments.
- The computational requirements for training SPT on gigapixel WSIs are substantial, with reported training times of up to 48 hours per epoch on 4 GPUs, though memory usage details are not provided.
- The method is evaluated on five tasks across three datasets, but all are within the cancer histopathology domain, limiting claims about broader applicability.

## Confidence

- **High confidence** in the core mechanism: treating WSIs as patch token sequences and applying informed transformations for self-supervised pretraining is well-supported by the results and grounded in established vision-language modeling principles
- **Medium confidence** in the claim of consistent performance improvements across all patch encoder types: while the paper shows improvements, the magnitude varies significantly and depends on the specific encoder and benchmark
- **Medium confidence** in the generalization claims: the method is evaluated on five tasks across three datasets, but all are within the cancer histopathology domain, limiting claims about broader applicability

## Next Checks

1. **Reproduce transformation parameters**: Systematically vary the splitting ratio, cropping area, and masking ratio parameters to determine their optimal ranges and verify the claimed 10-point MCA improvements
2. **Cross-domain validation**: Test SPT on non-cancer histopathology datasets (e.g., renal pathology, liver pathology) to assess generalization beyond the cancer domain
3. **Ablation of two-stage architecture**: Compare SPT performance with end-to-end training approaches to quantify the benefits of the two-stage design and pre-training strategy