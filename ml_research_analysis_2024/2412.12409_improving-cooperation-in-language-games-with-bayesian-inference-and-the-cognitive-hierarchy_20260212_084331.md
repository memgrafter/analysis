---
ver: rpa2
title: Improving Cooperation in Language Games with Bayesian Inference and the Cognitive
  Hierarchy
arxiv_id: '2412.12409'
source_url: https://arxiv.org/abs/2412.12409
tags:
- bayesian
- clue
- guesser
- agents
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Bayesian approach to improve cooperation in
  language games like Codenames, where agents must communicate using natural language
  but may have differing understandings of semantics (word relationships) and pragmatics
  (language use strategies). The method involves maintaining a probability distribution
  over possible teammate models (word embeddings and reasoning levels), updating these
  beliefs using Bayesian inference as observations are made, and selecting actions
  to maximize expected utility.
---

# Improving Cooperation in Language Games with Bayesian Inference and the Cognitive Hierarchy

## Quick Facts
- arXiv ID: 2412.12409
- Source URL: https://arxiv.org/abs/2412.12409
- Reference count: 40
- Primary result: Bayesian spymasters show win rates up to 0.872 vs. 0.718 for baseline against out-of-distribution guessers

## Executive Summary
This paper addresses cooperation challenges in language games like Codenames where agents may have differing understandings of word relationships and language use strategies. The proposed solution uses Bayesian inference to maintain probability distributions over possible teammate models (word embeddings and reasoning levels), updating beliefs as observations are made. The approach is tested on Codenames, showing significant improvements when Bayesian spymasters face out-of-distribution guessers, demonstrating robustness to semantic and pragmatic uncertainty that previous approaches lacked.

## Method Summary
The method involves maintaining a probability distribution over possible teammate models, updating these beliefs using Bayesian inference as observations are made, and selecting actions to maximize expected utility. Bayesian agents model uncertainty in both semantics (through noisy word embeddings) and pragmatics (through cognitive hierarchy levels). The approach uses a heuristic utility function to approximate expected game outcomes, guiding action selection toward cooperative strategies. The Bayesian spymaster selects clues while the Bayesian guesser interprets them, both maintaining beliefs about their teammate's model and reasoning level.

## Key Results
- Bayesian spymasters achieved win rates up to 0.872 against out-of-distribution guessers, compared to 0.718 for baseline
- Bayesian guessers showed moderate improvements in some cases
- The approach demonstrates robustness to semantic and pragmatic uncertainty
- This represents the first use of Bayesian reasoning with probability distributions over language models in cooperative language games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian inference over a set of language models enables the agent to adapt to semantic and pragmatic uncertainty in teammates.
- Mechanism: The agent maintains a probability distribution over possible teammate models, updating beliefs using Bayes' rule when observing teammate actions. This allows the agent to infer which model best explains observed behavior.
- Core assumption: Each teammate model has a non-zero probability of generating any possible action.
- Evidence anchors:
  - [abstract]: "Bayes' rule is used to update the agent's beliefs as it observes its teammate acting"
  - [section]: "Given observation of a guess g, the update is P(m|g)∝ P(g|m)P(m)"
  - [corpus]: Weak - no direct corpus evidence found for Bayesian inference in Codenames specifically

### Mechanism 2
- Claim: Adding noise to word embeddings creates stochastic teammate models that can generate all possible actions with non-zero probability.
- Mechanism: Gaussian noise is added to word embeddings before computing distances and making decisions, ensuring each model can potentially generate any action.
- Core assumption: The noise level is sufficient to cover the space of possible actions while maintaining meaningful semantic relationships.
- Evidence anchors:
  - [abstract]: "Fine-grained uncertainty in semantics is modeled using noise that is added to the embeddings of words"
  - [section]: "To avoid this problem and ensure all models may continue to be used for inference despite faults in the approximation, each teammate model should have a non-zero probability of generating any action"
  - [corpus]: Weak - corpus mentions uncertainty modeling but not specifically noise-based approaches in Codenames

### Mechanism 3
- Claim: The heuristic utility function approximates expected game outcomes, guiding action selection toward cooperative strategies.
- Mechanism: The utility function calculates marginal contribution to final score based on revealed cards, considering team, opponent, bystander, and assassin cards.
- Core assumption: This heuristic approximates the true expected utility of actions in the game.
- Evidence anchors:
  - [section]: "The Bayesian agents will utilize the following heuristic utility function which will provide a utility for a sequence of cards to be guessed on one turn"
  - [section]: "This heuristic calculates the marginal contribution to the score at the end of the game from the current turn"
  - [corpus]: Weak - no corpus evidence found for specific Codenames utility heuristics

## Foundational Learning

- Concept: Bayesian inference and updating beliefs
  - Why needed here: The agent must reason about uncertainty in teammate models and update beliefs as observations are made
  - Quick check question: If a Bayesian agent observes an action with P(a|m1) = 0.2, P(a|m2) = 0.8, and prior P(m1) = P(m2) = 0.5, what is P(m2|a) assuming normalization?

- Concept: Vector space word embeddings and semantic relationships
  - Why needed here: The agent must understand word relationships to interpret clues and make guesses
  - Quick check question: Given word vectors w1 = [1,2], w2 = [2,3], w3 = [10,10], which word is most similar to w1 using Euclidean distance?

- Concept: Cognitive hierarchy and strategic reasoning levels
  - Why needed here: The agent must model different levels of strategic reasoning in teammates
  - Quick check question: In a cognitive hierarchy, what does it mean for a level-k agent to "approximate a best response to level k-1"?

## Architecture Onboarding

- Component map: Bayesian spymaster (maintains beliefs over guessers, selects clues) ↔ Bayesian guesser (maintains beliefs over spymasters, makes guesses) ↔ Word embedding models (define semantic relationships)
- Critical path: Spymaster observes guess → updates beliefs via Bayes' rule → selects clue using expected utility → clue communicated → guesser observes clue → updates beliefs → selects guess → repeat
- Design tradeoffs: Computational cost vs. accuracy (sampling vs. exact computation), noise level vs. semantic preservation, model complexity vs. generalization
- Failure signatures: Poor performance with out-of-distribution teammates, high variance in results, computational timeouts during decision-making
- First 3 experiments:
  1. Run baseline agents (static spymaster/guesser) against each other to establish performance floor
  2. Implement Bayesian spymaster against static guessers, measure win rate improvement
  3. Implement Bayesian guesser against static spymasters, measure win rate improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Bayesian Codenames agents perform with human teammates compared to their performance with simulated agents?
- Basis in paper: [explicit] The authors note that "all experiments to date have involved only simulated language models" and express a desire to "see how agents perform with human subjects, who display both complex semantic and pragmatic reasoning."
- Why unresolved: The paper only evaluates the Bayesian agents against simulated agents with different word embeddings and noise models. Human reasoning patterns in language games may differ significantly from these simulations.
- What evidence would resolve it: Running experiments where the Bayesian agents play Codenames with human participants and comparing win rates, adaptation speed, and qualitative feedback against their performance with simulated agents.

### Open Question 2
- Question: What is the practical limit on implementing higher levels of the cognitive hierarchy with Bayesian agents?
- Basis in paper: [explicit] The authors state that "practical difficulties in implementing higher levels in the hierarchy made experimentation beyond the first level out of scope for this study."
- Why unresolved: The paper only tests Bayesian agents at level k=1. The computational complexity and modeling challenges for higher levels are not fully explored or quantified.
- What evidence would resolve it: Implementing and testing Bayesian agents at k=2, k=3, etc., measuring computational requirements, performance gains, and identifying specific bottlenecks that prevent scaling.

### Open Question 3
- Question: How sensitive are the Bayesian agents' performance to the choice of heuristic utility function?
- Basis in paper: [explicit] The authors note that "The heuristic utility function could easily be replaced by another, where motivated," suggesting they recognize this as an important consideration.
- Why unresolved: The paper uses a specific utility function based on "marginal contribution to the score at the end of the game" but doesn't explore alternatives or sensitivity analysis.
- What evidence would resolve it: Testing the Bayesian agents with multiple different utility functions (e.g., different scoring variants, risk-averse vs. risk-seeking utilities) and comparing performance across all agent pairings.

## Limitations
- The approach relies on the assumption that all possible actions can be generated by each teammate model, which is ensured by adding noise to word embeddings
- The heuristic utility function approximates expected game outcomes but has not been validated against actual game-theoretic solutions
- The Bayesian inference framework assumes teammates' models are static during inference, which may not hold in dynamic gameplay scenarios

## Confidence

**High Confidence**: The Bayesian inference framework itself is well-established theoretically, and the experimental methodology (comparing win rates against out-of-distribution teammates) is sound. The significant performance improvement (0.872 vs. 0.718 win rate) when Bayesian spymasters face out-of-distribution guessers is robust and well-supported.

**Medium Confidence**: The specific utility function and noise parameters were chosen heuristically rather than through optimization. While these choices appear reasonable and produce good results, they may not be optimal for all game configurations.

**Low Confidence**: The generalizability of these results to other cooperative language games beyond Codenames is uncertain. The semantic space used (word embeddings) may not capture all relevant aspects of natural language understanding needed for broader applications.

## Next Checks
1. **Utility Function Validation**: Implement a Monte Carlo simulation to compare the heuristic utility function against actual expected utilities computed through game tree search in simple Codenames scenarios.

2. **Noise Parameter Sensitivity**: Systematically vary the noise level added to word embeddings and measure its impact on inference quality and win rates across different semantic spaces to identify optimal noise parameters.

3. **Dynamic Teammate Modeling**: Extend the framework to handle time-varying teammate models by incorporating sequential Bayesian updating across multiple game turns, then test whether this improves performance in games where teammate behavior evolves.