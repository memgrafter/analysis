---
ver: rpa2
title: 'XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement
  Learning'
arxiv_id: '2406.08973'
source_url: https://arxiv.org/abs/2406.08973
tags:
- learning
- tasks
- dataset
- arxiv
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces XLand-100B, a large-scale dataset for in-context
  reinforcement learning (ICRL) containing nearly 30,000 unique tasks, 100 billion
  transitions, and 2.5 billion episodes. The dataset addresses the critical lack of
  challenging benchmarks in ICRL by providing complete learning histories across tasks
  of varying difficulty from the XLand-MiniGrid environment.
---

# XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.08973
- Source URL: https://arxiv.org/abs/2406.08973
- Authors: Alexander Nikulin; Ilya Zisman; Alexey Zemtsov; Vladislav Kurenkov
- Reference count: 40
- Key outcome: Introduces XLand-100B with 30k tasks, 100B transitions, and 2.5B episodes to benchmark in-context reinforcement learning

## Executive Summary
XLand-100B addresses the critical need for challenging benchmarks in in-context reinforcement learning (ICRL) by providing a large-scale dataset containing nearly 30,000 unique tasks with complete learning histories. The dataset enables researchers to study how transformers can learn to solve new tasks through context alone, without weight updates. Collected through a multi-stage process involving multi-task pre-training and single-task fine-tuning, the dataset captures policy improvement trajectories across tasks of varying difficulty in the XLand-MiniGrid environment.

The authors demonstrate that while Algorithm Distillation (AD) can achieve in-context learning on simpler tasks, performance degrades significantly on more complex rulesets, highlighting the need for further research in scalable ICRL methods. The dataset democratizes ICRL research by providing a challenging benchmark that was previously only feasible for large labs due to the substantial computational requirements of 50,000 GPU hours for collection.

## Method Summary
The dataset is collected through a multi-stage process: first pre-training on 65k tasks with task-conditioning, then fine-tuning on 30k tasks while recording transitions into learning histories. Expert actions are added through post-processing with PPO policy evaluation. Two main approaches are evaluated: Algorithm Distillation (AD), which trains transformers to autoregressively predict next actions given ordered learning histories, and Decision-Pretrained Transformer (DPT), which predicts optimal actions for query states using task-specific context. Both methods use PPO as the base RL algorithm, with training conducted on 8 H100/A100 GPUs using 25M parameters and cosine learning rate decay.

## Key Results
- AD achieves in-context learning on simpler tasks (â‰¤6 rules) with sequence lengths up to 4096
- AD performance degrades significantly on tasks with 7+ rules, indicating scalability limitations
- DPT fails to learn in Partially Observable MDPs (POMDPs) like the Dark Key-To-Door environment
- The full dataset requires 50,000 GPU hours to collect, highlighting the computational barrier to ICRL research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Algorithm Distillation (AD) can enable in-context learning on simpler tasks when trained on XLand-100B.
- Mechanism: AD trains a transformer to autoregressively predict next actions given history of previous interactions, organized by increasing return. The dataset provides thousands of tasks with learning histories, enabling the emergence of in-context learning ability.
- Core assumption: The dataset contains sufficient diversity and complexity of tasks for AD to learn generalizable patterns.
- Evidence anchors:
  - [abstract] "Experimental results show that while Algorithm Distillation (AD) can achieve in-context learning on simpler tasks, performance degrades significantly on more complex rulesets"
  - [section] "Algorithm Distillation. AD (Laskin et al., 2022) was one of the first to show that in-context learning was possible in RL, and captures the details of many other more recent methods... It trains a transformer, or any other sequence model, to autoregressively predict next actions given the history of previous interactions"
  - [corpus] Weak evidence - no direct mechanism described in corpus papers
- Break condition: Performance degrades significantly on tasks with more complex rulesets (7+ rules), suggesting AD cannot scale to highly complex tasks with current architecture.

### Mechanism 2
- Claim: Decision-Pretrained Transformer (DPT) requires expert actions for each transition to enable in-context learning.
- Mechanism: DPT predicts optimal action for a query state given a random, task-specific context containing transitions from the same task. The dataset provides expert actions through post-processing with PPO policy evaluation.
- Core assumption: Expert actions obtained from final PPO policy are close enough to optimal to enable learning.
- Evidence anchors:
  - [abstract] "We also store expert actions for each transition (see Section 4.2)"
  - [section] "Decision-Pretrained Transformer.DPT is an alternative approach inspired by the Bayesian inference approximation... Unlike AD, it trains a transformer to predict the optimal action for a query state given a random, task specific, context"
  - [section] "After fine-tuning, it was necessary to additionally label the transitions with the expert actions to support DPT-like methods"
- Break condition: DPT fails to learn in Partially Observable MDPs (POMDPs) like the Dark Key-To-Door environment, limiting its applicability to environments requiring memory of past states.

### Mechanism 3
- Claim: Multi-task pre-training followed by single-task fine-tuning generates learning histories necessary for in-context learning.
- Mechanism: The dataset collection involves pre-training on 65k tasks with task-conditioning, then fine-tuning on 30k tasks while recording transitions. This creates learning histories showing policy improvement over time.
- Core assumption: Learning histories with policy improvement are essential for in-context learning to emerge.
- Evidence anchors:
  - [abstract] "We collect the data through a multi-stage process involving multi-task pre-training, single-task fine-tuning with learning history recording"
  - [section] "For our main XLand-100B dataset we uniformly sampled tasks from medium-1m benchmark... We pre-train an agent in a multi-task task-conditioned manner... After that, we train the agent on 65k tasks simultaneously for 25B transitions"
  - [section] "This is a key stage in the data collection process, during which we finetune a pretrained agent while recording the transitions encountered into the dataset"
- Break condition: If pre-training is skipped (as in XLand-Trivial-20B), performance on harder tasks degrades significantly, indicating pre-training is crucial for handling complex tasks.

## Foundational Learning

- Concept: In-context learning in transformers
  - Why needed here: Understanding how transformers can learn from context without weight updates is fundamental to grasping why XLand-100B enables this capability
  - Quick check question: What is the key difference between in-weights learning and in-context learning?

- Concept: Reinforcement learning with partial observability
  - Why needed here: XLand-MiniGrid provides partially observable environments, affecting how agents process observations and plan actions
  - Quick check question: How does partial observability impact the agent's ability to solve tasks compared to fully observable environments?

- Concept: Multi-task meta-learning
  - Why needed here: The dataset contains thousands of tasks, requiring understanding of how learning across multiple tasks enables generalization to new tasks
  - Quick check question: Why is training on thousands of tasks necessary for in-context learning to emerge?

## Architecture Onboarding

- Component map: HDF5 files with groups for states, actions, rewards, dones, and expert_actions. Each task has multiple learning histories. PPO is used for data collection, transformers (AD/DPT) for in-context learning.
- Critical path: 1) Load dataset 2) Extract learning histories 3) Train transformer with AD or DPT 4) Evaluate in-context learning on unseen tasks
- Design tradeoffs: Large dataset size (326GB compressed) vs. comprehensive coverage of task diversity; expert action labeling vs. optimal action access; single-room layouts vs. complex multi-room environments
- Failure signatures: AD performance plateaus on complex tasks (7+ rules); DPT fails on POMDPs; both methods require substantial context length; data corruption during collection leads to task filtering
- First 3 experiments:
  1. Train AD on XLand-Trivial-20B with 1024 context length, evaluate on 1024 unseen tasks for 500 episodes
  2. Train AD on XLand-100B with 2048 context length, evaluate on 1024 unseen tasks for 500 episodes
  3. Train DPT on XLand-Trivial-20B with 1024 context length, evaluate on 1024 unseen tasks for 500 episodes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does scaling the number of tasks in XLand-100B affect the emergence and performance of in-context learning?
- Basis in paper: [explicit] The paper shows experiments with different numbers of tasks (100, 500, 1000, 3000, 5000, 10000 goals) and notes that performance increases from 100 to 3000 goals, suggesting scaling is important.
- Why unresolved: The paper mentions that training on the full dataset for a single epoch is insufficient but large-scale training is computationally expensive, leaving open the question of optimal task scaling for in-context learning.
- What evidence would resolve it: Systematic experiments varying the number of tasks across multiple orders of magnitude, measuring in-context learning emergence and performance metrics, would clarify the scaling relationship.

### Open Question 2
- Question: Can Decision-Pretrained Transformer (DPT) be adapted to work effectively in Partially Observable MDP (POMDP) environments like XLand?
- Basis in paper: [explicit] The paper demonstrates DPT's inability to learn in-context in POMDP environments and shows it fails on simple tasks in XLand-100B, attributing this to its inability to reason in POMDP settings.
- Why unresolved: While the paper identifies the limitation, it doesn't explore potential modifications to DPT architecture or training that might enable POMDP reasoning.
- What evidence would resolve it: Successful adaptation of DPT to POMDP environments through architectural changes (e.g., memory mechanisms) or training modifications that enable in-context learning on XLand tasks.

### Open Question 3
- Question: What architectural modifications to Algorithm Distillation (AD) would enable it to solve more complex rulesets in XLand-100B?
- Basis in paper: [explicit] The paper shows AD can solve simple tasks but performance degrades significantly on more complex rulesets, suggesting current architectures are insufficient for complex task generalization.
- Why unresolved: The paper identifies the limitation but doesn't explore specific architectural changes that might improve performance on complex tasks.
- What evidence would resolve it: Experiments testing modified AD architectures (e.g., different attention mechanisms, memory architectures, or training procedures) that demonstrate improved performance on complex XLand rulesets.

## Limitations

- Performance degradation on complex tasks (>7 rules) indicates fundamental scalability constraints in current AD approaches
- Expert action relabeling process introduces uncertainty about whether provided expert actions represent true optimality
- The 50,000 GPU-hour collection cost creates a substantial barrier to dataset reproduction for smaller research labs

## Confidence

- Dataset construction methodology: High
- Task diversity and scale claims: High
- AD performance on simple tasks: Medium
- AD scalability to complex tasks: Low
- DPT POMDP limitations: High

## Next Checks

1. Verify expert action quality by comparing relabeled actions against ground truth optimal policies on a subset of tasks where optimal solutions are known
2. Test AD performance sensitivity to context window size by training separate models with 512, 1024, 2048, and 4096 sequence lengths on identical task subsets
3. Evaluate whether pre-training is truly necessary by training AD directly on single-task fine-tuning data without multi-task pre-training, comparing performance on 5+ rule tasks