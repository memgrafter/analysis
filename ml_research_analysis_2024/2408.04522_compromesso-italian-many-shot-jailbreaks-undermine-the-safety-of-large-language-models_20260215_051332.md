---
ver: rpa2
title: Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language
  Models
arxiv_id: '2408.04522'
source_url: https://arxiv.org/abs/2408.04522
tags:
- unsafe
- safety
- arxiv
- jailbreaking
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether Italian-language large language models
  are vulnerable to many-shot jailbreaking, a method where models are prompted with
  numerous unsafe demonstrations to elicit harmful responses. The authors created
  a novel Italian dataset of 418 unsafe question-answer pairs derived from English
  safety benchmarks and translated/edited into Italian.
---

# Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models

## Quick Facts
- arXiv ID: 2408.04522
- Source URL: https://arxiv.org/abs/2408.04522
- Reference count: 5
- Primary result: Unsafe response rates increase from 68% at one shot to 84% at 32 shots when Italian LLMs are prompted with unsafe demonstrations.

## Executive Summary
This study investigates many-shot jailbreaking vulnerabilities in Italian-language large language models by testing whether repeated exposure to unsafe demonstrations increases harmful response generation. The authors created a novel Italian dataset of 418 unsafe question-answer pairs derived from English safety benchmarks and tested six open-weight models (Llama 3 8B, Mistral 7B, Qwen 1.5 4B/7B, Gemma 2B/7B) using negative log likelihood and response safety classification methods. Results demonstrate that unsafe responses escalate significantly with more demonstrations, rising from 68% at one shot to 84% at 32 shots across models, while NLL scores decrease indicating increased alignment with unsafe content.

The study reveals critical safety vulnerabilities in multilingual and non-English language models, with Qwen models showing lower vulnerability rates that may indicate benefits from multilingual training approaches. These findings highlight the urgent need for robust multilingual safety protocols in LLMs and suggest that demonstration-based jailbreaking poses a significant threat across language boundaries. The research demonstrates that even small, open-weight models exhibit concerning levels of vulnerability to many-shot jailbreaking, emphasizing the need for improved safety measures in language model deployment.

## Method Summary
The authors created an Italian dataset of 418 unsafe question-answer pairs derived from English safety benchmarks and tested six open-weight models (Llama 3 8B, Mistral 7B, Qwen 1.5 4B/7B, Gemma 2B/7B) using two evaluation methods: negative log likelihood (NLL) and response safety classification with a GPT-4 classifier. Models were tested incrementally with 1, 2, 4, 8, 16, 32, and 64 unsafe demonstrations to observe how response safety degrades with increasing demonstration count. The study focused on lightweight open-weight models to demonstrate the vulnerability across different architectures while maintaining practical computational constraints.

## Key Results
- Unsafe response rates increase from 68% at one shot to 84% at 32 shots across all tested models.
- NLL scores decrease consistently as demonstration count increases, indicating greater model alignment with unsafe content.
- Qwen 1.5 models (4B and 7B) show lower unsafe response rates compared to other models, suggesting multilingual design may provide defensive benefits.
- All tested models exhibit vulnerability to many-shot jailbreaking, with no model achieving safe response rates below 50% even at 64 shots.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing the number of unsafe demonstrations in a prompt escalates the likelihood of generating unsafe responses.
- Mechanism: The model's in-context learning interprets repeated unsafe demonstrations as acceptable behavior patterns, reducing its resistance to unsafe requests.
- Core assumption: LLMs treat sequential demonstrations as a valid context for response generation, even when those demonstrations are unsafe.
- Evidence anchors:
  - [abstract] "we find that the models exhibit unsafe behaviors even when prompted with few unsafe demonstrations, and–more alarmingly– that this tendency rapidly escalates with more demonstrations."
  - [section] "Results show that unsafe responses increase with more demonstrations, rising from 68% at one shot to 84% at 32 shots across models."
  - [corpus] No direct corpus evidence for this specific mechanism.
- Break condition: Safety protocols that override in-context learning or limit the influence of demonstration chains.

### Mechanism 2
- Claim: NLL reduction indicates alignment with unsafe content as demonstration count increases.
- Mechanism: Lower NLL scores suggest the model finds unsafe sequences more probable given the context of repeated demonstrations.
- Core assumption: NLL directly reflects model alignment with prompt content patterns.
- Evidence anchors:
  - [section] "Using NLL for evaluation (Figure 3), all tested models consistently show a decrease in NLL as the number of shots in the input increases."
  - [section] "This result suggests that, with more context provided, all models are more likely to generate responses aligned with the unsafe demonstrations."
  - [corpus] No direct corpus evidence for NLL-behavior correlation.
- Break condition: Models with built-in resistance to probability shifts from demonstration chains.

### Mechanism 3
- Claim: Multilingual models like Qwen show lower unsafe response rates due to inherent safety features in their training.
- Mechanism: Multilingual training introduces diverse linguistic patterns that may disrupt the effectiveness of demonstration-based jailbreaking.
- Core assumption: Multilingual training inherently improves safety robustness across all attack vectors.
- Evidence anchors:
  - [section] "Notably, the Qwen 1.5 models (4B and 7B) consistently demonstrate a lower proportion of unsafe responses, suggesting that their multilingual design could serve as a robust defense against such vulnerabilities."
  - [section] "Mistral7B is tailored for English, while Llama3, despite being pre-trained on multiple languages, primarily focuses on English."
  - [corpus] No direct corpus evidence for multilingual safety advantages.
- Break condition: Attack methods specifically designed to exploit multilingual model vulnerabilities.

## Foundational Learning

- Concept: In-context learning in LLMs
  - Why needed here: Understanding how LLMs interpret demonstration sequences is critical for analyzing jailbreaking effectiveness.
  - Quick check question: How does an LLM distinguish between demonstration examples and actual instructions in a prompt?

- Concept: Probability-based safety metrics (NLL)
  - Why needed here: NLL scores provide quantitative measures of model alignment with unsafe content patterns.
  - Quick check question: What does a decreasing NLL score indicate about model behavior in the context of unsafe demonstrations?

- Concept: Multilingual model training
  - Why needed here: Different training approaches affect model robustness against various attack methods.
  - Quick check question: How might multilingual training influence a model's vulnerability to demonstration-based attacks?

## Architecture Onboarding

- Component map: Dataset creation -> Model selection -> Prompt construction -> Response generation -> Safety classification -> Analysis
- Critical path: Prompt construction -> Response generation -> Safety classification
- Design tradeoffs: Larger context windows increase jailbreaking effectiveness but also computational costs; multilingual models may be safer but less optimized for specific languages.
- Failure signatures: Unexpected drops in unsafe responses (like Gemma 2B at 32 shots), inconsistent NLL trends, or classifier misclassification.
- First 3 experiments:
  1. Test single-shot jailbreaking across all six models to establish baseline vulnerability.
  2. Gradually increase demonstration count (1, 2, 4, 8 shots) to observe response trends.
  3. Compare Qwen models against monolingual counterparts using identical demonstration sets.

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- The study relies on a GPT-4 classifier for safety evaluation, which introduces potential bias and inconsistency in labeling responses as safe or unsafe.
- The exact translation process for creating the Italian dataset from English benchmarks is not fully specified, raising questions about linguistic equivalence.
- The evaluation focuses only on six open-weight models, limiting generalizability to other model architectures or proprietary systems.

## Confidence

- **High Confidence**: The core finding that unsafe response rates increase with demonstration count (68% → 84%) is well-supported by consistent results across multiple models and evaluation methods.
- **Medium Confidence**: The claim that Qwen models show lower vulnerability due to multilingual design is plausible but not definitively proven, as other factors could contribute to this difference.
- **Medium Confidence**: The NLL-based conclusions about model alignment with unsafe content are methodologically sound but rely on assumptions about NLL interpretation.

## Next Checks

1. **Cross-linguistic validation**: Test the same jailbreaking approach on non-Italian languages using models trained primarily in those languages to determine if multilingual training provides generalizable safety benefits.

2. **Classifier independence**: Implement an alternative safety classification method (such as human evaluation or a different automated classifier) to verify the GPT-4 classifier's consistency and reduce potential bias.

3. **Context window exploration**: Systematically test varying context window sizes to determine whether the observed jailbreaking effectiveness scales proportionally with available context or if there are practical limits to this attack vector.