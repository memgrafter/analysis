---
ver: rpa2
title: Building Retrieval Systems for the ClueWeb22-B Corpus
arxiv_id: '2402.04357'
source_url: https://arxiv.org/abs/2402.04357
tags:
- retrieval
- first
- used
- data
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reports on building retrieval baselines for the ClueWeb22-B
  corpus, which contains 87 million English web documents. The authors implemented
  BM25 and dense retrieval for first-stage retrieval and trained rerankers to improve
  the ranking of retrieved results.
---

# Building Retrieval Systems for the ClueWeb22-B Corpus

## Quick Facts
- arXiv ID: 2402.04357
- Source URL: https://arxiv.org/abs/2402.04357
- Authors: Harshit Mehrotra; Jamie Callan; Zhen Fan
- Reference count: 2
- Key outcome: BM25 achieved recall@1000 of 0.96, dense retrieval 0.74 on ClueWeb22-B corpus; best reranker trained on anchor text + MS-MARCO data achieved MRR 0.296

## Executive Summary
This paper reports on building retrieval baselines for the ClueWeb22-B corpus, which contains 87 million English web documents. The authors implemented BM25 and dense retrieval for first-stage retrieval and trained rerankers to improve the ranking of retrieved results. BM25 and dense retrieval achieved recall@1000 of 0.96 and 0.74 respectively on an evaluation set of 2110 queries. For reranking, the best model was trained on both inlink anchor text data and MS-MARCO document ranking data, achieving an MRR of 0.296. However, the existing BERT reranker trained on MS-MARCO and ClueWeb09 data still outperformed the newly trained rerankers. The authors identified the need for better quality relevance judgments to fairly compare BM25 and dense retrieval.

## Method Summary
The authors built retrieval systems using BM25 (sparse retrieval) and dense retrieval as first-stage retrieval methods on the ClueWeb22-B corpus. They created Lucene indexes for BM25 using Pyserini and Faiss indexes for dense retrieval using Contriever embeddings. For reranking, they trained BERT-based models on combinations of MS-MARCO document ranking data and inlink anchor text data. The reranking pipeline involved training on anchor text first, then fine-tuning on MS-MARCO data. They evaluated using standard IR metrics including recall@1000, MRR, P@5, and P@10.

## Key Results
- BM25 achieved recall@1000 of 0.96, while dense retrieval achieved 0.74 on the evaluation set
- The best reranker, trained on both anchor text and MS-MARCO data, achieved MRR of 0.296
- Existing BERT reranker trained on MS-MARCO and ClueWeb09 data outperformed all newly trained models
- Dense retrieval can surface semantically relevant documents missed by BM25, but evaluation bias may artificially deflate its performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anchor text data is effective for training dense retrievers but degrades reranker performance.
- Mechanism: Anchor texts provide weak supervision for dense retrieval because they create a semantic link between query-like text and a document, encouraging the retriever to capture semantic similarity. However, rerankers require precise relevance matching, and anchor text often describes only a narrow aspect of the linked page, leading to misalignment.
- Core assumption: The nature of the semantic signal differs between dense retrieval (broad semantic mapping) and reranking (fine-grained relevance ranking).
- Evidence anchors:
  - [section]: "Another indication towards the detrimental nature of anchor text data for final training and the beneficial nature of MS-MARCO data."
  - [abstract]: "However, the existing BERT reranker trained on MS-MARCO and ClueWeb09 data still outperformed the newly trained rerankers."
  - [corpus]: "Weak signal; anchor text may describe only a narrow aspect of the linked page."
- Break condition: If anchor text quality improves to more accurately reflect document relevance, or if rerankers are explicitly trained to tolerate partial matches.

### Mechanism 2
- Claim: Dense retrieval can surface semantically relevant documents missed by BM25, but evaluation bias favors lexical matching.
- Mechanism: Dense retrieval models (e.g., Contriever) use semantic embeddings to find documents related in meaning but not necessarily in exact terms, while BM25 relies on term overlap. The evaluation set was labeled using BM25 results, so documents retrieved by dense methods but not by BM25 are marked as irrelevant even if semantically relevant.
- Core assumption: Evaluation data reflects BM25 bias and does not capture true semantic relevance.
- Evidence anchors:
  - [section]: "The dense retrieval results directly answer the query... On the other hand, the relevant labeled document is an answers forum page predominantly about the settlement of Utah... That sentence too does not discuss the settlement of San Diego."
  - [abstract]: "BM25 and dense retrieval achieved recall@1000 of 0.96 and 0.74 respectively... the authors identified the need for better quality relevance judgments to fairly compare BM25 and dense retrieval."
  - [corpus]: "Limited; the cited paper does not provide a separate semantic relevance benchmark."
- Break condition: When evaluation uses interleaved judgments or true semantic relevance labels.

### Mechanism 3
- Claim: Pre-training dense retrievers on MS-MARCO and fine-tuning with anchor text improves dense retrieval but harms reranking.
- Mechanism: Anchor text provides weak but broad semantic supervision that helps the retriever learn general semantic similarity. When used for reranking, this same weak signal misguides the model toward irrelevant but semantically related documents, harming precision.
- Core assumption: The weak supervision from anchor text is beneficial for broad retrieval but detrimental for precise ranking.
- Evidence anchors:
  - [section]: "We also see that AT->Marco which was trained on MS-MARCO with the anchor text model as the starting point does much better than AT and even improves performance over BM25."
  - [abstract]: "In addition to the anchor texts, we also used the MS-MARCO document ranking dataset... Using these 2 datasets and their combinations, we train the following rerankers..."
  - [corpus]: "Weak evidence; the cited paper does not explicitly analyze the dual-role effect of anchor text on retrieval vs. ranking."
- Break condition: When the anchor text dataset is filtered or weighted to reduce noise, or when rerankers are trained with additional fine-grained relevance signals.

## Foundational Learning

- Concept: Sparse vs. Dense Retrieval
  - Why needed here: Understanding the difference explains why BM25 and dense retrieval achieve different recall and why they may surface different relevant documents.
  - Quick check question: What is the primary difference between how BM25 and dense retrieval represent and match documents?

- Concept: Reranker Training Pipeline
  - Why needed here: The paper trains rerankers in stages (anchor text → MS-MARCO) and understanding this pipeline is key to reproducing or improving results.
  - Quick check question: Why might training a reranker first on anchor text and then on MS-MARCO yield better results than training only on MS-MARCO?

- Concept: Relevance Judgment Bias
  - Why needed here: The evaluation set was labeled using BM25 results, introducing bias that affects the perceived performance of dense retrieval.
  - Quick check question: How does using BM25 results to label relevance judgments affect the fairness of comparing BM25 and dense retrieval?

## Architecture Onboarding

- Component map: Query → BM25 service (Lucene index lookup) → Reranker (BERT/RoBERTa) → Top-10 results; Alternative path: Query → Dense retrieval service (Faiss) → Reranker → Top-10 results
- Critical path: Query → BM25 service (Lucene index lookup) → Reranker (BERT/RoBERTa) → Top-10 results; Alternative path: Query → Dense retrieval service (Faiss) → Reranker → Top-10 results
- Design tradeoffs:
  - Index size vs. latency: Lucene indexes are 216GB each, Faiss indexes up to 70GB, loaded into memory for speed
  - Service flexibility: Lucene indexes are portable; previous Solr indexes were machine-bound
  - Reranker choice: BM25 first-stage chosen to preserve body text availability; dense retrieval would need extra lookup
- Failure signatures:
  - BM25 service: High latency or low recall suggests index partition or memory issues
  - Dense retrieval: Very low recall vs. BM25 suggests semantic mismatch or evaluation bias
  - Reranker: Poor MRR may indicate anchor text noise or insufficient fine-tuning data
- First 3 experiments:
  1. Compare recall@1000 of BM25 vs. dense retrieval on a held-out query set with unbiased relevance judgments
  2. Train a reranker using only MS-MARCO data and measure MRR vs. anchor text + MS-MARCO combination
  3. Test dense retrieval with different Faiss index types (e.g., IVF) to reduce query latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of anchor text data impact the effectiveness of rerankers when trained on both anchor text and MS-MARCO data?
- Basis in paper: [explicit] The paper mentions that 500k anchor texts were used but does not explore the impact of varying the size of anchor text data on reranker performance.
- Why unresolved: The study used a fixed amount of anchor text data (500k) and did not investigate how different sizes might affect the training outcome or the performance of the rerankers.
- What evidence would resolve it: Conducting experiments with varying sizes of anchor text data and comparing the reranker performance would provide insights into the optimal amount of anchor text data needed for effective training.

### Open Question 2
- Question: Why does anchor text data improve dense retriever training but not reranker training?
- Basis in paper: [explicit] The paper notes that anchor text data is effective for training dense retrievers but seems to degrade the performance of rerankers when used as the final stage of training.
- Why unresolved: The paper does not provide a clear explanation for the discrepancy in how anchor text data affects dense retrievers versus rerankers, indicating a gap in understanding the underlying mechanisms.
- What evidence would resolve it: Investigating the characteristics of anchor text data and how they interact with the training processes of dense retrievers and rerankers could clarify why there is a difference in effectiveness.

### Open Question 3
- Question: What is the optimal hyperparameter setting for training rerankers on a combination of anchor text and MS-MARCO data?
- Basis in paper: [inferred] The paper mentions that ablations should be conducted to find a better hyperparameter setting, suggesting that the current settings may not be optimal.
- Why unresolved: The study did not perform extensive hyperparameter tuning, leaving uncertainty about the best settings for training rerankers with mixed data sources.
- What evidence would resolve it: Systematic hyperparameter tuning experiments, including variations in learning rate, batch size, and training epochs, could identify the optimal settings for improved reranker performance.

### Open Question 4
- Question: How can the latency of dense retrieval be reduced while maintaining or improving recall?
- Basis in paper: [explicit] The paper notes that dense retrieval latency is high (around 6.4 seconds per query) and suggests exploring alternate index types in Faiss to reduce latency.
- Why unresolved: The paper does not explore different Faiss index types or other methods to optimize dense retrieval speed, leaving the potential for performance improvements unexplored.
- What evidence would resolve it: Testing various Faiss index configurations and other optimization techniques could reveal methods to decrease latency without sacrificing recall, thus enhancing the overall efficiency of dense retrieval systems.

## Limitations
- The evaluation set labeled using BM25 results introduces bias that may artificially deflate dense retrieval performance
- The claim that anchor text is detrimental for reranker training while beneficial for dense retrieval lacks direct empirical validation
- Exact implementation details of the existing BERT reranker trained on MS-MARCO and ClueWeb09 data are not provided, making fair comparison difficult

## Confidence
- High confidence: BM25 and dense retrieval recall metrics, MS-MARCO training effectiveness for rerankers
- Medium confidence: Anchor text's detrimental effect on rerankers, evaluation bias against dense retrieval
- Low confidence: Exact performance gap due to semantic vs. lexical relevance mismatch, optimal training pipeline for rerankers

## Next Checks
1. Conduct an interleaved evaluation comparing BM25 and dense retrieval results using human judgments to eliminate evaluation bias from BM25-based labeling
2. Train and evaluate a reranker using only MS-MARCO data, then incrementally add anchor text data to measure the specific impact on performance
3. Test dense retrieval with alternative indexing strategies (e.g., IVF) to quantify the trade-off between recall and query latency