---
ver: rpa2
title: 'SAM Decoding: Speculative Decoding via Suffix Automaton'
arxiv_id: '2411.10666'
source_url: https://arxiv.org/abs/2411.10666
tags:
- automaton
- decoding
- suffix
- draft
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SAM-Decoding, a novel retrieval-based speculative
  decoding method that uses suffix automaton to achieve efficient and accurate draft
  generation for LLM inference acceleration. The key innovation is constructing static
  and dynamic suffix automatons for text corpus and input prompts respectively, enabling
  fast longest suffix matching with O(1) average time complexity per generation step.
---

# SAM Decoding: Speculative Decoding via Suffix Automaton

## Quick Facts
- arXiv ID: 2411.10666
- Source URL: https://arxiv.org/abs/2411.10666
- Reference count: 13
- Key outcome: Achieves 18%+ speedup over retrieval-based methods using suffix automaton for exact longest suffix matching

## Executive Summary
This paper introduces SAM-Decoding, a retrieval-based speculative decoding method that leverages suffix automaton data structures to accelerate large language model inference. The key innovation is using suffix automatons to achieve O(1) average time complexity for longest suffix matching, enabling more accurate draft generation compared to traditional n-gram matching methods. The approach can integrate with existing speculative decoding methods and adaptively selects draft generation strategies based on match length, providing significant speedups across various-sized LLM backbones.

## Method Summary
SAM-Decoding constructs static suffix automatons from text corpora offline and dynamic suffix automatons from input prompts during inference. It uses longest suffix matching to generate draft tokens, which are then verified by the LLM and accepted if correct. The method adaptively selects between suffix automaton drafts and auxiliary methods (Token Recycling or EAGLE) based on match length thresholds. The suffix automaton structure enables O(1) average time complexity per generation step through efficient state transitions using 'next' and 'link' edges.

## Key Results
- Achieves 18%+ speedup over other retrieval-based methods on Spec-Bench benchmark
- When combined with EAGLE-2, provides 3.28%-11.13% additional speedup across various-sized LLM backbones
- Effectively addresses limitations of existing retrieval-based methods including incomplete retrieval resources, inefficient retrieval methods, and domain constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM-Decoding achieves O(1) average time complexity per generation step through suffix automaton state transitions.
- Mechanism: The suffix automaton maintains matching states for the generating text. Each new token triggers state transitions via the 'next' and 'link' edges. The amortized analysis shows that the total cost over L transitions is bounded by 2L, giving O(1) average per step.
- Core assumption: The amortized cost analysis correctly captures the average-case behavior, and the automaton structure ensures that state transitions are bounded.
- Evidence anchors:
  - [abstract]: "achieving an average time complexity of O(1) per generation step"
  - [section]: "Using amortized analysis, we can prove that the average complexity of state transfer is O(1), while the worst-case time complexity is O(L)"
  - [corpus]: Weak evidence - no direct comparison to O(1) claims in neighbors
- Break condition: If the input sequence contains pathological patterns that force many link traversals per token, the amortized guarantee may not hold in practice.

### Mechanism 2
- Claim: SAM-Decoding generates more accurate drafts by finding exact longest suffix matches rather than n-gram matching.
- Mechanism: The suffix automaton structure inherently represents all substrings and their relationships, allowing exact longest suffix matching in O(1) time. This contrasts with n-gram methods that have fixed-length matches and potential gaps.
- Core assumption: The suffix automaton correctly captures all substring relationships and can retrieve the longest matching suffix without approximation.
- Evidence anchors:
  - [abstract]: "Unlike existing n-gram matching methods, SAM-Decoding finds the exact longest suffix match"
  - [section]: "Unlike conventional approaches that rely on n-gram matching, SAM-Decoding employs a suffix automaton to solve the longest suffix match problem"
  - [corpus]: Weak evidence - neighbors mention retrieval-based methods but don't compare accuracy to n-gram methods
- Break condition: If the text corpus lacks sufficient context for the longest suffix match to be meaningful, the accuracy advantage may diminish.

### Mechanism 3
- Claim: SAM-Decoding can integrate with existing speculative decoding methods to improve performance across all task types.
- Mechanism: The method uses match length as an indicator of draft quality. When matches are short, it falls back to auxiliary methods like Token Recycling or EAGLE. This adaptive selection ensures performance even when retrieval is not effective.
- Core assumption: The match length is a reliable indicator of draft quality, and auxiliary methods can effectively handle cases where retrieval fails.
- Evidence anchors:
  - [abstract]: "it is designed as an approach that can be combined with existing methods, allowing SAM-Decoding to adaptively select a draft generation strategy based on the matching length"
  - [section]: "Consequently, if the retrieval-based method fails to produce a satisfactory draft, we can resort to these alternatives to enhance the draft's effectiveness"
  - [corpus]: Weak evidence - neighbors don't discuss adaptive integration strategies
- Break condition: If the threshold for switching between methods is poorly calibrated, the system may waste computation on low-quality drafts or miss opportunities for good retrievals.

## Foundational Learning

- Concept: Suffix Automaton Data Structure
  - Why needed here: SAM-Decoding relies on suffix automatons for efficient longest suffix matching, which is the core innovation enabling O(1) complexity
  - Quick check question: What is the difference between the 'next' and 'link' edges in a suffix automaton, and how do they enable efficient substring matching?

- Concept: Amortized Analysis
  - Why needed here: The O(1) average complexity claim depends on amortized analysis of state transitions over the entire generation sequence
  - Quick check question: How does amortized analysis differ from worst-case analysis, and why is it appropriate for analyzing suffix automaton state transitions?

- Concept: Speculative Decoding Framework
  - Why needed here: Understanding the draft generation, verification, and acceptance process is crucial for implementing SAM-Decoding correctly
  - Quick check question: In speculative decoding, what determines whether a draft token is accepted, and how does this affect the overall speedup?

## Architecture Onboarding

- Component map:
  - Static Suffix Automaton: Pre-built from text corpus, used for retrieving drafts from existing content
  - Dynamic Suffix Automaton: Built incrementally from input prompt, used for retrieving drafts from generated text
  - Auxiliary Draft Methods: Token Recycling and EAGLE variants for fallback when retrieval matches are poor
  - Adaptive Selector: Logic that chooses between static, dynamic, or auxiliary drafts based on match length

- Critical path:
  1. Build static SAM from corpus (offline)
  2. For each request, build dynamic SAM from prompt
  3. At each generation step: find longest suffix match in both SAMs
  4. Select draft based on match length thresholds
  5. Verify draft with LLM and accept tokens
  6. Update both SAMs with accepted tokens

- Design tradeoffs:
  - Static vs Dynamic SAM: Static provides broader context but may be less relevant; dynamic is more relevant but limited to what's been generated
  - Match length thresholds: Too high and you miss good drafts; too low and you waste computation on poor retrievals
  - Draft size: Larger drafts provide more potential speedup but increase verification cost and may reduce acceptance rates

- Failure signatures:
  - Low acceptance rates: Indicates poor draft quality, possibly due to incorrect match length thresholds or inadequate corpus
  - High computation overhead: Suggests the auxiliary methods are being used too frequently, or the SAM construction/maintenance is inefficient
  - Memory issues: Suffix automatons can grow large, especially the dynamic one; monitor memory usage during long generations

- First 3 experiments:
  1. Baseline comparison: Run SAM-Decoding on a simple summarization task with a small corpus to verify the O(1) behavior and compare to n-gram matching
  2. Threshold sensitivity: Vary lbias and lthreshold parameters systematically to find optimal values for different task types
  3. Corpus quality impact: Test with different corpus qualities (relevant vs irrelevant) to measure how much the static SAM contributes to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic SAM construction handle conflicts when multiple accepted tokens extend the automaton in different directions?
- Basis in paper: [explicit] The paper mentions that for the dynamic SAM, "we first expand its state based on the accepted tokens and then transfer the matching state" but doesn't detail how conflicts are resolved during expansion.
- Why unresolved: The paper describes the expansion process but doesn't address scenarios where multiple paths could be valid, which is critical for maintaining the automaton's correctness.
- What evidence would resolve it: A detailed algorithmic description or pseudocode showing how the dynamic SAM handles conflicting token sequences during expansion.

### Open Question 2
- Question: What is the impact of different text corpus compositions on SAM-Decoding's performance?
- Basis in paper: [inferred] The paper uses Stanford-alpaca, python-code-instruction-18k, and GSK8k for building the static SAM, but doesn't systematically explore how different corpus compositions affect performance.
- Why unresolved: The paper shows SAM-Decoding works with specific corpora but doesn't investigate the relationship between corpus diversity, size, and decoding speedups.
- What evidence would resolve it: Experiments varying corpus size, domain specificity, and token distribution while measuring speedup ratios and mean accepted tokens.

### Open Question 3
- Question: How does SAM-Decoding scale with increasing sequence lengths beyond what was tested in Spec-Bench?
- Basis in paper: [inferred] The paper reports results on Spec-Bench but doesn't explore the performance characteristics as sequence lengths grow significantly larger.
- Why unresolved: While the paper proves O(1) average time complexity, it doesn't empirically verify this scaling behavior with very long sequences.
- What evidence would resolve it: Benchmarking SAM-Decoding on tasks with progressively longer sequences (e.g., 10x, 100x longer than Spec-Bench) while measuring time complexity and accuracy degradation.

### Open Question 4
- Question: What is the optimal strategy for setting lbias and lthreshold across different task domains?
- Basis in paper: [explicit] The paper mentions that "lbias and lthreshold were set to 5" as a default but doesn't provide systematic analysis of optimal values for different tasks.
- Why unresolved: The paper shows these hyperparameters affect performance but doesn't provide domain-specific guidance or automated tuning strategies.
- What evidence would resolve it: A comprehensive study showing lbias/lthreshold optimization across multiple task categories with recommendations for different domains.

## Limitations

- Corpus Quality Dependency: Performance heavily depends on the quality and relevance of the text corpus used for building the static suffix automaton, with limited discussion of degradation with less relevant corpora.
- Dynamic Memory Overhead: The dynamic suffix automaton grows with each generation step, potentially creating substantial memory overhead for long sequences that isn't fully characterized.
- Hyperparameter Sensitivity: The paper mentions setting match length thresholds to 5 but provides limited systematic analysis of how sensitive performance is to these parameters across different task types.

## Confidence

**High Confidence**: The core technical contribution of using suffix automatons for O(1) longest suffix matching is well-founded with standard amortized analysis and established data structures.

**Medium Confidence**: Experimental results showing 18%+ speedup are reasonably convincing, though comparison methodology could be more detailed and generalization across different-sized LLM backbones needs more validation.

**Low Confidence**: Claims that SAM-Decoding "effectively addresses" all three limitations of existing retrieval-based methods are overstated, as the first and third limitations are only partially mitigated through adaptive fallback mechanisms with uncertain effectiveness thresholds.

## Next Checks

1. **Corpus Relevance Ablation Study**: Systematically test SAM-Decoding performance across corpora of varying relevance to the target task (completely unrelated, partially related, highly relevant) to quantify how much the static SAM contributes to overall performance.

2. **Long Sequence Memory Profiling**: Generate extended sequences (10K+ tokens) while monitoring both memory usage and state transition costs for the dynamic suffix automaton to verify O(1) average complexity holds in practice.

3. **Threshold Calibration Analysis**: Conduct a comprehensive sweep of match length thresholds (lbias and lthreshold) across different task types, model sizes, and corpus qualities to establish optimal configuration guidelines.