---
ver: rpa2
title: Examining Changes in Internal Representations of Continual Learning Models
  Through Tensor Decomposition
arxiv_id: '2405.03244'
source_url: https://arxiv.org/abs/2405.03244
tags:
- uni00a0t
- learning
- replay
- tensor
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a tensor component analysis (TCA) framework
  to study internal representations in continual learning (CL) models. By constructing
  three-dimensional tensors from layer activations, model snapshots, and representative
  inputs, the authors apply TCA to uncover patterns in how representations evolve
  across tasks and time.
---

# Examining Changes in Internal Representations of Continual Learning Models Through Tensor Decomposition

## Quick Facts
- **arXiv ID**: 2405.03244
- **Source URL**: https://arxiv.org/abs/2405.03244
- **Reference count**: 40
- **Primary result**: TCA can isolate specific classes and temporal regions in CL models but doesn't clearly reveal specialized neuron clusters or filter evolution patterns

## Executive Summary
This study proposes a tensor component analysis (TCA) framework to study internal representations in continual learning (CL) models. By constructing three-dimensional tensors from layer activations, model snapshots, and representative inputs, the authors apply TCA to uncover patterns in how representations evolve across tasks and time. Experiments were conducted across three model architectures (ResNet50, DeiTSmall, CvT13) and several CL strategies (Naive, Replay, EWC, MAS, RMN, WSN). Results showed that while TCA could isolate specific classes and temporal regions, it did not clearly highlight specialized neuron clusters or filter evolution patterns as hypothesized. Reconstruction errors varied significantly across architectures, with ResNet50 showing notably lower errors. Filter decompositions produced smoother temporal patterns but remained difficult to interpret.

## Method Summary
The authors propose a tensor component analysis framework to examine internal representations in continual learning models. They construct 3D tensors from model activations, representative inputs, and temporal snapshots, then apply tensor decomposition to extract interpretable components. The method involves training models with various CL strategies, collecting activation data at multiple time points, building tensors from this data, and analyzing the resulting TCA factors to understand how representations evolve. The framework was tested across multiple architectures and CL strategies to evaluate its ability to reveal patterns of representational change.

## Key Results
- TCA successfully isolated specific classes and temporal regions in activation tensors across all architectures
- Reconstruction errors varied dramatically between architectures, with ResNet50 showing significantly lower errors than CvT13 and DeiTSmall
- TCA did not clearly reveal specialized neuron clusters for specific tasks despite using importance-based CL methods designed to encourage specialization
- Filter decomposition produced smoother temporal patterns but remained difficult to interpret compared to activation decompositions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor decomposition can reveal evolving internal representations in continual learning models.
- Mechanism: TCA decomposes a 3D tensor of activations, inputs, and temporal snapshots into interpretable components that capture patterns of neural activity over time.
- Core assumption: Neural activations and model parameters change in structured, low-rank patterns that TCA can decompose into meaningful factors.
- Evidence anchors:
  - [abstract] "By conducting tensor component analysis (TCA), we aim to uncover meaningful patterns about how the internal representations evolve"
  - [section] "TCA approximates a three-way tensorX as a sum of rank-1 tensors, where each rank-1 tensor is an outer product of vectors"
  - [corpus] Weak. No direct neighbor papers mention TCA or similar interpretability techniques.
- Break condition: If activations evolve chaotically or in high-rank patterns, TCA decomposition will fail to isolate interpretable components.

### Mechanism 2
- Claim: Importance-based regularization methods can induce specialization in neuron clusters across tasks.
- Mechanism: By penalizing changes to important parameters, CL strategies like EWC and MAS encourage neurons to maintain task-specific activations, which TCA can detect as specialized clusters.
- Core assumption: Importance-based regularization leads to consistent, specialized activation patterns that persist across model snapshots.
- Evidence anchors:
  - [section] "Importance-based methods for CL, such as Elastic Weight Consolidation (EWC)...measure the importance of each parameter for a particular task and discourage certain parameters from significant changes"
  - [section] "In our experiments, we investigate whether these strategies ultimately result in the emergence of sets of neurons that exhibit specialization for particular tasks"
  - [corpus] Weak. No neighbor papers discuss continual learning or neuron specialization.
- Break condition: If regularization fails to enforce parameter importance, activation patterns will be task-agnostic and TCA cannot identify specialization.

### Mechanism 3
- Claim: Filter evolution tracking can reveal how CNN filters and transformer features adapt over time.
- Mechanism: By optimizing images to maximally activate specific filters/features and decomposing the resulting tensor, TCA can isolate which filters become active at different training stages.
- Core assumption: Filters and features evolve in a structured way, with certain ones becoming active for specific tasks, which TCA can capture in temporal components.
- Evidence anchors:
  - [section] "We wish to understand how individual CNN filters and transformer features for a chosen layer evolve across the training regime"
  - [section] "In this experiment, we may observe a scenario where a specific component exhibits the following characteristics: it selects one particular filter in the filter factors, corresponds to a particular segment in the temporal factors"
  - [corpus] Weak. No neighbor papers discuss filter evolution or tensor-based feature tracking.
- Break condition: If filters adapt randomly or uniformly, temporal components will not isolate specific filter activations.

## Foundational Learning

- Concept: Tensor decomposition (specifically CP/TCA)
  - Why needed here: TCA is the core technique for extracting interpretable patterns from 3D tensors of model activations.
  - Quick check question: What is the difference between TCA and PCA in terms of the number of dimensions they can decompose?

- Concept: Continual learning (CL) and catastrophic forgetting
  - Why needed here: Understanding how models forget and adapt across tasks is essential to interpret TCA results.
  - Quick check question: How do importance-based CL strategies like EWC prevent catastrophic forgetting?

- Concept: Neural network activations and feature maps
  - Why needed here: TCA operates on flattened activation tensors, so understanding their structure is critical.
  - Quick check question: What is the shape and meaning of a typical convolutional layer's activation tensor?

## Architecture Onboarding

- Component map:
  - Data ingestion: Load model snapshots and representative inputs
  - Tensor construction: Build 3D tensors from activations, inputs, and snapshots
  - TCA decomposition: Apply tensor decomposition to extract interpretable components
  - Analysis pipeline: Visualize and interpret TCA factors (activation, input, temporal)
  - Validation: Run masking experiments to confirm findings

- Critical path:
  1. Train models with CL strategies and save snapshots
  2. Select representative inputs per class
  3. Gather activations and build tensors
  4. Fit TCA models and select rank
  5. Analyze and interpret TCA components
  6. Validate findings with masking experiments

- Design tradeoffs:
  - TCA rank selection: Higher rank may capture more detail but risks overfitting
  - Input selection: Random vs. optimized inputs affect interpretability
  - Layer choice: Early vs. late layers capture different levels of abstraction

- Failure signatures:
  - Low TCA reconstruction error but uninterpretable components
  - Components that select many activations, inputs, and time points simultaneously
  - Inconsistent component patterns across similar strategies

- First 3 experiments:
  1. Fit TCA on activation tensors from ResNet50, rank=10, and visualize components
  2. Compare TCA components across CL strategies for a single layer
  3. Run masking experiments on filters highlighted by TCA to validate their importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do reconstruction errors vary so dramatically between architectures, with ResNet50 showing significantly lower errors than CvT13 and DeiTSmall?
- Basis in paper: [explicit] The paper explicitly states "the reconstruction error for the ResNet50 architecture is significantly lower than the other architectures" and shows this in Figure 2
- Why unresolved: The authors note this pattern but don't provide an explanation for the architectural differences in reconstruction performance
- What evidence would resolve it: Comparative analysis of architectural differences (residual connections, attention mechanisms, etc.) and their impact on tensor decomposition quality

### Open Question 2
- Question: Why don't the tensor decomposition components clearly identify specialized neuron clusters for specific tasks as hypothesized?
- Basis in paper: [explicit] "we found that our methodology did not directly highlight specialized clusters of neurons" and "our method does not readily identify specialized clusters of neurons across architectures"
- Why unresolved: Despite using importance-based methods designed to encourage specialization, the TCA method fails to reveal the expected patterns
- What evidence would resolve it: Systematic analysis of how TCA component selection relates to task-specific importance measures and parameter updates

### Open Question 3
- Question: Why do filter decomposition plots show smoother temporal patterns but remain difficult to interpret compared to activation decompositions?
- Basis in paper: [explicit] "the filter decomposition plots produce smoother curves on the temporal axis" but "it is difficult to try and intuit what a selected filter might represent"
- Why unresolved: The authors observe empirical differences in smoothness but can't explain the lack of interpretability in filter selections
- What evidence would resolve it: Analysis of the relationship between selected filters and their functional roles in the network, possibly through ablation studies

## Limitations
- TCA framework fails to clearly reveal specialized neuron clusters despite using importance-based CL methods designed to encourage specialization
- Significant variation in reconstruction errors across architectures suggests architectural dependencies that are not fully understood
- Filter decomposition remains difficult to interpret even though it produces smoother temporal patterns than activation decomposition

## Confidence
- High: TCA can successfully isolate specific classes and temporal regions in activation tensors
- Medium: TCA can reveal structured patterns in how internal representations evolve
- Low: TCA can clearly identify task-specific neuron specialization or filter evolution patterns

## Next Checks
1. Run ablation studies varying TCA rank to determine optimal settings for interpretability vs. reconstruction error
2. Compare TCA results with alternative interpretability methods (e.g., PCA, activation maximization) on the same data
3. Conduct cross-validation by training separate models and comparing TCA consistency across runs