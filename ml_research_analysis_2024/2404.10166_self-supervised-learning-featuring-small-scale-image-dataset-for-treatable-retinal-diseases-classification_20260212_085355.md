---
ver: rpa2
title: Self-Supervised Learning Featuring Small-Scale Image Dataset for Treatable
  Retinal Diseases Classification
arxiv_id: '2404.10166'
source_url: https://arxiv.org/abs/2404.10166
tags:
- learning
- images
- dataset
- training
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates self-supervised learning (SSL) methods for
  classifying treatable retinal diseases using small-scale OCT image datasets (125-4000
  images). Four SSL models (SwAV, MoCo-v2, SimCLR, BYOL) are compared against two
  transfer learning models (ResNet50, Inception-v3) on a 4-class classification task
  (CNV, DME, Drusen, Normal).
---

# Self-Supervised Learning Featuring Small-Scale Image Dataset for Treatable Retinal Diseases Classification

## Quick Facts
- arXiv ID: 2404.10166
- Source URL: https://arxiv.org/abs/2404.10166
- Reference count: 15
- Primary result: MoCo-v2 and SimCLR-v1 SSL models achieve 98.62-98.84% accuracy on 4-class retinal disease classification using only 4,000 OCT images

## Executive Summary
This study evaluates self-supervised learning (SSL) methods for classifying treatable retinal diseases from OCT images using small-scale datasets. Four SSL models (SwAV, MoCo-v2, SimCLR, BYOL) are compared against two transfer learning baselines (ResNet50, Inception-v3) on a 4-class classification task. The results demonstrate that SSL models, particularly MoCo-v2 and SimCLR-v1, achieve state-of-the-art accuracy (98.62-98.84%) with only 4,000 training images, significantly outperforming transfer learning approaches. The study highlights SSL's effectiveness in low-data regimes and its potential for expanding automated diagnosis to rare diseases with limited training data.

## Method Summary
The study uses OCT images from the UCSD Dataset, creating balanced and imbalanced training subsets of sizes 125-4,000 images across four classes (CNV, DME, Drusen, Normal). Four SSL models (SwAV, MoCo-v2, SimCLR-v1, BYOL) pre-trained on ImageNet are fine-tuned on OCT data with a training procedure that freezes representation layers initially, then unfreezes the last block. Transfer learning baselines (ResNet50, Inception-v3) are also fine-tuned. Data augmentation includes rotation, flipping, cropping, and normalization. Weighted cross-entropy loss is used for imbalanced training, and early stopping with 10 epochs patience is applied.

## Key Results
- MoCo-v2 and SimCLR-v1 achieve state-of-the-art accuracy of 98.62% and 98.84% respectively using only 4,000 training images
- SSL models consistently outperform transfer learning models under both balanced and imbalanced training scenarios
- SSL models demonstrate superior performance when training sets are smaller than 500 images
- The proposed SSL framework demonstrates computational efficiency, completing training in about one hour

## Why This Works (Mechanism)
The effectiveness of SSL in this study stems from its ability to learn rich, general representations from unlabeled data through contrastive learning objectives. By pre-training on ImageNet and fine-tuning on OCT data, SSL models develop robust feature extractors that capture domain-invariant patterns while maintaining task-specific discriminative power. This is particularly advantageous for small-scale medical datasets where labeled data is scarce, as SSL leverages unlabeled data to learn meaningful representations that transfer well to the target task.

## Foundational Learning
- Contrastive Learning: Why needed - to learn discriminative features by comparing similar and dissimilar samples. Quick check - ensure positive and negative pairs are correctly formed during training.
- Transfer Learning: Why needed - to leverage pre-trained models on large datasets to improve performance on small, specialized datasets. Quick check - verify that the pre-trained weights are properly loaded and fine-tuned.
- Imbalanced Learning: Why needed - to handle class imbalance in training data and prevent model bias. Quick check - monitor class-wise performance metrics to ensure balanced predictions.

## Architecture Onboarding

Component Map:
OCT Images -> Data Augmentation -> SSL/Transfer Learning Models -> Fine-tuning -> Classification Output

Critical Path:
Data Preprocessing -> Model Pre-training (ImageNet) -> Fine-tuning (OCT Data) -> Evaluation (Accuracy, F1 Score)

Design Tradeoffs:
- Pre-training on ImageNet vs. domain-specific medical images: ImageNet provides general features but may lack domain-specific knowledge
- Model complexity vs. training time: Larger models may achieve better performance but require more computational resources
- Data augmentation strength vs. overfitting: Stronger augmentation helps generalization but may slow convergence

Failure Signatures:
- Overfitting on small datasets: Characterized by high training accuracy but low validation/test performance
- Unstable training of SSL models: Indicated by fluctuating loss values and poor convergence
- Imbalanced predictions: Model consistently predicts majority class, resulting in low recall for minority classes

First Experiments:
1. Fine-tune SSL models on balanced 1,000-image subset and evaluate accuracy
2. Compare SSL and transfer learning performance on imbalanced 500-image subset
3. Analyze feature visualizations to understand learned representations

## Open Questions the Paper Calls Out
- How does the proposed SSL framework generalize to other medical imaging modalities beyond OCT, such as CT or MRI scans?
- How does the performance of SSL models compare when pre-trained on domain-specific datasets (medical images) rather than general datasets like ImageNet?
- What is the optimal balance between the number of training epochs and the size of the training dataset to achieve the best performance in SSL for medical image classification?

## Limitations
- Results are based on a single dataset (UCSD), limiting generalizability to other OCT datasets or imaging modalities
- Detailed hyperparameter configurations for SSL models are not provided, making exact reproduction challenging
- The computational efficiency claim (1-hour training) is dataset-dependent and may not generalize to larger or more complex models

## Confidence
- High confidence: SSL models outperform transfer learning baselines on small, imbalanced datasets
- Medium confidence: SSL models maintain performance advantages across different dataset sizes and imbalance ratios
- Low confidence: The computational efficiency claim (1-hour training) is dataset-dependent and may not generalize

## Next Checks
1. Reproduce the results on an independent OCT dataset to assess generalizability of the SSL framework
2. Conduct ablation studies to identify the impact of individual data augmentation techniques on SSL model performance
3. Evaluate the robustness of SSL models to distribution shifts by testing on OCT images from different scanners or protocols