---
ver: rpa2
title: Revisiting Experience Replayable Conditions
arxiv_id: '2402.10374'
source_url: https://arxiv.org/abs/2402.10374
tags:
- learning
- data
- policy
- empirical
- tricks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why experience replay (ER) is limited to
  off-policy reinforcement learning algorithms. The author hypothesizes that each
  RL algorithm has a specific set of empirical data it can learn from, and ER can
  only be applied if the replayed data belong to that set.
---

# Revisiting Experience Replayable Conditions

## Quick Facts
- arXiv ID: 2402.10374
- Source URL: https://arxiv.org/abs/2402.10374
- Authors: Taisuke Kobayashi
- Reference count: 13
- Key outcome: Proposes stabilization tricks enabling experience replay for on-policy algorithms, achieving performance comparable to SAC

## Executive Summary
This paper investigates why experience replay (ER) is typically limited to off-policy reinforcement learning algorithms. The author proposes that each RL algorithm has a specific set of empirical data it can learn from, and ER can only be applied if replayed data belongs to that set. Through theoretical analysis, the paper reveals that policy improvement algorithms can be viewed as minimizing a triplet loss in metric learning, inheriting instability factors such as hard negative samples and distribution shift. To address these issues, two stabilization tricks are proposed: a counteraction that expands the acceptable set of data by regularizing the policy to match the behavior policy, and mining that selectively replays data indistinguishable by an experience discriminator.

## Method Summary
The method involves implementing two stabilization tricks to enable experience replay for on-policy algorithms. First, an experience discriminator is implemented to estimate whether empirical data in the replay buffer is suitable for learning. The counteraction trick is then applied to expand the acceptable set of empirical data by regularizing the policy to match the behavior policy. Finally, the mining trick is implemented to selectively replay data that is indistinguishable by the experience discriminator. These tricks are applied to an advantage actor-critic (A2C) algorithm to demonstrate their effectiveness in enabling experience replay.

## Key Results
- Two stabilization tricks (counteraction and mining) enable experience replay for on-policy algorithms
- A2C with stabilization tricks achieves learning performance comparable to state-of-the-art off-policy algorithm SAC
- The proposed approach successfully solves complex tasks like Swimmer where SAC shows inconsistent performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy improvement algorithms can be viewed as minimizing a triplet loss in metric learning, inheriting instability factors such as hard negative samples and distribution shift.
- Mechanism: The paper derives that the gradient of policy improvement algorithms corresponds to the gradient of triplet loss, where anchor data (current policy π) should be close to positive data (π+) and away from negative data (π−). Random sampling from ER activates the instability factors, causing learning breakdowns.
- Core assumption: Policy improvements correspond to minimizing triplet loss, and instability factors of triplet loss are inherited by policy improvements.
- Evidence anchors: [abstract] and [section 3.1] show the mathematical relationship between policy improvement and triplet loss minimization.
- Break condition: If KL(π | π+) > KL(π | π−), the repulsive force from π− becomes stronger than the attraction to π+, leading to divergence.

### Mechanism 2
- Claim: The two stabilization tricks, counteraction and mining, alleviate the corresponding instability factors respectively, enabling experience replay to be applied to on-policy algorithms.
- Mechanism: Counteraction expands the acceptable set of empirical data by regularizing the policy to match the behavior policy, while mining selectively replays data indistinguishable by an experience discriminator.
- Core assumption: Counteraction and mining tricks can alleviate instability factors of triplet loss, enabling experience replay for on-policy algorithms.
- Evidence anchors: [abstract] confirms that stabilization tricks make ER applicable to A2C, and [section 4.2] explains how counteraction avoids hard negatives and stabilizes learning.
- Break condition: If counteraction and mining tricks are not appropriately designed or tuned, they may not effectively alleviate instability factors, and experience replay may still not be applicable to on-policy algorithms.

### Mechanism 3
- Claim: The proposed stabilization tricks enable A2C to achieve learning performance comparable to SAC, a state-of-the-art off-policy algorithm.
- Mechanism: By satisfying experience replayable conditions (ERC), A2C with stabilization tricks can utilize experience replay effectively, leading to improved learning performance comparable to SAC.
- Core assumption: Proposed stabilization tricks can effectively alleviate instability factors of triplet loss, enabling A2C to utilize experience replay and achieve comparable learning performance to SAC.
- Evidence anchors: [abstract] states that learning performance is comparable to SAC, and [section 5.4] shows experimental results demonstrating high performance on Swimmer task.
- Break condition: If proposed stabilization tricks do not effectively alleviate instability factors, or if A2C architecture is not suitable for tasks, learning performance may not be comparable to SAC.

## Foundational Learning

- Concept: Metric learning with triplet loss
  - Why needed here: The paper reveals that policy improvement algorithms can be viewed as minimizing a triplet loss in metric learning, inheriting instability factors.
  - Quick check question: What is the desired distance relationship in metric learning with triplet loss, and how does it relate to policy improvement algorithms?

- Concept: Experience replay in reinforcement learning
  - Why needed here: The paper investigates why experience replay is limited to off-policy algorithms and proposes stabilization tricks to enable its application to on-policy algorithms.
  - Quick check question: What is the main challenge in applying experience replay to on-policy algorithms, and how do the proposed stabilization tricks address this challenge?

- Concept: Policy gradient algorithms
  - Why needed here: The paper focuses on A2C, an on-policy policy gradient algorithm, and proposes stabilization tricks to enable experience replay.
  - Quick check question: What are the key differences between on-policy and off-policy policy gradient algorithms, and why is experience replay generally considered applicable only to off-policy algorithms?

## Architecture Onboarding

- Component map: A2C algorithm with value function V and policy π -> Experience replay buffer storing empirical data -> Counteraction trick (regularization) -> Mining trick (selective replay) -> SAC algorithm for performance comparison

- Critical path:
  1. Collect empirical data from environment interaction
  2. Store data in experience replay buffer
  3. Select data for replay using mining trick
  4. Apply counteraction regularization to expand acceptable data set
  5. Update A2C policy and value function using replayed data
  6. Evaluate learning performance and compare to SAC

- Design tradeoffs:
  - Balancing counteraction regularization strength to avoid saturation while ensuring stability
  - Tuning mining trick parameters to select useful data without excluding too much
  - Choosing appropriate neural network architectures for policy and value function approximation

- Failure signatures:
  - A2C fails to learn or learns slowly without the proposed stabilization tricks
  - Excessive saturation of counteraction regularization gain or mining dropout rate
  - High variance in learning performance across different random seeds

- First 3 experiments:
  1. Verify that the proposed stabilization tricks enable A2C to learn with experience replay on a simple task (e.g., DoublePendulum)
  2. Perform ablation tests to confirm the necessity of both counteraction and mining tricks
  3. Evaluate the learning performance of A2C with stabilization tricks on more complex tasks and compare to SAC

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical foundation linking policy improvement to triplet loss minimization relies heavily on theoretical derivation rather than extensive empirical validation
- The stabilization tricks lack detailed implementation specifics and clear guidance on parameter tuning
- The learning performance comparison is limited to a small set of tasks, suggesting potential task-dependency of the approach

## Confidence
- High: The theoretical foundation linking policy improvement to triplet loss minimization
- Medium: The effectiveness of the stabilization tricks in enabling experience replay for on-policy algorithms
- Low: The generalizability of the approach across diverse reinforcement learning tasks and environments

## Next Checks
1. Conduct ablation studies to isolate the individual contributions of the counteraction and mining tricks to the overall performance improvement.
2. Perform extensive hyperparameter sensitivity analysis to determine the robustness of the proposed approach to different settings.
3. Evaluate the proposed method on a larger and more diverse set of reinforcement learning benchmarks to assess its generalizability.