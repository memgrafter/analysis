---
ver: rpa2
title: 'DDIL: Diversity Enhancing Diffusion Distillation With Imitation Learning'
arxiv_id: '2410.11971'
source_url: https://arxiv.org/abs/2410.11971
tags:
- distillation
- student
- diffusion
- teacher
- ddil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of poor performance in multi-step
  distilled diffusion models due to covariate shift between training and inference
  time. The authors propose DDIL (Diffusion Distillation with Imitation Learning),
  a framework that enhances training distributions by incorporating both data distribution
  (forward diffusion) and student-induced distributions (backward diffusion at inference).
---

# DDIL: Diversity Enhancing Diffusion Distillation With Imitation Learning

## Quick Facts
- arXiv ID: 2410.11971
- Source URL: https://arxiv.org/abs/2410.11971
- Authors: Risheek Garrepalli; Shweta Mahajan; Munawar Hayat; Fatih Porikli
- Reference count: 40
- Key outcome: DDIL improves distilled diffusion models by addressing covariate shift through mixed rollouts, achieving 18.73 FID@30k with only 40K updates and batch size 7 on SSD1B.

## Executive Summary
This paper addresses the fundamental problem of poor performance in multi-step distilled diffusion models due to covariate shift between training and inference time. The authors propose DDIL (Diffusion Distillation with Imitation Learning), a framework that enhances training distributions by incorporating both data distribution (forward diffusion) and student-induced distributions (backward diffusion at inference). DDIL integrates with various distillation methods like progressive distillation, LCM, and DMD2, consistently improving both sample quality and diversity across these methods.

## Method Summary
DDIL is a diffusion distillation framework built on imitation learning principles that addresses covariate shift by training students on mixed trajectories from both forward diffusion (data distribution) and backward trajectories (student/teacher induced distributions). The method uses a prioritized replay buffer to store and sample from these mixed distributions, with reflected diffusion thresholding applied to both teacher and student models to enforce the support of the data distribution and improve training stability. The framework is agnostic to specific distillation techniques and can be integrated with progressive distillation, LCM, and DMD2.

## Key Results
- DDIL with DMD2 on SSD1B achieves 18.73 FID@30k with only 40K updates and batch size of 7
- Consistent quality improvements: 19.41% FID@30k boost over baseline DMD2
- Significant diversity improvements: 19.38% coverage boost over baseline DMD2
- Computational efficiency demonstrated through superior performance with minimal training resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DDIL improves distilled diffusion models by correcting covariate shift through mixed rollouts that combine teacher and student trajectories.
- Mechanism: The method alternates between sampling latents from forward diffusion (data distribution) and backward trajectories (student/teacher induced distributions). This exposes the student to the true marginal data distribution while also allowing corrective feedback on its own predictions.
- Core assumption: The discrepancy between training and inference distributions in few-step distilled models leads to compounding errors that degrade quality and diversity.
- Evidence anchors:
  - [abstract]: "We formulate diffusion distillation within imitation learning (DDIL) framework and enhance training distributions by incorporating both data distribution (forward diffusion) and student-induced distributions (backward diffusion at inference)."
  - [section 4.1]: "DDIL specifically focuses on improving the training distribution, remaining agnostic to the specific feedback mechanism utilized by different distillation techniques."
  - [corpus]: Weak - corpus papers focus on different distillation approaches without explicit covariate shift correction mechanisms.
- Break condition: If the teacher and student distributions become too dissimilar, or if the mixed rollout ratio doesn't properly balance exposure to data distribution vs. corrective feedback.

### Mechanism 2
- Claim: Reflected diffusion formulation improves training stability and further mitigates covariate shift.
- Mechanism: Thresholding is applied to both teacher and student score estimates to enforce the support of the data distribution, preventing noisy gradient feedback that can destabilize training.
- Core assumption: Without thresholding, the gradient feedback during distillation can be noisy and negatively impact training stability, especially with small batch sizes.
- Evidence anchors:
  - [section 4.1]: "We adopt thresholding for both the teacher and student diffusion models to enforce the support of the data distribution with reflected diffusion [20] for distillation."
  - [section 5.1]: "We observe that when training DMD2 with small batch (7), without thresholding performance deteriorates and when we adopt thresholding can observe monotonic improvement in performance."
  - [corpus]: Weak - corpus papers mention thresholding in different contexts but not specifically for covariate shift mitigation.
- Break condition: If the threshold values are set too aggressively, potentially clipping important score information.

### Mechanism 3
- Claim: DDIL maintains diversity while improving quality by preserving the marginal data distribution during training.
- Mechanism: By training on both the data distribution (forward diffusion) and the student's predictive distribution (backward trajectory), DDIL ensures the student model maintains the statistical properties of the original data rather than collapsing to mode-seeking behavior.
- Core assumption: Methods that only train on backward trajectories (like DMD2 or ImagineFlash) lose diversity because they don't preserve the marginal data distribution.
- Evidence anchors:
  - [abstract]: "Training on data distribution helps to diversify the generations by preserving marginal data distribution and training on student distribution addresses compounding error by correcting covariate shift."
  - [section 5.1]: "We can observe consistent boost in both quality i.e., FID@30k of 19.41% and diversity i.e, coverage boost of 19.38% over baseline DMD2 when DDIL is integrated."
  - [corpus]: Weak - corpus papers mention diversity issues but don't provide solutions that preserve both quality and diversity.
- Break condition: If the sampling priors βf rwd and βstudent bckwrd are not properly balanced, potentially leading to either loss of diversity or insufficient correction of covariate shift.

## Foundational Learning

- Concept: Imitation Learning and DAgger algorithm
  - Why needed here: DDIL is built on the DAgger framework from imitation learning, which addresses covariate shift in sequential decision making by collecting trajectories from both expert and learner.
  - Quick check question: How does DAgger differ from behavior cloning in addressing covariate shift?

- Concept: Diffusion Models and Reverse Process as MDP
  - Why needed here: Understanding the reverse denoising process as a Markov Decision Process is crucial for applying imitation learning techniques to diffusion distillation.
  - Quick check question: What are the states and actions in the MDP formulation of diffusion models?

- Concept: Covariate Shift in Sequential Models
  - Why needed here: The core problem DDIL addresses is the discrepancy between training and inference distributions in sequential models, which is particularly pronounced in few-step distilled models.
  - Quick check question: Why is covariate shift more problematic in few-step distilled models compared to the original teacher model?

## Architecture Onboarding

- Component map:
  Teacher diffusion model -> Prioritized replay buffer -> Student diffusion model -> DDIM solver

- Critical path:
  1. Sample latents from forward diffusion or mixed teacher/student rollouts
  2. Apply distillation loss (method-specific)
  3. Update student model parameters
  4. Periodically update replay buffer with new trajectories

- Design tradeoffs:
  - Batch size vs. training stability (reflected diffusion helps with small batches)
  - Teacher sampling rate vs. corrective feedback strength
  - Forward vs. backward trajectory sampling ratio for balancing diversity and covariate shift correction

- Failure signatures:
  - Degradation in FID/CLIP scores when teacher sampling rate is too low
  - Loss of diversity (low coverage scores) when forward diffusion sampling is insufficient
  - Training instability with large gradients when thresholding is not applied

- First 3 experiments:
  1. Ablation study: Compare DDIL with baseline distillation method using same batch size and training steps
  2. Teacher sampling rate sweep: Evaluate performance across different ratios of teacher vs. student trajectory sampling
  3. Threshold sensitivity: Test different threshold values for reflected diffusion to find optimal balance between stability and performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DDIL's performance compare to other diversity-preserving distillation methods like EM Distillation on the same computational budget?
- Basis in paper: [explicit] The paper mentions EM Distillation as a mode-covering approach that addresses diversity loss but doesn't explicitly compare DDIL's performance to it.
- Why unresolved: The paper only discusses DDIL's improvements over baseline methods (PD, LCM, DMD2) and doesn't benchmark against EM Distillation or similar diversity-preserving techniques.
- What evidence would resolve it: Direct comparison experiments between DDIL and EM Distillation on the same datasets (COCO 2014/2017) using identical computational resources and batch sizes.

### Open Question 2
- Question: What is the optimal sampling prior schedule (βfwd, βteach_bckwrd, βstudent_bckwrd) for DDIL across different distillation methods and model sizes?
- Basis in paper: [inferred] The paper mentions that sampling priors can be tailored based on training stage and objectives but only provides specific examples for LCM and DMD2, leaving the optimal scheduling strategy unclear.
- Why unresolved: The paper uses fixed sampling priors in experiments and doesn't explore dynamic scheduling strategies or their impact on performance across different distillation methods.
- What evidence would resolve it: Ablation studies showing performance variations with different sampling prior schedules across multiple distillation methods and model scales.

### Open Question 3
- Question: How does DDIL's diversity improvement affect downstream task performance compared to quality-focused distillation methods?
- Basis in paper: [explicit] The paper emphasizes DDIL's ability to improve both quality (FID) and diversity (coverage) but doesn't evaluate how this diversity translates to practical downstream applications.
- Why unresolved: The experiments focus solely on generation quality metrics and don't assess DDIL's impact on tasks like fine-tuning, adaptation, or specific use cases where diversity matters.
- What evidence would resolve it: Experiments measuring DDIL's performance on downstream tasks like domain adaptation, few-shot learning, or specialized generation tasks compared to quality-focused alternatives.

### Open Question 4
- Question: Can DDIL be effectively applied to non-text-to-image diffusion models like audio or video generation?
- Basis in paper: [inferred] The paper focuses exclusively on text-to-image diffusion models and doesn't explore whether DDIL's framework generalizes to other data modalities.
- Why unresolved: The mathematical formulation of DDIL is presented in a modality-agnostic way, but the experiments and analysis are limited to image generation, leaving generalization to other domains unexplored.
- What evidence would resolve it: Successful application and performance evaluation of DDIL on audio diffusion models (e.g., for speech synthesis) or video diffusion models (e.g., for frame interpolation).

## Limitations

- Effectiveness heavily depends on proper balance of sampling priors (βf rwd, βteach bckwrd, βstudent bckwrd) which are not fully specified
- Computational efficiency claims (18.73 FID@30k with 40K updates and batch size 7) need independent verification
- Performance gains may be architecture-specific and require validation across different diffusion model families

## Confidence

- **High Confidence:** The core mechanism of using imitation learning (DAgger framework) to address covariate shift in diffusion distillation is well-founded and theoretically sound.
- **Medium Confidence:** The specific implementation details of DDIL (reflected diffusion, prioritized replay buffer) and their contributions to performance improvements are plausible but require independent validation.
- **Medium Confidence:** The reported performance improvements across multiple datasets and distillation methods are significant but need replication to confirm generalizability.

## Next Checks

1. **Ablation Study Replication:** Independently reproduce the ablation experiments showing the impact of forward diffusion sampling (βf rwd) on diversity preservation and covariate shift correction.

2. **Hyperparameter Sensitivity Analysis:** Systematically test different values of sampling priors and thresholding parameters to identify optimal settings and their impact on stability vs. performance tradeoffs.

3. **Cross-Architecture Generalization:** Evaluate DDIL on different diffusion model architectures (beyond SDv1.5 and SSD1B) to assess whether the performance gains are architecture-agnostic or specific to certain model families.