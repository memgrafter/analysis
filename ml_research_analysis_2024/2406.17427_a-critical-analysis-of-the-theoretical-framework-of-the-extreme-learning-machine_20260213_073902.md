---
ver: rpa2
title: A Critical Analysis of the Theoretical Framework of the Extreme Learning Machine
arxiv_id: '2406.17427'
source_url: https://arxiv.org/abs/2406.17427
tags:
- learning
- theorem
- hidden
- which
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper critiques the Extreme Learning Machine (ELM) theoretical
  framework. It presents a counterexample dataset that ELM cannot learn, even with
  random weight initialization and multiple runs.
---

# A Critical Analysis of the Theoretical Framework of the Extreme Learning Machine

## Quick Facts
- arXiv ID: 2406.17427
- Source URL: https://arxiv.org/abs/2406.17427
- Reference count: 28
- One-line primary result: ELM theoretical framework contains fundamental flaws, with a counterexample dataset that cannot be learned even with random weight initialization

## Executive Summary
This paper provides a critical analysis of the Extreme Learning Machine (ELM) theoretical framework, presenting a counterexample dataset that ELM cannot learn despite random weight initialization and multiple runs. The authors refute the proofs of two main theorems from the original ELM work, identifying errors in their logic related to probability theory and matrix invertibility. They provide alternative theoretical statements that support ELM under stricter conditions and demonstrate that the counterexample can be learned by ELM with different hyperparameters (ReLU activation and 8000 hidden neurons). The work highlights a significant gap between ELM's theoretical justification and its practical effectiveness.

## Method Summary
The paper constructs a specific counterexample dataset S consisting of 400 training samples with carefully designed input-output pairs that ELM cannot learn exactly. The ELM algorithm is implemented with random weight initialization, sigmoid activation function, and varying numbers of hidden neurons (N=400, 300, 200, 100, 50). The authors analyze the hidden layer output matrix H for invertibility and examine the conditions under which ELM's theoretical guarantees fail. They also test ELM with alternative hyperparameters (ReLU activation and 8000 hidden neurons) to demonstrate that the counterexample can be learned under modified settings.

## Key Results
- A counterexample dataset S is constructed that ELM cannot learn exactly, even with random weight initialization and multiple runs
- The proofs of two main theorems from the original ELM work are refuted due to errors in probability theory and matrix analysis
- Alternative theoretical statements are provided that justify ELM's efficiency under stricter conditions
- The counterexample dataset can be learned by ELM with ReLU activation and 8000 hidden neurons instead of the original requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random weight initialization in ELM can fail to achieve invertibility of the hidden layer output matrix H when input vectors are linearly dependent or when the activation function has a countable set of critical points.
- Mechanism: The hidden layer output matrix H becomes singular if two or more hidden neurons produce identical columns, which happens when their pre-activation values map to the same activation output. This occurs with non-invertible combinations of random weights and biases, especially when input vectors are not linearly independent or the activation function's derivative vanishes on a countable set.
- Core assumption: The proof assumes that randomly chosen weights and biases from continuous distributions will almost surely produce an invertible H, but this fails when inputs are not linearly independent or the activation function has flat regions.
- Evidence anchors:
  - [abstract]: "we refute the proofs of two main statements, and we also create a dataset that provides a counterexample to the ELM learning algorithm"
  - [section]: "the set of weights and biases (w1, b1) ∈ Rn+1 such that H is not invertible, is a countable union of hyperplanes in Rn+1, whose interior is empty"
  - [corpus]: Weak - corpus focuses on applications, not theoretical counterexamples.
- Break condition: If input vectors are linearly dependent, or if the activation function has a countable set of critical points (where derivative is zero), then H may be singular with non-zero probability.

### Mechanism 2
- Claim: ELM with fixed N hidden neurons cannot guarantee exact learning of N distinct samples unless the dataset satisfies specific conditions.
- Mechanism: Theorem 2.1 claims that with N hidden nodes and infinitely differentiable activation, any N distinct samples can be learned exactly. However, the proof relies on incorrect assumptions about the behavior of random weight initialization and the properties of the activation function's critical points. The counterexample dataset in the paper shows that even with N=400 hidden nodes, exact learning fails for specially constructed input-output pairs.
- Core assumption: The original proof assumes that the mapping from weights/biases to hidden layer outputs is sufficiently "rich" to span the required space, but this fails for carefully constructed datasets where the activation function's output collapses to a subspace that cannot represent the target outputs.
- Evidence anchors:
  - [abstract]: "we also create a dataset that provides a counterexample to the ELM learning algorithm and explain its design"
  - [section]: "The dataset S is a counterexample to both statements of Theorem 2.1 and Theorem 2.2"
  - [corpus]: Weak - no direct support for counterexample methodology.
- Break condition: When the dataset is constructed such that the activation function's output for random weights/biases cannot span the space needed to represent the target outputs exactly.

### Mechanism 3
- Claim: The minimum norm least-squares solution via Moore-Penrose pseudoinverse does not guarantee solution existence or bounded error for ELM.
- Mechanism: The paper shows that the theoretical framework claims ELM finds the minimum norm least-squares solution β = H†T, but this solution may not exist uniquely, and the error ∥Hβ - T∥ may not be bounded by any ε for finite hidden neurons. The random nature of H means there's no guarantee of proximity to the target outputs.
- Core assumption: The original framework assumes that for any dataset, there exists some β such that the error can be made arbitrarily small, but this fails because the random initialization of H may place it in a subspace that cannot approximate the target outputs well.
- Evidence anchors:
  - [section]: "we can neither guarantee that the system (2) has a solution, nor that the distance between the matrix T and the set of matrices {Hβ | β ∈ R ˜N ×m} is smaller than any prescribed ε"
  - [abstract]: "we provide alternative statements of the foundations, which justify the efficiency of ELM in some theoretical cases"
  - [corpus]: Weak - corpus focuses on ELM applications, not theoretical limitations.
- Break condition: When the random H matrix cannot approximate the target outputs within any desired error tolerance, even with many hidden neurons.

## Foundational Learning

- Concept: Linear independence of input vectors
  - Why needed here: The proof of Theorem 2.1 critically depends on the assumption that input vectors are linearly independent to ensure the weight matrix has full rank and the hidden layer output matrix H is invertible with high probability.
  - Quick check question: Given three input vectors in R², can you determine if they are linearly independent?

- Concept: Properties of activation functions (differentiability and critical points)
  - Why needed here: The theoretical analysis requires understanding when activation functions can produce singular H matrices, particularly when the derivative is zero on a countable set, which can cause rank deficiency.
  - Quick check question: For g(x) = sin(x), what is the set of critical points where g'(x) = 0?

- Concept: Probability theory and measure theory (Jordan measurability, probability spaces)
  - Why needed here: The paper's critique and alternative results rely on understanding when events with probability zero correspond to empty interiors in continuous probability spaces, which is essential for the probabilistic guarantees claimed by ELM.
  - Quick check question: If a set has empty interior in Rn, what can you say about its Lebesgue measure?

## Architecture Onboarding

- Component map: Random weights -> bias addition -> activation function -> matrix H formation -> pseudoinverse calculation -> output weights β
- Critical path: Random weights → bias addition → activation → matrix H formation → pseudoinverse calculation → output weights β. The bottleneck is ensuring H is well-conditioned for inversion.
- Design tradeoffs: Random initialization trades computational efficiency for theoretical guarantees. More hidden neurons increase representational capacity but don't guarantee better performance if H becomes ill-conditioned. Different activation functions affect the probability of H being invertible.
- Failure signatures: Training error remains high even with many hidden neurons, output weights become numerically unstable, or the pseudoinverse calculation fails due to singular H. Visual inspection of outputs showing systematic deviation from targets indicates fundamental representation issues.
- First 3 experiments:
  1. Test ELM on a simple dataset where inputs are clearly linearly independent (e.g., identity matrix as inputs) with varying numbers of hidden neurons to verify when exact learning occurs.
  2. Create a dataset with linearly dependent inputs and observe how often ELM fails to learn exactly, measuring the condition number of H across multiple runs.
  3. Compare different activation functions (sigmoid, ReLU, sin) on the same dataset to quantify how the choice affects the probability of achieving low training error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ELM theoretical framework be repaired by adding probabilistic guarantees that are consistent with continuous probability spaces and discrete computer implementations?
- Basis in paper: [explicit] The authors note that "the biggest drawback is the identification of impossible events with events with probability 0 in continuous probability spaces" and provide a corrected theorem that adds proper probabilistic foundations.
- Why unresolved: The paper provides an alternative theoretical result (Theorem 1) that partially supports ELM under stricter conditions, but it requires further development to bridge the gap between theory and practical implementation on discrete computer systems.
- What evidence would resolve it: A complete theoretical framework that establishes rigorous probabilistic guarantees for ELM in discrete computational settings, with proofs showing how the continuous probability assumptions translate to finite-precision implementations.

### Open Question 2
- Question: What specific modifications to the ELM methodology could improve its theoretical justification while maintaining its practical effectiveness?
- Basis in paper: [explicit] The authors state "Our further work continues in this direction... we aim to develop other results that will allow some modification of the ELM technique, for example, by changing the random selection of weights and biases."
- Why unresolved: While the paper identifies flaws in the current theoretical framework and demonstrates ELM's practical effectiveness, it doesn't propose specific algorithmic modifications that would simultaneously satisfy theoretical requirements and preserve practical utility.
- What evidence would resolve it: Empirical and theoretical studies demonstrating modified ELM algorithms that satisfy rigorous mathematical conditions while achieving comparable or improved performance on benchmark problems.

### Open Question 3
- Question: How can the ELM learning algorithm be modified to handle datasets with sharp oscillations or functions with high-frequency components more effectively?
- Basis in paper: [explicit] The authors created a counterexample dataset with sharp oscillations that ELM cannot learn with standard hyperparameters, but can be learned with different settings (ReLU activation and 8000 hidden neurons instead of 400).
- Why unresolved: The paper shows that ELM fails on certain datasets with the original theoretical guarantees but succeeds with modified hyperparameters, suggesting that the current theoretical framework doesn't capture the full capabilities of ELM.
- What evidence would resolve it: A comprehensive analysis identifying the relationship between dataset characteristics (frequency content, smoothness, etc.) and the optimal ELM hyperparameters, along with theoretical guarantees for learning such datasets.

## Limitations

- The theoretical critique relies heavily on abstract mathematical constructions that may be difficult to verify empirically
- The counterexample dataset is specifically engineered to break ELM, which may not reflect typical real-world scenarios
- The paper's alternative theoretical statements apply under stricter conditions than the original ELM framework
- The paper does not provide complete implementation details or code for reproducing the experiments

## Confidence

- **High Confidence**: The identification of errors in the original ELM theorem proofs and the construction of a valid counterexample dataset
- **Medium Confidence**: The alternative theoretical statements provided as replacements for the original ELM framework
- **Low Confidence**: The extent to which the counterexample represents a fundamental limitation of ELM versus an edge case

## Next Checks

1. Implement the ELM algorithm with the exact parameters specified in the paper and verify that it cannot learn the counterexample dataset with sigmoid activation and N=400 hidden neurons, while successfully learning it with ReLU activation and 8000 hidden neurons.

2. Conduct multiple runs of ELM on various datasets with different properties (linearly independent vs. dependent inputs, different activation functions) to quantify the probability of achieving exact learning and the conditions under which it fails.

3. Independently verify the mathematical proofs of the alternative statements provided in the paper, particularly focusing on the conditions required for the probabilistic guarantees and the assumptions about the activation function's properties.