---
ver: rpa2
title: '2D-Malafide: Adversarial Attacks Against Face Deepfake Detection Systems'
arxiv_id: '2408.14143'
source_url: https://arxiv.org/abs/2408.14143
tags:
- adversarial
- deepfake
- d-malafide
- detection
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 2D-Malafide is an adversarial attack that uses 2D convolutional
  filters to deceive face deepfake detection systems. It optimizes a small number
  of filter coefficients to craft perturbations that significantly degrade detection
  performance, remaining effective across different face images.
---

# 2D-Malafide: Adversarial Attacks Against Face Deepfake Detection Systems

## Quick Facts
- arXiv ID: 2408.14143
- Source URL: https://arxiv.org/abs/2408.14143
- Reference count: 28
- Primary result: 2D-Malafide uses 2D convolutional filters to craft transferable adversarial perturbations that significantly degrade face deepfake detection performance across different detection systems.

## Executive Summary
2D-Malafide is an adversarial attack framework that uses optimized 2D convolutional filters to generate perturbations that deceive face deepfake detection systems. Unlike traditional additive noise approaches, it optimizes a small number of filter coefficients that can be applied via convolution across multiple images, making the attack computationally lightweight and transferable. The method demonstrates substantial degradation in detection performance across various filter sizes and maintains effectiveness in both white-box and black-box settings when tested on the FaceForensics++ dataset.

## Method Summary
2D-Malafide employs gradient-based optimization to train small 2D convolutional filters that maximize detection error when applied to deepfake images. The attack uses the Adam optimizer to tune filter coefficients on a subset of the FaceForensics++ dataset, then evaluates performance on held-out test data. The method tests multiple filter sizes (3×3, 9×9, 27×27, and 81×81) and evaluates effectiveness using Equal Error Rate (EER) as the primary metric. GradCAM analysis is used to visualize how the attack alters the image regions that detection systems rely on for classification decisions.

## Key Results
- 2D-Malafide substantially degrades detection performance across both white-box and black-box settings on FaceForensics++ dataset
- Larger filter sizes (27×27 and 81×81) produce the most significant degradation in detection performance
- GradCAM analysis shows the attack misleads detection by altering the image areas most used for classification, particularly hiding fake image artifacts that detectors rely upon

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2D convolutional filters optimized with gradient descent can generalize across multiple deepfake images while still being lightweight.
- Mechanism: Instead of optimizing individual pixel perturbations per image, the filter coefficients are learned once and applied via convolution across all images, reducing the number of parameters while maintaining transferability.
- Core assumption: A small set of shared convolutional filters can disrupt detection features common to many deepfakes.
- Evidence anchors:
  - [abstract] "Unlike traditional additive noise approaches, 2D-Malafide optimises a small number of filter coefficients to generate robust adversarial perturbations which are transferable across different face images."
  - [section] "In this work, we propose the first adversarial attack which attempts to fulfil the generalisability property through convolutive noise while still being computationally lightweight."
  - [corpus] Weak; neighbors do not address convolutional filter generalization directly.
- Break condition: If the target detector relies on high-frequency artifacts that vary significantly across deepfake methods, a single shared filter may not consistently disrupt detection.

### Mechanism 2
- Claim: Larger filter sizes (27×27 and 81×81) cause greater degradation in detection performance.
- Mechanism: Larger filters cover more spatial context, allowing manipulation of broader image regions that contain critical detection features, but at the cost of increased image distortion.
- Core assumption: Detection models rely on spatial patterns spanning multiple pixels that can be disrupted by larger receptive fields.
- Evidence anchors:
  - [abstract] "Experiments, conducted using the FaceForensics++ dataset, demonstrate that 2D-Malafide substantially degrades detection performance... with larger filter sizes having the greatest impact."
  - [section] "The filter should then be tuned to counter the reliance of the FDD system upon attack-specific artefacts... Larger filters allow for greater manipulation and stronger attacks but can also introduce greater distortion."
  - [corpus] Weak; neighbors focus on other attack strategies, not filter size effects.
- Break condition: If the detection system uses only very localized features, larger filters may oversmooth the image and reduce the attack's effectiveness.

### Mechanism 3
- Claim: 2D-Malafide misleads detection by altering the image areas most used for classification, as shown by GradCAM analysis.
- Mechanism: The adversarial filter modifies regions that the detector focuses on (e.g., central facial areas, eyes, eyebrows), causing the model to either lose focus on artifacts or shift attention to less discriminative regions.
- Core assumption: Detection models have identifiable attention patterns that can be disrupted by targeted spatial perturbations.
- Evidence anchors:
  - [abstract] "we report an explainability analysis using GradCAM which illustrates how 2D-Malafide misleads detection systems by altering the image areas used most for classification."
  - [section] "Heatmaps in columns (c) and (d) display results after application of 2D-Malafide and for fake face images... 2D-Malafide hides fake image artefacts upon which the detector relies."
  - [corpus] Weak; neighbors do not discuss GradCAM-based explainability in the context of convolutional filter attacks.
- Break condition: If the detection system uses attention mechanisms that are robust to spatial perturbations, altering GradCAM heatmaps may not consistently degrade performance.

## Foundational Learning

- Concept: 2D convolution and filter optimization
  - Why needed here: Understanding how 2D filters are applied and optimized is essential to grasp how 2D-Malafide generalizes perturbations.
  - Quick check question: What is the difference between applying a learned filter via convolution versus adding per-pixel noise?

- Concept: Gradient-based adversarial optimization
  - Why needed here: The attack uses gradient descent to tune filter coefficients to maximize detection error.
  - Quick check question: How does optimizing filter coefficients differ from optimizing input pixel values in terms of parameter efficiency?

- Concept: GradCAM and model explainability
  - Why needed here: GradCAM is used to visualize which image regions the detector relies on, revealing how the attack misleads the model.
  - Quick check question: What does a shift in GradCAM heatmaps indicate about a model's decision process after adversarial perturbation?

## Architecture Onboarding

- Component map: Filter optimizer -> Adversarial filter -> Detection system -> GradCAM explainer
- Critical path: Generate deepfake images → Train filter to maximize detection error → Apply filter to test set → Evaluate EER → Analyze GradCAM heatmaps
- Design tradeoffs:
  - Filter size vs. attack strength: Larger filters increase attack impact but also image distortion
  - Generalizability vs. specificity: Shared filters work across images but may be less effective against specific detectors
  - Computational cost vs. parameter efficiency: Fewer filter coefficients mean faster training but potentially weaker attacks
- Failure signatures:
  - EER does not increase after filtering (attack ineffective)
  - GradCAM heatmaps remain unchanged (model attention not disrupted)
  - Image quality severely degraded, affecting auxiliary systems
- First 3 experiments:
  1. Train 3×3 filter on CADDM and test on CADDM; measure EER change
  2. Train 27×27 filter on CADDM and test on SBI; measure EER change
  3. Apply 9×9 filter to FaceShifter images; compare GradCAM heatmaps before/after filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different convolutional filter sizes affect the transferability of adversarial attacks across various deepfake generation algorithms?
- Basis in paper: [explicit] The paper states that "larger filter sizes (27×27 and 81×81) tend to cause the most significant degradation in detection performance, particularly for CADDM" and that "the impact varies with filter size."
- Why unresolved: While the paper demonstrates that larger filters are more effective, it doesn't explore the relationship between filter size and transferability across different deepfake generation methods.
- What evidence would resolve it: A systematic study varying filter sizes and measuring attack success rates across multiple deepfake generation algorithms would clarify this relationship.

### Open Question 2
- Question: Can adversarial attacks be designed to target specific artifacts introduced by different deepfake generation methods?
- Basis in paper: [inferred] The paper mentions that "the filter should then be tuned to counter the reliance of the FDD system upon attack-specific artefacts" and discusses different deepfake generation methods like Face2Face, FaceSwap, Deepfakes, NeuralTextures, and FaceShifter.
- Why unresolved: The paper doesn't investigate whether attacks can be optimized to exploit artifacts specific to each deepfake generation method.
- What evidence would resolve it: An analysis comparing attack effectiveness when targeting method-specific artifacts versus general deepfake artifacts would provide insights.

### Open Question 3
- Question: How does the introduction of adversarial perturbations affect the performance of other computer vision tasks beyond deepfake detection, such as face recognition or emotion recognition?
- Basis in paper: [explicit] The paper states that "for detection settings in the absence of a human observer, this may have little consequence. However, where the FDD system is deployed alongside other systems, the distortion introduced to compromise the FDD system might also interfere with the behaviour of any other auxiliary system, e.g. an automatic face recognition system."
- Why unresolved: The paper only briefly mentions this potential issue without exploring its impact on other computer vision tasks.
- What evidence would resolve it: An evaluation of adversarial attack effects on multiple computer vision tasks when applied to the same images would quantify the broader impact.

## Limitations
- The attack's generalizability to deepfake methods outside the FaceForensics++ dataset remains unproven
- Claims about transferability to unseen detection architectures (beyond CADDM and SBI) lack supporting evidence
- The trade-off between attack effectiveness and perceptual image quality at larger filter sizes is not fully quantified

## Confidence

| Claim | Confidence |
|-------|------------|
| EER degradation under white-box and black-box settings | High |
| GradCAM analysis showing altered attention patterns | Medium |
| Transferability to unseen deepfake methods or real-world scenarios | Low |

## Next Checks
1. **Cross-architecture validation**: Test 2D-Malafide filters against additional detection systems (Xception, EfficientNet) to assess generalizability beyond CADDM and SBI
2. **Out-of-distribution testing**: Apply optimized filters to deepfake videos generated by methods not included in FaceForensics++ (e.g., StyleGAN-based approaches) to evaluate true transferability
3. **Perceptual quality analysis**: Conduct user studies or implement quantitative metrics (SSIM, LPIPS) to measure the trade-off between attack effectiveness and image distortion at different filter sizes