---
ver: rpa2
title: Retrieval with Learned Similarities
arxiv_id: '2407.15462'
source_url: https://arxiv.org/abs/2407.15462
tags:
- retrieval
- items
- learned
- query
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Mixture-of-Logits (MoL) as a universal approximator
  for learned similarity functions in retrieval systems, addressing the gap between
  theoretical expressiveness and practical efficiency. MoL uses multiple low-rank
  embeddings with adaptive gating weights to compute similarity scores, enabling efficient
  retrieval with tight error bounds.
---

# Retrieval with Learned Similarities

## Quick Facts
- arXiv ID: 2407.15462
- Source URL: https://arxiv.org/abs/2407.15462
- Reference count: 40
- Key outcome: Mixture-of-Logits (MoL) achieves up to 66× latency reduction while maintaining >0.99 recall compared to exact algorithms

## Executive Summary
This paper introduces Mixture-of-Logits (MoL) as a universal approximator for learned similarity functions in retrieval systems. MoL uses multiple low-rank embeddings with adaptive gating weights to compute similarity scores, enabling efficient retrieval with tight error bounds. The authors propose a mutual information-based load balancing loss to improve performance and demonstrate MoL's effectiveness across diverse scenarios, including recommendation systems and question answering.

## Method Summary
The paper presents MoL as a universal approximator for learned similarity functions by decomposing high-rank matrices into mixtures of low-rank components. The method employs mutual information-based load balancing loss to ensure efficient utilization of all component embeddings during training. For inference, approximate top-K retrieval algorithms are proposed that achieve high recall with significant latency reduction. The approach is validated on recommendation systems (MovieLens, Amazon Books) and question answering tasks (Natural Questions).

## Key Results
- MoL achieves up to 66× latency reduction compared to exact algorithms
- Maintains >0.99 recall rate across all tested scenarios
- Sets new state-of-the-art results in multiple domains
- Mutual information-based load balancing loss improves performance by 2.4% HR@1, 0.8% HR@10, and 1.4% MRR

## Why This Works (Mechanism)

### Mechanism 1
MoL provides universal approximation by decomposing high-rank matrices into weighted sums of low-rank matrices. Any similarity matrix can be represented as a mixture of simpler components with adaptive weights determining their contribution.

### Mechanism 2
The mutual information-based load balancing loss ensures even utilization of all component embeddings by maximizing global entropy while minimizing conditional entropy given specific query-item pairs.

### Mechanism 3
Approximate top-K retrieval algorithms achieve high recall by using candidate selection based on dot products followed by MoL score evaluation, with theoretical error bounds ensuring near-exact results.

## Foundational Learning

- **Concept:** Universal approximation theory
  - **Why needed here:** Understanding why MoL can represent arbitrary learned similarity functions requires knowledge of how complex functions can be decomposed into simpler components.
  - **Quick check question:** Can you explain why a mixture of low-rank matrices can approximate any high-rank matrix?

- **Concept:** Mutual information and entropy
  - **Why needed here:** The load balancing loss relies on entropy maximization and conditional entropy minimization to ensure efficient component utilization.
  - **Quick check question:** What's the difference between maximizing global entropy and minimizing conditional entropy in the context of component utilization?

- **Concept:** Error bounds and approximation algorithms
  - **Why needed here:** The approximate retrieval algorithms provide theoretical guarantees about their accuracy relative to exact methods.
  - **Quick check question:** How does the error bound guarantee that approximate results are close to exact results?

## Architecture Onboarding

- **Component map:** Query encoder → MoL embedding construction → Similarity computation → Approximate retrieval algorithms → Top-K results
- **Critical path:** Embed queries and items → Compute adaptive gating weights → Calculate MoL similarity scores → Apply approximate retrieval → Return results
- **Design tradeoffs:** Number of components vs. model complexity, component dimensionality vs. expressiveness, load balancing strength vs. sparsity, approximate algorithm parameters vs. accuracy-latency tradeoff
- **Failure signatures:** Poor performance (weak load balancing), high latency (too many components), memory issues (high dimensionality), training instability (poor initialization)
- **First 3 experiments:**
  1. Verify universal approximation by training MoL to reproduce a known high-rank similarity matrix
  2. Test load balancing effectiveness by comparing component utilization with and without L_{MI} loss
  3. Benchmark approximate retrieval algorithms against exact methods on a small dataset to verify error bounds

## Open Questions the Paper Calls Out

### Open Question 1
What is the exact relationship between the mutual information-based load balancing loss (L_ML) and empirical performance, specifically how does hyperparameter α affect the tradeoff between load balancing and model quality?

### Open Question 2
How does the efficiency of approximate top-K retrieval algorithms scale with the number of embedding pairs (P) and dimensionality of low-rank embeddings (d_P)?

### Open Question 3
Can the theoretical expressiveness of MoL be further enhanced by incorporating additional structural priors or constraints, and how would this impact practical performance?

## Limitations
- Reliance on approximate algorithms may not maintain claimed error rates across all similarity function distributions
- Mutual information-based load balancing loss introduces additional hyperparameters that may be challenging to tune
- Assumption of effective decomposition into low-rank components may not hold for highly complex similarity functions

## Confidence
- Universal approximation capability: High
- Load balancing effectiveness: Medium
- Approximate algorithm performance: Medium

## Next Checks
1. Conduct ablation studies varying the number of components P to quantify the tradeoff between approximation quality and computational efficiency
2. Test the robustness of the load balancing loss by training models with different α values and measuring component utilization patterns
3. Benchmark the approximate algorithms on datasets with known challenging similarity distributions to verify practical applicability of error bounds