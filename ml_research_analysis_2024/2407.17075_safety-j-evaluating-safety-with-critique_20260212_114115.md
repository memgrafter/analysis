---
ver: rpa2
title: 'SAFETY-J: Evaluating Safety with Critique'
arxiv_id: '2407.17075'
source_url: https://arxiv.org/abs/2407.17075
tags:
- safety
- response
- critiques
- critique
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAFETY-J is a bilingual English-Chinese generative safety evaluator
  that addresses the lack of interpretability in current LLM safety classification
  systems. It provides detailed critiques explaining why content is deemed safe or
  unsafe, addressing transparency concerns that limit model improvement and user trust.
---

# SAFETY-J: Evaluating Safety with Critique

## Quick Facts
- arXiv ID: 2407.17075
- Source URL: https://arxiv.org/abs/2407.17075
- Authors: Yixiu Liu; Yuxiang Zheng; Shijie Xia; Jiajun Li; Yi Tu; Chaoling Song; Pengfei Liu
- Reference count: 38
- Primary result: SAFETY-J achieves 84.1% average accuracy across five test sets, outperforming GPT-4o (77.6%) and setting a new state-of-the-art among open-source models

## Executive Summary
SAFETY-J is a bilingual English-Chinese generative safety evaluator that addresses the lack of interpretability in current LLM safety classification systems. Unlike traditional classifiers that provide only binary labels, SAFETY-J generates detailed critiques explaining why content is deemed safe or unsafe, addressing transparency concerns that limit model improvement and user trust. The system is trained on 19,030 query-response pairs using iterative preference learning to dynamically refine safety assessments, achieving state-of-the-art performance among open-source models while demonstrating strong critique quality with Meta-F1 scores of 76.0 (English) and 80.0 (Chinese).

## Method Summary
SAFETY-J fine-tunes the InternLM2-7B-Chat model on a curated dataset of 19,030 query-response pairs with safety labels and LLM-generated critiques. The system employs an automated meta-evaluation benchmark using Qwen-72B-Chat to extract Atomic Information Units (AIUs) and assess critique quality through precision, recall, and F1 scores. Iterative preference learning with Direct Preference Optimization (DPO) continuously refines the evaluator by training it to prefer better-scoring critiques. The model generates safety labels before critiques to optimize the attention mechanism, and outputs bilingual critiques with different formats for English (detailed) and Chinese (concise).

## Key Results
- SAFETY-J achieves 84.1% average accuracy across five test sets, outperforming proprietary models like GPT-4o (77.6%)
- Sets new state-of-the-art among open-source models with strong performance on BeaverTails (92.9%), DiaSafety (78.7%), Jade (86.2%), Flames (74.3%), and WildSafety (81.8%)
- Demonstrates strong critique quality with Meta-F1 scores of 76.0 (English) and 80.0 (Chinese) on the meta-evaluation test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative preference learning with DPO continuously refines the safety evaluator's performance by dynamically updating critiques based on meta-evaluation feedback
- Mechanism: The system starts with two versions of SAFETY-J (M1 and M2) that generate critiques for the same query-response pairs. A meta-evaluation benchmark automatically assesses which set of critiques is better based on precision, recall, and F1 scores. The better-performing model's critiques become the "chosen" critiques while the other becomes "rejected" critiques. These form a preference dataset that is used to train the next version (M3) through Direct Preference Optimization (DPO). This process iterates through M4 and M5
- Core assumption: Meta-evaluation can reliably identify better critiques without human intervention, and these preferences can be effectively captured through DPO to improve the model
- Evidence anchors: [abstract] "We propose an iterative preference learning method to ensure continuous enhancement of SAFETY-J." [section] "We apply Direct Preference Optimization (DPO) to the better model to create M3, due to DPO's stability and efficiency in aligning with human preferences"
- Break condition: If the meta-evaluation benchmark fails to accurately distinguish between good and bad critiques, the preference learning loop will reinforce incorrect patterns

### Mechanism 2
- Claim: The automated meta-evaluation benchmark provides objective, scalable assessment of critique quality with minimal human intervention
- Mechanism: When SAFETY-J generates a critique, Qwen-72B-Chat parses Atomic Information Units (AIUs) from both the generated critique and reference critiques. The system then measures precision (how many AIUs are factually accurate), recall (how many reference AIUs are captured), and F1 score (harmonic mean of precision and recall) at both micro (AIU level) and macro (critique level) scales
- Core assumption: AIUs extracted by Qwen-72B-Chat accurately represent the key factual components of critiques, and their presence/absence correlates with critique quality
- Evidence anchors: [abstract] "We establish an automated meta-evaluation benchmark that objectively assesses the quality of critiques with minimal human intervention" [section] "Following Sun et al. (2024), when SAFETY-J produces a critique, we use Qwen-72B-Chat to parse Atomic Information Units (AIUs) and measure the quality with precision, recall, and F1 scores"
- Break condition: If the AIU extraction process fails to capture semantically important aspects of critiques or if the precision/recall metrics don't correlate with actual human judgment of critique quality

### Mechanism 3
- Claim: Positioning the safety label before the critique in the model output improves performance compared to placing critiques first
- Mechanism: During training, SAFETY-J is conditioned to generate the safety label first, followed by the critique. This ordering allows the attention mechanism to capture crucial safety information before generating the explanation, leading to better alignment between labels and critiques
- Core assumption: The order of information in the output affects how the model processes and relates different components, with label-first ordering providing better signal for safety assessment
- Evidence anchors: [section] "Preliminary experiments show that generating the critique before predicting the final label leads to performance degradation, consistent with ShieldLM" [corpus] Weak - this is primarily based on internal experiments rather than established literature
- Break condition: If the attention mechanism can effectively capture bidirectional relationships regardless of output order, or if the performance difference is due to other factors in the training setup

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) for aligning LLMs to specific tasks
  - Why needed here: SAFETY-J requires fine-tuning a general-purpose LLM (Internlm2-7B-Chat) to perform safety evaluation with critique generation, which is not its original training objective
  - Quick check question: What is the key difference between SFT and prompt engineering for task adaptation?

- Concept: Direct Preference Optimization (DPO) for aligning models to human preferences
  - Why needed here: DPO is used to iteratively improve SAFETY-J by training it to prefer critiques that score better on the meta-evaluation benchmark, creating a feedback loop for continuous refinement
  - Quick check question: How does DPO differ from traditional reinforcement learning approaches in terms of computational efficiency and stability?

- Concept: Atomic Information Units (AIUs) for granular evaluation
  - Why needed here: AIUs enable fine-grained, automated assessment of critique quality by breaking down critiques into discrete factual components that can be individually evaluated for precision and recall
  - Quick check question: What are the advantages and limitations of using AIUs compared to holistic critique evaluation methods?

## Architecture Onboarding

- Component map: Training data curation pipeline -> Base model (Internlm2-7B-Chat) -> Meta-evaluation system (Qwen-72B-Chat) -> Iterative preference learning loop (DPO) -> Evaluation framework
- Critical path: Training data → Initial SFT → Meta-evaluation → Preference learning → Improved model → Evaluation
- Design tradeoffs:
  - Using automated meta-evaluation reduces human labor but may miss nuanced aspects of critique quality
  - English and Chinese bilingual support increases utility but requires more diverse training data
  - Iterative improvement provides continuous refinement but increases computational cost
- Failure signatures:
  - If precision scores are high but recall scores are low, the model may be generating safe but generic critiques
  - If both precision and recall are low, the model may be generating factually incorrect or irrelevant critiques
  - If performance plateaus across iterations, the preference learning signal may be saturated or the meta-evaluation may be flawed
- First 3 experiments:
  1. Test the impact of output ordering (label-first vs critique-first) on safety classification accuracy using a held-out validation set
  2. Evaluate whether AIU-based meta-evaluation correlates with human judgment of critique quality using a small manually annotated subset
  3. Compare the stability and performance of DPO vs. other preference learning methods (RLHF, DPO) on the preference dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SAFETY-J's iterative preference learning mechanism handle cases where meta-evaluation creates conflicting preferences between different versions of the model?
- Basis in paper: [explicit] The paper describes iterative preference learning where critiques are regenerated and DPO is applied, but doesn't address conflict resolution when different versions produce conflicting outputs
- Why unresolved: The paper shows performance improvements across iterations but doesn't detail how the system resolves situations where earlier versions might generate better critiques than later ones for certain cases
- What evidence would resolve it: A detailed analysis of preference conflicts during iterations, showing specific examples where different model versions produced contradictory critiques and how the system determined which to prioritize

### Open Question 2
- Question: What is the impact of SAFETY-J's critique quality on downstream applications beyond the online correction scenario presented in the paper?
- Basis in paper: [inferred] The paper demonstrates online correction with Qwen-14B-Chat but doesn't explore other potential applications or quantify the impact of critique quality on various downstream tasks
- Why unresolved: While the paper shows effectiveness in one application scenario, it doesn't investigate how critique quality affects other potential use cases like model training, policy enforcement, or user trust metrics
- What evidence would resolve it: Comparative studies showing how different critique qualities affect various downstream applications, including quantitative measures of user trust, model training efficiency, and policy compliance rates

### Open Question 3
- Question: How does SAFETY-J's performance degrade when evaluating multi-turn dialogues versus single-turn interactions?
- Basis in paper: [explicit] The paper acknowledges that SAFETY-J "currently does not support multi-turn dialogues" but doesn't quantify the performance difference between single-turn and multi-turn scenarios
- Why unresolved: The paper identifies this as a limitation but doesn't provide data on how the model performs when applied to multi-turn contexts, which would be valuable for understanding real-world applicability
- What evidence would resolve it: Empirical testing comparing SAFETY-J's accuracy and critique quality on single-turn versus multi-turn dialogue datasets, including specific error analysis for multi-turn cases

### Open Question 4
- Question: What is the computational overhead of SAFETY-J's iterative refinement process compared to traditional safety evaluation methods?
- Basis in paper: [inferred] The paper describes an iterative process but doesn't provide computational efficiency metrics or compare the time/resource costs against non-iterative approaches
- Why unresolved: While the paper demonstrates effectiveness improvements, it doesn't address the practical deployment considerations of computational cost versus performance gains
- What evidence would resolve it: Detailed benchmarking of inference time, GPU memory usage, and overall computational costs for each iteration compared to baseline safety evaluation methods

## Limitations
- SAFETY-J currently does not support multi-turn dialogues, limiting its applicability to conversational contexts
- The meta-evaluation approach using AIU extraction may miss nuanced aspects of critique quality and relies on the reliability of Qwen-72B-Chat
- Bilingual capability shows promise but has limited Chinese-specific validation and no cross-lingual comparison

## Confidence
- SAFETY-J outperforms proprietary models: High confidence
- Automated meta-evaluation with minimal human intervention: Medium confidence
- Iterative preference learning continuously improves performance: Medium confidence
- Bilingual critique quality: Low confidence

## Next Checks
1. **Cross-lingual critique consistency test**: Generate identical safety scenarios in both English and Chinese, evaluate whether SAFETY-J produces comparable critique quality (precision/recall/F1) and safety classifications across languages. This would validate whether the bilingual capability truly works or if one language domain dominates.

2. **Meta-evaluation robustness check**: Create adversarial test cases where human-annotated critiques contain subtle errors or omissions. Compare SAFETY-J's meta-evaluation scores against human expert judgments to quantify the reliability gap and identify failure modes in the automated assessment pipeline.

3. **Generalization to unseen safety domains**: Test SAFETY-J on safety scenarios outside its training distribution (e.g., medical misinformation, financial fraud) using the existing meta-evaluation framework. Measure whether the preference learning approach can generalize learned critique patterns to novel safety concerns or overfits to training domain specifics.