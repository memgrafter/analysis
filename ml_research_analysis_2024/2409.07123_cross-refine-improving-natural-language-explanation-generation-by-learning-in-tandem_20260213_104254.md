---
ver: rpa2
title: 'Cross-Refine: Improving Natural Language Explanation Generation by Learning
  in Tandem'
arxiv_id: '2409.07123'
source_url: https://arxiv.org/abs/2409.07123
tags:
- cross-refine
- explanations
- language
- association
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CROSS-REFINE, a method that uses two large\
  \ language models\u2014one as a generator and one as a critic\u2014to iteratively\
  \ improve natural language explanations. The generator produces an initial explanation,\
  \ while the critic provides targeted feedback and a suggested improvement, which\
  \ the generator uses to refine its output."
---

# Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem

## Quick Facts
- arXiv ID: 2409.07123
- Source URL: https://arxiv.org/abs/2409.07123
- Reference count: 30
- Key outcome: CROSS-REFINE consistently outperforms SELF-REFINE in generating better natural language explanations through iterative refinement using a critic model.

## Executive Summary
This paper introduces CROSS-REFINE, a method that improves natural language explanations by using two large language models in tandem - one as a generator and one as a critic. The generator produces an initial explanation, while the critic provides targeted feedback and a suggested improvement, which the generator uses to refine its output. Experiments on commonsense question answering, natural language inference, and fact-checking tasks show that CROSS-REFINE consistently outperforms SELF-REFINE in both automatic and human evaluations, especially when using the same LLM for both roles. The approach works effectively with smaller models, whereas SELF-REFINE performs well only with ChatGPT.

## Method Summary
CROSS-REFINE uses a two-LLM approach where one LLM acts as a generator to produce natural language explanations, and another LLM acts as a critic to evaluate these explanations and provide both feedback on errors and suggestions for improvement. The generator then refines its initial explanation using both the feedback and suggestions from the critic. The method relies on prompt-based few-shot learning using demonstrations from the FiXer dataset, and works with several state-of-the-art open-source LLMs without requiring additional training data.

## Key Results
- CROSS-REFINE outperforms SELF-REFINE on commonsense QA, natural language inference, and fact-checking tasks
- Both feedback and suggestions from the critic are essential for effective refinement
- CROSS-REFINE works effectively with less powerful models, unlike SELF-REFINE which requires ChatGPT
- The method generates better explanations in German than SELF-REFINE, demonstrating multilingual effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-Refine improves explanation quality by cross-referencing feedback and suggestions from a separate critic model.
- Mechanism: The generator produces an initial explanation, the critic evaluates it and provides both feedback on errors and a suggested improved explanation, and the generator uses both to refine its output.
- Core assumption: A separate critic model can identify errors in explanations that the generator cannot self-detect.
- Evidence anchors:
  - [abstract] "Cross-Refine involves deploying a base LLM as the generator to generate an NLE and a second LLM as the critic"
  - [section 3] "The generator outputs the initial explanation, while the critic offers feedback and suggestions on it, which can be used by the generator to refine the initial explanation"
  - [corpus] Weak evidence - corpus contains related papers on NLE evaluation but not direct validation of cross-referencing mechanism
- Break condition: If the critic model lacks domain knowledge or hallucinates, it may provide incorrect feedback that degrades explanation quality rather than improving it.

### Mechanism 2
- Claim: Suggestions from the critic are as important as feedback for refining explanations.
- Mechanism: The critic provides not just error identification but also a complete alternative explanation that the generator can use as a template for improvement.
- Core assumption: The generator can effectively incorporate suggestions from the critic to produce better explanations than self-feedback alone.
- Evidence anchors:
  - [abstract] "Both of them play an important role in refining explanations" (referring to feedback and suggestions)
  - [section 6.3] "suggestions play an equally important role in the refinement of explanations"
  - [corpus] Weak evidence - corpus contains papers on NLE evaluation but not direct comparison of feedback vs suggestions importance
- Break condition: If the generator cannot effectively integrate the critic's suggestions, or if the suggestions are too dissimilar from the initial explanation, refinement may not occur.

### Mechanism 3
- Claim: Cross-Refine works effectively with less powerful models compared to self-refinement methods.
- Mechanism: By using a separate critic model, even smaller generators can benefit from external feedback rather than relying on their own limited self-assessment capabilities.
- Core assumption: External feedback provides more reliable guidance than self-feedback, especially for smaller models.
- Evidence anchors:
  - [abstract] "Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT"
  - [section 6] "CROSS-REFINE can easily outperform SELF-REFINE on ECQA and eSNLI" and "CROSS-REFINE works effectively with less powerful LLMs"
  - [corpus] Weak evidence - corpus contains related papers but not direct validation of performance across model sizes
- Break condition: If the critic model is also small and lacks knowledge, the cross-referencing may not provide sufficient improvement over self-refinement.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Cross-Refine uses CoT prompting to generate initial explanations, which helps models reason step-by-step before producing explanations
  - Quick check question: What is the primary benefit of using CoT prompting in Cross-Refine's initial explanation generation?

- Concept: In-context learning
  - Why needed here: Cross-Refine relies on demonstrations (FiXer dataset) for in-context learning to guide both critic and generator behavior
  - Quick check question: How does the FiXer dataset support Cross-Refine's few-shot prompting approach?

- Concept: Evaluation metrics for explanations
  - Why needed here: Cross-Refine's effectiveness is validated using multiple metrics (BLEURT, BARTScore, TIGERScore) and human evaluation
  - Quick check question: Why does Cross-Refine use both automated and human evaluation for assessing explanation quality?

## Architecture Onboarding

- Component map:
  Generator LLM -> Initial explanation -> Critic LLM -> Feedback + Suggestion -> Generator LLM -> Refined explanation -> Evaluation pipeline

- Critical path:
  1. Generate initial explanation using CoT prompting
  2. Critic evaluates and provides feedback + suggestion
  3. Generator refines explanation using both inputs
  4. Evaluate refined explanation using metrics
  5. (Optional) Human evaluation for subjective assessment

- Design tradeoffs:
  - Using same model for both roles vs different models (affects performance and resource usage)
  - Number of demonstrations in prompts (affects context length and performance)
  - Model size choices (affects cost, inference time, and effectiveness)
  - Evaluation metrics selection (automated vs human, task-specific vs general)

- Failure signatures:
  - Low BLEURT/BARTScore improvements indicate refinement isn't working
  - High TIGERScore penalties suggest explanations contain errors
  - Poor human evaluation scores indicate explanations lack faithfulness or coherence
  - If refined explanations are similar to initial ones, cross-referencing isn't effective

- First 3 experiments:
  1. Run Cross-Refine vs Self-Refine on ECQA with Qwen2-7B to verify basic effectiveness
  2. Test same-model vs different-model combinations on HealthFC to assess domain knowledge requirements
  3. Conduct ablation study (feedback only vs suggestions only) on eSNLI to quantify component contributions

## Open Questions the Paper Calls Out
- How does Cross-Refine perform when using human-crafted feedback and suggestions compared to LLM-generated ones?
- What is the extent to which Cross-Refine can address specific types of errors in explanations?
- How does the interpretation of quality metrics (faithfulness, insightfulness) impact the results of human evaluations?
- How would Cross-Refine perform with LLMs specifically trained on medical data for the HealthFC task?
- What is the optimal number and selection of demonstrations for few-shot prompting in Cross-Refine?

## Limitations
- Performance degradation on domain-specific tasks like medical fact-checking when using general-purpose models
- Language generation issues where models default to English even when prompted for German responses
- Heavy reliance on prompt-based few-shot learning that may not generalize well to new tasks
- Increased computational costs and inference time compared to single-model approaches

## Confidence
**High Confidence**: The core mechanism of using separate generator and critic models with feedback and suggestions is well-supported by experimental results showing consistent improvements over Self-Refine across multiple tasks and datasets.

**Medium Confidence**: The claim that Cross-Refine works effectively with less powerful models is supported by comparisons with ChatGPT, but the experimental setup doesn't test the absolute lower bound of model sizes that can still benefit from this approach.

**Low Confidence**: The assertion that suggestions are as important as feedback for refinement is based on ablation studies, but the paper doesn't explore whether this relationship holds across all task types or model combinations.

## Next Checks
1. Test Cross-Refine's performance on a highly specialized domain (e.g., legal reasoning or advanced mathematics) to validate whether the critic-generator dynamic can compensate for domain knowledge gaps.

2. Conduct experiments varying the number of refinement iterations beyond the single refinement step used in the paper to determine if additional iterations provide further improvements or lead to diminishing returns.

3. Compare Cross-Refine against other explanation refinement methods like CoT-based approaches or ensemble methods to establish whether the cross-referencing mechanism provides unique advantages over alternative architectures.