---
ver: rpa2
title: Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback
arxiv_id: '2404.10776'
source_url: https://arxiv.org/abs/2404.10776
tags:
- adversarial
- bandits
- feedback
- dueling
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles contextual dueling bandits in the presence of
  adversarial feedback, where an adversary can flip the true preference labels. The
  key challenge is to design an algorithm robust to such adversarial attacks while
  maintaining strong theoretical guarantees.
---

# Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback

## Quick Facts
- **arXiv ID**: 2404.10776
- **Source URL**: https://arxiv.org/abs/2404.10776
- **Reference count**: 40
- **Primary result**: RCDB achieves O(d√T/κ + dC/κ) regret bound, which is nearly optimal for contextual dueling bandits with adversarial feedback

## Executive Summary
This paper addresses the challenge of contextual dueling bandits in the presence of adversarial feedback, where an adversary can corrupt preference labels. The authors propose Robust Contextual Dueling Bandits (RCDB), which uses an uncertainty-weighted maximum likelihood estimator to assign lower weights to potentially corrupted feedback. The algorithm achieves near-optimal regret bounds while being robust to adversarial attacks. For the specific case of sigmoid link functions, they further improve the regret bound by incorporating local derivatives into the analysis, eliminating the κ dependence in the leading term.

## Method Summary
The RCDB algorithm tackles contextual dueling bandits with adversarial feedback by using an uncertainty-weighted maximum likelihood estimator. The key innovation is the weight formula w_i = min{1, α/∥ϕ_i∥Σ_i^{-1}}, which reduces the influence of feedback with high uncertainty that is more likely to be adversarial. The algorithm maintains a covariance matrix and solves a weighted MLE equation to estimate preference probabilities. For action selection, it uses an upper confidence bound approach with an exploration bonus. The method is extended to handle unknown numbers of adversarial feedback by using an optimistic estimator of C. For sigmoid link functions specifically, the algorithm incorporates local derivative estimates to further improve the regret bound.

## Key Results
- RCDB achieves O(d√T/κ + dC/κ) regret bound, which is nearly optimal and matches the lower bound
- The algorithm extends to unknown C with a two-case bound: optimal when C is small, O(T) when C is large
- For sigmoid link functions, RCDB-S improves the bound to O(dB^(3/2)√T + dBC/κ + d²B²(1/κ² + B/κ)), eliminating κ dependence in the leading term
- Experiments show RCDB outperforms state-of-the-art algorithms under various adversarial attack patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The uncertainty-weighted maximum likelihood estimator assigns lower weights to potentially corrupted feedback.
- **Mechanism**: The algorithm computes weights as `wi = min{1, α/∥ϕi∥Σ−1i}` where the norm of the feature difference is inversely proportional to the weight. This reduces the influence of feedback with high uncertainty, which is more likely to be adversarial.
- **Core assumption**: The adversarial feedback introduces higher uncertainty in the estimated preference probability.
- **Evidence anchors**:
  - [abstract]: "uncertainty-weighted maximum likelihood estimator" and "assigns lower weights to feedback with higher uncertainty"
  - [section 4]: Detailed derivation showing how variance of estimated probability relates to uncertainty and weight assignment
  - [corpus]: No direct evidence found in corpus neighbors
- **Break condition**: If adversarial feedback coincidentally has low uncertainty (e.g., occurs when actions are very different), the weight reduction may be insufficient.

### Mechanism 2
- **Claim**: The algorithm maintains near-optimal regret bounds even when the number of adversarial feedback C is unknown.
- **Mechanism**: Uses an optimistic estimator of C with a threshold parameter. When C is smaller than the threshold, it maintains the optimal bound; when C is larger, it degrades gracefully to O(T).
- **Core assumption**: The algorithm can bound regret using an overestimate of C without significant performance loss.
- **Evidence anchors**:
  - [abstract]: "extend our previous result to address the challenge posed by an unknown number of adversarial feedback C"
  - [section 5.2]: Theorem 5.5 showing the two-case regret bound depending on whether C exceeds the threshold
  - [corpus]: No direct evidence found in corpus neighbors
- **Break condition**: If C is significantly larger than the threshold, the regret bound degrades to O(T), losing the near-optimal guarantee.

### Mechanism 3
- **Claim**: For sigmoid link functions, using local derivatives instead of uniform lower bounds eliminates κ dependence in the leading term.
- **Mechanism**: The refined algorithm estimates local derivatives ˙σ(b∆i) and uses them as weights in the covariance matrix, exploiting the structure of the sigmoid function where derivatives vary significantly near zero.
- **Core assumption**: The sigmoid link function's derivative varies enough to justify using local estimates instead of uniform bounds.
- **Evidence anchors**:
  - [abstract]: "eliminate the κ dependence in the leading term with respect to T" and "local derivatives in maximum likelihood estimation"
  - [section 6.1]: Detailed derivation showing how local derivatives improve the bound
  - [corpus]: No direct evidence found in corpus neighbors
- **Break condition**: If the local derivative estimation is inaccurate or if actions are far from optimal (where local derivative ≈ κ), the improvement may not materialize.

## Foundational Learning

- **Concept**: Contextual dueling bandits with adversarial feedback
  - Why needed here: This is the problem setting being solved - understanding the difference between regular contextual bandits and dueling bandits, and how adversarial feedback corrupts preference labels
  - Quick check question: In contextual dueling bandits, what does the agent observe in each round instead of a single reward?

- **Concept**: Maximum likelihood estimation with uncertainty weighting
  - Why needed here: The core algorithmic technique - understanding how weighted MLE differs from standard MLE and why uncertainty-based weights help with robustness
  - Quick check question: How does the weight formula wi = min{1, α/∥ϕi∥Σ−1i} relate to the uncertainty of the feedback?

- **Concept**: Confidence bounds and concentration inequalities for bandit algorithms
  - Why needed here: The theoretical analysis relies on establishing high-probability bounds on estimation errors using techniques like Azuma-Hoeffding and self-normalized concentration
  - Quick check question: What is the key difference between the confidence bounds used for this algorithm versus standard linear bandits?

## Architecture Onboarding

- **Component map**: Context → Action selection → Feedback observation → Weight computation → MLE solver → Covariance update → Confidence radius computation → Next round
- **Critical path**: Context → Action selection → Feedback observation → Weight update → MLE update → Next round
- **Design tradeoffs**: 
  - Using local derivatives (RCDB-S) vs uniform bounds (RCDB) - better theoretical bounds but more complex computation
  - Uncertainty weighting vs standard weighting - improved robustness but requires norm computations
  - Known vs unknown C - simpler algorithm vs more practical but with potential degradation
- **Failure signatures**:
  - If weights become too small, the algorithm may not learn effectively
  - If confidence radius is too large, exploration may be insufficient
  - If C is severely underestimated, adversarial feedback may dominate learning
- **First 3 experiments**:
  1. Test with no adversarial feedback to verify baseline performance matches standard contextual dueling bandit algorithms
  2. Test with known C using greedy adversarial attack to verify linear dependence on C in regret
  3. Test with unknown C using random adversarial attack to verify the two-case regret bound behavior

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the discussion and limitations, several important directions emerge:

## Limitations
- The extension to unknown C provides a two-case bound but lacks a unified bound and requires setting a threshold parameter
- The sigmoid-specific improvements rely on local derivative estimates that may be difficult to compute accurately
- Experiments only demonstrate performance on synthetic data with specific adversarial attack patterns

## Confidence
- **High confidence**: The core uncertainty-weighted MLE mechanism and its theoretical guarantees for known C
- **Medium confidence**: The extension to unknown C follows logically but introduces additional complexity
- **Medium confidence**: The sigmoid-specific improvements are theoretically sound but rely on assumptions about local derivative behavior

## Next Checks
1. Test RCDB with noisy preference observations to verify that uncertainty weighting effectively mitigates the impact of corrupted feedback
2. Implement RCDB on problems with high-dimensional contexts (d > 10) to assess computational feasibility of the weighted MLE solver
3. Apply RCDB to a recommendation system dataset with simulated adversarial feedback to evaluate practical performance compared to the theoretical bounds