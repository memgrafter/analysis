---
ver: rpa2
title: Can Large Language Models Generate High-quality Patent Claims?
arxiv_id: '2406.19465'
source_url: https://arxiv.org/abs/2406.19465
tags:
- patent
- claims
- claim
- generation
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) for generating
  patent claims from patent descriptions. The authors construct the first dataset
  for description-based claim generation and compare it with previous abstract-based
  methods.
---

# Can Large Language Models Generate High-quality Patent Claims?

## Quick Facts
- arXiv ID: 2406.19465
- Source URL: https://arxiv.org/abs/2406.19465
- Reference count: 22
- Primary result: GPT-4 generates the highest quality patent claims from descriptions, outperforming abstract-based methods

## Executive Summary
This paper presents the first comprehensive evaluation of large language models for generating patent claims from patent descriptions. The authors construct a novel dataset and compare description-based generation against previous abstract-based methods, finding that description-based approaches significantly outperform abstract-based methods in completeness of invention features. GPT-4 demonstrates superior performance in human evaluations by patent experts across multiple quality dimensions including feature coverage, conceptual clarity, and technical coherence, though all models struggle with generating subsequent dependent claims.

## Method Summary
The authors construct a novel dataset for description-based patent claim generation and compare it with previous abstract-based methods. They evaluate multiple LLMs including GPT-4, Claude-3-Sonnet, and Llama-3 through human evaluations by patent experts across multiple quality dimensions. The study also investigates the effects of fine-tuning and multi-task fine-tuning on claim generation quality, measuring performance across feature completeness, conceptual clarity, and technical coherence metrics.

## Key Results
- Description-based generation significantly outperforms abstract-based methods in completeness of invention features
- GPT-4 demonstrates best performance in human evaluations by patent experts
- All models struggle with generating subsequent dependent claims after the first independent claim
- Fine-tuning improves feature completeness and conceptual clarity, while multi-task fine-tuning reduces conceptual clarity

## Why This Works (Mechanism)
Description-based generation provides more comprehensive technical context than abstract-based methods, allowing LLMs to better understand the complete invention and generate more complete claims. The richer technical detail in patent descriptions enables better feature coverage and more accurate representation of the invention's scope. Patent experts can more effectively evaluate claims when they have access to the full technical disclosure rather than just the abstract summary.

## Foundational Learning

1. **Patent claim structure** - Understanding independent and dependent claim relationships
   - Why needed: Essential for evaluating claim generation quality and legal validity
   - Quick check: Can identify claim types and their hierarchical relationships

2. **Natural language processing for technical domains** - Specialized NLP techniques for patent language
   - Why needed: Patents use specific terminology and legal language
   - Quick check: Can parse complex technical sentences and legal terms

3. **Human evaluation methodology for technical content** - Structured expert assessment frameworks
   - Why needed: Automated metrics insufficient for legal quality assessment
   - Quick check: Consistent scoring across multiple evaluators

## Architecture Onboarding

**Component Map**: Patent Description -> LLM Input -> Claim Generation -> Human Evaluation -> Quality Assessment

**Critical Path**: Patent description preprocessing → LLM inference → Claim formatting → Expert evaluation

**Design Tradeoffs**: Description-based vs abstract-based generation; single-task vs multi-task fine-tuning; model size vs response quality

**Failure Signatures**: Incomplete feature coverage, loss of technical coherence, inability to generate dependent claims, overly broad or narrow claim scope

**First Experiments**: 1) Generate claims from patent abstracts vs full descriptions 2) Compare single-task vs multi-task fine-tuned models 3) Evaluate independent claim vs dependent claim generation quality

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation relies on limited pool of patent professionals, introducing potential subjectivity
- Dataset construction limited to specific patent corpus, potentially constraining generalizability
- Does not explore newer or specialized patent-focused LLM models that may have emerged

## Confidence
- Primary finding (description-based superior): High
- Model performance rankings: Medium
- Dependent claim generation limitation: High

## Next Checks
1. Replicate evaluation with larger and more diverse panel of patent experts to establish inter-rater reliability
2. Test methodology with newer LLM variants and specialized patent-focused models
3. Conduct longitudinal study comparing LLM-generated claims against actual patent examiner decisions