---
ver: rpa2
title: Approaching Human-Level Forecasting with Language Models
arxiv_id: '2402.18563'
source_url: https://arxiv.org/abs/2402.18563
tags:
- question
- system
- questions
- forecasting
- crowd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a language model (LM) system for automated
  event forecasting that nears human crowd performance on competitive forecasting
  platforms. The authors develop a retrieval-augmented pipeline that automatically
  searches for relevant news articles, generates forecasts, and aggregates predictions.
---

# Approaching Human-Level Forecasting with Language Models

## Quick Facts
- arXiv ID: 2402.18563
- Source URL: https://arxiv.org/abs/2402.18563
- Authors: Danny Halawi; Fred Zhang; Chen Yueh-Han; Jacob Steinhardt
- Reference count: 40
- Primary result: Automated LM system achieves Brier score of 0.179, approaching human crowd performance of 0.149 on competitive forecasting platform questions

## Executive Summary
This paper introduces a language model system for automated event forecasting that approaches human crowd performance on competitive forecasting platforms. The authors develop a retrieval-augmented pipeline that automatically searches for relevant news articles, generates forecasts, and aggregates predictions. To optimize the system, they curate a large dataset of questions from five forecasting platforms, focusing on binary questions published after June 2023 to avoid model training leakage. The system uses LM-generated search queries, relevance filtering, and summarization to provide up-to-date information to the reasoning component. The reasoning model is fine-tuned on self-supervised data where the model outperforms human crowds, and uses scratchpad prompts to elicit explanations alongside predictions. The system ensembles multiple forecasts and aggregates them using trimmed mean.

## Method Summary
The system combines retrieval-augmented LM reasoning with selective forecasting based on model strengths. It uses LM-generated search queries to retrieve relevant news articles, filters and summarizes them, then prompts LMs with scratchpad reasoning instructions. The model ensemble and trimmed mean aggregation combine multiple forecasts. Selective forecasting allows the system to only predict when conditions favor its strengths (e.g. early retrieval dates, many relevant articles, or when crowd uncertainty is high). The reasoning model is fine-tuned on self-supervised data where it outperforms human crowds, teaching the model which reasoning patterns lead to accurate predictions.

## Key Results
- System achieves Brier score of 0.179 on test set, approaching human crowd score of 0.149
- Significantly outperforms crowd when crowd's predictions express high uncertainty
- Nears performance of crowd when there are at least 5 relevant articles
- Surpasses human performance in certain settings through selective forecasting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system nears human-level forecasting by combining retrieval-augmented LM reasoning with selective forecasting based on model strengths.
- Mechanism: The system uses LM-generated search queries to retrieve relevant news articles, filters and summarizes them, then prompts LMs with scratchpad reasoning instructions. The model ensemble and trimmed mean aggregation combine multiple forecasts. Selective forecasting allows the system to only predict when conditions favor its strengths (e.g. early retrieval dates, many relevant articles, or when crowd uncertainty is high).
- Core assumption: LM-generated queries and relevance filtering can identify and present articles that improve reasoning, and the model can learn to recognize when it is more likely to outperform the crowd.
- Evidence anchors:
  - [abstract] "Under a test set published after June 2023... the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it."
  - [section] "Our system significantly outperforms the crowd when the crowd's predictions express high uncertainty... Our system nears the performance of the crowd when there are at least 5 relevant articles."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.517, average citations=0.0. Top related titles: Reasoning and Tools for Human-Level Forecasting, Consistency Checks for Language Model Forecasters, Shall We Play a Game? Language Models for Open-ended Wargames.
- Break condition: If the retrieval system fails to identify relevant articles or the model cannot learn to recognize its strengths, the selective forecasting advantage disappears and performance drops to baseline levels.

### Mechanism 2
- Claim: Fine-tuning on self-supervised data where the model outperforms the crowd improves reasoning capability.
- Mechanism: The system generates multiple candidate forecasts per question using different configurations. It selects only those forecasts that beat the crowd and have predictions within 0.15 of the crowd prediction, then fine-tunes a new model on these strong examples. This teaches the model which reasoning patterns lead to accurate predictions.
- Core assumption: There is a signal in the LM's outputs that correlates with better-than-crowd performance, and this signal can be captured through self-supervised fine-tuning.
- Evidence anchors:
  - [abstract] "We propose and apply a self-supervised fine-tuning method to improve LM's capability in reasoning about forecasting tasks."
  - [section] "To select the data, we only keep outputs that give a lower Brier score than the crowd's... The resulting fine-tuning data has the following structure..."
  - [corpus] Weak - corpus neighbors show related work on LLM forecasters but don't directly address this self-supervised fine-tuning approach.
- Break condition: If the selection criteria don't actually identify genuinely better forecasts, or if the model overfits to the training distribution, the fine-tuning won't improve performance and may even harm it.

### Mechanism 3
- Claim: Ensembling multiple forecasts through trimmed mean aggregation outperforms individual models and reduces calibration error.
- Mechanism: The system elicits forecasts from both the base model (with scratchpad prompts) and the fine-tuned model (without instructions). It creates 6 forecasts total (3 from each) and aggregates them using trimmed mean, which reduces the weight of outliers. This ensemble is more calibrated than individual models.
- Core assumption: Different model instantiations and prompting strategies capture complementary information, and trimmed mean effectively combines this information while reducing the impact of poor forecasts.
- Evidence anchors:
  - [abstract] "The system ensembles multiple forecasts and aggregates them using trimmed mean."
  - [section] "We implement 5 ensembling methods... Trimmed mean performs the best in our evaluation."
  - [corpus] Weak - corpus neighbors show related work on LLM forecasters but don't specifically address trimmed mean ensembling for forecasting.
- Break condition: If the forecasts from different models are highly correlated or if one model consistently dominates, the ensemble won't provide much benefit over the best individual model.

## Foundational Learning

- Concept: Brier score as proper scoring rule for probabilistic forecasts
  - Why needed here: The system evaluates and optimizes forecasting performance using Brier score, which properly incentivizes calibrated probability estimates rather than just correct classifications
  - Quick check question: If a forecaster believes an event has 30% probability, what probability should they report to maximize their expected Brier score?

- Concept: Calibration and its relationship to forecast accuracy
  - Why needed here: The system aims for well-calibrated forecasts (where predicted probabilities match observed frequencies), which is essential for reliable decision-making
  - Quick check question: If a model predicts 70% probability for 100 events and 70 of them occur, is the model well-calibrated?

- Concept: Retrieval-augmented generation and its application to forecasting
  - Why needed here: The system uses news retrieval to provide up-to-date information for reasoning about future events, similar to how humans gather information before making forecasts
  - Quick check question: What are the key differences between using retrieval for question answering versus forecasting tasks?

## Architecture Onboarding

- Component map: Retrieval pipeline (query generation → news retrieval → relevance filtering → summarization) → Reasoning pipeline (scratchpad prompts → forecast generation → ensemble aggregation) → Fine-tuning creates specialized reasoning model from self-supervised data → Selective forecasting applies heuristics based on crowd uncertainty, retrieval date, and article count
- Critical path: For each question and retrieval date: generate search queries → retrieve articles → filter and summarize → prompt reasoning models → ensemble forecasts → apply selective forecasting criteria → output final prediction
- Design tradeoffs: The system trades computational cost and latency for improved accuracy through multiple model invocations, extensive retrieval, and ensembling. Selective forecasting reduces cost but may miss some questions where the model could have performed well.
- Failure signatures: Poor performance indicates either retrieval failures (irrelevant articles), reasoning failures (model can't process information effectively), or calibration issues (predictions systematically biased). The system provides intermediate outputs (article summaries, scratchpad reasoning) to diagnose where failures occur.
- First 3 experiments:
  1. Run the retrieval pipeline with different query generation prompts and evaluate article relevance scores to optimize the retrieval component in isolation
  2. Test the reasoning pipeline with base models and different scratchpad prompts on a small validation set to identify the best reasoning approach
  3. Implement the full system end-to-end on a subset of validation data to verify that improvements from individual components combine effectively

## Open Questions the Paper Calls Out

- Can language models improve their forecasting accuracy through iterative self-supervised fine-tuning without access to future events?
  - Basis in paper: Explicit - The paper suggests this as a future research direction, noting that "With a larger training corpus, our self-supervised fine-tuning approach can be used for iterative self-improvement."
  - Why unresolved: The paper only proposes this approach and does not implement or evaluate it. The effectiveness of such an iterative process remains untested.
  - What evidence would resolve it: Experiments demonstrating improved forecasting performance across multiple rounds of self-supervised fine-tuning on a large dataset of historical forecasting questions, while maintaining no access to future events.

## Limitations

- System performance advantage is conditional on specific settings (high crowd uncertainty, sufficient relevant articles, early retrieval dates)
- Selective forecasting mechanism may miss questions where model could have performed well
- Performance is bounded by quality and timeliness of available news information sources
- Evaluation focuses on binary questions from specific platforms, limiting generalizability

## Confidence

- High confidence: The system's overall approach of retrieval-augmented reasoning with ensemble aggregation is sound and well-supported by the results. The comparison against human crowd performance on held-out test data provides robust evidence of the system's capabilities.
- Medium confidence: The selective forecasting mechanism's effectiveness is demonstrated but may be overfit to the specific dataset characteristics. The self-supervised fine-tuning approach shows promise but the selection criteria (beating crowd performance by specific margins) may not generalize well to different forecasting domains.
- Low confidence: The exact prompt engineering details and hyperparameters are not fully specified, making it difficult to reproduce the precise performance reported. The system's performance on zero-shot forecasting tasks is notably poor, suggesting heavy dependence on the retrieval and fine-tuning components.

## Next Checks

1. **Retrieval quality validation**: Test the retrieval pipeline with different query generation prompts and measure article relevance scores on a held-out validation set. This isolates whether the retrieval component effectively identifies useful information or if improvements are coming primarily from the reasoning component.

2. **Selective forecasting calibration**: Evaluate the selective forecasting criteria on a diverse set of questions not used in training. Test whether the model's self-assessment of its strengths (crowd uncertainty, article count, retrieval date) accurately predicts when it will outperform the crowd, or if these criteria are overfit to the specific dataset.

3. **Cross-platform generalization**: Apply the fine-tuned reasoning model to forecasting questions from platforms not included in the training data, particularly those with different resolution criteria or time horizons. This tests whether the self-supervised learning captured general forecasting reasoning patterns or merely specialized to the training platforms' characteristics.