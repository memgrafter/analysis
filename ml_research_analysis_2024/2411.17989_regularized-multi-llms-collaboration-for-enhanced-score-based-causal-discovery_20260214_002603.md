---
ver: rpa2
title: Regularized Multi-LLMs Collaboration for Enhanced Score-based Causal Discovery
arxiv_id: '2411.17989'
source_url: https://arxiv.org/abs/2411.17989
tags:
- causal
- llms
- discovery
- knowledge
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework that integrates multiple large
  language models (LLMs) to enhance score-based causal discovery methods. The key
  idea is to use LLM-derived knowledge as an alternative to costly expert knowledge,
  combining information from multiple LLMs using weighted sums to improve reliability.
---

# Regularized Multi-LLMs Collaboration for Enhanced Score-based Causal Discovery

## Quick Facts
- arXiv ID: 2411.17989
- Source URL: https://arxiv.org/abs/2411.17989
- Authors: Xiaoxuan Li; Yao Liu; Ruoyu Wang; Lina Yao
- Reference count: 21
- Primary result: LLM-enhanced versions of GES, NOTEARS, and KCRL algorithms outperform original versions on LUCAS, Asia, Earthquake, Child, and SACHS datasets in terms of SHD, TPR, and FDR metrics.

## Executive Summary
This paper proposes a framework that integrates multiple large language models (LLMs) to enhance score-based causal discovery methods. The key innovation is using LLM-derived knowledge as an alternative to costly expert knowledge, combining information from multiple LLMs using weighted sums to improve reliability. The framework introduces a penalty term based on the difference between estimated and LLM-generated causal graphs, which is incorporated into existing score functions. Experiments show that LLM-enhanced versions of popular causal discovery algorithms achieve better performance across multiple metrics compared to their original versions.

## Method Summary
The framework integrates multiple LLMs by first generating causal graph predictions from different LLMs using a 3-stage prompt approach. These predictions are combined using weighted sums where weights are derived from normalized score function evaluations. The method introduces penalty terms (L1 for decomposable scores like BIC, L2 for differentiable continuous optimization like NOTEARS) that capture the difference between estimated and LLM-generated causal graphs. These penalties are incorporated into existing score functions of algorithms like GES, NOTEARS, and KCRL, effectively guiding the search process with supplementary causal knowledge.

## Key Results
- LLM-enhanced KCRL achieves FDR=0 on LUCAS, Asia, and Earthquake datasets
- On Asia dataset, TPR increases to 1 with LLM enhancement, indicating perfect edge detection
- The framework consistently improves SHD, TPR, and FDR metrics across all tested datasets compared to original algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple LLM results combined via weighted sum improve reliability over single LLM predictions.
- Mechanism: Individual LLMs provide noisy but partially correct causal graph predictions; aggregating them with weights derived from normalized score functions reduces variance and fills in missing correct edges.
- Core assumption: Different LLMs trained on similar corpora capture complementary causal knowledge; weights based on normalized score functions reflect true predictive quality.
- Evidence anchors:
  - [abstract] "combine information from multiple LLMs using weighted sums to improve reliability"
  - [section] "We normalize them to ensure their summation equals one, obtaining a weight µ for each LLM model"
- Break condition: If LLM outputs are highly correlated errors or if weights poorly reflect actual quality, aggregation fails.

### Mechanism 2
- Claim: L1 and L2 penalty terms added to score functions preserve decomposability or differentiability needed for optimization.
- Mechanism: L1-penalty is additive across nodes, matching decomposable score functions like BIC in GES; L2-penalty is differentiable, enabling gradient-based updates in continuous optimization methods like NOTEARS.
- Core assumption: Original score functions are decomposable (GES) or continuous (NOTEARS), and penalty terms align mathematically with these structures.
- Evidence anchors:
  - [section] "we prove that both the penalties are decomposable, therefore, we can impose our LLM result penalty into GES"
  - [section] "the derivative of the l2-penalty is: ∇Pl2(M) = Xmodel −2 × µmodel(M − Mmodel)"
- Break condition: If the penalty term introduces non-decomposability or non-differentiability incompatible with the optimization method, the framework fails.

### Mechanism 3
- Claim: LLM-enhanced methods improve FDR, TPR, and SHD metrics by integrating supplementary causal knowledge.
- Mechanism: LLM-derived prior knowledge guides the search away from incorrect edges and toward true causal relationships, effectively constraining the hypothesis space.
- Core assumption: LLM knowledge, even if imperfect, contains more signal than noise relative to the prior distribution of possible causal graphs.
- Evidence anchors:
  - [abstract] "experiments... show that the LLM-enhanced versions... outperform their original versions in terms of SHD, TPR, and FDR metrics"
  - [section] "On the Asia dataset, the TPR increases to 1, indicating that the algorithm can predict all the correct edges in the ground truth graph with the help of LLM"
- Break condition: If LLM knowledge is systematically wrong or misleading, metrics degrade.

## Foundational Learning

- Concept: Bayesian Information Criterion (BIC) and its role in score-based causal discovery
  - Why needed here: BIC is the default score function for GES and NOTEARS, and the paper shows how to augment it with LLM penalties.
  - Quick check question: What is the formula for BIC, and why does it balance likelihood with model complexity?

- Concept: Markov equivalence classes and DAG acyclicity constraints
  - Why needed here: Causal graphs must be acyclic, and score-based methods search over equivalence classes; understanding this helps reason about the penalty integration.
  - Quick check question: How does the acyclicity constraint manifest in continuous form in NOTEARS, and why is it important?

- Concept: Conditional independence testing and constraint-based causal discovery
  - Why needed here: Provides context for why score-based methods are used and how prior knowledge can substitute for conditional independence tests.
  - Quick check question: How do constraint-based methods differ from score-based ones, and what are the trade-offs?

## Architecture Onboarding

- Component map: LLM Query Engine -> LLMs (GPT-3.5, GPT-4, Gemini) -> Weight Assignment (normalized score-based ranking) -> Penalty Calculation (L1 or L2) -> Score Function Augmentation -> Causal Discovery Algorithm (GES/NOTEARS/KCRL) -> Evaluation Metrics (SHD, TPR, FDR)
- Critical path: Prompt -> LLM outputs -> weight normalization -> penalty term computation -> score function update -> algorithm execution -> evaluation
- Design tradeoffs:
  - Single vs. multiple LLMs: More LLMs improve coverage but increase cost and complexity of weight aggregation.
  - L1 vs. L2 penalty: L1 is decomposable for GES, L2 is differentiable for NOTEARS; choice depends on algorithm compatibility.
  - Penalty weight λ: Controls influence of LLM knowledge; too high causes overfitting to LLM noise, too low provides no benefit.
- Failure signatures:
  - Degraded metrics (higher SHD, lower TPR, higher FDR) when λ is mis-tuned.
  - Algorithm convergence failure if penalty term violates decomposability or differentiability.
  - Poor LLM outputs leading to misleading penalties.
- First 3 experiments:
  1. Run GES on LUCAS dataset without LLM enhancement; record baseline SHD, TPR, FDR.
  2. Run GES with LLM enhancement using L1 penalty; compare metrics to baseline.
  3. Run NOTEARS on Asia dataset with L2 penalty; verify metric improvements and check gradient stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the quality of LLM-derived causal relationships be automatically assessed and validated without relying on ground truth?
- Basis in paper: [inferred] The paper discusses challenges with LLM result quality and mentions the need for "exploring more sophisticated methods for integrating this information" but doesn't address validation mechanisms
- Why unresolved: The paper acknowledges LLM outputs vary in quality but doesn't propose systematic validation approaches
- What evidence would resolve it: Development and testing of automated validation metrics that correlate with actual causal discovery accuracy

### Open Question 2
- Question: What is the optimal strategy for combining multiple LLM outputs beyond simple weighted averaging?
- Basis in paper: [explicit] The paper mentions that "exploring more sophisticated methods for integrating this information could yield superior results" but only implements weighted sum
- Why unresolved: The paper only uses weighted averaging despite noting it as a limitation
- What evidence would resolve it: Comparative experiments showing performance gains from alternative combination strategies (ensemble methods, meta-learning approaches, etc.)

### Open Question 3
- Question: How does the confidence parameter λ affect the trade-off between LLM-derived and data-driven causal relationships across different types of causal structures?
- Basis in paper: [explicit] The paper notes "tuning this parameter remains a complex task" and doesn't provide systematic analysis of its impact
- Why unresolved: The paper acknowledges the complexity of parameter tuning but doesn't provide systematic analysis or guidelines
- What evidence would resolve it: Empirical studies mapping λ values to performance across different dataset characteristics and causal structures

## Limitations
- Framework performance depends heavily on LLM output quality, which is not systematically quantified across datasets
- Weight assignment relies on normalized score functions that may not accurately reflect true LLM predictive quality
- Limited testing across diverse real-world domains beyond the five benchmark datasets

## Confidence
- **High**: The mathematical formulation of L1/L2 penalties and their compatibility with GES/NOTEARS optimization
- **Medium**: The weighted aggregation approach for multiple LLM outputs, assuming diverse LLM training
- **Low**: Generalization to real-world datasets with complex causal structures and noise

## Next Checks
1. Conduct ablation studies removing individual LLMs to quantify their marginal contribution to metric improvements
2. Test the framework on datasets with known challenging causal structures (latent confounders, selection bias)
3. Evaluate performance sensitivity to penalty weight λ across a broader range of values to identify optimal tuning procedures