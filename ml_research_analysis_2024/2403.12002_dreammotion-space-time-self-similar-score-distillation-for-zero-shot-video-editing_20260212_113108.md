---
ver: rpa2
title: 'DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot Video
  Editing'
arxiv_id: '2403.12002'
source_url: https://arxiv.org/abs/2403.12002
tags:
- video
- diffusion
- arxiv
- motion
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DreamMotion introduces a novel zero-shot video editing framework
  that leverages score distillation sampling (SDS) to edit videos while preserving
  their original structure and motion. Unlike traditional approaches that rely on
  reverse diffusion starting from noise, DreamMotion begins with an input video that
  already exhibits natural motion and uses Delta Denoising Score (DDS) gradients to
  progressively modify the video's appearance based on a target text prompt.
---

# DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot Video Editing

## Quick Facts
- arXiv ID: 2403.12002
- Source URL: https://arxiv.org/abs/2403.12002
- Authors: Hyeonho Jeong; Jinho Chang; Geon Yeong Park; Jong Chul Ye
- Reference count: 40
- Key outcome: Achieves 0.8209 text alignment, 0.9726 frame consistency, and 0.9259 motion fidelity on Zeroscope, outperforming baselines

## Executive Summary
DreamMotion introduces a novel zero-shot video editing framework that leverages score distillation sampling (SDS) to edit videos while preserving their original structure and motion. Unlike traditional approaches that rely on reverse diffusion starting from noise, DreamMotion begins with an input video that already exhibits natural motion and uses Delta Denoising Score (DDS) gradients to progressively modify the video's appearance based on a target text prompt. To prevent structural and motion deviations caused by accumulated errors during optimization, the method employs space-time self-similarity regularization: spatial self-similarity matching preserves object structures across frames, while temporal self-similarity matching ensures smooth motion and prevents flickering artifacts. The approach is model-agnostic and works with both cascaded and non-cascaded video diffusion frameworks.

## Method Summary
DreamMotion optimizes an input video using score distillation sampling with Delta Denoising Score gradients, initializing with the original video to preserve natural motion. The method applies three losses: LV-DDS for appearance injection via delta denoising score, LS-SSM for spatial self-similarity matching to preserve structures, and LT-SSM for temporal self-similarity matching to ensure smooth motion. Space-time self-similarity regularization uses diffusion features from attention keys to compute self-similarity maps between target and reference videos. The optimization runs for 200 steps with SGD at learning rate 0.4, using timestep sampling from U(0.05, 0.95). Binary masks enable selective editing by filtering DDS gradients, preventing unwanted changes to preserved regions.

## Key Results
- Achieves 0.8209 text alignment on Zeroscope, outperforming baselines
- Maintains 0.9726 frame consistency across edited video sequences
- Preserves motion with 0.9259 motion fidelity score
- Human evaluations show superiority: 4.14/5 edit accuracy, 4.21/5 frame consistency, 4.33/5 structure/motion preservation

## Why This Works (Mechanism)

### Mechanism 1
Video score distillation with Delta Denoising Score (DDS) gradients enables effective appearance injection while avoiding blurriness and over-saturation. The method initializes with an input video containing natural motion, then progressively modifies the video's appearance by minimizing the difference between the noisy score of the target video (under text prompt y) and the noisy score of a reference video (under reference text ˆy). This delta formulation cancels out common denoising errors, leading to cleaner gradients.

### Mechanism 2
Spatial self-similarity regularization preserves object structures across frames by matching diffusion feature patterns between original and edited videos. During optimization, the method computes self-similarity maps from diffusion features of both the target and reference videos. By minimizing the difference between these maps, it ensures that structural relationships in the original video are maintained in the edited version.

### Mechanism 3
Temporal self-similarity regularization prevents flickering artifacts and ensures smooth motion by matching temporal correlation patterns between original and edited videos. The method computes temporal self-similarity from spatially marginalized diffusion features, then minimizes the difference between target and reference videos. This encourages the edited video to maintain the same frame-to-frame transitions as the original.

## Foundational Learning

- Concept: Score Distillation Sampling (SDS)
  - Why needed here: Provides a way to optimize videos using gradients from a pre-trained text-to-video diffusion model without running full reverse diffusion.
  - Quick check question: How does SDS differ from standard denoising diffusion training in terms of gradient direction?

- Concept: Delta Denoising Score (DDS)
  - Why needed here: Improves SDS by subtracting reference noisy scores to reduce blurriness and over-saturation during appearance injection.
  - Quick check question: What is the mathematical form of DDS loss and how does it cancel denoising errors?

- Concept: Self-similarity descriptors
  - Why needed here: Enable structural preservation by focusing on relative appearance relationships rather than absolute pixel values.
  - Quick check question: How is spatial self-similarity computed from diffusion features and why is this robust to texture variations?

## Architecture Onboarding

- Component map:
  Input video (ˆx1:N) and target text (y) -> Pre-trained video diffusion U-Net (ϵϕ) -> Mask generator -> Optimization loop with three losses (LV-DDS, LS-SSM, LT-SSM) -> Output: edited video (x1:N)

- Critical path:
  1. Initialize target video with input video
  2. For each optimization step:
     - Sample timestep t
     - Compute noisy scores for target and reference videos
     - Calculate LV-DDS loss with mask filtering
     - Extract diffusion features and compute spatial self-similarity
     - Calculate LS-SSM loss
     - Compute temporal self-similarity and calculate LT-SSM loss
     - Backpropagate combined loss and update video parameters
  3. Return optimized video

- Design tradeoffs:
  - Using SDS vs. full reverse diffusion: Faster optimization but limited to appearance changes that preserve input structure
  - Spatial vs. temporal self-similarity: Spatial preserves objects, temporal preserves motion; both needed for complete preservation
  - Mask conditioning: Prevents unwanted changes but requires accurate mask generation

- Failure signatures:
  - Motion distortion: Indicates insufficient temporal self-similarity regularization
  - Structural inconsistencies: Suggests spatial self-similarity matching is not effective
  - Over-saturation/blurriness: Points to inadequate mask filtering of DDS gradients

- First 3 experiments:
  1. Implement LV-DDS only optimization and verify appearance injection without structure preservation
  2. Add LS-SSM to confirm structural preservation while monitoring for motion artifacts
  3. Include LT-SSM and evaluate complete preservation of both structure and motion across frames

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DreamMotion scale with increasing video resolution and length beyond the tested 16 frames and standard resolutions? The paper tests on videos up to 16 frames and mentions computational efficiency but doesn't explore scaling limits. Experiments showing qualitative and quantitative results on videos with 30+ frames and 4K resolution, including computational cost analysis and any degradation in structural preservation, would resolve this.

### Open Question 2
What is the exact mechanism by which self-similarity regularization prevents motion deviation compared to other potential regularization methods? While the paper demonstrates effectiveness, it doesn't explain why self-similarity specifically works better than other potential regularizers like optical flow consistency or direct frame-to-frame matching. Comparative ablation studies showing DreamMotion performance against versions using alternative regularization methods across multiple metrics would resolve this.

### Open Question 3
How does DreamMotion's performance vary across different video content domains (e.g., human faces, natural scenes, abstract motion)? The paper tests on general video datasets but doesn't analyze performance across different content types or domains. Systematic testing on specialized datasets with domain-specific metrics and analysis of where the method succeeds or fails for different content types would resolve this.

## Limitations
- Performance depends heavily on accurate binary mask generation for selective editing regions
- Limited evaluation on video content beyond general scenes, lacking domain-specific analysis
- Optimization relies on fixed hyperparameters (learning rate 0.4, 200 steps) that may not generalize well

## Confidence
**High Confidence**: The core mechanism of using Delta Denoising Score gradients for appearance injection is well-supported by the mathematical formulation and quantitative results showing improved text alignment (0.8209) compared to baselines.

**Medium Confidence**: The space-time self-similarity regularization approach is theoretically sound and shows good quantitative results (0.9726 frame consistency, 0.9259 motion fidelity), but the effectiveness depends heavily on the quality of diffusion feature extraction and self-similarity computation, which are not fully detailed.

**Low Confidence**: The generalization claims to other video diffusion models beyond Zeroscope are not thoroughly validated. The cascaded pipeline extension lacks comprehensive evaluation, and the method's performance on diverse video types remains unexplored.

## Next Checks
1. **Mask Sensitivity Analysis**: Systematically evaluate DreamMotion's performance across varying mask quality levels (perfect masks, noisy masks, partial masks) to determine the robustness threshold and identify failure modes when mask generation is imperfect.

2. **Temporal Consistency Under Motion Stress**: Test the method on videos with rapid motion changes, camera movements, and occlusions to verify that temporal self-similarity regularization maintains frame-to-frame consistency in challenging scenarios beyond the controlled DAVIS/WebVid datasets.

3. **Cross-Model Generalization**: Implement DreamMotion with at least two additional video diffusion models (beyond Zeroscope and Show-1) and compare quantitative metrics across all models to validate the claimed model-agnostic nature and identify any architecture-specific limitations.