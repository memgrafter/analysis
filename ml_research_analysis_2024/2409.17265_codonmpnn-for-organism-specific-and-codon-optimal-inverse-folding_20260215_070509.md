---
ver: rpa2
title: CodonMPNN for Organism Specific and Codon Optimal Inverse Folding
arxiv_id: '2409.17265'
source_url: https://arxiv.org/abs/2409.17265
tags:
- codon
- sequences
- protein
- codonmpnn
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodonMPNN addresses the challenge of suboptimal codon sequences
  in protein engineering, which can lead to low expression yields in host organisms.
  The core method involves generating codon sequences conditioned on protein backbone
  structures and organism labels, using an autoregressive model architecture similar
  to ProteinMPNN but adapted for 64 codons instead of 20 amino acids.
---

# CodonMPNN for Organism Specific and Codon Optimal Inverse Folding

## Quick Facts
- arXiv ID: 2409.17265
- Source URL: https://arxiv.org/abs/2409.17265
- Reference count: 24
- CodonMPNN achieves 24.8% codon recovery rate vs 20.5% for ProteinMPNN baseline

## Executive Summary
CodonMPNN addresses the challenge of suboptimal codon sequences in protein engineering by generating organism-specific codon sequences conditioned on protein backbone structures and host organism labels. The model uses an autoregressive architecture similar to ProteinMPNN but adapted for 64 codons instead of 20 amino acids, incorporating taxonomic cluster embeddings to enable host-specific generation. Experiments show CodonMPNN outperforms baselines in codon recovery while maintaining structure prediction accuracy, and correctly predicts higher likelihoods for highly expressed codon sequences in 72.4% of tested cases.

## Method Summary
CodonMPNN is an autoregressive model that generates codon sequences from protein backbone structures and organism labels. It uses a 3-layer message passing neural network (MPNN) encoder to process 3D protein structures, with optional taxon conditioning through balanced tree partitioning of the NCBI taxonomy. The decoder generates sequences codon-by-codon, producing a probability distribution over 64 possible codons. The model is trained on naturally occurring protein sequences with taxonomic information, enabling it to learn patterns of codon usage that correlate with expression fitness in specific organisms.

## Key Results
- Codon recovery rate of 24.8% compared to 20.5% for ProteinMPNN baseline
- TM-Score of 0.897 for generated structures, maintaining ProteinMPNN's structural accuracy
- Correctly predicts higher likelihoods for highly expressed codon sequences in 72.4% of synonymous mutation cases tested

## Why This Works (Mechanism)

### Mechanism 1
CodonMPNN learns to generate codon sequences closer to natural codon optimality than heuristic methods. By training on naturally occurring protein codon sequences, the model implicitly learns the distribution of codon usage that corresponds to high expression yields in specific organisms. The core assumption is that naturally occurring DNA sequences are closer to codon optimality than random codon sequences encoding the same protein.

### Mechanism 2
Taxon conditioning allows CodonMPNN to generate organism-specific codon sequences. The model incorporates taxonomic cluster embeddings that represent host organism characteristics, enabling it to condition codon generation on the cellular environment. Different taxonomic clusters have sufficiently distinct codon usage patterns to be learned by the model.

### Mechanism 3
CodonMPNN assigns higher likelihoods to highly expressed synonymous codon sequences. The autoregressive architecture learns the probability distribution of codon sequences, which correlates with expression fitness due to training on naturally occurring sequences. Expression fitness correlates with the likelihood under the model's learned distribution.

## Foundational Learning

- Concept: Autoregressive sequence modeling
  - Why needed here: CodonMPNN generates sequences one codon at a time, conditioning each prediction on previously generated codons and the protein structure
  - Quick check question: How does the autoregressive factorization p(s) = Π p(s_σ(i) | s_σ(<i), x; σ) enable generation of variable-length sequences?

- Concept: Message passing neural networks for structure encoding
  - Why needed here: The encoder must convert 3D protein backbone coordinates into meaningful embeddings that guide codon selection based on structural features
  - Quick check question: Why does the model use distances between alpha carbons rather than all atom coordinates for edge features?

- Concept: Taxonomic tree partitioning for balanced clustering
  - Why needed here: The algorithm creates clusters that preserve hierarchical information while maintaining approximately equal sizes for effective conditioning
  - Quick check question: How does the balanced tree partitioning algorithm avoid creating clusters that are too large (mixing different expression environments) or too small (insufficient training data)?

## Architecture Onboarding

- Component map: Encoder (3 MPNN layers) → Taxon embeddings (optional) → Decoder (3 MPNN layers with autoregressive masking) → Codon probability distribution
- Critical path: Structure input → Encoder → Taxon conditioning → Decoder → Codon output
- Design tradeoffs: Using 64 codons vs 20 amino acids increases vocabulary size but captures organism-specific optimization; taxon conditioning adds complexity but enables host-specific generation
- Failure signatures: Poor recovery rates (model not learning codon patterns), collapsed taxon clusters (taxonomy partitioning not working), loss divergence (training instability)
- First 3 experiments:
  1. Verify recovery rates on held-out test set match ProteinMPNN baseline
  2. Test taxon conditioning by generating sequences with and without taxon labels and measuring expression fitness correlation
  3. Validate balanced tree partitioning by checking cluster size distribution and taxonomic diversity within clusters

## Open Questions the Paper Calls Out

### Open Question 1
Could fine-tuning protein language models to generate codon sequences conditioned on amino acid sequences and taxon labels provide better performance than CodonMPNN for codon optimization tasks without available structures? The authors propose this as future work, suggesting uncertainty about whether this alternative approach would outperform CodonMPNN.

## Limitations
- The core assumption that naturally occurring codon sequences are closer to codon optimality than random sequences is not empirically validated beyond one organism
- Performance improvements in codon recovery rates have unclear biological significance without direct expression yield experiments
- The balanced tree partitioning algorithm for taxonomic clustering is described but not thoroughly validated for preserving meaningful expression environment distinctions

## Confidence

- **High Confidence:** The autoregressive architecture and taxon conditioning approach are technically sound and implementable
- **Medium Confidence:** The codon recovery rate improvements are demonstrated, but biological significance remains unclear
- **Low Confidence:** The claim that CodonMPNN "could learn to generate codon sequences with higher expression yields" is speculative without direct experimental validation

## Next Checks
1. Conduct expression yield experiments comparing CodonMPNN-generated sequences against wild-type sequences and sequences optimized by commercial codon optimization tools across multiple organisms
2. Perform ablation studies removing the taxon conditioning to quantify its contribution to performance
3. Validate the balanced tree partitioning algorithm by analyzing cluster composition and checking that clusters contain organisms with similar codon usage patterns