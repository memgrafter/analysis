---
ver: rpa2
title: 'OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs'
arxiv_id: '2411.14199'
source_url: https://arxiv.org/abs/2411.14199
tags:
- scholar
- open
- data
- evaluation
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OPEN SCHOLAR is a retrieval-augmented language model system for
  synthesizing scientific literature. It uses a large datastore of 45 million open-access
  papers and specialized retrieval and reranking components to retrieve relevant passages,
  then generates responses with accurate citations.
---

# OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs

## Quick Facts
- arXiv ID: 2411.14199
- Source URL: https://arxiv.org/abs/2411.14199
- Reference count: 40
- Key outcome: OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness on ScholarQABench

## Executive Summary
OpenScholar is a retrieval-augmented language model system designed to synthesize scientific literature by answering complex queries about recent research. The system uses a large datastore of 45 million open-access papers with specialized retrieval and reranking components to retrieve relevant passages, then generates responses with accurate citations. A key innovation is the iterative self-feedback generation mechanism that refines outputs through multiple retrieval cycles, improving both correctness and citation accuracy. On the new ScholarQABench benchmark, OpenScholar-8B achieves state-of-the-art performance, outperforming larger models while maintaining citation accuracy on par with human experts.

## Method Summary
OpenScholar uses a retrieval-augmented generation pipeline with a 45 million paper datastore containing 237 million passage embeddings. The system employs a bi-encoder retriever for initial passage retrieval and a cross-encoder for reranking, followed by an 8B LM for response generation. A novel iterative self-feedback mechanism allows the model to identify gaps and refine outputs through multiple retrieval cycles. The system also includes a citation verifier and can train smaller LMs on high-quality synthetic data generated through the inference pipeline. For inference, queries are processed through iterative refinement with retrieval, generating natural language feedback to identify needed improvements and incorporating additional passages as required.

## Key Results
- OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness on ScholarQABench
- Human experts preferred OpenScholar-8B responses over expert-written ones 51% of the time
- OpenScholar-GPT4o achieves 12% improvement in correctness compared to GPT-4o alone
- Citation accuracy matches human expert levels on ScholarQABench

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative self-feedback generation with retrieval significantly improves both correctness and citation accuracy by allowing the model to identify gaps and refine its outputs through multiple retrieval cycles.
- Mechanism: The model generates an initial response, then creates natural language feedback identifying missing content or improvements needed. If feedback indicates missing content, the model generates a retrieval query to fetch additional passages, which are incorporated into the next iteration. This process repeats until all feedback is addressed, ensuring comprehensive coverage and accurate citations.
- Core assumption: The language model can effectively self-assess its output quality and generate meaningful feedback that leads to improved responses when incorporated with additional retrieval.
- Evidence anchors:
  - [abstract]: "OpenScholar iteratively refines its outputs through natural language feedback, which improves quality and adaptively incorporates supplementary information."
  - [section]: "Unlike prior work that relies on a predefined set of feedback signals (Asai et al., 2024), our approach allows the LM to generate flexible natural language feedback on various aspects of the response, such as organization, completeness, or additional needed information."

### Mechanism 2
- Claim: Training smaller LMs on high-quality synthetic data generated by the inference pipeline enables efficient models to achieve performance comparable to larger proprietary models.
- Mechanism: The inference-time pipeline is used to generate synthetic queries and instructions from sampled datastore passages, producing intermediate and final outputs. This data is filtered and mixed with existing instruction-tuning data, then used to train smaller LMs that can effectively utilize retrieval-augmented generation without requiring expensive inference-time feedback loops.
- Core assumption: The synthetic data generated through the inference pipeline is of sufficient quality to train smaller models to effectively handle scientific literature synthesis tasks.
- Evidence anchors:
  - [abstract]: "The OpenScholar pipeline can also enhance off-the-shelf LMs. For example, when using GPT-4o as the underlying model, OpenScholar-GPT4o achieves a 12% improvement in correctness compared to GPT-4o alone."
  - [section]: "To train a smaller yet competitive 8B LM, we generate high-quality training data using this inference-time pipeline followed by data filtering and mixing."

### Mechanism 3
- Claim: The specialized datastore with trained retrievers and rerankers significantly outperforms general-purpose retrieval systems in scientific domains, enabling more relevant passage retrieval for literature synthesis.
- Mechanism: OpenScholar uses a 45 million paper datastore with specialized bi-encoder and cross-encoder models trained on scientific literature. The bi-encoder retrieves initial candidates, and the cross-encoder reranks them based on query-passage relevance, ensuring high-quality passages are passed to the generator.
- Core assumption: Scientific literature requires domain-specific retrieval models trained on relevant corpora to achieve high recall and precision compared to general-purpose models.
- Evidence anchors:
  - [abstract]: "OpenScholar uses our new OpenScholar-DataStore (OSDS), which contains 45 million open-access papers from Semantic Scholar, along with 237 million corresponding passage embeddings."
  - [section]: "Off-the-shelf retrieval models often struggle in out-of-domain scenarios (Thakur et al., 2021). To overcome this limitation, we develop θbi by continually pre-training Contriever (Izacard et al., 2022) on the peS2o datastore in an unsupervised fashion to improve domain-specific retrieval performance."

## Foundational Learning

- Concept: Dense passage retrieval with bi-encoder models
  - Why needed here: Scientific literature synthesis requires efficient retrieval of relevant passages from millions of papers, and dense retrieval with learned embeddings provides better semantic matching than keyword-based methods.
  - Quick check question: How does a bi-encoder model differ from a cross-encoder in terms of computational efficiency and retrieval effectiveness?

- Concept: Cross-encoder reranking for passage selection
  - Why needed here: Initial retrieval with bi-encoders can include irrelevant passages; cross-encoders provide more accurate relevance scoring by jointly encoding queries and passages, improving the quality of retrieved context.
  - Quick check question: Why is reranking necessary after initial retrieval, and what specific problem does it solve in the OpenScholar pipeline?

- Concept: Synthetic data generation for training smaller LMs
  - Why needed here: Creating high-quality training data for scientific literature synthesis is expensive and time-consuming; synthetic data generation allows scaling up training data while maintaining quality through careful filtering.
  - Quick check question: What are the key steps in OpenScholar's synthetic data generation pipeline, and how does it ensure the quality of generated training instances?

## Architecture Onboarding

- Component map:
  DataStore -> Retriever (bi-encoder) -> Reranker (cross-encoder) -> Generator (LM) -> Feedback Generator -> Citation Verifier

- Critical path:
  1. Input query → Retriever → Top N passages
  2. Passages + query → Generator → Initial response + feedback
  3. Feedback → Query generator (if needed) → Additional retrieval
  4. Updated passages + feedback → Generator → Refined response
  5. Response → Citation verifier → Final output

- Design tradeoffs:
  - Retriever complexity vs. retrieval quality: More complex retrievers improve quality but increase latency and cost
  - Number of feedback iterations vs. response quality: More iterations improve quality but increase inference time
  - Training data size vs. model performance: Larger training sets improve performance but require more computational resources
  - Passage count vs. context handling: More passages provide better coverage but may overwhelm smaller models

- Failure signatures:
  - Low citation accuracy: Issues with citation verification or passage relevance
  - Incomplete responses: Insufficient feedback iterations or poor initial retrieval
  - Hallucinations: Lack of proper retrieval or inadequate training data
  - Slow inference: Excessive feedback iterations or large context windows
  - Poor coverage: Insufficient passage retrieval or inadequate datastore

- First 3 experiments:
  1. Compare OpenScholar with and without self-feedback on a subset of ScholarQABench to measure improvement magnitude
  2. Test different numbers of feedback iterations (0, 1, 2, 3) to find optimal balance between quality and latency
  3. Evaluate retrieval quality using different datastore versions (v2 vs v3) to assess impact of corpus updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve retrieval accuracy for scientific literature synthesis tasks, particularly in biomedical domains?
- Basis in paper: Explicit - The paper discusses limitations of existing retrieval models and notes that even when citations refer to real papers, the majority are not substantiated by the corresponding abstracts, resulting in near-zero citation accuracy. The authors suggest enhancing retrieval methodologies by incorporating additional information like citation networks or metadata like publication recency.
- Why unresolved: Current retrieval models struggle with domain-specific scientific literature, often retrieving irrelevant passages or failing to capture the full context needed for accurate synthesis. The paper demonstrates this through experiments showing models without retrieval struggle significantly compared to retrieval-augmented models.
- What evidence would resolve it: A systematic evaluation comparing different retrieval architectures (dense vs sparse, cross-encoder vs bi-encoder) on the same benchmark, showing improvements in both retrieval accuracy and downstream generation quality when incorporating citation networks, publication recency, or other domain-specific metadata.

### Open Question 2
- Question: What is the optimal balance between synthetic training data generation and real expert-annotated data for training scientific literature synthesis models?
- Basis in paper: Explicit - The paper describes using self-feedback inference to generate synthetic training data but notes that synthetic data may contain hallucinations, repetitive writing, or limited instruction-following. They implement data filtering processes but acknowledge this is an ongoing challenge.
- Why unresolved: The paper demonstrates the effectiveness of their synthetic data approach but doesn't provide systematic comparison with different ratios of synthetic vs real data, or explore the limits of synthetic data quality. The optimal training mixture for achieving both efficiency and high-quality outputs remains unknown.
- What evidence would resolve it: Controlled experiments varying the proportion of synthetic vs real training data, measuring both final model performance and training efficiency, to determine the point of diminishing returns for synthetic data generation.

### Open Question 3
- Question: How can we develop evaluation metrics that better capture the quality of scientific literature synthesis beyond citation accuracy and coverage?
- Basis in paper: Explicit - The authors note that conventional similarity-based metrics like ROUGE show low correlation with human judgments for long-form generation tasks. They developed new evaluation protocols but acknowledge limitations, particularly that their automatic evaluation pipelines may not perfectly capture quality and that LLM evaluators struggle with evaluating overall usefulness.
- Why unresolved: Current evaluation metrics focus on surface-level features (citations, coverage, organization) but may miss deeper scientific understanding, critical analysis, or the ability to identify gaps in the literature. The paper shows that even when models achieve high scores on existing metrics, human experts identify limitations.
- What evidence would resolve it: Development and validation of evaluation metrics that capture deeper aspects of scientific synthesis, such as the ability to identify research gaps, evaluate methodological soundness of cited work, or synthesize conflicting findings. This would require extensive human evaluation to establish ground truth and correlation with existing metrics.

## Limitations

- The system's performance heavily depends on the quality and recency of the underlying datastore, which may not cover all recent scientific developments
- The iterative feedback mechanism introduces significant computational overhead, making the system expensive for large-scale deployment
- Long-term sustainability is uncertain due to computational costs of iterative refinement and need for continuous datastore updates

## Confidence

**High Confidence**: The retrieval-augmented architecture with specialized scientific retrievers and the iterative self-feedback mechanism are well-supported by the experimental results on ScholarQABench.

**Medium Confidence**: The synthetic data generation approach for training smaller models is theoretically sound but has limited empirical validation.

**Low Confidence**: The long-term sustainability of the system given computational costs and the need for continuous datastore updates to maintain relevance.

## Next Checks

1. **Cost-Performance Trade-off Analysis**: Measure the marginal improvement in correctness and citation accuracy for each additional feedback iteration, and determine the optimal number of iterations that balances quality gains against computational costs.

2. **Generalization Testing**: Evaluate OpenScholar on scientific domains not represented in the training data or datastore to assess how well the specialized retrievers and trained models generalize beyond their training distribution.

3. **Datastore Currency Impact Study**: Compare system performance using different versions of the datastore (v2 vs v3) and measure how quickly retrieval quality and overall performance degrade as the underlying corpus becomes outdated.