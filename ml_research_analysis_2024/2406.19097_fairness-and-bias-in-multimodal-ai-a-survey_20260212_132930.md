---
ver: rpa2
title: 'Fairness and Bias in Multimodal AI: A Survey'
arxiv_id: '2406.19097'
source_url: https://arxiv.org/abs/2406.19097
tags:
- bias
- language
- pages
- fairness
- proceedings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey addresses the gap in studying fairness and bias in
  Large Multimodal Models (LMMs) compared to Large Language Models (LLMs), providing
  a comprehensive review of 50 datasets and models affected by bias. It highlights
  the preprocessing method as a less-discussed approach for mitigating bias, in contrast
  to intrinsic and extrinsic methods.
---

# Fairness and Bias in Multimodal AI: A Survey

## Quick Facts
- arXiv ID: 2406.19097
- Source URL: https://arxiv.org/abs/2406.19097
- Reference count: 40
- One-line primary result: This survey identifies a significant research gap in fairness and bias studies for multimodal AI compared to language models, while highlighting preprocessing as an under-discussed mitigation approach.

## Executive Summary
This survey addresses the imbalance in fairness and bias research between Large Multimodal Models (LMMs) and Large Language Models (LLMs), providing evidence that LMMs have received significantly less attention despite facing similar challenges. The study comprehensively reviews 50 datasets and models affected by bias across various domains including gender, race, and cultural stereotypes. A key contribution is highlighting preprocessing (preuse) as a less-discussed mitigation method compared to the more commonly studied intrinsic and extrinsic approaches, offering a novel perspective on bias reduction strategies in AI systems.

## Method Summary
The survey employed a multi-phase literature search using Google Scholar and Web of Science with Boolean queries targeting fairness and bias in both multimodal and language models. Researchers filtered results to include only peer-reviewed papers from 2014-2024, excluding irrelevant and predatory publications. The remaining papers were critically reviewed to identify datasets, models, and bias mitigation strategies, with particular attention to categorizing mitigation methods into preprocessing, intrinsic, and extrinsic approaches. The analysis focused on documenting the disparity in research attention and evaluating the effectiveness of various debiasing techniques.

## Key Results
- Google Scholar search returned 33,400 links for multimodal models versus 538,000 for language models, indicating significant research disparity
- Preprocessing methods are substantially under-discussed compared to intrinsic and extrinsic mitigation approaches in the literature
- The survey identifies gender, racial, and cultural biases as key challenges, with methods including counterfactual data augmentation and debiasing word embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Search engine result counts reflect research focus disparity between multimodal and language models
- Mechanism: Boolean search queries across Google Scholar and Web of Science produce numerical differences indicating relative attention
- Core assumption: Query syntax correctly captures relevant literature without significant false positives/negatives
- Evidence anchors:
  - [abstract] States "a query search on Google Scholar returns 33,400 links compared to 538,000 links for 'Fairness and bias in Large Language Models'"
  - [section] Shows "33,400 and 538,000 links are the initial results, respectively, for Scholar while 4 and 50 links are the initial results, respectively, for WoS"
  - [corpus] Weak: neighbor titles focus on fairness in LLMs, not multimodal bias disparity
- Break condition: Query syntax fails to exclude irrelevant papers or includes false negatives

### Mechanism 2
- Claim: Preprocessing (preuse) is under-discussed in NLP bias mitigation compared to intrinsic and extrinsic methods
- Mechanism: Literature review reveals fewer papers on preprocessing approaches than on in-processing and post-processing
- Core assumption: Paper categorization by mitigation type is accurate
- Evidence anchors:
  - [abstract] States "The method is less-mentioned compared to the two well-known ones in the literature: intrinsic and extrinsic mitigation methods"
  - [section] Notes "More work has focused on the latter two than the first"
  - [corpus] Weak: neighbor papers mostly address fairness in LLMs, not mitigation category balance
- Break condition: Misclassification of mitigation method types in reviewed papers

### Mechanism 3
- Claim: Quantitative bias metrics (e.g., bipol, WEAT, cosine similarity) enable systematic bias evaluation
- Mechanism: Embedding association tests and similarity measures detect systematic associations between sensitive attributes and target terms
- Core assumption: Metrics accurately capture bias without systematic overestimation
- Evidence anchors:
  - [section] Discusses "Embedding Association Tests (EATs) have been used in several studies" and "cosine similarity...may also be used to audit fairness and bias"
  - [abstract] Mentions "Bipol has the weakness that if the bias classifier is not accurate enough, false positives will weaken the evaluation score"
  - [corpus] Weak: neighbor papers do not focus on bias evaluation metrics
- Break condition: Metrics produce high false positive or false negative rates

## Foundational Learning

- Concept: Boolean search logic
  - Why needed here: Understanding how search queries combine terms affects result counts and literature coverage
  - Quick check question: What is the difference between "AND" and "OR" in Boolean search?

- Concept: Bias mitigation categories
  - Why needed here: Preprocessing, intrinsic, and extrinsic methods represent distinct phases of model development for bias reduction
  - Quick check question: Which mitigation category involves modifying training data before model training?

- Concept: Embedding association tests
  - Why needed here: These tests quantify bias by measuring semantic associations in vector representations
  - Quick check question: How does WEAT measure bias using word embeddings?

## Architecture Onboarding

- Component map: Literature search → Filtering → Categorization → Analysis → Discussion
- Critical path: Accurate search queries → Relevant paper selection → Correct categorization → Valid conclusions
- Design tradeoffs: Broader queries capture more literature but increase noise; narrower queries reduce noise but may miss relevant work
- Failure signatures: Inconsistent result counts across search engines; misclassification of mitigation methods; weak evidence anchors
- First 3 experiments:
  1. Replicate search queries with slightly modified boolean operators to test result stability
  2. Sample papers from each mitigation category to verify classification accuracy
  3. Apply multiple bias metrics to the same dataset to compare effectiveness and consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific quantitative thresholds or benchmarks should be established to determine acceptable levels of fairness and bias mitigation in AI systems?
- Basis in paper: [inferred] from discussion on the need for better evaluation tools and strategies to address fairness and bias in AI systems
- Why unresolved: The paper emphasizes the importance of fairness and bias mitigation but does not provide specific quantitative thresholds or benchmarks for acceptable levels
- What evidence would resolve it: Research studies that propose and validate quantitative thresholds or benchmarks for fairness and bias in AI systems across different domains and applications

### Open Question 2
- Question: How can the effectiveness of different debiasing strategies be systematically compared and evaluated across various types of AI models and datasets?
- Basis in paper: [explicit] from the discussion on various debiasing strategies such as counterfactual data augmentation, improved filtering, and linear projection, but a lack of comparative analysis
- Why unresolved: While the paper discusses multiple debiasing strategies, it does not provide a systematic framework for comparing their effectiveness across different AI models and datasets
- What evidence would resolve it: Comparative studies that evaluate the performance of different debiasing strategies on diverse AI models and datasets, using standardized metrics and evaluation frameworks

### Open Question 3
- Question: What are the long-term societal impacts of implementing fairness and bias mitigation strategies in AI systems, and how can these impacts be measured and monitored?
- Basis in paper: [inferred] from the mention of the need for social analysis of AI use and the impact of bias overall, as well as the example of DALL-E 2's gender bias despite guardrails
- Why unresolved: The paper highlights the importance of considering the broader societal impact of AI fairness and bias mitigation but does not provide a framework for measuring and monitoring these long-term effects
- What evidence would resolve it: Longitudinal studies that track the societal impacts of fairness and bias mitigation strategies in AI systems, using both quantitative and qualitative measures to assess their effectiveness and unintended consequences

## Limitations
- The quantitative disparity claims depend on search query sensitivity and may not fully capture all relevant literature
- Limited evidence anchors from neighboring papers suggest potential gaps in the literature review's contextual foundation
- The characterization of preprocessing as under-discussed requires verification through systematic categorization of reviewed papers

## Confidence
- Disparity in research focus between LMM and LLM: Medium
- Preprocessing as under-discussed mitigation approach: Medium
- Quantitative bias metrics enable systematic evaluation: Medium

## Next Checks
1. Replicate the search queries using alternative boolean operators and time ranges to test result stability
2. Conduct inter-rater reliability assessment on a sample of papers to verify classification into preprocessing/intrinsic/extrinsic categories
3. Apply multiple bias metrics (WEAT, cosine similarity, embedding association tests) to the same datasets to compare consistency and false positive/negative rates