---
ver: rpa2
title: Towards Robust Graph Structural Learning Beyond Homophily via Preserving Neighbor
  Similarity
arxiv_id: '2401.09754'
source_url: https://arxiv.org/abs/2401.09754
tags:
- graph
- graphs
- links
- node
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NSPGNN, a novel robust graph neural network
  designed to enhance adversarial robustness on both homophilic and heterophilic graphs.
  The key insight is that graph structural attacks tend to connect node pairs with
  dissimilar aggregated neighbor features rather than ego features.
---

# Towards Robust Graph Structural Learning Beyond Homophily via Preserving Neighbor Similarity

## Quick Facts
- arXiv ID: 2401.09754
- Source URL: https://arxiv.org/abs/2401.09754
- Reference count: 40
- This paper proposes NSPGNN, a novel robust graph neural network designed to enhance adversarial robustness on both homophilic and heterophilic graphs.

## Executive Summary
This paper introduces NSPGNN, a novel robust graph neural network designed to enhance adversarial robustness on both homophilic and heterophilic graphs. The key insight is that graph structural attacks tend to connect node pairs with dissimilar aggregated neighbor features rather than ego features. Based on this theoretical finding, NSPGNN introduces a dual-kNN graph pipeline to construct positive and negative kNN graphs based on neighbor similarity. The model then propagates node features along these graphs using low-pass and high-pass filters to preserve neighbor similarity. Extensive experiments demonstrate that NSPGNN significantly outperforms state-of-the-art methods on both homophilic and heterophilic graphs under various adversarial attacks, achieving up to 34.31% higher accuracy on Squirrel dataset with 25% attack power.

## Method Summary
NSPGNN enhances adversarial robustness by preserving neighbor similarity through a dual-kNN graph construction and propagation framework. The method constructs positive kNN graphs (capturing high-similarity information) and negative kNN graphs (capturing low-similarity information) based on the similarity of aggregated neighbor features. These graphs are used with low-pass and high-pass filters respectively, with learnable weights that adaptively balance the relative importance of one-hop and two-hop neighbor information. The framework is trained end-to-end using standard semi-supervised classification objectives.

## Key Results
- NSPGNN significantly outperforms state-of-the-art methods on both homophilic and heterophilic graphs under adversarial attacks
- Achieves up to 34.31% higher accuracy than competitors on Squirrel dataset with 25% attack power
- Improves clean graph accuracy, serving as an effective data augmentation technique
- Demonstrates superior performance on heterophilic graphs where traditional GNNs struggle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph structural attacks are more effective when they connect node pairs with dissimilar aggregated neighbor features rather than ego features.
- Mechanism: The attack loss update magnitude is negatively correlated with the pairwise similarity matrix based on the τ-th powered aggregated neighbor features (A^τ X). This means connecting dissimilar nodes based on neighbor features maximizes attack impact.
- Core assumption: The gradient of the attack loss with respect to the graph structure depends on the similarity between aggregated neighbor features.
- Evidence anchors:
  - [abstract] "we theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features"
  - [section] "the magnitude of the update of the attack loss is negatively related to the similarity matrix K = A^τ X(A^τ X)^⊤"
- Break condition: If the graph structure changes do not significantly affect the aggregated neighbor feature similarity matrix, or if the attack loss gradient becomes independent of this similarity.

### Mechanism 2
- Claim: Dual-kNN graphs construction with positive and negative similarity information provides contrastive supervision for robust propagation.
- Mechanism: Positive kNN graphs capture high-similarity information and are used with low-pass filters to smooth features of similar nodes. Negative kNN graphs capture low-similarity information and are used with high-pass filters to discriminate dissimilar nodes.
- Core assumption: Contrastive supervision from both similar and dissimilar node pairs improves robustness against structural attacks.
- Evidence anchors:
  - [abstract] "NSPGNN introduces a dual-kNN graph constructions pipeline to supervise the neighbor-similarity-preserved propagation"
  - [section] "we construct positive kNN graphs and negative kNN graphs based on the descending order and ascending order of the top-k similarity scores of A^τ X"
- Break condition: If the dual-kNN construction becomes computationally prohibitive for large graphs, or if the contrastive signals become noisy and counterproductive.

### Mechanism 3
- Claim: Adaptive neighbor-similarity-guided propagation balances low-pass and high-pass filtering based on learned weights.
- Mechanism: Learnable weights α and β are trained via MLP to adaptively balance the relative importance between one-hop and two-hop neighbor similarity information for both positive and negative kNN graphs.
- Core assumption: The model can learn optimal weights to balance smoothing and discrimination based on the specific graph structure and attack scenario.
- Evidence anchors:
  - [abstract] "The model then propagates node features along these graphs using low-pass and high-pass filters to preserve neighbor similarity"
  - [section] "we train the learnable weight matrix α^(l) and β^(l) at each layer by implementing a multi-layer perceptron (MLP) on the nodal feature matrix H^(l-1)"
- Break condition: If the learned weights become saturated (all near 0 or 1), or if the MLP fails to capture the necessary relationships between node features and optimal filtering.

## Foundational Learning

- Concept: Graph Neural Networks and their vulnerability to structural attacks
  - Why needed here: The paper addresses adversarial robustness of GNNs on both homophilic and heterophilic graphs, which requires understanding how GNNs work and why they are vulnerable.
  - Quick check question: How does the aggregation mechanism in standard GNNs make them susceptible to graph structural attacks?

- Concept: Homophily and heterophily in graph data
  - Why needed here: The proposed method aims to work universally on both homophilic and heterophilic graphs, which have fundamentally different connectivity patterns.
  - Quick check question: What is the key difference between homophilic and heterophilic graphs in terms of node connectivity patterns?

- Concept: Similarity metrics for node features and their aggregation
  - Why needed here: The core mechanism relies on computing similarities based on aggregated neighbor features rather than ego features, which is a key innovation.
  - Quick check question: How does the similarity metric Ω(A^τ X) differ from traditional ego-feature-based similarity measures?

## Architecture Onboarding

- Component map: Input (A, X, Y_train) -> Bi-kNN Graph Pipeline -> Neighbor-Similarity-Guided Propagation -> Output (node embeddings)
- Critical path:
  1. Compute similarity matrix Ω(A^τ X) for neighbor features
  2. Construct positive and negative kNN graphs
  3. Learn adaptive weights α and β via MLP
  4. Propagate features through low-pass and high-pass filters
  5. Fuse embeddings and perform classification
- Design tradeoffs:
  - kNN graph size vs. computational efficiency
  - Depth τ of aggregated features vs. over-smoothing
  - Balance between low-pass and high-pass filtering
- Failure signatures:
  - Performance degradation on clean graphs may indicate over-pruning of normal connections
  - High sensitivity to kNN parameters may indicate instability
  - Similar performance to baseline GNNs may indicate ineffective neighbor similarity preservation
- First 3 experiments:
  1. Test NSPGNN on clean Cora dataset with varying k1 and k2 values to find optimal hyperparameters
  2. Evaluate NSPGNN against Mettack on Chameleon with different attack powers to verify robustness claims
  3. Compare NSPGNN with GCN-Jaccard on heterophilic graphs to demonstrate superiority of neighbor similarity over ego similarity approach

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the methodology and results, several important open questions emerge:

### Open Question 1
- Question: What is the optimal value of τ (the power of the aggregated neighbor features) for NSPGNN in practice?
- Basis in paper: [explicit] The paper mentions that over-smoothing occurs with high-order Ω(AτX), leading to mixed densities for malicious and benign links. However, it does not provide a systematic method to determine the optimal τ.
- Why unresolved: The choice of τ affects the model's ability to distinguish malicious links while avoiding over-smoothing. This requires further empirical studies across diverse datasets.
- What evidence would resolve it: A comprehensive sensitivity analysis across multiple datasets, measuring the trade-off between link detection accuracy and over-smoothing, would clarify the optimal τ.

### Open Question 2
- Question: How does NSPGNN perform on graphs with extreme heterophily (H(G) close to 0)?
- Basis in paper: [inferred] The paper evaluates NSPGNN on datasets with moderate heterophily (H(G) around 0.22-0.23) but does not test its robustness on graphs with near-zero homophily ratios.
- Why unresolved: Extreme heterophily may challenge the neighbor similarity preservation mechanism, as the similarity matrix Ω(AτX) might become less discriminative.
- What evidence would resolve it: Experiments on synthetic or real-world graphs with H(G) approaching 0 would reveal NSPGNN's limitations and potential adaptations needed.

### Open Question 3
- Question: Can NSPGNN be extended to dynamic graphs where topology changes over time?
- Basis in paper: [inferred] The paper focuses on static graphs, and the dual-kNN construction relies on fixed aggregated features. Dynamic graphs introduce temporal dependencies that are not addressed.
- Why unresolved: Real-world graphs often evolve, and the current framework may not efficiently update kNN graphs or propagate features in a streaming fashion.
- What evidence would resolve it: A dynamic extension of NSPGNN, tested on temporal graph datasets, would demonstrate its scalability and adaptability to evolving structures.

## Limitations
- Theoretical analysis establishes correlation between attack loss and neighbor similarity, but practical effectiveness depends on specific graph structure and attack strategy
- Dual-kNN graph construction introduces computational overhead that may limit scalability to very large graphs
- Performance on extremely heterophilic graphs (with homophily ratio below 0.1) remains unverified

## Confidence
- **High Confidence**: NSPGNN's effectiveness in improving clean graph accuracy through neighbor similarity preservation
- **Medium Confidence**: The claim that NSPGNN outperforms state-of-the-art methods on both homophilic and heterophilic graphs under adversarial attacks
- **Low Confidence**: The universal applicability of NSPGNN across all graph types and attack strategies

## Next Checks
1. **Scalability Test**: Evaluate NSPGNN's performance and computational efficiency on graphs with 100K+ nodes to assess practical scalability limitations
2. **Extreme Heterophily Case**: Test NSPGNN on graphs with homophily ratio below 0.1 (e.g., certain biological networks) to verify robustness claims across the full spectrum of graph structures
3. **Attack Transferability**: Assess NSPGNN's performance when attacked by unseen attack strategies (e.g., adversarial node feature perturbations) to evaluate the method's general adversarial robustness beyond structural attacks