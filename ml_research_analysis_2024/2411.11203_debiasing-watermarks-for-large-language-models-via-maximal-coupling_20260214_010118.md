---
ver: rpa2
title: Debiasing Watermarks for Large Language Models via Maximal Coupling
arxiv_id: '2411.11203'
source_url: https://arxiv.org/abs/2411.11203
tags:
- watermark
- green
- text
- test
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a watermarking method for large language
  models that uses maximal coupling to maintain unbiased token sampling while embedding
  a pseudorandom watermark signal. The approach partitions the vocabulary into green
  and red lists, increases the probability of green tokens, and applies maximal coupling
  to correct bias.
---

# Debiasing Watermarks for Large Language Models via Maximal Coupling

## Quick Facts
- arXiv ID: 2411.11203
- Source URL: https://arxiv.org/abs/2411.11203
- Authors: Yangxinyu Xie; Xiang Li; Tanwi Mallick; Weijie J. Su; Ruixun Zhang
- Reference count: 40
- Primary result: A watermarking method using maximal coupling maintains unbiased token sampling while embedding a pseudorandom watermark signal.

## Executive Summary
This paper introduces a novel watermarking approach for large language models that uses maximal coupling to preserve unbiased token sampling while embedding a pseudorandom watermark signal. The method partitions the vocabulary into green and red lists, increases the probability of green tokens, and applies maximal coupling to correct bias. A uniform coin flip decides whether to apply bias correction, with the result embedded as a pseudorandom signal. Theoretical analysis confirms the approach's unbiased nature and robust detection capabilities. Experimental results show it outperforms prior techniques by preserving text quality while maintaining high detectability, and it demonstrates resilience to targeted modifications aimed at improving text quality.

## Method Summary
The method uses maximal coupling to sample tokens from the next-token-prediction (NTP) distribution while embedding a watermark signal. The vocabulary is partitioned into green and red lists using pseudorandom functions. For each token generation step, the algorithm constructs a modified distribution that increases the probability of green tokens, then applies maximal coupling with a uniform random variable to sample the next token. This ensures the token distribution remains unbiased while the uniform variables corresponding to green tokens form a detectable watermark signal. Detection is performed using hypothesis testing with either a sum test or higher criticism statistic.

## Key Results
- Outperforms prior techniques by preserving text quality while maintaining high detectability
- Demonstrates resilience to targeted modifications aimed at improving text quality
- Shows sum test outperforms higher criticism for shorter texts (≤1000 tokens)

## Why This Works (Mechanism)

### Mechanism 1
Maximal coupling preserves the original token distribution while embedding watermark signal. The algorithm samples tokens using overlap and excess distributions. If ζ ≤ p (overlap probability), it samples from the normalized overlap distribution ∝ min(Pt, Qt); otherwise, it samples from the normalized excess distribution ∝ max(0, Pt − Qt). The maximal coupling procedure maintains unbiasedness when ζ is a standard uniform variable.

### Mechanism 2
Conditional distribution of ζt given green token provides detectable watermark signal. When a token wt is in the green list Gt, the conditional distribution of ζt given (Pt, Gt) is uniform on [0, Pt,Gt]. This creates a statistical signal that differs from the null hypothesis where ζt is independent of the green/red list.

### Mechanism 3
Higher criticism test achieves adaptive optimality for sparse watermark signal detection. The test statistic HC*m = max(HCm,t) where HCm,t = √m · (t/m - ζ(t)) / (ζ(t)(1 - ζ(t))) orders the ζt values and finds the maximum standardized deviation. This test is non-parametric and doesn't require knowledge of Pt,Gt.

## Foundational Learning

- Concept: Maximal coupling in probability theory
  - Why needed here: The watermarking method relies on maximal coupling to sample tokens while preserving the original distribution
  - Quick check question: What is the key property of maximal coupling that makes it suitable for unbiased watermarking?

- Concept: Hypothesis testing and detection boundaries
  - Why needed here: The paper reformulates watermark detection as a hypothesis testing problem and establishes detection boundaries for different scenarios
  - Quick check question: What is the difference between the sum test and higher criticism in terms of their detection boundaries?

- Concept: Sparse mixture detection
  - Why needed here: The paper models user modifications as sparse mixture detection problems and applies results from Donoho and Jin (2004, 2015)
  - Quick check question: Under what conditions does the paper show that watermark detection becomes impossible?

## Architecture Onboarding

- Component map:
  - Language model provider -> Green list generator -> Maximal coupling sampler -> Text generator
  - User -> Text modifier -> Detector
  - Detector -> Hypothesis tester -> Detection result

- Critical path:
  1. Generate pseudorandom green lists G1:n and uniform variables ζ1:n
  2. For each token generation step: construct Qt, apply maximal coupling with ζt
  3. Detect watermark by analyzing ζt values corresponding to green tokens
  4. Apply hypothesis test (sum or HC) to determine if watermark is present

- Design tradeoffs:
  - Single green list vs. separate green lists per token: Single list reduces complexity but may be more vulnerable to attacks
  - Sum test vs. HC: Sum test is more powerful for smaller texts, HC has better asymptotic properties
  - Green list size (γ): Larger lists increase watermark signal but may reduce text quality

- Failure signatures:
  - Low detection power despite watermark presence: Check if Pt,Gt is too close to 1 or if εm is too large
  - High false positive rate: Verify test statistic calculation and critical value selection
  - Poor text quality: Check if maximal coupling implementation is correct and if green list partitioning is appropriate

- First 3 experiments:
  1. Verify unbiasedness: Generate text with and without watermarking, compare token distributions
  2. Test detection power: Apply detection tests to watermarked vs. non-watermarked text
  3. Evaluate text quality: Measure S-BERT similarity and repetition rates between watermarked and original text

## Open Questions the Paper Calls Out

### Open Question 1
How does the single green list approach affect watermark robustness against adaptive attacks where an adversary attempts to learn the green list generator? The paper mentions this as an interesting direction for future work without empirical testing or theoretical analysis of attack scenarios.

### Open Question 2
What is the impact of using maximal coupling with the soft green/red list watermark on detection power and text quality compared to the hard partitioning approach? The paper only provides theoretical discussion of this adaptation without experimental validation.

### Open Question 3
How does the detection power of higher criticism versus sum tests scale with text length in finite samples, particularly for practical text lengths below the asymptotic regime? While the paper shows empirical differences for specific text lengths, it doesn't provide a comprehensive analysis of the scaling relationship.

### Open Question 4
How would watermarking performance change if the target model in speculative decoding had partial knowledge of the watermark signal, simulating a human editor who can recognize and attempt to preserve watermark patterns? The current model assumes the target model is completely ignorant of watermarking.

## Limitations

- Theoretical analysis assumes infinite sequences and specific decay rates that may not translate to finite, real-world applications
- Experimental evaluation focuses on small language models and limited datasets, leaving performance on larger models unknown
- Critical implementation details are underspecified, including green list generation mechanism and maximal coupling algorithm

## Confidence

**High Confidence** (Supported by both theoretical analysis and experimental evidence):
- The maximal coupling approach achieves unbiased token sampling while embedding a watermark signal
- The watermark embedding increases the probability of green tokens from Pt,Gt to Qt,Gt = γPt,Gt/(γPt,Gt + (1-γ)(1-Pt,Gt))
- The detection methods (sum test and higher criticism) can distinguish watermarked from non-watermarked text with high accuracy
- The watermark embedding improves text quality compared to traditional green/red list methods

**Medium Confidence** (Supported by theoretical analysis but limited experimental validation):
- The method's robustness against targeted modifications aimed at improving text quality
- The detection power guarantees under various sparsity conditions (Theorems 3.3, 3.4, 3.7)
- The security against spoofing attacks through contrastive learning

**Low Confidence** (Theoretical claims with minimal experimental support):
- Performance on larger, more capable language models
- Robustness against sophisticated adversarial attacks beyond the tested scenarios
- Practical detection accuracy with finite-length texts (as opposed to asymptotic guarantees)

## Next Checks

**Check 1: Detection Power on Large Models**
Replicate the watermarking and detection pipeline on state-of-the-art models (GPT-4, Claude, Gemini) using diverse datasets. Measure detection accuracy across varying text lengths and evaluate how the detection power scales with sequence length compared to theoretical predictions.

**Check 2: Attack Resistance Evaluation**
Systematically evaluate the watermark's resistance to advanced adversarial attacks, including: (a) gradient-based token optimization to remove watermark signals while preserving text semantics, (b) black-box attacks using paraphrase models or controlled generation, and (c) statistical attacks that exploit knowledge of the watermarking mechanism.

**Check 3: Implementation Verification**
Implement the complete watermarking pipeline including: (a) the maximal coupling sampling algorithm with proper normalization, (b) the green list generation function using previous tokens as input, and (c) the detection tests (sum and higher criticism). Verify that the implementation maintains unbiasedness through statistical tests comparing token distributions with and without watermarking.