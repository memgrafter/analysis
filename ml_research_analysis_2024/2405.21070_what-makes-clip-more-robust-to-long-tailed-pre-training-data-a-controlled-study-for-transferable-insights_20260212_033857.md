---
ver: rpa2
title: What Makes CLIP More Robust to Long-Tailed Pre-Training Data? A Controlled
  Study for Transferable Insights
arxiv_id: '2405.21070'
source_url: https://arxiv.org/abs/2405.21070
tags:
- clip
- data
- class
- learning
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why CLIP models show better robustness to
  long-tailed pre-training data compared to supervised learning. Through controlled
  experiments, the authors find that CLIP's pretext task forms a dynamic classification
  problem where only a subset of classes is present during training, implicitly balancing
  the learning signal and isolating bias from dominant classes.
---

# What Makes CLIP More Robust to Long-Tailed Pre-Training Data? A Controlled Study for Transferable Insights

## Quick Facts
- arXiv ID: 2405.21070
- Source URL: https://arxiv.org/abs/2405.21070
- Authors: Xin Wen; Bingchen Zhao; Yilun Chen; Jiangmiao Pang; Xiaojuan Qi
- Reference count: 40
- One-line primary result: CLIP's robustness to long-tailed pre-training data stems from dynamic vocabulary subsampling, descriptive language supervision, and open-world concept utilization

## Executive Summary
This paper investigates why CLIP models demonstrate superior robustness to long-tailed pre-training data compared to supervised learning approaches. Through controlled experiments, the authors identify three key mechanisms: CLIP's dynamic classification problem that subsamples vocabulary during training, the benefit of more descriptive language supervision, and the utilization of open-world concepts. The findings reveal that CLIP's implicit de-biasing through vocabulary subsampling can be transferred to supervised and self-supervised learning frameworks, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks.

## Method Summary
The study employs controlled experiments across multiple datasets including ImageNet-Captions and LAIONet, using both CLIP and supervised learning models with varying hyperparameters. CLIP models are trained with cross-entropy loss using AdamW optimizer (lr=0.001, batch=1024, 32 epochs), while supervised models use SGD+Nesterov (lr=0.1, batch=256, 90 epochs). The experiments systematically vary vocabulary size, language descriptiveness, and data scale to isolate the effects of each mechanism. Performance is evaluated using ImageNet zero-shot classification accuracy and Spearman correlation between class frequency and model statistics.

## Key Results
- CLIP's dynamic vocabulary subsampling during training implicitly balances the learning signal and isolates bias from dominant classes
- More descriptive language supervision improves both CLIP's robustness and discriminability
- CLIP's ability to utilize open-world concepts (inaccessible to supervised learning) enhances generalization when data scales up
- Vocabulary subsampling technique transfers to supervised learning, achieving CLIP-level performance on imbalanced data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's pretext task forms a dynamic classification problem where only a subset of classes is present during training, implicitly balancing the learning signal.
- Mechanism: By subsampling the vocabulary during training, CLIP isolates the bias from dominant classes and prevents over-optimization of head class representations.
- Core assumption: The model can still learn effective representations when the training vocabulary is dynamically restricted.
- Evidence anchors:
  - [abstract] "CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training."
  - [section] "We note that the pretext of • template-based CLIP still differs from ✖SL... the vocabulary (classes in a mini-batch) of CLIP is much smaller than SL (all classes)."
  - [corpus] Weak evidence; related papers focus on long-tailed detection rather than pretext task analysis.
- Break condition: If the subsampled vocabulary becomes too small to provide sufficient negative examples, model performance degrades.

### Mechanism 2
- Claim: CLIP's robustness and discriminability improve with more descriptive language supervision.
- Mechanism: Increased descriptiveness in language supervision provides richer semantic context, enabling better feature separation and robustness to class frequency imbalance.
- Core assumption: More descriptive captions contain additional discriminative information beyond class names.
- Evidence anchors:
  - [abstract] "the robustness and discriminability of CLIP improve with more descriptive language supervision"
  - [section] "• higher text descriptiveness results in improvements in both robustness and discriminability of CLIP"
  - [corpus] Weak evidence; related papers don't directly address language supervision descriptiveness.
- Break condition: If language supervision becomes overly verbose or noisy, it may harm rather than help model performance.

### Mechanism 3
- Claim: CLIP's robustness benefits from its ability to utilize open-world concepts, a privilege not accessible to supervised learning.
- Mechanism: Open-world concepts share useful information with close-set ones, and generalization occurs when data scales up.
- Core assumption: Concepts outside the target vocabulary can still provide meaningful learning signals.
- Evidence anchors:
  - [abstract] "CLIP's robustness and discriminability improve... with broader open-world concepts, which are inaccessible to supervised learning"
  - [section] "CLIP only requires weak image-text supervision and is thus not bound by a pre-defined vocabulary"
  - [corpus] Weak evidence; related papers focus on long-tailed recognition rather than open-world concept utilization.
- Break condition: If open-world concepts are too dissimilar from target classes, they may introduce noise rather than helpful signals.

## Foundational Learning

- Concept: Contrastive learning and its role in representation learning
  - Why needed here: Understanding how CLIP's cross-modal contrastive learning differs from standard supervised learning is crucial for grasping why it handles imbalance better.
  - Quick check question: What is the key difference between CLIP's training objective and standard supervised classification?

- Concept: Neural collapse and its implications for long-tailed learning
  - Why needed here: Neural collapse theory explains why classifiers tend to overfit to head classes, which is relevant to understanding CLIP's de-biasing mechanisms.
  - Quick check question: How does neural collapse typically manifest in long-tailed classification problems?

- Concept: Vocabulary subsampling and its regularization effects
  - Why needed here: This technique is central to both CLIP's success and its transferability to supervised learning.
  - Quick check question: What is the relationship between vocabulary size and classifier bias in long-tailed settings?

## Architecture Onboarding

- Component map: Vision encoder (ViT/ResNet) -> Text encoder (Transformer) -> Contrastive loss function -> Image-text representation alignment
- Critical path: The key insight is that CLIP's dynamic vocabulary during training implicitly rebalances the learning signal, which can be transferred to supervised learning by subsampling the classification vocabulary.
- Design tradeoffs: Using smaller vocabularies improves robustness but may reduce discrimination capability if too restrictive.
- Failure signatures: If vocabulary subsampling is too aggressive, the model may fail to learn meaningful representations; if too conservative, it may not effectively de-bias the classifier.
- First 3 experiments:
  1. Implement vocabulary subsampling in a standard supervised classifier and measure its effect on prediction bias.
  2. Compare CLIP's performance with and without frozen text encoders on imbalanced data.
  3. Test different vocabulary sizes to find the optimal balance between robustness and discriminability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CLIP's performance vary when evaluated on concept sets that are not well-represented in web-crawled data, such as fine-grained biological categories?
- Basis in paper: [explicit] The authors note that CLIP's robustness decreases on datasets like CUB and Flowers-102, which contain fine-grained biological concepts that are less common in web data.
- Why unresolved: The paper only provides correlation statistics for a limited set of concept sets, and does not conduct in-depth experiments on underrepresented concept categories.
- What evidence would resolve it: Detailed experiments evaluating CLIP's performance on a wider range of underrepresented concept sets, including fine-grained biological categories, would clarify the model's limitations in handling rare concepts.

### Open Question 2
- Question: What are the underlying reasons for the modality gap between vision and text features in CLIP, and how does this affect the model's ability to handle imbalanced data?
- Basis in paper: [inferred] The authors observe that vision and text features are plotted separately in the multi-modal feature space, suggesting a modality gap. They hypothesize this could be related to CLIP's bias.
- Why unresolved: The paper does not provide a thorough analysis of the modality gap or its impact on CLIP's performance.
- What evidence would resolve it: A detailed study of the modality gap, including its causes and effects on CLIP's performance, would shed light on this issue.

### Open Question 3
- Question: Can the techniques used to improve CLIP's robustness to imbalanced data be applied to other vision-language models, such as those based on generative approaches?
- Basis in paper: [explicit] The authors suggest that their findings are transferable to other domains, including supervised and self-supervised learning frameworks.
- Why unresolved: The paper does not explore the application of these techniques to other vision-language models, particularly generative ones.
- What evidence would resolve it: Experiments applying the proposed techniques to other vision-language models, including generative ones, would demonstrate their generalizability.

## Limitations

- The paper doesn't provide direct evidence that CLIP's dynamic classification problem is the primary reason for its robustness, only that it correlates with improved performance.
- The language supervision experiments, while showing descriptive captions help, don't isolate whether this effect is due to semantic richness or simply more training signal.
- The transferability claims to supervised learning are demonstrated but not thoroughly explored across diverse architectures and datasets.

## Confidence

- **High confidence**: The experimental observation that vocabulary subsampling improves supervised learning robustness on imbalanced data is well-supported by controlled comparisons.
- **Medium confidence**: The claim that CLIP's dynamic classification mechanism isolates bias from dominant classes is plausible but not conclusively proven, as the experiments don't directly measure bias isolation.
- **Low confidence**: The assertion that open-world concepts significantly contribute to CLIP's robustness lacks strong empirical support in the paper, with limited validation of this mechanism.

## Next Checks

1. Design an experiment that directly measures bias isolation by comparing class representation drift in CLIP vs supervised models during training on imbalanced data.
2. Conduct ablation studies varying only the descriptiveness of captions while controlling for dataset size to isolate the effect of semantic richness.
3. Test the vocabulary subsampling technique across multiple supervised architectures (CNNs, ViTs, MLPs) to validate its generalizability beyond the specific models used in this study.