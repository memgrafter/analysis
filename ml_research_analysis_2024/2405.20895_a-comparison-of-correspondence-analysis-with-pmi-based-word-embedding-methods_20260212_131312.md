---
ver: rpa2
title: A comparison of correspondence analysis with PMI-based word embedding methods
arxiv_id: '2405.20895'
source_url: https://arxiv.org/abs/2405.20895
tags:
- word
- matrix
- methods
- values
- root-ca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the relationship between correspondence analysis
  (CA) and PMI-based word embedding methods, including GloVe and Word2Vec. CA is shown
  to be mathematically close to a weighted factorization of the PMI matrix, differing
  mainly in its weighting function.
---

# A comparison of correspondence analysis with PMI-based word embedding methods

## Quick Facts
- arXiv ID: 2405.20895
- Source URL: https://arxiv.org/abs/2405.20895
- Reference count: 40
- This study shows that correspondence analysis variants (ROOT-CA, ROOTROOT-CA) outperform standard PMI-based word embedding methods on word similarity tasks

## Executive Summary
This study explores the relationship between correspondence analysis (CA) and PMI-based word embedding methods, including GloVe and Word2Vec. CA is shown to be mathematically close to a weighted factorization of the PMI matrix, differing mainly in its weighting function. Two variants of CA, ROOT-CA and ROOTROOT-CA, are introduced and found to perform better than standard PMI-based methods on word similarity tasks. Empirical results demonstrate that ROOT-CA and ROOTROOT-CA achieve competitive or superior performance compared to PMI-based methods and BERT, especially in handling overdispersed word-context matrices.

## Method Summary
The study compares correspondence analysis (CA) with PMI-based word embedding methods using SVD on word-context co-occurrence matrices. CA variants include ROOT-CA (square-root transformation) and ROOTROOT-CA (fourth-root transformation) applied before SVD. PMI-based methods include PMI-SVD, PPMI-SVD, PMI-GSVD, GloVe, and SGNS. Models are trained on three corpora (Text8, BNC, Wiki052024) and evaluated on five word similarity datasets using Spearman's correlation coefficient ρ between cosine similarities and human similarity scores.

## Key Results
- ROOT-CA and ROOTROOT-CA variants outperform standard PMI-based methods on word similarity tasks
- CA variants achieve competitive or superior performance compared to BERT on most datasets
- Root transformations effectively handle overdispersion in word-context matrices, improving embedding quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Correspondence Analysis (CA) is mathematically close to a weighted factorization of the PMI matrix when the contingency ratio differences are small.
- **Mechanism:** The objective function of CA minimizes a weighted least squares loss where the fitting function is (pij/(pi+p+j) - 1) and the weighting function is the product of row and column margins pi+p+j. When (pij/(pi+p+j) - 1) is small, this approximates log(pij/(pi+p+j)) through a Taylor expansion, making CA close to PMI-SVD which uses log(pij/(pi+p+j)) with uniform weighting.
- **Core assumption:** The contingency ratios are close to independence (pij ≈ pi+p+j) in most cases, making the linear approximation valid.
- **Evidence anchors:** [abstract] "CA is mathematically close to the weighted factorization of the PMI matrix"; [section] "CA is approximately a weighted matrix factorization of log (pij/(pi+p+j)) with weighting function pi+p+j"

### Mechanism 2
- **Claim:** Overdispersion in word-context matrices negatively affects CA performance by creating extreme values that dominate the first dimensions.
- **Mechanism:** Overdispersed data creates extreme contingency ratio differences that contribute disproportionately to the total inertia. These extreme values cause the first few dimensions of CA to be dominated by single word pairs rather than capturing broader semantic relationships. Root transformations (square-root and fourth-root) stabilize variance and reduce the impact of these extreme values.
- **Core assumption:** Word-context co-occurrence data follows a Poisson distribution with overdispersion, requiring variance stabilization.
- **Evidence anchors:** [abstract] "CA is also applied to the fourth-root transformation... to deal with this overdispersion"; [section] "Overdispersion may negatively affect the performance of CA"

### Mechanism 3
- **Claim:** ROOT-CA and ROOTROOT-CA variants outperform standard CA by stabilizing variance in overdispersed word-context matrices.
- **Mechanism:** ROOT-CA applies a square-root transformation to word counts before CA, stabilizing variance for Poisson-distributed data. ROOTROOT-CA applies a fourth-root transformation, providing additional variance stabilization for highly overdispersed data. These transformations reduce the impact of extreme values while preserving the underlying structure needed for word embeddings.
- **Core assumption:** The square-root and fourth-root transformations effectively stabilize variance without losing meaningful semantic information.
- **Evidence anchors:** [abstract] "CA applied to a matrix where the entries undergo a square-root transformation (ROOT-CA) and a root-root transformation (ROOTROOT-CA)"; [section] "ROOT-CA differs from ROOT-CCA in the following way"

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: All CA variants and PMI-based methods use SVD to decompose matrices and extract low-dimensional representations. Understanding SVD is essential for implementing and modifying these methods.
  - Quick check question: What is the relationship between the rank of a matrix and the number of non-zero singular values in its SVD?

- **Concept: Pointwise Mutual Information (PMI)**
  - Why needed here: PMI is the core measure that connects all the methods studied. Understanding how PMI captures word-context relationships is crucial for interpreting results and making improvements.
  - Quick check question: How does PMI differ from simple co-occurrence counts, and why is this difference important for word embeddings?

- **Concept: Variance Stabilization**
  - Why needed here: Root transformations are used to stabilize variance in overdispersed data. Understanding variance stabilization helps in choosing appropriate transformations and interpreting their effects.
  - Quick check question: Why does a square-root transformation stabilize variance for Poisson-distributed data?

## Architecture Onboarding

- **Component Map:** Corpus loading -> Tokenization -> Co-occurrence counting -> Matrix construction -> Transformation (none/√/√√) -> SVD decomposition -> Embedding extraction -> Cosine similarity calculation -> Spearman correlation evaluation

- **Critical Path:** Load corpus and build word-context matrix → Apply appropriate transformation → Compute SVD of transformed matrix → Extract embeddings using singular vectors weighted by singular values → Evaluate on word similarity tasks

- **Design Tradeoffs:**
  - Computational cost: SVD on full matrices vs. sparse matrix methods
  - Memory usage: Dense matrix storage vs. sparse representations
  - Parameter tuning: Choice of transformation power, number of dimensions, singular value weighting
  - Evaluation: Spearman correlation vs. other metrics, different word similarity datasets

- **Failure Signatures:**
  - Poor performance: Check for extreme values dominating dimensions, insufficient variance stabilization
  - Memory errors: Matrix size too large, consider sparse representations or sampling
  - Slow computation: Full SVD on large matrices, consider incremental methods or GPU acceleration
  - Unstable results: Insufficient corpus size, consider data augmentation or regularization

- **First 3 Experiments:**
  1. Compare CA, ROOT-CA, and ROOTROOT-CA on a small corpus (Text8) with varying dimensions (50, 100, 200) to observe variance stabilization effects
  2. Test PMI-GSVD vs PMI-SVD to understand the impact of weighting function on extreme values
  3. Evaluate BERT vs CA variants on different word similarity datasets to identify task-specific strengths

## Open Questions the Paper Calls Out
None

## Limitations
- Claims based on relatively small-scale experiments with three corpora and five word similarity datasets
- Theoretical framework relies on assumptions about contingency ratio distributions that may not hold for all text data
- Does not explore computational efficiency comparisons between CA and PMI-based methods

## Confidence
- **High Confidence**: The mathematical relationship between CA and PMI matrix factorization is well-established through formal proofs and derivations.
- **Medium Confidence**: The empirical results showing ROOT-CA and ROOTROOT-CA outperforming standard PMI-based methods, as these depend on specific dataset choices and hyperparameter settings.
- **Medium Confidence**: The claim that root transformations effectively handle overdispersion, as this relies on assumptions about Poisson-distributed word-context data.

## Next Checks
1. Test ROOT-CA and ROOTROOT-CA variants on additional word similarity datasets and tasks (e.g., analogy completion, semantic textual similarity) to assess generalizability beyond the current five datasets.

2. Benchmark the runtime and memory requirements of CA variants versus PMI-based methods on corpora of varying sizes to evaluate practical feasibility for large-scale applications.

3. Evaluate performance across corpora with different characteristics (formal vs. informal language, technical vs. general domain) to understand when root transformations provide the most benefit.