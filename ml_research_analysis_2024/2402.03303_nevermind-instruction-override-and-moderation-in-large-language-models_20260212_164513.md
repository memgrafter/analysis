---
ver: rpa2
title: 'Nevermind: Instruction Override and Moderation in Large Language Models'
arxiv_id: '2402.03303'
source_url: https://arxiv.org/abs/2402.03303
tags:
- instruction
- arxiv
- context
- language
- following
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how well Large Language Models (LLMs) can
  follow instructions in conflicting situations, such as overriding knowledge baked
  into model weights, moderating context information, or performing jailbreaks. The
  study evaluates various open-source models and popular proprietary models across
  different parameter sizes (7B to 120B) using tasks like "needle in a haystack" retrieval,
  training overrides, moderation prompts, and full jailbreaks.
---

# Nevermind: Instruction Override and Moderation in Large Language Models

## Quick Facts
- arXiv ID: 2402.03303
- Source URL: https://arxiv.org/abs/2402.03303
- Reference count: 40
- One-line primary result: Larger LLMs (120B) significantly outperform smaller models (7B) at following instructions in conflicting scenarios including jailbreaks and moderation overrides.

## Executive Summary
This paper investigates how well Large Language Models (LLMs) can follow instructions in conflicting situations, such as overriding knowledge baked into model weights, moderating context information, or performing jailbreaks. The study evaluates various open-source models and popular proprietary models across different parameter sizes (7B to 120B) using tasks like "needle in a haystack" retrieval, training overrides, moderation prompts, and full jailbreaks. Results show that larger models perform significantly better at following instructions in these conflicting scenarios. Specifically, GPT-4 and Tess XL 120B were the best performers, particularly in handling overrides and jailbreaks. Additionally, the paper finds that scaling context lengths with NTK rope scaling requires maintaining a buffer from the perplexity cliff to preserve instruction-following capabilities.

## Method Summary
The study evaluates instruction-following capabilities across multiple model families (GPT-3.5, GPT-4, Llama 2 variants, Tess XL) with parameter sizes ranging from 7B to 120B. Test tasks include needle-in-haystack retrieval with varying context lengths (100-4096 tokens), override training with conflicting information, override moderation with suppression instructions, and jailbreak prompts. Evaluation uses substring matching with penalties, and context scaling employs NTK rope scaling with alpha parameters tuned to avoid perplexity cliffs. The study also implements a speculative decoding-based thought inhibition framework for external moderation.

## Key Results
- Larger models (120B) significantly outperform smaller models (7B) at instruction-following tasks, with Tess XL 120B achieving 50% jailbreak success rate versus 0% for 7B models
- Scaling context lengths with NTK rope scaling requires maintaining a buffer from the perplexity cliff to preserve instruction-following capabilities
- Improving instruction-following capability inherently increases susceptibility to jailbreaks, creating a fundamental tension with safety compliance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models demonstrate superior instruction-following in conflicting situations.
- Mechanism: Increased parameter count correlates with improved ability to override internal knowledge, moderate contextual information, and perform jailbreaks.
- Core assumption: Model size directly influences instruction-following capability.
- Evidence anchors:
  - [abstract]: "Results show that larger models perform significantly better at following instructions in these conflicting scenarios."
  - [section]: "Larger models follow instructions better."
  - [corpus]: Weak evidence; no direct citation of this specific claim in related works.
- Break condition: If model scaling plateaus or exhibits diminishing returns on instruction-following performance.

### Mechanism 2
- Claim: Scaling context lengths requires maintaining a buffer from the perplexity cliff.
- Mechanism: NTK rope scaling allows for extended context handling, but requires careful alpha parameter tuning to avoid performance degradation.
- Core assumption: Perplexity is a reliable proxy for instruction-following capability.
- Evidence anchors:
  - [abstract]: "When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities."
  - [section]: "Perplexity is a measure to evaluate the performance of language models... perplexity gives us a quantitative view on the performance of progressively larger models and context lengths."
  - [corpus]: No direct evidence in related works; this appears to be a novel finding.
- Break condition: If alternative scaling methods prove more effective or if perplexity proves unreliable.

### Mechanism 3
- Claim: Improving instruction-following and jailbreaks is fundamentally at odds with safety compliance.
- Mechanism: Models that are better at following instructions are also more susceptible to jailbreaks, necessitating external moderation mechanisms.
- Core assumption: Instruction-following and safety compliance are competing objectives.
- Evidence anchors:
  - [abstract]: "Finally, we observe improving instruction following, and subsequently instruction overrides/jailbreaks, is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines."
  - [section]: "Given that the most responsive/instruction following LLMs are the ones that can be most easily jailbroken, we discuss some methods of content moderation."
  - [corpus]: Weak evidence; related works focus on instruction-following but do not explicitly discuss the tension with safety compliance.
- Break condition: If new alignment techniques successfully reconcile instruction-following with safety compliance.

## Foundational Learning

- Concept: Perplexity
  - Why needed here: Perplexity serves as a quantitative metric for evaluating language model performance and is used to assess the impact of context scaling on instruction-following capability.
  - Quick check question: What does a lower perplexity value indicate about a language model's performance?
- Concept: Instruction-following
  - Why needed here: Instruction-following is the core ability being evaluated and is central to understanding the trade-offs between model performance and safety compliance.
  - Quick check question: How does the paper define instruction-following in the context of conflicting situations?
- Concept: Jailbreak
  - Why needed here: Jailbreaks are used as a test case for evaluating instruction-following in extreme scenarios and highlight the tension between performance and safety.
  - Quick check question: What is the purpose of a jailbreak prompt in the context of this paper?

## Architecture Onboarding

- Component map: Language model -> Context window -> Instruction prompt -> Evaluation metrics (perplexity, substring matching) -> Moderation mechanisms (keyword filtering, speculative decoding)
- Critical path: Input instruction prompt -> Language model processes instruction -> Model generates response -> Response evaluated against expected outcome -> Moderation applied if necessary
- Design tradeoffs: Model size vs. instruction-following capability; Context length vs. performance (perplexity cliff); Instruction-following vs. safety compliance
- Failure signatures: Incorrect response to instruction; Failure to override internal knowledge; Inability to moderate contextual information; Susceptibility to jailbreaks
- First 3 experiments: 1) Needle in a haystack test with varying context lengths and model sizes; 2) Override training test with conflicting information; 3) Jailbreak test with ignore all previous prompts instruction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of external moderation mechanisms like speculative decoding compare to fine-tuning approaches in maintaining both safety and model performance?
- Basis in paper: [explicit] The paper concludes that improving instruction-following and jailbreaks is fundamentally at odds with safety compliance, suggesting external moderation mechanisms are more effective.
- Why unresolved: The paper provides a proof-of-concept using speculative decoding but doesn't systematically compare this approach to various fine-tuning methods in terms of safety effectiveness and performance retention.
- What evidence would resolve it: Comparative studies measuring the performance degradation and safety effectiveness of speculative decoding-based moderation versus different fine-tuning approaches (RLHF, Constitutional AI, etc.) on the same benchmark tasks.

### Open Question 2
- Question: What is the relationship between perplexity cliffs in rope scaling and the degradation of instruction-following capabilities in longer context windows?
- Basis in paper: [explicit] The paper finds that scaling context lengths with NTK rope scaling requires maintaining a buffer from the perplexity cliff to preserve instruction-following capabilities.
- Why unresolved: While the paper identifies this relationship, it doesn't establish the exact mathematical or causal relationship between perplexity cliffs and instruction-following performance degradation.
- What evidence would resolve it: Detailed analysis mapping perplexity values to specific instruction-following metrics across different context lengths and rope scaling parameters, potentially revealing a predictive model for when instruction-following degrades.

### Open Question 3
- Question: Does the instruction-following capability gap between smaller and larger models persist across different model architectures (transformers vs. alternatives) or is it specific to transformer-based LLMs?
- Basis in paper: [inferred] The paper shows larger transformer models perform significantly better at instruction-following, but doesn't test alternative architectures like state-space models or other non-transformer approaches.
- Why unresolved: The study is limited to transformer-based models of varying sizes, leaving open whether the instruction-following advantages of scale are architecture-dependent.
- What evidence would resolve it: Comparative evaluation of instruction-following tasks across different model architectures (transformers, Mamba, RWKV, etc.) at various scales to determine if the scaling advantage is universal or architecture-specific.

## Limitations
- The paper does not explore whether the observed relationship between model size and instruction-following capability is causal or influenced by confounding factors like training methodology
- The proposed speculative decoding-based moderation framework is described but not extensively validated against traditional moderation techniques
- The tension between instruction-following and safety compliance is empirically observed but lacks formal theoretical grounding

## Confidence
- High Confidence: The observation that larger models perform better at instruction-following tasks, supported by direct experimental evidence across multiple model families and sizes
- Medium Confidence: The assertion that instruction-following and safety compliance are fundamentally at odds, based on empirical evidence but lacking formal theoretical framework
- Medium Confidence: The recommendation for external moderation mechanisms over baked-in safety filters, inferred from observed trade-offs but lacking comprehensive comparative analysis

## Next Checks
1. Conduct ablation studies to isolate whether the observed improvements in instruction-following are directly attributable to parameter count or other factors like architectural changes
2. Develop a formal framework that quantifies the relationship between instruction-following capability and safety compliance across different model architectures
3. Implement and benchmark multiple moderation strategies (traditional keyword filtering, speculative decoding, and hybrid approaches) across various instruction-following tasks to measure safety compliance and performance impact