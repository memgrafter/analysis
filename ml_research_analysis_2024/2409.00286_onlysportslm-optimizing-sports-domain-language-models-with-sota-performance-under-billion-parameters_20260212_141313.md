---
ver: rpa2
title: 'OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA Performance
  under Billion Parameters'
arxiv_id: '2409.00286'
source_url: https://arxiv.org/abs/2409.00286
tags:
- sports
- language
- onlysports
- performance
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OnlySportsLM, a 196M parameter language model
  optimized for sports-domain tasks. The authors created OnlySports Dataset, a 600B
  token corpus extracted from FineWeb, and OnlySports Benchmark, a novel evaluation
  method for sports knowledge generation.
---

# OnlySportsLM: Optimizing Sports-Domain Language Models with SOTA Performance under Billion Parameters

## Quick Facts
- arXiv ID: 2409.00286
- Source URL: https://arxiv.org/abs/2409.00286
- Authors: Zexin Chen; Chengxi Li; Xiangyu Xie; Parijat Dube
- Reference count: 10
- Key outcome: 196M parameter model achieves 37.62%/34.08% accuracy improvements over previous 135M/360M SOTA models and matches 1.7B/1.5B model performance in sports domain tasks

## Executive Summary
This paper presents OnlySportsLM, a 196M parameter language model optimized for sports-domain tasks. The authors created OnlySports Dataset, a 600B token corpus extracted from FineWeb, and OnlySports Benchmark, a novel evaluation method for sports knowledge generation. Using a deep and thin RWKV-v6 architecture (20 layers, 640 dimensions), they trained OnlySportsLM on half of the dataset. The model achieved significant accuracy improvements over previous state-of-the-art models while being substantially smaller, demonstrating that extensive domain-specific training data with optimized small model structures can overcome model size constraints in specialized domains.

## Method Summary
The authors developed a comprehensive pipeline for creating OnlySportsLM. First, they filtered the FineWeb corpus using URL keyword matching and a sports text classifier (Snowflake-arctic-embed-xs with binary classification layer) to create a 600B token sports-specific dataset. They then designed and trained a 196M parameter RWKV-v6 model with 20 layers and 640 dimensions using AdamW optimizer with weight decay 0.1, learning rate 6e-4 with cosine decay, training on half the dataset (315B tokens). The model was evaluated using OnlySports Benchmark, which employs multiple LLM evaluators (GPT-4o and Claude 3.5 Sonnet) to assess sports knowledge generation across accuracy/factuality and continuity/relevancy criteria.

## Key Results
- OnlySportsLM achieves 37.62% accuracy improvement over 135M parameter SOTA and 34.08% over 360M parameter SOTA
- Matches performance of larger models (SomlLM 1.7B, Qwen 1.5B) in sports domain while being 61% smaller
- Deep and thin architecture (20L-640D) performs comparably to wider alternatives in domain-specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Deep and thin architectures can outperform traditional wider models in domain-specific tasks when parameter count is constrained.
- **Mechanism**: Increasing depth while reducing width allows the model to learn more hierarchical representations specific to the domain, compensating for reduced parameter count.
- **Core assumption**: The domain-specific data contains enough structure that deeper layers can exploit this for better performance.
- **Evidence anchors**:
  - [abstract]: "optimizing the RWKV architecture for sports-related tasks, resulting in a 196M parameters model with 20-layer, 640-dimension structure"
  - [section 3.3]: "We observe that both traditional wider models and moderately deeper architectures perform well on OnlySports Benchmark. While the 12-layer wider model has the highest OS-acc (1.88) and OS-rel (2.42) scores, the 20 layers model shows comparable results in relevancy score and slightly less OS-acc (1.84)"
  - [corpus]: Weak evidence - corpus neighbors don't directly address architecture depth-width tradeoffs in domain-specific models

### Mechanism 2
- **Claim**: Extensive domain-specific training data can compensate for smaller model size in achieving competitive performance.
- **Mechanism**: The massive 600B token sports-specific dataset provides rich coverage of domain concepts, terminology, and patterns that smaller models can effectively learn.
- **Core assumption**: Quality and quantity of domain-specific data are more important than raw model size for specialized tasks.
- **Evidence anchors**:
  - [abstract]: "OnlySportsLM achieves a 37.62%/34.08% accuracy improvements over previous 135M/360M state-of-the-art models and matches the performance of larger models such as SomlLM 1.7B and Qwen 1.5B in the sports domain"
  - [section 4.2.1]: "OnlySportsLM outperforms all models by a significant margin. Notably, our model gains 34.44% accuracy over Qwen2-0.5B while being 61% smaller in size"
  - [corpus]: Weak evidence - corpus neighbors focus on general small model optimization rather than domain-specific data scaling effects

### Mechanism 3
- **Claim**: Domain-specific evaluation methods provide more accurate assessment of model capabilities than general benchmarks.
- **Mechanism**: The OnlySports Benchmark uses multiple LLM evaluators to assess sports knowledge generation, capturing domain-specific nuances that general benchmarks miss.
- **Core assumption**: Sports domain has unique characteristics requiring specialized evaluation criteria beyond general language understanding.
- **Evidence anchors**:
  - [section 3.2]: "we employ multiple state-of-the-art language models as evaluators, assessing generated responses across two key criteria: accuracy and factuality, and continuity and relevancy"
  - [abstract]: "OnlySports Benchmark, a novel evaluation method for sports knowledge generation, using 1000 diverse prompts and state-of-the-art (SOTA) language models for evaluation"
  - [corpus]: No direct evidence - corpus neighbors don't discuss domain-specific evaluation methodologies

## Foundational Learning

- **Concept**: Domain-specific data curation and filtering
  - Why needed here: Creating a high-quality 600B token sports dataset requires effective filtering of irrelevant content from massive web crawls
  - Quick check question: How does the URL filtering step reduce dataset size by 85% while maintaining domain relevance?

- **Concept**: Deep and thin model architecture design
  - Why needed here: Understanding the tradeoffs between depth and width in sub-billion parameter models for domain-specific tasks
  - Quick check question: Why does the 20-layer, 640-dimension model achieve comparable performance to the 12-layer, 768-dimension model despite having fewer parameters?

- **Concept**: Multi-dimensional evaluation using LLM judges
  - Why needed here: Assessing sports knowledge generation requires evaluating both factual accuracy and contextual continuity
  - Quick check question: How do the two evaluation criteria (accuracy/factuality and continuity/relevancy) complement each other in assessing sports domain performance?

## Architecture Onboarding

- **Component map**: Data filtering → Tokenization → RWKV-v6 transformer (20L, 640D) → AdamW optimization → Cosine learning rate decay → OnlySports Benchmark evaluation
- **Critical path**: Data filtering → Tokenization → Model training → Evaluation with OnlySports Benchmark
- **Design tradeoffs**: Depth vs width tradeoff (20L vs 12L), dataset size vs training budget (600B vs 315B tokens), evaluation criteria selection
- **Failure signatures**: Loss spikes during training, inconsistent performance across different benchmark types, overfitting on domain-specific data
- **First 3 experiments**:
  1. Train baseline 12-layer model on 100M tokens to establish performance floor
  2. Train 20-layer model with same parameter count on 100M tokens to test depth advantage
  3. Train 20-layer model on full 315B token subset to evaluate scaling effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the model's general benchmark performance improvement indicate actual transfer learning or just a correlation artifact?
- Basis in paper: [inferred] The authors note that OnlySportsLM shows unexpected performance improvements across all general benchmarks during training, suggesting domain-specific training may enhance general language understanding.
- Why unresolved: The study doesn't investigate whether the general benchmark improvements are due to genuine transfer learning or could be explained by other factors like regularization effects or specific task correlations.
- What evidence would resolve it: Controlled experiments comparing OnlySportsLM against models trained on other specific domains, or ablation studies removing sports-specific elements to isolate the effect.

### Open Question 2
- Question: What is the minimum dataset size required to achieve the observed performance improvements in domain-specific language models?
- Basis in paper: [explicit] The authors state they only trained on half of the 600B token dataset due to funding constraints, yet achieved significant improvements over previous state-of-the-art models.
- Why unresolved: The paper doesn't explore whether the performance gains would continue with smaller dataset sizes or if there's a threshold below which the benefits disappear.
- What evidence would resolve it: Training and evaluating models on progressively smaller subsets of the OnlySports Dataset to identify the point of diminishing returns.

### Open Question 3
- Question: How does the deep and thin architecture compare to other efficient model designs across different specialized domains?
- Basis in paper: [explicit] The authors chose a 20-layer, 640-dimension structure based on experiments showing comparable results to wider models, but this contradicts previous findings on general-purpose models.
- Why unresolved: The paper only tests this architecture on sports data, leaving open whether the observed performance pattern generalizes to other domains or is specific to sports-related tasks.
- What evidence would resolve it: Replicating the architectural experiments across multiple domains (e.g., medical, finance, legal) to identify if the deep and thin approach consistently outperforms or underperforms alternative designs.

## Limitations
- The architectural claims rest on comparing only three configurations across limited benchmark variations
- Dataset filtering methodology relies on URL keyword matching without reporting precision/recall metrics
- Evaluation methodology depends on LLM judges introducing potential biases not quantified or controlled

## Confidence
- **High confidence**: OnlySportsLM achieves 37.62%/34.08% accuracy improvements over previous 135M/360M models - directly measurable from benchmark results
- **Medium confidence**: OnlySportsLM matches performance of larger models (1.7B/1.5B) in sports domain tasks - supported by benchmark results but relies on self-reported performance and novel evaluation method
- **Low confidence**: Deep and thin architectures are universally superior for domain-specific tasks - evidence shows only single domain performance without broader generalization

## Next Checks
1. **Architecture ablation study**: Systematically test 5-7 different depth-width configurations on the same dataset to establish optimal points
2. **Dataset quality validation**: Conduct manual inspection of randomly sampled 1,000 passages to estimate false positive rate in sports domain classification
3. **Evaluation methodology stress test**: Re-run the OnlySports Benchmark with multiple different LLM judges to assess consistency and quantify inter-judge agreement