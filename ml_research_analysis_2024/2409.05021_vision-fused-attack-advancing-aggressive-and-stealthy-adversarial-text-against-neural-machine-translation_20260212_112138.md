---
ver: rpa2
title: 'Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text against
  Neural Machine Translation'
arxiv_id: '2409.05021'
source_url: https://arxiv.org/abs/2409.05021
tags:
- adversarial
- text
- space
- translation
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of neural machine translation
  (NMT) models to adversarial attacks. While existing attacks focus on semantic perturbations,
  they often fail to achieve high attack success rates or maintain human imperceptibility.
---

# Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text against Neural Machine Translation

## Quick Facts
- arXiv ID: 2409.05021
- Source URL: https://arxiv.org/abs/2409.05021
- Authors: Yanni Xue; Haojie Hao; Jiakai Wang; Qiang Sheng; Renshuai Tao; Yu Liang; Pu Feng; Xianglong Liu
- Reference count: 40
- Key outcome: VFA framework achieves up to 81%/14% improvements on attack success rate/SSIM compared to existing methods

## Executive Summary
This paper addresses the vulnerability of neural machine translation (NMT) models to adversarial attacks, where existing methods often fail to achieve high attack success rates while maintaining human imperceptibility. The authors propose a Vision-fused Attack (VFA) framework that enhances the solution space by incorporating visual characteristics and selects adversarial text based on human perception. By mapping semantic solution space into a visual solution space and using perception-retained selection strategies, VFA demonstrates significant improvements in both attack aggressiveness and stealthiness across various NMT models including large language models like LLaMA and GPT-3.5.

## Method Summary
The Vision-fused Attack (VFA) framework addresses adversarial attacks on NMT systems through a two-pronged approach: vision-merged solution space enhancement and perception-retained adversarial text selection. The method uses a reverse translation block to expand the semantic solution space by generating semantically similar sentences, then maps this space into a visual solution space via text-image transformation using character radical and pixel similarity. The perception-retained selection strategy applies global perceptual constraints and improved word replacement operations to ensure the final adversarial text is both aggressive in degrading translation quality and stealthy in maintaining human perceptual similarity. The framework is evaluated on multiple datasets (WMT19, WMT18, TED, ASPEC) and shows substantial improvements in attack success rate and structural similarity index measure compared to existing methods.

## Key Results
- VFA achieves up to 81% improvement in attack success rate compared to baseline methods
- Perceptual similarity (SSIM) improves by up to 14% while maintaining high attack effectiveness
- Demonstrates effectiveness against both traditional NMT models and large language models like LLaMA and GPT-3.5
- Outperforms existing methods by large margins across multiple datasets and attack scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision-fused solution space enhancement improves attack success rate by expanding the searchable adversarial candidate set.
- Mechanism: The framework maps semantic solution space into a visual solution space via text-image transformation, allowing access to characters that are visually similar but semantically different.
- Core assumption: Human text perception relies more on visual similarity than semantic similarity for recognition.
- Evidence anchors:
  - [abstract] "we design the vision-merged solution space enhancement strategy to enlarge the limited semantic solution space"
  - [section 3.3] "we transform the original input to increase the variety of words... Thus, we can map semantic space into a larger visual space"
  - [corpus] Weak evidence: neighbor papers focus on stealthy attacks but not specifically on vision-fused NMT adversarial text
- Break condition: If visual perception model fails to capture human perceptual similarity or if NMT models become robust to visual perturbations.

### Mechanism 2
- Claim: Perception-retained adversarial text selection ensures human imperceptibility while maintaining attack effectiveness.
- Mechanism: The selection strategy uses improved word replacement operations and global perceptual constraints to filter candidates that align with human text-reading mechanisms.
- Core assumption: Humans process text visually before semantically, so visual similarity is crucial for imperceptibility.
- Evidence anchors:
  - [abstract] "we propose the perception-retained adversarial text selection strategy to align the human text-reading mechanism"
  - [section 3.4] "we introduce a visual perceptual similarity score... This constraint measures the global perceptual similarity"
  - [corpus] Weak evidence: neighbor papers discuss stealthy attacks but don't specifically address human perceptual alignment in NMT
- Break condition: If the perceptual similarity metric doesn't correlate with human judgment or if semantic constraints become too restrictive.

### Mechanism 3
- Claim: Reverse translation block enhances the semantic solution space before visual mapping.
- Mechanism: Auxiliary translation models generate semantically similar sentences that serve as expanded search space for adversarial candidates.
- Core assumption: Additional semantically similar sentences provide more diverse word choices for adversarial generation.
- Evidence anchors:
  - [section 3.3] "we initially expand essential semantic space through reverse translation... For each reference translation y, we use an auxiliary translation model Maux to obtain transformed ˆ x"
  - [abstract] "we design the vision-merged solution space enhancement strategy to enlarge the limited semantic solution space"
  - [corpus] Weak evidence: neighbor papers don't specifically address reverse translation for semantic space expansion in adversarial attacks
- Break condition: If reverse translation introduces semantic drift or if auxiliary models produce poor quality translations.

## Foundational Learning

- Concept: Semantic similarity vs. visual similarity in text representation
  - Why needed here: The framework relies on distinguishing between semantic and visual solution spaces for adversarial text generation
  - Quick check question: What's the difference between semantic similarity metrics (like BLEU) and visual similarity metrics (like SSIM) in evaluating text?

- Concept: Masked language model for word importance ranking
  - Why needed here: Used to identify which words to target for adversarial modifications
  - Quick check question: How does masking words and predicting their probability help determine word importance in a sentence?

- Concept: Text-image transformation and glyph-based character similarity
  - Why needed here: Core mechanism for mapping semantic space to visual space through character radical and pixel similarity
  - Quick check question: How can characters be compared visually using their radical components or pixel representations?

## Architecture Onboarding

- Component map:
  Input -> Reverse Translation Block -> Semantic Importance Ranking -> Text-Image Transformation Block -> Perception-Retained Selection Strategy -> Adversarial Text Output

- Critical path:
  1. Generate reverse translation candidates (ˆ x)
  2. Rank word importance in candidates
  3. Transform semantic space to visual space (S = Srad ∪ Spix)
  4. Apply improved word replacement with perceptual constraints
  5. Select final adversarial text based on similarity thresholds

- Design tradeoffs:
  - Visual solution space provides more candidates but may include semantically inappropriate options
  - Global perceptual constraint ensures human imperceptibility but may limit attack aggressiveness
  - Reverse translation enhances semantic space but adds computational overhead

- Failure signatures:
  - Low ASR with high SSIM: Perceptual constraints too restrictive
  - High ASR with low SSIM: Visual similarity metrics not aligned with human perception
  - Poor performance on LLMs: Visual perturbations less effective against models with human-like perception

- First 3 experiments:
  1. Test basic vision-fused attack without reverse translation on small dataset to verify visual solution space benefits
  2. Compare perceptual similarity metrics (SSIM vs. human judgment) on generated adversarial texts
  3. Evaluate attack transferability from common NMT models to LLMs with different perceptual capabilities

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several implicit questions emerge:

### Open Question 1
- Question: How does the effectiveness of VFA scale with increasing model size and complexity beyond LLaMA and GPT-3.5?
- Basis in paper: [explicit] The paper evaluates VFA on LLaMA and GPT-3.5, but does not explore larger or more complex models.
- Why unresolved: The paper focuses on specific models, leaving open the question of generalizability to future, more advanced models.
- What evidence would resolve it: Experiments demonstrating VFA's effectiveness on significantly larger models like GPT-4 or other state-of-the-art language models.

### Open Question 2
- Question: What is the impact of VFA on machine translation models for languages with complex character systems, such as Arabic or Korean?
- Basis in paper: [inferred] VFA focuses on Chinese and Japanese, which have character-based writing systems, but does not address other complex scripts.
- Why unresolved: The paper does not explore the applicability of VFA to other languages with different character complexities.
- What evidence would resolve it: Experiments applying VFA to translation tasks involving Arabic, Korean, or other languages with complex scripts.

### Open Question 3
- Question: How does VFA perform under black-box settings where the architecture and parameters of the target model are unknown?
- Basis in paper: [explicit] The paper primarily discusses white-box and some black-box settings but does not provide extensive black-box attack results.
- Why unresolved: The paper lacks detailed analysis of VFA's effectiveness in black-box scenarios, which are more realistic in practice.
- What evidence would resolve it: Comprehensive experiments evaluating VFA's performance in black-box settings across various models and datasets.

## Limitations
- Visual similarity metrics may not fully correlate with human perceptual judgment across different scripts and languages
- Effectiveness of vision-based perturbations on LLMs raises questions about whether these models process text more like humans or through semantic pathways
- Reverse translation block introduces additional complexity that may affect semantic fidelity of adversarial examples

## Confidence
- **High Confidence:** The framework's core architecture and the general approach of incorporating visual similarity for stealthy attacks are well-founded and supported by experimental results
- **Medium Confidence:** The effectiveness of vision-fused attacks against LLMs, as these models may have different perceptual mechanisms compared to traditional NMT models
- **Low Confidence:** The long-term robustness of vision-fused attacks, particularly as NMT models evolve to become more resistant to visual perturbations

## Next Checks
1. **Human Perception Validation:** Conduct human subject studies to verify that SSIM scores correlate with actual human perceptual similarity judgments across different languages and scripts
2. **Cross-Lingual Transferability:** Test the vision-fused attack's effectiveness on language pairs not seen during training, particularly focusing on non-Latin scripts to assess the visual similarity metrics' generalizability
3. **Adversarial Training Resistance:** Evaluate how traditional adversarial training techniques affect the robustness of NMT models against vision-fused attacks, and whether models can be made resistant to visual perturbations