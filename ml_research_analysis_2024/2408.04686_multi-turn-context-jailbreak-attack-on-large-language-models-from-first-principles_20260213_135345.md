---
ver: rpa2
title: Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles
arxiv_id: '2408.04686'
source_url: https://arxiv.org/abs/2408.04686
tags:
- attack
- attacks
- multi-turn
- jailbreak
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a key limitation in existing multi-turn jailbreak
  attacks on large language models (LLMs): they lack targeted strategies for leveraging
  multi-turn dialogues to enhance attack effectiveness while maintaining semantic
  coherence. The authors propose Context Fusion Attack (CFA), a method that filters
  and extracts malicious keywords from attack targets, generates contextual scenarios
  around these terms, dynamically integrates the target into the scenarios, and replaces
  malicious key terms to reduce direct malicious intent.'
---

# Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles

## Quick Facts
- arXiv ID: 2408.04686
- Source URL: https://arxiv.org/abs/2408.04686
- Reference count: 9
- Key outcome: Context Fusion Attack (CFA) significantly outperforms baseline methods in attack success rate, divergence, and harmfulness across six mainstream LLMs

## Executive Summary
This paper addresses a critical limitation in existing multi-turn jailbreak attacks on large language models (LLMs): the lack of targeted strategies for leveraging multi-turn dialogues to enhance attack effectiveness while maintaining semantic coherence. The authors propose Context Fusion Attack (CFA), a method that filters and extracts malicious keywords from attack targets, generates contextual scenarios around these terms, dynamically integrates the target into the scenarios, and replaces malicious key terms to reduce direct malicious intent. CFA is validated against six mainstream LLMs (including Llama3 and GPT-4) and three red team datasets, demonstrating significant improvements over baseline methods.

## Method Summary
The Context Fusion Attack method involves four main steps: (1) filtering and extracting malicious keywords from the target prompt based on semantic relevance, (2) generating contextual scenarios around the extracted keywords using prompt engineering frameworks like CO-STAR, (3) dynamically integrating the attack target into the contextual scenarios, and (4) replacing malicious keywords with contextually appropriate alternatives to reduce overt malicious intent. The approach is tested across three public datasets (Advbench, MaliciousInstruct, Jailbreakbench) containing 520, 100, and 100 malicious prompts respectively, targeting six mainstream LLMs.

## Key Results
- CFA achieves significantly higher attack success rates compared to baseline methods across all tested LLMs
- The method demonstrates superior effectiveness on Llama3 and GPT-4, the two most capable models tested
- CFA shows improved performance in both divergence (semantic similarity and matching degree) and harmfulness metrics (toxicity and insult assessments)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CFA reduces overt malicious intent by replacing harmful keywords with contextually appropriate terms while maintaining semantic coherence.
- Mechanism: The method filters and extracts malicious keywords from the target prompt, generates contextual scenarios around these keywords, and dynamically integrates the target into these scenarios. It then replaces the malicious keywords with contextually relevant alternatives, effectively disguising the harmful intent.
- Core assumption: Large language models trained primarily on single-turn or limited multi-turn data lack robust contextual understanding in complex scenarios, creating a vulnerability that can be exploited.
- Evidence anchors:
  - [abstract]: "filtering and extracting key terms from the target, constructing contextual scenarios around these terms, dynamically integrating the target into the scenarios, replacing malicious key terms within the target, and thereby concealing the direct malicious intent"
  - [section]: "Initial keyword filtering removes obviously malicious terms lacking semantic necessity. Subsequently, keyword extraction identifies terms closely associated with malicious behaviors. These keywords guide context-building to ensure direct relevance to the attack target."
  - [corpus]: Weak evidence - neighboring papers focus on general multi-turn jailbreak approaches but don't specifically address the keyword replacement mechanism described here.
- Break condition: If LLMs receive comprehensive training on multi-turn contextual scenarios with harmful content, the security threshold for context-aware prompts would increase, making keyword replacement less effective.

### Mechanism 2
- Claim: Multi-turn contexts can be leveraged to dynamically load malicious objectives, reducing the likelihood of triggering security mechanisms.
- Mechanism: By constructing multi-turn dialogue sequences where each turn builds upon previous context, CFA can introduce malicious content progressively while maintaining plausible deniability. The context provides semantic cover that makes the final malicious prompt appear benign.
- Core assumption: LLMs exhibit different security thresholds for single-turn versus multi-turn prompts, with multi-turn prompts receiving more lenient treatment due to the complexity of evaluating context.
- Evidence anchors:
  - [abstract]: "context-based contextual fusion black-box jailbreak attack method, named Context Fusion Attack (CFA)"
  - [section]: "We have simplified the security mechanisms of LLMs into a threshold-based triggering mechanism... Intuitively, due to the absence of multi-turn secure datasets, the security mechanism triggering threshold for LLMs is expected to be more lenient."
  - [corpus]: Moderate evidence - neighboring papers like "RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking" suggest this is an active area of research, though specific mechanisms differ.
- Break condition: If security systems implement context-aware evaluation that considers the full dialogue history rather than treating each prompt independently, the effectiveness of this approach would diminish.

### Mechanism 3
- Claim: Stepwise simplification of automation requirements for LLMs in multi-turn attacks reduces false positives and improves attack consistency.
- Mechanism: CFA breaks down the complex task of generating malicious content into smaller, more manageable steps (keyword extraction, context generation, target integration), each of which can be handled by the LLM with higher reliability. This reduces the cognitive load on the model and improves semantic coherence.
- Core assumption: Complex multi-turn attack strategies require strong comprehension and logical reasoning capabilities from LLMs, which can lead to semantic divergence and false positives when attempted in a single step.
- Evidence anchors:
  - [abstract]: "simplifying the automation dependencies of LLMs, reducing the capability demands of attack strategies on LLMs, and enhancing attack stability"
  - [section]: "Attack automation often relies on the generation capabilities of LLMs, but in multi-turn attacks, complex attack strategies require strong comprehension and logical reasoning capabilities from LLMs."
  - [corpus]: Weak evidence - neighboring papers focus on attack effectiveness but don't specifically address the stepwise simplification approach described here.
- Break condition: If LLMs develop more robust reasoning capabilities that can handle complex multi-turn instructions in a single step, the stepwise approach would become unnecessary.

## Foundational Learning

- Concept: Natural Language Understanding (NLU) and keyword extraction
  - Why needed here: The attack method relies on accurately identifying and extracting semantically relevant keywords from malicious prompts to construct appropriate contexts.
  - Quick check question: How would you design an algorithm to distinguish between malicious keywords that carry substantive meaning versus those that are merely descriptive?

- Concept: Context-aware prompt engineering and multi-turn dialogue systems
  - Why needed here: CFA's effectiveness depends on understanding how LLMs process context across multiple turns and how to construct prompts that leverage this behavior.
  - Quick check question: What are the key differences between how LLMs handle single-turn versus multi-turn prompts, and how does this affect their security responses?

- Concept: Dynamic loading techniques in software security
  - Why needed here: The paper explicitly draws inspiration from dynamic loading techniques, where malicious behavior manifests at runtime based on triggering conditions rather than being immediately apparent.
  - Quick check question: How does the concept of "dynamic loading" in traditional software security translate to the context of LLM jailbreak attacks?

## Architecture Onboarding

- Component map: Keyword Extraction Module -> Context Generation Engine -> Target Integration System -> Keyword Replacement Layer -> LLM Interaction Interface

- Critical path:
  1. Receive attack target prompt
  2. Extract and filter keywords
  3. Generate contextual scenarios
  4. Integrate target into context
  5. Replace malicious keywords
  6. Execute multi-turn attack sequence
  7. Evaluate output for success

- Design tradeoffs:
  - Keyword granularity vs. semantic coherence: More specific keywords improve context relevance but may increase detection risk
  - Context complexity vs. execution efficiency: Richer contexts improve attack success but require more turns and computational resources
  - Keyword replacement strategy vs. attack clarity: More extensive replacement improves stealth but may reduce the clarity of the final malicious output

- Failure signatures:
  - Semantic divergence: Generated contexts don't logically connect to the attack target
  - Model refusal: LLM detects the malicious intent despite keyword replacement
  - Output incoherence: Final output doesn't match the intended malicious content
  - Context breakdown: Multi-turn conversation loses coherence partway through

- First 3 experiments:
  1. Baseline effectiveness test: Run standard attack prompts against target models to establish baseline success rates
  2. Keyword extraction validation: Test the keyword extraction algorithm on diverse malicious prompts to ensure accurate identification
  3. Context generation quality: Evaluate the coherence and relevance of generated contexts for a sample of attack targets before attempting full multi-turn attacks

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the effectiveness of CFA scale with the complexity and length of attack targets, particularly for targets requiring nuanced contextual understanding?
  - Basis in paper: [inferred] The paper mentions that current secure alignment datasets lack long-text multi-turn interactions with complex contextual understanding, and that constructing such datasets is costly and effort-intensive.
  - Why unresolved: The paper does not provide systematic experiments varying the complexity or length of attack targets to measure how CFA's effectiveness changes. It only tests on existing public datasets without varying target complexity systematically.
  - What evidence would resolve it: Controlled experiments testing CFA across attack targets of varying semantic complexity, length, and contextual requirements, measuring success rates, semantic deviation, and harmfulness metrics for each complexity level.

- **Open Question 2**: To what extent can LLM security mechanisms adapt to defend against CFA-style attacks that leverage contextual multi-turn dialogue patterns?
  - Basis in paper: [explicit] The authors note that "multi-turn dialogues represent a comprehensive reflection of the capabilities of LLMs" and that current secure alignment often neglects complex multi-turn contextual scenarios, creating a "scarcity" that diminishes the integrity of LLMs protection strategies.
  - Why unresolved: The paper does not test how existing security mechanisms (like perplexity filtering, gradient probing, or malicious content detection) perform against CFA specifically, nor does it propose or evaluate potential defensive adaptations.
  - What evidence would resolve it: Empirical studies testing CFA against enhanced security mechanisms that specifically address contextual multi-turn scenarios, including new detection methods that can identify malicious patterns within contextually rich dialogues.

- **Open Question 3**: What are the fundamental limitations of the dynamic loading intuition that CFA draws from software security, and how do these limitations manifest in real-world attack scenarios?
  - Basis in paper: [explicit] The authors explicitly draw inspiration from dynamic loading techniques in software security, stating that CFA transforms "static subversive attacks into contextual dynamics, thereby effectively circumventing existing security mechanisms."
  - Why unresolved: The paper does not critically examine the assumptions behind this analogy or test scenarios where contextual dynamics might fail, such as when LLMs can detect the gradual buildup of malicious intent or when context becomes too complex for the attack to maintain coherence.
  - What evidence would resolve it: Systematic analysis of CFA failures in scenarios where contextual complexity exceeds the model's comprehension capacity, or where security mechanisms successfully detect the progressive nature of the attack despite contextual camouflage.

## Limitations

- The paper does not provide complete implementation details for key components like keyword extraction and context generation, making faithful reproduction difficult.
- Effectiveness on models other than Llama3 and GPT-4 is not thoroughly analyzed, with noted issues of output repetition and chaotic generation on Vicuna.
- The simplified threshold-based security model may not capture the full complexity of modern LLM safety systems.

## Confidence

- **High Confidence**: The core claim that multi-turn contexts can be leveraged to improve jailbreak attack effectiveness is well-supported by experimental results and aligns with existing research on LLM vulnerabilities.
- **Medium Confidence**: The specific effectiveness improvements reported are based on the experimental setup described, but lack of implementation details introduces uncertainty about reproducibility and generalizability.
- **Low Confidence**: The claim that stepwise simplification of automation requirements is the primary driver of improved attack stability is weakly supported, as the paper doesn't provide direct comparative evidence.

## Next Checks

1. **Reproducibility Test**: Implement the CFA method based solely on the information provided in the paper, then attempt to reproduce the reported attack success rates on at least two of the tested models (e.g., Llama3 and GPT-4). Compare results with the paper's claims to assess implementation fidelity.

2. **Cross-Model Generalization**: Test the CFA method on additional LLM models not included in the original evaluation (e.g., Claude, Gemini, or other open-source models of similar scale). This would validate whether the attack effectiveness is generalizable beyond the specific models tested.

3. **Security System Analysis**: Conduct a detailed analysis of how different LLM security systems respond to CFA attacks. This includes examining whether the attacks succeed by exploiting specific vulnerabilities in certain safety implementations or whether they represent a more fundamental weakness in multi-turn context processing across different security architectures.